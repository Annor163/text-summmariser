@&#MAIN-TITLE@&#
Growing construction of conlitron and multiconlitron

@&#HIGHLIGHTS@&#
Multiconlitron is a general piecewise linear classifier as a union of conlitrons.A new technique is proposed for improving the performance of a conlitron/multiconlitron.This technique consists of two basic operations, SQUEEZE and INFLATE.SQUEEZE and INFLATE result in two new algorithms, GSCA and GSMA.GSCA/GSMA can construct a better conlitron/multiconlitron.

@&#KEYPHRASES@&#
Conlitron,Multiconlitron,Piecewise linear classifiers,Piecewise linear learning,Support vector machines,

@&#ABSTRACT@&#
Based on the concepts of conlitron and multiconlitron, we propose a growing construction technique for improving the performance of piecewise linear classifiers on two-class problems. This growing technique consists of two basic operations: SQUEEZE and INFLATE, which can produce relatively reliable linear boundaries. In the convexly separable case, the growing process, forming a growing support conlitron algorithm (GSCA), starts with an initial conlitron and uses SQUEEZE to train a new conlitron, moving its classification boundary closer to the interior convex region and fitting the data distribution better statistically. In the commonly separable case, the growing process, forming a growing support multiconlitron algorithm (GSMA), starts with an initial multiconlitron and uses INFLATE and SQUEEZE to train a new multiconlitron, making its classification boundary adjusted to improve the generalization ability. Experimental evaluation shows that the growing technique can simplify the structure of a conlitron/multiconlitron effectively by reducing the number of linear functions, largely keeping and even greatly improving the level of classification performances. Therefore, it would come to play an important role in the subsequent development of piecewise linear learning, with the main goal to improve piecewise linear classifiers in a general framework.

@&#INTRODUCTION@&#
There have been extensive studies on designing a classifier with one hyperplane. If two data sets are linearly separable, a perception algorithm [1] may separate them without guarantee of the optimality, whereas a support vector machine (SVM) [2,3] can produce an optimal separating hyperplane in the sense of maximal margin. However, it needs an appropriate kernel function [4,5] for solving nonlinear problems. This may lead to some difficulties in the interpretability/transparency of metric change without intuitive geometric meaning [6,7]. Accordingly, there remains a problem that, how to directly design a large margin classifier in the original space, not using any nonlinear kernels. A possible solution is to construct a piecewise linear classifier (PLC) via large margin training with multiple hyperplanes instead of one hyperplane.The main advantage of a PLC is very simple to implement with requirement of low memory usage, making it suitable for small reconnaissance robots, intelligent cameras, imbedded and real-time systems, and portable devices. Despite of the simplicity, a PLC-building technique usually requires complex computational procedures for selecting the number of hyperplanes and minimizing the error of classification. In fact, it is a challenging and complicated task to synthesize an optimal PLC with appropriate number of hyperplanes. For addressing this problem, there have been a lot of methods such as based on linear programming [8,9], local training [10–12], decision tree [13,14], and max–min separability [15–18]. However, these methods generally construct a PLC not from the viewpoint of large margin, which has been widely accepted as an effective paradigm in machine learning. Moreover, they need to manually specify a set of integers in advance, for describing how to organize linear functions in groups.Using the notion of large margin, our previous research [19] presented a general framework for constructing a PLC based on the concepts of convex separability, conlitron and multiconlitron, etc. Under the framework, cross-distance minimization algorithm (CDMA) is applied to solving linearly separable problems, support conlitron algorithm (SCA) to solving convexly separable problems, and support multiconlitron algorithm (SMA) to solving commonly separable problems. In theory, support conlitrons and support multiconlitrons can be thought of as a nonkernel extension of SVMs. They can be dynamically constructed in training without a prespecified set of integers grouping linear functions. And they often perform better than linear SVMs, but below Gaussian kernel SVMs. The related problems to improving their generalization performances are forming the new direction of piecewise linear learning.In this paper, our motivation is to propose a growing construction technique for improving the performance of SCA and SMA. This work can effectively simplify the structure of a PLC by reducing the number of linear functions in a conlitron/multiconlitron, while largely keeping and even greatly improving the level of classification performances. Hence, it is significant in terms of integrating a PLC into small reconnaissance robots, intelligent cameras, real-time systems, portable devices or even in financial systems (e.g., recognizing bankrupt and nonbankrupt enterprises).We organize the rest of the paper as follows. In Section 2, we give a review of related work. In Section 3, we describe a more efficient CDMA. Then, we present two basic growing operations: SQUEEZE and INFLATE, to develop a growing support conlitron algorithm (GSCA) and a growing support multiconlitron algorithm (GSMA) in Sections 4 and 5, respectively. Furthermore, in Section 6 we compare their performances with SCA and SMA as well as with linear SVM and RBF SVM by experimenting on UCI benchmark data sets and artificial data sets. Finally, we summarize the main contributions and discuss future research in Section 7.

@&#CONCLUSIONS@&#
