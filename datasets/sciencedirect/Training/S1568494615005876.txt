@&#MAIN-TITLE@&#
A multi-attribute decision-making model for the robust classification of multiple inputs and outputs datasets with uncertainty

@&#HIGHLIGHTS@&#
The proposed multiple inputs and outputs (MIO) classification method designated as the FVM-index method integrates fuzzy set theory (FST), variable precision rough set (VPRS) theory, and a modified cluster validity index (MCVI) function, and is designed specifically to filter out the uncertainty and inaccuracy inherent in the surveyed MIO real-valued dataset; thereby improving the classification performance.The results confirm that the proposed FVM-index method provides a good MIO classification performance even in the presence of inaccuracy and uncertainty. As a result, it provides a robust approach for the extraction of reliable decision-making rules.The proposed FVM-index method could effectively applied to the real applications of augmented reality product design and data envelopment analysis.

@&#KEYPHRASES@&#
Multi-criteria decision making,Multiple inputs and outputs datasets,Variable precision rough set,Fuzzy set theory,FVM-index method,

@&#ABSTRACT@&#
Many multiple-criteria decision-making (MCDM) methods have been proposed for decision-making environments. However, the performance of these methods is degraded by the uncertainty and inaccuracy which characterizes most practical decision-making environments as a result of the inherent prejudices and preferences of the decision-makers or experts and an insufficient volume of multiple inputs and outputs (MIO) information. Accordingly, the present study proposes an enhanced MIO classification method to address these limitations of existing MCDM methods. The proposed MIO classification method designated as the FVM-index method integrates fuzzy set theory (FST), variable precision rough set (VPRS) theory, and a modified cluster validity index (MCVI) function, and is designed specifically to filter out the uncertainty and inaccuracy inherent in the surveyed MIO real-valued dataset; thereby improving the classification performance. The effectiveness of the proposed approach is first demonstrated by comparing the MIO classification results obtained for three relating UCI datasets: (1) the original dataset; (2) a dataset with a large amount of inaccurate instances; and (3) an FVM-index filtered dataset extracted from the original dataset using a statistical approach. Then, the validity of the proposed approach is illustrated by using an Augmented Reality product design and a hospital related datasets. The results confirm that the proposed FVM-index method provides a good classification performance even in the presence of inaccuracy and uncertainty. As a result, it provides a robust approach for the extraction of reliable decision-making rules.

@&#INTRODUCTION@&#
As computer technology and information-gathering methods have advanced in recent decades, the size and complexity of modern day data repositories have increased accordingly. Data mining technique is widely used to extract information or knowledge from large volumes of data and can be performed using a variety of different tools, including association, classification, clustering, prediction, sequential patterns, and similar time sequences [1–4]. Among these methodologies, classification and regression, referring to the type of predictive modeling, build a model that will permit the value of one response variable (also called the dependent, or target variable) to be predicted from the known values of other explanatory variables (also called the independent or predictor variables) [1,2]. Decision-makers typically use machine learning or data mining techniques, such as classification or regression methods, to discover the useful knowledge within a dataset. Thus, in enabling decision-makers to make reliable decisions when faced with such large quantities of data, highly efficient and robust data classification/regression methods are required. For the case where the decision involves a single criterion (e.g., the cost), the decision-making process is relatively straightforward. However, in many practical cases, the decision-making process involves a number of different criteria, e.g., price, size, power consumption, functionality, and so on; all of these have a different weighting and must be jointly considered. In such a case, more sophisticated multi-criteria decision-making methods are required to support the decision-maker in evaluating the competing alternatives. Consequently, the problem of multi-criteria decision-making (MCDM), also referred to as multi-criteria decision-analysis (MCDA) has attracted increasing attention in recent decades. It is formulated by considering a set of alternatives and a set of criteria G=(g1, g2, ..., gm), where n is the number of alternatives and m is the number of criteria. In most approaches, the multi criteria evaluation for an alternative aiis presented by the vector g(ai)=(g1(ai), g2(ai), ..., gm(ai)), where gj(ai) is the performance of the alternative ai∈A on criterion gj(ai) [5]. For example, the use of Data Envelopment Analysis (DEA), which is a tool for Multiple Criteria Decision Making (MCDM) and is applied to handle the multiple inputs and outputs (MIO) process [6], would be investigated for defining the maximizing criteria as outputs and the minimizing criteria as inputs to measure productive efficiency of alternatives (decision making units) [5,7]. Broadly speaking, MCDM methods can be classified in terms of the nature of the problem they considered, namely continuous or discrete [8]. Methods of the former type are referred to as multiple-objective decision-making (MODM) methods, while those of the latter type are referred to as multiple-attribute decision-making (MADM) methods [9]. The basic objective of MODM methods is to obtain the alternatives of decision-making through mathematical programming model. By contrast, the aim of MADM methods [10–12] is to find the optimal solution among a set of limited alternatives which consist of multiple attributes by evaluating the corresponding importance of each attribute.Fig. 1, reproduced from [13,14] illustrates the basic concepts and techniques pertinent to a complex MCDM problem-solving environment. As shown in the figure, the MCDM process involves three stages, namely (1) Data Processing/Statistical and Multivariate Analysis, (2) Planning/Designing, and (3) Evaluating/Choosing. Although the first two stages are relatively straightforward for most decision-making problems, the third stage, namely Evaluating/Choosing, is often far more problematic. For example, in evaluating and choosing between competing alternatives, the MCDM model must minimize the effects of the inherent preferences and prejudices of the decision-maker or experts in order to achieve an objective outcome. Moreover, the relations among the various attributes in the dataset are extremely complicated for most multi-input (conditional attributes and independent variables) and multi-output (decision attributes and dependent variables) information systems. In the end, many large real-world datasets are characterized by inaccuracy and uncertainty. (Note that the term inaccuracy refers to the case where the values of same multi-independent variables correspond to those of different multi-dependent variables. Similarly, the term uncertainty refers to a state of having limited knowledge where it is impossible to exactly describe the existing state.) Survey errors is the central focus of a study by Groves [15,16], in which the author found that non-response, sampling, interviewer effects, mode effects, various other types of measurement errors, and processing errors would affect the usefulness of surveys. Furthermore, Wolfson et al. and other researchers [17–19] demonstrated that in applications requiring interaction with the physical world, data uncertainty is an inherent property due to measurement inaccuracy, sampling discrepancy, outdated data sources, or other errors. Accordingly, Huang and Lin [20] proposed a VPRS (variable precision rough set)-based index approach to filter out the uncertainty inherent in the datasets with mixed attribute type (numerical, categorical). One major limitation of this approach is that it aimed at classifying the single-dependent-variable datasets.Little is known about how to filter out uncertain instances from the original dataset when the existing MCDM methods are applied, even though a large number of studies have been made on proposed MCDM methods for decision-making environments. Thus, in facilitating reliable decisions, MCDM models must be capable of filtering out such inaccuracies and uncertainties. However, due to a lack of training samples, most of these MCDM models are unable to filter out the inaccurate or uncertain instances; therefore throwing doubt on the validity of the identified decision rules. It has been argued, by Triantaphyllou [21] and others, that although this MCDM is very relevant in practice, there are few methods available and their quality is hard to determine, i.e., the difficulty that always occurs when trying to compare decision methods and choose the best one is that a paradox is reached. As Lin [10] noted in his review of state-of-the-art in fuzzy decision-making methods, “none of the fuzzy decision-making methods examined in this study is perfectly effective in terms of both evaluative criteria. The methods are less accurate when the decision-making problems become more complex.” In addition, the relationship among attributes becomes more complex and thus the corresponding computation cost increases as the number of attributes increases, it will be limited to find the efficient and effective resolution using the conventional MADM approaches when faced with such MIO datasets with inaccuracy and uncertainty[22–24].Furthermore, there are several studies in literature that used discretization approach as preprocessing for most inductive learning methods [25–28], such as VPRS theory. Accordingly, the present study proposes an enhanced MIO classification method designated as the FVM-index method, which integrates fuzzy set theory (FST), VPRS theory, and a cluster validity index (CVI) function in order to discretize each real-valued attribute, to filter out the inherent uncertainty and inaccuracy in the surveyed datasets and to obtain a more reliable and robust MIO classification outcome as a result. The validity of the proposed approach is demonstrated using some datasets for illustration purposes.The remainder of this paper is organized as follows. Section 2 presents the fundamental principles of FST theory, VPRS theory, and the proposed CVI function. Section 3 describes the integration of these concepts to form the proposed FVM-index method. Section 4 evaluates and discusses the MIO classification performance of the proposed method. Finally, Section 5 provides some brief concluding remarks and indicates the intended direction of future research.VPRS theory is a generalized form of rough set (RS) theory which inherits all of the basic mathematical properties of the original RS model. The RS model assumes that the universe under consideration is known and that all of the conclusions derived from the model are applicable only to this universe. However, in practice, there is evidence to suggest that only a smaller set of examples suffices to generalize the conclusions drawn from a larger population [29,30]. In contrast to the original RS model, the VPRS model provides a more robust classification performance in the case of uncertainty. For example, provided that the majority of the available data can be correctly classified, even partially correct classification rules still provide valuable trend information regarding future test cases. VPRS operates on what may be represented as a knowledge-represented system or an information system [31]. The basic principles and notations of information systems (S), and the application of VPRS theory to the processing of such systems, are described in the following sub-sections.For a given dataset, the records which are indistinguishable from one another when evaluated using a specific subset of all the system attributes can be defined using an equivalence or indiscernibility relationship. In VPRS theory, this indiscernibility concept is operated using approximate sets. A typical information system has the form S=(U, A, Vq, fq), where U is a non-empty finite set of records, A is a non-empty finite set of attributes describing these records, X⊆U, and R⊆A. Generally speaking, the attributes in set A can be partitioned into a set of conditional attributes C≠ϕ and a set of decision attributes D≠ϕ, i.e., A=C∪D and C∩D=ϕ. For each attribute, q∈A, Vqrepresents the domain of q, i.e., V=⋃Vq. Finally, fq:U×A→V is an information function defined such that f(x, q)∈Vqfor ∀q∈A and ∀x∈U.For rough set (RS) to be capable of performing a complete classification requires that the collected data must be fully correct or certain [32,33]. The classification with a controlled degree of uncertainty or misclassification error is outside the realm of RS approach. In an attempt to extend the applicability of the RS method, Ziarko developed the Variable Precision Rough Set (VPRS) theory [31] into which probabilistic information was incorporated. This VPRS model is parametric in which the definitions of positive and negative regions depend on the settings of permissible levels of uncertainty associated with each of the approximation regions. Then, a non-parametric modification of the VPRS model, called the Bayesian Rough Set (BRS) model, was presented and made it appropriate for applications concerned with achieving any certainty gain in decision-making processes, rather than meeting specific certainty goals [33].In the present study, the VPRS component of the proposed FVM-index method uses the systematic approach proposed by the current author [34] to determine a suitable value for the threshold parameter β, i.e., the value of β at which a certain proportion of the records in a specific conditional class are assigned to the same decision class.Although the number of decision attributes in set A is greater or equal to 2 [13,31,34–36], research to date has tended to focus on single-decision attribute problem rather than multiple-decision attributes problem when the VPRS/RS theory is applied. The complex multiple-decision attributes problem will be evaluated in this study. Furthermore, Ziarko [31] defined the β value as a classification error and it was defined to be in the domain [0.0, 0.5). However, An et al. [35] and other researchers [36,37] used β to denote the proportion of correct classifications, in such case the appropriate range is (0.5,1.0]. They referred this technique as ‘Enhanced RST’. This definition of β will be used in this study. Accordingly, when processing an information system in which each instance consists of multiple-decision attributes using a VPRS model with 0.5<β≤1, the aim is to identify the β-lower and β-upper approximate sets associated with each indiscernible discretized Decision Attribute Cluster Vector (DACV). It should be noted that, in the case of an MADM dataset, a DACV represents the discretized decision attribute values of an instance, and the c-th indiscernible DACV is the decision attribute vector relating to the c-th decision-attributes class (see Section 3.1, Step 3).In general [37], the β-lower approximate sets X⊆U and P⊆C are given by(1)R_Pβ(X)={x∈U:P(X/[x]P)≥β}=∪{[x]P:P(X/[x]P)≥β}.Similarly, the β-upper approximate sets X⊆U and P⊆C can be expressed as(2)R¯Pβ(X)={x∈U:P(X/[x]P)>1−β}=∪{[x]P:P(X/[x]P)>1−β}.Where [x]Rdenotes the equivalence class determined by x with respect to R, i.e.,xR=y∈U:x,y∈IR, and the equivalence relation is denoted byIR=(x,y)∈U×U:f(x,q)=f(y,q)∀q∈R. Note thatP(X/Y)=X∩Y/YifY>0, and P(X/Y)=1 otherwise. Note also thatXindicates the cardinality of set X. In the particular case of β=1,R_Pβ(X)andR¯Pβ(X)are equivalent to the lower and upper approximate sets in RS theory, respectively. In other words, the VPRS model reverts to the traditional RS model.Ziarko [31] defined the following expression for the β-boundary region (the region of uncertainty) of X in S:(3)BβNDP(X)={x∈U:1−β<P(X/[x]R)<β}=∪{[x]P:1−β<P(X/[x]R)<β}.The β-boundary region is referred to as the region of uncertainty, and in which consists of inaccurate or uncertain instances.The β-accuracy (relative accuracy), the same notation as Ziarko [31] except for 0.5<β≤1, can be quantified as follows:(4)αcβ=R_Pβ(X)/R¯Pβ(X),where X={x:Cd(x)=c, ∀x∈U}; and|R_Pβ(X)|andR¯Pβ(X)are the cardinalities of the β-lower and β-upper approximate sets, respectively, when ranking the instances (x) in the dataset in terms of the c-th indiscernible discretized DACV.The Fuzzy Clustering Method (FCM), first developed by Dunn in 1973 and later refined by Bezdek, is an unsupervised clustering algorithm with multiple applications, ranging from feature analysis to clustering and classifier design. Fuzzy clustering techniques differ from hard clustering algorithms such as the K-means scheme, in that a single instance may be mapped simultaneously to multiple clusters rather than assigned exclusively to a single cluster. FCM clustering consists of two basic procedures, namely (1) calculating the cluster centers and assigning the data points to these centers on the basis of their Euclidean distance, and (2) determining the cluster memberships of each sample point. The first procedure is repeated continuously until the cluster centers remain stable from one iteration to the next. However, before the iteration procedure can commence, it is first necessary to select an initial set of membership values. In doing so, FCM imposes the following constraint on the fuzzy membership function values associated with each point:(5)∑j=1pμj(xi)=1,i=1,2,3...,kwhere p is the specified number of clusters, k is the number of instances, xiis the i-th instance, and μj(xi) is the membership value of xiin the j-th cluster, i.e., the degree of belongingness of xito the j-th cluster. Clearly, the sum of the cluster membership values of each instance for each attribute must be equal to one [41]. The objective of the FCM clustering process is to minimize a standard loss function expressed as the weighted sum of the squared error within each cluster, i.e.,(6)l=∑j=1p∑i=1nμj(xi)m′xi−cj2,1<m′<∞,where p is the specified number of clusters, n is the number of instances, μj(xi) is the membership value of instance xiin the j-th cluster, xiis the i-th instance, m′ is the fuzzification parameter, and cjis the center of the j-th cluster.Ifxi−cj>0for all i and j, then the loss function is minimized when m′>1 [41]. Under this condition, the cluster center can be computed as(7)cj=∑i[μj(xi)]m′xi∑i[μj(xi)]m′for1≤j≤p.Having calculated the cluster centers, the second step in the FCM procedure is to determine the cluster membership function values of each instance. In practice, this is achieved by computing the Euclidean distance between the point and each cluster center in accordance with(8)dji=xi−cj2,where djiis the distance of instance xifrom the center of cluster cj.Since the FCM algorithm constrains the total cluster membership values of each instance to one, the membership value of any instance to a particular cluster can be expressed as a fraction of all the total possible membership function values associated with that instance. In other words, the membership function value of instance xito the j-th cluster can be given as(9)μj(xi)=1dji1/m′−1∑k=1p1dki1/m′−1=1∑k=1pdjidki1/m′−1for1≤j≤p,1≤i≤n,where djiis the Euclidean distance of xifrom the center of cluster cj, m′ is the fuzzification parameter, p is the specified number of clusters, and dkiis the Euclidean distance of xifrom the center of cluster ck.Having computed the new value of μj(xi), it is used in place of the original value of μj(xi) in the first step of the FCM procedure. This two-step procedure is repeated iteratively until the centers of all the clusters within the dataset converge.For a MCDM information system, fuzzy-based clustering methods yield more reliable decision-making rules than conventional crisp clustering schemes such as the equal-width or equal-frequency method [45]. Before applying the MD index function to real-valued datasets, it is subjected to a process of discretization in which fuzzy distances were incorporated to calculate the compactness of each indiscernible DACV. In fuzzy sets, all of the properties can be expressed using the membership functions of the sets involved and the union, intersection and complement operations of traditional set theory. Consider a dataset with M decision attributes and assume that the j-th and k-th attributes, i.e., djand dk, can be clustered into plclusters. Thus,μmj(xi(dj))andμmk(xi(dk))give the membership function values of the index of the mj-th and mk-th cluster of m-th DACV to which the j-th and k-th decision attributes of instance xibelong to. For the case of a MADM information system, the confluence of the membership function values of each decision attribute to the fuzzy distances for each indiscernible DACV can be manipulated using many different operators, e.g., minimize, maximize, product, and so forth [46,47]. Zimmermann [48] first extended his fuzzy linear programming approach [49] to a conventional multi-objective linear programming (MOLP) problem. Then, the corresponding linear membership function is defined and the minimum operator proposed by Bellman and Zadeh [50] is applied to combine all objective functions. Accordingly, in the fuzzy set theory, literatures [51,37,52–54] used the standard fuzzy intersection, which represents a form of limiting factor analysis by applying the minimum operator. Thus, in the FVM-index method proposed in this study, the minimize operator is used to aggregate the membership function values of the M decision attributes.In other words, the minimize operator applied to obtain the aggregated membership function value,μ¯jid(xj(d))is defined as follows:(10)μ¯jid(xj(d))=min(μm1(xi(d1)),μm2(xi(d2)),…,μmM(xi(dM)))=μm1(xi(d1))∧μm2(xi(d2))∧…∧μmM(xi(dM))Before applying the VPRS model to real-valued datasets, it is first subjected to a process of discretization [26]. Fuzzy C-Means (FCM) method, proposed by Bezdek [41], is an effective discretization approach in data mining that handles the issue of interval partitioning since it can take the density of data points into account and produce unequal-sized intervals [55]. This algorithm requires the user to predefine the number of clusters for every attribute in the dataset. Unfortunately, this information is not generally known in advance. Finding the optimal number of clusters for discretizing a set of real-valued attributes is an NP-hard problem [56,57]. To prevent this NP-hard situation, a cluster validity index[58] can be integrated with the VPRS model in order to assess the clustering quality given different numbers of clusters. Many cluster validity indexes have been proposed for fuzzy clustering methods [59–61]. However, none of these indexes take proper account of the complicated interrelationships among the various attributes in the dataset. Accordingly, the following index function is an extended form of Huang-index [62]:(11)MD(C)=1C×E1F¯NI×D¯NI,where C and NI denote the number of clusters of the decision attributes and the number of indiscernible DACVs, respectively; andαcβis the β-accuracy when evaluated in terms of the c-th indiscernible DACV. In addition, E1 is a constant (equal toE¯1in the PBMF-index function [60] andF¯NIis obtained by accumulating the value ofE′ifor each indiscernible DACV, whereE′iis given byE′i=∑j=1nμ¯jim′dxj−z′i/αiβin whichμ¯jid(xj(d))is the aggregated membership function. (Note that in the present study, the minimize fuzzy set operation is used to compute the aggregated membership function of instance xjin the clusters indicated by the c-th indiscernible DACV.) Furthermore,z′cis the multi-dimensional centroid of the β-lower approximate sets associated with the clusters indicated by the c-th indiscernible DACV, and is obtained by computing the mean values of the conditional and decision attributes of each instance within the corresponding sets. In addition, m′ is the fuzzification parameter and n is the total number of instances in the dataset. Finally,D′NIis the maximum separation distance amongst the centroids of all the lower approximate sets associated with each indiscernible DACV, i.e.,D¯NI=maxi,j=1NIz′i−z′j.Table 1summarizes the major components of the MD index function and the PBMF-index function. At a high level, four main differences exist between the two functions, namely (i) the MD index function is based on the individual attribute values of the instances within the dataset, whereas the PBMF-index function is based on the norms of the instances; (ii) the MD index function is based on the aggregated membership function values associated with each DACV, whereas the PBMF-index function is based on the membership function values of each instance; (iii) the MD index function takes explicit account of the β-accuracy when evaluating the quality of the clustering results, whereas the PBMF-index function considers only the optimal number of clusters within the datasets; and (iv) the MD index function is based onz′c, i.e., the centroids of the β-lower approximate sets associated with each indiscernible DACV, whereas the PBMF-index function is based on zk, i.e., the centroid of the k-th cluster obtained when clustering the dataset using the FCM method.As a result, when integrated with the specified classification method (i.e., the VPRS classification model in the present study), the MD index function provides a more effective basis for the discretization/classification of complex, real-world datasets than the PBMF-index function.Existing methods for solving multi-objective decision-making problems of MIO datasets can be characterized by continuous values, generally considering only the stage of Evaluating/Choosing. Most of these MCDM models are limited to obtain an effective resolution when faced with such datasets with inaccuracy and uncertainty in Data Processing/Statistical and Multivariate Analysis stage. Thus, the FVM-index MIO classification model proposed in the present study focuses on the MIO real-valued (continuous) datasets in Data Processing/Statistical and Multivariate Analysis stage.This section describes the integration of the FCM method, Fuzzy arithmetic relations, Variable Precision Rough Set (VPRS) theory, and the MD cluster validity index function in order to create the FVM-index MIO classification model proposed in this study for classifying datasets containing multiple decision attributes with continuous values. Fig. 2presents a flowchart of the proposed FVM-index method. The details of each step are presented as follows:(1)Specify the number of clusters per conditional and decision attribute in the interval [2, Nmax].Fuzzify the attribute values of each instance using the FCM method.Assign each attribute of every instance to an appropriate conditional or decision attribute cluster.Utilize the index function (see Appendix A)Cal(xi)=Imax(μj(xi(al)))=Index(max(μj(xi(al))))for 1≤l≤m, 1≤i≤n, to determine the conditional or decision attribute cluster to which each attribute belongs and to obtain the corresponding DACV.Identify VPRS sets, aggregated membership function value and β-accuracy.Having mapped the attribute values of all the instances to the appropriate conditional or decision attribute clusters, the procedure described in [30] is applied to determine an appropriate value of the threshold parameter β in the VPRS model. The steps above yield the decision-making attribute clustering indexes of every attribute. For each DACV, Fuzzy arithmetic relations are used to consolidate the membership function values of the vector into a single membership function value (see Section Fuzzy Sets Operator). Finally, the β-accuracy associated with each indiscernible DACV is then obtained by computing the cardinality ratio of the corresponding β-lower approximate sets to the β-upper approximate sets.Calculate centroids of β-lower approximate sets associated with each indiscernible DACV.Determine the β-lower approximate set, β-upper approximate set, and boundary set of each indiscernible DACV. Compute the VPRS β-accuracy of the c-th indiscernible DACV as the ratio of the number of elements in the β-lower approximate set to the number of elements in the β-upper approximate set.Assume that there is more than one real-valued attribute in the information system. Thus, the centroids of the β-approximate sets associated with each DACV are computed as the arithmetic means of the conditional and decision attribute values of all the instances within the β-lower approximate sets associated with each DACV (including each of all conditional attributes and decision attributes).Determine value of MD cluster validity index.Having determined the aggregated membership functions, β-accuracy, and centroids of all the β-lower approximate sets, the quality of the clustering results is evaluated using the MD index function.Check termination criterion.Having calculated the cluster validity index value for the current cluster number N, check whether the value of N is equal to Nmax. If the current value of N is less than Nmax, increment its value by one and return to Step 1. Else; go to Step 8.Identify value of MD cluster validity index.Once the termination criterion has been satisfied, the values of the MD index function obtained for N=Nmin∼Nmax are compared in order to identify the clustering solution which yields the maximum index function value, i.e., the clustering solution which optimizes both the number of clusters per attribute and the overall β-accuracy of the dataset.This section illustrates the derivation of the MD index value for a Hospital inputs and outputs data comprising just 10 instances (hospitals), as shown in Table 2(see [63], Table 1.2). An assumption is made that each instance (hospital) has two conditional attributes (inputs) (i.e. nursing hours and medical supplies), a1, a2; and two decision attributes (outputs) (i.e. inpatient admissions and outpatient visits), d1, d2. The values of each attribute of every instance (hospital) are summarized in Table 2. As described in Section 2.2.1, the real-valued data in the hypothetical dataset are first discretized using the FCM technique. In doing so, it is assumed that each conditional and decision attribute can be partitioned into 2 clusters. The membership function values of each attribute of every instance are summarized in Table 3. The attribute values of each instance are then assigned to appropriate conditional or decision attribute clusters by applying the index function Imax (see Appendix A) to the corresponding membership function values. The mapping results are presented in Table 4. For example, the DACVs of the first four instances xi(Ia1,Ia2,Id1,Id2)have the forms x1(2, 2, 1, 2), x2(2, 1, 2, 1), x3(2, 1, 2, 1), and x4(1, 1, 1, 1), respectively. The β-upper and β-lower approximate sets associated with each indiscernible DACV are shown on the right-hand side of Table 4.Having computed the β-upper and β-lower approximate sets, the aggregated membership function values of each instance are calculated and the β-accuracy of the clustering solution computed accordingly. Taking the first instance x1 for illustration purposes, the four indiscernible DACVs have values of CD(x1)=[1,2], [1,2], [1,1] and [2,2], respectively (see Table 4). The aggregated membership function value of x1 in the first DACV [1,2], i.e.,μ¯11d, is obtained from the minimization operator as min(μ1(x1(d1)),μ2(x1(d2)))=min(0.516, 0.680)=0.516. Similarly, the aggregated membership function value of x1 in the second DACV [1,2], i.e.,μ¯21d, is obtained as min(μ1(x1(d2)),μ2(x1(d1)))=min(0.484, 0.320)=0.320. The aggregated membership function values of the ten instances in this dataset are shown in Table 5. The β-accuracy associated with each indiscernible DACV is obtained by computing the cardinality ratio of the corresponding β-lower approximate sets to the β-upper approximate sets. In the present example, the classification accuracies are therefore found to beβα1=2/(2+3)=0.400,βα2=4/(4+3)=0.571,βα3=1/1=1.000, andβα4=1/3=0.333, respectively.Having computed the MIO classification accuracies, the multi-dimensional centroids of the β-lower approximate sets associated with each indiscernible DACV are determined by calculating the mean attribute values (both conditional and decision) of all the instances within the corresponding sets. The centroids of the β-lower approximate sets associated with the three indiscernible DACVs are obtained asz′c=mean(x|x∈R_β(X),CD(x)=c), the function mean(x) will return the arithmetic mean of the conditional and decision attribute values of each instance within the corresponding sets associated with the c-th cluster of the decision attributes D. In the present example,z′1=mean(x|x∈R_β(X),CD(x)=1)=mean(x|x∈{x8,x10})=((2314+1669)/2,(3456+4500)/2,(877+530)/2,(252+390)/2)=(1991.50,3978.00,703.50,321.00),z′2=mean(x|x∈R_β(X),CD(x)=2)=mean(x|x∈{x2,x3,x5,x6})=(411.00,1341.50,197.25,116.25)andz′3=mean(x|x∈R_β(X),CD(x)=3)=mean(x|x∈{x4})=(2200.00,1450.00,560.00,71.00), respectively.Having determined the aggregated membership function values of all the instances, the β-accuracy, and the centroids of the β-lower approximate sets, the optimality of the discretization/classification outcome is evaluated using the MD index function, i.e.,MD(C)=1C×E1F¯NI×D¯NI. In describing the derivation ofF¯NI(whereF¯NI=∑c=1NIE′c), the following discussions arbitrarily consider the computation ofE′2. The first instance in the dataset, x1, has attribute values of x1(567.00, 2678.00, 409.00, 211.00). In addition, the centroid of the β-lower approximate sets associated with the first indiscernible DACV is given byz′1(1991.50,3978.00,703.50,321.00). As a result,(x1(a1)−z′1(a1))=(567.00−1991.50)=−1424.50,(x1(a2)−z′1(a2))=(2678.00−3978.00)=−1300.00,(x1(d1)−z′1(d1))=(409.00−703.50)=−294.50, and(x1(d2)−z′1(d2))=(211.00−321.00)=−110.00. Therefore, the vector ofx11=x1−z′1has the formx11a1,x11a2,x11d1,x11d2=−1424.50,−1300.00,−294.50,−110.00. The corresponding norm is equal tox1−z′1=x12(a1)2+x12(a2)2+x12(d1)2+x12(d2)2=(−1424.50)2+(−1300.00)2+(−294.50)2+(−110.00)2=1953.978. Let the fuzzification parameter m′ be specified as 2.0. Applying the notationxj1=μj12d(xj(d))×xj−z′1, the effect of instance x1 onz′1, i.e.,x11, is obtained by multiplyingx1−z′1by the square of the corresponding membership function, i.e.,μ112dx1D=0.5162=0.266. Thus,x11has a value of 0.000.x22,x32andx42are calculated using an identical procedure. The corresponding results are shown in Table 6. The value ofE′1is thus obtained asE′1=∑j=110μj12d(xj(d))xj−z′1/α1β=∑j=110xj1/α1β=(x11+x21+...+x101)/α1β=(519.283+0.002+…+0.000+526.190)/0400=1565.996/0.400=3914.991.. Utilizing an identical approach to that described above, the values ofE′2andE′3are obtained as 5085.225 and 265.899.F¯NIis thus found to have a value ofF¯NI=∑c=13E′=9266.115.Factor E1 in the MD index function is a constant for a given dataset in which the instances belong to only one cluster. As a result, the attribute values of the centroid z1 of the illustrative dataset can be obtained from the arithmetic mean function mean(x|x∈{xi}, i=1, 2, ..., 10) as ((567.00+350.00+...+560.00+1669.00), (2678.00+1200.00+...+4000.00+4500.00), (409.00+90.00+...+189.00+530.00), (409.00+90.00+...+189.00+530.00))=z1(911.00, 2455.20, 346.20, 175.60). Based on the vector of centroid z1, it can be shown that (x1(a1)−z1(a1))=(567.00−911.00)=−344.00, (x1(a2)−z1(a2))=(2678.00−2455.20)=222.80, (x1(d1)−z1(d1))=(409.00−346.20)=62.80, and (x1(d2)−z1(d2))=(211.00−175.60)=35.40. Therefore, the vector of x11=x1−z1 has the formx11a1,x11a2,x11d1,x11d2=−344.00,222.80,62.80,35.40, and the corresponding norm is equal tox1−z1=x11(a1)2+x11(a2)2+x11(d1)2+x11(d2)2=(−344.00)2+222.82+62.802+35.402=416.140Similarly, the norms ofx2−z1tox10−z1are found to be 1401.462, 961.323, 1651.847, 1640.699, 958.659, 1029.165, 1804.881, 1597.618 and 2198.982, respectively. The value of E1 in the MD index function is then obtained by summing the norms ofxj−z1where j=1, 2, ..., 10, yielding a value of E1=13660.775.Finally, the value ofD′NIin the MD index function is obtained by calculating the maximum separation distance among the centroids of all the lower approximate sets associated with the two indiscernible DACVs. In the present example, these centroids are given byz′1(1991.50,3978.00,703.50,321.00)andz′2(411.00,1341.50,197.25,116.25), respectively. Thus, the vector ofz12=z′1−z′2which maximizes the value ofD′NI=maxi,j=1NIz′i−z′jhas the formz12a1,z12a2,z12d1,z12d2=1580.50,2636.50,506.25,204.75. The corresponding norm is therefore equal to1580.502+2636.502+506.252+204.752=3122.07.Given the parameter values specified/derived above (i.e., C=2, E1=13660.775,F¯NI=9266.115andD′NI=3122.07), the MD index functionMD(C)=1C×E1F¯NI×D¯NIreturns a value of 2301.391.This proposed index function comprises three factors, namely, 1/C,E1/F¯NIandD¯NI. The first factor decreases as C, the number of clusters of each attribute, increases and this, therefore, reduces the index value. The second term is the ratio E1 toF¯NI. The E1 is constant for a given data set.F¯NIrepresents the sum of allE′(=∑j=1nμ¯jim′dxj−z′i/αiβ)which each includes the factor of the β-accuracyβαiwhen evaluated in terms of the i-th indiscernible DACV. This factor is not only a measure of the compactness of a C cluster system, but also an evaluation of the β-accuracy of each indiscernible DACV in a C cluster system. TheE′iusually decreases as C increases. Hence, MD-index increases asE′Cdecreases. The third term,D¯NI, measures the maximum distance between the centroids of all the β-lower approximate sets associated with the different DACV, and increases as C increases. This factor signifies while cluster separation increases. The FVM-index method is used for filtering out the uncertainty and inaccuracy, and simultaneously optimizing the number of clusters and the β-accuracy in the MIO real-valued dataset.As described in the Introduction section, the MCDM decision-making process involves three stages, namely (1) Data Processing/Statistical and Multivariate Analysis, (2) Planning/Designing, and (3) Evaluating/Choosing. In the FVM-index method proposed in this study, the uncertain and inaccurate instances in the three different surveyed datasets are removed during the first stage using VPRS, and the remaining accurate instances are then used to derive reliable decision-making rules in the final stage. The proposed FVM-index method incorporated into the process of MCDM is shown as Fig. 3.This section commences with an example for a simple hypothetical dataset showing the difference of classification results obtained using the proposed FVM-index method to two single-attribute decision-making methods. The performance of the proposed FVM-index method is then evaluated by means of three illustrative case studies, namely (1) a case relating to three UCI datasets which consist of an original dataset, a dataset with a large amount of inaccurate instances, and an FVM-index filtered dataset extracted from the original dataset using a statistical approach; (2) a case relating to AR Product Image Evaluation Matrix (ARPIEM) datasets [64,65] which comprise an original ARPIEM dataset and an FVM-index filtered ARPIEM dataset; and (3) a case relating to Hospital dataset (Table 2) which is composed of an original Hospital dataset and an FVM-index filtered Hospital dataset.In this section, the effectiveness of the FVM-index method is evaluated using the Concrete Slump Test dataset in the UCI database [66]. The dataset contains 103 instances; each with 7 conditional attributes and 3 decision attributes.The performance of the FVM-index method in classifying the dataset was evaluated by comparing the optimal number of clusters (and the corresponding β-accuracy) given various numbers and combinations of the decision attributes. In every case, the maximum number of clusters per attribute was specified as N=30. The corresponding results are presented in Table 7.As shown, when all three decision attributes are considered, the optimal number of clusters is equal to 23. For the case of two decision attributes, the optimal number of clusters is as follows: (1) SLUMP+FLOW, optimal cluster number: 4; (2) SLUMP+Compressive Strength, optimal cluster number: 16; and (3) FLOW+Compressive Strength, optimal cluster number: 12. For the case where only a single decision attribute is considered, the optimal number of clusters is as follows: (1) SLUMP, optimal cluster number: 2; (2) FLOW, optimal cluster number: 2; and (3) Compressive Strength, optimal cluster number: 5. Overall, the results presented in Table 7 indicate that the optimal number of clusters tends to increase as the number of decision attributes considered in the clustering process increases. Similarly, the β-accuracy also increases as the number of decision attributes is increased.In the FVM-index method, instances x99 and x100 are deleted from the original dataset since the two instances does not belong to the β-lower approximate sets. As a result, the dataset is reduced from 103 instances to 101. To demonstrate the efficiency of the FVM-index method proposed in the present study, the Canonical Correlation Analysis (CCA) method [67–71] was used to compare the canonical correlation results obtained for the original MIO dataset and the FVM-index filtered MIO dataset extracted from the original MIO dataset, respectively.In the present example, the linear combination [69,70] comprising p(=7) dimensions of explanatory variables (i.e., conditional attributes {ai|ai∈C, i=1, 2, ..., p}) can be represented as χi(composed of the conditional attributes with weighted average values of Wij,χi=∑j=1pWijaj). Similarly, the linear combination comprising q(=3) dimensions of response variables (i.e., decision attributes {di|di∈D, i=1, 2, ..., q}) can be represented as ηk(composed of the decision attributes with weighted average values ofW′kj,ηk=∑j=1qW′kjdj).The number of linear correlations for the two datasets is equal to t=min(p, q)=3. In other words, the number of canonical dimensions which represent the relationship (correlational) between two linear canonical variates η and χ should be equal to 3. In general, not all the canonical dimensions will be statistically significant. A significant canonical dimension corresponds to a significant canonical correlation and vice versa. In the canonical Dimension Reduction Analysis (DRA), the statistical significance test is to find the significant dimensions at the p≤0.05 level. The first DRA evaluates whether all three dimensions combined are significant, the next DRA evaluates whether dimensions 2 and 3 combined are significant. Finally, the last DRA evaluates whether dimension 3, by itself, is significant.To demonstrate the effectiveness of the FVM-index method in filtering out the inaccurate data in the original dataset, the following discussions compare the canonical correlation results obtained for the original dataset, the dataset with a large amount of inaccurate instances, and the FVM-index filtered dataset extracted from the original dataset.Let five copies of the two instances x99 and x100 filtered by the FVM-index method be added to the original dataset, i.e., add five x99 instances and five x100 instances, and designate the resulting dataset as Concrete1. Similarly, add ten copies of the two instances filtered by the FVM-index method to the original dataset, i.e., add ten x99 instances and ten x100 instances, and designate the resulting dataset as Concrete2. Filter dataset Concreteidx, and then add ten instances extracted from the filtered Concreteidxdataset randomly to the original dataset to form a new dataset designated as Concrete3. Finally, add twenty instances extracted from the filtered Concreteidxdataset randomly to the original dataset to form a new dataset designated asConcrete4.Table 8presents the CCA results for the Concrete, Concreteidx, Concrete1, Concrete2, Concrete3 and Concrete4 datasets with different amounts of uncertain data, respectively. It is seen that for the two datasets containing the greatest volume of inaccurate instances, i.e., Concrete1 and Concrete2, three canonical correlations reach the significance level (p<0.05). In other words, both canonical varieties should show the consideration for the three canonical correlations.Moreover, it is seen that for all of the remaining datasets, only the first two Canonical Correlations reach the significance level (p<0.05). In other words, it is again necessary to consider only the first two canonical correlations for these two canonical varieties.In general, the results presented above show that the FVM-index method successfully filters out the inaccurate instances in the dataset. As a result, the accuracy of the decision-making rules offered to the decision-maker is significantly improved.In this example, the effectiveness of the FVM-index method is accessed using the ARPIEM dataset [64,65]. The dataset contains 43 selected Augmented Reality (AR) [72,73] products (instances); each with 7 product feature elements and 2 product images. Canonical correlation [69–71] was performed between the set of product feature elements (i.e. conditional attributes a1∼a7) and the set of product images (i.e. decision attributes d1∼d2) of ARPIEM dataset. The product feature elements consisted of animation, picture, sound, background color of text, object image transparency, interface manipulation, and maker object. The set of product images consisted of one-way-respond versus immediate interaction and silent/boring-active versus creative, respectively.In the FVM-index method, 21 AR products (instances, alternatives) {x1,x3, x5, x7∼x9, x11, x12, x14∼x16, x18, x21, x24, x28, x29, x31∼x33, x35, x36} are removed from the original ARPIEM dataset since the 21 instances does not belong to the β-lower approximate sets. As a result, the filtered dataset is reduced from 43 instances (products, alternatives) to 22. To illustrate the validity of the proposed FVM-index method in the present example of AR product design, repeated measurements of CCA method were used to compare the canonical correlation results obtained for the original ARPIEM dataset, and the FVM-index filtered ARPIEM dataset extracted from the original ARPIEM dataset to form a new dataset designated as ARPIEMidx.In the canonical DRA, the statistical significance test is to find the significant dimensions at the p≤0.05 level. For the original ARPIEM dataset, the obtained p value of the DRA is equal to 0.000 in 1To2, and therefore reaches the specified significance level (0.05); however, the p value of the DRA in 2To2 is 0.493, which is bigger than 0.05 (i.e., the significance level is not attained). In other words, the first of the two canonical correlations, which has a value of 0.807 is statistically significant at the 0.05 level; indicating that it is sufficient only to consider the first canonical correlation between the two sets of variables. Similarly, it is observed for the ARPIEMidxdataset, that the first canonical correlation reaches the significance level (p<0.05), and the corresponding value is 0.949; while the second canonical correlation fails to reach the significance level because p=0.344>0.05. Thus, it is again necessary only to consider the first canonical correlation between the two sets of variables for the ARPIEMidxdataset.In CCA, three result studies were discussed, namely (1) the resulting eigenvalues, called canonical correlation coefficients, representing the labels of correction between patterns of dependent variables (decision attributes, product images) and patterns of independent variables (conditional attributes, product feature elements) [67]; (2) the resulting proportion of variance showing the proportion that can be extracted by any given number of canonical relationships [74]; and (3) the standardized canonical weights (canonical coefficients or canonical function coefficients) used to determine which variables (attributes) were most important in a given pair of canonical variates [75,76]. The CCA results of the first canonical dimension for the original ARPIEM dataset and ARPIEMidxdataset are shown in Table 9.First, because of the original ARPIEM dataset containing the greatest volume of inaccurate instances, the eigenvalue obtained for ARPIEM dataset (1.866) is smaller than the eigenvalue obtained for ARPIEMidxdataset (12.617). Second, Table 9 also shows that the Proportion of Variance for ARPIEM dataset (92.211) is smaller than the eigenvalue obtained for ARPIEMidxdataset (95.958). Finally, with a cut-off correlation of 0.30 [77], the predicted variable sound, a3, was highly correlated with the variate, and both variables of the product images set were correlated with the variate for ARPIEM dataset; In contrast, the two predicted variables sound and maker object (i.e. a3 and a7) were highly correlated with the variate, and only the one-way-respond versus immediate interaction, d1, was correlated with the variate for ARPIEMidxdataset. Thus, the effectiveness of the FVM-index method is confirmed by comparing the CCA results of eigenvalue and Proportion of Variance between the two ARPIEM and ARPIEMidxdatasets.In this paper, the use of data envelopment analysis (DEA) as a tool for multiple criteria decision making (MCDM) is investigated for assessing the effectiveness of the FVM-index method in filtering out the inaccurate data in the original hospital dataset. And the CCR model [78–81] is applied to measuring the relative efficiency value of a decision making unit (DMU) [7] that extends to situations involving multiple inputs and outputs.The hospital inputs and outputs dataset (see Table 2) illustrated in Section 3.2 was used again in this example. The dataset contains 10 hospitals (alternatives, instances); each with 2 conditional attributes, a1, a2, and 2 decision attributes, d1, d2. To demonstrate the effectiveness of the FVM-index method in filtering out the inaccurate data in the original hospital dataset, the following discussions compare the relative efficiency values (Effk) obtained using CCR method for the original dataset, the dataset with some inaccurate instances of β-upper approximate sets, and the FVM-index filtered dataset extracted from the original dataset.The original hospital dataset designates as H, while the dataset filtered by the FVM-index method designates as Hidx. In this filtered FVM-index dataset, Hidx, three inaccurate instances x1, x7 and x9 were removed from the original dataset.Then, let the instance x1 was removed from the original dataset, and designate the resulting dataset as H1. Similarly, another inaccurate instance (i.e. x7 and x9) was removed from the original dataset, and designate the resulting datasets as H2 and H3, respectively. Finally, two instances (i.e. (x1, x7), (x1,x9), and (x7,x9)) were removed from the original dataset H to form three new datasets designated as H4,H5, and H6, respectively.Table 10presents the relative efficiency values results for the H, Hidx, H1, H2, H3, H4, H5 and H6 datasets, respectively. Because all eight datasets except the Hidx(i.e., H, H1, H2, H3, H4, H5 and H6) containing some inaccurate instances (alternatives, hospitals), some of the relative efficiency values, Effk, of these seven datasets obtained using CCR method do not reach the “best ratio” (Effk<1.000). In contract, it is shown that for the filtered Hidxdataset, all of the relative efficiency values, Effk, obtained using CCR method reach the “best ratio” (Effk<1.000). In other words, the FVM-index method facilitates the achievement of maximizing the resulting relative efficiency values for each of the hospitals (DMUs). Thus, it again shows that the FVM-index method successfully filters out the inaccurate instances (alternatives, hospitals) in the dataset. As a result, the reliability of obtained relative efficiency values, Effk, using CCR method for the filtered hospital dataset is significantly improved.

@&#CONCLUSIONS@&#
This study has proposed an enhanced MIO classification method, designated as the FVM-index method, comprising Fuzzy Set theory, VPRS theory, and a refined Huang index function. The proposed method provides the means to determine both the optimal number of attribute clusters within the dataset and the optimal β-accuracy. The effectiveness of the proposed method has been confirmed by comparing the canonical correlation results obtained for the three different FVM-index filtered datasets extracted from the three original datasets with the corresponding results obtained for these datasets with a large amount of inaccurate instances. The results have shown that the proposed FVM-index method provides an effective means of filtering inaccurate instances from the surveyed datasets and extracting reliable decision-making rules. In general, the evaluation results support the following major conclusions:(1)As the number of attributes in the dataset increases, the importance of considering the multiple cause-and-effect factors among the various attributes also increases. Consequently, the complexity of the classification process increases accordingly.As the number of decision attributes increases, the relationships between these decision attributes, and the relationships between the conditional attributes and these decision attributes becomes more complex. Thus, the number of instances in the lower approximate sets reduces, while the number of instances remaining after the filtering process increases.In performing the clustering of a dataset with three decision attributes (i.e., A, B and C), if attributes A and B are combined with one another other, the optimal number of clusters tends to decrease. However, if decision attributes A and B are combined separately with decision attribute C, the optimal number of clusters tends to increase. Finally, if decision attribute C is simply ignored in the clustering process, the optimal number of clusters tends to decrease. In other words, the choice of the number of decision attributes has a significant effect on the clustering outcome.The CCA results have confirmed the ability of the FVM-index method to filter out the inaccurate instances in the original dataset. Thus, the reliability of the extracted decision-making rules is significantly improved.For a dataset with multiple response variables and multiple explanatory variables, and containing only accurate instances or a small amount of inaccurate instances, both the numbers of significant canonical dimensions obtained are the same by applying a Dimension Reduction Analysis technique. Conversely, if the dataset contains a large amount of inaccurate instances, the number of significant canonical dimensions obtained is different from that of former datasets and it is impossible to determine whether the obtained CCA results or decision-making rules is correct.For the original AR Product Image Evaluation Matrix dataset obtained applying Kansei Engineering scheme to the Augmented Reality product design, the CCA results have validated that the FVM-index method could be used as the basis for an effective decision-making mechanism designed to provide a robust approach on the applications of MCDM.For a hospital dataset with multiple conditional attributes and multiple decision attributes, the CCR ratio results have again verified the ability of the FVM-index method to filter out the inaccurate instances in the original hospital dataset. Thus, the maximizing the relative efficiency values obtained using CCR method for the filtered hospitals (DMUs) is significantly improved.Overall, the results show that the Multiple Inputs and Outputs classification method proposed in this study provides an effective means of filtering out the inaccurate instances from the surveyed dataset and therefore improves the reliability of the extracted decision-making rules. However, decision-makers should carefully select the number of decision attributes to be used in the clustering process in order to enhance the efficiency of the classification algorithm. Accordingly, in a future study, the FVM-index method will be used as the basis for an effective decision-making mechanism designed to provide a robust approach on the other applications of MCDM.