@&#MAIN-TITLE@&#
A Generalized Maximum Entropy (GME) estimation approach to fuzzy regression model

@&#HIGHLIGHTS@&#
We consider two fuzzy regression models from fuzzy least squares tradition.We rewrite these models within the Generalized Maximum Entropy Approach of estimation.We compare LS and GME approaches in the multicollinearity problem.Monte Carlo studies show increasing multicollinearity GME outperforms LS in efficiency.Empirical evidence shows some applicative advantages of GME.

@&#KEYPHRASES@&#
Fuzzy regression models,Generalized Maximum Entropy,Fuzzy statistics,Data mining,

@&#ABSTRACT@&#
Fuzzy statistics provides useful techniques for handling real situations which are affected by vagueness and imprecision. Several fuzzy statistical techniques (e.g., fuzzy regression, fuzzy principal component analysis, fuzzy clustering) have been developed over the years. Among these, fuzzy regression can be considered an important tool for modeling the relation between a dependent variable and a set of independent variables in order to evaluate how the independent variables explain the empirical data which are modeled through the regression system. In general, the standard fuzzy least squares method has been used in these situations. However, several applicative contexts, such as for example, analysis with small samples and short and fat matrices, violation of distributional assumptions, matrices affected by multicollinearity (ill-posed problems), may show more complex situations which cannot successfully be solved by the fuzzy least squares. In all these cases, different estimation methods should instead be preferred. In this paper we address the problem of estimating fuzzy regression models characterized by ill-posed features. We introduce a novel fuzzy regression framework based on the Generalized Maximum Entropy (GME) estimation method. Finally, in order to better highlight some characteristics of the proposed method, we perform two Monte Carlo experiments and we analyze a real case study.

@&#INTRODUCTION@&#
Regression analysis can be considered one of the most widely used data analysis techniques in engineering, social sciences, biology, data mining, pattern recognition, etc. In general, its purpose is to model the relation between a dependent variable and a set of independent variables by means of a suitable mathematical model (e.g., linear, polynomial, quadratic) in order to understand whether the independent variables predict the dependent variable and how the independent variables explain the empirical data which are modeled. In the standard regression frameworky=Xβ+ϵ, in order to fit the modelXβto the empirical datay, it is necessary to estimate a vectorβof parameters from the data vectoryand the model matrixXwhich is in turn assumed having complete rank. The estimation of the vectorβcan be performed by using the least square method which consists in minimizing the sum of square of residuals between the model and the empirical data that are expressed by the distance ∥y−Xβ∥2.Linear regression analysis has been mainly applied to standard crisp data (i.e., vectors/matrices with single-valued data). However, some researchers have extended the regression framework also to more complex data (e.g., interval, symbolic or fuzzy) in order to better model situations in which data contain vague and imprecise information [1–3]. In this context, fuzzy sets can be considered a natural way to model imprecision and vagueness in the empirical data [4]. Such type of data have been extensively studied in fuzzy statistics, a branch of statistical theory devoted to handle with data characterized by a particular type of uncertainty, called fuzziness. Nowadays, several fuzzy regression models and techniques are available [5–7]. In particular, some of these models have been developed using the concept of LR-fuzzy number [8] which may be considered one of the most important topic in Fuzzy Set Theory (FST). Moreover, LR-fuzzy numbers provide an elegant and compact way to describe a large variety of fuzzy data in different applicative situations [9,10].In general, three main approaches can be used to handle with LR-fuzzy data. These can be described according to the nature of the input data, the method and the output data considered (input-method-output schema). In particular, in the first approach, named fuzzy-crisp-crisp, the fuzzy input dataX˜are transformed into crisp data (e.g., by means of some defuzzification procedures) and standard statistical methods are used to perform data analysis (e.g., crisp least squares) [11]. The resulting output of this procedure are also crisp datay. In the second approach, fuzzy-crisp-fuzzy, the fuzzy input dataX˜are analyzed by means of standard statistical methods that are extended in order to take into account the LR-fuzzy representation (e.g., fuzzy least squares) [3]. Unlike the first approach, the resulting output of this procedure are fuzzy datay˜. In the third approach, fuzzy-fuzzy-fuzzy, fuzzy input dataX˜are manipulated with suitable fuzzy statistical methods (e.g., Tanaka's minimum fuzziness criterion) in order to obtain fuzzy output datay˜[12]. Although the first approach does not take into account fuzzy characteristics of the empirical data, the second and third approaches can efficiently manage fuzzy data with specific statistical procedures. However, although they are both fuzzy methods, unlike the third approach, the fuzzy-crisp-fuzzy can manage fuzzy data by extending standard statistical methods to take into account fuzzy properties of the data structures. In this way, this approach can easily manipulate crisp and fuzzy data at the same time and, above all, it can inherit the well-known properties of the standard statistical methods.In some applicative contexts of linear regression models such as, for example, analysis with small samples and/or fat matrices (matrices with no complete rank), violation of distributional assumptions, ill-posed problems (e.g., multicollinearity), use of prior information on the parameters estimation, standard fuzzy statistical methods may be inappropriate to handle with these kind of situations. A case of particular interest in such situations concerns the presence of multicollinearity in the model matrixX˜that may affect different empirical situations (e.g., [13–15]). Clearly, in this situation standard statistical methods such as, for example, fuzzy least squares, can result distort and it may not yield to accurate estimations. A possible way out consists in adopting ad-hoc data analysis procedure to transform the collinear data matrix into a new well-posed data matrix (e.g., by using a PCA-regression method). However, a serious limitation of this data transformation procedure is that it uses a subset of orthogonal new variables from the original set of variables that may artificially mask some relevant information contained in the original (not orthogonal) variables. As a consequence, in this article we propose a novel fuzzy regression framework which is entirely based on the well-known Generalized Maximum Entropy (GME) estimation approach [16,17] and the fuzzy-crisp-fuzzy perspective [18]. In this respect, unlike fuzzy least squares, the GME-fuzzy proposal always guarantees accurate and not distort estimation processes.The reminder of the article is organized as follows. Section 2 briefly describe the basic characteristics of LR-fuzzy data. Section 3 exposes the GME-fuzzy regression approach for LR-fuzzy data together with its main features. Moreover, this section also describes some useful procedures for data fitting and model evaluation. Section 4 describe a Monte Carlo study assessing the stability and reliability of the proposed approach as compared to the fuzzy least squares. Section 5 illustrates how the proposed GME-fuzzy regression works through an empirical case study. Finally, Section 6 concludes this article providing final remarks and suggestions for future extensions of our proposal.In this section we briefly recall some basic features of LR-fuzzy numbers. In general, a fuzzy setQ˜can be described by its α-setsQ˜α={x∈U|μQ˜(x)>α}with α∈[0, 1] and where U andμQ˜indicate the universal set and the membership function ofQ˜, respectively. If the α-sets ofQ˜are defined to be convex, thenQ˜is called a convex fuzzy set. The support ofQ˜can be denoted byQ˜0={x∈U|μQ˜(x)>0}whereas the setQ˜g={x∈U|μQ˜(x)=maxy∈UμQ˜(y)}of all its maximal points is called the core ofQ˜. The height ofQ˜is defined ashgt(Q˜)=max[μQ˜(x)]and, ifhgt(Q˜)=1, thenQ˜is also called a normal fuzzy set. Now, ifQ˜satisfies the conditions of normality, convexity and unimodality (Q˜gis singleton), thenQ˜is named LR-fuzzy number[19] and can be denoted byq˜. Moreover, the membership function ofq˜can also be described by some monotonic decreasing and left-continuous smooth functions L and R that can be used for a specific parametric representation ofq˜. In this way, we may define different types of LR-fuzzy numbers, such as, for example, triangular fuzzy numbers, trapezoidal fuzzy numbers, Gaussian fuzzy numbers, etc. In particular, for a triangular fuzzy number the parametric representation is denoted byq˜=(c,l,r)LR. Note that, the LR-tuple conveys the main information about the fuzzy set, namely its precision (by means of its core or modal value c) and fuzziness (by means of l and r). More details about the formal properties of L and R together with other important features of the LR-representation can be found in [8].In this section we provide a detailed description of the proposed GME-fuzzy approach. In particular, we first briefly explain the GME rationale within the more general and simple case of regression problem. Next, we describe from the GME perspective two simple but still relevant fuzzy regressions, namely crisp-input/fuzzy-output and fuzzy-input/crisp-output models [20]. These models were chosen according to the fact that the relation between crisp independent variables (input) and fuzzy dependent variables (output), as well as the relation between fuzzy independent variables (input) and crisp independent variables (output), plays a special role in several research contexts. For instance, in socio-economic studies, the relation between crisp (e.g., family income) and fuzzy quantities (e.g., quality of service, quality of teaching) are often take into account by researchers [21–23]. Moreover, such type of relations may be also considered for the first explanatory steps of research whereas more complex representations (e.g., fuzzy-input/fuzzy-output models) might be considered in subsequent steps [24].Generalized Maximum Entropy (GME) was firstly proposed by A. Golan [25] as an extension of the well-known Maximum Entropy (ME) principle developed by E.T. Jaynes in the past century [26,27]. The Jaynes's idea was mainly based on the basic features of Shannon's Information Theory and the related Entropy measure [28]. Broadly speaking, entropy can be considered a measure of the average information carried out by a probabilistic source of data. Due to its mathematical properties, in many statistical applications entropy can be used as an information recovering device and/or as a method of estimation [29–32]. In this way, the Golan's idea is to apply a slightly-modified Maximum Entropy based method, called Generalized Maximum Entropy, in order to estimate the parameters of some statistical models, such as for instance regression models, simultaneously equation models, dynamic models, etc. In order to show how GME rationale can be adopted in different statistical problems, consider the well-known regression problem from a GME perspective. To this end, consider the following regression equationyn,1=Xn,mβm,1+ϵn,1. The main idea underlying GME is the re-parametrization of each parameter of the regression model,βandϵ, as the expected value of a discrete random variable, namelyβm,1=Zm,mkβpmk,1β≡(Im,m⊗zk,1β)·(1m,1⊗pk,1β)andϵn,1=Zn,nhϵpnh,1ϵ≡(In,n⊗zh,1ϵ)·(1n,1⊗ph,1ϵ). In this context,zβandzϵare k×1 and h×1 centered around zero vectors (usually 3≤k, h≤7) called support vector whereasph,1βandph,1ϵare the corresponding vectors of probabilities. Note that,Im,mis an identity matrix, 1m,1 is a vector of ones, and ⊗ denotes the Kronecker product. A relevant issue concerns how set up these support vectors. Indeed, the vectorszβandzϵplay an important role in the GME estimation procedure. In general, they may be: fixed ad-hoc using some objective prior information (e.g.,z5,1β=[−2,−1,0,1,2]because [−2, 2] is the natural range of the regression parameter considered), set up as large as possible when no prior information is available (as we will see in Section 5), chosen by means of a sensitivity analysis [33] or the three-sigma rule [34]. It is straightforward to note that in this context, the support vectorszβandzϵcodify the prior information associated to the corresponding model parameter. Next, in order to estimate the model parameters, the GME approach recovers the unknown vectors of probabilitiespβandpϵby solving the following NLP system:(1)GME-problemMaximizing:H(pβ,pϵ)=−pβTlogpβ−pϵTlogpϵSubjectto:y−X(Zβpβ)+(Zϵpϵ)=01−pβT1=01−pϵT1=0where the first equation is called consistency constrain and represents the prior information about X whereas, on the contrary, the last two equations are normalization constraints on the recovered probability vectors. The problem can be solved, for instance, using the Lagrangian method, whose Lagrangian function is as follows:L(pβ,pϵλ,ϕ,τ)=−(pβTlogpβ−pϵTlogpϵ)+λ(y−X(Zβpβ)+(Zϵpϵ))+τ(1−pβT1)+ϕ(1−pϵT1)By equating to zero the gradient of the above function∇(L)=0, the final solutions are:pˆβ=e−λTXZβ/Ω(τ)pϵˆ=eλTZϵ/Ψ(ϕ)whereΩ(τ)=∑ke−λTXZβandΨ(ϕ)=∑keϕTZϵare scalars representing the normalization factors for the probability distributions associated with the model parameters, whereas τ and ϕ are the Lagrangian multipliers which are usually numerically found [35].Finally, several works have shown the advantages of the GME method of estimation (e.g., [15,32,25,36]). For illustrative purpose, we can consider that: GME does not require distributional errors assumptions, it is robust for a general class of error distributions, it works well with small samples and ill-posed matrices (statistical units less than variables, multicollinearity, etc.), it allows to use inequality constraints in the parameters estimation procedure which can be used for describing particular prior information about the phenomenon which is statistically modeled.Model and data analysis. LetXbe a n (cases) ×m (variables) matrix representing the set of independent variables andy˜be a n×1 vector representing the dependent (or predicted) fuzzy variable. The generic element yiofy˜defines the array yi={c, l, r} which represents a parametrized fuzzy set. By adopting the LR-parametrization,y˜can be re-expressed by a collection of n×1 vectorsc,landr, containing the set of parameters involved by the LR-representation. The crisp-input/fuzzy-output regression model can be expressed as follows:(2)c=Xβc+ϵl=(Xβc)βl+1αl+λr=(Xβc)βr+1αr+ρwhereβcis a m×1 vector of regression coefficient for the centers, αland αrare regression coefficients (scalars) for the intercepts of left and right spreads, βland βrare regression coefficients (scalars) for the left and right spreads, 1 is a n×1 vector of ones andϵ,λandρare n×1 vectors of residual terms. By adopting the GME approach, such regression model can be re-written as follows:(3)cn,1=Xn,m(Zm,mkcpmk,1c)+(Zn,nhϵpnh,1ϵ)ln,1=[Xn,m(Zm,mkcpmk,1c)]((zk,1l)Tpk,1l)+1n,1((zk,1αl)Tpk,1αl)+(Zn,nhλpnh,1λ)rn,1=[Xn,m(Zm,mkcpmk,1c)]((zk,1r)Tpk,1r)+1n,1((zk,1αr)Tpk,1αr)+(Zn,nhρpnh,1ρ)Considering the regression system (2), note that:(4)βc=Zm,mkcpmk,1c≡(Im,m⊗zk,1c)·(1m,1⊗pk,1c)βl=(zk,1l)Tpk,1lϵ=Zn,nhϵpnh,1ϵ≡(In,n⊗zh,1ϵ)·(1n,1⊗ph,1ϵ)αl=(zk,1αl)Tpk,1αlλ=Zn,nhλpnh,1λ≡(In,n⊗zh,1λ)·(1n,1⊗ph,1λ)βr=(zk,1r)Tpk,1rρ=Zn,nhρpnh,1ρ≡(In,n⊗zh,1ρ)·(1n,1⊗ph,1ρ)αr=(zk,1αr)Tpk,1αrrepresent the re-parametrization equations, where ⊗ is the Kronecker-product whereasIis an identity-matrix.Estimation procedure. The parameters are estimated recovering the corresponding probabilities vectors by maximizing the following functional:(5)H(pc,pl,pr,pαl,pαr,pϵ,pλ,pρ)=−(pmk,1c)Tlog(pmk,1c)−(pk,1l)Tlog(pk,1l)−(pk,1r)Tlog(pk,1r)−(pk,1αl)Tlog(pk,1αl)−(pk,1αr)Tlog(pk,1αr)−(pnh,1ϵ)Tlog(pnh,1ϵ)−(pnh,1λ)Tlog(pnh,1λ)−(pnh,1ρ)Tlog(pnh,1ρ)Subject to the following normalization constraints:(6)(i)(Im,m⊗1k,1)T(pmk,1c)=1m,1(v)(In,n⊗1h,1)T(pnh,1ϵ)=1n,1(ii)(pk,1l)T·1k,1=1(vi)(pk,1r)T·1k,1=1(iii)(pk,1αl)T·1k,1=1(vii)(pk,1αr)T·1k,1=1(iv)(In,n⊗1h,1)T(pnh,1λ)=1n,1(viii)(In,n⊗1h,1)T(pnh,1ρ)=1n,1and the three consistency constraints represented by the equations for the centers and spreads of the regression model in (1). The problem can be solved by adopting a NLP-method [35]. The parametrized solutions found using the Lagrangian multipliers method are available in the Appendix of this article.Model and data analysis. LetX˜be a n×m matrix representing a set of independent fuzzy variables andybe a n×1 vector representing the dependent (or predicted) variable. Like for the previous regression model, also in this case the generic element xijofX˜is an array xij={c, l, r}. In this context,X˜can be re-expressed by a collection of n×m matricesC,LandRwhich contain all the parameters involved by the LR-representation. The fuzzy-input/crisp-output regression model can be expressed as follows:(7)y=Cβc+Lβl+Rβr+ϵwhereβc,βl, andβrare m×1 vectors containing the regression parameters whereasϵis the n×1 vector of residual terms. Without loss of generality, after a simple algebra, such model can be re-written using an iterative structure with three nested equations, as follows:(8)yc=Cβc+ϵcwhereyc=y−Lβl−Rβryl=Lβl+ϵlwhereyl=y−Cβr−Rβryr=Rβr+ϵrwhereyr=y−Lβl−Cβcwhereyc,ylandyrare the deflated dependent variables. The GME re-parametrization of this model is as follows:(9)(i)(yn,1c)t=Cn,m(Zm,mkcpmk,1c)t+(Zn,nhϵcpnh,1ϵc)twhere:(yn,1c)t=yn,1−Ln,m(Zm,mklpmk,1l)t−1−Rn,m(Zm,mkrpmk,1r)t−1(ii)(yn,1l)t=Ln,m(Zm,mklpmk,1l)t+(Zn,nhϵlpnh,1ϵl)twhere:(yn,1l)t=yn,1−Cn,m(Zm,mkcpmk,1c)t−1−Rn,m(Zm,mkrpmk,1r)t−1(iii)(yn,1r)t=Rn,m(Zm,mkrpmk,1r)t+(Zn,nhϵrpnh,1ϵr)twhere:(yn,1r)t=yn,1−Cn,m(Zm,mkcpmk,1c)t−1−Ln,m(Zm,mklpmk,1l)t−1where t indicates the iteration. Considering the regression system (9), note that:βc=Zm,mkcpmk,1c≡(Im,m⊗zk,1c)·(1m,1⊗pk,1c)ϵc=Zn,nhϵcpnh,1ϵc≡(In,n⊗zh,1ϵc)·(1n,1⊗ph,1ϵc)βl=Zm,mklpmk,1l≡(Im,m⊗zk,1l)·(1m,1⊗pk,1l)ϵr=Zn,nhϵrpnh,1ϵr≡(In,n⊗zh,1ϵr)·(1n,1⊗ph,1ϵr)βr=Zm,mkrpmk,1r≡(Im,m⊗zk,1r)·(1m,1⊗pk,1r)ϵl=Zn,nhϵlpnh,1ϵl≡(In,n⊗zh,1ϵl)·(1n,1⊗ph,1ϵl)where ⊗ andIare defined as for the previous case.Estimation procedure. This regression system is represented by three nested NLP-problems [35]. The general estimation is performed by adopting an iterative descendent algorithm whose convergence is evaluated, for example, when the minimum change in the estimated parameters values occurred. For a fixed step of the descendent algorithm, the regression parameters are estimated recovering the corresponding probabilities vectors by simultaneously solving the following NLP problems:(10)(i)Maximize:H(pc,pϵc)=−(pmk,1c)Tlog(pmk,1c)−(pnh,1ϵc)Tlog(pnh,1ϵc)Subjectto:(Im,m⊗1k,1)T(pmk,1c)=1m,1(In,n⊗1h,1)T(pnH,1ϵc)=1n,1(yn,1c)t=Cn,m(Zm,mkcpmk,1c)t+(Zn,nhϵcpnh,1ϵc)t(ii)Maximize:H(pl,pϵl)=−(pmk,1l)Tlog(pmk,1l)−(pnh,1ϵl)Tlog(pnh,1ϵl)Subjectto:(Im,m⊗1k,1)T(pmk,1l)=1m,1(In,n⊗1h,1)T(pnH,1ϵl)=1n,1(yn,1l)t=Ln,m(Zm,mklpmk,1l)t+(Zn,nhϵlpnh,1ϵl)t(iii)Maximize:H(pr,pϵr)=−(pmk,1r)Tlog(pmk,1r)−(pnh,1ϵr)Tlog(pnh,1ϵr)Subjectto:(Im,m⊗1k,1)T(pmk,1r)=1m,1(In,n⊗1h,1)T(pnH,1ϵr)=1n,1(yn,1r)t=Rn,m(Zm,mkrpmk,1r)t+(Zn,nhϵrpnh,1ϵr)tThe final parametrized solutions are available in the Appendix of this article.Variables selection procedure can be considered an important topic in the regression framework. In the context of GME approach, this well-known procedure can be performed by measuring the reduction of uncertainty which is due to the consistency constraints on the GME problem. In particular, letyn,1=Xn,mβm,1 be the simplest linear model where, for the jth variable,βj=zk,1Tpk,1. The reduction of uncertainty which is due to the jth variable, can be measured by evaluating its normalized entropy index[25]:(11)S(p)j=−pk,1Tlog(pk,1)log(k)with:0≤S(p)j≤1In particular, S(p)jmeasures the divergence between the probability distributionpjand the uniform distribution, which is considered the distribution with the maximum degree of uncertainty. Therefore, when S(p)j<0.99 the jth variable highly contributes to the reduction of uncertainty inasmuch as its probability distribution tends to diverge from the uniform distribution. In the regression context, this means that the jth variable can be considered significant. On the contrary, variables with S(p)j≥0.99 show probability distributions closed to the maximum degree of uncertainty and, therefore, they can be considered not significant.In order to evaluate the performance of the GME regression models, we consider the following GME based normalized index [16,25]. Consider the simplest linear modelyn,1=Xn,mβm,1 whereβm,1=Zm,mkpmk,1≡(Im,m⊗zk,1)·(1m,1⊗pk,1). The GME based goodness-of-fit index is defined as follows:(12)Rpseudo2=1−(−pmk,1)Tlog(pmk,1)m·log(k)with:0≤Rpseudo2≤1which captures the reduction of uncertainty that is due to the consistency constraints defined in the GME optimization problem. Its interpretation is similar to the Soofis pseudo-R2[37]. In particular, whenRpseudo2tends to 0 the portion of uncertainty explained by the model is very low, whereas, on the contrary, whenRpseudo2tends to 1 the reduction of uncertainty is considered significant and, therefore, the model is very good.In this section we describe a series of Monte Carlo studies which were performed in order to assess the stability and reliability of the proposed GME-fuzzy vs. the standard fuzzy least squares. In particular, we studied the performances of both the approaches in two main conditions, namely a general case (GC) and an ill-posed case (IC). Unlike GC, in the second condition we corrupted the model matrix by augmenting the collinearity among the explanatory variables. More technically, in a first step, independent standard normal variables z1, z2, …, zm, z_m+1 was generated; in a second step, a matrixXn,mwas created by using the formulaxj=α·zm+1+(1−α2)·zj(∀j=2…m) [38]. This data generating process is based on an additional variable zm+1 and uses it to generate each xjso that the correlation ρ between any xjand xk(for j≠k) is given by α2. This condition was repeated within a main loop where initially α was equal to zero and then was increased from 0.949 to 0.99 by a step of 0.001, so that the correlation ρ between any two explanatory variables will be in the range from 0.9 to 0.98, since α2=ρ. Finally, the results were evaluated by considering:(i)the mean values of the regression coefficients (computed as average of the 1000 replications for both approaches) and their standard errors;the relative bias (RB) of the regression coefficients according to the formulaRB=(E(θˆ)−θ)/θwhereE(θˆ)is the average of the 1000 Monte Carlo replications. In particular, positive RB indicates an over-estimation of the true parameter θ whereas negative RB indicates under-estimation;the model prediction by evaluating the RMSE of the models;the relative efficiency of the two approaches (GME vs. fuzzy least squares) evaluated by the formulaRE(θˆgme,θˆols)=MSE(θˆols)/MSE(θˆgme). In particular, values lesser than one (RE <1) indicate that fuzzy-OLS is more efficient than GME-fuzzy (in terms of estimation accuracy) whereas, on the contrary, values greater than one for RE (RE>1) mean that GME-fuzzy is more efficient than fuzzy-OLS. In the particular situation in which RE=1 GME-fuzzy and fuzzy-OLS produce the same results in terms of estimation accuracy.The experiments GC and IC were conducted according to the following conditions:(a)m+1 independent standard normal variables z1, z2, …, zm, z_m+1 were generated;two levels of sample size, n=25 and n=50, were considered with three explanatory variables plus the intercept (m=4);the dependent variablescn,1,ln,1 andrn,1 were computed by applying the model described in equation (2) with the following regression parameters(βc)T=[−1.5,−0.2,1.6,2.3], βl=1.7, βr=0.9, αl=−4.2, αr=−2.1;the error termsϵn,1,λn,1 andρn,1 were drawn from a normal distribution N(0, 1);for the GME approach, the supports of the regression coefficients(zk,1c)T,(zk,1l)T,(zk,1r)T,(zk,1αl)T, and(zk,1αr)Twere set up on [−100, −50, 0, 50, 100] (no restrictions on the parameters space [36]) whereas those ones of the residual termszh,1ϵ,zh,1λ, andzh,1ρwere obtained by the three-sigma-rule [−3σ, 0, 3σ] [34] where σ is the standard deviation of the dependent variablesc,l, andr, respectively;the matrixXn,mwas generated according to the formulaxj=α·zm+1+(1−α2)·zj(∀j=2…m) [38];steps (a)–(f) were replicated within a loop where initially α was equal to zero and then was increased from 0.949 to 0.99 by a step of 0.001, so that the correlation ρ=α2 will be in the range 0.9 to 0.98;steps (a)–(g) were replicated 1000 times.

@&#CONCLUSIONS@&#
In this paper we proposed a novel estimation method for fuzzy regression models based on the Generalized Maximum Entropy (GME) approach. The proposed GME-fuzzy regressions allowed to take into account the main advantages of such entropy-based estimation method (namely, correct estimation process in ill-posed cases, use of external information in the estimation process, peculiar variable selection procedure, excellent work with distributional violations). To better illustrate the GME-fuzzy features, we also described two Monte Carlo studies and a real application. The empirical results suggest that using GME within a unified fuzzy regression framework is a more useful approach especially when researchers have to deal with ill-posed cases and real cases in which powerful variable selection procedures are required. Considering the ill-posed cases, in which some variables may show the same prediction power, GME allows to explain what is the real contribution of such variables in the model prediction. Indeed, unlike OLS, GME always produces small standard errors, negligible estimation biases, and trivial type-II errors. In such a context, using latent variables in place of the original collinear variables (e.g., by means of PCA) may be considered as an alternative method to transform the ill-posed data matrix in a new well-posed matrix. In this respect, the regression analysis is performed on a new set of orthogonal and uncorrelated variables which are obtained as linear combination of the original (collinear) independent variables. However, this approach works on a set of latent variables and it does not use the original data matrix. On the contrary, GME always works on the original variables and no further transformation of the variables is required in the case of ill-posed matrices. Moreover, the PCA approach does not take into account the part of the unexplained variance due to selection of sub-set of new variables as the model considers predictors without measurement errors.Different possible extensions of our proposal can be taken into account. For instance, unlike GME which takes into account uniform subjective prior information, the adoption of the so-called Generalized Cross Entropy (GCE) approach would extend our proposal in order to take into account prior subjective information which may be modeled by suitable probabilistic distributions. A future venue of research may also consist in the adoption of a fuzzy entropy based representation (e.g., [42]) in order to develop a system which can also handle with non-random uncertainty which is represented in the empirical data. Finally, an extensive Monte Carlo study may be performed in order to better understand the sample performances of the GME parameters on other but still important fuzzy regression models (e.g., fuzzy-input/fuzzy-output).In order to obtain the analytic solutions for the model we adopt the Lagrangian multipliers method. The Lagrangian functional takes the following form:(A.1)L=H(pc,pl,pr,pαl,pαr,pϵ,pλ,pρ)+τn,1T(cn,1−Xn,m(Zm,mkcpmk,1c)+(Zn,nhϵpnh,1ϵ)+(Zn,nhϵpnh,1ϵ))+μn,1T(ln,1−[Xn,m(Zm,mkcpmk,1c)]((zk,1l)Tpk,1l)+1n,1((zk,1αl)Tpk,1αl)+(Zn,nhλpnh,1λ))+δn,1T(rn,1−[Xn,m(Zm,mkcpmk,1c)]((zk,1r)Tpk,1r)+1n,1((zk,1αr)Tpk,1αr)+(Zn,nhρpnh,1ρ))+ξl(1−(pk,1l)T·1k,1)+ξr(1−(pk,1r)T·1k,1)+θl(1−(pk,1αl)T·1k,1)+θr(1−(pk,1αr)T·1k,1)+ωm,1T(1m,1−(Im,m⊗1k,1)T(pmk,1c))+γn,1T(1n,1−(In,n⊗1h,1)T(pnh,1ϵ))+ηn,1T(1n,1−(In,n⊗1h,1)T(pnh,1λ))+ϕn,1T(1n,1−(In,n⊗1h,1)T(pnh,1ρ))whereτ,μ,δ, ξl, ξr, θl, θr,η,γ,ω, andϕare Lagrangian multipliers. Solving its first order conditions yields to the following parametrized solutions:pˆmk,1c=e−τn,1TXn,mZm,mkc−δn,1TXn,mZm,mkc((zk,1r)Tpk,1r)−μn,1TXn,mZm,mkc((zk,1l)Tpk,1l)∑ke−τn,1TXn,mZm,mkc−δn,1TXn,mZm,mkc((zk,1r)Tpk,1r)−μn,1TXn,mZm,mkc((zk,1l)Tpk,1l)≡Ω(ω)mpˆk,1l=e−μn,1TXn,mZm,mkcpmk,1czk,1l∑ke−μn,1TXn,mZm,mkcpmk,1czk,1l≡Ω(μ)pˆk,1αl=eμn,1T1n,1zk,1αl∑keμn,1T1n,1zk,1αl≡Ω(θl)pˆk,1r=e−δn,1TXn,mZm,mkcpmk,1czk,1r∑ke−δn,1TXn,mZm,mkcpmk,1czk,1r≡Ω(δ)pˆk,1αr=eδn,1T1n,1zk,1αr∑keδn,1T1n,1zk,1αr≡Ω(θr)pˆnh,1ϵ=eτn,1TZn,nhϵ∑ieτn,1TZn,nhϵ≡Ψ(γ)npˆnh,1λ=eμn,1TZn,nhλ∑ieμn,1TZn,nhλ≡Ψ(η)npˆnh,1ρ=eδn,1TZn,nhρ∑ieδn,1TZn,nhρ≡Ψ(ϕ)nwhereΩ(ω)mis the normalization factor for the mth centers,Ω(μ),Ω(δ), Ω(θl) and Ω(θr) are the normalization factors for the left/right spreads and intercepts, whereasΨ(γ),Ψ(η) andΨ(ϕ) are the normalization factors for the error terms associated to the centers and left/right spreads. Once the probability vectors of the regression parameters are estimated, the final regression coefficients can be numerically computed.In order to obtain the analytic solutions for the models, we use the Lagrangian multipliers method for each regression equation of the fuzzy-input/crisp-output iterative system. For the sake of simplicity, we show only the solution for the first model (note that, for the other models the solving procedure is the same).The Lagrangian function is as follows:(B.1)L(i)=H(pc,pϵc)+τn,1(yn,1c−Cn,m(Zm,mkcpmk,1c)t+(Zn,nhϵcpnh,1ϵc))+ωm,1(1m,1−(Im,m⊗1k,1)T(pmk,1l))+γn,1T(1n,1−(In,n⊗1h,1)T(pnH,1ϵc))whereas the solutions at the first order conditions are:pˆmk,1c=e−τn,1Cn,mZm,mKc∑ke−τn,1Cn,mZm,mKc≡Ω(ω)mpˆnh,1ϵc=e−τn,1Zn,nhϵ∑he−e−τn,1Zn,nhϵ≡Ψ(γ)n