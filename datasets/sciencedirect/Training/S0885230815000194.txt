@&#MAIN-TITLE@&#
Situated language understanding for a spoken dialog system within vehicles

@&#HIGHLIGHTS@&#
We implemented and analyzed issues in situated language understanding in moving car.We analyzed timing of utterances, spatial relationships between the car and targets.Our algorithms improved the target identification rate by 24.1%.

@&#KEYPHRASES@&#
Spoken dialog systems,Situated dialog,Language understanding,Reference resolution,

@&#ABSTRACT@&#
In this paper, we address issues in situated language understanding in a moving car, which has the additional challenge of being a rapidly changing environment. More specifically, we propose methods for understanding user queries regarding specific target buildings in their surroundings. Unlike previous studies on physically situated interactions, such as interactions with mobile robots, the task at hand is very time sensitive because the spatial relationship between the car and target changes while the user is speaking. We collected situated utterances from drivers using our research system called Townsurfer, which was embedded in a real vehicle. Based on this data, we analyzed the timing of user queries, the spatial relationships between the car and the targets, the head pose of the user, and linguistic cues. Based on this analysis, we further propose methods to optimize timing and spatial distances and to make use of linguistic cues. Finally, we demonstrate that our algorithms improved the target identification rate by 24.1% absolute.

@&#INTRODUCTION@&#
Recent advances in sensing technologies have enabled researchers to explore applications requiring a clear awareness of a system's dynamic context and physical surroundings. Such applications include multi-participant conversation systems (Bohus and Horvitz, 2009) and human robot interaction systems (Tellex et al., 2011; Sugiura et al., 2011). The general problem of understanding and interacting with human users in such environments is referred to as situated interaction.In this current study, we address another environment in which situated interactions take place, i.e., a moving car. In our previous work, we collected over 60h of in-car human–human interactions in which drivers interacted with an expert co-pilot sitting next to them in the vehicle (Cohen et al., 2014). One of the insights from the analysis on this corpus is that drivers frequently use referring expressions about their surroundings (e.g., What is that big building on the right?). Based on this insight, we developed Townsurfer (Lane et al., 2012; Misu et al., 2013), a situated in-car intelligent assistant. Using geolocation information, the system can answer user queries/questions that contain object references regarding points-of-interest (POIs) in their surroundings. We use driver (user) face orientation to understand their queries and provide the requested information regarding the POI they are looking at. We previously demonstrated and evaluated this system in a simulated environment (Lane et al., 2012). In this paper, we evaluate its utility in real driving situations.Compared to conventional situated dialog tasks, query understanding in our task is expected to be more time sensitive, due to the rapidly changing environment while driving. Typically, a car moves 10m in one second while driving at 25mi/h (40km/h). Therefore, timing can be a crucial factor. In addition, it is not well understood what kind of linguistic cues are naturally provided by drivers, and their contributions to situated language understanding in such an environment. To the best of our knowledge, this is the first study that tackles the issue of situated language understanding in rapidly moving vehicles.In addition to this introductory section, we present an overview of the Townsurfer in-car spoken dialog system in Section2. Based on our data collection using this system, we analyze user behavior while using the system with a focus on language understanding; our findings are presented in Section3. More specifically, we address the following research questions regarding the task and the system through data collection and analysis:1Is timing an important factor of situated language understanding?Does head pose play an important role in language understanding? Or is spatial distance information enough?What is the role of linguistic cues in this task? What kinds of linguistic cues do drivers naturally provide?The Townsurfer system uses three main input modalities, namely speech, geolocation, and head pose. Speech is the primary input modality and is used to trigger interactions with the system. User speech is recognized, and requested concepts and values are then extracted. Geolocation and head pose information are used to understand the target POI of the user query. An overview of the system and its process flow is illustrated in Fig. 1; an example dialog with the system is shown in Table 1.In our research, we address issues in identifying user-intended POIs; this is a form of reference resolution using multi-modal information sources. That is, we handle information request about specific POIs, such as U1 and U3 in Table 1. We do not currently address issues in language understanding related to dialog history and query type, for example, requests regarding specific properties of POIs, such as U2 in the table. The POI identification process consists of the three steps summarized below (cf. Fig. 1). Note that these steps are similar to, but different from our previous work on landmark-based destination setting (Ma et al., 2012).(1)The system lists candidate POIs based on geolocation at the time of a driver query. Relative positions of POIs to the car are calculated based on geolocation and the heading of the car.Based on spatial linguistic cues in the user's speech (e.g., to my right, on the left), a 2D scoring function is selected to identify areas in which the target POI is likely to be present. This function takes into account the position of the POI relative to the car, as well as the driver's head pose. Scores for all candidate POIs are calculated.Posterior probabilities of each POI are calculated using the score of step 2 as prior, and non-spatial linguistic information (e.g., POI categories, building properties) as observations. This posterior calculation is computed using our Bayesian belief tracker, DPOT (Raux and Ma, 2011).Further details are provided in Section4.The system hardware consists of a 3D depth sensor (PrimeSense Carmine 1.09), a USB GPS (BU-353S4), an inertial measurement unit (IMU) sensor (3DM-GX3-25), and a close-talking microphone (Plantronics Voyager Legend UC). We installed these consumer-grade sensors in our experimental Honda Pilot. We used the Point Cloud Library22http://pointclouds.org/.(PCL) for face direction estimation, with the model included in the library. The geolocation was estimated based on an Extended Kalman filter-based algorithm using GPS and gyro information as input at 1.5Hz. The system was implemented based on the Robot Operating System (ROS) (Quigley et al., 2009). Each component was implemented as a node of the ROS, and the communication between nodes was performed using standard message passing mechanisms of ROS.We collected data using a test route that passed through downtown Mountain View and residential areas around the Honda Research Institute. We assumed that a POI is downtown when it is located within the rectangle formed by two geolocation coordinates. We manually constructed a database that contained 250 POIs including businesses, restaurants, and other companies in the area. Each database entry (i.e., POI) had a name, geolocation, category and property information (as explained in Section3.4). POI geolocation was represented as a latitude–longitude pair (e.g., 37.4010, −122.0539). The size and shape of buildings were not taken into account. The route took approximately 30min to drive, with major difference between the residential areas and downtown in the POI density. While each POI located downtown had on average 7.2 other POIs within 50m, in residential areas, POIs had only 1.9 such neighbors. Speed limits also differed between the two (35mi/h and 25mi/h, respectively).We collected data from 14 subjects. Each participant was asked to drive the test route and make queries about surrounding businesses. We showed a demo video of the system to the users before starting the data collection. We also told them that the objective was data collection for a situated spoken dialog system rather than the evaluation of the system. We asked subjects to include the full description of the target POI within a single utterance to avoid queries whose understanding required dialog history information.33Including dialog history information is earmarked as part of our future work.Although the system answered based on the baseline strategy explained in Section4.1, we asked subjects to ignore system responses.44We thought that giving system responses to the users will create better engagement for them.We collected 399 queries with valid target POIs. Example user utterances are shown in Table 2. Queries regarding businesses that do not exist in our database (typically a vacant store) were excluded. The data contain 171 and 228 POIs in the downtown and residential areas, respectively. The queries were transcribed and user-intended POIs were manually annotated by confirming the intended target POI with the subjects after data collection based on a video taken during the drive.55This means that there is a possibility of annotation errors.We first analyzed the spatial relationship between position cues (right/left) and the position of the user-intended target POIs. Out of the collected 399 queries, 237 (59.4%) contained either right or left position cues (e.g., What is that on the left?). The relationship between the position cues (cf. Fig. 2) and POI positions at the start-of-speech timing66Specifically, the latest GPS and face direction information at that timing is used.is plotted in Fig. 3. In the figure, the X-axis is the lateral distance (i.e., the distance in the direction orthogonal to the heading with a positive value indicating the right) and the Y-axis is the axial distance (i.e., the distance in the heading direction with a negative value indicating that the POI is in back of the car). The most obvious finding from the scatterplot is that right and left are certainly powerful cues for the system in identifying target POIs.77There were some cases that points which were geometrically to the left of the car were described as being to the right meaning that a driver was speaking while making a left turn. This is because there is one intersection with left turn in the test route that does not have a traffic signal (cf. Appendix 3rd maneuver), and the subjects entered the intersection without slowing down. On the other hand, all right turns in the test route were made in the intersections with traffic signals, thus the subjects stopped and made queries. We think this is the reason why the corresponding error for left did not occur.We also observed that the POI position distribution has a large standard deviation. This is partly because the route has multiple sites from both downtown and the residential areas. While the average distance to the target POI downtown was 37.0m, that in the residential areas was 57.4m. POI positions per site are illustrated in Fig. 4.We also analyzed the relationship between face direction and POI positions. Fig. 5plots the relationship between the axial distance and angular difference θ (i.e., between the user face direction and the target POI direction) (cf. Fig. 2). The scatterplot suggests that the angular differences for distant target POIs are often small. For close target POIs, the angular differences are larger and have a larger variance.88We will discuss the reason for this in Section6.2.Referring expressions such as “the building on the right” must be resolved with respect to the context in which the user intended; however, in a moving car, such a context (i.e., the position of the car and the situation in the surroundings) can be very different between the time when the user starts speaking the sentence and the time they finish speaking it. Therefore, situated understanding must be very time sensitive.To confirm and investigate this issue, we analyze the difference in the POI positions between the time the ASR result is output vs. the time the user actually started speaking. The hypothesis is that the latter yields a more accurate context in which to interpret the user sentence. In contrast, our baseline system uses the more straightforward approach of resolving expressions using the context at the time of resolution, i.e., whenever the ASR/NLU has finished processing an utterance (hereafter “ASR results timing”).Specifically, we compare the average axial distance to the target POIs and its standard deviation between these two timings; Table 3lists these figures as per the query types of position cues and sites. The average axial distance from the car to the target POIs is small at the ASR result timing; however the standard deviation is generally small at the start-of-speech timing.99The results are not statistically significant.This indicates that the target POI positions at the start-of-speech timing are more consistent across users and sentence lengths than that at the ASR result timing. This result indicates the possibility of a better POI likelihood function using the context (i.e., car position and orientation) at the start-of-speech timing than using the ASR result timing.We analyzed the linguistic cues provided by the users. Here, we focus on objective and stable cues. We excluded subjective cues (e.g., big, beautiful, colorful) and cues that might change in a short period of time (e.g., with a woman dressed in green in front).1010Percentages of user utterances containing these cues are 3.3% and 0.5%, respectively.We categorized the linguistic cues used to describe the target POIs; Table 4lists the cue types and the percentage of user utterances containing each cue type.The cues that the users most often provided concern POI positions relative to the car (right and left). Nearly 60% of the queries included this type of cue, and every subject provided it at least once. The second most frequent cue is the category of business, especially downtown. Users also provided colors of POIs; other cues include cuisine, equipment, and relative position to the road (e.g., on the corner).Another interesting finding is that users provided more linguistic cues as the number of candidate POIs in their field of view increased. More specifically, users provided an average of 1.51 categories per query downtown, and an average of only 1.03 categories in the residential areas, though the difference was not statistically significant (cf. POI density in Section3.2: 7.2 vs. 1.9). This indicates that users provided cues based in part on environment complexity.We used our previous version (Misu et al., 2013) as a baseline system for situated language understanding. The baseline strategy is summarized in the three paragraphs below, which correspond to processes (1), (2), and (3) in Section2 and Fig. 1.The system performs a POI lookup based on the geolocation information at the time the ASR result is obtained. The search range of candidate POIs was within the range (i.e., relative geolocation of POIs against the car location) of −50 to 200m in the traveling direction and 100m to both the left and right in the lateral direction. The ASR result timing was also used to measure distances to the candidate POIs.POI priors were calculated based on the distance from the car (the axial distance) following the “the closer to the car the more likely” principle. We used a likelihood function inversely proportional to the distance. Furthermore, we used position cues simply to remove POIs from a list of candidates. For example, the “right” position cue was used to remove candidate POIs located to the left (with a lateral distance less than zero). When no right or left cue was provided, POIs outside 45° from the face direction were removed from the list of candidates.No linguistic cues except right and left were used to calculate POI posterior probabilities. Therefore, the system selected the POI with the highest prior (POI score) as the language understanding result.To achieve better situated language understanding (and therefore POI identification) based on our analysis in Section3, we modified steps (1), (2), and (3) as follows:1Use start-of-speech timing for the POI prior calculationUse a Gaussian mixture model (GMM)-based POI probability (prior) calculationUse linguistic cues for the posterior calculation.We used start-of-speech timing instead of the time the ASR result is output; because the standard deviations of the POI distances were small (cf. Section3.2), we expect that a better POI probability score estimation with the POI positions at this timing will be achieved in subsequent processes. The POI lookup range was the same as the baseline.We applied a GMM with diagonal covariance matrices over the input parameter space (Bishop, 2006). The POI probability (prior) p(x) was given by the following equation.p(x)=∑k=1KπkN(x|μk,Σk).where x is a d-dimensional data vector (input parameters),πk, k=1, …, K, are the mixture weights, andN(x|μk,Σk)are the component Gaussian densities. Each component density is a d-variate Gaussian function of the form,N(x|μk,Σk)=1(2π)d/2|Σk|1/2exp−12(x−μk)′Σk−1(x−μk),with mean vectorμkand diagonal covariance matrix Σk. The mixture weights sum up to 1 (Σk=1Kπk=1). These parameters are estimated using EM algorithm (Bishop, 2006) based on data. We used two input parameters (d=2), i.e., the lateral and axial distances for queries with right or left cue; for queries without right or left cues, we used three parameters (d=3), i.e., the lateral and axial distances, and the difference in degree between the target and head pose directions (The effect of these parameters is discussed in Section6.2). We empirically set the number of Gaussian components to 2 (K=2). An example GMM fitting to the POI positions for queries with right and left cues is illustrated in Fig. 6in which the center of the ellipse is the mean of the Gaussian.We used the five linguistic cue categories of Section3.4 for the posterior calculation by the belief tracker (Raux and Ma, 2011). In our system, because we do not use contextual information the likelihood is given by agreement between the NLU result and system properties. In the following experiments, we used either 1 or 0 as the likelihoods of the natural language understanding (NLU) observations. The likelihood of a category value is 1 if a user query (NLU result) contains the target value or a user query does not contain any target value; otherwise, 0. The likelihood of a query is given by the product of the likelihoods of these categories. This corresponds to a strategy of simply removing candidate POIs that do not have category values specified by the user, and using the priors of the remaining candidate POIs as their posteriors. Here, we assumed a clean POI database with all properties manually annotated.

@&#CONCLUSIONS@&#
