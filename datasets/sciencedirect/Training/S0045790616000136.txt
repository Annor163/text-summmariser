@&#MAIN-TITLE@&#
Image and medical annotations using non-homogeneous 2D ruler learning models

@&#HIGHLIGHTS@&#
A metric learning model directly calibrating measured numerics of feature data.A 2D semi-metric model by relaxing one degree of freedom.Simple and intuitively natural models solvable by convex quadratic programming.

@&#KEYPHRASES@&#
Semidefinite programming,Image annotation,Medical annotation,Prediction model,

@&#ABSTRACT@&#
Graphical abstractImage, graphical abstract

@&#INTRODUCTION@&#
In recent decades, there is a misleading trend in the artificial intelligence community that researchers devise increasingly complicated methods, which is not based on an in-depth analysis of the intrinsic properties of the training and testing datasets. Instead, they focus on other aspects such as the convexity of the optimization problem. As a result, they lose the potential opportunity to discover certain ingenious places where adaptive parameters exist. If these adaptive parameters can be learned in a formalized learning model, a learning process or intelligence happens.A traditional mode for machine learning researchers to solve problems is that, they firstly devise a new method from mathematics or biological inspirations, and then try to apply the new method to a variety of problems. In this paper, we attempt to show an inverse methodology: it originates from the problem itself by analyzing the dataset [1], aiming to change some underlying assumptions for achieving higher recognition accuracy in several simple experiments. If such underlying assumptions and the corresponding successful ways to change them are identified, these underlying assumptions can then be broken, generalized and modeled as adaptive parameters to be learned from the training set, and applied to the testing set [2].We use the theory of evolution as a philosophical analogue to compare the aforementioned two methodologies for their contribution to the evolution of artificial intelligence. The former traditional one is more like the process of natural selection: a newly devised method, which is either partly or entirely different from previous methods, is just like the mutation of the genes, and its theoretical and experimental validation act like the natural selection, in which, only the ones with the highest performance11Here the performance can be recognition accuracy, robustness, computational cost, extensibility, flexibility etc.survive. It is well-known that, the natural selection process is of low efficiency. What makes the later inverse methodology more efficiently contribute to the development of artificial intelligence is that, it incorporates the intelligence of artificial intelligence researchers (from human brains that can think symbolically) when they analyze the dataset, to some extend.In this paper, we present such a case study using the later inverse methodology. At first glance, our final proposed model seems to like building a non-linear embedding to project the sample data points with the same label closer to each other in the embedding space. But after a closer scrutiny and comparison with the LLE (Locally Linear Embedding) or other non-linear embedding methods, there is fundamental difference: The LLE is to find low-dimensional global coordinates when the sample points approximately lie on a manifold embedded in a high- dimensional space [3]. Its essential idea is to perform a different linear dimensionality reduction at each sample point (locally a manifold looks linear) and then combine these with minimal discrepancy [13]. But in our proposed models, there is no dimensionality reduction or increase.The organization of the paper is as below: In Section 2, we first discuss some simple preliminary experimental results, based on which, our non-homogeneous 2D ruler model is proposed in Section 3; experiments are reported and discussed in Sections 4, and 5 gives some conclusions and also directions for future work.We commence solving the image classification problem with the simplest kNN classifier. It is acknowledged that, even combined with the simplest kNN classifiers, learning a distance metric from labeled examples yields quite competitive results, such as reported in [4]. Without the loss of generality, the LabelMe dataset [6] is used as an example. All experimental settings are the same as those in Section 4.Denote S=(Xi,Li) (1≤i≤N) as a training set of N labeled samples with feature vectorsXi∈ Rqand their associated labelsLi. We useXi[k] to denote the kth component (or dimension) ofXi. Given two q-dimensional sample pointsXiandXj, the Lp norm distance is defined as:(1)d(Xi,Xj)=(∑k=1q|Xi[k]−Xj[k]|p)1/pNote that when p=2 and 1, it is the commonly used L2 norm Euclidean distance and L1 norm absolute difference value sum respectively. We used various p values, and the recognition rate by the simplest kNN classifier (k=7) is listed in Table 1. As a comparison, we cite the reported results in [7]: the SVM achieved 71%, and the Wang's proposed method achieved 77%.It is seen from Table 1 that, the recognition rate peaks when p is around 0.6. To give the readers a natural intuitive explanation, we have to first revisit the meaning of the measured feature dataXi(see [6–8] and Section 4):Xiis the 256-dimensional histogram of the ith training image, in which,Xi[k] counts the number of image patches belong to the kth codeword generated by the k-means clustering (i.e.Xi[k] is the number of the kth-class image patches). As a human being, if we seeXi[k] andXj[k] differ greatly, it means the ith and jth images have thoroughly different number of kth-class image patches. Thus when |Xi[k]-Xj[k]| is very large, |Xi[k]-Xj[k]|=100, 200 or 500 should make almost no difference in the distance penalty of d(Xi,Li). This observation explains why the L2 norm performs so badly, and why L0.6 norm beats L1 norm here.Instead of using subtraction to measure the difference between the two quantitiesXi[k] andXj[k], we use the division arithmetic:(2)d(Xi,Xj)=(∑k=1q|logXi[k]+1Xj[k]+1|p)1/pNote that, the logarithms must be taken here, so that a/b and b/a lead to the same distance penalty for any a,b ∈ R, as ln|a/b|=ln|b/a|. Otherwise, d(Xi,Xj) does not equal to d(Xj,Xi). All feature data are added by 1 to avoid division by zero. Similar as above, Table 2tabulates the results of the various p values used.It is seen from Table 2 that, the recognition rate increases by approximately 1% when using the log Lp norm Eq.(2). An intuitive explanation is below: for the same |Xi[k]-Xj[k]| value, the actual meaning might be very different, e.g. ifXi[k]=10 andXj[k]=5, it means the ith image has twice kth-class image patches than the jth image; ifXi[k]=110 andXj[k]=105, it means the ith and jth image have almost the same kth-class image patches. This observation explains why division is better than subtraction here.Note that the kNN classifier used above is the simplest and most primitive hard kNN (i.e. the k neighbors have equal voting weight), if we simply change the traditional kNN to the well-known soft kNN with the voting weight defined as 1/(d+100000), where d is the distance between the testing sample point and a k-nearest neighbor, the recognition rate increases to 76.89%, which is the same as the state-of-the-art method by Wang et al. [7]. Because tuning parameters are not the focus of this study, we use this kNN classifier (k=7) through the rest of the paper.Here the L0.6 norm and the logarithmic space have no generally applicable meanings: it depends on the datasets, and is not optimal. The optimal norm might be L0.62 or somewhere else, the optimal space to measure the difference of two scalar quantities might be a linear combination or a more complicated transitional space between the original space and its logarithmic space. And there are different ways to map to various logarithmic spaces.But anyway, the recognition rate is sensitive to any of the changes we made in these simple experiments, and thus inspired us that, if these changes can be generalized and parameterized, they are exactly the pivotal adaptive parameters that should be learned in a formalized learning model.In this paper, we propose one feasible method of such generalization and parameterization: if we calibrate the measured numerics of the observed feature data in a deliberate way, even simple kNN can achieve competitive recognition accuracy.In the proposed non-homogeneous ruler model, the pivotal adaptive parameters to learn is the distance between two uncalibrated numerics x and y. It can be considered as the 1D model with one degree of freedom added: the distance between the numerics x and y is parameterized as R[x,y]. The variables (or the adaptive parameters to learn) are R[0,1],R[0,2],R[0,3],….,R[0,V] with 2D indices, so it is called the non-homogeneous ruler 2D model. Where V is the maximal value ofXi[k]. AsXiis a histogram, V cannot exceed the number of image patches per image (which is 2500 for the joint image classification/annotation problem). Actually, there is far less than V variables here, because many numerics x ∈ [1,V] never appear inXi[k], especially when x is large.Because for any numeric x, y, z, it is possible that R[x,y]+R[y,z]<R[x,z], so the 2D model violates the triangle inequality, one of the metric axioms. But still, we force the 2D model to comply with the constraints R[x,x]=0, R[x,y]=R[x,y] so that there are reduced number of variables, i.e. R[x,y] is a variable only if x<y. We force(3)R[x,y]≤R[x,y,]ify<y′andR[x,y]≥R[x′,y]ifx<x′,foranynumericx,y,x′,y′So that the resulted distance function is a semi-metric22So far we did not see a successful distance function learning methods (such as [4,9]) violate either the non-negativity, or the symmetry axioms. Strictly speaking, all the so-called metrics are pseudo-metrics, as the identity of indiscernibles axiom is not satisfied: there exist many ddij=0 for i≠j, as long asLi=Lji.e.XiandXjhave exact the same labels.. To develop our proposed objective function, we still follow the basic idea of the Lp norm metric in Eq. (1). We set p=1 to make our model easy to solve(4)d(Xi,Xj)=∑k=1qR[min{Xi[k],Xj[k]},max{Xi[k],Xj[k]}]When solving machine learning problems, attention should be paid to equality constraints. Many equality rarely holds either theoretically (because the problem is over-constraint) or practically (because error is inevitable). In fact, most equality only implies approximate equality. Often, the difference between the left and right side of the equality is measured by a defined norm, and then accumulated (or integrated for continuous case) as an energy objective function for optimization. Using the least square criterion to measure the difference between both sides of Eq. (4) yields an energy objective function(5)E(R)=∑i=1N∑j=i+1N(∑k=1qR[min{Xi[k],Xj[k]},max{Xi[k],Xj[k]}]−ddij)2Because d(Xi,Xj) equals to d(Xj,Xi) and d(Xi,Xi)=0, we only count d(Xi,Xj) when i<j. Where ddijis the desired distance betweenXiandXj. Since our basic logic is to make samples with similar labels closer in the calibrated space, any two training samplesXiandXjshould have a desired distance ddijcompletely determined by their labels. [4] deals with 7 single-label classification datasets, in which, ddijis 0 ifLi=Lj, and 1 otherwise33Actually, it is equivalent to the intuition of the LDA (Linear Discriminant Analysis), i.e. minimize the distance of data points within the same class, and maximize the distance of data points between classes. The importance of the minimization and the maximization may be adjusted via a weight. But in [4] and our methods, this weight is implicitly set to 1. There might be some slight performance increase by tuning this weight.. Although the feature vectors can be incorporated for computing the desired distance ddijin an iterative framework to avoid overfitting and outliers [9], it is beyond the scope of this study. Here only inter-label dissimilarity is used to determine ddij.If aligning all variables R[x,y] (x<y) into a vector R, Eqs. (3) and (5) also constitute a convex quadratic programming problem [10]. It is obvious that E(R)≥0, so the resulted quadratic form is positive semidefinite, and the quadratic programming is convex. There are some publicly available generic packages for solving this kind of problems effectively [11]. More specifically, the optimization of the proposed 2D model costs 13160 seconds averagely on an Itanium 7–5500U (Quad Core 2.4GHz, 8G Ram memory) computer using Matlab R2013b.It is not difficult to see that, the proposed 2D model is a generalized form of all the distance functions we used in Section 2, for whatever p values. The diagram of the complete main workflow of our proposed model is shown in Fig. 1.

@&#CONCLUSIONS@&#
