@&#MAIN-TITLE@&#
Hierarchical classification of images by sparse approximation

@&#HIGHLIGHTS@&#
A new hierarchical classification scheme by sparse approximation is proposed.Leverage large scale structured data for the accurate hierarchical classification.Distance function taking into account the hierarchical structure is introduced.Defined two images to be similar if they shared a similar path in the hierarchyAchieved better performances than flat 1-vs-N classification methods

@&#KEYPHRASES@&#
Sparse approximation,Sparse sensing,Sparsity,Image classification,Hierarchy,Structured data classification,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Recent advances in computer vision and machine learning have enabled the design of recognition methods that are capable of classifying images into large number of visual categories (typically, hundreds) [11,8,6,14]. In one of the current paradigms for image categorization, image classes are organized in a flat structure and the problem is to discover the class (among all those in the flat structure) that best represents (in term of a distance function) the visual content of a given query image.Recently, researchers have explored the idea of organizing visual data in a hierarchical structure rather than in a flat one. This paradigm addresses some of the limitations of the flat structure: i) it allows for a significant gain in efficiency, typically logarithmic with the number of categories, as addressed by Marszalek and Schmid [16] and Griffin and Perona [12]; ii) it enables the construction of a more meaningful distance metric for image classification; and iii) it echoes the way how humans organize data, as addressed by Palmer [17]. However, a critical question still remains controversial: would structuring data in hierarchical sense also help classification accuracy? Up to date there is no definite answer to that question. For instance, top-down classification schemes (applied on hierarchical structures) proposed by Marszalek and Schmid [16] and Griffin and Perona [12] have produced inconclusive evidence as for whether hierarchy has a beneficial effect on classification accuracy. Classification methods based on Hierarchical Support Vector Machines can be used to trade off accuracy against speed as in Griffin and Perona [12] or employed to increase classification accuracy as originally proposed by Tsochantaridis et al. [21] and utilized for image classification, as suggested by Binder et al. [2]. Although [2] has shown promising results, it is computationally very demanding as the number of categories becomes larger than 30~50. Finally, methods based on combining models from different levels of the hierarchy proposed by Zweig and Weinshall [23] have also shown positive results but are yet to be validated on deeper and larger hierarchical structures.In this paper we attempt to address the issues discussed above and show that the hierarchical structure of a database can be successfully used to enhance classification accuracy using a sparse approximation framework. The key idea is to introduce a distance function that takes into account the hierarchical structure of the visual categories (Fig. 1) and to identify two images to be similar if they share a similar path in the hierarchy. We show that this distance function (or similarity metric) is equivalent to the Hamming Distance (HD) for vectors that encode the hierarchy. This allows us to cast the categorization problem as the one of discovering the category in the tree structure that has the smallest HD from the query category label. We solve this problem via sparse approximation and introduce a new formulation of the sparse approximation problem which we call hierarchical sparse approximation. In the typical sparse approximation problems [22,5,20], a query image can be identified as the sparsest representation over the set of training images, as proposed by Wright et al. [22] or basis functions, as proposed by Mairal et al. [15] for all object classes; that is, the sparsest solution is one (or a combination of a few) image out of all possible images in the dataset. We call this the flat sparse approximation problem. The key novelty of our approach relies on the idea that the sparse representation is not constructed over a flat structure of object classes (as in the classic sparse sensing problem) but rather by enforcing that the solution must be one (or a combination of a few) path out of all possible paths on a given hierarchy of object classes (training set). Moreover, classification accuracy is measured in hierarchical sense (that is, by considering the HD between the query path and the ground truth one). Since our method relies on the sparsity of the representation, our approach is suitable for large scale classification problems; i.e., the conditions underlying the sparsity assumptions are best verified when the dataset is large and distribution of visual categories is diversified. In this work we present sufficient conditions under which our hierarchical sparse formulation can be used with success and small error bounds are guaranteed. Furthermore, a crucial property of our classification framework is that it is capable of classifying multiple object categories at the same time if more than one (dominant) object appears in the query image (Fig. 1 (c)).We have carried out extensive quantitative and qualitative experimental evaluation on a number of branches of the Imagenet database [7] as well as Caltech-256 [11]. Each branch comprises hundreds of visual categories organized in the hierarchical structure. All the experiments demonstrate that our hierarchical approximation framework yields much better hierarchical classification accuracy over flat sparse approximation. Evaluation was carried out by comparing average precision measured in terms of HD as well as by measuring the actual classification accuracy at each level of the hierarchy. Our method achieves a performance increase ranging from 5% to 10% for the most critical levels of the hierarchy. Additional experiments on multi-category classification also show very promising results.The rest of this paper is organized as follows. In Section 2, we will briefly review how sparse approximation can be applied to image classification problem. The formal definition of hierarchical classification and our proposed embedding is provided in Section 3. A number of experiments are carried out to validate our scheme in Section 4. Finally, we summarize our work in Section 5.In this section, we describe our image representation and introduce the basic formulation of the flat image classification problem based on sparse approximation. We assume a database of images is available. Furthermore, we assume that such a database comprises a large number of categories and each category has a large number of image instances. We assume that each image has a dominant object instance with some level of background clutter as in Caltech-256 [11] or the ImageNet [7]. In classification, we assume that the query image (with unknown category label) contains one (or multiple) dominant object(s) whose category label is represented by the dataset. Of course, the query object instance itself is not necessarily included in the dataset. The classification problem can be solved by seeking, among all the images (object instances) in the database, the one that is closest to the query object(s). The category such image belongs to is the classification result. If the query image contains multiple dominant objects, the classifier must return multiple category labels associated to all of the dominant objects in the query image.Assessing whether an image is “close” to another one relies on the construction of a distance function which depends on the way how the visual content of an image is represented. Following a common representation used in computer vision, we describe an image using a normalized histogram of codewords (i.e., the bag of words representation, also named BOW) [6] or, equivalently, a histogram capturing a spatial pyramid of codewords [14,10]. In either cases, we denote such histogram by a vector x. Codewords are drawn from a learnt dictionary of vector quantized features as described in [6,14,10]. The size of the dictionary is denoted by K. Thus x is a column vector of size K, if we use a simple histogram of codewords to represent the image. Notice that other types of representations are also possible. The similarity between two images represented by xiand xjcan be measured by computing the lnnorm distance between xiand xj, where n can be 0, 1, etc. Similar images will have small distances.Let us stack all the histograms of images in the database as columns of the matrix H. Thus, H will be K×N, where N is the number of images in the dataset. We call this matrix H the flat model matrix. Under the assumption that the database is sufficiently large, any query image can be represented as a superposition of one or more images in the training data with small error e such that x=Hm+e. Note that N×1 vector m is called the mixing vector and consists of a few non-zero entries associated to the images in the database that contribute to represent the query image by superposition. Note that the error e captures background clutter and the intra-class variability. A similar representation was introduced in [22] and was shown to be suitable for face recognition problems.We argue that is also a reasonable model for the generic object classification problem. As long as the training set is large enough the image representation will yield satisfactorily discriminative features for classifying object classes as demonstrated in [11,14]. In order to further justify the model, we show empirical evidence that mixing vectors m are both fairly sparse and concentrated using a number of datasets in the following Section 2.2.1.In this section, we provide empirical evidence of the assumption that a query image x can be both sparsely and accurately represented by a few linear combinations of BOW descriptors of the same category. The following experimental evaluation is carried out by using the hierarchical Caltech-256 dataset with ‘dog’ category. See Section 4 for more details about the structure of this dataset. Let us denote the K×N matrix Hsas the matrix that is formed by taking the columns in H that correspond to the same category as x. Thus, N is the number of images in a category. Note that, K=4200 and N=30 for this particular dataset and also that K>N. Then, we empirically show that x=Hsms+e has a solutionm^sthat is sparse and gives a small approximation errorx−Hm^s2.To computem^sfor a given x we solve,minmsx−Hsms2+λms1,which is also known as the least absolute shrinkage and selection operator (LASSO) [19]. The first term of the cost function ensures that the approximation error is small and the second term ensures that the solution is sparse. Fig. 2shows an example of a plot ofm^sobtained by solving the above minimization problem. We can see thatm^sis indeed sparse with only two non-zero coefficients and has a small approximation error of 0.58.In order to demonstrate that such behavior is common across most queries x, we repeat the above for 512 queries x that belong to different categories and evaluate how sparse and accurate the solutions are by computingm^s1m^s2andx−Hsm^s2, respectively. We note that1≤ms1ms2≤30and the closer this fraction is to 1 the sparser them^sand vice versa. The average value ofm^s1m^s2andx−Hsm^s2for a large number of trials are 2.41 and 0.52, respectively, which show that x can indeed be sparsely and accurately represented by the columns of the same category.Next we show that for a large number of queries, x is best represented by the columns of the same category than by those of other categories. In the Caltech-256 dataset there are in total 256 categories. For each query x, we solve the above minimization problem for all 256 categories, where each category has a different Hsthat is constructed by taking the appropriate columns in H. Then for all 256 solutions, we evaluatem^s1m^s2andx−Hsm^s2as a measure of sparsity and accuracy. To assess whether or not x is better represented by the columns of the true category, we compute how many other categories resulted in a representationm^sthat gave 10% better performance in terms of the two measures simultaneously. We repeat this procedure for 512 different query images that belong to different categories and plot in Fig. 2 the histogram of the number of categories that resulted in a better representation than the true category. Out of 512 trials for exactly327 query images, the true category was able to better represent x than others. This and the fact that this histogram exhibits a high concentration close to zero shows that for most queries, the true category provides more sparse and accurate representations than other categories.Clearly m contains the information that allows us to estimate the class label of the query image. Therefore, the classification problem (what is the object class?) is recast into the problem of estimating the vector m (where is a non-zero entry?). Furthermore, this formulation allows us to discover multiple dominant object categories in the image. Suppose the image contains three objects as in Fig. 1 (c), then the query image may be expressed as a superposition of s=3 training histograms and the non-zero entries of m will return the 3 classes appearing in x (i.e., dog, human and vacuum). Solving m is challenging because the system is under-determined (N≫K) and has an infinite number of solutions. Because we postulate or seek an s-sparse mixing vector m, we can formulate this problem as a sparse approximation problem and seek to find the sparsest solution that best approximates (in ℓ0 error) the observed instance. Notice that the pseudo-norm ||·||0 counts the number of non-zero entries in a vector.Problem 0min‖m‖0 subject to ‖Hm−x‖2≤ϵ.Unfortunately, the above problem is an NP-hard problem in general (given an arbitrary matrix H and an arbitrary vector x). We can, however, solve this problem in polynomial time with appropriate geometric assumptions on H; if the maximum entry of the matrix |H⁎H-I|, or the coherence11An equivalent definition of μ(H) is the maximum dot-product of different columns of H, μ(H)=maxi≠j|<Hi,Hj>|.μ(H), of the matrix is small, then there are several algorithmic solutions. Let us assume for now that the training set contains the query image x. As proposed by [4,22], one method is to observe that Problem 0 is an optimization problem with a non-convex objective function and that a convex relaxation of this problem yields a problem which can be solved efficiently with standard optimization techniques [5],Problem 1min‖m‖1 subject to ‖Hm−x‖2≤ϵ.A second algorithmic approach is to use a greedy algorithm, one that identifies image instances iteratively, such as Orthogonal Matching Pursuit (OMP). See [20] and the references therein for details on this algorithm. In Section 4.3 we show that the coherence between individual images decreases as a function of their hierarchical distance; thus, while the overall coherence μ(H) is high, with high probability, the coherence between any two images is quite small and OMP can distinguish among these images and choose a representation close to the ground truth.While the model x=Hm+e is reasonable and empirical evidence suggests that it is fairly accurate, it fails to take into account any hierarchical information amongst the classes. Furthermore, the error metrics for typical sparse approximation algorithms [20,18] do not take into account structural relationships amongst the columns of H. Indeed, a small error in the mixing vectorm^s−m2or in the reconstruction of the observation x does not necessarily guarantee hierarchical similarity betweenm^and m. For instance, suppose the ground truth label of a query image is “dog”. Assume two possible classification results are generated: “stapler” and“cat”. These two results would be associated to the same flat classification errorm^−m2if the model in x=Hm+e were employed, whereas the classification error associated to “cat” would be smaller than that associated to “stapler” if the error function was defined in hierarchical sense (Fig. 1).In this section, we assume that object categories are structured in a (rooted, labeled, recursive) tree T that reflects the semantic (parental) relationships among object categories. Note that each node of T contains all of the images representative of the visual category label associated to that node. A schematic illustration of such data structure is given in Figs. 1 and 3. We define T′, the data structure induced by the semantic tree, that contains two types of nodes, category labels and individual column vectors of H (images) (Fig. 3). It encodes the semantic relationship amongst the categories and the assignment of columns of H to those categories, but, unlike the tree T, both categories and individual columns of H make up the nodes. A key contribution of our work is to introduce a suitable encoding matrix E that embeds the flat model matrix H into a hierarchical (tree) model matrix Φ and to show that the resulting hierarchical sparse approximation is solvable and appropriate for classification.The encoding matrix E is constructed so as to map the mixing vector m into an embedded mixing vector ℓ=Em, whose non-zero entries correspond to the paths in T′ from the image to the root of the tree (Fig. 3). More concretely, for C object categories along with N images, we define E to be the (N+C)×N matrix that embeds a column of H and its path to the root in the tree T′. Without loss of generality, we can permute the rows of E so that E has the following structure E=[I LT]Twhere I is the N×N identity matrix and the C×N matrix L consists of the hierarchical labels of each image. Each row of L corresponds to a category and each column to a training image; Li,j=1 if category i is on the path to the root from training image j. Each row encodes which training images are descendants of category j. Note that the length of ℓ is N+C. If we denote E† the pseudo-inverse of E, then we define Φ=HE†.The hierarchical embedding allows to reformulate Problem 1 as a hierarchical sparse approximation problem and find a solution for ℓ given x:Problem 2minl1subject toΦl−x2≤ϵUnlike the original sparse approximation problem, in this problem, the sparsity pattern of the vector ℓ is constrained to lie on a single path (or subtree) of the tree T′. While the embedding Em=ℓ increases the number of non-zeros in ℓ (as compared to that of m), it also enforces a model that these non-zero entries must follow; they must lie on paths from individual columns of H to the root of the tree T′. Because the sparsity of ℓ follows a model and Φ has more columns than rows, this problem has the structure of a model-based compressive sensing problem [1].Problem 2 can be solved efficiently by a greedy algorithm called Tree-OMP[13], which is a special case of the more general algorithm Model-CoSaMP[1], assuming that Φ satisfies a geometric condition, referred to as model-Restricted Isometry Property (model RIP). (See Algorithm 1) Tree-OMP is similar to the OMP algorithm with the additional step that for all non-zero components in the vector ℓ, the algorithm enforces that all the components that correspond to ancestors in the tree are non-zero. This constraint guarantees that the estimated solutionl^corresponds to one (or more) physical path(s) in the tree.Algorithm 1TREE-OMP [13]In this subsection, we show that the hierarchical embedding in Section 3 produces a matrix Φ that, on average, satisfies the model RIP. We also show thatl^, the output of Tree-OMP, is close to the ground truth embedded vector ℓ=Em not only in l2 error, but, more importantly, in HD. These results are summarized in the following theorem. Moreover these results enable the construction of a classification algorithm that we call Sparse Path Selection (SPS) (see Algorithm 2).Algorithm 2Sparse Path Selection (SPS)Given a normalized test image x (‖x‖2=1) which is sd-sparse with background “noise” n, we can solveΦl=x+nfor the embedded mixing vector ℓ withTree-OMP. After T>log(sd) iterations, the output vectorl^has at most Td non-zero entries and satisfiesl−l^2≤2−T+Cn2.In addition, if the noisen2≤Tdη−2−Tis small enough compared to a learnt threshold η (See SPS algorithm), thenHDl^l=0; i.e., we correctly identify all the categories on the ground-truth hierarchical path.First, we note that the embedded vector ℓ=Em follows a model-sparse pattern as defined in [1].If m is a s-sparse vector, then ℓ=Em has a sparse tree structure; that is, it encodes a rooted tree with s leaves.From [1], a signal model Mkis the union of mk canonical k-dimensional subspacesMk=∪m=1mkχmwhere each k-dimensional subspaceχm=yyΩmc=0contains all signals y with support in Ωm. The model Mkis defined by the set of possible k-sparse supportsΩ1,…,Ωmkand, if we restrict ourselves to those sets that are defined by a rooted tree structure, we have a model-sparse signal. Our embedding, by construction, yields such a vector ℓ; it is model k≤sd sparse (where d is the depth of the tree T′).The matrix Φ is well-approximated by an iid (sub-)Gaussian random matrix.We model22In practice, the assignment of labels to training images is deterministic and we have more descendant images for a category the higher in the tree it is. The indexing of the columns is, however, arbitrary so we can order them at random initially and fixed throughout the remainder of the algorithm. A more realistic model is to change the probability p as a function of the depth of the category in the tree. The root has p=1 and a deep category has p close to 0.the label matrix L as an iid random Bernoulli matrix; each entry Li,j=1 with probability p and 0 with probability 1-p. LetE˜=12IL˜TTwhereL˜i,j=1Cp1−pLj,i−pis a centered version of the transpose of L. Observe that, on average,E˜=E†, asEL˜Lj,l=E∑k=1CL˜j,kLk,l=∑k=1C1Cp1−pELk,j−pELk,l=0andEL˜Lj,j=E1Cp1−p∑k=1CLk,j−pLk,j=1Cp1−p∑k=1Cp1−p=1.Then, on average,Φ=HE˜=12HHL˜TTand the entries in the columns corresponding to theHL˜block areHL˜j,l=∑k=1NHj,kL˜k,l=∑k=1NHj,k1Cp1−pLk,l−papproximately iid Gaussian random variables as they are (large) sums of bounded random variables with mean 0.This analysis describes the average behavior of Φ only. Any instance of E† has non-zero entries in the off-diagonal terms. These entries are also bounded random variables, and, hence, the product Φ=HE† consists of entries that are approximately Gaussian random variables.From Lemma 1 and 2, we can conclude that Φ satisfies a model-RIP property [3]. Furthermore, we can use the result in [1] to conclude that after T iterations of Tree-OMP, the outputl^contains at most Td non-zero entries and satisfiesl−l^2≤2−T+Cn2. While the l2 distance between two vectors is meaningful, it does not tell us how close the path(s) corresponding to the vectorl^are compared to the ground-truth vector ℓ, it conflates the paths with the coefficients on those paths. The error bound tells us what the average error inl^is and, as long as it is below our learned threshold,1Td2−T+n2<η, we will not introduce spurious nodes in the path nor miss them and hence,HDl^l=0.After solving Problem 2, we obtain an estimate of the path ℓ in the hierarchical database associated to the query image. However, ℓ cannot be used directly for image classification. Ideally, the sparsest solution of Problem 2 should return a vector of “1” and “0” where the non-zero elements in ℓ allow to estimate the category labels of the query object as well as its parents. Unfortunately, this is not always the case and values between “0” and “1” can be also found because of the estimation noise. To solve this issue, we perform a post processing step. The idea is to introduce a threshold η and interpret it as a positive response any value that is above η (and as negative response, otherwise). Finding this threshold, however, is not trivial as it may vary with different datasets. Thus, in our experiments, we propose to automatically learn these thresholds using a binary MAP estimator trained using a validation set. Such evaluation set is then removed from the dataset so as to avoid contamination during testing. Our entire classification scheme is summarized in the Algorithm 2. We call this algorithm SPS.As discussed in the previous sections, if the input vector x describes an image comprised of s categories, the mixing vector m is an s-sparse vector and the corresponding embedded mixing vector ℓ defines a subtree composed of s paths. Each of these paths is associated to one of the categories in x. (Fig. 1) Thus, solving Problem 2 and obtaining an estimatem^of m allows us to simultaneously discover the presence of multiple categories in the image. Even if this appears to be an appealing property, one critical question must be addressed. How many categories s can we simultaneously handle until the conditions (i.e. sparsity, etc.) underlying the solution of Problem 2 are violated? The bounds in [1] suggest that we need at least O(sd) rows in the histograms, where d is depth of hierarchical tree and Section 4.7 gives some empirical evidence that multiple category classification is possible with these algorithms.

@&#CONCLUSIONS@&#
