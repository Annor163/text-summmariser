@&#MAIN-TITLE@&#
Teaching–learning-based optimization with learning experience of other learners and its application

@&#HIGHLIGHTS@&#
The learning experience of some other learners is introduced into TLBO so as to improve its performance.We design a random learning strategy whatever in the Learner Phase or in the Teacher Phase.18 benchmark functions and two real-world problems are used in experimental study.The results indicate that the proposed algorithm has shown interesting outcomes.

@&#KEYPHRASES@&#
Teaching–learning-base optimization (TLBO),Global optimization,Learning information,Evolutionary algorithms (EAs),

@&#ABSTRACT@&#
To improve the global performance of the standard teaching–learning-based optimization (TLBO) algorithm, an improved TLBO algorithm (LETLBO) with learning experience of other learners is proposed in the paper. In LETLBO, two random possibilities are used to determine the learning methods of learners in different phases. In the Teacher Phase, the learners improve their grades by utilizing the mean information of the class and the learning experience of other learners according to a random probability. In Learner Phase, the learner learns knowledge from another learner which is randomly selected from the whole class or the mutual learning experience of two randomly selected learners according to a random probability. Moreover, area copying operator which is used in Producer–Scrounger model is used for parts of learners to increase its learning speed. The feasibility and effectiveness of the proposed algorithm are tested on 18 benchmark functions and two practical optimization problems. The merits of the improved method are compared with those of some other evolutionary algorithms (EAs), the results show that the proposed algorithm is an effective method for global optimization problems.

@&#INTRODUCTION@&#
In the real world, with the rapid development of technology and science, more and more engineering problems can be modeled as seriously optimization problems. The early works mainly focus on various mathematical techniques, but these methods may not be used efficiently for finding global optima. On the other hand, many intelligent optimization techniques have been developed by mimicking natural phenomena and widely applied as an alternative to traditional techniques in various fields of science. These intelligent optimization techniques have shown promising results for solving complex engineering problems such as structural design [1–5], multi-pass turning operations [6–8] and milling operations [9,10] of the manufacturing industry, and hence have attracted growing research interest of more and more researchers from many research fields.Simulating teaching–learning procedure in a class, the teaching–learning-based optimization algorithm (TLBO) [11,12] has been presented in recent years. It is based on the effect of the influence of a teacher on the output of learners in a class [13]. There are no parameters that should be determined in the updating equations of learners. It is easily implemented and requires less computational memory when comparing with some other evolutionary algorithms. It has been successfully used in function optimizations, engineering optimization problems and some other application fields. As a population-based optimization algorithm, standard TLBO also gets into local optima when solving complex global optimization problems. The current works for TLBO can be classified into three kinds. The first class is modifying the learning way of TLBO to improve its performance. Mutation operator is used in TLBO after the Learner Phase to improve the diversity of the class, and it is used for non-smooth optimal power flow [13,14]. Elitism concept is introduced into the TLBO algorithm (ETLBO) [15] to improve the performance of it and ETLBO is used for multi-level production planning in a petrochemical industry [16]. Dynamic group strategy is introduced into TLBO to improve the performance of it for global optimization problems [17]. In the method, each learner learns knowledge from the mean of his corresponding group, rather than the mean of the class. Local learning and self-learning methods is used in TLBO so as to enhance the search ability of it [18], every individual learns from both the teacher of the current generation and other individuals in Teacher Phase, individuals either renew their positions according to their own gradient information or randomly exploit new positions in self-learning phase. A bi-phase crossover scheme and special local search operators [19] are incorporated into TLBO to balance the exploration and exploitation capabilities and the improved algorithm is utilized for solving flexible job-shop scheduling problem successfully. Variable neighborhood search is introduced into TLBO to improve the global performance of it and it is used for permutation flow shop scheduling problem [20] and optimization of artificial neural network (ANN) [21]. Scale factor is used in TLBO to modify the updating equation of learners [22], and the effectiveness of the improved algorithm is proved by some testing results for benchmark functions. A modified teaching factor and mutation operator are introduced into TLBO to adjust the convergence speed and avoid premature convergence of the original TLBO [23]. The modified algorithm is used for automatic voltage regulators in distribution systems. Orthogonal design with a new selection strategy is applied to decrease the number of generations and make the algorithm converge faster [24]. Producer–scrounger model is proposed by our research group to decrease the computation cost of TLBO [25]. Self-learning is used in TLBO to solve the non-convex non-linear dispatch problems [26]. The idea of group discussion is presented to improve the performance of TLBO [27]. In the method, every learner is assigned to at least one group and the grades of them are improved by the group leaders. A new self-learning phase is added in TLBO to complement the Learner Phase to motivate the learners to gain knowledge by themselves [28]. The second class is that TLBO combines with some other evolutionary optimization algorithms. Artificial bee colony algorithm (ABC) is combined with TLBO to predict the berm geometry with a set of laboratory tests [29]. Double differential evolution (DDE) algorithm is combined with TLBO is used to handle the ORPD problem [30]. In the method, the students are fed to DDE algorithm if they cannot get to better positions whatever in Teacher Phase and Learner Phase. Rough set theory is used in TLBO for feature selection [31]. A novel hybrid optimization approach based on teaching–learning based optimization (TLBO) algorithm and Taguchi's method is used to develop a new optimization approach for multi-pass turning operations in the manufacturing area [32]. The third class is to extend the application fields of TLBO. TLBO is applied for the optimization of plate-fin heat exchangers [33] and modern machining processes [34]. A simplified teaching–learning-based optimization is presented for solving disassembly sequence planning problem [35]. Multi-objective optimal power flow (MOOPF) problems while satisfying various operational constraints are solved by TLBO [36]. TLBO is improved and used for solving short-term hydrothermal scheduling (HTS) problems in practical power system [37]. Self-motivated learning is introduced into TLBO for multi-objective optimization of a two-stage thermoelectric cooler [38]. Contrast pyramid (CP) with teaching learning based optimization (TLBO) is designed to solve image fusion problem for visible and infrared images under different spectrum of complicated scene [39]. In additional, some TLBO variants are also used for flow-hop rescheduling problems [40,41].As a population-based evolutionary algorithm, the learners in TLBO learn knowledge from the teacher of the whole class in Teacher Phase and another learner which is randomly chosen from the class in Learner Phase. It emphasizes that all learners learn knowledge from the global scene. The learners don’t borrow the learning knowledge of the others whatever in Teacher Phase or Learner Phase. Generally, in the real world, the individuals are living around a certain social neighborhood, and those individuals tend to imitate their behavior on the other individuals’ behavior from the same neighborhood. In fact, each learner is always learning from his neighbors. Hence, learning experience of other learners can be introduced into TLBO to improve the learning efficiency of learners. Motivated by this consideration, in this paper, an improved TLBO algorithm with the learning experience of other learners (LETLBO) is presented to improve the global performance of TLBO. In the LETLBO algorithm, parts of learners learn knowledge from the teacher and the remainders learn knowledge with the learning experience of the others. The diversity of the improved algorithm is improved without large destroy the mean solution of the whole class.The remainders of the paper are organized as follow. The standard TLBO is briefly introduced in Section 2. The LETLBO is described in Section 3. Section 4 presents the experiments, and some conclusions are given in Section 5.TLBO is a recently proposed population-based evolutionary optimization algorithm, and it is mainly based on the effect of influence of a teacher on the learners in a class. In addition to the common control parameters like population size, the number of generations, there is no specific paremeters that should be needed in TLBO. It has been successfully used for some optimization problems. There are two important phases in TLBO: the one is Teacher Phase and the other is Learner Phase. The best solution in the entire population is considered as the teacher of the class, the teacher shares his or her knowledge with the students to improve their outputs (i.e., grades or marks), the quality of the teacher affects the outcomes of the learners. The learners also learn knowledge from other students to improve their outputs in Learner Phase. The detail description of TLBO can be found in Ref. [11]. In this part, only the two important phases of TLBO are introduced.In Teacher Phase, the best learner is chosen as the teacher according to the fitness value and the mean position of all learners should be calculated for updating the positions of all learners. Assume Xi={xi,1,xi,2, …, xi,n} is the position of the ith learner for an n-dimensional optimization problem, the mean position of the current class is expressed by XMeanand the best position of current teacher is noted as XTeacher. Teacher Phase is formulated as follows:(1)Xnew,i=Xold,i+rand(⋅)*(XTeacher−TF*XMean)where Xnew,iand Xold,iare the new and old positions of ith learner, XTeacheris the position of the current teacher, rand(·) is the random number within the range [0,1]. All learners should be re-evaluated after each iteration of Teacher Phase, If Xnew,iis better than Xold,i, Xnew,iwill be accepted and flowed to Learner Phase, otherwise Xold,iis not changed. TF is the teaching factor, and its value is heuristically set to either 1 or 2. It is determined by Eq. (2).(2)TF=round[1+rand(0,1){2−1}]For the ith learner in Learner Phase, another learner which is different from it will be randomly selected from the class. The learning process is described as follows.Assume that the jth learner is randomly selected from the class, the operation of learner is shown in Eq. (3).(3)Xnew,i=Xold,i+rand(⋅)(Xold,i−Xold,j)iff(Xold,i)<f(Xold,j)Xold,i+rand(⋅)(Xold,j−Xold,i)otherwiseAccept Xnew,iif it is better than Xold,i. Where, Xnew,iis the new position of ith learner, Xold,iand Xold,jare the old positions of ith and jth learners. rand(·) is the random number in the range [0,1]. The pseudo-code for standard TLBO can be shown in Algorithm 1.Algorithm 1 TLBO(·)1Begin2Initialize N(number of learners) and D(number of dimensions);3Initialize learners and evaluate them;4Choose the best learner as XTeacherand calculate the mean XMeanof all learners;5while(stopping condition is not met);6for all learners7TF=round(1+rand(0,1));8Update all learners according to Eq. (1);9end for10Evaluated the new learners;11Accept the new solutions if it is better than the old one;12for all learners13Randomly select another learner which is different from it;14Update the learners according to Eq. (3);15end for16Accept the new solution if it is better than the old one;17Update the teacher and the mean;18end while19endThere are some methods by modifying the updating process of learners to improve the global performance of TLBO, but there is no method that introduces experience of other learners so far. In this section, an improved TLBO with learning experience of others (LETLBO) is proposed. The flowchart of the LETLBO algorithm is given in Fig. 1. The main parts of the improved algorithm (LETLBO) are described as follows.The main motivation of our method is using learning experience of other learners to improve the grades or scores of learners, thus improving the global performance of TLBO. It can be seen that, the searching process of TLBO is greedy. The better individual is followed to the next generation whatever in Teacher Phase or Learner Phase, the diversity of learner in TLBO might be decreased with increasing of evolution generation. In the teaching–learning process of a real classroom, learners can borrowed the learning experience to improve himself/herself grades or scores through mutual exchanges and discussions between them. In the paper, learning experience of other learners are introduced into TLBO to maintain the diversity of population, thus improving the global optimization performance for solving complex problems.In Teacher Phase of LETLBO, the learners are probabilistically learning by means of the TLBO learning strategy or the learning method with the experience of others. That is, Teacher Phase of LETLBO has the option of redefining learning rule at each generation through the ‘if–then–else’ rule. The random probability is used to determine the ratio of learners which adopt learning method from the experience of others. The larger probability will make the number of learners which using learning knowledge from the experience of others decrease. The detailed process of Teacher Phase of LETLBO is shown as follows.Generally, for the ith learner in Teacher Phase, if a<b (a, b∼rand(·)), the updating Eq. (1) is used for the learner. If the new position of the ith learner is better than that of the old one, the new position will be accepted.On the contrary, if a>b (a, b∼rand(·)), then another learner (assume it is the jth learner) is randomly selected from the whole class. If the jth learner is better than the ith one, the ith learner will update its position with using the learning experience of jth learner. The learning method is presented in Eq. (4).(4)Xnew,i=Xold,i+rand(⋅)*(XTeacher−Xold,j)Otherwise, area copying operator which is used in Producer–Scrounger model [42] is used by the learner, and the process can be described in Eq. (5).(5)Xnew,i=Xold,i+rand(⋅)*(XTeacher−Xold,i)where rand(·) is the random number in the range [0,1]. The second part of Eq. (4) is the learning experience from the teacher of the jth learner. Xnew,iand Xold,iare the new and the old positions of ith learner, respectively. Xold,jis old position of jth learner.The pseudo-code of Teacher Phase in LETLBO can be summarized as Algorithm 2.Algorithm 2Teacher Phase of LETLBO1Begin% Teacher Phase2fori=1:N (N is the population size)3ifa<b (a, b∼rand(·))4TF=round(1+rand(0,1));5Update the learner according to Eq. (1);6else7Randomly select the jth learner which is different from the ith learner8ifjth learner is better than the ith learner9Update the learner according to Eq. (4);10else11Update the learner according to Eq. (5);12endif13end if14Accept Xnew,iif it is better than Xold,i15endfor16endIn Learner Phase of LETLBO, the learners are also probabilistically learning by means of the TLBO learning strategy or the learning method with the experience of others. In the method, some learners learn knowledge from a randomly selected learner from the whole class as it is used in Eq. (3), and other remainders will learn knowledege from the mutual learning experience of the randomly selected two learners from the class.Generally, for the ith learner, if a<b (a, b∼rand(·)), the updating Eq. (3) is used for the learners. If the new position of the ith learner is better than the old one, the new position will be accepted. On the contrary, if a>b (a, b∼rand(·)), then two learners which are different from the ith learner are selected for updating the position of ith learner. If the kth learner is better than the lth learner, the ith learner will be updated according to Eq. (6); otherwise, the ith learner will be updated according to Eq. (7).(6)Xnew,i=Xold,i+rand(⋅)*(Xold,k−Xold,l)(7)Xnew,i=Xold,i+rand(⋅)*(Xold,l−Xold,k)where, rand(·) is the random number in the range [0,1]. Xnew,iand Xold,iare the new and the old positions of the ith learner, respectively. Xold,kand Xold,lare the positions of the kth and lth learners which are randomly selected from the class.The pseudo-code of Learner Phase in LETLBO is shown in Algorithm 3.Algorithm 3Learner Phase of LETLBO1Begin% Learner Phase2fori=1:N (N is the population size)3ifa<b (a, b∼rand(·))4Update the learner according to Eq. (3);5else6Randomly select the kth and the jth learner which is different from the ith learner7if the kth learner is better than the jth learner8Update the learner according to Eq. (6);9else10Update the learner according to Eq. (7);11endif12end if13Accept Xnew,iif it is better than Xold,i14endfor15endFurther experimental tests with benchmark functions are carried out in this section to validate the proposed LETLBO Algorithm. To compare the performance of LETLBO on benchmark functions, some existing optimization algorithms are examined through our experiments and they are described in detail as follows.FIPSO is a fully informed particle swarm optimization algorithm proposed by Rui et al. [43]. FIPSO makes use of the information from all other particles around it, which is conceptually more concise and promises to perform better than the traditional particle swarm algorithm. FDR-PSO is a new version of the PSO algorithm proposed by Peram et al. [44]. FDR-PSO is developed to improve the global performance of PSO by extending the updating equation of particles so that particles are also studied from the neighbors in comparison to the best particle in current generation and the best position at which they have arrived so far. jDE is a new version of the DE algorithm proposed by Brest et al. [45]. In this method, control parameters F and Cr are encoded into the individual and adjusted them by introducing two probability parameters. SaDE is a self-adaptive DE algorithm proposed by Qin et al. [46]. In this method, both trial vector generation strategies and their associated control parameter values are gradually self-adapted by learning from their previous experiences in generating promising solutions.Eighteen benchmark functions listed in Table 1are used for the experimental tests here. Functions f1 to f5 are unimodal functions, f6 to f10 are multimodal functions, and f11 to f18 are rotated models of f3 to f10. To compare the performance of LETLBO on these benchmark functions, FIPSO, FDR-PSO, jDE, SaDE, TLBO, LETLBO are also simulated in the paper and the results of different methods are compared.In our simulations, each function was independently simulated 30 runs to reduce the statistical errors; the mean results are used for comparison. 30-D (D being the dimensionality of the problem) and 50-D (D being the dimensionality of the problem) functions were, respectively, simulated with different algorithms. The “Range” in Table 1 is the lower and upper bounds of the variables. “fmin” is the theoretical global minimum solution, and “Acceptance” is the acceptable solutions. The parameters of other algorithms were taken from the relevant literature except for the maximal function evaluations (FEs) and population size. The maximal function evaluations (FEs) is 300,000, the population size is 50. The function values are used as the fitness function of all algorithms.The mean optimum solutions and standard deviations of 30 runs of the algorithms for 30-D and 50-D functions over 300,000 FEs are shown in Tables 2 and 3. The best solutions are shown in bold. Tables 2 and 3 show that LETLBO has better performance than some other algorithms for large part functions. The mean solutions of LETLBO are the best for 14 30-D functions and 15 for 50-D functions among those seven algorithms. Table 2 indicates that LETLBO can converge to the theoretical optimal solutions for functions f1, f2, f3, f7, f8, f9, f11, f15, f16 and f17. For function f1, f3, f8, f9, f11, f16 and f17, the two merits of LETLBO are the same as those of TLBO and ETLBO. For functions f8, f9 and f17, jDE, SaDE, TLBO, ETLBO and LETLBO have the same means and standard deviations. For functions f5, f10, f13 and f18, the mean best solutions for jDE are better than those of other algorithms. For function f13, the standard deviations of FIPSO are the smallest among all algorithms. Table 3 displays that LETLBO can converge to the theoretical optimal solutions for functions f1, f2, f3, f7, f8, f9, f11, f15, f16 and f17. For functions f1, f3, f8, f9, f11, f14, f16 and f17, TLBO, ETLBO and LETLBO have the same performance in terms of the mean solutions and standard deviations. For functions f5, the mean solution of FDR-PSO is the smallest and the standard deviation of FIPSO is the smallest. jDE and LETLBO have the same merits for function f7. jDE and SaDE have the same best merits for function f10. jDE has the best performance for function f13. The excellent and good rates of LETLBO for 30-D and 50-D functions are 77.78%. It is high than other six algorithms in the paper.The mean number of FEs (mFEs) is often used to measure the convergence speed of the algorithms. When the algorithms can converge to the acceptable solutions, the mFEs of the algorithms for 30-D and 50-D functions are shown in Tables 4 and 5. In the tables, “mFEs” is the mean function evolutions of the algorithms when they converge to the acceptable solutions in Table 1. “NaN” indicates that the algorithm cannot converge to the acceptable solutions in all 30 runs. The best results are shown with boldfaces.Table 4 displays that the LETLBO has the smallest mFEs 13 functions and it can arrive the acceptable solutions with 100% ratios for 14 functions. ETLBO shows the good performance in terms of the mFEs for function f7. jDE and SaDE have the high convergence ratios for function f10, and the mFEs of jDE is the smallest among the algorithms. For function f18, SaDE has the smallest mFEs.Table 5 again shows that LETLBO generally requires the fewest mFEs to attain acceptable solutions for 50-D functions. The mean successful ratios of LETLBO are high than it for 30-D functions, it shows the best performance in terms of mFEs and ratios for 16 functions. jDE has the smallest mFEs for function f10, and TLBO has the smallest mFEs for function f18. The successful ratios of jDE is the highest among seven algorithms, it can converge to the acceptable solutions for 17 functions.According to the theorem of ‘no free lunch’ [47], one algorithm cannot offer better performance than all the others on every metric or on every kind of problem. This can be observed in our experimental results. For example, the mFEs of LETLBO is the smallest among all algorithms, but the successful ratios of it is lower than that of jDE. To show the convergence process of the algorithms more directly, the convergence figures of four functions shown in Fig. 2. Fig. 2 indicates that the convergence speed of LETLBO algorithms is quicker than that of the other algorithms for these examples.For a thorough comparison, the t-test [48] is carried out. The t values and the p values on every function of this two-tailed test with a significant level of 0.05 between LETLBO and another algorithm are shown in Tables 6 and 7. Rows “B”, “W”, and “S” represent the number of functions that LETLBO performs significantly better than, significantly worse than and almost the same as the compared algorithm, respectively. Row “R’ is the difference of “B” and “W”. The two tables show that the performance of LETLBO is general better than the other algorithms. Table 6 shows that the mean excellent ratio of LETLBO is 61.11% for 30-D functions. Table 7 indicates that the value 64.81% for 50-D functions. The better solutions are signed by bold words in the two tables.Proportional-integral-derivative (PID) controllers [49–51] have been widely used in industrial processes for its simple structure and easy to be implemented. In real system, the plant may have some features, such as nonlinearity, time variability, and time delay. The performance of PID controllers is usually determined by the parameters, so the main issue for PID controllers is how to accurately and efficiently tune the parameters [51]. Some evolutionary computation algorithms have been utilized to tune the parameters of PID controller. To test the performance of LETLBO, PID controller is also designed in this paper. The model of the plant is come from Ref. [49].A typical PID control system can be shown in Fig. 2, where r(t) and y(t) are the reference signal and the system output, respectively. The continuous PID controller can be described as follows:(8)u(t)=Kpe(t)+KI∫0te(t)dt+KDde(t)dtwhere u(t) is output of the controller, e(t) is the error between r(t) and y(t), KP, KIand KDare the proportional, integral, and derivate parameters of PID controller, respectively. The discrete-type of PID control is shown in Eq. (9).(9)u(k)=Kpe(K)+KI∑j=1ke(j)Δt+KDe(k)−e(k−1)Δtwhere k represents the time step, Δt is the sampling period.The fitness function [51] of all algorithms is shown in Eq. (10).(10)J(t)=∫0∞[w1e(t)+w2u2(t)]dt+w3tr,ifΔy(t)≥0∫0∞[w1e(t)+w2u2(t)+w4Δy(t)]dt+w3tr,ifΔy(t)<0where Δy(t)=y(t)−y(t−1) and wi, i=1, 2, 3, 4, are weight coefficients.To demonstrate the search performance of LETLBO algorithm in the tuning of the PID parameters, the following plant is used [52], and FDR-PSO, jDE, TLBO, ETLBO are also simulated for comparison.(11)G(s)=16s2+2.584s+16The system sampling time is 0.05 second and the control value u is limited in the range of [−10,10]. Other relevant system variables are Kp∈[0,20], KI∈[0,20], KI∈[0,10]. The weight coefficients of the cost function are set as w1=1.2, w2=0.01, w3=2.5, and w4=100. The training and testing results are shown in Table 8. In the table, KP, KIand KDare the best solutions of 30 runs, the solutions in terms of “overshoot”, “peak time” and “rise time” are the indexes correspond to the three best parameters. “cost function” and “CPU time” are the average solutions of 30 runs. Table 8 indicates that the “overshoot” and the “cost function” of LETLBO is the smallest. FIPSO has the best performance in terms of “peak time” and “rise time”. The average time of CPU with jDE algorithm is the smallest. The changing process of average fitness is shown in Fig. 3. The step responsing curve is shown in Fig. 4. To clearly display the responsing process, only the results of four algorithms (jDE, FIPSO, TLBO, LETLBO) are given. The figures show that LETLBO has good performance for designing the PID controller.This optimization problem is to optimize the gear ratio for a compound gear train that contains three gears [53]. It is to be designed that the gear ratio is as close as possible to 1/6.931. The number of teeth must be between 12 and 60 for each gear. The mathematical model of the problem can be described as follows:(12)f(x)=1/6.931−x1⋅x2x3⋅x4xi∈[12,60],i=1,2,3,4For the algorithms, the problem is a 4-D optimization problem. All the algorithms involved in the previous experiments were tested on the problem. The training parameters of the algorithms were the same as those used in the previous experiments except that the population size is 30 and the maximal function evolutions (FES) is 20,000. The statistical results over 30 runs are shown in Table 9. The changing process of average best fitness is shown in Fig. 5. Table 9 displays that PDRPSO has the best performance in terms of the minimal solution, the average solution and standard deviation amongst these seven algorithms. The results also indicate that LETLBO also has good performance comparing to some other algorithms except for jDE and FDRPSO.

@&#CONCLUSIONS@&#
This paper has investigated a new LETLBO algorithm that can enable the learners adaptively learn knowledge from experience information of some other learners whatever in Learner Phase or Teacher Phase. In LETLBO, the learners update their positions with different methods according to two random probabilities.The simulation experiments on some test problems were carried out in this paper. From the comparison results on all the test problems, some conclusions can be drawn for the LETLBO algorithm. First of all, LETLBO provides more learning method for the learners, the different learning methods according to a random probability might increase the speed for finding the optimal solutions. Second, LETLBO significantly enhances the performance of TLBO in terms of the performance metrics used in the paper, and it also outperforms than some other peer algorithms in some respect. Third, LETLBO is also used to solve the two real world applications with high performance in the paper though it is not the best for all metrics.We cannot expect the improved algorithm (LETLBO) to solve every global optimizaiton problems. This is also indicated in the results of simulation experiments. The main goal is to improve the performance of TLBO by using the advantages of it. In the future work, we will focus on improving the performance of TLBO with balancing the diversity and the mean solution of the population.