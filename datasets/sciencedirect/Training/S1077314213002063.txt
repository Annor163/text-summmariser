@&#MAIN-TITLE@&#
Facial expression recognition based on anatomy

@&#HIGHLIGHTS@&#
A set of anatomy based features are proposed for facial expression recognition.We decompose a facial action into a unique set of muscle activity scores.We tested these features on CK+ database through three classifiers.Our results are close to human performance in expression recognition.Muscle-based features allow exploration of unknown mechanisms of expressions.

@&#KEYPHRASES@&#
Facial anatomy,Muscle force,Features,Facial action coding system,

@&#ABSTRACT@&#
In this study, we propose a novel approach to facial expression recognition that capitalizes on the anatomical structure of the human face. We model human face with a high-polygon wireframe model that embeds all major muscles. Influence regions of facial muscles are estimated through a semi-automatic customization process. These regions are projected to the image plane to determine feature points. Relative displacement of each feature point between two image frames is treated as an evidence of muscular activity. Feature point displacements are projected back to the 3D space to estimate the new coordinates of the wireframe vertices. Muscular activities that would produce the estimated deformation are solved through a least squares algorithm. We demonstrate the representative power of muscle force based features on three classifiers; NB, SVM and Adaboost. Ability to extract muscle forces that compose a facial expression will enable detection of subtle expressions, replicating an expression on animated characters and exploration of psychologically unknown mechanisms of facial expressions.

@&#INTRODUCTION@&#
Emotions are often conveyed through the gestures of face and body rather than verbal communication. A pronounced feeling may be countered by a facial expression, emphasizing the feeling conveyed by the expression through sarcasm. Mehrabian [1] points out an irrefutable example; it is possible to verbally express hatred and pass on exactly the opposite feeling. In his later works he goes one step further and claims that the feeling or attitude conveyed by a speaker is 55% facial, 38% vocal and only 7% verbal [2].Many forms of non-verbal behavior such as physical contact, distance from the listener and posture influence the message to be conveyed. Nevertheless, it is true that we mostly stare on the face of a speaker to infer the exact feeling in uttered words. In perception, visual component frequently dominates the auditory component even when the auditory information is clear; sometimes changing what we believe to hear. An intriguing illustration of this phenomenon is the McGurk effect [3]. It has been demonstrated that what is heard by the audience on a spoken syllable can be influenced by the displayed video of speech. In McGurk’s experiment the visual stimulus is so overwhelming that subjects believe to hear what they see.As technology replaced human professionals with machines and established new channels of communication between humans, we were forced to abandon naturally evolved means of conveying our emotions, i.e. non-verbal communication. The lack of non-verbal communication degrades the performance of many implementations such as intelligent tutoring systems. It has been demonstrated that increased number of modalities in computer–human communication has a positive impact on learners’ perception of the learning experience [4].Social intelligence in computing, i.e. ability to interpret a human user’s social signals and behaviors, is an emerging field that has many uses [5,6]. Identification of states of interest, boredom, confusion and stress can lead to new leaps in the field of human–computer interaction. Detection of fatigue or inattention has important implications for human operators who handle critical assignments. It has been reported that 80% of the crashes are related to driver inattention [7]. In recent studies up to 95% accuracy is reported in detecting driver drowsiness based mostly on facial behavior [8,9].Current research on facial expression analysis revolves around recognition of six globally common facial expressions by use of geometric and appearance based features. These basic expressions are happiness, sadness, surprise, fear, anger and disgust, as defined by Ekman in 1976 [10]. Appearance based methods process the texture of the face, reducing its complexity and making it suitable for comparison with templates. Choosing the right preprocessing methods and the extent of dimensionality reduction is critical in appearance based methods.Geometric features are derived from distinctive regions of the face, such as eyes, nostrils and mouth. The chosen set of feature points can be tracked, and classified with neural networks (NNs) [11], hidden Markov models (HMMs) [12], AdaBoost [13] along with many other classification algorithms. Choosing the right set of geometric features for description of facial expressions remains an open question. It is unfeasible to simply choose all pixels on a face image due to high dimensionality of data and limited number of examples. On the other hand, we need to choose a minimal set of base functions that would span all possible facial expressions. Facial action coding system (FACS) provides a good and intuitive set of such features called action units (AUs) [14].Action units are derived by human experts based on experiments in psychology. Studies that use FACS approach have reported classification accuracies that range from 70% to 95% on six basic facial expressions [15–17]. The best performances achieved so far are well above the recognition ceiling of humans, which is estimated to be in the range 87.0% [18] to 91.7% [10].These figures are very promising, but most algorithms fail to perform as successfully on novel examples, such as expressions of unseen subjects. For that reason facial expression recognition is still not a solved problem in a real world scenario, except for simple expressions such as smile. In this paper, we are investigating possible setbacks in current approach to facial expression recognition and propose a new set of features that have one to one correspondence to facial anatomy.We start our discussion in Section 2 with current research on facial expression recognition, emphasizing major hurdles in this field. In Section 3 we will elucidate the inherent limitations of FACS based approaches, addressing how we propose to eliminate these limitations. Section 4 will introduce a set of novel features for facial expression detection. We will report our results in Section 5 and conclude this report in Section 6.The process of facial expression recognition commonly commences with detection of the face region in an input image. When the input is a video, the face or features detected in the first frame are tracked throughout the rest of the frames. Detection of faces is considered a solved problem for less cluttered scenes, by use of wavelet coefficient histograms and their positions [19], NNs [20] and rectangle features with Adaboost [21].It is unfeasible to classify expressions through pixel-wise comparison between detected face images due to the dimensionality of data and limited number of examples. In order to reduce the complexity of classification, feature detection or extraction becomes a necessity. An intuitive method to identify features is to locate distinctive regions on the face image such as brows, eye corners, nostrils, and lip corners. These regions can be detected using specialized feature detectors [22,23]. Gabor filters are frequently used to detect edges in varying scales and orientations [24,25]. Face alignment and registration methods offer global solutions to feature detection problem [26–31]. Although face detectors are remarkably successful, detection of facial features is still an open problem.When the input to the expression recognition system is a video, it is possible to make use of the temporal dynamics of features by tracking them. Tracking can be done relying upon the similarity of texture in susequent frames [32–34].The primary focus of facial expression recognition systems is to classify the displacement of features in the progress of video or their deviation from a defined template of neutral face. SVMs [35,36] have shown to be effective for modeling the non-Gaussian distribution of classes. Michel and Kaliouby [37] utilized displacements of 22 feature points to train a SVM classifier and achieved 72% accuracy in person independent classification. Littlewort et al. [13] used Adaboost to select a subset of Gabor filters, trained SVM on this subset and classifed FACS AUs.Model based approaches offer an alternative by defining a parametric deformable model of the human face. Active Shape Model (ASM) [38] is a statistical method that derives the variation modes of a shape through a set of training examples. Transformation and shape parameters are estimated iteratively to fit the mean shape to the observed object. Active Appearance Model (AAM) [39,40] integrates texture in the derived statistical model. The parameterized model is projected onto the face image and iteratively modified for a better fit. It was shown that the training stage can be sped up using spherical representation of 3D feature vectors and rotation invariant kernels [41]. Matthews and Baker [42] proposed a computationally efficient algorithm with high convergence rate to improve the fitting performance. AAM algorithm can also be applied on video for improved accuracy [43].Model based trackers make use of prior information on the face, by integrating its shape, orientation and features in a holistic face model. One of the earliest facial expression recognition studies using ASM was carried out by Lanitis et al. [44]. Researchers reported 74% accuracy over six basic emotions and neutral expression on a dataset of 118 test images. Ahlberg [45], and Dornaika and Davoine [46] demonstrated tracking of head orientation and facial features through AAM. Sung et al. [47] combined AAM with cylinder head models for an initial estimation of global motion parameters, achieving significantly improved tracking rates. Huang et al. [48] incorporated temporal filters in AAM to cover large pose deviations and fast motion in face tracking. Cheon and Kim [49] utilized differential AAM features and manifold learning to categorize six basic emotions and reported 86.5% recognition performance.Shape, appearance and model-based features can be exploited to extract metadata that is more expressive of facial expressions. FACS takes extraction of metadata one step further with 46 predefined action bases for expressions. Although it was not originally intended for automated classification of expressions, FACS provides one of the most frequently used sets of features in the literature [50–54].FACS AUs have been shown to be features with high discrimination power in a multitude of studies. Sebe et al. [17] defined motion units that are similar to AUs but continuous in nature. They achieved 93.0% accuracy in classification of six basic expressions and neutral on the Cohn–Kanade (CK) [55] dataset. Kotsia et al. [56] chose 17 action units, achieving up to 92.3% accuracy on the same set of expressions and dataset. Lucey et al. [16] reported up to 83.3% accuracy in classification of seven emotions (including contempt) on the extended Cohn–Kanade (CK+) [16] dataset.In this paper we report a hybrid 2D appearance and 3D model based approach for extracting facial muscle forces, which are proposed as novel features. We use a computationally efficient, non-iterative method to customize a generic face model to the face image on the first frame of the video. Vertices on the customized face model that fall in a muscular region of influence are projected onto the face image. These projections are initialized as feature points that are to be tracked based on appearance in the subsequent video frames. Coordinates of the tracked feature points on the image plane are projected back onto the customized model to estimate 3D deformations due to facial expressions. We model the skin as a system of springs and solve muscle forces in a linear, over-determined system of equations.

@&#CONCLUSIONS@&#
