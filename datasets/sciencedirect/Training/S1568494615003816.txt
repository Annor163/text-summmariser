@&#MAIN-TITLE@&#
Hybrid differential evolution algorithm for optimal clustering

@&#HIGHLIGHTS@&#
New hybrid algorithm for optimal clustering is proposed.Algorithm combines differential evolution with k-means.Efficient heuristic for rearrangement of cluster centers is proposed.Hybrid algorithm with center rearrangement is much faster than other variants.

@&#KEYPHRASES@&#
Optimal clustering,Adaptive differential evolution,k-means algorithm,Hybrid search,Heuristic rearrangement of cluster centers,Numerical comparison,

@&#ABSTRACT@&#
The problem of optimal non-hierarchical clustering is addressed. A new algorithm combining differential evolution and k-means is proposed and tested on eight well-known real-world data sets. Two criteria (clustering validity indexes), namely TRW and VCR, were used in the optimization of classification. The classification of objects to be optimized is encoded by the cluster centers in differential evolution (DE) algorithm. It induced the problem of rearrangement of centers in the population to ensure an efficient search via application of evolutionary operators. A new efficient heuristic for this rearrangement was also proposed. The plain DE variants with and without the rearrangement were compared with corresponding hybrid k-means variants. The experimental results showed that hybrid variants with k-means algorithm are essentially more efficient than the non-hybrid ones. Compared to a standard k-means algorithm with restart, the new hybrid algorithm was found more reliable and more efficient, especially in difficult tasks. The results for TRW and VCR criterion were compared. Both criteria provided the same optimal partitions and no significant differences were found in efficiency of the algorithms using these criteria.

@&#INTRODUCTION@&#
Cluster analysis (or clustering) is an important exploratory technique used for splitting a collection of objects into relatively homogeneous groups (called clusters) based on object similarities. Clustering is completely data driven process denoted as unsupervised machine learning.Clustering problem can be defined formally as follows. LetObe a set of n objects to be grouped. The aim of the clustering algorithm is to find such a partitionC={C1,C2,…,Ck}fulfilling the conditions:1Cl≠∅foralll=1,2,…,kCl∩Cm=∅alll≠m∪l=1kCl=Oobjects belonging to the same cluster are as similar to each other as possible, while the objects belonging to different clusters are as dissimilar as possible.Each partition fulfilling conditions 1–3 is called feasible partition. The count of different feasible partitions is given by Stirling number of the second kind S(n, k)(1)S(n,k)=1k!∑l=0k(−1)lkk−l(k−l)n=1k!∑l=1k(−1)k−lklln.Let us suppose that each object ofOis characterized by p real-valued attributes. Data matrixZof size n×p is composed of n row vectorszi, where each element zijrepresents the j-th real-valued attribute of the i-th object. Then each feasible partition can be evaluated by a function (criterion) that quantifies the goodness of the partition based on the similarity or dissimilarity of the objects. The function reflects the vague condition 4. The solution of clustering problem is to find such a partition that optimizes the function.It is known that the clustering problem is hard, Brucker [1] has shown that the clustering problem is NP-hard when k>3. The count of feasible partitions is very large, e.g. for popular test problems like Iris is S(150, 3)≈6×1070 and for Glass even S(214, 6)≈4×10165. This implies that exhaustive search is not applicable in the solution of clustering problem. Thus, heuristics have to be used. There are two main approaches how to solve the clustering problem, hierarchical and non-hierarchical. The non-hierarchical (partition) clustering algorithms try to decompose the data sets directly into a set of disjoint clusters via optimizing a chosen function. Such functions are also called the clustering validity indices or criteria.Because of difficulty of the clustering problem, the historically first clustering algorithms inclusive of the most popular k-means algorithm are iterative. Their weakness consists in the fact that the solution found by them is often suboptimal only. The iterative clustering algorithms are described in many textbooks, for an overview see, e.g. [2]. Several important iterative clustering algorithms are experimentally compared in Hamerly and Elkan [3].The goal of our paper is to propose a novel hybrid algorithm combining differential evolution and k-means algorithm and apply it to non-hierarchical clustering. The improved hybrid algorithm is derived from our results [4,5], where a preliminary versions of hybridization were introduced and proved to be more efficient than non-hybrid differential evolution.The rest of the paper is organized as follows. Applications of evolutionary algorithms to clustering are surveyed briefly in Section 2. Two criteria (clustering validity indices) used in experimental comparison are defined in Section 3. The basic frame of DE algorithm is described in Section 4 and encoding of partition for DE in Section 5. New heuristic rearrangement of cluster centers is proposed in Section 6. Section 7 presents the proposed hybrid algorithm in detail. The organization of experiments and experimental setting are described in Sections 8–10. The experimental results are shown and discussed in Section 11 and the paper is closed by concluding remarks in Section 12.Application of evolutionary algorithms or swarm intelligence to optimal clustering seems to be a natural choice how to solve this algorithmically difficult clustering problem. The paper by Paterlini and Krink [6] can be considered a pioneering work in this field. They compared the performance of floating point encoded genetic algorithm (GA), differential evolution (DE), and particle swarm optimization (PSO). The experimental results showed that DE outperformed the other algorithms in the comparison, which is in accordance with [7], where DE was also the best performing in experimental comparison with GA and PSO on a set of standard benchmark optimization problems.Das et al. [8] applied an improved DE to clustering including the search of optimal number of clusters. DE was also used in fuzzy-clustering algorithm [9]. Appropriate setting of DE control parameters for clustering problems (especially parameter controlling the exponential crossover) was also studied in [10]. A comprehensive survey of evolutionary algorithms for clustering up to 2009 was published by Hruschka et al. [11].Hybridization of DE by using k-means algorithm for a local search appeared in several papers. Very similar approach was suggested independently in [12] and [4]. A hybrid DE variant with a special rearrangement of the rank of cluster centers after completing each generation [5] proved to be substantially more efficient than standard DE. The efficiency of k-means hybridization in evolutionary algorithms was studied by Naldi et al. [13] and found to be beneficial. Karaboga and Ozturk [14] used Artificial Bee Colony (ABC) algorithm in classification of objects and compared it with PSO and other classification algorithms on 13 test problems. They found that ABC can be successfully applied to clustering for the purpose of classification. Abraham et al. [15] provided a very comprehensive survey on swarm intelligence algorithms for data clustering. A survey of evolutionary and swarm intelligence algorithms for clustering can be also found in [16] and [17].There are several optimizing criteria convenient for comparing the degree of optimality over all possible partitions, see e.g. [2,18]. In order to preserve the possibility of comparison with the results presented in literature [6], two criteria are used in experimental tests of the proposed algorithm.Trace within criterion (hereafter TRW), proposed by Friedman and Rubin [19], is based on minimizing the trace of pooled-within groups scatter matrix (W) defined as(2)W=∑l=1kWl.Wlis the variance matrix of attributes for the objects belonging to cluster Clgiven by(3)Wl=∑j=1nl(zj(l)−z¯(l))(zj(l)−z¯(l))T,wherezj(l)is the vector of attributes for the j-th object of cluster Cl,z¯(l)=∑j=1nlzj(l)/nlthe vector of means (centroids) for cluster Cl, and nl=|Cl|. Thus, TRW is simply(4)TRW=tr(W).The between groups scatter matrix can be expressed analogously in the form(5)B=∑l=1knl(z¯(l)−z¯)(z¯(l)−z¯)T,z¯=∑i=1nzi)/nbeing the vector of means for all objects,n=∑l=1knl. It can be easily proved that the total scatter matrixT, defined asT=∑i=1n(zi−z¯)(zi−z¯)T,meets the equalityT=W+B.Variance ratio criterion (VRC) is based on maximizing the following ratio(6)VRC=tr(B)/(k−1)tr(W)/(n−k).For given k>1, VCR criterion can be expressed as VCR=c1/TRW−c2, where c1=(n−k)×tr(T)/(k−1) and c2=(n−k)/(k−1). It shows that the global optimum point is equivalent for both criteria. From this point of view, there is no reason to use the VCR criterion if k is given. However, when the heuristic search of the global optimum point is used, the shape of one function (landscape) may be more convenient than the other.The differential evolution (DE), introduced by Storn and Price [20], has become one of the most frequently evolutionary algorithms used for solving the continuous global optimization problems [21]. When considering the minimization problem, for a real functionf(x)→ℝ, wherexis a continuous variable (vector of length d) with the domainD⊂ℝd, the global minimum pointx* satisfying condition f(x*)≤f(x) for ∀x∈D is to be found. The domain D is defined by specifying boundary constraints,D=∏j=1d[aj,bj],aj<bj,j=1,2,…,d.The initial population of N points is generated at random, uniformly distributed in D, each point in D is considered as a candidate of the solution and then the population is evolving generation by generation until the stopping condition is met. Next generation Q is created by application of evolutionary operators to the current generation P. The structure of DE algorithm is written in Algorithm 1.Algorithm 1Differential evolution1:generate an initial generation P, (xi∈D, i=1, 2, …, N)2:evaluate f(xi), i=1, 2, …, N3:while stopping condition not achieved do4:fori:=1 toNdo5:generate a new trial vectoryusing P6:iff(y)≤f(xi) then7:insertyinto Q8:else9:insertxiinto Q10:end if11:end for12:P:=Q12:end whileA new trial pointyis generated by using mutation and crossover. There are various kinds of mutation and crossover [20–24]. The most popular mutation strategy rand/1/ generates the mutant pointuby adding the weighted difference of two points(7)u=r1+F(r2−r3),F>0,wherer1,r2, andr3 are three mutually distinct points randomly taken from population P, not coinciding with the currentxi, and scale factor F is an input parameter.The elements yj, j=1, 2, …, D, of the trial pointyare built up by the crossover of the current pointxiand the mutant pointu. The most frequently used kind of crossover is called binomial. It uses the following rule of combination of parents’ elements(8)yj=ujifUj≤CRorj=lxijifUj>CRandj≠l,where l is a randomly chosen integer from {1, 2, …, D}, U1, U2, …, UDare independent random variables uniformly distributed in [0, 1), and CR∈[0, 1] is an input parameter influencing the number of elements to be exchanged by crossover. The rule given by Eq. (8) ensures that at least one element of vectorxiis changed, even if CR=0.A combination of the mutation and the crossover forms the DE strategy, mostly abbreviated by DE/m/n/c, where m stands for the type of mutation, n for the number of differences used in mutation, and c for the crossover type. The strategy with the setting of F and CR defines a DE variant. Efficiency of the DE variants varies very substantially and is problem-depending. That is why many adaptive or self-adaptive DE algorithms were proposed, e.g. [25–28].The DE algorithm could be directly applied to minimization of TRW and maximization of VCR via minimizing the value of (-VCR). However, it is necessary to solve how to encode individuals in the DE population and an object-to-cluster association.Data matrixZis size of n×p with real-valued elements and it should be partitioned into k clusters. Each center of the cluster of a partition could be considered to be just one vector of length p so that each partition could be represented by k-tuple of such vectors. Therefore, any partition of k clusters can be encoded using a floating point vector of length k×p and each object is classified into a cluster with the least Euclidean distance to the center of the cluster.However, encoding the partition by a vector of centers (total length of individual is k×p) does not guarantee that the classification of objects is feasible. It may happen that a cluster or even more clusters are empty during the search process, which makes the current classification unfeasible. Such an unfeasible classification must be corrected to a feasible one. One way how to do it is assigning some objects to empty clusters, e.g. the element of the cluster of the highest cardinality which is most distant from the center can be assigned to an empty cluster until there is any empty cluster. This way of correcting is used in the implemented algorithms.Another question to be solved is how to encode an object-to-cluster association. The direct encoding is based on the idea to represent any feasible partition by a vector (of length n) whose ith component gives the index of the corresponding cluster. However, such a direct encoding of the object-to-cluster association is ambiguous. For given data, let us consider two feasible classifications, G1 and G2. Each classification is a vector of the length n and its elements are integers ℓ∈{1, 2, …, k} assigning the corresponding object to the ℓth cluster. Then we can construct so called classification matrixKwhich element Kijcontains the number of objects assigned to the ith cluster by G1 and to the jth cluster by G2. Based on the classification matrix, the (dis)similarity of the classifications can be evaluated. The criterion defined as follows is called dissimilarity measure, DM,(9)DM=1−tr(K)nIt is obvious that DM=0 means the perfect agreement of the classifications and DM=1 means the perfect disagreement. However, the classifications with high value of DM can be similar with respect to the criterion to be optimized.This situation is explained in the following example, where two feasible classifications below are dissimilar (even DM=1) but the value of the criterion is the same for both classifications:G1=111111222222223333333333G2=333333111111112222222222In this case it is easy to find such a rearrangement of G1 to get minimum dissimilarity of the classifications preserving the values of the criteria. G1 should be changed into G1new, the rule od the rearrangement is: change 1 to 3, 2 to 1, and 3 to 2, which can abbreviated by a vectorr=(3, 1, 2). When we applied the ruler, the classification matrix changes as follows006600800→08001000010DM=1DM=0The dissimilarity of G1 and G2 is caused by a different order of centers in two individuals of the population, which generates such classifications. If there are individuals in a population that have similar or even the same values of the objective function but different orders of their centers, we can hardly expect that the mutation and the crossover operations are effective in generating of trial individuals improving the fitness of the population. For making the mutation and the crossover operations effective, it is necessary to rearrange the order of the centers in order to have the individuals in the population generating the classifications as similar as possible.The task is to find such a rulerfor the rearrangement of G1 (and the order of the centers generating G1) minimizing the dissimilarity measure DM. Exhaustive search is not applicable for higher values of k due to its time complexity O(k!). That is why a heuristic search is proposed, the basic scheme of the heuristic is shown in Algorithm 2, the source code in Matlab is available at http://www1.osu.cz/tvrdik/.The time complexity of this heuristic searching for the best reordering ruleris determined by k searches of maximum in the array ofCwith k2 rows at most, which results in the most pessimistic estimate O(k3). In real-world examples with many zero elements in the first column ofC, the computational time is substantially less.The time costs of the heuristic and exhaustive search were compared experimentally on simulated data for the number of clusters k≤8. The results of the experimental comparison (not presented in this paper) have shown that the same ruleris found by both methods of the search in the cases where the solution found by the exhaustive factorial search is unique and the computational time required by the heuristic search is much less for k≥5 than for the exhaustive search and comparable for smaller k.Algorithm 2Heuristic rearrangement of centers1:construct classification matrixKfrom two feasible classifications G1 and G22:create a matrixCof dimension k2×3 containing the elements ofKin the first column and the corresponding row- and column-indexes with respect toKin the second and the third columns, respectively3:discard all the rows ofCwith zero values in the first column4:initialize the rule vectorrby zeros5:find the number (indmax) of the row inCwith maximum value in the first column6:while rule vector contains any zero element andCis not empty do7:ifr(indmax)=0 then8:r(C(indmax, 2)):=C(indmax, 3)9:end if10:discard the indmax-th row ofC11:find the row (indmax) ofCwith maximum value in the first column12:end while13:while any element ofris zero do14:complete the empty element ofrfrom the row ofCnot applied yet12:end whileConsidering hybridization of DE algorithm, it is very natural to apply the k-means algorithm as a local optimizer in the search process. After finding a trial vectorysatisfying the condition f(y)≤f(xi), the k-means algorithm withyas input is used to get the locally best solution. This solution is then used as the trial vector. Similar approach has been recently applied in [12] as well as in our previous papers [4,5]. Advantages of k-means algorithm are fast convergence to a local minimum and low time complexity.Algorithm 3Hybrid differential evolution for clustering1:generate an initial generation P, (xi, i=1, 2, …, N)2:evaluate f(xi, i=1, 2, …, N)3:while stopping condition not achieved do4:fori:=1 toNdo5:generate a new trial vectoryusing Pthen6:iff(y)≤f(xi)7:y*=kmeans(y)8:inserty* into Q9:else10:insertxiinto Q11:end if12:end for13:if condition for rearrangement fulfilled then14:rearrange the centers to similarity with the best individual15:end if16:P:=Q17:end whileTaking into account the topics discussed in previous sections, the hybrid DE algorithm with k-means can be proposed as it is shown in Algorithm 3. Compared to standard DE algorithm (1), line 7 with k-means algorithm and lines 13 to 15 with rearrangement are inserted. The rearrangement is carried out conditionally, not obligatory in each generation. It has two reasons: The rearrangement is time-consuming because the whole population should be processed, which results in N−1 calls of Algorithm 2 in each generation. The second reason is based on the preliminary results [5], where it was found that rearrangement in early stage of search process is not very beneficial. Notice that the proposed hybridization and rearrangement is applicable in all DE strategies including the adaptive ones, for example [25–28].Any DE algorithm for optimal clustering based on the scheme shown in Algorithm 3 can be easily modified into four variants depending on the presence/absence of local k-means and rearrangement of centers. The experimental comparison of these variants provides us an insight to the influence of the hybridization and the rearrangement on the performance of the algorithm.Three DE variants were used in experimental evaluation of performance of the hybrid DE algorithm for clustering. One of them is a classical DE/rand/1/bin proposed by Storn and Price [20] with the control parameters F=0.8 and CR=0.8.The other are chosen from the family of adaptive DE variants. One of them is jDE [25] which uses a self-adaptive mechanism of values of control parameter F and CR. It is considered as one of the state-of-the-art adaptive DE algorithms [22]. The second is b6e6rl variant [29], where adaptation is based on competition of strategies. This variant was compared with the state-of-the-art adaptive DE algorithms jDE [25] SADE [26], JADE [27], and EPSDE [28] and two versions of DE algorithm with composite trial vector generation strategies and control parameters [30]. The b6e6rl appeared the most reliable and the second fastest algorithm among those seven algorithms in the benchmark set of six shifted functions at three levels of dimension (D=10, 30, and 100) [31]. This is the reason why b6e6rl was preferred to SADE and EPSDE, where the adaptive mechanism is also based on competition. jDE is chosen due to the fact that it uses the completely different self-adaptive mechanism.A simple and efficient adaptive jDE variant was proposed by Brest et al. [25]. It uses the DE/rand/1/bin strategy with an evolutionary self-adaptation of F and CR values. The tuple of these control parameters is encoded with each individual of the population and survives if an individual is successful, i.e. if it generates such a trial vector which is inserted into next generation.The values of F and CR are initialized randomly for each point in population and survive with the individuals in the population, but they can be randomly mutated in each generation with given probabilities τ1 and τ2. If the mutation condition happens, new values of CR∈[0, 1] uniformly distributed, and F also distributed uniformly in [Fl, Fu] are used in generating a trial vector and stored in the new population. Input parameters are set to Fl=0.1, Fu=0.9, τ1=0.1, and τ2=0.1 as applied in [25].DE with competitive setting of the DE strategies and the control parameters was originally proposed in [32]. In this self-adaptive approach, we choose randomly among H different DE strategies or settings of control parameters (F, CR) with probabilities qh, h=1, 2, …, H.These probabilities change according to the success rate of the settings in preceding steps of the search process. The h-th setting is considered successful if it generates a trial pointybetter than its counter-partnerxiin the old generation P, i.e. f(y)≤f(xi). Probability qhis evaluated as the relative frequency(10)qh=nh+n0∑j=1H(nj+n0),where nhis the current count of the h-th setting's successes, and n0>0 is a constant. The input parameter n0>1 prevents a dramatic change in qhby one random successful use of the hth parameter setting. To avoid degeneration of the search process, the current values of qhare reset to their starting values qh=1/H if any probability qhdecreases bellow the given limit δ>0 during the search process. The input parameters controlling competition are recommended to set up to n0=2 and δ=1/(5×H).Several combinations of DE strategies and control parameter settings were compared in different benchmark tests [29,33]. The variant denoted b6e6rl using twelve different DE strategies or parameter settings was found as one of the most efficient among all the tested DE variants with competing strategies. The b6e6rl uses six strategies with the binomial crossover and six strategies with the exponential crossover. Values of scaling parameter are set to F=0.5 and F=0.8 and each combined with three different values of CR. The setting of CR values is based on Zaharie's results [34].All DE algorithms under consideration were tested using eight real-world data sets. Well-known Iris plants data set (hereafter iris) contains three different classes of 50 objects each, where each object has four numeric attributes. No missing attribute values occur.Wisconsin breast cancer data set (hereafter bcw) contains only two classes. In the original data set, there are 16 objects containing a single missing attribute value. When the objects with missing values were omitted, the database contained overall 683 objects, from that 444 objects belonging to malignant class and 239 objects to benign class.Glass identification data set (hereafter glass) includes data of six classes (types of glass) with 70,76, 17, 13, 9, and 29 objects, respectively.Vowel data set (hereafter vowel) consists of overall 870 objects There are six strongly overlapping classes: ó (72 objects), a (89 objects), i (172 objects), u (151 objects), e (207 objects), and o (180 objects).Wine recognition data set (hereafter wine) consists of 178 objects (wines grown in the same region of Italy), each object having 13 numeric attributes. The database contains data of three classes corresponding to three different cultivars: class 1 (59 objects), class 2 (71 objects), and class 3 (48 objects). No missing attribute values are present.Ionosphere database (hereafter iono) contains radar data of 351 objects, each of which has 34 numeric attributes. The second attribute (no name) with constant 0 value for all the objects is eliminated. There are only two classes: good and bad. No missing attribute values occur. The second attribute (no name) with constant zero value for all the objects is eliminated. The values of TRW and VCR criteria are not influenced by the elimination of this attribute.Thyroid data set (hereafter thyroid) contains 215 objects (patients) subdivided into three classes: normal (150 objects), hyper (35 objects), and hypo (30 objects). Each patient is characterized by five numeric attributes. Liver disorders database (hereafter liver) contains data of 345 objects (single male individuals), each of them is characterized by six numeric attributes. The data set is split into two classes denoted as 1 and 2 without any explanation. There are no missing attribute values in the data.The data labeled as bcw, iris, glass, and vowel were received directly from Paterlini [6], while the remaining data were taken from the Machine learning repository [35]. The size of the data sets and number of clusters are summarized in Table 1.The best values of criteria that have been found before in literature [6,10,4,5] or in our preliminary experiments are presented in Table 2. These values are used as reference in assessment of the precision of results found by newly proposed algorithms.Three types of DE algorithms described in Section 8, namely DE/rand/1/bin, jDE, and b6e6rl, were applied to the optimization of the criteria TRW and VCR. Each algorithm was tested in four variants:•plain differential evolution, denoted by the suffix ‘NN’ hereafterplain differential evolution with the rearrangement of the cluster centers, denoted by the suffix ‘NG’hybrid DE with k-means algorithm used for finding a local best classification generated by an accepted trial pointy, denoted by the suffix ‘HN’hybrid DE with k-means algorithm used for finding a local best classification generated by an accepted trial pointyand the rearrangement of the cluster center, denoted by the suffix ‘HG’Lower and upper bounds of the search space are specified for each problem directly from the input data setZ. The lower bounds are given by k iterations of the vector of attribute minima, min(Z), and upper bounds analogously by k iterations of the vector max(Z).The population size was set up to N=30 in all the test problems, the stopping condition was defined as follows(11)fmax−fminfmin≤1×10−4orFES≥MaxFES,where fmaxand fminare the highest and the lowest function values in the current generation, respectively, FES is the number of the function evaluations needed to reach stopping condition, and MaxFES is maximum allowed number of function evaluations, MaxFES= 20, 000×d, d=p×k.In hybrid variants, k-means algorithm was stopped if the relative difference of the values of the objective function in two subsequent iterations was less than 1×10−4. In the variants with the rearrangement of the centers, the order of the centers in all individuals of the population were changed to get the maximum similarity to the individual with the best value of the objective function. The rearrangement was carried out if the condition was achieved:(12)rand(0,1)>fmax−fminfmin+τwas achieved, τ is an input parameter, 0<τ<1 and rand(0,1) is random number uniformly distributed in the interval [0, 1]. Based on preliminary experiments, the value of τ was set up to 0.5. According to (12), the probability of rearrangement is zero at early stage of the search when the value of (fmax−fmin)/fminis high. Then it is increasing gradually to almost 0.5 at the end of the search, which results in one rearrangement performed in two subsequent generations on average.The experiments were done on eight test problems specified in Section 9, 50 independent runs were carried out per problem and per DE variant. The algorithms were implemented in Matlab, version 2009a, all computations were carried out on a standard PC with Windows XP, Intel Pentium 4CPU, 3.00 GHz, 992 MB RAM.

@&#CONCLUSIONS@&#
