@&#MAIN-TITLE@&#
A disassembly line balancing problem with fixed number of workstations

@&#HIGHLIGHTS@&#
We consider Disassembly Lines with fixed number of workstations.We aim to assign the tasks to the workstations so as to maximize the total revenue.We present several upper and lower bounding procedures.Our computational study indicates that our procedures lead to satisfactory behaviors.

@&#KEYPHRASES@&#
Integer programming,Heuristics,Disassembly lines,Linear programming relaxation,

@&#ABSTRACT@&#
In this study, a Disassembly Line Balancing Problem with a fixed number of workstations is considered. The product to be disassembled comprises various components, which are referred to as its parts. There is a specified finite supply of the product to be disassembled and specified minimum release quantities (possible zero) for each part of the product. All units of the product are identical, however different parts can be released from different units of the product. There is a finite number of identical workstations that perform the necessary disassembly operations, referred to as tasks. We present several upper and lower bounding procedures that assign the tasks to the workstations so as to maximize the total net revenue. The computational study has revealed that the procedures produce satisfactory results.

@&#INTRODUCTION@&#
The environmental regulations, customer awareness and recent advances in technology all together have shifted the product recovery process from the act of disposing to the act of remanufacturing and recycling. Recycling preserves the material content of the discarded (used) products via some manufacturing and disassembly operations. Remanufacturing, on the other hand, keeps the functional content of the used products and improves their quality up to a desired usable level via disassembly operations and some manufacturing.Disassembly is the first important step of product recovery activities (McGovern and Gupta, 2011) and it is methodical extraction of valuable parts, operations involve the separation of the reusable parts from the discarded products. The parts are either subject to some remanufacturing process or sold to suppliers. Disassembly operations are usually performed on a disassembly line that consists of a number of serial workstations. The first workstation takes the product to be disassembled. The cycle terminates, i.e., the product leaves the line, whenever all its required parts are disassembled.The Disassembly Line Balancing Problem (DLBP) assigns the set of tasks to each workstation for each product to be disassembled. The problem is critical in minimizing the use of valuable resources (such as time and money) invested in disassembly, and maximizing the level of automation of the disassembly process and the quality of the parts or materials recovered.This study considers a DLBP with a fixed number of workstations. It is assumed that there is a specified supply for the products to be disassembled. For each part, a defined minimum release quantity must be met. The amount in excess of the minimum release quantity can also be sold in the market. Hence the excess quantity is produced, provided that the part is profitable, and that there is enough supply. The aim is to assign the tasks to the disassembly workstations so as to maximize the total net revenue of the parts, while meeting their specified minimum release quantities, and without exceeding the specified cycle time. To satisfy the minimum release quantities, different parts may be released from different units of the product. The challenge is to use different line balances, hence use different task assignments to the already mounted workstations, while disassembling different units of the product. To the best of our knowledge, the study is the first attempt to solve the DLBP with minimum release quantities and fixed number of workstations.Our study will have a direct impact on the industries that experience continuous advances in their technology. These advances naturally affect the function and fashion oriented expectations of the consumers. One such application area that we take our motivation from is the electrical and electronic equipment industry. The consumers of the personal computers, televisions or cellular phones, replace their products within a few months to a few years, even when the products (thereby their parts) are functioning properly. Electrical and electronic equipments consist of many different substances some of which may contain hazardous components and valuable parts (Capraz, Polat, & Güngör, 2015). As the used products have proper conditions, their valuable parts (like memories, CPUs, valuable metals) can be used in manufacturing new models. The minimum release quantities for these parts (that we define in our models) are associated to the demands from the third party like the producers of the new models. Moreover, as we assume, the amount released excess of the minimum release quantities of the valuable parts can be stored in the inventory for future orders. Another application area is the automotive industry where the consumers of the luxurious cars replace their not-so-much-used cars with new brands that make use of the most recent technology.The rest of the study is organized as follows. Section 2 reviews the disassembly process and literature on the DLBP whereas Section 3 defines our problem. In Section 4 the mathematical models and their use in finding optimal solutions are discussed. In Section 5, we present upper bounds together with the mechanisms used to strengthen them and present a heuristic procedure. The computational experiment is discussed in Section 6 and the study is concluded in Section 7.Güngör and Gupta (2001b) defined disassembly as a systematic process of separating a product into its constituent parts, components, subassemblies or other groupings. The issues in the area of disassembly can be classified into two broad categories as design and operational. Crowther (1999) considered a design for disassembly issue and mentioned that a life cycle model that incorporates the stages of a disassembly strategy can highlight the environmental advantages of designing for disassembly, showing how it can extend service life and thereby improve sustainability. As a practical application, he discussed the construction industry and mentioned that experience gained from disassemblable buildings can be used to create guidelines for other products. As discussed in Brennan, Gupta, and Taleb (1994), the design aspects for disassembly have being recognized by the industries that generate huge amount of ferrous and plastic waste like motor cars and appliance sectors.When the old products come to disassembly plant so that their components can be recovered in the assembly plant or re-used, operational problems arise. The operational problems that are likely to arise are layout, resource allocation, process sequencing and disassembly line balancing. As mentioned by Brennan et al. (1994), the operational problems have environmental concerns that are even forced by the governments like the recycling regulations, and limitations on the energy consumptions. These regulations affect the operating costs as extra costs are incurred for covering the expenses related to the environmental matters.In their review paper McGovern and Gupta (2011) discussed many aspects of the operational problems with an emphasis on the disassembly sequencing problem and the DLBP. The disassembly line sequencing problem decides on the disassembly process sequence of a specified disassembly product and the DLBP decides on the assignment of the tasks to each disassembly workstation.Lambert (2003) provided a review of the disassembly sequencing literature. Some noteworthy studies on disassembly sequencing are due to Lambert (1997), Navin-Chandra (1994) and Lambert (2002). Navin-Chandra (1994) used a modified traveling salesman problem so as to minimize total costs while meeting obligatory reclamation of definite parts. Lambert (1997) proposed a graph based method so as to maximize the economic performance of the disassembly process within given technical and environmental constraints. Both methods were applicable to the disassembly of complex consumer products like automotive vehicles, consumer electronics and mechanical assemblies and were applied to a headlamp and ballpoint pen disassembly products. Lambert (2002) proposed a linear programming based solution procedure for the minimum cost disassembly sequence of an electronic equipment. His method was applicable all products having hierarchical modular structures.The DLBP was first introduced in Güngör and Gupta (2001b). Güngör and Gupta (2001a) mentioned several complications like early leaving, self-skipping, skipping, disappearing and revisiting work pieces that could be faced in disassembly line practices. To reduce the effects of the complications on the disassembly process, they proposed a shortest path based solution algorithm. McGovern and Gupta (2007a) showed that the decision version of the DLBP is NP-complete. McGovern and Gupta (2007b) considered a DLBP to minimize the number of workstations while balancing the idle times between the workstations. They proposed an exhaustive search method that returns optimal solutions for small sized problem instances and a genetic algorithm that finds high quality solutions, for the large sized problem instances.Güngör and Gupta (2002) proposed a heuristic procedure for the DLBP under complete disassembly. They assumed an infinite supply of a single product and considered the efficient utilization of the resources while satisfying the minimum release quantity. They illustrated the proposed heuristic on an eight task personal computer disassembly example. Koc, Sabuncuoglu, and Erel (2009) considered a complete disassembly DLBP so as to minimize the number of workstations. They introduced AND/OR graphs and proposed Integer and Dynamic Programming formulations.Altekin, Kandiller, and Özdemirel (2008) studied the DLBP under partial disassembly and an infinite supply of a single product. They formulated their net revenue maximization model as a mixed-integer linear program and used its relaxations to find lower and upper bounds. They stated that their approach could be used in designing and operating remanufacturing systems where large volumes of similar products should be disassembled. Altekin and Akkan (2012) proposed a predictive–reactive approach based on a mixed-integer model to improve the profitability of the disassembly line. A predictive balance was created and then given a failure, the line was rebalanced. They stated that their algorithm could be used as a guide by the disassembly workers about how to proceed in case of a task failure.The recent disassembly lines research considered the uncertainty of the task times and product quality. Bentaha, Battaïa, and Dolgui (2014a, 2014b, 2014c, 2015) and Bentaha, Battaïa, and Dolgui (2014d) studied complete and partial disassembly models, respectively. Bentaha et al. (2014a) modeled uncertainty using the notion of resource cost and proposed a sample average approximation method. Bentaha et al. (2014b) studied the joint problem of disassembly line balancing and sequencing. Bentaha et al. 2014c proposed a lagrangian relaxation approach to maximize the total profit. Bentaha et al. (2015) considered the problem of minimizing the workstation operation costs and additional costs for handling the hazardous parts. They developed several lower and upper bounding mechanisms. Bentaha et al. (2014d) addressed workload balancing problem with fixed number of workstations. They developed a stochastic binary program.Ding, Feng, Tan, and Gao (2010) stated that a successful disassembly line often requires an integrated consideration of many objectives and proposed an ant colony algorithm to generate the efficient set. They tested their algorithm with three objectives: number of the workstations, workload balancing between the workstations and demand rating. They illustrated the heuristic on a 25 task cellular phone disassembly example. Paksoy, Güngör, Özceylan, and Hancilar (2013) mentioned that in real world applications the objectives could be fuzzy due to incomplete information and proposed fuzzy goal programming and fuzzy multi-objective programming. They considered three objectives: number of the workstations, workload balancing between the workstations and cycle time. The approaches were applied on 10 tasks flash light and 30 tasks radio examples. Hezer and Kara (2014) introduced parallel line dissassembly balancing problem and proposed a network model based approach for its solution. For more information on the dissassembly line balancing and planning, one may refer to Ullerich (2014).The most closely related published work to ours is Altekin et al. (2008)’s study. Our problem environment differs from theirs in the sense that we take the number of workstations and cycle time as parameters whereas they decide on the number of workstations and on the cycle time. We assume that there is a finite supply of disassembly product and that there is a lower bound on the quantity of the part releases whereas they assume infinite supply and take minimum release quantities as upper bounds on the quantity of the part releases. Hence their solution considers a single line balance for all units, whereas, to satisfy the minimum release quantities, we use different task assignments to disassemble different units.We consider a disassembly line balancing model with a finite supply of a single product. There are N tasks where task i is specified by its cost ciand processing time ti. The tasks that release parts are called part releasing tasks. The minimum release quantity of each part-releasing task i is diunits. diis the associated demand for part i from the third party like the producers of the new product models. The part-releasing task i has a revenue rithat includes its market value (for example when the part is sold to the third party) and recycled material value. The parts may have negative revenues that represent the disposal costs. Riis the net revenue of task i, Ri= ri – ci. At the end of the period, each unit of part i that is released excess of diunits can be held in the inventory for future orders and generates a unit revenue riwhen it is sold. Such excess releases would be favored if they lead to increases in the total net revenue. Our problem is to assign the tasks to the workstations of the disassembly line for each unit of the product so as to maximize the total net revenue. We make the following additional assumptions.•The workstations are already set up and there are K workstations.All workstations are identically equipped and are capable of performing any one of the tasks at the same pace.There is a single disassembly product with a finite supply of S units.There is a single disassembly period of length L, that is sufficient to disassemble all S units. That is, there is a single batch of size S to be disassembled in L time units.The cycle time, C, is the time between the completion of two units consecutively leaving the disassembly line, hence it is the processing workload that can be allocated to each workstation. One may view the cycle time to be the ratio L/S .The cycle time, C, is known with certainty and is not subject to change as the period length L and supply S, are known with certainty and are not subject to change.All units of the product are identical, hence contain all disassembly parts.A partial disassembly is done, i.e., a subset of -but not- all parts is released.The subset of parts released from one unit of the product may be different from another unit, hence different task assignments are possible while disassembling different units of the product.The network comprises AND and OR type relations. Predecessor AND (PAND) relations imply that a task cannot start before all of its predecessor tasks are finished. The following figure illustrates the AND precedence relations.According to Fig. 1, tasks a, b, c and d are predecessors of task e. Task d is the immediate predecessor of task e as there is no task in between.We use AND(i) to denote the set of all AND predecessors of task i and PAND(i) to denote the set of all AND immediate predecessors of task i.Predecessor OR (POR) relations imply that at least one task in a specified set should be complete before the successor task may begin. Fig. 2illustrates POR type relations.According to the above figure, at least one task in set { f, g, h} must be completed before task i may begin. We let POR(i) denote the set of OR predecessors of task i.The mathematical formulation given below determines the optimum task assignments to the workstations while disassembling each unit that maximizes the total net revenue. The decision variables of the model are,Xiks={1iftaskiisassignedtoworkstationkforsthunitoftheproduct0otherwiseConstraint set (1) deals with the minimum release quantities. The amount of each part released should be higher than its minimum release quantity.(1)∑k=1K∑s=1SXiks≥dii=1,…,NConstraint set (2) states the cycle time limit should not be exceeded at any workstation for any unit.(2)∑i=1NtiXiks≤Ck=1,…,K;s=1,…,SConstraint sets (3) and (4) ensure that the precedence requirements are observed. The AND precedence relations should be satisfied. Thus task i cannot be assigned to a workstation that is before the workstation of its AND predecessor task l.(3)∑k=1KkXlks≤∑k=1KkXiksi=1,…,N,l∈PAND(i),s=1,…,SSimilarly, the OR precedence relations should be satisfied. Thus task i can be assigned to workstation k if and only if at least one of its OR predecessors is assigned to workstations 1 through k.(4)Xiks≤∑h=1k∑j∈POR(i)Xjhsi=1,…,N;k=1,…,K;s=1,…,SConstraint set (5) ensures that a task can be assigned to at most one workstation.(5)∑k=1KXiks≤1i=1,…,N;s=1,…,SThe assignment variables should be nonnegative as by constraint set (6).(6)Xiks≥0andintegeri=1,…,N;k=1,…,K;s=1,…,SIt follows from Constraint Sets (5) and (6) that Xiksis binary.The model, hereafter referred to as Model I, is defined by objective function (7) below, together with constraint sets (1) through (6).(7)Max∑i=1N∑k=1K∑s=1SRiXiksModel I has N * K * S binary decision variables.McGovern and Gupta (2007b) showed that the decision version of the DLBP is strongly NP-complete. It follows that our DLBP problem is NP-hard in the strong sense. We thus resort to several approaches to solve the DLBP approximately or heuristically. We also develop theoretical instruments, more specifically, valid constraints, that improve the solution effort. These topics are discussed in the remainder of the paper. In this section, we introduce the key insight into reducing the problem size.In typical implementations, the model solution yields different disassembly tasks and different part-releasing assignments for the batch of S units. Typically, first a subset of the batch is disassembled to satisfy the minimum release quantity constraints. Recall that minimum quantities for parts are novel and distinguishing aspect of the current work. Once the minimum release quantities are satisfied, though, the solution understandably favors task assignments that yield the maximum revenue. Note that, once all of the part minimum release quantities have been met, the specific task assignments to attain the maximum revenue is independent of the remaining quantity to be disassembled. In fact, the maximum revenue yielding task assignments can be found quite easily by solving Model I with S = 1. The maximum revenue yielding task assignments could be replicated as many times as needed for the remaining parts.The above insight immediately suggests an approach to make problems with large S more tractable. If one had access to the number of units needed, say Y ≤ S, to be disassembled in order to satisfy the minimum release quantities, then Model I could be solved using Y instead of S. Since Y ≤ S, and sometimes even Y << S, the computations are simplified. The remaining task assignments are then computed by solving Model I with S = 1 and then replicating the solution for the remaining S–Y units.Clearly, the above approach would also work for an upper bound of Y. Even an approximate value of Y may be useful. If the chosen value of Y gives an infeasible solution, then we could investigate ways to increase Y to obtain a feasible solution. In the following sections, we discuss how the quantity Y may be computed, exactly or approximately, and how bounds for Y may be obtained.The formulation given below, henceforth referred to as Model II, finds the minimum number of units to be disassembled to satisfy all minimum release quantities.Model II can be defined by the following additional variable and constraint.Ys={1ifsthunitisdisassembledtosatisfyminimumreleasequantity0otherwise(8)∑i=1N∑k=1KXiks≤μ*Yss=1,…,SConstraint Set (9) assigns Ysto 1, if there is at least once task assigned to any workstation while disassembling sth unit of the product. Note that∑i=1N∑k=1KXiksis bounded from above by μ. We use μ = N*K. If∑i=1N∑k=1KXiks≥1then Ys= 1 otherwise Ysis zero as we are minimizingY=∑s=1SYs.Y=∑s=1SYsgives the total number of units, with at least one task assignment. Hence it is the minimum number of disassembly units to satisfy all minimum release quantities.The objective function of Model II is expressed as below.(9)Min∑s=1SYs−ɛP∑i=1N∑k=1K∑s=1SRiXiksThe above objective primarily minimizes the number of disassembly units to satisfy all minimum release quantities. Among the solutions that attain the minimum number of disassembly units Y, the model selects the one with the maximum net revenue. To maximize the total net revenue while keeping the minimum number of disassembly units, some parts are released more than their minimum release requirements. The remaining S–Y units are handled by Model III (Section 4.2).The magnitude of εpused in (9) is important in the sense that it should not increaseY=∑s=1SYsen route to increasing total net revenue. εpshould be set small enough to satisfy constraint (10).(10)∑s=1SYs−ɛP*Pmin≤∑s=1SYs+1−ɛP*Pmaxwhere Pmin(Pmax) = minimum (maximum) possible total net revenue.Hence any solution with a largerY=∑s=1SYsvalue should not be favored even if it leads to the maximum increase in the total net revenue value. Constraint (10) implies thatɛP*Pmax−ɛP*Pmin≤1.This followsɛP≤1Pmax−PminWe usePmax=S*∑i=1NRidiandPmin=0.HenceɛP=1S*∑i=1NRidiguarantees the maximum revenue solution among ones with the minimum number of disassembled units.Model II is completely defined by the objective function expressed in (9) together with constraint sets (1)–(6) and (8).The model satisfies all minimum release requirements by disassemblingY=∑s=1SYsunits. For the remaining (S–Y) units, the same task assignments are used. To find these task assignments we modify Model I by setting S = 1, and then replicating the solution for the remaining S–Y units. This solution, that is, Model I with S = 1 and then replicated as needed, is referred to as Model III. In other words, Model III finds the maximum net revenue without considering minimum number of part releases.In the following model, Model III, we drop subscript s from our decision variable Xiksas we are considering a single unit. The decision variable becomes XikwhereXik={1iftaskiisassignedtoworkstationk0otherwiseThe objective function is as expressed in (11).(11)Max∑i=1N∑k=1KRiXikWe also drop the minimum release quantity constraint as they are met by Model II. Model III is defined by the objective function expressed in (11) together with constraint sets (2) through (6) by setting S = 1.Model I given above yields the optimum solution but suffers from computational difficulties as the problem size increases. Alternatively, Model II and Model III may now be used to find an optimal solution as given by the following procedure.Procedure 1.Optimal Solution via Model II and Model IIIStep 1. Use Model II to find the task assignments for the firstY=∑s=1SYsdisassembly units.Step 2. Use Model III to find the task assignments for the last (S–Y) disassembly units.Step 3. The optimal total net revenue over all S units is:Total Net revenue by Model II + (S – Y) * Total Net revenue by Model IIIModel II is more complex than Model I due to the added Ysvariables, hence in finding the optimal solutions, Procedure 1 should not be favored to Model I. However, Procedure 1 is of importance, since we use the procedure to derive upper and lower bounds on the optimal total net revenue value.Model I and Model III together may be used to find optimal solutions as well. Recall thatY=∑s=1SYsreturned by Model II is the minimum number units to be disassembled to satisfy all minimum part requirements. If an upper bound on Y, UB(Y), is available, then Model I can be solved with UB(Y) in place of S units. Thereafter, Model III optimally solves the remaining S–UB(Y) units. Procedure 2, below, is used to find an optimal solution using Model I and Model III. The upper bound UB(Y) may be guessed, or sought using a simple algorithm such as bisection search. We will revisit the task of finding UB(Y) in the following sections.Procedure 2.Optimal Solution via Model I and Model IIIStep 1. Solve Model I with S = UB(Y).Step 2. Solve Model III to find the task assignments for the remaining S –UB(Y) units.Step 3. Find the optimal total net revenue over all S units as:Total net revenue over UB(Y) units by Model I+(S–UB(Y)) units*Total net revenue over a single unit by Model IIINote that there are three ways of finding an optimal solution: the original Model I, Procedure 1, and Procedure 2. Procedure 2 should be favored to original Model I, in particular when compared to S, a small UB(Y) is available.In our experiments, we use Model I to find the optimal solutions. We develop two upper bounds to the optimum objective function value that use the Linear Programming Relaxations (LPRs) of the original model. We benefit from the ideas used in Model II and III to find a third upper bound and to develop a heuristic solution procedure. These are presented next.As mentioned, our DLBP is NP-hard in the strong sense. Hence there is little chance of finding polynomial even a pseudo-polynomial algorithm that solves it optimally. In the section we present our solution approaches that provide upper bounds and a heuristic solution to our DLBP.Upper Bound 1 (UB1): An optimal solution to any relaxation provides an upper bound on the optimal objective function value of our maximization problem.Model I is solved to optimality by relaxing the integrality constraints on the Xiksvalues. The resulting optimal objective function value, ZLP, provides an upper bound on the optimal total net revenue. The only difference between the original model and its LPR is the integrality requirement of the Xiksvalues. Consequently, ZLPis an upper bound on the optimal objective function value Z*.Upper Bound 2 (UB2): UB2 is found by adding valid cuts to the LPR of Model I. The valid cuts are found by investigating the properties of the optimal solutions that may not be satisfied by the optimal LPR.Cut1–Fixing the minimum release quantitiesTheorem 1There exists an optimal solution in which di units of part i are released from the first di disassembly units.ProofAssume an optimal solution in which the first diunits are released from diunits of the product. If the tasks in any two units, say a and b, are changed, i.e., Xikais set to Xikband Xikbis set to Xikafor all i , then the optimal net revenue is not affected. Assume an optimal solution in which part i is released in units a+1, a+ di. Setting Xjkr = Xjk(a+r)for all j, r ≤ djand Xjk(ta+r) = Xjkrdoes not change the revenue value. Then, there is an optimal solution in which djunits of part i is released in the first diunits. □Using Theorem 1, drunits for any part r can be obtained from the first drdisassembly units. Constraint set (12) below ensures that drunits of part r are released from the first drdisassembly units. To increase the power of (12), we select task r such that dr=Maxi{di }.(12)∑k=1K∑s=1drXrks=drs=1,…,drIn computational studies, we observe that the constraint set (12) eliminates many alternate optimal solutions, thereby decreasing the computational effort. The reduction in computational effort was reflected as significantly reduced CPU times.Cut2 –Lower Bound on the workstation positionsCut2 provides a lower bound on the workstation index that any particular task.Theorem 2In all optimal solutions, task i cannot be assigned to workstations1,2,…,Ei−1whereEi=⌈ti+Minj∈POR(i){tj}+∑j∈AND(i)tjC⌉.ProofTask i cannot start before its predecessors and at least one of its immediate POR predecessors are completed. Hence it should wait at least Minj ∈ POR(i){tj} units for its POR predecessors and ∑j ∈ AND(i)tjunits for its AND predecessors to start and at leastti+Minj∈POR(i){tj}+∑j∈AND(i)tjunits to complete. When task splitting is allowed and other tasks are ignored,ti+Minj∈POR(i){tj}+∑j∈AND(i)tjunits require⌈ti+Minj∈POR(i){tj}+∑j∈AND(i)tjC⌉workstations. When task splitting is not allowed and other tasks are considered⌈ti+Minj∈POR(i){tj}+∑j∈AND(i)tjC⌉is a lower bound on the earliest station for task i.□In the absence of POR relations, Theorem 2 reduces to the one derived in Patterson and Albracht (1975) for the classical assembly line balancing problem.Constraint set (13) below supports Theorem 2.(13)∑k=1Ei−1∑s=1SXiks=0i=1,…,nConstraint set (13) ensures that task i is not assigned to workstations 1, 2, …, Ei–1. The network in Fig. 3is used to illustrate Eicomputations.Note that { t1 + t2} is the minimum total processing load to start task A and Min {t3, t4}+ t5is the minimum total processing load to start task B. Then Min{(t1 + t2+ tA), (Min {t3, t4} + t5 + tB)} is the minimum total processing load to start task j.NotethatEi=⌈ti+MinimumtotalprocessingloadtostarttaskiC⌉.HenceEi=⌈ti+Min{(t1+t2+tA),(Min{t3,t4}+t5+tB)}C⌉.The network in Fig. 4is adapted from Bourjault (1984) to illustrate Theorem 2.We take K = 4, C = 63, S = 80 and the task times are as given below:Tasks12345678910111213141516171819202122ti13716751581731412174111815194181619E13=⌈t13+Min{(t1+t3+t4),(t1+t2)}+t22+t7C⌉E13=⌈17+Min{(13+1+6),(13+7)}+19+1563⌉=⌈7163⌉=2The theorem states that task i cannot be assigned to workstations 1, 2, …, Ei– 1. Accordingly, task 13 cannot be assigned to workstation 1 (E13– 1 = 1).Cut3–Projecting minimum release quantitiesAltekin et al. (2008) stated that in the optimal solution, the total fraction of any task assigned is never greater than the total fraction of each of its assigned AND predecessors and the total fractional assignment of all of its OR predecessors, and moreover the constraints may not be satisfied for the optimal LPR. Their results hold for any problem that contains PAND and POR relations and an infinite supply. We generalize their results to our problem where the supply is finite and impose the constraint sets (14) and (15).(14)∑k=1K∑s=1SXiks≤∑k=1K∑s=1SXjksi=1,…,n;j∈PAND(i).(15)∑k=1K∑s=!SXiks≤∑j∈POR(i)∑k=1K∑s=1SXjksi=1,…,nCut4–Existence of a feasible solutionOur last cut uses the results of the following two theorems.Theorem 3If there exists a feasible schedule that processes all tasks other than task i and its successors in r workstations then there exists an optimal schedule in which task i is processed in workstations 1 through r.ProofAssume task i is assigned to workstation k such that k ≤ r and workstations r +1 through K process the successors of task i. Taking task i from workstation k and placing to workstation l such that l ≥ r + 1 cannot increase the total net revenue as such a replacement cannot allow more task assignments. This is due to the fact that all tasks in workstations r + 1 through K are successors of task i, hence cannot be processed in workstations 1 through r, since task i is to be processed later. Then, there exists an optimal solution in which task i is processed in the first r workstations. □Theorem 4If there exists a feasible schedule that processes all tasks other than task i and its predecessors in the last r workstations, then there is an optimal schedule in which task i is processed in workstations K – r + 1 through K.ProofAssume task i is assigned to workstation k such that k ≥ K – r + 1 and workstations 1 through K – r process the predecessors of task j. Taking task i from workstation k and placing it to workstation l such that l ≤ K – r cannot increase the total net revenue, as such a replacement cannot allow more Assignments. This is due to the fact that all tasks in workstations 1 through K–r are predecessors of task i, hence cannot be processed in workstations K – r + 1 through K, since task i is processed earlier. Then, there exists an optimal solution in which task i is processed in the last r workstations. □Note that Theorems 3 and 4 require a feasible solution. To find such a solution we develop the following simple rule.Let A be a set of tasks that should be assigned to K workstations. Order the tasks in set A according to non-decreasing task times. Take the tasks from the order starting from the first feasible task. A task is feasible if its inclusion to the current workstation does not violate the precedence relations and cycle time constraints. If no such task exists, close the current workstation and open a new one. Stop when all jobs in set A is assigned. Let m(A) be the resulting number of workstations.Following Theorem 3, we set A = A1 where A1 is the set of all tasks except the successors of task i. Following Theorem 4, we set A = A2 where A2 is the set of all tasks except the predecessors of task j. After m(A1) and m(A2) are obtained, implementing the heuristic for sets A1 and A2, we include one of the two cuts (16) and (17).(16)∑s=1S∑k=m(A1)+1KXiks=0i=1,…,N(17)∑s=1S∑k=1m(A2)−1Xiks=0i=1,…,NWe select one of the cuts for the LPR using the following rule.Rule. Use Cut (16) if m(A1) ≥ m(A2) – 1, else use Cut (17)The rule selects the cut that prevents more assignments, hence the stronger one.We hereafter refer to the LPR using the cuts (12) through (17) as strengthened LPR. UB2is the total net revenue returned by the strengthened LPR.Upper Bound 3 (UB3): To obtain UB3, we use the idea used in Model II. Recall that Model II aims to satisfy all minimum release quantities (∑idi) and minimizesY=∑s=1SYs. Here in place of Y, we use LB(Y), to get rid of the Ysbinary variables.First, we find LB(Y), and then increase it one by one, until a feasible solution is reached. Theorem 5 states LB(Y).We let dnewi=Max{di,Maxj|i∈ANDj{dj}}. Note thatdnewiis the minimum release quantity of task i which is modified by its minimum release quantity and the minimum release quantities of all its AND successors.Theorem 5LB(Y)=⌈∑i=1nti*dnewiK*C⌉is a valid lower bound on the number of units to satisfy all minimum release quantities.ProofThe total processing required to produce all minimum release quantity is∑i=1nti*dnewiEach unit can be disassembled in no more than K*C time units as each workstation is available for C time units.⌈∑i=1nti*dnewiK*C⌉is the number of units disassembled to satisfy all minimum release quantity if task splitting between the disassembly units and workstations were allowed. As no task splitting is allowed⌈∑i=1nti*dnewiK*C⌉is a lower bound on the number of units disassembled.□We illustrate Theorem 5 and UB3on the network in Fig. 4 with K = 4, S = 80 and C = 63. The minimum release quantities, task times and dnewivalues are given in Table 1.Using Theorem 5 we findLB(Y)=⌈13*4+7*4+⋯+19*44*63⌉=3. We first set the number of units to LB(Y) = 3. However a feasible solution for the LPR is not found. There is no feasible solution with 4 units as well. The LPR finds a feasible solution with 5 units. Then the minimum number of units to satisfy all minimum release quantities, Y = 5. Model III is used to find the line balance for the remaining 75 units. Formally we let,ZA= maximum net revenue for the strengthened LPR problem with Y unitsZB= maximum net revenue for a single unit problem with Model III.UB3=ZA+(S−−Y)*ZBIn our example, the strengthened LPR finds ZAas 1626. Model III finds ZB ,as 326. Then UB3= 1626 + (80 – 5) * 326 = 26.076.Note that here, UB2 = UB3. ZA is found much easier than the LPR with S = 80. Moreover ZB is found easily as it considers a single unit. Hence one should expect to obtain UB3much quicker than UB2.For this instance we find that UB1=26.796 and the optimal net revenue = 26.076. Note that UB3,improves UB1by 720 money units and catches the optimal solution in considerably small solution time.The lower bound uses UB3as a stepping stone to find a feasible solution. The motivation to use UB3is its satisfactory behavior in terms of its solution quality and time. The bound first solves UB3, then fixes its variables that are assigned to 1 and obtains a reduced problem. The reduced problem has only the partially assigned or unassigned tasks of the UB3solution. As the partial assignments are quite low compared to the original variables, the reduced problem is very small in size. Since our MILP solver can handle small-sized problems very quickly, we prefer to use it to find an optimal solution for partially assigned tasks.After fixing the variables to 1, the resulting solution obtained by MILP may violate the cycle time constraints or the precedence constraints. If the cycle time constraint is violated, then we increase the number of disassembly units one by one, until we obtain a feasible solution. If the precedence relations are violated, the tasks whose fixings cause violation, are set to zero, and the MILP is solved again.Below is the stepwise description of our lower bounding procedure.Step 0. LetLB(Y)=⌈∑i=1nti*dnewiK*C⌉and let T = LB(Y)Step 1. Solve the LPR with cuts for T units of the disassembly product.Fix the variables that have value ‘1’ in the LPR solution.Step 2. Find the optimal solution by Model I for the non-fixed tasks.If the solution is feasible then let ZCbe the objective function value and go to Step 4.Step 3. If infeasibility is due to the cycle time constraints then let T = T+1, go to Step 1.If infeasibility is due to the precedence relations then set the variable of a task to zero if one of its predecessor tasks is not assigned to ‘1’, go to Step 2.Step 4. Find an optimal solution to the single unit problem using Model III.Let ZDbe the objective function valueThe heuristic gives a total net revenue of ZC + (S – T) * ZD.In this section the data generation scheme is discussed and the computational results are evaluated. Two networks from the previous works are used for network generation.1st Network (22 tasks).Bourjault (1984) illustrated a disassembly network for a 10-part ballpoint pen. We adapt this network by replacing the successor OR relations with successor AND relations and give the resulting network in Fig. 4.2nd Network (34 tasks).Lambert (1997) illustrated a disassembly network for a 20-part radio. We adapt this network by replacing the successor OR relations with successor AND relations and give the resulting network in Fig. 5.Three networks are found by extending the small networks using additional arcs.3rd Network (47 tasks). 2nd network and a part of the 1st network are combined with some arcs added.4th Network (60 tasks). 3rd network and a part of the 1st network are combined with some arcs added.5th Network (73 tasks). 4th network and a part of the 1st network are combined with some arcs added.The following two sets for the number of workstations K are used.SetK1.K=2forsmallnetworkswith20,34,47tasksSetK1.K=4forlargenetworkswith60,73tasksSetK2.K=4forsmallnetworkswith20,34,47tasksSetK2.K=8forlargenetworkswith60,73tasksThe following two sets for the cycle times, C are used.SetC1.C=⌈∑tiK⌉SetC2.C=1.5*⌈∑tiK⌉The processing times are generated from a discrete uniform distribution [1, 20]. A task receives a minimum release quantity, hence called a part-releasing task, according to a random process. We generate a random number between 0 and 1. If the generated number is below 0.3, we set its minimum release quantity to zero.Two distributions are used to generate their minimum release quantities.Set D1. Minimum release quantity is uniform [1, 5]Set D2. Minimum release quantity is uniform [5, 10]Note that Set D1 contains instances with a low minimum release quantity, and Set D2, with a high minimum release quantity.The number of units, S, is set to 80. The costs are generated from a discrete uniform distribution between 5 and 20 and the revenues are generated from a discrete uniform distribution between 10 and 50.There are 40 combinations due to 5, 2, 2 and 2 alternatives for N, K, C and D respectively (3*2*2*2 = 24 for small networks and 2*2*2*2 = 16 for large networks). For each combination, 10 problem instances are generated. That is, a total of 400 problem instances are used in our experiments.The mathematical models are solved by CPLEX 10 and the algorithms are coded in Microsoft Visual C++ 2008. The experiments are run in Intel Core2 Duo 2.00 gigahertz, 2 gigabyte RAM.The execution of the MILP (Model I) is terminated if the optimal solution is not returned in 3600 seconds. The optimal solutions are found within 3600 seconds when N = 22, 34 and 47. However, optimal solutions were not found when N = 60 and 73 .First the performance of our upper bounds is studied. The deviation of an instance is found as%Dev=(UBi−OPTOPT)×100where OPT = the optimal net revenue and UBi= the total net revenue by upper bound i. The average and maximum deviations and number of fractional variables are reported in Table 2 and Table 3, for UB1and UB2, respectively.As can be observed from the tables, the deviations are consistently low over all problem sets. For UB1, the average deviations are below 1 percent and almost all maximum deviations are below 3 percent when N = 22, 34 and 47. Note that the deviations do not deteriorate with an increase in N. However they deteriorate with an increase in K due to the inflation of the number of binary variables. When N and K values are fixed, the minimum release quantities and cycle times do not affect the deviations significantly.UB2produces slightly smaller deviations due to the power of our cuts. For example, the maximum deviation for UB1is 3 percent, whereas it is nearly zero for UB2in sets K1, C1 and D1 for N = 22. We also give the number of fractional variables in Tables 2, 3 and 4. The number of the fractional variables produced by the LPR is too high. For example when N = 22, for sets K2, C2 and D2 (i.e., 4 workstations, high minimum release quantity, and high cycle time cases), 3306 out of 7040 variables are found to be fractional. This value reduces to 412 when cuts are added. On the average, the cuts reduce the number of fractional variables from 1900.4 to 180.3. The number of fractional variables is generally affected by the N values. For example, from Tables 2–4, it can be seen that for sets K2, C2 and D2, increasing N increases the number of fractional variables of UB2, on the average. Note that the average number of fractional variables are 1900.4, 3102.6 and 4654.9 for N = 22, 34 and 47 respectively.It can also be observed from the tables that, for fixed N, an increase in the number of workstations increases the number of the fractional variables. For example for N = 22 and sets C1 and D1, the average number of fractional variables is 722.4 when K = 2, and 2437.5 when K = 4. This is due to the fact that more workstations lead to higher task splits, leading to more fractional variables. For fixed N and K, we observe that increasing C reduces the number of fractional variables (with few exceptions). For example, when N = 47, K = 2, and D1 combination, the average number of fractional variables are 1184 and 584.2 for low and high C, respectively. This is due to the fact that increasing C gives more room for complete task assignments, hence reducing the number of fractional variables. In our experiments, we do not observe a significant effect of the minimum release quantity figures on the number of fractional variables.Table 4reports the number of different line balances incurred in a period, while S units are disassembled, for small networks.Note that the number of different line balances returned by Model II is quite small relative to the total number of disassembly units. For D1, there are about 5 and 6 different line balances. Increasing D from D1 to D2, almost doubles the number of different balances, as more units have to be used to satisfy the minimum release quantities. UB3is based on the idea of the different line balances, hence one should expect that it runs much faster than UB2.The solution times are measured in central processing unit (CPU) seconds. The average and maximum CPU times for our three upper bounds are given in Table 5and Table 6for small and large networks, respectively. Table 5 includes the CPU times of the MILP, as the small networks could be solved to optimality.As can be observed from the tables, the upper bounds are produced very quickly. Compared to UB1, UB2is computed more quickly, due to the power of the cuts, i.e., their effectiveness in reducing the solution space.Moreover, compared to UB2, UB3is computed more quickly as relatively fewer units, hence fewer variables, are used by the LPRs. For example, for N = 22 and the instances in sets K2, C2 and D2, the CPU time decreases from 0.59 to 0.25 seconds by adding the cuts, and similarly, the CPU time decreases from 0.25 to 0.08 seconds by using up to 10 supply units instead of 80. The CPU times of finding the upper bounds increase with an increase in N and K. This is due to the increase in the dimensions of the linear programs. For example, for sets K1, C2 and D2, as N increases from 60 to 73, the average CPU times of UB1, UB2and UB3increase from 3.18, 0.72, 0.11 to 4.52, 0.83, 0.13 respectively.For N = 34 and sets C2 and D2, as K increases, the average CPU times increase from 0.43, 0.08, 0.06 to 1.18, 0.31, 0.09 seconds for UB1, UB2and UB3respectively.For fixed N, K and D values, an increase in the C value decreases the CPU time. This is due to the fact that for large C, more tasks find an assignment for a workstation, and hence assignment decisions are produced easier by linear programs. For example, for N = 34 and Sets K2 and D2, increasing C from C1 to C2, decreases the average CPU times from 1.67, 0.46, 0.11 seconds to 1.18, 0.31, 0.09 seconds for UB1, UB2and UB3, respectively. When the other parameters are fixed, the increase in the D value increases the CPU times slightly. This is because when D increases, the number of units with different assignments increases. For example, for N = 34 and sets C1 and K2, increasing D from D1 to D2, increases the average CPU times from 1.27, 0.44, 0.08 seconds to 1.67, 0.46, 0.11 seconds for UB1, UB2and UB3respectively. The effect of K is more dominant for the MILP, due to the inflation of the number of decision variables.For fixed N, an increase in the K value increases the CPU times remarkably. For example for N = 22 and sets C1 and D1, an increase in K from K1 to K2 increases the CPU time from 3.83 seconds to 1078.32 seconds. However the effect of N is not as significant as that of K. For example, for K = 2, when N increases from 22 to 34 for sets C1 and D1, the CPU time increases from 3.83 to 4.37 seconds. Note from Table 5 that for fixed N and K, an increase in the C value decreases the CPU times remarkably. This is due to the fact that a workstation has room to accommodate many tasks, thereby leading to easier decisions. However D values do not have a consistent effect on the solution time of the MILP.Fig. 6illustrates the CPU times of UB1for sets K, C and D when N = 60 and 73.As can be observed from Fig. 6 the CPU times of UB1increase with increases in N and K values and decrease with an increase in C value.To summarize, our experiments have revealed that high quality upper bounds could be obtained even for large networks in less than half a minute. The upper bound deviations are not affected from the increases in the problem sizes and CPU times to obtain them increase at a linear rate. On the contrary, the solution times of the MILP model increase at an exponential rate and the model cannot find any solution in one hour, for large networks. Hence our upper bounds could be used in place of the optimal solutions to evaluate the performances of the lower bounds that are used to find feasible solutions.We finally investigate the performances of the lower bound. For small networks with 22, 34 and 47 tasks, the optimal solutions are found by the MILP and the deviations are measured relative to the optimal solutions, i.e.,PercentDev=(OPT−LBOPT)×100.For large networks, the optimal solutions are not available and the deviations are measured relative to UB2in place of optimal solutions, i.e.,PercentGap=(UB2−LBUB2)×100.The average and maximum deviations of the lower bounds are reported in Table 7.Note from the table that the lower bound solutions are quite satisfactory over all problem sets. The solutions have small deviations and are obtained in negligible times. The performance does not deteriorate with an increase in N. However the deviations slightly increase with an increase in K. For example for N = 60 and sets C1 and D2, the average deviations of K1 and K2 are 1 percent, whereas the maximum deviations are 1 percent and 2 percent for K1 and K2, respectively. The CPU times to find the lower bounds increase with an increase in N and K with only a few exceptions. This is due to the fact that the linear and mixed-integer linear programs have more decision variables for higher N and K. For example, for sets K1, C2 and D2, as N increases from 60 to 73, the average CPU times increase from 0.13 to 0.16 seconds. For N = 47 and sets C2 and D2, increasing K from 2 to 4, increases the average CPU times from 0.12 to 0.15 seconds.We could not observe any notable effect of C on the lower bound performances and CPU times. For fixed N, K and C, increasing D, does not change the performance, but increases the CPU times slightly. This increase can be attributed to the increase in LB(Y). For example for N = 60 and sets C1 and K2, the average CPU times are 0.20 seconds and 0.41 seconds for set D1 and set D2, respectively.To summarize, our computational experiment has revealed the excellent performances of the bounds that do not deteriorate as the number of tasks increases. On the other hand, the optimal total revenue values increase significantly with the increases in the number of the tasks. Fig. 7shows the optimal total net revenue values for each n, over all other parameter combinations, for small networks. For the large networks, the optimal solutions, hence their total net revenue values are not available.Note from the above figure that the average optimal total net revenue values are around 30, 45 and 63 money units for 22, 34 and 47 tasks, respectively. The respective maximum total net revenue values are 37, 54 and 78 money units and the respective maximum lower bound deviations are around 1 percent, 2 percent and 1 percent. These altogether imply that the practitioners that use the results of our study would lose no more than 1000 money units over the optimal total net revenue that are of magnitude of around 50.000 money units.

@&#CONCLUSIONS@&#
This study considers disassembly systems that have gained significant importance in recent years. This heightened importance stems from the recognition of environmental issues and the advances in manufacturing technologies. A disassembly line balancing problem, where the disassembly line is configured with defined workstations, is studied. The units of the product to be disassembled are identical and the parts have defined minimum release quantity, cost and revenue. It is assumed that the amount released excess of minimum release quantity can be sold at the end of the period. The aim is to assign the tasks to the workstations so that the total net revenue is maximized.A MILP model that could solve the problems with up to 50 tasks, is developed. For larger sized instances, we propose upper and lower bounds that are motivated by the satisfactory behavior of the LPR. The LPR is strengthened by same valid cuts.The experimental results have revealed that the bounding mechanisms produce high quality solutions very quickly. For the maximum trial size of 73 tasks, there is a gap of less than 5 percent between our lower and upper bounds.To the best of our knowledge, the study is the first attempt to solve the DLBP with minimum release quantities and fixed number of workstations. The proposed model assumes that all units of a specified part bring the same revenue. The presented methods can be directly applied when the unit revenues of the first level (minimum release quantity) and second level (excess of minimum release quantity) are different. Note that as the minimum release quantities are fixed, the contribution of the first level's total revenue to the objective function is fixed, hence irrelevant for the model solutions.We hope that the results of our study will trigger some advances in the disassembly literature. Some extensions of the study may consider the Successor OR type precedence relations, non-identical products (different units of the disassembly product with different parts) and a stochastic nature of the disassembly outcome (some parts may be found to be defective or damaged during disassembly).