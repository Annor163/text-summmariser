@&#MAIN-TITLE@&#
Eye-state analysis using an interdependence and adaptive scale mean shift (IASMS) algorithm

@&#HIGHLIGHTS@&#
A novel technique integrates a mean shift tracking algorithm with an adaptive scale scheme.Non-rigid eye movement evaluated from successive frames using eye position on face and distance between the eyes.The design focuses primarily on aiding human fatigue detection systems.A specially developed facial fatigue database was used to aid and test the development described in the paper.

@&#KEYPHRASES@&#
Mean shift algorithm,Eye tracking,Eye state analysis,Adaptive scale,Fatigue assessment,

@&#ABSTRACT@&#
Eye state analysis in real-time is a main input source for Fatigue Detection Systems and Human Computer Interaction applications. This paper presents a novel eye state analysis design aimed for human fatigue evaluation systems. The design is based on an interdependence and adaptive scale mean shift (IASMS) algorithm. IASMS uses moment features to track and estimate the iris area in order to quantify the state of the eye. The proposed system is shown to substantially improve non-rigid eye tracking performance, robustness and reliability. For evaluating the design performance an established eye blink database for blink frequency analysis was used. The design performance was further assessed using the newly formed Strathclyde Facial Fatigue (SFF) video footage database11The Strathclyde Facial Fatigue (SFF) video footage database was developed in collaboration with the Psychology Department, University of Strathclyde and the Sleep Centre, University of Glasgow, and it was approved by the Ethics Committee of the University of Strathclyde.of controlled sleep-deprived volunteers.

@&#INTRODUCTION@&#
This paper introduces a new approach which is capable of tracking and quantifying eye state in real-time. Through the innovative use of an adaptive scale mean shift tracking algorithm, the resulting system is also shown to be scale invariant. In general, eyes’ state or blink detection methods can be classified into three categories: (i) knowledge-based methods [1] which encode human knowledge regarding the eye state; (ii) feature-based methods [2] which employ features from the eye such as colours, shapes, and edges; and (iii) appearance-based methods [1] which use the competency of machine learning algorithms to evaluate the eye state from specifically extracted features. There are several types of classifiers commonly deployed in these applications such as boosted classifiers, Support Vector Machines (SVM) and Neural Networks.Knowledge-based methods typically use template matching algorithms using pre-prepared eye model templates. Chau et al. [3] and Wu et al. [4] created the open eye template, where the eye is already located, and they used it to track the eyes and the correlation of the template regions in between two frames. The state of the eye is determined by the computed correlation with a high correlation score signifying an opened eye and a low score for a closed eye. However, this approach has difficulties when it deals with multi-size of eyes and multi-pose of faces. In papers [5,6] statistical active appearance shape models are utilised in order to remedy the face multi-pose problem. This technique requires a set of the annotated eye open and closed images, which is applied in a statistical model training algorithm. The experimental results demonstrate that this technique is robust to the face multi-pose issues. Nevertheless, this technique encounters problems when it deals with face orientation changes such as face look up with partial head rotation.The eyes have various features such as colour, texture and edges that can be utilised to evaluate the eye state. For instance, Liying et al. [7] use of skin colour to distinguish between eye open and close states [8,9], exploits the changes of intensity within the eye region as a method to detect the blink. Moris et al. [10] compute the variance map of the eye in order to obtain the distinctive features of the eye state. However, these approaches are particularly sensitive to illumination variations. Fast eye state detection algorithms reported in [11–13] analyse the movement of the eyelid using an optical flow algorithm. These techniques are effectively detecting the movement of the eyelid, but they are sensitive to face movements. Techniques that are based on appearance and use machine intelligence require a large number of images of eye states to train the classifiers. For example, Wang et al. [14] extract the eye features using the Gabor wavelet technique and classify them through Neural Networks, whilst in [15,16] a closed eye state is determined using the Adaboost classifier [43]. Research in [17] determines the eye state based on rectangular eye position pixels; this position is trained and classified using SVM. The performance of this type of method relies on the features and the number of images available for training.The mean shift tracking algorithm is a popular method of object tracking that is based on the probability of colour histogram [18,19]. This algorithm has been successfully applied to track distinctive colour objects in a variety of background scenes [20–22]. In the eye tracking technique, there are several examples which have applied mean shift to track the eye [23–25]. However, these eye tracking techniques are specifically based on the coloured region of the eye and lack any mechanisms to measure the eye state.In this paper, a novel technique is introduced that integrates a conventional mean shift tracking algorithm with an adaptive scale scheme. This results in a system which improves on previous approaches as it is also able to measure the state of the eye during tracking in a scale invariant way. The resulting interdependence and adaptive scale mean shift (IASMS) algorithm tracks non-rigid eye movement by evaluating the eye position on the face and the distance between the eyes from successive frame sequences. The remainder of this paper is organised as follows: Section 2 provides an overview of the complete IASMS based eye state analysis system; Section 3 explains the IASMS algorithm in detail; Section 4 discusses the obtained experimental results; conclusions to the paper are provided in Section 5.The overall design, depicted in Fig. 1, presents four algorithm stages. The first stage is a front face detection process. The output of this stage contains distinct face features which are useful for estimating the position of the eyes, nose and mouth. The locations of the eyes are evaluated using a profile of intensity values projection technique as implemented in [26,27]. Following this, a classification stage is used to decide whether the eye is opened or closed. The third stage is an iris detection and localisation process. By implementing eye-open classification, the number of false-positive errors in the iris localisation algorithm is shown to be substantially reduced. The iris localisation process produces the location and size of the iris which is then used for tracking and quantifying the eye state in the IASMS algorithm. In the front face detection step of the design a hybrid method that comprises an association of the appearance based method with the features based method, is applied. This is a combination of a skin colour segmentation technique, connected components of binary images and a state of the art learning machine classifier [28,29] as indicated in Fig. 2. The skin colour segmentation method uses the combination threshold of multi-colour space value. In this approach, RGB, YCbCr and HSV colour spaces have been utilised. The output of a segmented colour skin image is in binary format (Fig. 2(a)), and consists of eight connected components for each of the segmented skin regions. From these components, a rectangular box is formed (Fig. 2(b)) in order to create a specific region of interest to be subsequently processed. Each of the rectangular boxes is then examined for shape and size to ensure that only boxes likely to contain face information remain for the next process. As outlined in [28] this is achieved using a Support Vector Machine (SVM) classifier and the horizontal projection features which, for a face, have a distinctive pattern.The final step involves applying a Viola Jones [30] classifier on the remaining rectangular boxes to evaluate whether a box contains face information or not. This classifier is an integration technique between the AdaBoost learning algorithm [43] and a cascade classifier. This technique extracts the features vectors using a Haar-like features technique from integral images [30]. The hybrid method that is employed obviously can reduce the number of false-positive errors that normally occur in a complex background image.When the front face is detected, the process to evaluate facial features is easier compared to the multi-poses face detection algorithm where not all facial features are visible. In such cases, the location of the eyes is identified from horizontal and vertical profile projections of the front face. When a face is detected, as illustrated in Fig. 3, the horizontal grayscale profile is projected in order to obtain the region of the eyes. Then, the locations of the eyes are obtained by projecting the vertical profile of the eyes region.Once the eye location is determined, the status of the eyes is examined, i.e. whether the eyes are open or closed. This stage is important to ensure the eyes are open initially before the irises are detected. In our approach a new technique is used which comprises an adaptive threshold value and a connected component binary image to classify the status of the eyes. Firstly, the input image is enhanced g′(x, y) by transforming the histogram of the image to spread the level of colour values evenly but without changing the shape of the input histogram. This is computed as follows [31]:(1)g′(x,y)=G′max−G′minGmax−Gmin[g(x,y)−Gmin]+G′minThis linear transform, illustrated in Fig. 4, stretches the histogram of the input image into the desired output intensity level G′ of the histogram betweenG′minandG′max, where Gmin and Gmax are the minimum and maximum input image intensities G respectively. From this enhanced histogram, the cumulative histogram Hiis calculated as follows:(2)Hi=∑j=1ihjEq. (2) generates histogram bins between 0 and 255. The specific region that is extracted from the profile projection contains the eye and the eyebrow. The darkest area in this region is from the iris, the eyebrow and, also, the eyelashes. It has been experimentally observed that, on average, the darkest area constitutes approximately 10% of the entire obtained region of interest. Based on this percentage, the darkest area in the histogram is situated between 0 and X intensity levels. The intensity value X can thus be obtained by determining the area of the first 10% of the total number of pixels, contained in the enhanced histogram image (Fig. 5(a)) and reading the corresponding intensity value off the x-axis.(3)0<X<255Then, the adaptive threshold Tavalue is calculated as follows:(4)Ta=X−PminPmax−Pminwhere Pmax and Pmin are the maximum and minimum pixel value respectively.The input eye region is then converted to a binary image using the adaptive threshold as shown in Fig. 6(a) and (b). The connected component of the binary image is utilised to form a bounding box in the region of interest as shown in Fig. 6(c). The characteristic of the region of interest in this bounding box is examined in terms of the aspect ratio of the bounding box and the area of region of interest within it. Firstly, the aspect ratio of the bounding box shape S of height (H) and width (W) is calculated using (5). Based on experiments involving 420 eye closed images taken from ZJU [15], MMU [32], CASIA [33], and the Strathclyde Facial Fatigue (SFF) databases, the best ratio of the bounding box that represents eye closed was experimentally found to be less than 0.4. When this condition is not met the region of interest in the bounding box needs to be examined further.(5)S=0HW<0.41otherwiseIn case further examination of the bounding box region is required, i.e. when S=1 in (5), the difference between eye open and eye closed is based on the area of the region of interest within the bounding box. A high percentage in the value of the region of interest, relative to the size of the bounding box, indicates that the eye is open, otherwise the eye is closed. Based on experimental results using 420 eye closed images, it was empirically evaluated that if the area of region of interest, as shown in Fig. 6(c), is less than 70% of the bounding box area then the eye is considered to be closed.Tracking the irises and quantifying their area are important tasks in the described process. To ensure an accurate location of the irises and best possible performance for the IASMS process a Daugman's iris localisation method [34,35] is used. Daugman [34] introduced an Integro-Differential Operator to detect the boundary of the iris and the pupil. This technique performs circular edge detection by convolving the image with a Gaussian operator in order to smooth the image so that the edge information is highlighted. The operator structure is:(6)max(r,x0,yo)Gσ(r)∗∂∂r∮r,x0,y0I(x,y)2πrdswhere x0 and y0 are the coordinates of the iris centre, and r is the iris radius. I(x, y) is the input iris image smoothed by the Gaussian function Gσ(r). The smoothed image is scanned for searching a circle edge that has a maximum gradient change. This maximum gradient represents the boundary of the iris which is used, in turn, to find the centre of the iris. This final information is then delivered to the IASMS algorithm.The interdependence and adaptive scale mean shift (IASMS) algorithm has been designed specifically for tracking and quantifying the iris size in order to analyse the state of the eyes. This algorithm is a main contribution to this paper and presents a hybrid of a conventional mean shift tracking algorithm, and an adaptive scale technique, which utilises moment features and an interdependence tracking scheme based on the position of the eyes.The mean shift algorithm, introduced by Comaniciu et al. [18], is a popular method for tracking due to its simplicity and efficiency. The mean shift algorithm tracks the object based on the probability of colour of the object. In this paper, the mean shift tracking algorithm is employed to track the non-rigid iris movement wherein the iris is tracked based on the probability of the colour histogram of the iris. The primary task of the mean shift tracking algorithm is to find the target location in the current frame. Comaniciu et al. in [18] have derived a new location targeted y which can be computed as follows:(7)y=∑i=1nxiwi∑i=1nwiwhere xiis a normalised pixel location from 1 to n with the target iris centred at 0 and the weight wiis computed as:(8)wi=∑u=1mqˆupˆu(y0)δ[b(xi)−u]pˆ(y)is an estimation of the Bhattacharyya coefficient, m is the number of histogram bins,qˆuis a probability of the colour histogram, δ is a Kronecker delta function, b(xi) is a function determines the bin for pixel xi, and u is a histogram bin value.The adaptive scale scheme is an essential part in this algorithm in order to quantify the size of the iris. The scheme allows the iris size to be quantified at the same time as the eye movement is tracked. The conventional mean shift tracking algorithm was designed for static colour distributions meaning that the target search region remains unchanged relative to variations in object size. However, Bradski [36] has extended the mean shift algorithm in order to deal with dynamic changes in colour probability distribution, and this technique is the continuously adaptive mean shift (CAMSHIFT) algorithm. The algorithm exploits the moment features, which represents a global description of shape to obtain the area, height and width of the object. The area can be measured by finding the zero-th moment of the region which is computed as follows:(9)M00=∑x,yI(x,y)When the area of the object is obtained, in this case the iris area, the centroid of the object can be recovered using a first-order moment which is computed as follows:(10)M10=∑x,yxI(x,y)M01=∑x,yyI(x,y)M11=∑x,yxyI(x,y)where I(x, y) is the probability pixel value within the object region in x and y range. The centroid point (xc, yc) of the region is then accomplished as:(11)xc=M10M00,yc=M01M00In the mean shift tracking algorithm essentially there are two forms of the search tracking region, rectangular and ellipsoidal. The form of the target search region can significantly impact the performance of the mean shift Tracking algorithm. One of the major challenges faced when using the mean shift algorithm is the probability that the colour of the search region does not represent the exact probability of the tracked object [37]. Commonly, the tracked object is not precisely allocated in the search region form because, due to its shape, the background scene region is included in the search region form and this contributes noise in the probability colour of the search region. However, in this application, an ellipsoidal search region is employed. This search region form is appropriate with eye shape characteristics, thus, the aforementioned problem can be reduced.In the CAMSHIFT algorithm, the target location of the object is estimated using a weighted image which is determined using the hue-based colour histogram. The hue colour is unstable at low saturations which in certain to below the threshold of pixels are not included in the colour histogram. In this algorithm, by referring to the results from the analysis carried out in [20], the weighted image from Eq. (8) is utilised to quantify the scale of the iris. Therefore, the value of the probability pixel I(x, y) in (9) and (10) can be changed to the weight of the pixel wi(8).In order to obtain an accurate size of the iris, the width, height and also orientation must be acquired by computing the second order moment features as follows:(12)M20=∑x,yx2w(x,y),M02=∑x,yy2w(x,y)From the second-order moment, the rotation of ellipse search target region can be described by calculating the second-order centre moment as follows:(13)μ20=M20M00−xc2,μ02=M02M00−yc2,μ11=M11M00−xcycThe degree of orientation of the ellipse of the search target region can be derived by employing the second-order centre moment as follows:(14)θ=12tan−12μ11μ20−μ02Ideally, this algorithm quantifies the eye's state based on the iris area. However, the actual region of the iris is difficult to compute in the searching region of the algorithm. Therefore, the iris area is approximated by the size of an elliptical search area. The ellipse of the search area can be calculated when the width and height of the ellipse are obtained. The semi-major axis of the ellipse a and the semi-minor axis of the ellipse b are produced as follows:(15)a=l1Aπl2,b=lA2πl1where A is the area of region computed from the zero-th moment (9), while l1 and l2 are calculated as follows:(16)l1=(μ20+μ02)+4μ112+(μ20−μ02)2(17)l2=(μ20+μ02)−4μ112+(μ20−μ02)2The common mean shift tracking is based on single tracking, which is independent of other objects. However, in this application, there is a pair of eyes and, therefore, interrelated. By utilising information from the pair of eyes, we will incorporate an interdependence technique that aims to track the non-rigid movement of the eyes. There are two primary pieces of information employed in this algorithm, the centre locations of the irises and the distance between them. Initially the eye centres (18) are obtained from the pre-processing algorithms which will be discussed in the following section. Then the mean shift algorithm updates these centre positions for every tracking iteration. The Crand Clin (18) denote the centre points of the right and left eye respectively.(18)Cr=(xr,yr)Cl=(xl,yl)From the obtained centre points of the irises, the distance Debetween them is continually updated in every tracking iteration, and is calculated as follows:(19)De(p,q)=(xr−xl)2+(yr−yl)2The common drawback of the mean shift tracking algorithm is erroneous colour tracking. When dealing with colour, there is the possibility of multiple objects having similar colours. In this case, in order to avoid any erroneous results the following new approach is introduced. First, a specific eye region is defined on the image known as the Focus of Eye Region (FER) with the size and location of the region updated during each iteration. Then the mean shift algorithm is used to track the iris in the FER region of the image. The size of the FER depends on the centres of the irises Crand Cland the distance between them Dein the following way:(20)IFER(xR,yR)=I(xr,yr)xCr,l−13De≤xr,l≤xCr,l+13DeIFER(xL,yL)=I(xl,yl)y−C,l13De≤yr,l≤yCr,l+13DeOccasionally the target object can be occluded; in this case the eye is usually occluded by the hand. To enable system continued evaluation when occlusion or wrong tracking occurs, the sum of absolute difference (SAD) value of FER in between two frames is obtained as indicated next:(21)ISAD=∑i,jI1(i,j)−I2(x+i,y+j)The input image is converted to grayscale to obtain the normalised values. The SAD value is normalised and computed as follows:(22)InSAD=ISAD255×W×Hwhere W and H denote the width and height of FER respectively. The value of InSADis examined in order to ensure that the eyes are always tracked accurately. Fig. 7shows a typical sequence of frames from a normal eye opening until the eye is occluded. As indicated in Fig. 1 the differences of SAD in FER of the eye opened and the eye closed does not indicate significant changes from frame 1 to frame 60 (Fig. 7). However, when the eye is totally occluded in frame 68 the normalised SAD value for the FER frame 68 increases rapidly (Fig. 8). This significant change is used as an indicator to stop the tracking. The steps involved in the complete process are given in Table 1.

@&#CONCLUSIONS@&#
