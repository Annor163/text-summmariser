@&#MAIN-TITLE@&#
Decision-network polynomials and the sensitivity of decision-support models

@&#HIGHLIGHTS@&#
We study the sensitivity of decision-support models.We introduce the notion of decision-network polynomials.The absence of differentiability at indifference points is investigated in detail.We utilize finite change sensitivity indices for providing managerial insights.The representation of certainty equivalents is discussed.

@&#KEYPHRASES@&#
Decision analysis,Influence diagrams,Decision trees,Bayesian networks,Sensitivity analysis,

@&#ABSTRACT@&#
Decision makers benefit from the utilization of decision-support models in several applications. Obtaining managerial insights is essential to better inform the decision-process. This work offers an in-depth investigation into the structural properties of decision-support models. We show that the input–output mapping in influence diagrams, decision trees and decision networks is piecewise multilinear. The conditions under which sensitivity information cannot be extracted through differentiation are examined in detail. By complementing high-order derivatives with finite change sensitivity indices, we obtain a systematic approach that allows analysts to gain a wide range of managerial insights. A well-known case study in the medical sector illustrates the findings.

@&#INTRODUCTION@&#
Decision trees, influence diagrams, Bayesian networks and event trees support the solution of decision analysis problems in several applications. Their use is nowadays facilitated by a number of software programs (see Bielza, Gomez, & Shenoy, 2011; Jensen & Nielsen, 2007).1See also the associated website http://bndg.cs.aau.dk/ for a list of software programs.1Computer implementation enables analysts to develop sophisticated codes that incorporate a variety of aspects of the problems under investigation (Dillon, Paté-Cornell, & Guikema, 2003). However, model complexity exposes analysts and decision-makers to the risk of a partial understanding of the model input–output response. Then, deriving insights about the structure of the model becomes essential to make robust conclusions and inferences.Researchers have developed methods for exploring the informational content of decision-support models. For Bayesian networks several of the most recent findings rest on the fundamental result that the input-mapping in Bayesian networks is a multilinear polynomial (Castillo, Gutiérrez, & Hadi, 1996, 1997). For instance, multilinearity is crucial for arithmetic and decision circuits (Bhattacharjya & Shachter, 2012; Darwiche, 2003). However, there are unique challenges in sensitivity analysis for influence diagrams due to the non-linearities created by maximization operations for making decisions (Bhattacharjya & Shachter, 2010, p. 1). Indeed, this issue is transversal to all decision-support models that include a maximization (or minimization) operator. The maximization operation induces piecewise-definiteness and impairs differentiation. Thus, properties of Bayesian networks cannot be transferred directly to models such as influence diagrams, decision trees and decision circuits. The difficulties associated with differentiation then raise broader issues concerning the methodology for deriving managerial insights from decision-support models.In this work, we conduct a systematic investigation into the mathematical properties of decision-support models. We show that the decision-theoretical principles underlying their construction (Savage, 1954) lead to a piecewise defined input–output mapping. Each piece corresponds to an available strategy and is multilinear in probabilities and utilities. We call the mapping a decision-network polynomial.Non-differentiability occurs at those values of the model inputs (probabilities/utilities) for which the decision-maker is indifferent among alternative strategies. Using the terminology of Howard (1968) (see also Bhattacharjya & Shachter, 2010),2Drawing from the systems analysis literature, Howard, 1968refers to varying a parameter and computing the certain equivalent at a fixed strategy as open loop analysis; when the strategy is allowed to vary by re-evaluating the decision situation, it is closed loop analysis (Bhattacharjya & Shachter, 2010, p. 3).2this result suggests that differentiation finds its natural application in an open-loop analysis, i.e., when the preferred strategy (or any other strategy) is under scrutiny. In a closed-loop analysis differentiation might not be possible. We then employ sensitivity measures based on an orthogonal decomposition via finite difference operators (Borgonovo, 2010), that do not require differentiability. These finite change sensitivity indices allow us to obtain a clear understanding of the model response, by apportioning the change in expected utility to the individual effects and interactions among the model inputs.The next step is the analysis of the model structure when consequences are monetary and certainty equivalents are the output of interest. Findings show that the input–output mapping remains piecewise-defined but each piece is composite multilinear. We link high order derivatives of certainty equivalents polynomials analytically to the derivatives of the corresponding decision-network polynomials. Finite change sensitivity indices acquire a direct interpretation as the monetary gain (loss) associated with the model input variations.The well known case-study of Felli and Hazen (2004) helps us illustrating how the approach can be used for: (i) understanding direction of change, (ii) quantifying the relevance of interactions; and (iii) identifying the model inputs on which to focus managerial attention during implementation (Eschenbach, 1992, pp. 40-41).The remainder of the paper is organized as follows. Section 2 offers a literature review. Section 3 presents decision-network polynomials. Section 4 investigates the link between indifference and differentiation. Section 5 discusses the presence of interactions and specializes finite change sensitivity indices to the case of decision-network polynomials. Section 6 discusses results for certainty equivalents. Section 7 illustrates the derivation of decision-making insights through a case study in the medical sector. Section 8 offers conclusions. All proofs are in Appendix A.This section offers a concise overview of relevant literature on decision support models and their sensitivity analysis. For a broad overview of these models we refer to Bielza et al. (2011).Bayesian networks are among the most widely used decision-support models for the factorization of probability distributions. Their applications range from reliability analysis to genetics (Darwiche, 2010; Jensen & Nielsen, 2007). Technically, a Bayesian network is a directed acyclic graphG=(N,A), where N and A are the sets of associated nodes and arcs. N contains chance nodes. Each chance node represents a random variable. A contains arcs or edges, which join pairs of nodes. The lack of an arc signifies probabilistic independence, while the presence of an arc signifies a possible probabilistic dependence. A conditional probability table (CPT) is assigned to each random variable. Algorithms for evaluating Bayesian networks have been widely studied. We refer to Jensen and Nielsen (2007) and Darwiche (2010) for thorough overviews.A second class of decision-support models for the factorization of probability distributions are event trees. Event trees are often applied in reliability analysis in conjunctions with fault trees. Papazoglou (1998) offers a rigorous mathematical formalization. Marsh and Bearfiled (2008) show that there is always a unique Bayesian network corresponding to an event tree, but there might be multiple event trees corresponding to a Bayesian network. The event tree is unique once the Markovian assumptions of a Bayesian network are satisfied (Darwiche, 2010).Influence diagrams are Bayesian networks augmented with decision-nodes and value nodes, where value nodes have no descendant (Nielsen & Jensen, 2003, p. 223). Thus, the set of nodes is now partitioned into value, chance and decision nodes (Shachter, 1986, p. 874). Value (or utility) nodes conclude the diagram and display the decision-maker’s utility (or payoff) over consequences. Arcs evidence the flow of information, besides probabilistic dependence Shachter (1986, p. 417). As far as chance nodes are concerned, the corresponding random variables can be discrete (Howard & Matheson, 1981; Shachter, 1986, 1988) or continuous (Cobb & Shenoy, 2008; Shachter & Kenley, 1989). Influence diagrams fully retain the probabilistic meaning of Bayesian networks so that the various well established algorithms for Bayesian net evaluation can be used in influence diagram evaluation (Qi & Poole, 1995, p. 501). Bhattacharjya and Shachter (2010) provide a thorough review on solution methods for influence diagrams.The compact representation offered by influence diagrams does not allow us to appreciate the detailed combination of decisions and outcomes. To reveal them, we need to convert the influence diagram into the corresponding decision-tree. Under the single decision-makerand no-forgettingconditions (which we assume throughout this work) Howard and Matheson (1981), an influence diagram can be uniquely associated with a decision-tree. At the graphical level, a decision tree contains decision, chance nodes, branches and end nodes (or leaves). They display the combinations of outcomes and alternatives that lead to the end consequences. Any such combination is called a scenario. The size of decision trees grows exponentially with the number of nodes, which is one of their main limitations Bielza and Shenoy (1999).Sequential decision-diagrams (Covaliu & Oliver, 1995), unconstrained influence diagrams (Jensen & Vomlelova, 2002), limited memory influence diagrams (Lauritzen & Nilsson, 2001), valuation networks (Shenoy, 1992), sequential valuation networks (Demirer & Shenoy, 2006) extend the family of decision-support models.Given the complexity of decision-support models in practical applications, their sensitivity analysis plays a central role for extracting managerial insights. For Bayesian networks, we recall one-way, distance-based and differentiation based sensitivity analysis. One-way sensitivity analysis is the simplest type of analysis and it consists of systematically varying one of the network’s parameter probabilities while keeping all other parameters fixed (van der Gaag, Renooij, & Coupé, 2007, p. 104). The works of Castillo et al. (1996, 1997) obtain analytically the sensitivity function, namely the dependence of the posterior probability of interest on the network parameter under scrutiny. An intrinsic limitation of any one-way approach is its reliance on the variation of a single parameter. Chan and Darwiche (2005) extend the robustness question to simultaneous variations. They introduce a new metric for bounding global belief changes that result from either the perturbation of local conditional beliefs or the accommodation of soft evidence (Chan & Darwiche, 2005). Brosnan (2006) relies on Shannon’s entropy and the Kullback–Leibler divergence for answering the same questions. As to differentiation, Chan and Darwiche (2004) discuss the determination of the individual or simultaneous changes in Bayesian network-parameters that ensure the satisfaction of a query constraint using a differential approach. In Bayesian networks, partial derivatives can also be obtained by differentiating sensitivity functions; the result is called sensitivity value in van der Gaag et al. (2007, p. 104). In Blackmond-Laskey (1995) partial derivatives are used to focus elicitation efforts on the most important model inputs. Darwiche (2003) and Park and Darwiche (2004), augment differentiation with probabilistic semantics. They also introduce numerically efficient ways of obtaining first and second order derivatives by upwards and downward passes in arithmetic circuits.In influence diagrams, the same sensitivity questions can be asked, but we have two notable differences. The first distinction is marked by the need to consider decision sensitivity besides value sensitivity. That is, changing the model inputs does not only modify the value of the decision-problem, but may cause the decision-maker to change strategy. Robustness then becomes the problem of assessing the region in the input parameter space over which the optimal policy is invariant. The second distinction is marked by the presence of utilities in influence diagrams, which play the role of parameters together with probabilities. Nielsen and Jensen (2003) discuss thoroughly both value- and decision-sensitivity while performing one-way and n-way sensitivity analysis of influence diagrams with respect to (w.r.t.) the parameters. In particular, Nielsen and Jensen (2003) elaborate on the concept of admissible domain, studying the set of values over which a parameter/group can be varied without changing the optimal strategy. Bhattacharjya and Shachter (2008) discuss sensitivity analysis in influence diagrams based on decision-circuits (Bhattacharjya & Shachter, 2012). Bhattacharjya and Shachter (2008) address admissible intervals, one-way sensitivity and differentiation. Bhattacharjya and Shachter (2010) present three new ways for sensitivity analysis in decision circuits based on the knowledge of partial derivatives obtained by the decision-circuit encoding.A further widely used technique is represented by Tornado diagrams (Howard, 1988). Their construction is detailed in Eschenbach (1992). Tornado diagrams are graphical representations of a series of one-at-a-time sensitivities, in which a single model parameter is varied, with the others remaining fixed. From a Tornado diagram one infers sign of change and can rank parameters based on their impact on the value of the decision-problem. The sensitivity measures of Tornado diagrams have been recently generalized to include interaction effects in Borgonovo and Smith (2011).An analysis of the literature reveals also the following. First, one tends to transfer properties from Bayesian networks to influence diagrams directly. However, the properties of Bayesian networks that remain valid when we augment the network with decision-nodes have to be determined. Second, several sensitivity methods rely on differentiation. However, differentiability of decision-support models is not insured. Third, the most well studied methods consider individual parameter variations. However, also simultaneous variations are of practical interest.In the next sections, we provide an investigation towards filling these gaps; our first step is determining the mathematical form of the input output mapping.The purpose of this section is to obtain the formal representation of the input–output mapping in decision-support models starting from the theoretical principles of expected utility theory. In fact, decision support models are developed consistently with the normative perspective of decision-making, namely, the expected utility model ofvon Neumann and Morgenstern (1944)and the subjective expected utility model ofSavage (1954) (Smith & von Winterfeldt, 2004, p. 561).In this respect, we observe that the solution of influence diagrams and decision-trees requires that decision-makers are capable of fully specifying the utilities and probabilities needed to solve the associated decision problems. This assumption is transversal in the literature mentioned sofar and will be maintained throughout the reminder of this work. However, a wide research stream has addressed the solution of decision-analysis problems when decision-makers are unable to fully specify utilities and/or probabilities (see Hazen (1986) on potential optimality and Moskowitz, Preckel, & Yang (1993) for imprecisely assigned utilities and probabilities. Liesiö & Salo (2012) offer a thorough overview). The optimization problems a decision-maker faces might be different than the ones we are to address in this work. Their study is, however, outside the scope of the present work.For clarifying notation, we make reference throughoutto the running example in Fig. 1. In the decision-problem of Fig. 1, the decision-maker has to select between alternatives I and II, then, after having observed the realization of random variableC1, she has to choose between alternatives III and IV. If she chooses III, she is then faced with the realization of random variableC2. The decision-tree reveals asymmetries in the problem.In accordance with Savage (1954), a strategy is a function from the state space (Ω) to the space of consequences (Q):(1)m:Ω→QwhereQ=q1,q2,…,qQ. The decision maker associates a corresponding utilityuqwith each consequence. LetD=D1,D2,…,Dδrepresent the set of decision nodes. Leta(r)be the number of alternatives available at nodeDr,r=1,2,…,δ. A strategy is a sequence (collection) of alternatives that comprises exactly one alternative for each of the involved decision nodes. In a symmetric decision-problem the number of strategies equals the number of combinations of alternatives in the various decision nodes, namely∏r=1δa(r). However, asymmetries can reduce the number of available strategies, whose final number is here denoted by M. To illustrate, in the decision-problem of Fig. 1, we have 5 consequences,Q=q1,q2,…,q5, and three strategies,M=m1,m2,m3, withm1=Choose I atD1,m2=choose II atD1and then III atD2, andm3=Choose II atD1and then IV atD2.By Savage (1954), the decision-maker selects the strategy with the associated highest expected utility. Savage (1954) shows that this expected utility is, for each strategy m, a functional of the form:(2)Um=∑q=1QP(q|m)u(q),m=1,2,…,MwhereP(q|m)is the probability of consequence q given that strategy m is selected andu(q)the utility of consequence q. Savage’s expected utility functional is linear in both probabilities and utilities. Eq. (2) directly applies to single event problems. In applications, however, we mostly face multiple-event problems. Then, decision-support models provide the factorization ofP(q|m)(Kirkwood, 1993). LetC=C1,C2,…,Cγdenote the set of random variables (and corresponding chance nodes) in the model. The possible realizations ofCs(the random variable of node s,s=1,2,…,γ) are here denoted by{cs,1,cs,2,…,cs,μ(s)}. Savage’s state space is, then,Ω=C1×C2×…×Cγ.Fixed strategy m, one calls scenario leading to consequence q the complete sequence of chance node outcomes that, from the initial {alternative}/{chance node outcome}, leads to consequence q. In a symmetric problem, fixed m, we have∏s=1γbscombinations, i.e., there are∏s=1γbscomplete scenarios, wherebsis the number of outcomes of chance nodeCs. If chance asymmetries are present, not all these scenarios are present. Given a scenario involving k chance node outcomes,Ωjm={ωj1,ωj2,…,ωjk}denotes the set of realizations of the k random variables in the scenario. As a reference, in Fig. 1, we have two chance nodes,C1andC2, each with 2 realizations. Selecting the first strategy, due to the asymmetry, we have an unique scenario leading directly to consequenceq5. Selecting the second strategy, we have 3 scenariosΩ12={ω1,1=c1,1,c2,1}andΩ22={ω1,1=c1,1,c2,2}andΩ32={ω3,1=c1,2}, leading, respectively, to consequencesq1,q2andq4. Selecting strategym=3, we have two scenariosΩ13={c1,1}andΩ23={c1,2}, leading respectively toq3andq4.Then, coming back to Savage’s functional, by probability factorization we find(3)Um=∑q=1Q∑j:Ωjm∈OqmP(ωjk-1|ωjk-2,…,ωj1,m)·…·P(ωj1|m)uq,m=1,2,…,MEq. (3) shows that the expected utility of a given strategy under Savage’s (1954) framework is a multilinear function of probabilities and utilities see also Felli and Hazen (2004).We letx=(p,u)denote the set of conditional probabilities and utilities (the parameters). To illustrate, in our example, we have 4 probabilities,x1,…,x4and 5 utilities,x5=u(q1),=x6=u(q2),…,x9=u(q5). The resulting multilinear polynomials areU1(x)=x9,U2(x)=x1(x3x5+x4x6)+x2x8andU3(x)=x1x7+x2x8.Definition 1LetX⊆Rn. We call decision-network polynomial the mapping(4)U:X→Rdefined at eachx∈Xby(5)U(x)=maxm=1,2,…,M[Um(x)]whereUm(x)is the expected utility of strategy min a subjective expected utility problem.U(x)is the value of the decision problem and equals the expected utility of the preferred strategy asxvaries inX⊆Rn, where n is the number of model inputs given by the sum of the number of utilities (Q) and conditional probabilities (np), so thatn=np+Q. In addition, it isX=Xp×Xu, withXp=[0,1]np,Xu⊆RQ. If the von Neumann Morgenstern utilities are normalized between 0 and unity, thenUm:[0,1]n→R.Definition 2Let m be a given strategy (m=1,2,…,M). The open set(6)Xm=x:Um(x)>Uj(x),j=1,2,…,M,m≠jis called admissible domain of strategy m.Clearly, if, for no value ofx, strategy m is optimal, thenXm=∅. Otherwise,Xiis non-empty. In our example, we have three admissible regions defined by(7)X1={x:x9>x1(x3x5+x4x6)+x2x8∧x9>x1x7+x2x8}X2={x:x1(x3x5+x4x6)+x2x8>x9∧x1(x3x5+x4x6)+x2x8>x1x7+x2x8}X3={x:x1x7+x2x8>x9∧x1x7+x2x8>x1(x3x5+x4x6)+x2x8}Ifx∈Xm, thenU(x)is uniquely defined as equal toUm(x). However, for some values of the probabilities and utilities, we can have indifference among strategies. For instance, we can havex1(x3x5+x4x6)+x2x8=x9, orx1(x3x5+x4x6)+x2x8=x1x7+x2x8. Say that strategies m and k are equally preferred atx. In that case, we registerUm(x)=Uk. However, note that even ifUm(x)=Uk(x)atx,but these two strategies are less preferred than some other strategy, sayUl(x), thenxwould not be an indifference point for the decision problem: the value of the decision problem,U(x), would be equal toUl(x). Conversely, consider a group of k strategies denoted by indicesi1,i2,…,ik. Suppose that the decision-maker is indifferent among them. Then, it isUi1(x)=Ui2(x)=⋯=Uik(x)greater thanUl(x), where l denotes any other strategy not in the group(l∈{1,2,…,n}⧹{i1,i2,…,in}). To formalize these facts, we propose the following definition.Definition 3One calls indifference hypersurface of strategiesi1,i2,…,ikthe set∂Xi1,i2,…,ik=x∈X:Ui1(x)=Ui2(x)=⋯=Uik(x)=U(x)=maxm=1,2,…,M[Um(x)].and indifference frontier of strategy m the set(8)∂Xm=⋃k=2,m∈i1,i2,…,ikM∂Xi1,i2,…,ik∂Xmis the union of all indifference hypersurfaces that involve strategy m. The union ofXmand∂Xmis the set of all points where strategy m is preferred. We callXm⋃∂Xmthe preference region of strategy m. Then, we can say the following (please refer to Appendix A for all proofs).Proposition 1A decision-network polynomialU(x)is equivalent to the following sequence of multilinear functions:(9)U(x)=U1(x)ifx∈X1‾U2(x)ifx∈X2‾⧹∂X1…UM(x)ifx∈XM‾⧹∂XM-1∩∂XM-2∩⋯∩∂X1In Eq. (9), the set differencesXk‾⧹∂Xk-1∩∂Xk-2∩⋯∩∂X1are needed to preserve the one-to-one correspondence. According to the terminology of Borgonovo and Peccati (2010),U(x)is a piecewise defined function of type 2, induced by the max operator. The genericUm(x)(m=1,2,…,M) is called a piece ofU. Eq. (9) shows that: (i) a decision-network polynomial is piecewise-defined and (ii) each piece is the multilinear function associated with the strategy preferred atx.To clarify, Eq. (9) for our running example is(10)U(x)=x9ifx∈X‾1x1(x3x5+x4x6)+x2x8ifx∈X‾2⧹∂X2x1x7+x2x8ifx∈X‾3⧹∂X2∩∂X1We recall that fixed a strategy – i.e., once the operators in decision-nodes are fixed, – a decision-network polynomial reduces to a Bayesian-network polynomial. Thus, a decision-network polynomial is a sequence of Bayesian-network polynomials.Table 1summarizes the observations developed so far. Models that concern the factorization of probability distributions (e.g., Bayesian networks, event trees, fault trees) are characterized by Bayesian-network polynomials. Models that, in addition, allow for decision evaluation are characterized by Decision-Network polynomials. In particular, Eq. (9) holds for all decision-support models sustained by the powerful theoretical axioms of subjective utility theory. Of course, implementation issues can make analysts prefer one representation tool over another.Remark 1Eq. (5) describes the decision-problem in normal form. Similarly,U(x)in Eq. (10) has been written in normal form. However, the input–output mapping of the decision problem can also be written in extensive form. In our example, we would have(11)U(x)=max{x9;[x1max{x3x5+x4x6,x7}+x2x8]}By simple manipulation, we find that Eq. (11) is equivalent to Eq. (10). This observation is true in general. In fact, dynamic consistency, one of the fundamental properties of expected utility theory, insures us that the normal and extensive forms of any decision-problem are equivalent.Piecewise-definiteness represents the main departure of decision-network polynomials from Bayesian-network polynomials. The differentiability of a piecewise-defined function is a subtle matter. A general result is presented in Proposition 1 of Borgonovo and Peccati (2010) states that not only the regularity of the pieces, but also by the order of their contact along the frontier of each admissible domain determines differentiability. For decision-network polynomials, the following holds.Proposition 2LetU(x)be a decision-network polynomial.1.If pointx0is interior to an admissible domain, thenU(x)is infinitely many times differentiable atx0;Ifx0∈∂Xi, then the decision-network polynomial is not differentiable, but its partial derivatives (a la Gateaux) exist, and they depend on the strategy selected by the decision-maker for approachingx0.Let us illustrate the second item in Proposition 2 through our running Example (Fig. 1). Let∂kU(x0)∂x1,∂x2,…,∂xkdenote the ordinary partial derivative andDvU(x0)the Gateaux derivative in directionv. The reference point in the next discussion isx0in Table 2.We haveU1(x0)=U3(x0)=0.320andU2(x0)=0.308. Thus,x0belongs to the frontier ofX‾1andX‾3, and it isU(x0)=U1(x0)=U3(x0). LetH(x0)be a neighborhood ofx0, andΔx=εv, wherevis a unit vector andε⩾0. Suppose that, forεsufficiently small,x0+εvis included inX3, thenU(x0)=U3(x0)=(see Eq. (7)) for allx0+εv. The directional derivative of the decision-network polynomialU(x)alongvis(12)DvU(x0)=limε→0U3(x0+εv)-U3(x0)ε=v1x7+x1v7+v2x8+x2v8Then, consider the first partial derivative. We get∂U∂x1=x7=0.4[this is obtained by settingv=(1,0,0,0,0,0,0,0,0)in Eq. (12)]. Conversely, suppose that, forεsufficiently small,x0+εv∈H(x0)⊂X1. Then, we haveU(x)=U1(x)=x9andDvU(x0)becomes(13)DvU(x0)=DvU1(x0)=limε→0U1(x0+εv)-U1(x0)ε=v9Whence,∂U∂x1=0.From a numerical viewpoint, Proposition 2 states that a differentiation algorithm is directly applicable at all pointsx0which are internal to an admissible domain. If insteadx0belongs to the frontier of two or more admissible domains, then the algorithm can still be applied, but its results shall depend upon the trajectory chosen for tending tox0.From a decision-analytical viewpoint, Proposition 2 amounts to state that a decision-network polynomial is differentiable if and only if the decision-maker selects a preferred strategy. Thus, differentiation finds its full meaning in a post-optimality or in an open-loop setting.In the remainder of this work, we consider one of the most typical sensitivity analysis exercises. We have a decision support modelU(x):X→R, whereX⊂Rn, and we select two points of interest inX,x0andx1. One usually callsx0andx1base case and sensitivity case, respectively. We writeΔx=x1-x0=[Δx1,Δx2,…,Δxn], andΔxi=xi1-xi0. Then, we evaluate the model atx1andx0computing the changeΔU=U(x1)-U(x0).For decision-support models, three results are possible:(a)ΔU=Um(x1)-Um(x0)ifx0,x1∈Xm;ΔU=0ifx0,x1∈∂Xm∩∂Xj;ΔU=Uj(x1)-Um(x0)ifx0∈Xmandx1∈Xj.Case (a) is registered if strategy m is preferred at both x1 and x0, case (b) ifx0andx1belong to an indifference hypersurface of the preferred strategy atx0, and case (c) if the preferred strategy changes from m to j. Independently of the case, the following decomposition ofΔUholds.Proposition 3Given any two pointsx0,x1∈X, let(i1,i2,…,ik)(k⩽n)represent any subset of model input indices, and(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)the point obtained by setting all model inputs at the base case, but for model inputsxi1,xi2,…,xik, which are set at the sensitivity case. Then,(14)ΔU=U(x1)-U(x0)=∑k=1n∑i1<i2<⋯<ikϕi1,i2,…,ikkwhere(15)ϕi1,i2,…,ikk=U(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)-∑s=1k-1∑i1<i2<⋯<isϕi1,i2,…,iss-U(x0)ΔUis decomposed in2n-1terms. The termsϕi1,i2,…,ikkare called finite change sensitivity indices of order k (Borgonovo, 2010). To illustrate,(16)ϕi1=U(x∼i0,xi1)-U(x0)ϕi,j2=U(x∼i,j0,xi1,xj1)-ϕi-ϕj-U(x0)ϕi,j,k3=U(x∼i,j,k0,xi1,xj1,xk1)-ϕi,j2-ϕi,k2-ϕj,k2-ϕi-ϕj-ϕk-U(x0)…Because Eq. (14) stems from an orthogonal function decomposition (see the Proof in Appendix A), the terms in Eq. (14) are orthogonal. This means that first order terms,ϕi1, quantify the individual effects of model inputs. Moreover, by the first equation in (16),ϕi1(i=1,2,…,n) are the sensitivity measures displayed in Tornado diagrams. The second order terms,ϕi,j2, quantify the residual effect of the interaction betweenxiandxj. Similarly, higher order terms in the expansion quantify the residual high order interactions inΔU.Multilinear functions register a tidy correspondence between finite change sensitivity indices and partial derivatives. In fact, ifH(x)is a multilinear function, the following relationship holds (Proposition 1 in Borgonovo & Smith, 2011):(17)ϕi1,i2,…,iks=∂H(x0)∂xi1,xi2,…,xis·Δxi1·Δxi2…·ΔxisEq. (17) implies that interaction terms of order s (ϕi1,i2,…,iss) are in one-to-one correspondence with the corresponding partial derivatives ofH(x). Furthermore, the decomposition of a finite changeΔH=H(x1)-H(x0)through Taylor expansion and through finite differences are equivalent. That is(18)ΔH=∑s=1n∑i1<i2<⋯<isϕi1,i2,…,iss=∑s=1n∑i1<i2<⋯<is∂H(x0)∂xi1,xi2,…,xis·Δxi1·Δxi2…·ΔxisWhen we come to decision-network polynomials, these two facts hold for any of its pieces taken individually, that is, in an open-loop analysis. In a closed-loop analysis, the following hold.Proposition 4LetU(x):X→Rbe a decision-network polynomial and considerx0,x1∈Xin a closed-loop analysis.(1.a)Ifx0∈Xmandx1∈XmandIf, for all subset of indicesi1,i2,…,ik,k=1,2,…,n, the points(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)∈Xm, then(19)ϕi1,i2,…,ikk=∂Um(x0)∂xi1,xi2,…,xik·Δxi1·Δxi2…·Δxikand(20)ΔU=∑k=1n∑i1<i2<⋯<ik∂Um(x0)∂xi1,xi2,…,xik·Δxi1·Δxi2…·Δxik;Ifx0∈∂Xm∩Xj‾and(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)∈Xjfor all the choices of(i1,i2,…,ik),k=1,2,…,n, then Eqs.(19) and (20)apply withUj(x0)replacingUm(x0).If the assumptions in 1 or 2 do not apply, theϕi1,i2,…,issare still defined by Eq.(16), but Eqs.(17) and (18)do not hold.Proposition 4 states that the relationships betweenϕi1,i2,…,issand∂U∂xi1,xi2,…,xisvalid for Bayesian-network polynomials (Eqs. (17) and (18)) apply to decision-network polynomials under the condition that the finite difference operation involves points all contained in the same preference region. Then,ϕi1,i2,…,issallows us to know the corresponding derivative through the ratioϕi1,i2,…,issΔxi1·Δxi2…·Δxis(Eq. (19)). In case (3) of Proposition 4, derivatives are undefined, but the exact decomposition ofΔUis still achieved through finite change sensitivity indices.By adding all finite change sensitivity indices which are related to a specific model inputxi, we obtain the so called total effectϕiT:(21)ϕiT≔∑s=1n∑i1<i2…<isi∈{i1,i2,…,is}ϕi1i2…iss.The total effectϕiTis the portion ofΔUcontributed byxi, either alone or in its interactions with the remaining model inputs. The quantityϕiI(22)ϕiI≔ϕiT-ϕi1=∑s=2n∑i1<i2…<isi∈{i1,i2,…,is}ϕi1i2…iss,i=1,2,…,nis called the interaction effect ofxi, and represents the portion ofΔUassociated with the interactions ofxiwith the remaining model inputs. A model inputxiis not involved in interactions with other model inputs, when the model inputs change fromx0tox1, if and only ifϕiI=0.The finite change sensitivity indicesϕiT,ϕiIandϕi1generalize the ordinary Tornado sensitivity measures by providing information about the relevance of interactions (Borgonovo & Smith, 2011). It can be shown (Borgonovo, 2010) that(23)ϕiT=U(x∼i1,xi0)-U(x1)where recall that(x∼i1,xi0)is the point obtained setting all model inputs at the sensitivity casex∼i1, butxi, which remains fixed atx0. By Eq. (23) the total effectsϕ1T,…,ϕnTare obtained at the cost ofn+1model evaluations. Thus, to obtain the triplets(ϕi1,ϕiT,ϕiI)of finite change sensitivity indices for each model input we need two series of one-way sensitivities: precisely, we need to evaluateϕi1=U(x∼i0,xi1)-U(x0)andϕiT=U(x∼i1,xi0)-U(x1)fori=1,…,n. This is the same computational cost of a Tornado diagram.Proposition 5LetU(x):X→Rbe a decision-network polynomial. If the following conditions apply:(1)x0∈Xmandx1∈Xj;∂Um(x0)∂xi≠0and∂Uj(x)∂xi=0for allx∈Xj;atx1, the change in the solexifromxi1toxi0does not cause the preferred strategy j to change; then(24)ϕi1=-ϕiIandϕiT=0.If condition (1) above applies together with:∂Um(x)∂xi=0for allx∈Xmand∂Uj(x1)∂xi≠0;atx0, the change in the solexifromxi0toxi1does not cause the preferred strategy m to change; then(25)ϕiT=ϕiIandϕi1=0.Proposition 5 unveils a peculiar feature of the response of decision-support models to input changes, which is generated by their piecewise-definiteness. The first three conditions in Proposition 5 concern a model inputxithat influences the preferred strategy atx0, but is not involved in the piece of the decision-network polynomial associated with the preferred strategy atx1. Thenxihas a null total effect, because we register an interaction effect equal in magnitude but opposite in sign to its individual effect. Conditions 1–4–5 address the symmetric situation of a model input that appears in the piece of the decision-network polynomial atx1but is not involved in the preferred strategy atx0. In this case, we obtain a null first order effect, but a non-null total effect.Propositions 3–5 concern sensitivity both in an open-loop and closed-loop settings. In particular, Proposition 5 sheds further light on the differences between the two settings. In an open-loop setting, we are interested in the sensitivity of a given strategy (the preferred or another strategy). In this case, the decision-network polynomial is a multilinear function of the parameters. We can writeUm(x)=a(x∼i)xi+b(x∼i), wherea(x∼i)=∂Um(x)∂xi. Then, it isΔUm=∂Um(x)∂xiΔxi, which implies that, provided that∂Um(x)∂xi≠0, a change inxiinduces a change inUm(x). Thus, an open-loop analysis shall always register a non-null value of the sensitivity index given a change inxi, ifUmis dependent onxi. Proposition 5 suggests that this is not always the case in a closed-loop analysis. Here, target of the sensitivity isU(x). We can findΔU=0given a change inxi, even ifU(x)is formally dependent onxi. The reason is the piecewise-defined nature of the decision-network polynomial.To illustrate, we refer to our running example (Eq. (10)) at the same point as in Section 4. Consider now a shift in parameterx6from0.2to0.41. At this new point, we registerU2(x1)=0.3164. That is,ΔU2=Δx6x1x4=0.0084. Thus, the open-loop sensitivity of alternative 2 foresees a non-null first order effect ofx6. In a closed-loop setting, however,x6has a null effect. In fact, atx1, we registerU1(x1)=U3(x1)=0.32>U2(x0)=0.3164, so that the decision-maker is still indifferent between I and III. Hence, the value of the decision-problemU(x)is still equal to0.320. Thus, because the shift inx6is not sufficient to makeU2the preferred alternative, the value of the decision-problem is insensitive to the change inx6.We end this section showing that knowledge of the first order indices (ϕi1) allows us to obtain the corresponding one-way sensitivity functions. Letym(xi)denote the sensitivity function of the expected utility of strategy m asxivaries. By linearity, we obtain(26)ym(xi)=ϕi1xi-xi0Δxi+Um(x0)whereϕi1=Um(x1)-Um(x0), andΔxi=xi1-xi0.Eq. (26) relates first order finite change sensitivity indices to one-way sensitivity functions. Eq. (26) suggests that, once we have generated a Tornado, we are able to plot the one-way sensitivity functions for all alternatives. In fact,ϕi1,xi1,xi0andUm(x0)are given once we have obtained the Tornado diagram.In an open-loop analysis, we have as many sensitivity functions as many alternatives we are comparing. In a closed-loop, we have a unique one-way sensitivity function which is the envelope of these curves, given byy(xi)=maxj=1,2,…,myj(xi). The functiony(xi)is the one-way sensitivity function of the decision-network polynomial with respect toxi, and represents the value of the decision-problem, i.e. expected utility or expected payoff, asxivaries in the assigned range.To illustrate, we refer back to our running example and consider pointx0and the variation inx6previously mentioned. We obtainϕ61=0.0084. Using our knowledge ofUm(x0)(m=1,2,3) and inserting into Eq. (26), we obtain the one-way sensitivity plot in Fig. 2.Fig. 2 shows the individual one-way sensitivity functions on parameterx6of the three multilinear polynomials associated with the three alternatives. The envelope (solid curve) allows us to grasp the variation ranges ofx6over which, ceteris paribus, a given alternative is preferred. The indifference point is a corner point fory(x6), and the decision-network polynomial is not differentiable atx6=0.5.In several applications, consequences are monetary payoffs and a utility function u over monetary consequences might be assessed. With a slight abuse of notation, letq∈Q⊆Rbe the monetary payoff of the corresponding consequence and setuq=u(q)the consequent utility value. In order to express the decision-maker’s risk attitude, without loss of generality, we can assumeu:R→[0,1],umonotone and convex (or concave). An important feature of this representation is the possibility of utilizing certainty equivalents. “The certainty equivalent for an alternative is the certain (monetary) amount that is equally preferred to the alternative” (Kirkwood, 2002, Ch. 2, p. 19). Under the above assumptions on u, the certainty equivalent of alternative m, denoted byCEm, is the unique number defined by the equation(27)CEm=u-1(Um).Because a certainty equivalent is a monotonic transformation of the corresponding expected utility, the link between certainty equivalents and expected utilities is strict.First, certainty equivalents can be represented as piecewise-defined functions ofx.Corollary 1The functions(28)CEm(x)=u-1Um(x),m=1,2,…,Mare composite multilinear and(29)CE(x)=CE1(x)ifx∈X1‾CE2(x)ifx∈X2‾⧹∂X1…CEM(x)ifx∈XM‾⧹∂X1∩∂X2∩…∩∂XM-1.Second, at differentiability points, all higher order partial derivatives ofCEcan be retrieved by the derivatives ofU.Proposition 6Letx0∈Xm. Then:(30)∂kCEm(x0)∂xi1,xi2,…,xik=∑t=1k∂t(u-1(Um))∂Um(x0)·∑Ipartition ofks.t.|I|=tσcompatible withIDσkUm(x0)where k is the order of the derivative of interest, I a set of positive integers whose sum isk,tthe cardinality of I andσa subdivision of the k model inputs into t subsets, whose cardinalities are the integers in I.We refer to Appendix A for further technical details and the proof. To illustrate Eq. (30), for derivatives of order 3, we have∂CE(x)∂xi1,xi2,xi3=∂u-1(Um)∂Um∂3Um∂xi1,xi2,xi3+∂2u-1(Um)∂2Um∑j1,j2∈{i1,i2,i3}j1<j2∂2Um∂xj1,xj2∂Um∂x∼j1j2+∂3u-1(Um)∂3Um∂Um∂xi3∂Um∂xi2∂Um∂xi1.Example 1An important class of utility functions is the exponential familyu(q)=1-e-αq. Sinceu′(q)=a(1-u(q)), it is∂u-1∂Um=-1α(Um-1)and we find(31)∂ku-1∂kUm=1α(-1)k(k-1)!(Um-1)k.By Eq. (30), the partial derivatives of certainty equivalents with exponential utility functions atx0∈Xmare(32)∂CE(x0)∂xi1,xi2,…,xik=1α∑t=1k(-1)t(k-1)!(Um(x0)-1)t·∑Ipartition ofks.t.|I|=tσcompatible withIDσkUm(x0).Third, formally identical results hold for generic changes in certainty equivalent and decision-network polynomials.Corollary 2Given any two pointsx0,x1∈X, letΔCE=CE(x1)-CE(x0). Then:(A)(33)ΔCE=∑i=1nϕi1,CE+∑i<jϕi,j2,CE+⋯+ϕ1,2,…nn,CEwhere(34)ϕi1,CE=CE(x∼i0,xi1)-CE(x0)ϕi,j2,CE=CE(x∼i,j0,xi1,xj1)-ϕi1-ϕj1-CE(x0)…Ifx0∈Xmandx1∈Xmand for all subset of indicesi1,i2,…,ik,(k=1,2,…,n)the points(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)∈Xm, then(35)ϕi1,i2,…,ikk,CE=∂CEm(x0)∂xi1,xi2,…,xik·Δxi1·Δxi2…·Δxik,and(36)ΔCE=∑k=1n∑i1<i2<⋯<ik∂CEm(x0)∂xi1,xi2,…,xik·Δxi1·Δxi2…·Δxik;Ifx0∈∂Xm∩Xk‾∩Xj‾∩⋯∩Xr‾and the points(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)∈Xjfor all(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1),k=1,2,…,n, then Eqs.(35) and (20)apply withCEj(x0)replacingCEm(x0).Fourth, the structural features of decision-network polynomials are inherited by certainty equivalent polynomials.Corollary 3Under conditions 1, 2 and 3 ofProposition 5,(37)ϕi1,CE=-ϕiI,CEandϕiT,CE=0Under conditions 1, 4 and 5,(38)ϕiT,CE=ϕiI,CEandϕi1,CE=0Finally, finite change sensitivity indices acquire a more direct managerial interpretation when applied to certainty equivalents. The magnitude ofϕi1,CErepresents the monetary change in value of the decision-problem due to the individual change inxi. The magnitude ofϕiI,CErepresents the increase/decrease due to the simultaneous variation ofxiand of the other model inputs involved in the change fromx0tox1. The magnitude ofϕiT,CErepresents the total monetary change (increase or decrease) associated withxias sum of its individual and interaction effects.In this section, we discuss the derivation of managerial insights through a well-known case study. The case-study is presented in Felli and Hazen (2004), and refers to the giant cell arthritis problem (GCA) of Buchbinder and Detsky (1992), which describes a realistic-size application (Felli & Hazen, 2004, p. 101). We refer to Buchbinder and Detsky (1992) and Felli and Hazen (2004) for a thorough overview. The treatment of the GCA problem foresees a prednisone therapy, which may lead to collateral complications. Four medical strategies are possible: (I) Treat None, (II) Biopsy and Treat Positive, (III) Biopsy and Treat All, (IV) Treat All. The problem is represented as an influence diagram in Fig. 3. The chance nodes are: GCA; severe complications (GCAcompl); answer to biopsy test (Test), and prednisone therapy complications (Prednisone Complication).The decision-network polynomial of this problem is(39)U(x)=U1(x)ifx∈X1‾U2(x)ifx∈X2‾⧹∂X1U3(x)ifx∈X3‾⧹∂X1∩∂X2∩U4(x)ifx∈X4‾⧹∂X1∩∂X2∩∂X3with(40)U1(x)=x1x2x8+x1(1-x2)x9+(1-x1)x10U2(x)=x3x4x5[x6x11+(1-x6)x12]+x3x4(1-x5)x13+x3(1-x4)x5[x6x14+(1-x6)x15]+x3(1-x4)(1-x5)x16+(1-x3)x7[x2x17+(1-x2)x18]+(1-x3)(1-x7)x19U3(x)=x4[x3x5+(1-x3)x7][x6x11+(1-x6)x12]+x4x13[x3(1-x5)+(1-x3)(1-x7)]+x6x7x14(1-x3)(1-x4)+(1-x4)(1-x6)x15[x3x5+(1-x3)x7]+(1-x4)x16[x3(1-x5)+(1-x3)(1-x7)]U4(x)=x4x1x6x20+x4x1(1-x6)x21+x4(1-x1)x22+(1-x4)x1x6x23+(1-x4)x1(1-x6)x24+(1-x4)(1-x1)x25We haven=25exogenous variables,np=7probabilities andnQ=18utilities. A description of the probabilities involved in the model(xi,i=1,2,…,7)is reported in Table 3. Their numerical values and the numerical values of utilities(xi,i=8,9,…,25), are reported in Table 4. Table 4 reports the base case (x0) and the sensitivity case (x1). All numerical values are obtained from Table 4 in Felli and Hazen (2004). The sensitivity case is the worst case scenario in Felli and Hazen (2004).Atx0, the expected utilities of the alternatives are, respectively,UI(x0)=0.6920,UII(x0)=0.7634,UIII(x0)=0.7453andUIV(x0)=0.7246. Thus, alternative II (biopsy and treat positive) is preferred (see also Felli & Hazen, 2004, p. 104). Atx1, we registerUI(x1)=0.5200,UII(x1)=0.3800,UIII(x1)=0.2490andUIV(x1)=0.2290. Thus, we have a change in preferred strategy when passing fromx0tox1, with I becoming preferred. The change in the expected utility of the decision problem isΔU=UI(x1)-UII(x0)=-0.172. As far as certainty equivalents are concerned, they are not provided in the original analysis of Felli and Hazen (2004). However, because they are of interest in the present work, we discuss them for illustration purposes. Using an exponential utility function of the formu(q)=1-exp(-λq), with a symbolicλ=1/USD, we obtainCEI(x0)=1.1616,CEII(x0)=1.4166,CEIII(x0)=1.3466andCEIV(x0)=1.2723, andCEI(x1)=0.7319,CEII(x1)=0.4764,CEIII(x1)=0.2863andCEIV(x1)=0.2601. Thus, the changex0→x1leads to a monetary loss ofΔCE=CEI(x1)-CEII(x0)=-0.6847USD.To explain the changes in values of the decision-problem and certainty equivalent, we proceed as follows. Because,x1∈XA‾andx0∈XB‾, we are in the third case of Proposition 4, and the decision-network polynomial is not differentiable. Nonetheless, we can decomposeΔUandΔCEthrough Eqs. (14) and (33), respectively.Fig. 4reports the finite change sensitivity indices in the form of a generalized Tornado diagram. Graph (a) plotsϕiT,ϕiI,ϕi1and graph (b)ϕiT,CE,ϕiI,CEϕi1,CE. Individual effects, (ϕi1,ϕi1,CE, the usual bars of a Tornado diagram) are in white, interaction effects (ϕiI,ϕiI,CE) in grey and total effects (ϕiT,ϕiT,CE) in black.In accordance with the factor prioritization setting of Saltelli et al. (2008), the model inputs are ordered based on their total effectsϕiTandϕiT,CE. Then,x12,x2,x4andx8are the most relevant model inputs, followed by the remaining ones. The ranking is coincident forU(x)[graph (a)] andCE(x)[graph (b)].Interactions matter in explainingΔU(x0)andΔCE(x). The magnitudes ofϕiI(ϕiI,CE) indicate that the changes in expected utility and certainty equivalent acrossx0andx1are not the simple superimposition of individual effects. For instance,x12is associated with a strong interaction effect(ϕ12I=-0.1103), which prevails over its individual effect(ϕ121=-0.0377). Also, the sign of the interaction effects coincides with the sign of the first and total effect, in practically all cases, signaling that interactions tend to amplify individual effects.This leads us to a third aspect, namely, direction of change. We observe that all factors have null or negative total effects. This is in accordance with the decrease in expected utility (certainty equivalent) when passing fromx0tox1.Proposition 5 explains the sensitivity results for model inputsx17,x15,x14,x11,x7,x6,x3andx8. Model inputsx17,x15,x14,x11,x7,x6,x3display null total effects, but non-null individual effects.x8is associated with a null first order but a non-null total effect. To illustrate, let us consider model inputx3. This model input appears in the decision-tree branch related to strategy II, but in no other branch of the model. We registerϕ31=-0.0188(andϕ31,CE=-0.0747). Thus, varied alone,x3causes a non-null change in the value of the expected utility (certainty equivalent) of the decision-problem. Atx1,Abecomes the preferred strategy. The variationx31→x30does not change the expected utility of the problem becausex3does not appear in the multilinear pieceUI(x). At the same time, it can be verified that the magnitude of the change inx3is not enough to cause the preferred strategy to change. Then, whenx3undergoes the changex31→x30atx1,UII(x)changes, but becauseUI(x)is preferred, and there is no change in preferred strategy, we obtainϕ3T=0. Correspondingly, it isCE(x1)-CE(x∼31,x30)=ϕ3T,CE=0(see graph b in Fig. 4). A similar consideration applies to the sensitivity results ofx17,x15,x14,x11,x7, andx6. For factorx8the following holds.x8is the utility associated with the first leaf (consequence) of the decision-tree of strategy I. As such, it does not appear as a variable inUII(x)and it has no influence on the expected value of strategy II, which is preferred atx0. Thus, it satisfies assumption 4 in Proposition 5 and its first order indexϕ31=0. At the same time, it plays a role inUI(x)satisfying assumption 5 in Proposition 5. Consequently we obtain a change in the value ofUI(x)whenx8undergoes the changex81→x80, while all other factors are fixed atx1. In particular, it isUI(x∼81,x80)-UI(x1)=-0.04=ϕ8T. Because the assumptions of Proposition 5 are satisfied, we also obtainϕ8I=-0.04. A similar result holds forϕ8T,CEandϕ8I, as per Corollary 3.We can use our knowledge of the first order indices to obtain the sensitivity functions [Eq. (26)]. We consider the first four factors by magnitude of their total order sensitivity measures, namely,x12,x2,x4andx8. Fig. 5displays the results.In Fig. 5, graph (a) portrays the four sensitivity functionsyI(x12)(--),yII(x12)(-.),yIII(x12)(..) andyIV(x12)(--), and their envelope (solid). Similarly, graphs (b)–(d) display the sensitivity functions forx2,x4andx8. From these graphs, a decision-maker can zoom-into the individual effects of each variable as the other remains fixed.Finally, the application of finite change sensitivity indices to certainty equivalents provides us with an additional managerial interpretation, namely, we can appreciate the monetary amount associated with each model input in the change from the base case to the worst case. For instance,x12is associated with a potential loss of around 0.37USD, of which 0.14USD due to its individual action and the remainder due to interactions with the other factors. In this respect, it is well known that when a model input is prevented from varying, not only its individual effect is canceled, but one annuls also its interaction effect. Thus, fixingx12, we avoid the (potential) monetary loss of 0.37USD. A Tornado diagram would allow us to appreciate a loss of 0.14USD, but not the additional loss of 0.23USD associated with the interactions ofx12with the remaining factors.

@&#CONCLUSIONS@&#
We have derived the representation of the input–output mapping in decision-support models from the principles of expected utility theory. We have called this representation a decision-network polynomial. We have seen that it is a piecewise defined function of probabilities and utilities. Each piece is multilinear and infinitely many times differentiable at points interior to an admissible domain. Differentiability is lost at indifference points. At these points, directional derivatives exist, provided that the decision-maker resolves indifference. Thus, differentiation finds its natural setting in an open-loop analysis. In a closed-loop setting, sensitivity information cannot be retrieved through differentiation in general. We have then studied the application of finite change sensitivity indices. First order indices are in direct correspondence with one-way sensitivity functions and, when, the model is differentiable, finite change sensitivity indices are in one-to-one correspondence with partial derivatives.For certainty equivalents, formally identical results hold for the decomposition of finite changes. Finite change sensitivity indices acquire an additional managerial interpretation. They allow us to appreciate the portion of the gain/loss in problem value attributable to the individual action of a model input or due to its interactions with the other model inputs involved in the change.Proof of Proposition 1We start the proof noting that strategies can be labeled in any order. Thus, there is no loss of generality if we start with the strategy that is labeled 1. By Definition 2X1is the open set ofxwhere strategy 1 is preferred, that ismaxm=1,2,…,M[Um(x)]=U(x)=U1(x). By Definition 3,X1‾=X1∪∂X1.∂X1is the set of all pointsxinXat whichU1(x)is equal to any (one or more) of the remaining strategies. In particular, it is equal toU2(x). Then, we need to assignU(x)=U2(x)for allx∈X2‾, but for the points whereU2(x)=U1(x), because at these points the decision-network polynomial is already defined by the previous assignment. Note that, in any case,U2(x)=U1(x)at these points, but, if these points were left in, we would be dealing with a multi-function, in principle; problem which is, however, readily avoided by this assignment. One can then proceed in a similar way with the higher order terms.□(1)Ifx0is interior toXi, thenUis multilinear and, therefore, infinitely many times differentiable.Ifx∈∂Xi, thenx0is at the border of some strategy preference regions (sayXjandXi, for simplicity). It isUi(x)=Uj(x). Letvdenote a unit vector and writexv=x0+εv, whereεis a positive real number and letUε(x0)=xv:xv-x0⩽εdenote a neighborhood ofx0. Then, fixedv, considerεsuch thatUε(x0)⊂Xi,∀ε. Then, it is(41)limε→0E[u(xv)]-U(x0)ε=limε→0Ui(x0+εv)-Ui(x0)ε=DvUi(x0)Conversely, ifvandεare such thatUε(x0)⊂Xj(42)limε→0E[u(xv)]-U(x0)ε=limε→0+Uj(x0+εv)-Uj(x0)ε=DvUj(x0)Finally, ifvandεare such thatx0+εv∈∂Xi∩∂Xi∀ε(43)limε→0E[u(xv)]-U(x0)ε=limε→0Uj(x0+εv)-Uj(x0)ε=limε→00ε=0□According to Borgonovo (2010), it is possible decompose any finite change in a measurable functiong,Δg=g(x1)-g(x0), as:(44)Δg=g(x1)-g(x0)=∑i=1nΔgi+∑i<jΔgi,j+⋯+Δg1,2,…nwhere(45)ϕi=g(xi1,x∼i0)-g(x0)ϕi,j=g(xi1,xj1,x∼i,j0)-g(x0)-ϕi-ϕj…that is,(46)Δg=∑i=1nϕi+∑i<jϕi,j+⋯+ϕ1,2,…,nThen, becauseU(x)is measurable,Eq. (46) holds for decision-network polynomials.□(1)Ifx0andx1belong to the same preference region, and all points involved by the differentiation are internal to the same preference region, thenU(x)equals the multilinear functionUm(x)at all steps of the differentiation process. More precisely, because(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)is assumed internal toUm(x), there is a (small enough) neighborhood of(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)in whichU(x)is the multilinear functionUm(x). The result then follows by Proposition 1 in Borgonovo and Smith (2011).Consider nowx0andx1, such thatx0∈∂Xm∩Xk‾∩Xj‾∩⋯∩Xr‾andx1∈Xj. Then, observe thatU(x)is not differentiable atx0(Proposition 2), butU(x)becomes differentiable if, when decomposing the change, we hit points all contained in some preference region, sayXj. This latter condition corresponds to the assumptionx0∈∂Xm∩Xj‾,(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)∈Xjfor all(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)and(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)∈Xj. Hence, for all(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1),there is a (small enough) neighborhood of(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)in whichU(x)is the multilinear functionUj(x). The result then follows by Proposition 1 in Borgonovo and Smith (2011).If not all(x∼(i1,i2,…,ik)0,xi11,xi21,…,xik1)are contained in the same preference region, the decision-network polynomial is no more represented by a unique multilinear function and is, therefore, not differentiable. Thus, we cannot conclude anything about the relationship betweenϕi1,i2,…,issand∂U∂xi1,xi2,…,xis.□The decision-network polynomial is differentiable atx0. By Proposition 4, we have(47)ϕi1=∂Um(x0)∂xiΔxi=∂Um(x0)∂xi(xi1-xi0)≠0.Consider now pointx1. If∂Uj(x1)∂xi=0, then the value ofUj(x1)is unaffected by the change inxifromxi1toxi0, by Eq. (17). In other words,(48)Uj(x∼i1,xi0)=Uj(x1)Now, if the change inxifromxi1toxi0is not strong enough to cause some other strategy to become preferred, then we registerUj(x∼i1,xi0)>Um(x∼i1,xi0)and, consequently,U(x∼i1,xi0)=Uj(x∼i1,xi0). Thus, we have(49)U(x∼i1,xi0)-U(x1)=Uj(x∼i1,xi0)-Uj(x1)=Uj(x1)-Uj(x1)=0By Eq. (23), then we have(50)ϕiT=Uj(x∼i1,xi0)-Uj(x1)=0Correspondingly, by Eq. (22), it isϕi1+ϕiI=0.The proof of Eq. (25) follows in a similar way and is omitted for brevity.□This corollary follows immediately by the fact that if u is a monotonic utility function, then the ranking induced by certainty equivalents is equivalent to the ranking induced by expected utility. More formally, we observe thatu-1is a assumed to be a monotonic transformation. Then, Eq. (27) states that for allx∈X,(51)CEm(x)=u-1(Um(x))Therefore, for allx∈X, the certainty equivalent of the decision-problem is equal to(52)CE(x)=maxm=1,2,…,MCEm(x)=maxm=1,2,…,Mu-1(Um(x))Now, we know that the maximum across m ofCEm(x)is registered at m that makesUm(x)maximum, due to the fact thatu-1is monotonic: ifUm(x)>Uj(x), then monotonicity implies that alsou-1(Um(x))>u-1(Uj(x)). By Eq. (27) then, we have that at eachxthe certainty equivalent of the decision problem is the certainty equivalent of the preferred strategy. This leads us to write:(53)CE(x)=u-1(U1(x))ifx∈X1‾u-1(U2(x))ifx∈X2‾⧹∂X1…u-1(UM(x))ifx∈XM‾⧹∂X1∩∂X2∩⋯∩∂XM-1□The results follow by Eq. (29) and by applying the reasoning used in the proofs of Propositions 3 and 4.□First, we observe that by Eq. (30), at everyxit is:(54)∂CEj(x)∂xi=∂u-1(Uj)∂Um·∂Uj∂xiThus, if∂Uj(x)∂xi=0at everyxinXjit is also∂CEj(x)∂xi=0at everyxinXj. Thus, if atx1the change inxialone fromxi1andxi0does not cause the preferred strategy to change, then the certainty equivalent remains the same. The proof then proceeds in an identical way as the proof of Proposition 5.□We start introducing a notation for products of partial derivatives ofUm. I denotes a set of positive integers, t, we mean a set of positive integers (with cardinality⩾1) whose sum is t. We call t the degree of I, denoted bydegI. With|I|we indicate its cardinality, namely, the number of integers in I. For example, ifI=2,2,1,1thent=6(reads, I is of degree 6) and|I|=4. We say thatσis a choice amongxi1,xi2,…,xikcoherent with I ifσis an unordered choice ofdegIdifferent model inputs amongxi1,xi2,…,xik, collected into sets whose cardinalities are the elements of I. Such aσwill chose|I|sets of different variables amongxi1,xi2,…,xikand thus identifies a product of|I|partial derivatives ofUm. For example, a choice of 6 variables coherent withI=2,2,1,1isσ={x1,x2},{x5,x8},{x3},{x6}and its corresponding product of partial derivativesis∂2Um∂x1,x2∂2Um∂x5,x8∂Um∂x3∂Um∂x6, which we shortly denoteDσ6Um. Denote with g the functionu-1and withg(m)the derivative∂mu-1∂Um.Everything then follows by iterated application of the derivation rule for composite functions. The difficult part is to write in a formal way the formula of the proposition, introducing an adequate notation. The proof then follows by induction. Clearly∂CE(x)∂xi1=∂g∂Um·∂Um∂xi1. The second order derivative is∂CE(x)∂xi1,xi2=∂∂xi2g(1)·∂Um∂xi1=g(1)∂∂xi2∂Um∂xi1+g(2)∂Um∂xi1∂Um∂xi2and the one of order three is:∂CE(x)∂xi1,xi2,xi3=∂∂xi3g(2)∂Um∂xi2∂Um∂xi1+g(1)∂2Um∂xi1,xi2=g(1)∂∂xi3∂2Um∂xi1,xi2+g(2)∂Um∂xi3∂2Um∂xi1,xi2+∂∂xi3∂Um∂xi2∂Um∂xi1+g(3)∂Um∂xi3∂Um∂xi2∂Um∂xi1=g(1)∂3Um∂xi1,xi2,xi3+g(2)∑j1,j2∈{i1,i2,i3}j1<j2∂2Um∂xj1,xj2∂Um∂x∼j1,j2+g(3)∂Um∂xi3∂Um∂xi2∂Um∂xi1.We now proceed by induction. Assume that∂k-1CEm(x0)∂xi1,xi2,…,xik-1=∑t=1k-1∂t(g)∂Um(x0)·∑Ipartition ofk-1s.t.|I|=tσcompatible withIDσkUm(x0).The aim is to show that an analogous formula holds for the derivative∂kCEm(x0)∂xi1,xi2,…,xik. We compute the partial derivative of∂k-1CEm(x0)∂xi1,xi2,…,xik-1w.r.t.xik(different fromxi1,xi2,…,xik-1). The termg(t)=∂t(u-1(Um))∂Umcan be obtained in two ways:1.differentiating w.r.t.xikthe functiong(t-1)ordifferentiating w.r.t.xikthe expression∑Ipartition ofk-1s.t.|I|=tσcompatible withIDσkUm(x0), which is the coefficient ofg(t)in∂k-1CEm(x0)∂xi1,xi2,…,xik-1.In the first caseg(t)multiplies the expression∑Ipartition ofk-1s.t|I|=t-1σcompatible withIDσkUm(x0)∂Um∂xik: this is a sum of the termsDσkUm(x0)whereσis compatible with the partitionI′obtained asI′=I,1andσchoosesxikas variable corresponding to the 1 inserted inI′(I′is thus a partition of k of length t).In the second case the coefficient ofg(t)is∂∂xik∑Ipartition ofk-1s.t.|I|=tσcompatible withIDσkUm(x0). An indexσappearing in this sum generates more indices: a choiceσofk-1variables subdivided in t groups will give the choicesσ′(of k variables of length t) obtained by adding the variablexikto one group ofσ. In terms of partition, the generated partitions can be obtained by adding a 1 to one number of the original partition ofk-1of length t.Considering both terms, we obtain as coefficient ofg(t)a sum over all the partitionsI′of k of length t and over all choices of variables compatible withI′ofDσkUm(x0), as claimed. As already remarked, this expression must be symmetric w.r.t. the variablesxi1,xi2,…,xik.□