@&#MAIN-TITLE@&#
Computationally efficient optic nerve head detection in retinal fundus images

@&#HIGHLIGHTS@&#
A computationally efficient method for detection of optic nerve head has been proposed.Both color and fluorescein angiography retinal fundus images have been studied.The method involves Radon transformation of multi-overlapping windows within an optimization framework.

@&#KEYPHRASES@&#
Computationally efficient optic nerve head detection,Radon transformation,Color retinal fundus images,Fluorescein angiography retinal fundus images,

@&#ABSTRACT@&#
This paper presents a computationally efficient method for detection of optic nerve head in both color and fluorescein angiography retinal fundus images. It involves Radon transformation of multi-overlapping windows within an optimization framework in order to achieve computational efficiency as well as high detection rates in the presence of various structural, color, and intensity variations in such images. Three databases of STARE, DRIVE, and a local database have been examined. It is shown that this method provides high detection rates while achieving faster processing speeds than the existing methods that have reported comparable detection rates. For example, the detection rate for the STARE database which is the most widely used database is found to be 96.3% with a processing time of about 3s per image.

@&#INTRODUCTION@&#
Computer Assisted Diagnosis (CAD) of retinopathy is currently being used to lower the workload of ophthalmologists as it provides a non-labor intensive approach to the detection of anatomical landmarks and lesions in retinal fundus images. Improvements in the computational efficiency of this CAD system would allow the screening of more patients in a day while using existing camera systems.The localization of retinal landmarks, in particular optic nerve head (ONH), plays a key role toward identifying pathological conditions; for example in Diabetic Retinopathy (DR) [1,2]. ONH appears as a yellowish region in a color retinal fundus image (see Fig. 1a). The main characteristic of ONH is its rapid intensity variations due to dark blood vessels that are in its vicinity [3]. ONH has three characteristics that have been used in the literature for its localization: (1) it appears as a bright disk nearly 1600μm in diameter; (2) arteries leave from and veins enter it; and (3) blood vessels diverge from it. As noted in [4], detection of ONH is challenging due to the discontinuity of its boundary caused by large vessels as well as its considerable color or intensity variations as a result of structures such as exudates.There exist a large number of algorithms that determine the location (generally center) of ONH or its boundary. Sinthanayothin et al. [3] used the area with the highest average intensity variation to detect ONH using an adaptive local contrast enhancement method. Walter and Klein [5] obtained the ONH center by detecting the center of the brightest connected object in a fundus image. Foracchia et al. [6] used the convergence of vessels to detect the ONH center. Youssif et al. [7] utilized the directional pattern of the retinal blood vessels for the detection of ONH. Their method involved normalizing luminosity and contrast using illumination equalization and adaptive histogram equalization methods. Lu and Lim [8] located ONH based on its bright appearance in a color fundus image by using a set of concentric lines with different directions and evaluated the image variation along multiple directions. The detection of ONH was achieved via the orientation of the line segment having the maximum or minimum variation.A number of segmentation-based methods have also appeared in the literature. Li and Chutatape [9–11] used an active shape model to detect ONH. An active contour model was also discussed in [12,13] by Osareh et al. to detect ONH. Lowell et al. [14] designed an ONH template and correlated it to the intensity component of the fundus image using the Pearson-R correlation. Another model-based approach was presented by Xu et al. [15], where clustering-based classification of contour points was integrated into an active contour formulation. Wong et al. [16] used the level-set technique followed by ellipse matching. They obtained the ONH location by means of histogram analysis and a modified version of the conventional level-set method using the red channel. Lu [17] designed a circular transformation to capture simultaneously both the circular shape of ONH and the image variation across the ONH boundary. A Hausdorff-based template matching approach together with a pyramidal decomposition were proposed by Lalonde et al. [18]. Haar [19] used illumination equalization in the green channel to address the difficulty of pyramidal decomposition in dealing with large areas of bright pixels toward ONH detection.Some ONH localization techniques, e.g. [20–24], not only use the ONH characteristics, but also exploit the location and orientation of vessels. For example, Niemeijer et al. [24] presented the use of local vessel geometry and image intensity features. Tobin et al. [25] applied a method that mainly relied on vessels related to the ONH characteristics where a Bayesian classifier was used to classify each pixel in red-free images. Abràmoff and Niemeijer [26] utilized the same ONH characteristics to detect ONH via kNN regression. The method introduced by Abràmoff et al. [27] involved a pixel classification approach. In a recent study, Hsiao et al. [28] localized ONH by an illumination correction operation followed by contour segmentation via a supervised gradient vector flow snake. Yu et al. [29] identified ONH candidates by first using template matching and then by using vessel characteristics. Finally, some methods, e.g. [6,7,19,30], have made use of the fact that major retinal vessels converge into ONH. In [31], we discussed the detection of ONH in fluorescein angiography (FA) retinal fundus images.The issue of computational complexity of the existing methods has not yet been addressed in a systematic way. In this paper, our aim has thus been to examine the computational complexity aspect. Among the above methods, we have identified the three most computationally efficient methods that have reported relatively high detection rates. These methods are listed them in Table 1. The detection rates listed correspond to the images appearing in the STARE [32] database. As noted in [6,7,32], the detected location of ONH is considered correct or clinically acceptable if its center falls within 60 pixels of a manually identified ONH center.In this paper, our method initially introduced in [31] is enhanced and extended to color retinal fundus images. Most importantly, the computational efficiency aspect of our method is compared to the ones listed in Table 1 in terms of both computational complexity and detection rate. Furthermore, we have examined three databases instead of only one database. These databases include the two public domain databases of DRIVE [33] and STARE and the private domain database of MUMS-DB used in [31]. In the next section, the details of our computationally efficient detection method are provided.Our detection method is based on the fact that ONH appears as a bright region in a fundus image. More specifically, Radon transform (RT) is used for this purpose. RT generates the integration of pixel intensities along different directions which leads to making ONH a prominent structure in the Radon space. It is worth pointing out that although it is possible to use other ONH characteristics such as merging points of blood vessels, we have only considered the brightness information here to gain high processing speeds. For example, as noted in the second entry of Table 2, the computational time dramatically increases by using the blood vessel information. In our computationally efficient method, a fundus image is first partitioned into overlapping blocks or sub-images. RT is applied to each block or sub-image. The sub-images exhibiting peaks in the Radon space are then processed in order to locate the ONH. The processing pipeline of our method is illustrated in Fig. 2. The pipeline for fluorescein angiography images slightly differs from the pipeline for color images in terms of the color channel component used. The components of our detection algorithm as well as the computational complexity aspect are further explained in the subsections that follow.At first, it is important to separate fundus from background (or region that is out of the fundus field). A fundus image consists of a circular fundus and a dark background surrounding the fundus. Fundus constitutes the region-of-interest and thus the processing is not applied to the background region. A fundus mask is normally supplied with retinal images of a database. In a fundus mask, fundus pixels are marked with 1's and the background pixels with 0's. With the help of the fundus mask, the detection algorithm would only process the pixels belonging to the fundus and not the background.For those databases for which the fundus mask is not provided, the fundus can be easily separated from the background by converting the image to the HSI color space and then applying a simple segmentation to the intensity channel. Since the background pixels are significantly darker than the fundus pixels, the segmentation generates the fundus mask. A fundus mask obtained in this manner is shown in Fig. 3.In our method, a fundus image is partitioned into widows or sub-images for the detection of ONH. To find objects on the border of sub-images, overlapping sliding windows are considered. The approximate size of the targeted object or ONH is used to determine the size of the sub-image or sliding window (n). If n is set to be smaller than the ONH diameter, ONH will not fit inside the sub-image and the algorithm fails to detect the ONH. On the other hand, if n is set bigger than the ONH diameter, the ONH will not be prominent in the Radon space to provide high detection rates. The effect of this parameter on the detection outcome is reported in the results section. Based on the knowledge of the ONH size and its maximum diameter, an appropriate n is selected. Here, the size of n is chosen to be equal to the maximum diameter of ONH in a database. If the resolution of images is different, n is selected accordingly. For example, for the MUMS-DB database, n was chosen to be 313 pixels, while for the DRIVE database, n was chosen to be 79 pixels and for the STARE database, n was chosen to be 130 pixels. The window size can also be determined automatically based on the scale factor of the imaging system used. Given the actual size of ONH expressed in μm and the scale factor, the ONH size in pixels and subsequently the window size are estimated.Another parameter that has influence on the outcome and also the computational complexity is the windows shifting ratio (named step). If this ratio is equal to one, every pixel is examined just one time and sub-images would have no overlap. Depending on the value of this parameter, each pixel gets examined step times either in horizontal or vertical sliding direction and as a result each pixel gets processed in up to step2 sub-images (see Fig. 4). In general, by selecting a large step, more accurate ONH detection or higher detection rate can be achieved; however, the computational complexity would increase dramatically. The effect of the parameter step on the detection rate as well as the computation time is reported later in the results section. Based on our experimentations, step was set to 4 meaning ¼ size window overlap as this value provided a tradeoff between computational complexity and detection rate.RT is widely used in X-ray tomography. Projections of a 2D function f(x, y) can be represented by a set of line integrals. The Radon function computes the line integrals from multiple parallel paths or beams. The beams are spaced 1 pixel unit apart. To represent an image, the Radon function takes multiple, parallel-beam projections of the image from different angles around its center. Fig. 5shows a single projection at an angle θ. The RT of the function f can be written as(1)f⌣θ(s)=∫−∞+∞∫−∞+∞f(x,y)δ(s−xcosθ−ysinθ)dxdywhere θ and s are depicted in Fig. 5. A single projection of the objectf⌣θ(s)is stated by Eq. (1), where the Dirac delta function δ defines the path of the line integral. This equation expresses the relationship between the function f(x, y) and the projectionf⌣θ(s). The projectionf⌣θ(s)in Eq. (1) may be interpreted as the one-dimensional functionf⌣θ(s)of a single variable s with θ as a parameter. With the arrangement exhibited in Fig. 5,f⌣θ(s)is referred to as a parallel projection. RT is able to transform a pattern to a line in the Radon space and thus allows it to be easily distinguished from other patterns.ONH is characterized by its bright circular characteristic and because of large vessels going into or coming out of it, its edges are ill defined. ONH appears non-uniform in intensity, size, and location. High intensity differences between ONH and background in color or FA fundus images cause ONH to be associated with peaks in the Radon space. Moreover, in color images, the blue (B) component of sub-images is selected for applying RT to detect ONH due to its high contrast between its ONH and background. In the B channel, the red features such as vessels, Microaneurysms, and Hemorrhages have low contrast and only yellowish pattern of ONH or exudates have high contrast. In other words, the B component provides a good separation for the white and yellow colors in comparison to the background and thus it eliminates the need for any preprocessing (see Fig. 6).RT is then applied to the sub-images. In order to avoid the peak in the Radon space occurring along diagonal directions, a circular mask is first considered and RT is applied to the masked sub-images. The masking process is illustrated in Figs. 7 and 8. In order to save computation time, the Radon projections can be limited to for example 12 directions from 0 to 165° in 15° increment.An ONH location candidate in a sub-image is associated with a peak in the Radon space (see Fig. 9). At this stage, all the sub-images which have a peak higher than a threshold are considered to be candidates containing the ONH. The experimentations for the selection of this threshold are reported in the results section. The ONH candidates are then refined to find the final ONH location. ONH candidates are obtained by first transforming all the sub-images to the Radon space. For every sub-image, the maximum along all the projections is considered to be its peak. The sub-image containing the maximum peak, as well as all the sub-images with peaks greater than or equal to 0.9 of the maximum peak are then regarded as ONH candidates.The concept of refinement is similar to examining peaks along all the projection angles in Radon transformation of a sub-image with a central point pattern. Based on this concept, all the sub-images which have a peak profile higher than the above threshold are refined.To detect ONH, the RT property for round objects is used. For a round object, RT provides the same profile along all the directions. Due to the roundness of ONH, the profiles related to the projections do not differ much. As a result, one can detect the sub-image that contains the ONH. This is achieved by computing the mean square error (MSE) between the projections. The MSE between the projections inside the sub-images is used as a similarity measure for the Radon peaks. In other words, the sub-image with the least MSE among all the ONH candidates is used to locate the ONH (see Figs. 10 and 11). The MSE of a sub-image is computed as follows:(2)MSE=1S.Θ∑s=1S∑θ=1Θ(f⌣θ(s)−f¯(s))2wheref⌣θ(s)denotes the (s, θ)th component of the Radon matrix, S and Θ are the dimensions of Radon matrix, S point transform along Θ directions (see Fig. 9) and the vectorf¯is the mean of Radon matrix along Θ directions computed as follows:(3)f¯(s)=1Θ∑θ=1Θf⌣θ(s)After the above validation of ONH, the center of the sub-image is considered to be the center of the ONH. Some sample results of the FA and color fundus images from the three databases examined are shown in Figs. 12–15.The main attribute of our algorithm is its computational efficiency compared with other algorithms. If the input image is of size M×N, the window of size n with the step p, the numbers of operations are as follows:(i)Fundus area detection: NMRadon transform: 9MNp2Θ, where Θ indicates the number of projections in RTRefinement: 3Θnc, where c indicates the number of candidatesHence, the total number of operations involved in our algorithm is NM(1+9Θp2)+3Θnc.

@&#CONCLUSIONS@&#
