@&#MAIN-TITLE@&#
An enhanced harmony search based algorithm for feature selection: Applications in epileptic seizure detection and prediction

@&#HIGHLIGHTS@&#
A feature selection approach is proposed using the harmony search algorithm.A new harmony memory initialization is adopted and dynamic parameters are used.The proposed method and other metaheuristic algorithms give comparable performance.The enhanced harmony search algorithm outperforms the standard algorithm.

@&#KEYPHRASES@&#
Harmony search,Feature selection,Wavelet neural networks,Epileptic seizure classification,Epileptic seizure prediction,

@&#ABSTRACT@&#
Graphical abstractImage, graphical abstract

@&#INTRODUCTION@&#
The task of feature selection entails the search of an optimal feature subset that can best represent a given set of data, and hence, maximizes the performance of classifiers. Feature selection algorithms offer many attractive advantages. Apart from eliminating the redundant and irrelevant features [1], the proposed feature subset, with reduced numbers of features, will indirectly cut down computational cost and time. Moreover, the features selected are useful for the purpose of both data mining and data analysis. The final output of the feature selection algorithm will indicate which features are important in characterizing the entire data set.It should be noted that the term feature selection refers to the algorithms that return a subset of the input features, whereas the term feature extraction encompasses the techniques used to derive new features from the original data set. This pre-processing step is normally accomplished through mathematical transformations. The two aforementioned terms, feature selection and feature extraction, are two related, yet distinct fields of study. This work emphasizes on the former algorithm.The problem of feature selection, which is known to be NP-hard [2], has been studied extensively in the domains of machine learning and knowledge discovery. Mathematically, the problem of feature selection can be formulated as follows: Given an original set of data, A, with a features, the ultimate goal is to find a subset B, with b features, where b < a and B⊂A, so as to maximize the overall classification accuracy. In general, the algorithms used fall into three main categories, namely wrapper, filter, and hybrid methods [3–4].The wrapper methods are classifiers-dependent. As the name suggests, the feature selection process is “wrapped” in a learning algorithm. The classification accuracy reported by the classifiers is used as the criterion to evaluate the quality of the chosen feature subsets. As the classifiers need to be retrained for each of the proposed feature subsets, wrapper methods incur a very high computational cost.Instead of using the classification accuracy of the classifiers to score feature subsets, the filter methods employ a pre-determined evaluation metric to serve the same purpose. As such, these methods are not as computationally intensive as the wrapper methods, as the evaluation metric is relatively simple and easy to compute. Nonetheless, as these methods are independent of classifiers, a feature subset with good evaluation score does not necessarily yield high classification accuracy when the features are trained and tested using the machine classifiers. Unlike the features selected by the wrapper methods, features chosen by filter methods are not specifically tuned to a particular type of classifier.In the case of hybrid methods, they combine the merits of both wrapper and filter approaches. The features are first chosen using filter methods, which are computationally fast and efficient. The subsequent step, accomplished by wrapper methods, concerns the fine-tuning process, which aims at further narrowing down the number of candidate features. This step incorporates the classifiers in the decision making process to determine which features to be included and excluded.To tackle the problem of feature selection, an exhaustive approach can be employed but the colossal amount of computational cost and time imposed by the algorithm render the problem virtually intractable and implausible. To illustrate, for a problem domain with n features, as many as2n−1combinations are possible, provided that at least one feature must be selected. The number of possible feature subsets grows exponentially with the value of n.To circumvent this stumbling block, metaheuristic methods can be used to find sub-optimal solutions. The global search and local search features embedded in metaheuristic algorithms are able to explore the entire solution space efficiently and propose desirably good candidate solutions. Harmony search algorithm is a type of population-based search technique that has gained widespread popularity owing to its attractive advantages over other metaheuristic algorithms.Feature selection has found widespread applications in various fields. In particular, it is a powerful tool that can be used in medical domain to determine the most prominent features that best characterize a certain medical condition. For example, in [5], a novel strategy that uses a weighted combination of four feature selection scores is developed to study the brain activities using the functional magnetic resonance imaging (fMRI). In [6], a binary particle swarm optimization (PSO) algorithm is embedded in the proposed feature selection method to investigate four medical conditions, namely heart, cancer, diabetes, and erythemato-squamous diseases.In the present work, a new variant of harmony search based algorithm is considered in the task of feature selection. The efficiency and robustness of the proposed algorithm is tested and verified using ten UCI benchmark problems, as well as two real life applications, namely epileptic seizure detection and prediction. The remaining of the paper is organized as follows. Section 2 gives an overview of the existing metaheuristic algorithms and their applications in the field of feature selection. Section 3 presents the novel harmony search algorithm. Section 4 details the experimental setup of the standard benchmark problems and two real life applications. Section 5 presents the results and discussion. Lastly, Section 6 concludes the paper and provides some future research directions.Metaheuristics are a class of algorithmic framework that attempts to find sufficiently good or near-optimal solutions by incorporating intelligence in the search process. In general, exploration (global search) and exploitation (local search), both of which are controlled by user-defined parameters, are the two most commonly used concepts or search strategies. Striking a balance between these two search strategies is crucial in defining the success of a particular metaheuristic algorithm.Metaheuristic algorithms draw inspiration from many processes occurring in nature, such as Darwinian natural selection (genetic algorithm), movement of swarm or group of organisms (particle swarm optimization, ant colony optimization, artificial bee colony), flashing behavior of fireflies (firefly algorithm), and echolocation behavior of microbats (bat algorithm). In addition, some metaheuristics are motivated and modeled by man-made processes, such as the heating and cooling of materials (simulated annealing), and the improvisation of harmony in the field of music theory (harmony search).Most of the aforementioned metaheuristic algorithms are first proposed and developed for continuous optimization problems that involve real values. For example, the algorithms are used to locate the minimum or maximum points of hyperplanes. Due to the success of their implementation, these metaheuristic algorithms have been modified and suited for mathematical problems that deal with integers and binary values. In particular, the process of feature selection can be formulated as an optimization problem, where the variables are encoded in a list of bits. These binary values will in turn, determine the inclusion or exclusion of a particular feature in the feature subset.Taking into consideration all the modifications proposed in the literature, a total of three enhancements from different aspects based on the existing metaheuristic strategies are identified and summarized as follows:(i)Initialization: In general, the first set of solutions is generated randomly. The nature of this initialization method and the limited choice of the output values in binary form pose a challenge, as the candidate solutions proposed for the next iteration depend greatly on this initial set of solutions.Search strategy: The local search (exploitation) and global search (exploration) are two important pillars of any metaheuristic algorithms. There is often a trade-off between these two elements. A careful consideration, therefore, should be put forth in determining the rules or updating mechanisms in order to achieve a delicate balance between these two search strategies. Also, instead of using a parameter of a fixed value throughout all the iterations, a dynamic parameter can be introduced to govern the local and global search in the entire search space.Evaluation function: The filter methods operate independently of the learning algorithm, as opposed to the wrapper methods, where the classification accuracy obtained from the machine classifiers is used as the evaluation metric.Originally proposed by Geem et al. [7], the metaheuristic harmony search (HS) algorithm is a population-based evolutionary algorithm that is inspired by the improvisation process of musicians when composing a new piece of music.The HS algorithm has gained widespread attention among researchers from various disciplines. This music-inspired algorithm has been developed and used to solve a wide range of optimization problems in the field of science and engineering. The success of this HS algorithm stems from the fact that this metaheuristic strategy offers many attractive advantages. The merits [7] are listed as follows:(i)The HS algorithm employs a stochastic random search technique instead of a gradient-based search. Therefore, derivative information is not required.The HS algorithm imposes very few mathematical requirements. Hence, the initial value settings of the decision variables are not required.The local and global search embedded in HS algorithm is able to prevent the solutions from getting trapped at local minima.In HS, a new solution is generated based on a set of solutions (usually 10) stored in harmony memory. In contrast, a new solution is derived from only two parent solutions in GA.The HS algorithm can handle both continuous and discrete variables.The five main steps of the HS algorithm are listed as follows:Step 1: Initialize parametersStep 2: Initialize harmony memory (HM)Step 3: Improvise a new harmony (solution)Step 4: Update HM if a better solution is foundStep 5: Repeat step 3 and 4 until stopping criterion is metThe pre-defined parameters are harmony memory size (HMS), harmony memory consideration rate (HMCR), pitch adjusting rate (PAR), distance bandwidth (BW) and number of improvisations (NI). HMS is the number of sets of solutions that are stored in the harmony memory (HM), which is a matrix where each row corresponds to a solution vector. The value of HMCR and PAR are set to be between 0 and 1. These two parameters control the global and local search of the HS algorithm. BW refers to the step size and its value is found using the formulaα(xiU−xiL), where α is a small value (0.01 or 0.001), whereasxiUandxiLare the upper bound and lower bound of a decision variable, respectively. NI is used as stopping criterion. When the number of iteration reaches NI, the search process terminates.The harmony memory (HM) is a matrix that stores solutions generated by the HS algorithm. Initially, a total of HMS solutions are generated. The HM takes the following form:(1)HM=[x11x21⋯xN1f(x1)x12x22⋯xN2f(x2)⋮⋮⋱⋮⋮x1HMSx2HMS⋯xNHMSf(xHMS)].The notationxijdenotes the ith decision variable of the jth solution. In other words, the subscript denotes the position of the decision variable, whereas the superscript denotes the order or number of solution. In (1), each row represents a solution vector,[x1,x2,x3,…,xN]. The cost or fitness function, f evaluates the quality of the solution and stores the output, f(x) in the last column of each row.During this step, a new harmony, or a new candidate solution, is improvised based on the existing solutions stored in HM. The new value of a decision variable,xi′, is obtained using the following rules:(2)xi′←{xi∈[xiL,xiU],ifr>HMCRxi∈HM={xi1,xi2,xi3,…,xiHMS},ifr<HMCRxi+α(xiL−xiU)|xi′∈HM,ifr<PAR,i=1,2,…,N.A random number 0 < r < 1 is generated and its value determines the new numerical value of a decision variable,xi′. The value of r is compared with the two pre-defined parameters, HMCR and PAR. If r > HMCR, then the valuexi′will be taken randomly from the entire possible solution range for that particular decision variable. If < HMCR, then the new value,xi′will take the form of one of the values stored in the same column in HM. If r < PAR, then a small increment size will be added to the existing value. The newly generated value is also checked to ensure that it falls within the range of the solution space.For each of the newly generated solution in Step 3, its quality will be evaluated using the cost function, f. The output, f(x′), is compared with all the other outputs stored in HM. If the new solution is found to be better, the new solution vector, x′ will replace the worst solution in HM. Mathematically, the rule is given as:(3)x′∈HM∧xworst∉HM.The algorithm terminates when it reaches the maximum number of iteration, NI. The best solution is the one that gives the least or greatest output, depending on whether the cost function is to be minimized or maximized. All the remaining sub-optimal solutions that are stored in HM can be used as alternative solutions.The pseudocode of the HS algorithm is given in Fig. 1.The simplicity and the ease of implementation of the standard HS algorithm have given rise to many different improved versions of the algorithm. In [8], the authors point out the drawback of using fixed value parameters. The justification of the need of a dynamic parameter is given as follows: The value of PAR controls the local search. In addition, a larger BW value is needed in the early stage to increase diversification of solutions. Toward the end of the algorithm, a smaller BW value is required to perform the fine-tuning process more efficiently. In view of this, two formulas are proposed in the improved harmony search (IHS) algorithm, where the values of PAR and BW change in each iteration. The value of PAR increases linearly, whereas the value of BW decreases exponentially with each successive iteration.In [9], a novel variant of evolutionary algorithm, called global-best harmony search (GHS) algorithm is presented. The paper highlights the difficulty in tuning the value of BW as BW can take any value from the range (0, ∞). The authors also mention the major drawback of the previous IHS algorithm, that is, the lower and upper values for BW are generally hard to define a priori as they are problem dependent. In GHS, the BW parameter is replaced where the new solution or harmony simply takes the value of the best harmony stored in the HM. The enhanced GHS algorithm works well in both continuous and discrete optimization problems. The paper reveals that a large HMCR value favors optimization problems with high dimensionality.In [10], the issue of premature convergence of the GHS algorithm is debated. An algorithm called self adaptive harmony search (SHS) algorithm is given, where the BW parameter is replaced and the new harmony is updated according to the maximal and minimal values stored in HM. The two formulas proposed are in accordance with the fact that the maximum and minimum values of variables would approach their respective optimum values gradually. The pitch adjustment rule proposed through this mechanism does not violate the boundary constraints imposed on the decision variables. Besides, an initialization method based on the concept of low-discrepancy sequences is employed instead of the standard pseudo-random number based approach. The justification given being that the values generated by the low-discrepancy sequences are more evenly distributed. Despite this new initialization method, it is found that the classification results are not affected by the different initialization methods used.In [11], the self-adaptive global best harmony search (SGHS) algorithm is presented to solve continuous optimization problems. Three parameters, namely HMCR, PAR, and BW, are dynamically adjusted during the improvisation step to generate new harmonies. The updating mechanism for BW is done in such a way that a delicate balance between exploration (during the early stage) and exploitation (during the final stage) is achieved. The effect of using different values of HMS is also investigated. It is reported that the SGHS algorithm favors small values (5 and 10) over larger values (20 and 50). The smaller values are expected because the HS algorithm is imitating musicians’ short-term memory capability.Having reviewed several variants of the HS algorithm, it is worth mentioning that the efforts for improving the standard HS algorithm is devoted to devising new updating mechanisms for several parameters, which include HMS, HMCR, PAR, and BW. Apart from that, different initialization strategies are utilized to enhance the effectiveness of the search algorithm.To employ the HS algorithm in the problem of feature selection, two different approaches can be formulated. These two approaches are covered and discussed in [12], where two new approaches, namely horizontal approach and vertical approach, are presented. In horizontal approach, each musician is assigned one of the two binary values. On the contrary, for vertical approach, a different technique is employed. Here, each musician is treated as an individual expert who will determine which feature to be chosen and included in the feature subset. Different musicians are allowed to choose the same feature. In addition, they can choose no feature at all. The final harmony is generated based on the decisions made by all the individual musicians. The merit of the vertical approach is that it makes good use of the principles embedded in the HS algorithm, where a diverse range of candidate solutions can be generated. Nevertheless, the drawback of the approach is that an additional tunable parameter needs to be introduced, which complicates the structure of the algorithm.In [13], a dynamic parameter is introduced as an alternative to the standard, fixed value. The value of this adaptive parameter changes during various stages (initialization, intermediate, and termination) of the search process. As a result, a wide range of solutions are generated and proposed during the initialization stage. This encourages deep exploration of the entire solution space. During the intermediate stage, a steady improvement in terms of the quality of solutions is noted. Lastly, during the termination stage, further fine-tuning takes place and a fast convergence rate is observed.In [14], an algorithm that couples the HS algorithm with the optimum-path forest classifier (OPF) is proposed. The HS algorithm is chosen due to its simple rules and fast computational speed. The fitness function provided by the OPF serves as rules to guide the search process of the HS algorithm. The efficiency and robustness of the algorithm is tested in the real life application of non-technical losses detection in power distribution systems.In [15], the HS algorithm is used in conjunction with Markov blanket. It is reported that the hybrid wrapper-filter algorithm is able to identify and eliminate redundant genes for the gene selection problems performed on microarray datasets. In addition to preserving the overall classification accuracy, the proposed algorithm also manages to reduce the number of genes (features) effectively.The challenge of incorporating the HS algorithm in the binary feature selection problem lies in the fact that each of the decision variables can take only one of the two possible values. Unlike binary optimization problems, the decision variables for standard continuous optimization problems take a range of values on some intervals. The binary values give a new interpretation to the optimization problems at hands.Furthermore, some of the parameters defined in the standard HS algorithm need to be modified or discarded completely. To illustrate, BW and PAR are not entirely relevant in the context of optimization problems involving feature selection. In the standard HS algorithm, these two parameters are used to fine-tune the value of variables. The concept of neighborhood is involved where a small step size will be added to the existing value. This, however, is not the case for binary decision variables as there exists no relationship between the values of two adjacent decision variables. In light of this shortcoming in the standard HS algorithm, two proposals are given in an enhanced variant of the HS algorithm.The first idea is in the aspect of number of features. It is noticed that the different variants of HS algorithms reported in the literature do not take this factor into consideration. Statistically speaking, letX1,X2,…,Xnbe binary random variables, each represented by the following probability distribution:(4)Xi={0,w.p.P(Xi=0)=121,w.p.P(Xi=1)=12,fori=1,2,…,n.In (4), the abbreviation w.p. stands for with probability. As shown in (4), it is noticed thatP(Xi=0)=P(Xi=1)=0.5.In other words, during the random initialization stage, each decision variable has an equal probability of being assigned a value of 0 or 1. In a feature selection problem with n independent decision variables, the average number of decision variables that will be assigned a value of 1, denoted asX¯onecan be calculated as follows:(5)X¯one=n·E(X)=n·(0·P(X=0)+1·P(X=1))=n·(0·0.5+1·0.5)=n2Put in words, Eq. (5) asserts that when the solution vectors are being randomly initialized in the second step of the HS algorithm, on average, 50% of all the decision variables will be included and the rest will be excluded. The initialization stage is as important as the improvisation step because the solutions generated in this initial phase will direct and guide the successive iterations. The snowball effect comes into mind if the initial pool of solutions is generated poorly. In reality, the optimum number of variables could be less than half the total number of decision variables.To circumvent this problem, a strategy is formulated in such a way that a mixture of differentX¯onevalues is obtained for different solution vectors during the initialization stage. In general, less than 50% of the features are sufficient to give good classification accuracy as they contain most of the vital and useful information that characterize a data set. In view of this, a few constants, namely 1.5, 2, 2.5, 3, and 4 (which correspond to 66.7%, 50%, 40%, 33.3%, and 25%, respectively) are chosen such that the decision variables that will be assigned values of 1 are less than 50% of the total number of decision variables. Specifically, the variableX¯onehas the following probability distribution:(6)X¯one={⌊n1.5⌋,w.p.15⌊n2⌋,w.p.15⌊n2.5⌋,w.p.15⌊n3⌋,w.p.15⌊n4⌋,w.p.15,where n is the number of decision variables and ⌊ · ⌋ is the floor function. This is achieved through an additional checking step. The solution vectors will be initialized randomly without any restriction imposed on them. Next, the number of decision variables that are assigned one will be added up and the value will be compared with the value ofX¯onegenerated from Eq. (6). If the number of variables that are assigned one is greater than (or less than) the value ofX¯one, the decision variables will be excluded (or included) accordingly.Instead of starting with 10 random solutions and allocating all the computational cost to generating and evaluating the successive iterations, a new division is proposed in this work, where 10% of the total computational cost is assigned to generating an initial pool of solutions. Here, Eq. (6) is used to govern the number of decision variables that is assigned one. Each of these solution vectors will be evaluated to obtain the corresponding fitness values. The best 10 solutions will be stored in the HM. The successive improvisation stage uses the remaining 90% of the computational cost. For example, for a problem with 1000 iterations, instead of initializing the HM with 10 random solutions and performing 1000 iterations, this new division will first generate 100 random solutions. The best 10 solutions out of the 100 solutions will be stored in the HM. This is followed by the improvisation stage, where another 900 solution vectors will be generated and evaluated accordingly.The improvisation stage is also deemed a crucial step as new solutions are proposed based on the values of two vital parameters, namely HMCR and PAR. The improvisation steps are achieved through the second proposal which is outlined below. Letxi1,xi2,…,xiHMSbe the values of the ith decision variable, stored in the same column of the HM. Denote |xi| as the sum ofxi1,xi2,…,xiHMS. Because of the nature of the binary variables, a value of|xi|=mindicates that there are m variables that are assigned a value of 1. In other words, these features are included in the feature subset. It should be noted that the terms “features” and “decision variables” refer to the same concept and they are used interchangeably in this work. Define two new variables xMand xm, where the superscript M and m stand for majority and minority, respectively. The two variables are defined as follows:(7)xiM={0,if|xi|<HMS21,if|xi|≥HMS2.(8)xim={0,if|xi|≥HMS21,if|xi|<HMS2.(9)xiM+xim=1,∀i=1,2,…,n.Letxi′be the new value of the ith decision variable, and r be a random value generated uniformly from the interval (0,1). In this work, the updating rules or mechanisms are revised as follows:(i)If r < HMCR,xi′=xiM.If r ≥ HMCR,xi′=xim.Also, the PAR will act as a bit-flip operator. Definexifas:(10)xif={0,ifxi=11,ifxi=0.(iii)If r < PAR,xi′=xif.If r ≥ PAR,xi′=xi(value remains unchanged).Using constant values of HMCR and PAR throughout all the iterations would hamper the search capability of an algorithm. Taking into account this observation, different values of HMCR and PAR are used during different iteration phases (beginning, intermediate, and termination) using the following equations:(11)HMCRi={0.7,i=1,2,…,⌊ntotal3⌋0.7+i−⌊ntotal3⌋⌊ntotal3⌋×0.3,i=⌊ntotal3⌋+1,⌊ntotal3⌋+2,…,⌊23ntotal⌋1.0,i=⌊23ntotal⌋+1,⌊23ntotal⌋+2,…,ntotal,(12)PARi={0.0,i=1,2,…,⌊ntotal3⌋0.3,i=⌊ntotal3⌋+1,⌊ntotal3⌋+2,…,⌊23ntotal⌋{0.0,forA90%1.0,forA10%,i=⌊23ntotal⌋+1,⌊23ntotal⌋+2,…,ntotal,where the subscript i stands for the ith iteration and ntotal is the total number of iteration. Also, A90% is a subset that represents 90% of all the decision variables of the ith iteration selected randomly. The notation A10% is defined similarly.The entire iteration stage is further divided into three phases of equal size. During the beginning phase, The HMCR is set to a constant value of 0.7. This relatively low value is chosen to encourage exploration of the entire solution space so as to diversify the candidate solutions generated. Moreover, the PAR is set to 0.0. This means that PAR does not play a role during this beginning phase. The value of 0.0 is chosen because during the beginning phase, the PAR parameter, which serves as a bit-flip operator, plays little or no role at all at generating the solutions. This is partly due to the nature of the binary variables. In other words, the task of generating solution vectors during this beginning phase is handled and governed completely by the sole parameter, HMCR.Upon the completion of the previous beginning phase, many sub-optimal solutions of good quality have been identified and stored in the HM. Therefore, during the intermediate phase, a linearly increasing function is adopted to define the value of HMCR. As iteration proceeds, the chances of selecting a value from the one stored in the HM increases. Furthermore, the PAR is assigned a fixed value of 0.3. This implies that the values of some of the decision variables will be reversed. This helps the solutions escape from local minima.During the termination phase, the HMCR is assigned a value of 1.0. In other words, the values of all the decision variables are governed by Eqs. (7) and (8), where the concept of majority and minority are used. The justification of selecting this constant value of 1.0 being that many good solutions have been found during the beginning and intermediate phases. As such, the fine-tuning process performed during this termination phase only relies on the value of PAR. Each solution vector will be divided randomly into two disjoint subsets, namely A10% and A90%. This means that the values of 10% of all the decision variables of each solution vector will change or flip between the two values. The values of the remaining 90% of the decision variables will remain unchanged.A specific example is given as follows: For a problem wherentotal=900, the values of HMCR and PAR are defined by the following piecewise functions:(13)HMCRi={0.7,i=1,2,…,3000.7+i−300300×0.3,i=301,302,…,6001.0,i=601,602,…,900(14)PARi={0.0,i=1,2,…,3000.3,i=301,302,…,600{0.0,forA90%1.0,forA10%,i=601,602,…,900,An example of how the values of decision variables of a candidate solution change over iterations is given in Fig. 2. The pseudo code of the proposed harmony search-based feature selection algorithm is given in Fig. 3.In the algorithm detailed in Fig. 3, lines 1–15 involve the initialization stage of the harmony memory, HM. In line 2, the entire data set is first divided into two disjoint sets, namely training set, Strain and testing set, Stest.The for loop in line 3 creates a total of 0.1 · NI sets of candidate solutions. The inner for loop in lines 4–5 assigns a random integer, either 0 or 1, to each of the decision variable in a candidate solution. In line 7, the value ofX¯oneis evaluated. This value is compared with the value generated from Eq. (6) in line 8 and modifications are performed accordingly. Following this step, the new training set,Strain′and testing setStest′are generated, using the criterionxik=1, as given in lines 9–10. The generation or selection of these training and testing sets are illustrated using a simple data set, as shown in Fig. 4. In line 11, the classifier is trained using the training set,Strain′and subsequently, tested using the testing set,Stest′. In line 12, the overall classification accuracy is obtained from the testing set and the value is stored. Having evaluated all the 0.1 · NI solutions, the values of the reported classification accuracy, f(xk) are sorted in descending order in line 14. In line 15, the best 10 solutions, namely the 10 solutions with the highest classification accuracy are identified and they are stored in the harmony memory matrix, HM. With this, the initialization step of HM is complete.The second section of the algorithm, as covered by lines 16–29, is dedicated to the improvisation step, where new solutions are generated using the rules governed by the values of HMCR and PAR. Line 17 creates a total of 0.9 · NI solutions. In line 18, the values of several parameters are evaluated. They include the worst solution, Hworst, that is stored in the HM, as well as the values of|xi|,xiM,andxim.The inner for loop in lines 19–22 is responsible for creating a new harmony or candidate solution, by using the corresponding values of HMCR and PAR. Lines 23–26 are similar to lines 9–12, where the training set and testing set are created. For every candidate solution that is proposed, the quality of the solution is evaluated by comparing the classification accuracy obtained with those that have been stored previously in the HM. If a better solution is found, or in other words, a higher classification accuracy is obtained, then the new solution, or harmony, Hjwill replace the worst solution, Hworst. In line 29, the best solution or harmony, H* is identified from the HM. This solution vector represents the best feature subset that yields the highest classification accuracy.To evaluate the effectiveness and robustness of the proposed algorithm in the task of feature selection, simulations are performed on different data sets. A total of ten different benchmark data sets are selected from the UCI machine repository [16]. Additionally, the algorithm is tested on two real life applications, namely epileptic seizure detection and prediction, using the wavelet coefficients extracted from the biomedical electroencephalography (EEG) signals. For consistency purposes, all the experiments that employ the feature selection stage (standard HS, enhanced HS, PSO, and GA) are run for 1000 iterations.The ten UCI benchmark data sets that are used for simulation are presented in Table 1. The number (size) of sample, number of feature (dimension), and number of class are also listed in Table 1.Let N be the number of features or dimension of a particular data set. All the features in each set of the benchmark data set are replicated in order to introduce redundancy to the existing data set, thereby doubling the dimension or the number of features. Furthermore, a total of N sets of irrelevant feature are generated randomly using Boolean values. On top of this, an additional N irrelevant features are created, where the values of these features are random numbers between 0 and 1, generated using the rand function. Upon the inclusion of the redundant and irrelevant features, a data set with a dimension of N originally has now been recreated and extended to a larger data set with a total dimension of 4N. In other words, with the introduction of additional artificially-generated features, the number of attributes has increased four-fold. The generation and inclusion of these extra pseudo sets of data in the original set of data are illustrated in Table 2. The ultimate goal of the proposed feature selection algorithm is to eliminate the redundant and irrelevant features in the data set, thereby increase the overall classification accuracy.Epilepsy is a very common neurological disorder that arises from the abnormal firing of electrical signals in the brain. It is estimated that there are more than 50 million people worldwide who are diagnosed with this disease [17]. With this staggering amount of patients, the need of better healthcare service in the field of epileptology is of utmost importance. First introduced by German neuropsychiatrist Hans Berger, electroencephalography (EEG) is a versatile clinical tool that is used to study the mechanisms of ictogenesis. Additionally, the biomedical signals recorded provide useful insight into the dynamics of the brain.By examining the EEG signals, the different segments of signals can be identified. In general, the four aforementioned segments are interictal (seizure-free period), pre-ictal (before the onset of seizure), ictal (during the occurrence of seizure), and post-ictal (after the onset of seizure). In the task of epileptic seizure detection, the goal is to differentiate between interictal and ictal EEG signals. In the absence of automated machine classifiers, trained epileptologists are resorted to performing visual inspection manually on the EEG signals that could be recorded for days, or even weeks, for pre-surgical evaluation purpose. As such, scrutinizing EEG signals manually is a time-consuming process, not to mention the high medical cost incurred. The development of machine classifiers or expert systems that can aid in the automated classification of EEG signals is hence of paramount importance.The benchmark data set used for the purpose of epileptic seizure detection in this study is obtained from [18]. The five groups of EEG signals are labeled A until E. Each set of data consists of 100 segments, where each segment is an EEG signal recorded for 23.6s, at a sampling frequency of 173.61Hz, which accounts for the total amount of 4097 data points. The five sets of data are recorded from different people subjected to different conditions. Set A and set B were both recorded from healthy volunteers. Set A was recorded with their eyes open, whereas set B was recorded with their eyes closed. On the contrary, set C until E were recorded from epileptic patients. Both set C and D were recorded during the interictal or seizure free period. Set C was recorded from the hippocampal formation of the opposite hemisphere of the brain. On the other hand, set D was recorded from a different region of the brain, namely within the epileptogenic zone. The last set of data, set E, was recorded when the patients were experiencing seizure. In other words, set E contains ictal data. In short, set A until D belong to the same class of normal EEG signal, as opposed to set E, which contains epileptic EEG signal. The summary of the data is given in Table 3.The research in epileptic seizure detection has grown exponentially over the past decades. Recent works on epileptic seizure classification reported in the literature are discussed in [19], where the issue of generalization of epileptic seizure detection is examined. Instead of designing machine classifiers that can detect patient-specific seizures, the works reported in [19] are dedicated to developing expert systems that are able to perform such classification tasks across all epileptic patients.The raw EEG signals are obtained in the time domain, where the magnitude of the amplitude is recorded over a period of time. Nonetheless, the temporal nature of the signals does not provide much useful information. Therefore, a suitable transformation is deemed necessary for the subsequent feature extraction stage [20]. The Daubechies wavelet of order 4 (db4) is chosen to analyze the EEG signals, based on the empirical finding and recommendation given in [21]. In this work, the 4 levels of decomposition scheme is employed, which yields five groups of wavelet coefficients that correspond to different frequency subbands: d1 (43.4–86.8Hz), d2 (21.7–43.4Hz), d3 (10.8–21.7Hz), d4 (5.4–10.8Hz), and a4 (0–5.4Hz). For each group, eight summary statistics are computed. They are (i) maximum, (ii), minimum, (iii) 90th percentile, (iv) 10th percentile, (v) mean, (vi) standard deviation, (vii) skewness, and (viii) kurtosis of the wavelet coefficients.The mean, μ, standard deviation, σ, skewness, s, and kurtosis, k, are defined as(15)μ=∑i=1NxiN,(16)σ=∑i=1N(xi−μ)2N,(17)s=E(x−μ)3σ3,(18)k=E(x−μ)4σ4,where x is the variable under study, N is the total number of variable, and E(x) is the expected value of x.Therefore, each EEG signal is characterized by a total of5×8=40values or features. The job of the feature selection algorithm is to identify which, among these 40 features, are the most imperative in the task of differentiating between normal and epileptic EEG signal.While the task of epileptic seizure detection aims at differentiating between interictal and ictal EEG signals, the task of epileptic seizure prediction intends to distinguish between pre-ictal and ictal EEG signals. In [22], the use of relative combinations of sub-band spectral powers of EEG signals to study the EEG signals is reported. The existence of the so-called pre-ictal period has been confirmed and reported in [23], where a decrease in the value of dynamical similarity is noticed just minutes before the occurrence of seizure from the intracranial EEG signals. The localization of the pre-ictal EEG signals is extremely beneficial and of great importance in the medical community as it could assist greatly in the development of an alarm system that is able to issue warning to the epileptic patients of impending seizure attacks. With the realization of such warning system, many seizure-related injuries such as drowning and road accidents can be prevented, thus increasing the quality of life of patients.The data set used for the purpose of epileptic seizure prediction is acquired from [24]. The publicly available data set, recorded at the Epilepsy Center of the University of Freiburg, Germany, contains invasive EEG recordings of 21 epileptic patients. The recordings were performed using a Neurofile NT digital video EEG system with 128 channels. The sampling frequency used was 256Hz. For each of the 21 patients, six electrodes were used to acquire the EEG signals. Three of them, termed focal electrodes, were placed on seizure onset areas, whereas the remaining three were extrafocal electrodes. The EEG recording of each patient contains a minimum of 24h of interictal signals and 50mins of pre-ictal signals. The number of seizures experienced by each of these patients ranged from 2 to 5. For consistency purpose, only 10 patients, each with 5 seizures, were chosen for analysis. The details of the patients and the type of seizure are summarized in Table 4.For this work, a time window of 16s is used. With a sampling frequency of 256Hz, this gives each EEG segment a total of 4096 data points. For feature extraction, the same method, as explained in Section 4.2 is adopted. Each EEG signal is hence represented by a vector of 40 values. Since the EEG recordings were done simultaneously using 6 electrodes (3 focal and 3 extrafocal), this gives a total of6×40=240values.

@&#CONCLUSIONS@&#
