@&#MAIN-TITLE@&#
A new bound for the midpoint solution in minmax regret optimization with an application to the robust shortest path problem

@&#HIGHLIGHTS@&#
We develop a new lower bound for minmax regret combinatorial optimization problems.We give algorithms that improve the approximation guarantee of the midpoint solution.We use the new lower bound to improve exact branch and bound algorithms.

@&#KEYPHRASES@&#
Combinatorial optimization,Minmax regret,Robust optimization,Approximation,Robust shortest paths,

@&#ABSTRACT@&#
Minmax regret optimization aims at finding robust solutions that perform best in the worst-case, compared to the respective optimum objective value in each scenario. Even for simple uncertainty sets like boxes, most polynomially solvable optimization problems have strongly NP-complete minmax regret counterparts. Thus, heuristics with performance guarantees can potentially be of great value, but only few such guarantees exist.A popular heuristic for combinatorial optimization problems is to compute the midpoint solution of the original problem. It is a well-known result that the regret of the midpoint solution is at most 2 times the optimal regret. Besides some academic instances showing that this bound is tight, most instances reveal a way better approximation ratio.We introduce a new lower bound for the optimal value of the minmax regret problem. Using this lower bound we state an algorithm that gives an instance-dependent performance guarantee for the midpoint solution that is at most 2. The computational complexity of the algorithm depends on the minmax regret problem under consideration; we show that our sharpened guarantee can be computed in strongly polynomial time for several classes of combinatorial optimization problems.To illustrate the quality of the proposed bound, we use it within a branch and bound framework for the robust shortest path problem. In an experimental study comparing this approach with a bound from the literature, we find a considerable improvement in computation times.

@&#INTRODUCTION@&#
Robust optimization is a paradigm for optimization under uncertainty, that has been receiving increasing attention over the last two decades. While many variants of robust optimization exist, most aim at optimizing the worst-case performance of a solution without knowledge of a probability distribution over the uncertain input data. For general surveys on robust optimization, see Ben-Tal, Ghaoui, and Nemirovski (2009), Ben-Tal and Nemirovski (2002), Goerigk and Schöbel (2013).One of the best-known approaches in robust optimization is to minimize the maximum difference in the objective value of the robust solution over all scenarios, compared to the best possible objective value achievable in each scenario. This approach is usually known as minmax regret, see Kouvelis and Yu (1997), Aissi, Bazgan, and Vanderpooten (2009).However, due to its min-max-min structure, these problems are typically very hard to solve, both from a theoretical and from a practical point of view (for theoretical complexity, we refer to Aissi, Bazgan, and Vanderpooten (2007); 2009); Averbakh and Lebedev (2005), while practical complexity is demonstrated, e.g., for the minmax regret spanning tree problem (Kasperski, Makuchowski, and Zieliński, 2012; Montemanni, 2006; Yaman, Karaşan, and Pınar, 2001)).Thus, heuristic algorithms with performance guarantees are highly desirable; however, only few such heuristics for minmax regret versions of general combinatorial problems exist. To the best of our knowledge, the heuristic with the current best approximation ratio of 2 for interval data is the midpoint algorithm (Conde, 2012; Kasperski and Zieliński, 2006).While there exist academic instances that show the tightness of this approximation ratio, the midpoint solution shows a considerably better performance in practice. In this paper we present an instance-dependent approach to determine an upper bound on the approximation ratio that is always at most 2. While this approach can be applied to any combinatorial optimization problem, we discuss problem types where this bound can be computed in strongly polynomial time.Furthermore, we analyze the performance of the proposed bound within a branch and bound (BB) framework for the robust shortest path problem. Comparing the bound with the one proposed in Montemanni, Gambardella, and Donati (2004), we find considerable improvements in computation times.The remainder of this paper is structured as follows. In Section 2, we briefly recapitulate the formal definition of the minmax regret problem, and introduce necessary notation. The main part of this paper is the lower bound on minmax regret problems introduced in Section 3, which is then used to derive stronger upper bounds on the approximation ratio of the midpoint solution. To this end, an auxiliary problem needs to be solved, which is described in detail in Section 4. We proceed to discuss how this bound can be computed for the shortest path problem, the minimum spanning tree problem, the assignment problem, and the minimums−tcut problem in the same section. Section 5 describes how to apply the analysis to partially fixed solutions, and how this can be used within a branch and bound algorithm. In Section 6 we present computational data to compare our bound to other approaches. Finally, we conclude this work and point out further research questions in Section 7.Many combinatorial optimization problems (e.g., shortest path, min cut, minimum spanning tree, ...) can be represented asPminx∈X∑i=1ncixi,whereX⊆{0,1}n=:Bndenotes the set of feasible solutions, and cithe costs of element i. We refer to(P)as the classic optimization problem.In contrast to classic optimization where one assumes to know the set of feasible solutions and the cost function exactly, in robust optimization it is assumed to have uncertainty in the set of feasible solutions and/or the cost function. In this paper, as usual for minmax regret optimization, we consider the case where the set of feasible solutions is known and only the cost function is uncertain. That is, instead of considering only one specific cost vector c we assume to have an uncertainty setU⊆Rnof different possible cost vectors c.There are several ways to reformulate such an uncertain optimization problem to a well-defined problem formulation, which is also known as the robust counterpart (Goerigk and Schöbel, 2013). Here we consider the approach of minmax regret optimization. The regret is defined as the difference between the realized objective value and the best possible objective value over all scenarios. Formally, the regret problem can be formulated asRminx∈Xmaxc∈U(∑i=1ncixi−valc*),wherevalc*:=miny∈X∑i=1nciyidenotes the best possible objective value in scenarioc∈U.As frequently done for such problems, we consider uncertainty setsUwhich are the Cartesian product of intervals, i.e.,U:=×i=1n[c̲i,c¯i].Unfortunately, the minmax regret problem has a higher computational complexity than the original optimization problem in most cases. For example, the minmax regret problem of the shortest path problem, which is a polynomially solvable problem for a positive cost vector c, is strongly NP-complete for interval uncertainty (Averbakh and Lebedev, 2004).A frequently used scenario for the interval uncertainty is the midpoint scenario, where every entry of the cost vector ciis just the average of the lower boundciand the upper boundc¯i. This average is denoted asc^i,i.e.,c^:=12(c̲+c¯).As NP-complete problems are often hard to solve exactly, one can try to find approximate optimal solutions in polynomial time. One such approach is to use the midpoint solutionxmid:=argminx∈X∑i=1nc^ixi,which can be computed by solving the midpoint scenario of the classic optimization problem of type(P). The midpoint solution is always a2−approximation (Kasperski and Zieliński, 2006). In other words, the regret of the midpoint solution xmidis always not more than twice the regret of the optimal solution of(R).There are problem instances where this approximation guarantee is tight. But for a lot of instances the midpoint solution has smaller regret than twice the optimum regret, and is often even an optimal solution to(R). As the2−approximation result is tight we can of course not improve this result for all instances, but we can improve the approximation guarantee for specific instances with only small computational effort. To do so, we first show an easy technique to find a lower bound for the optimal value of the minmax regret problem, and then use the lower bound obtained this way to improve the2−approximation guarantee depending on the instance. Furthermore, one consequence of our new analysis method is the already proven2−approximation guarantee for the midpoint solution.To simplify the presentation we introduce some more notations. We abbreviateN:={1,…,n},and denote byval(x,c):=∑i=1ncixithe objective value of a solution x for the cost vector c. Furthermore, denote byopt(c)=argminx∈X∑i=1ncixiany optimal solution of the classic optimization problem for a given cost vector c, and byReg(x)=maxc∈U(val(x,c)−valc*)the (maximum) regret for a solutionx∈X. We write OPT forminx∈XReg(x).Note that we do not use any probability distribution in the above definition of the minmax regret problem. Still, we make use of probability distributions in the following analysis. To this end, denote by P a probability distribution over the setU,and byEP(·)the expected value with respect to P. We follow the convention to denote random variables with capital letters (e.g., we write C instead of c to denote random cost vectors).We specify the elements that are chosen in a solutionx∈Xbyx={i∈N:xi=1}. Furthermore, we define the worst-case scenario of x as cx, wherecix:={c¯iifi∈xc̲iifi∉x.This definition extends to vectorscS∈U,where S is any subset ofN. One directly finds the following result, which is also shown in Aissi et al. (2009).Lemma 2.1For allx∈X,we have thatReg(x)=val(x,cx)−valcx*.The following observation is crucial for the later analysis.Observation 3.1Let any probability distribution P over the uncertainty setUbe given. Then,Reg(x)=maxc∈U(val(x,c)−valc*)≥EP(val(x,C)−valC*)for allx∈X.For any fixed probability distribution P, we have:OPT≥minx∈Xval(x,EP(C))−EP(valC*)(1)OPT=minx∈Xmaxc∈U(val(x,c)−valc*)≥minx∈XEP(val(x,C)−valC*)(2)=minx∈XEP(val(x,C))−EP(valC*)(3)=minx∈Xval(x,EP(C))−EP(valC*)Inequality (1) follows from Observation 3.1, and Equalities (2) and (3) use the linearity of the expectation and the objective function.□We call a probability distribution P centered ifEP(Ci)=c^ifor alli=1,…,n.Using Lemma 3.2, we get for all centered probability distributions P a lower bound LB(P) for the optimal value of the regret problem.Definition 3.4Given a centered probability distribution P, we setLB(P):=val(xmid,c^)−EP(valC*)Note that LB(P) is indeed a lower bound of OPT, asOPT≥minx∈Xval(x,EP(C))−EP(valC*)=minx∈Xval(x,c^)−EP(valC*)=val(xmid,c^)−EP(valC*)=LB(P).To get the best such bound, one has to find a centered probability distribution that minimizesEP(valC*). Due to the linearity of the objective function in c we know thatvalc*≥valc̲*for allc∈U. Hence, it follows thatEP(valC*)≥valc̲*for all probability distributions P.As a consequence we also haveLB(P)≤val(xmid,c^)−valc̲*. This gives us an immediate upper bound for the lower bound. Note that this upper bound is not necessarily a lower bound for(R).Denote byPthe set of all centered probability distributions onU. We consider the problem of finding the best possible lower bound, which can be considered as the optimization problemminP∈PEP(valC*).To solve this problem efficiently we restrict the solution space to the special classP′of probability distributions defined as follows. We say a scenario inUis an extreme scenario, if and only ifci∈{c̲i,c¯i}for alli∈N.P′are all probability distributions that have only two outcomes c1 and c2, which are both extreme scenarios and equally likely. As the probability distributions are centered, it follows that the scenarios c1 and c2 must be complementary in the sense that ifci1=c̲i,it follows thatci2=c¯iand vice versa for alli∈N.Hence, we can identify every probability distributionP∈P′by the elementsS={i∣ci1=c¯i}⊆Nthat attain their upper bound cost in the first scenario. As before, denote by cSthe extreme scenario whereciS=c¯i∀i∈SandciS=c̲i∀i∈S¯. The probability distributionP∈P′identified with the set S therefore has the nice property thatEP(valC*)=0.5(valc1*+valc2*)=0.5(valcS*+valcS¯*). Hence, the optimization problem can be simplified tominP∈PEP(valC*)≤minP∈P′EP(valC*)=minP∈P′12(valc1*+valc2*)=12minS⊆N(valcS*+valcS¯*)(S)How to solve problem(S)is further analyzed in Section 4. We denote by LB* the lower bound that is given by an optimal solution of(S). We are now in the position to state the main result of this paper.Theorem 3.5The midpoint solution xmidis aλ−approximation for the minmax regret problem(R),whereλ=val(xmid,cxmid)−valcxmid*val(xmid,c^)−minS⊆N12(valcS*+valcS¯*)≤2.Furthermore, we have thatOPT=0⇔Reg(xmid)=0.From the preceding argumentation follows that(4)OPT≥val(xmid,c^)−12minS⊆N(valcS*+valcS¯*).To verify the approximation property we have to show thatReg(xmid)OPT≤λ. To this end, we estimate:(5)minS⊆N(valcS*+valcS¯*)≤valcxmid*+valcxmid¯*≤val(opt(cxmid),cxmid)+val(xmid,cxmid¯).The first inequality follows from the fact that the left hand side is the minimum of a set valued function over all possible sets S, and the right hand side is just the same function evaluated for one specific set (the set xmid). The second inequality holds asval(opt(cxmid¯),cxmid¯)≤val(xmid,cxmid¯).Note that2val(xmid,c^)−val(xmid,cxmid¯)=val(xmid,cxmid),as seen by the following equations.2val(xmid,c^)−val(xmid,cxmid¯)=2∑i∈xmidc^i−∑i∈xmidc̲i=∑i∈xmid2c¯i+c̲i2−c̲i=∑i∈xmidc¯i=val(xmid,cxmid)We reconsider inequality (5). By addingval(xmid,cxmid)on the left hand side and2val(xmid,c^)−val(xmid,cxmid¯)on the right hand side we get:minS⊆N(valcS*+valcS¯*)+val(xmid,cxmid)≤val(opt(cxmid),cxmid)+val(xmid,cxmid¯)+2val(xmid,c^)−val(xmid,cxmid¯)By rearranging these terms we get:(6)val(xmid,cxmid)−val(opt(cxmid),cxmid)≤2val(xmid,c^)−minS⊂N(valcS*+valcS¯*)From Lemma 2.1 we know that the left hand side of (6) equals Reg(xmid). IfReg(xmid)=0,the midpoint solution is already the optimal solution and we are done, as 0 is a lower bound for all regret problems. If Reg(xmid) > 0 we use (4) and (6) to show that OPT > 0.2OPT≥2val(xmid,c^)−minS⊂N(valcS*+valcS¯*)≥Reg(xmid)>0Hence we concludeReg(xmid)=0⇔OPT=0.Assuming that Reg(xmid) > 0, we know thatval(xmid,c^)−minS⊂N12(valcS*+valcS¯*)>0and we can divide it on both sides of Inequality (6) to getλ:=val(xmid,cxmid)−val(opt(cxmid),cxmid)val(xmid,c^)−minS⊂N12(valcS*+valcS¯*)≤2.From Lemma 2.1 and Inequality (4) follows the desired approximation guaranteeReg(xmid)OPT≤λ.□Theorem 3.5 motivates the analysis of the optimization problem(S). If the set xmidsolves the optimization problem(S)to optimality, we get(7)minS⊆N(valcS*+valcS¯*)=val(opt(cxmid),cxmid)+val(opt(cxmid¯),cxmid¯)=val(opt(cxmid),cxmid)+val(xmid,cxmid¯).we can use this equality to tighten the proof of Theorem 3.5 and conclude thatλ=2in this caseObservation 3.6If the set xmidsolves the optimization problem(S)to optimality, thenλ=2.λ=2is a direct consequence of Eq. (7) and the fact that2val(xmid,c^)−val(xmid,cxmid¯)=val(xmid,cxmid):λ=val(xmid,cxmid)−val(opt(cxmid),cxmid)val(xmid,c^)−minS⊂N12(valcS*+valcS¯*)=val(xmid,cxmid)−val(opt(cxmid),cxmid)val(xmid,c^)−0.5val(opt(cxmid),cxmid)−0.5val(xmid,cxmid¯)=val(xmid,cxmid)−val(opt(cxmid),cxmid)0.5val(xmid,cxmid)−0.5val(opt(cxmid),cxmid)=2□If, on the other hand, xmidis not the optimal solution, there is a possibility to improve the approximation guarantee.As a general heuristic for the problem(S)we can first solve the classic optimization problem for the cost vectorc, denote the solution that is obtained in this way by x′, and then use the partition (x′, x′′) for(S),where x′′ is the complement of x′. To evaluate the objective functionvalcx′*+valcx′′*we just have to solve one additional classic optimization problem for the cost vectorcx′to obtainvalcx′*,as we have already computedvalc̲*=valcx′′*. In the next section we consider problem(S)in more detail.As before, let an arbitrary combinatorial optimization problem of type(P)be given. The purpose of this section is to compute the best possible lower bound LB*. To solve problem(S),we need to find a partition of all elements fromNinto the two sets S andS¯. We can represent problem(S)in the following way.(8)min∑i=1nxi(c̲i+zi(c¯i−c̲i))+∑i=1nyi(c¯i−zi(c¯i−c̲i))(9)s.t.x,y∈X(10)z∈BnThe partition of the setNis modeled via the variables zi, which determine if an element is in S or inS¯,respectively. Additionally to the z variables, we compute two feasible solutionsx,y∈Xsuch that the total sum of objective values is minimized.Note that the formulation (8)–(10) is nonlinear. As only binary variables are involved, it can easily be linearized using the following problem formulation, where z′ indicates which elements are used in both solutions x and y.min∑i=1n(c̲ixi+c̲iyi+(c¯i−c̲i)zi′)s.t.zi′≥xi+yi−1∀i∈Nx,y∈Xz′∈BnHowever, for the purpose of the following analysis, the problem structure becomes more obvious by writingmin∑i=1nfi(xi+yi)s.t.x,y∈Xwithfi:{0,1,2}→R,fi(u)={0ifu=0c̲iifu=1c̲i+c¯iifu=2In the next sections we show how to solve this problem formulation efficiently for the shortest path, the minimum spanning tree, the assignment, and the minimums−tcut problem. Note that the minmax regret counterpart of all these problems is strongly NP-complete (Aissi, Bazgan, and Vanderpooten, 2005; 2008; Averbakh and Lebedev, 2004).Let a graphG=(V,E,c)be given, with node set V, edge set E, and a cost functionc:E→R+,as well as two nodes s, t ∈ V. The goal is to find the shortest path from s to t, where the length of a path is the sum of the costs ceover every edge e of the path. We can describe the shortest path problem as a combinatorial optimization problem over the ground set E with feasible setX={x∣xrepresents as−tpath}.To solve (S), the idea is to create a new multigraphG′=(V,E′,c′)with the same node set V and the edge setE′={e=(i,j),e*=(i,j)∣∀e=(i,j)∈E}containing the old edge set E as well as one additional parallel edge e* for every original edge e ∈ E. The cost functionc′:E′→R+is defined to becefor every edge e ∈ E andc¯efor all copied versions e*.Problem(S)reduces to finding two edge-disjoints−tpaths in G′, as can be seen by the following relationship between two paths in G with the cost function f as defined in Section 4.1, and two edge-disjoints−tpaths in G′. If an edge e and the copied version e* is used on both paths, this edge pair contributes costs of sizec̲e+c¯eto the objective function, which are exactly the same costs as paid in problem(S)for using an edge twice. If only one edge of the edge pair e and e* is used, it will always be edge e in an optimal solution due to its lower costs, and hence the contribution in the objective function isce, which are again the same costs as paid in problem(S)for using an edge once. If no edge of an edge pair is used, no costs accrue in both model formulations.The problem of finding two edge-disjoint paths in G′ can be represented as a minimum cost flow problem or can be solved directly using, e.g., Suurballe’s algorithm (Suurballe and Tarjan, 1984).Theorem 4.1LB* can be computed in strongly polynomial time for the shortest path problem.Let again a graphG=(V,E,c)be given with node set V, edge set E, and a cost functionc:E→R+. The goal is to find a spanning tree with minimal costs where the cost of a tree are just the sum over the costs ceover every edge e in the tree. We can describe the minimum spanning tree problem as a combinatorial optimization problem over the ground set E with feasible setX={x∣xrepresentsaspanningtree}.We can reutilize the idea used for the shortest path problem (see Section 4.2) and create the same auxiliary multigraphG′=(V,E′,c′). But instead of searching for two edge-disjoints−tpaths, we search for two edge-disjoint minimum spanning trees in G′. This problem can be solved efficiently as shown in Roskind and Tarjan (1985).Theorem 4.2LB* can be computed in strongly polynomial time for the minimum spanning tree problem.In the assignment problem we are given a complete bipartite graphG=(V,E,c)with a bipartition of the node set in equally sized setsV1∪V2=Vand a cost functionc:E→R+. The goal is to find a perfect matching of minimum total cost.It is a well known result that the assignment problem can be transformed to a min cost flow problem on an auxiliary graphG′=(V′,E′,c′,u′)(Ahuja, Magnanti, and Orlin, 1993). The idea is to enlarge the graph by introducing one new source node s and one new sink node t to the graph and to connect s with all vertices of V1 and t with all vertices of V2. The capacity of all edges is set to 1 and the cost of all edges incident to s or t is set to 0.We define an auxiliary graphG′′=(V′′,E′′,c′′,u′′)similar to G′ by using the same methodology as well as the idea used for the shortest path or minimum spanning tree problem. As before, we introduce one new source node s and one new sink node t to the graph and connect them with the vertices of V1 and V2. Further, we create for every edge e ∈ E a parallel copy e*. The costs of all connector edges are set to 0 and tocefor all edges e ∈ E and toc¯efor all copies e*. The capacity of every connector edge is set to 2 and the capacity of every edge e ∈ E and their corresponding copy e* is set to 1. We can translate a flow with flow value 2|V1| in G′′ to two assignments. Note that due to the capacity constraints of the connecting edges and the required flow value, two units of flow leave every node of V1 and enter every node of V2. We call two nodes v and u connected if flow is sent from v to u or from u to v. If both units of flow leaving a node of v1 ∈ V1 are sent through an edgee=(v1,v2)and their corresponding copye*=(v1,v2),v1 is assigned to v2 in both assignments. After removing all nodes assigned in that way, we have that every node of V1 is connected to two different nodes of V2 and vice versa. We now pick an arbitrary not yet assigned node v1 ∈ V1 that is connected to v2 andv2′∈V2. We assign v1 to v2 in the first assignment and delete the flow sent from v1 to v2. By doing this we reduce the number of nodes that are connected to v2 from 2 to 1. Letv1′be the node that is still connected to node v2. We assignv1′to v2 in the second assignment and delete the flow sent fromv1′to v2. By doing this we reduce the number of nodes that are connected tov1′from 2 to 1. Hence, we know again which assignment pair we have to pick for the first assignment.By repeating this argumentation we generate a cycle, see Fig. 1. Every edge picked with his original orientation in this cycle represents a part for the first assignment. All other edges picked represent a part of the second assignment. If there are still not yet assigned nodes left, we can just start the process again by picking a new not yet assigned nodev1*∈V1. The argumentation about the costs is the same as in the shortest path or minimum spanning tree problem. If a specific assignment is made twice, two units of flow must be sent through edge e and their corresponding copy e* contributingc̲e+c¯eto the costs. If a specific assignment is made only once, in an optimal solution the unit of flow will always be sent over the cheaper edge e contributingceto the costs.Theorem 4.3LB* can be computed in strongly polynomial time for the assignment problem.Let a graphG=(V,E,c)be given with node set V, arc set E, a cost functionc:E→R+as well as two nodes s, t ∈ V. The goal is to find ans−tcut separating s and t with minimal costs. We can describe the minimums−tcut problem as a combinatorial optimization problem by using the following well known mixed-integer programming formulation (Lawler, 1976). The variable deindicates if arc e is used in thes−tcut.min∑e∈Ecedes.t.de−pi+pj≥0∀e=(i,j)∈Eps−pt≥1d∈B|E|p∈R|V|We transform the graph G to the graphG′=(V′,E′,c′)by replacing each arc with a pair of arcs. We introduce for every arce=(i,j)∈Ea new node veas well as two new arcse1=(i,ve)ande2=(ve,j). The node setV′=V∪{ve∣∀e∈E}consists of all nodes from V combined with all new nodes. The arc setE′={e1,e2∣∀e∈E}is the set of all new generated arcs. The new cost function c′ is defined as,c′(e1)=c̲eandc′(e2)=c¯e. It can be seen thatthe following problem is equivalent to(S)for the mins−tcut problem:min∑e∈E′dec̲e+d˜ec¯es.t.de−pi+pve≥0∀e=(i,j)∈E′d˜e−pve+pj≥0∀e=(i,j)∈E′ps−pt≥2d,d˜∈B|E′|p∈R|V′|Furthermore, as the coefficient matrix for the above problem is totally unimodular, we may solve the LP relaxation instead. As the dual of the LP relaxation is a min-cost flow problem, we have shown the next theorem.Theorem 4.4LB* can be computed in strongly polynomial time for the minimums−tcut problem.To find the optimal solution of NP-complete minmax regret problems one might resort on branch and bound algorithms. An important ingredient of a successful branch and bound algorithm is an effective and easy to compute lower bound for every node in the branch and bound tree. As every node in a branch and bound tree defines a restriction for the set of feasible solutions, we have to show how the lower bound introduced in this paper can be computed, if parts of the solutions are already fixed, to make it usable in this context.Assume a branch and bound node defines a restricted feasible setX′⊂X. Hence the minmax regret problem in this node reduces tominx∈X′maxc∈U(val(x,c)−valc*)Denote byIN(X′)={i∈N∣xi=1∀x∈X′}the set of all ground elements that have to be part of every feasible solution inX′and byOUT(X′)={i∈N∣xi=0∀x∈X′}the set of all ground elements that must not be part of any feasible solution inX′. Using Lemma 2.1 we can restrict the setUto the setU′={c∈U∣ci=c¯i∀i∈IN(X′)∧ci=c̲i∀i∈OUT(X′)}without changing the problem. Next we use the argumentation as before, but instead of letting P be an arbitrary distribution overU,we choose an arbitrary distribution P′ overU′.minx∈X′maxc∈U(val(x,c)−valc*)=minx∈X′maxc∈U′(val(x,c)−valc*)≥minx∈X′EP′(val(x,C)−valC*)=minx∈X′EP′(val(x,C))−EP′(valC*)=minx∈X′val(x,EP′(C))−EP′(valC*)Note that if the solution is completely fixed, i.e.,X′={x′},then alsoU′={c′}reduces to a set with only one element. In this case the probability distribution P has only one outcome – the vector c′ – and, hence, the lower bound is equal to Reg(x′), which is an upper bound of the optimal regret value. Therefore, the gap between lower and upper bound vanishes for this branch and bound node.As before, we assume we are given a graphG=(V,E,c)with node set V, edge set E, a cost functionc:E→R+as well as two nodes s, t ∈ V. In Montemanni et al. (2004) a branch and bound algorithm for the minmax regret shortest path problem is presented. We refer to Montemanni et al. (2004) for a detailed description of the branching scheme. Let a node in the branching tree be given, and let this node be characterized by the setX′of feasible solutions. Then they use the following lower bound on the regret of this node:LBold(X′)=SP(c¯,IN(X′),OUT(X′))−SP(cOUT(X′)¯,∅,∅)where SP(c, A, B) is the value of the shortests−tpath including all edges in A and excluding all edges of B for the cost vector c. Applying our approach we get as lower bound for a distribution P′.LBnew(X′)=minx∈X′val(x,EP′(C))−EP′(valC*)=∑e∈IN(X′)c¯e−c^e+SP(c^,IN(X′),OUT(X′))−EP′(valC*)The problem of choosing a good distribution P′ can again be reformulated as a min cost flow problem or can be solved directly with Suurballe’s algorithm applied to an accordingly adjusted instance, see Section 4.2. Again, we create for each arc e a parallel copy e*, but instead of assigning cost ofceto e andc¯eto e* for all arcs, we assign∀e∈IN(X′)(∀e∈OUT(X′))costs ofc¯e(ce) to arc e and his copy e*. For all other arcse∉IN(X′)∪OUT(X′)we use the common cost assignment. Hence the effort of computingLBnew(X′)is roughly the same as for three shortest path computations, which is still reasonable compared to the two shortest path computations needed to computeLBold(X′). A direct comparison of both bounds is presented in Appendix B.

@&#CONCLUSIONS@&#
