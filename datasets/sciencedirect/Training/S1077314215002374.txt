@&#MAIN-TITLE@&#
RIMOC, a feature to discriminate unstructured motions: Application to violence detection for video-surveillance

@&#HIGHLIGHTS@&#
A novel and compact feature discriminating structuredness of observed motions.A feature embedded in a weakly supervised learning framework.An efficient method for real-time violence detection in on-board video-surveillance.Ability of the learned model to generalize training data for varied contexts.A new dataset representative of the targeted application for extensive evaluation.

@&#KEYPHRASES@&#
Violence detection,Aggression detection,Violent event detection,Unstructured motion,Abnormality detection,Video-surveillance,

@&#ABSTRACT@&#
In video-surveillance, violent event detection is of utmost interest. Although action recognition has been well studied in computer vision, literature for violence detection in video is far sparser, and even more for surveillance applications. As aggressive events are difficult to define due to their variability and often need high-level interpretation, we decided to first try to characterize what is frequently present in video with violent human behaviors, at a low level: jerky and unstructured motion. Thus, a novel problem-specific Rotation-Invariant feature modeling MOtion Coherence (RIMOC) was proposed, in order to capture its structure and discriminate the unstructured motions. It is based on the eigenvalues obtained from the second-order statistics of the Histograms of Optical Flow vectors from consecutive temporal instants, locally and densely computed, and further embedded into a spheric Riemannian manifold.The proposed RIMOC feature is used to learn statistical models of normal coherent motions in a weakly supervised manner. A multi-scale scheme applied on an inference-based method allows the events with erratic motion to be detected in space and time, as good candidates of aggressive events.We experimentally show that the proposed method produces results comparable to a state-of-the-art supervised approach, with added simplicity in training and computation. Thanks to the compactness of the feature, real-time computation is achieved in learning as well as in detection phase. Extensive experimental tests on more than 18 h of video are provided in different in-lab and real contexts, such as railway cars equipped with on-board cameras.

@&#INTRODUCTION@&#
Automated vision-based analysis of human behaviors is an important task with many applications in surveillance, ambient assisted living, customer behavior analysis, gaming, concept-based video retrieval, automatic video annotation and summarization. The problem of generic human behavior interpretation is still very complex and cannot yet be solved with one type of approach. Most of the time, it is the application with its specific context and final objective that drive the type of approach to use. As reported by Salah et al., contextual information constrains the problem that involves complex spatiotemporal and semantic reasoning. Context can include scene geometry (view point and 3D composition), scene type (outdoor, indoor, street, football field), object types in scene, people in scenes (number and appearance), knowledge of prior actions and interaction semantics [1].In this paper, we focus on the problem of violent behavior detection for video-surveillance applications. Whereas action recognition has been widely studied, the literature about violence detection is much sparser. Violent or aggressive behaviors are complex to model and detect, especially because: they have not a unique definition and show great variability; they may need specific interpretation relative to the context to be confirmed; and their intensity may be assessed with some subjectivity. As for any human action, there exist large variances in body poses, non-rigid body movements, camera angles, clothing textures, lighting conditions. The additional difficulty is that violent behavior cannot be always summarized by a precise action or gesture, but is generally reflected by a set of human interactions. Aggression on the street, bag snatching with violence, vandalism and riot are pretty different from gesture of fighting, boxing or sumo wrestling. In addition, complex real-life environments (dynamic and cluttered background, high people density, occlusions) make scene analysis more difficult.Researchers have addressed violence detection with many contexts of application. Some studies are devoted to violence detection in movies for automatic tagging of violent scenes. Both audio and visual cues (e.g., explosion, car-braking, crash, gunshot, flame, skin with blood) are generally detected. By the way, this task has been part of the annual challenge MediaEval since 2011 [2–4]. To qualify violent scenes in this task, either a subjective definition is adopted (scenes one would not let an 8-year-old child see because they contain physical violence) or a more objective one (physical violence or accident resulting in human injury or pain). But both have very high-level of semantic, very difficult to translate in the computer vision world. Other studies address the similar application of web video content analysis for automatic rating, tagging and filtering [5]. In all these articles, the multimedia analysis means that not only visual cues are used, but also audio cues, textual tags, etc. When comparing the importance of each modality separately, audio cues are the most efficient [6]. However, in video-surveillance applications, audio and color are not necessarily available, as pointed out by Deniz et al. [7]. And explosions, blood and running are useful cues for action movies, but rare in real-world situations.Other authors focus on fight detection, but in general, with few (two) people in a facilitated context (isolated in front of uncluttured background) [8]. Well-defined gesture or unitary interactions can then be modeled and detected. Even if it is not their primary focus, many datasets for general action recognition containing some instances of fight can then be used (e.g., “Punching” or “Sumo wrestling” actions from UCF101 dataset). On the contrary, a dedicated dataset for fight detection in hockey games was proposed by Bermejo et al. [9]. Still, Deniz et al. [7] noticed that the detection on this dataset was easier than on action movie dataset because fights in televised hockey footage were more consistently similar and also because the supervised classifier was biased by other cues (e.g., camera zoom-in during fights contrasting with wide-angle shots during non-fight segments). Thus, this highlights the importance of the training and testing data and, consequently, raises the question of which application the violence detector is intended for.Our work focus on violence detection for video-surveillance as automated alert systems are necessary to filter the useful information for the security operator monitoring a wall of CCTV screens. Some studies dealing with this topic use again audio and visual cues [10–15]. Fewer tackle the problem by vision only. Interestingly, even within surveillance-purpose, the developed approaches can be very different according to the type of scene present in the dataset. For example, to detect violence in a crowd, Hassner et al. consider the crowd as a dynamic texture as they assume people appear in low resolution from a viewpoint far from the scene [16].Even if different levels of analysis were studied (body pose, body part detection, trajectory analysis...), most of authors worked on developing low-level features used in a learning framework to detect violent event in a more efficient and generic manner. Often, the detection problem is transformed into a classification problem by considering video clips pre-segmented from a video stream. The most frequently adopted framework is a supervised binary classifier trained on numerous examples of violent and non-violent events, i.e., the direct “reduction” to two classes of the multi-class classification problem tackled in generic action recognition. The features used can be generic descriptors for describing any action (MoSIFT [17] or descriptors computed on spatiotemporal interest points STIP [18]) combining appearance and/or motion information. The major drawbacks of supervised approaches are: to gather a representative training dataset with balanced number of positive and negative examples, to manually label them which is tedious and expensive. Furthermore, it is difficult to have pure positive instances of the violent behaviors in real-world environments because a temporal and spatial annotation (segmentation) is necessary to train only on pure events without influence of background normal (non-violent) elements.More rarely, one-class classification is used for violence detection [19]. Nevertheless, violence in video-surveillance is a low frequency yet high impact event. Consequently, abnormal event detection approaches seem totally appropriate for such a rare event. Indeed, these methods do not require data for the rare (abnormal) event to be detected but only normal events in order to model, during a training stage, what will not be detected. This weakly supervised learning approach is particularly interesting in an applicative point-of-view because violence data is much more difficult to collect and annotate than non-violent data. The main drawback is that even if abnormal detected events are rare and unexpected [20], they are not necessarily interesting for the purpose of violence detection if the feature used is not appropriate, i.e., if it cannot discriminate violent from non-violent events. Indeed, the choice of descriptors substantially determines the type of rare events that will be detected (e.g., crowd escape panic [21,22], traffic-jam, wrong way or tail-gating detection [23–26]). Thus, it is generally more efficient to define features specific to targeted events.This is why we propose in this paper a novel Rotation-Invariant feature modeling MOtion Coherence (RIMOC) that intends to code structured motions and discriminate them from erratic or jerky motions, assumed to be present in violent behaviors. A codebook of visual words representing this unitary structured motion is learned offline on non-violent data, as well as their space-time statistical configurations. Then, these models feed an abnormal event detection framework, and a multi-scale scheme applied on an inference-based method allows the events with jerky motion to be detected in space and time, as good candidates of aggressive events.This feature should be specific to violent events but also generic enough so as the learned models not to be too much dependent on the specific appearance of the normal events observed during the training stage. Indeed, training data will hardly represent all the normal non-violent events. As pointed by Deniz et al., “appearance also may confuse the detector” and overfitting the training dataset is a risk [7]. Ideally, the learned models could be transferred from one camera view to another for whole adaptivity of the method. Some other important criteria when dealing with efficiency of a method for a surveillance application are the real-time processing (e.g., “detection alert should be made within a few seconds of the outbreak of violence” [16]) and the compactness in memory (e.g., too large feature make unfeasible the learning of a visual codebook on a large training dataset, even with a parallel K-means [7]).The most relevant contributions of this paper are the following:•it is one of the rare studies tackling the video-based detection of aggressive events in complex and realistic surveillance environment, such as railway cars with dynamic cluttered background and occlusions;it defines a novel and compact low-level feature space that codes the structuredness of the observed motions;it provides a first attentional module for a greater cascade system of violence detection by embedding this feature in a multiscale scheme to detect good candidates of aggressive events occurring at different locations and scales;it provides experimental tests showing that the use of the proposed feature space embedded in a statistical anomaly detection framework can produce results comparable to a state-of-the-art supervised approach, with added training and computation simplicity;it provides extensive experimental tests using more than 18 h of video and 276 aggressions, both from real and lab-recorded sequences and from different contexts, some with poor-quality CCTV cameras.it provides temporally annotated ground truth for violent event and jerky motion detection in a publicly available dataset, as well as a new dataset for real-life violence for video-surveillance applications and results on a new on-board violence surveillance dataset.thanks to the simplicity and compactness of the feature, and an easy training process, a real-time implementation of both learning and detection phases has been achieved.This paper is organized as follows. Section 2 reviews related work. Section 3 briefly explains the overall method proposed in this paper. Section 4 presents the novel feature space used to code the low-level structure of the motions. The learning and detection frameworks utilized in the experiences are described in Section 5. Section 6 describes the existing datasets and those used to get the experimental results presented in Section 7. A discussion of the results, jointly with considerations for future work is presented in Section 8 and Section 9 concludes the paper.

@&#CONCLUSIONS@&#
