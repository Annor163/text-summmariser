@&#MAIN-TITLE@&#
Geometric multiscale decompositions of dynamic low-rank matrices

@&#HIGHLIGHTS@&#
We show that for any manifold our transform achieves an optimal N-term approximation rate for piecewise smooth curves.We introduce a general method to define weighted averages in manifolds, based on the notion of retraction pairs introduced in the paper.We describe several algorithms taylored to the geometry of the Stiefel manifold and provide numerical examples.An application in the compression of hyperspectral image data concludes the article.

@&#KEYPHRASES@&#
Riemannian data,Low-rank approximation,N-term approximation,Compression,Manifold-valued wavelet transforms,

@&#ABSTRACT@&#
The present paper is concerned with the study of manifold-valued multiscale transforms with a focus on the Stiefel manifold. For this specific geometry we derive several formulas and algorithms for the computation of geometric means which will later enable us to construct multiscale transforms of wavelet type. As an application we study compression of piecewise smooth families of low-rank matrices both for synthetic data and also real-world data arising in hyperspectral imaging. As a main theoretical contribution we show that the manifold-valued wavelet transforms can achieve an optimal N-term approximation rate for piecewise smooth functions with possible discontinuities. This latter result is valid for arbitrary manifolds.

@&#INTRODUCTION@&#
This paper is concerned with two themes, namely the handling of manifold-valued data in general and more specifically the efficient compression and analysis of sequences of low-rank matrices.Low-rank matrix or tensor-valued sequences arise in many important applications, whenever low-rank approximations of a parameterized family of large matrices or tensors (which otherwise would not be accessible computationally) are considered.Examples include, but are not limited to, signal processing (in particular video and hyperspectral imaging), latent semantic indexing in information retrieval, dynamic graphs (and the associated graph Laplacians), solutions of high-dimensional time-dependent PDEʼs (such as the chemical master equation or the Schrödinger equation), or more general parameter dependent linear operators.In order to connect this specific type of data to geometry, write a matrixA∈Rn×nof rank p as(1)A=USV⊤,where(2)U,V∈Rn×p,U⊤U=V⊤V=IpandS∈Rp×p.One particular way to arrive at a decomposition of the form (1) is to apply a truncated singular value decomposition to A in which case S is always a diagonal matrix. Due to (2), the matrices U, V lie in the orthogonal Stiefel manifoldSt(n,p), see Section 2.2 below.In this work we exploit this geometric structure to develop wavelet-type multiscale decompositions for sequences of low-rank matrices of a fixed rank. These decompositions, which are in the spirit of Rahman et al. (2006), turn out to be optimally adapted to piecewise smooth data which in practice most often occurs. We prove this for general manifolds and confirm our results numerically. As an application we develop a compression scheme for hyperspectral image data.The contributions of this paper are in two directions. Firstly, we study general manifold-valued multiscale decompositions and their properties. In this regard, one of our main findings is an existence theorem for geometric averaging operations based on arbitrary choices of retractions onto the manifold analogous to the exponential mapping, see Section 2.1. In Karcher (1977) similar averages are developed which however require the efficient computability of the exponential and logarithm mapping of the manifold – an assumption which for instance in the case of the Stiefel manifold is not fulfilled. Based on geometric averaging operations we show how wavelet-like multiscale decompositions can be constructed. These constructions are much in the spirit of Rahman et al. (2006) but with more freedom in the choice of averaging operations.After introducing these constructions we prove our main theoretical result. In Theorem 3.7 we show that the manifold-valued wavelet decompositions achieve an optimal best N-term approximation rate for piecewise smooth functions. For linear wavelet constructions this result is of course also true and it may be regarded as one of the pillars of their success. Our result states that this behavior can be retained in arbitrary manifolds.A good part of the proofs of our results relies on ideas developed in earlier work on the approximation theory of nonlinear wavelet transforms, such as in Grohs and Wallner (2009), Grohs (2010b). With these technical tools in hand, it turns out that our main result can be established without too much technical difficulty. Nevertheless, we consider the theoretical result that the manifold-valued wavelet transform enjoys an optimal best N-term approximation rate of great importance for applications.Secondly, we study a number of results, algorithms and applications tailored to the specific case of the Stiefel manifold. This geometry which – as we have already outlined – is of tremendous importance for many applications is not treated in Rahman et al. (2006) and presents many challenges. For instance, Rahman et al. (2006) assumes the efficient computability of exponential and logarithm mapping, which is not given in the Stiefel manifold, especially for high dimensional matrices. As a remedy, we develop alternative retractions in Section 2.2.3 and present practical algorithms for their computation. By our general results these constructions perform as well as those based on the exponential mapping, but with a huge gain in efficiency.To demonstrate the practical usefulness of our algorithms we develop a compression scheme for hyperspectral image data in Section 4.2.To put this paper in perspective we comment on some related work. In regards to multiscale analysis of geometric data there exists by now a substantial body of work (Rahman et al., 2006; Grohs and Wallner, 2009; Nava Yazdani and Yu, 2011; Weinmann, 2012; Grohs, 2010b). Our paper builds on these results and ideas. On the other hand, low rank approximation is probably the oldest and by far most popular and significant model reduction technique in computational science.The theory of dynamic low-rank approximation resulting in piecewise smooth families of low-rank matrices has been studied in Koch and Lubich (2008, 2010), Nonnenmacher and Lubich (2008), Baumann and Helmke (2003), Bunse-Gerstner et al. (1991), Dieci and Eirola (1999). We also refer to this work for further information on application areas.Structured low-rank approximation has been studied in Chu et al. (2003). We expect our work to be extendible to decompositions of the type studied in that paper.Other applications of data with values in the Stiefel manifold and related geometries can be found in Absil et al. (2008), Edelman et al. (1999), Hairer et al. (2006).Even though in the present paper we confine the discussion to low-rank matrices, we expect an extension to the tensor case to be feasible, see also Koch and Lubich (2010).To facilitate the reading we give a brief outline of the paper. In Section 2.1 we start by defining geometric notions of averages in general manifolds. The main result of this section is a proof of well-definedness of these constructions, extending classical results in Karcher (1977). After that, in Section 2.2.3 we study specific choices for retractions on the Stiefel manifold which are necessary in the definition of a geometric average. We obtain explicit and to our knowledge new expressions and algorithms for several computationally attractive retractions and their inverses.In Sections 3.2 and 3.1 we give two constructions of multiscale transforms of manifold-valued functions, inspired by Rahman et al. (2006) but based on our more general averaging operations. Then, in Section 3.3 we prove our main theoretical result, namely that these constructions attain an optimal approximation rate for piecewise smooth functions.Section 4 contains applications and computational experiments, first with synthetic data and later, in Section 4.2, we present a method for the compression of hyperspectral image data. Finally, in Section 4.3 we comment on the interesting work (Koch and Lubich, 2008) for the purpose of obtaining piecewise smooth low-rank approximations to a given matrix curve.We use the symbolA≲Bto describe that a quantity A is bounded by a fixed constant times the quantity B. Matrices are always written in boldface letters. We use the symbolInfor then×nidentity matrix and0nfor then×nzero matrix.The present section contains basic geometric facts which we will use later on. In the first Section 2.1 we treat general manifolds and exhibit useful methods of defining intrinsic notions of a weighted average of points. These results universally apply to any Riemannian manifold and are of independent interest.Then, in Section 2.2 we specify these findings to the geometry of the Stiefel manifold. In particular we construct several useful notions of retractions which, along with their inverses, turn out to be efficiently computable.For now we consider a general Riemannian manifold M with Riemannian metric〈⋅,⋅〉Mdefined on the tangent bundle TM. We assume some familiarity with the basics of Riemannian geometry, a good entrance point into this topic is DoCarmo (1992). An important operation in our analysis will be the addition of a tangent vector to a point. Classically for Riemannian manifolds, this is done via the exponential mappingexpx(ξ)which maps a tangent vectorξ∈TxM, the tangent space of M at a pointx∈M, to the endpoint of the (locally) unique geodesic curve emanating from x with initial velocity ξ. The exponential mapping also possesses an inverse which maps two pointsx,y∈Mto the (locally) unique vectorlog(x,y)∈TxMsuch that(3)expx(log(x,y))=y.In order to arrive at computationally efficient algorithms we are also considering alternative pairs(f,g)of functions, analogous to the pair(exp,log).Definition 2.1A pair(f,g)of functions(4)f:TM→M,g:M×M→TMis called a retraction pair if(5)f(x,g(x,y))=yfor allx,y∈M,and(6)f(x,0)=x,∂∂ξf(x,0)=Idfor allx∈M.In general f will only be locally defined around M and g around the diagonal ofM×M. Certainly, the pair(exp,log)satisfies the above assumptions (DoCarmo, 1992) and therefore forms a retraction pair.Given a retraction pair(f,g), points(xi)i=1d∈Mdand weights(wi)i=1d∈Rdsuch that(7)∑i=1dwi=1,we are now going to define a manifold-valued weighted average of the pointsxiwith weightswi. As a motivation we first consider the linear caseM=R, where the retraction pair induced by the exponential map is given byfLIN(x,v)=x+vandgLIN(x,y)=y−x. In that case we certainly know how to compute a weighted average(8)x⁎=∑i=1dwixi.Clearly the definition (8) makes no sense in a manifold-valued context since we cannot form linear combinations of points in M. Nevertheless, we can rewrite (8) slightly to arrive at the alternative definition of the averagex⁎as the unique pointx⁎∈Rwith(9)∑i=1dwigLIN(x⁎,xi)=0.Note that we have used∑i=1dwi=1. The expression (9) makes sense for a general manifold M with retraction pair(f,g). In the following we show that under some assumptions on the points(xi)i=1dand for any retraction pair(f,g)on a manifold M, there exists a unique pointx⁎which satisfies∑i=1dwig(x⁎,xi)=0∈Tx⁎M. Therefore we will later define this pointx⁎as a geometric average of the points(xi)i=1d.We now show the important fact that for any retraction pair, the definition motivated above yields a well-defined notion of average. To formulate our result we denoteBδ(p)the geodesic ball of radiusδ>0aroundp∈M.Proposition 2.2Let M be a Riemannian manifold with retraction pair(f,g). For each(wi)i=1d∈Rdwith∑i=1dwi=1andp∈Mthere existΔ(p)⩾δ(p)>0, such that for all(xi)i=1d⊂Bδ(p)(p), there exists a uniquex⁎∈BΔ(p)such that∑i=1dwig(x⁎,xi)=0∈Tx⁎M.As a main ingredient in the proof of this result we use the following theorem which is proven in Groisser (2004), Corollary 2.6(a).Theorem 2.3Assume M is a Riemannian manifold and thatBρ⊂Mis a convex geodesic ball of radiusρ>0such the sectional curvature of M restricted toBρis bounded from below and from above. Then there existsε>0(only depending on the maximal and minimal sectional curvature ofBρand the convexity radius ofBρ) such that for every vector field ϒ defined onBρsatisfying the pointwise bound‖ϒ‖⩽εand‖Id+∇ϒ‖⩽ε(∇ denoting the covariant derivative), there exists a unique pointx⁎∈Bρsuch thatϒ(x⁎)=0.A more quantitative version of the previous theorem can be found in Groisser (2004), Theorem 2.8. We can now proceed with the proof of Proposition 2.2.Proof of Proposition 2.2A short computation involving (6) and (5) shows that by puttingϒy(x):=g(x,y)to be the vector field of difference vectors pointing toy∈M, we get(10)∇ξϒx(x)=−ξ,x∈M,ξ∈TxM,where ∇ denotes the covariant derivative. Indeed, this may be seen by first differentiating (5) w.r.t. x (if necessary in a chart) which yields∂∂xf(x,g(x,y))+∂∂ξf(x,g(x,y))∂∂xg(x,y)=0.Using (6) this implies that withx=yId+∂∂xg(x,y)=0.Finally, using the fact thatϒy(y)=0, we deduce that∂∂xϒy(x)|x=yξ=∇ξϒy(y),which gives (10). Therefore, using (7), together with the fact thatg(y,y)=0for ally∈M, we can find for allp∈M,ε>0radiiΔ(p)⩾δ(p)>0such that for all points(xi)i=1d⊂Bδ(p)(p)andx∈BΔ(p)(p)we have(11)‖∑i=1dwig(x,xi)‖⩽εand(12)‖Id+∇∑i=1dwiϒxi(x)‖⩽ε.Eqs. (11) and (12), together with Theorem 2.3 applied to the vector fieldϒ:=∑i=1dwiϒxisuffice to establish the claim. □The well-definedness result in Proposition 2.2 leads us to the following definition of a geometric average.Definition 2.4Let M be a manifold with retraction pair(f,g). For points(xi)i=1d∈Mdand weights(wi)i=1d∈Rdsatisfying∑i=1dwi=1we define the averageavg((wi),(xi))as the unique pointx⁎which satisfies the balance equation(13)∑i=1dwig(x⁎,xi)=0∈Tx⁎M.For the case of(f,g)=(exp,log)the associated geometric average is known as the Karcher-mean and has been studied in Karcher (1977).We have used the results of Groisser (2004) in our proof of well-definedness of the averaging operation. In this work one can also find algorithms for the computation ofavg((wi),(xi)). For completeness we mention a simple iterative fixed-point method in Algorithm 1, which converges to the desired average. A better choice would be a Newton-type method, see again Groisser (2004) and also Absil et al. (2008) for more information.In our experiments we have found that the iteration in Algorithm 1 typically requires 3–5 iterations until it reaches machine precision.We now specify our previous findings to the geometry of the Stiefel manifold. But first we start with a short motivation why we are interested in this particular geometry. After that we will exhibit several useful choices for retractions.The fact that geometry plays a role in low rank approximation has already been noted in Absil et al. (2008), Edelman et al. (1999) (and possibly much earlier elsewhere) where it is observed that the spaceMn,p:={A∈Rn×n:rank(A)=p}ofn×nmatrices of rank p admits a manifold structure. In this paper we will actually not use this structure but factorizations of the form (1) with matrices U, V which lie on the Stiefel manifoldSt(n,p):={X∈Rn×p:X⊤X=Ip}.The main result which enables us to do this is the existence of a smooth singular value decomposition:Theorem 2.5(SeeChern and Dieci, 2001, Theorem 2.4.) Assume thatγ:t↦A(t)∈Mn,pis aCscurve. Then there existCsmatrix curvesγU:t↦U(t)∈St(n,p),γV:t↦V(t)∈St(n,p),γS:t↦S(t)∈Rp×p,such thatA(t)=U(t)S(t)V(t)⊤.We now assume that we are given a piecewise smooth curveγ:I→Mn,p, whereI⊂Ris an interval. This curve induces three piecewise smooth familiesγU:t↦U(t)∈St(n,p),γV:t↦V(t)∈St(n,p)andγS:t↦S(t)∈Rp×p.The curveγSis simply a vector valued curve which can be further processed with standard data processing methods such as wavelets. For the curvesγU,γVthis is not the case since they take values in a nonlinear space. When we wish to compress these curves it is important to respect the underlying nonlinear constraints so that the columns of the resulting matrices are maximally uncorrelated, ensuring minimal redundancy.It is easy to see that the setSt(n,p)carries the structure of a smooth manifold by viewing it as the zero set of the submersionF:{Rn×p→Rsymmp×p,X↦X⊤X−Ip.From this we also see thatdim(St(n,p))=np−12p(p+1).Furthermore, it is straightforward to obtain the following simple expression for the tangent space attached to a pointX∈St(n,p):TXSt(n,p)={ξ∈Rn×p:ξ⊤X+X⊤ξ=0p}.Attaching the linear tangent space in X to the point X yields the affine tangent spaceTXaffSt(n,p):=X+TXSt(n,p).The metric properties of the Stiefel manifold are slightly more complicated. In particular there exist two nonequivalent canonical choices for a metric onSt(n,p). The first one views the manifold as a factor of the orthogonal groupO(n):St(n,p)=O(n)/O(n−p),by identifying the Stiefel manifoldSt(n,p)with the equivalence classes[Q]:={Q(Ip00Qn−p):Qn−p∈O(n−p)},Q∈O(n). The tangent space ofSt(n,p)at[Q]is identified with the horizontal space at Q consisting of vectors(14)Q(μ−η⊤η0n−p),whereμ∈Rp×pis skew symmetric, see Edelman et al. (1999). The Riemannian metric is defined by restriction of the metric onO(n). The so-defined metric is sometimes called the canonical metric. Another approach defines the Riemannian metric by viewingSt(n,p)as a Riemannian submanifold ofRn×pwith the metric coming from the inner product〈ξ,η〉:=tr(ξ⊤η).We will mainly focus on the latter approach which we call the embedded metric.We now turn to the problem of defining retractions on the Stiefel manifold. A natural choice would be to take the exponential function. Of course, the functions exp and log depend on the Riemannian metric imposed onSt(n,p).For the canonical metric the exponential and logarithm functions can be easily defined using the matrix exponentialexp(ξ):=∑j⩾0ξjj!,ξ∈sl(n)and its local inverse log. Then, for a vectorξ∈TXSt(n,p),X=[Q]∈St(n,p), where ξ has a representation of the form (14) we can putexpX(ξ):=[Qexp((μ−η⊤η0n−p))].Algorithmically, these formulas require us to deal withn×nmatrix operations which is clearly impractical if p is much smaller than n. There exist other expressions for the exponential function associated with the canonical metric which only require the handling ofn×pmatrices, see Edelman et al. (1999), but no simple expression for the logarithm function is known to us.For the embedded metric, a formula for the computation of the exponential function is given byexpX(ξ)=(X,ξ)exp(X⊤ξ−ξ⊤ξIpX⊤ξ)(Ip0p)exp(−X⊤ξ),see e.g. Edelman et al. (1999), Absil et al. (2008). Again, we are not aware of a computationally simple expression for the logarithm mapping.Our requirement is that f and g should both be computable efficiently. Probably the most efficient choice for f is given byf(X,ξ):=π(X+ξ),whereξ∈TXSt(n,p)andπ:Rn×p→St(n,p)denotes the closest-point projection ontoSt(n,p)with respect to the metric induced by〈⋅,⋅〉. This can be computed via a polar decomposition of a matrixX=RHwithR∈St(n,p)andH∈Rp×pis symmetric. This decomposition is unique whenever X is nonsingular and can be computed efficiently (Kovarik, 1970; Higham, 1989). Algorithm 2gives a quadratically convergent iterative scheme. See also Absil et al. (2012) for further information about this projection. In particular, by the results of Absil et al. (2012) (or by a short and easy computation), for this choice of f, Eq. (6) holds true.We also need to find a simple expression for the inverse g of f. It turns out that such an expression exists.Proposition 2.6Assume thatX⊤Yhas only eigenvalues with positive real part (in other words,−X⊤Yis a Hurwitz matrix). Then we haveg(X,Y)=2Y∫0∞exp(−tX⊤Y)exp(−tY⊤X)dt−X,orπ(2Y∫0∞exp(−tX⊤Y)exp(−tY⊤X)dt)=Y.Before we get to the proof we collect some further auxiliary results regarding the embedded geometry of the Stiefel manifold. For a general matrixZ∈Rn×pandX∈St(n,p), the orthogonal projectionπTXaffSt(n,p)onto the affine tangent space in X is given by(15)πTXaffSt(n,p)(Z)=X+Z−Xsym(X⊤Z),wheresym(A):=12(A+A⊤)denotes the symmetric part of a matrix. We also need a description of the normal spaceNXSt(n,p)ofSt(n,p)at a pointX∈St(n,p). We haveNXSt(n,p)={XS:S∈Rp×p,Ssymmetric}and the orthogonal projection onto the normal space is given by(16)πNXSt(n,p)(Z)=Xsym(X⊤Z).With these preparations we can now proceed with the proof of the representation of g.Proof of Proposition 2.6Clearlyg(X,Y)is uniquely defined as the matrix ξ which lies both in the affine tangent space at X and the normal space of Y. Therefore, we must have thatξ=YS,S∈Rp×p,Ssymmetric,such thatX⊤(ξ−X)+(ξ−X)⊤X=0p.Therefore we haveX⊤(YS−X)+(YS−X)⊤X=0p.Now we use thatX⊤X=Ipand obtain(17)X⊤YS+SY⊤X−2Ip=0p.Equations of the form (17) are well-known and frequently studied in the area of control theory, and it turns out that an explicit solution formula for S symmetric is given by the expression of our proposition, see Sontag (1998). □In practice we will not use Proposition 2.6 directly, but computeg(X,Y)via the iterative procedure outlined in Algorithm 3:Using (16) and (15), this iteration can be computed very efficiently.Lemma 2.7The iteration inAlgorithm 3converges linearly towardsg(X,Y).ProofThe iterative procedure in Algorithm 3 is an instance of a very common way to iteratively compute the intersection of two convex sets (in our case affine subspaces) by alternating projections. A proof of convergence of these algorithms can be found in Bregman (1967), Malick (2004). □Again, in our experiments we have found that the iteration in Algorithm 3 typically requires 3–5 iterations until it reaches machine precision.For the construction of the pair(f,g)we started with the choice of f and found its inverse g. Another approach to construct a retraction is to define the function g first and then find out whether the corresponding f is efficiently computable. A natural choice is to put(18)g˜(X,Y):=πTXaffSt(n,p)(Y)−X=Y−Xsym(X⊤Y),the orthogonal projection onto the tangent space in X. In contrast to the previous example, it is now not so clear how to compute the corresponding retractionf˜. The next result, which can be found in Hairer et al. (2006), Chapter IV.9, shows how to easily compute it. For the convenience of the reader we also provide a short proof. The retraction f is called orthographic retraction in Absil et al. (2012) and studied therein.Proposition 2.8The functionf˜, satisfying(19)f˜(X,g˜(X,Y))=Yfor allY∈St(n,p)andg˜as in(18)can be taken to bef˜(X,ξ):=π˜X(X+ξ), where(20)π˜X(Y):=Y+XS,whereS∈Rp×pis symmetric and satisfies the algebraic Riccati equation(21)Y⊤Y+SY⊤X+Y⊤XS+S2−Ip=0p.ProofFirst note that, if we want (19) to hold, then certainlyf˜(X,ξ)=X+ξ+XSfor some symmetric matrixS∈Rp×p. Therefore, since alsof˜(X,ξ)⊤f˜(X,ξ)=Ip,we get Eq. (21). □We would like to remark that Eq. (21) can be solved efficiently, see for instance Pappas et al. (1980).We briefly summarize what we have achieved so far. For the Stiefel manifold we have constructed efficiently computable pairs(f,g)of functions satisfying (4), (5) and (6). These will be crucial in our later construction of a multiscale decomposition of Stiefel-valued curves.Using the above defined retraction pairs we can now define a geometric averaging operation onSt(n,p)which satisfies certain natural invariance properties.There exists a natural transitive action τ of the groupO(n)onSt(n,p)viaτQ(X):=QX,Q∈O(n).Lemma 2.9For all functions g considered above, the average is invariant under the action τ: we haveavg((wi),(τQ(Xi)))=τQ(avg((wi),(Xi))).ProofThis follows from the fact that the functions g are always invariant under the action of τ. □Lemma 2.9 shows that the geometric averaging operation defined in this section is invariant under an orthogonal change of coordinates, as it should be. Fig. 1shows the different choices of retractions for the circle which is equal toSt(2,1).Remark 2.10In order to compute a weighted average of points(Xi)i=1da minimal requirement is that there exists a pointX⁎such that all operationsg(X⁎,Xi),i=1,…,dare well-defined. In this respect, the inverse of the closest-point projection appears to be the worst choice, see Fig. 1. If this requirement is not fulfilled, some ad-hoc strategies seem necessary. In our experiments we simply compute the linear average of the points(Xi)i=1dand project the result back onto the manifold.In this section we describe the wavelet-type decomposition of manifold valued data, using a retraction pair of functions(f,g)as in the previous section. All results of the present section apply to general manifolds M, therefore, for now, we shall consider a manifold M, together with a pair of functions(f,g)such that (4), (6) and (5) hold. Furthermore, we assume that an averaging operation defined by (13) is well-defined.For instance in the general case of a Riemannian manifold M and(f,g)=(exp,log), where exp and log denote the exponential resp. the logarithm function on M, it can be shown that all these assumptions are valid. Another example is, of course, M the Stiefel manifold and the functions(f,g)defined in the previous section.Up to minor variations, the material in Section 3.1 and Section 3.2 is taken from Rahman et al. (2006). The main difference is that we allow for more general retractions than the exponential function and that we utilize a different, in our view more natural notion of geometric average.One of the main theoretical results of this paper is contained in Section 3.3 where it is shown that manifold valued wavelets are capable of approximating piecewise smooth functions with the same efficiency as globally smooth functions.The general idea for the construction of manifold valued wavelet transforms is to start with a linear construction and replace all averaging operations by geometric averages and all difference operations by the point–point difference function g. For more information on wavelets in general we recommend Daubechies (1992), Chui (1992).Convention:Since our wavelet constructions will be based on geometric averages which in general are only locally defined we understand all the theoretical results in this section under the tacit assumption that all underlying operations are well-defined.Here, we describe the construction of interpolating wavelets in manifolds using geometric averaging operations, see Rahman et al. (2006), Grohs and Wallner (2009), Donoho (2001) for more information. The corresponding linear construction is due to Donoho (1992). We first describe this linear construction. The basic idea is that a smooth functionu:R→Rcan be well approximated by a polynomial. Define the sampling operatoruh:=(u(hi))i∈Zand pick an odd integer D. We try to exploit possible redundancies between the samplesu2−jandu2−(j+1)by computing a simple estimate(22)uˆ2−(j+1)∼u2−(j+1),based only on the knowledge ofu2−j. We have already noted that, whenever u is smooth, it can be well approximated by a polynomial p of degree D. Therefore, it seems natural to defineuˆ2−(j+1)in such a way thatpˆ2−(j+1)=p2−(j+1)for all polynomials of degree D. One can achieve this as follows: Denoteπk,jthe degree D polynomial which interpolates the data(2−j(k−⌊D/2⌋)…2−jk…2−j(k+⌊D/2⌋)u(2−j(k−⌊D/2⌋))…u(2−jk)…u(2−j(k+⌊D/2⌋))).Then we can putuˆ2−(j+1)(2k):=πk,j(2−jk)=u2−j(k)anduˆ2−(j+1)(2k+1):=πk,j(2−j(k+1/2)),k∈Z.In general, the relation (22) will not be an equality, therefore we need to store the errorαj:=(αj(k))k∈Z:=(u(2−j(k+1/2))−uˆ2−(j+1)(2k+1))k∈Z.We have now transformed the sequenceu2−(j+1)into two sequences(u2−j,αj), each of which is half the size of the original sequence. Fig. 2shows an illustration of this scheme. The point of this procedure is that, provided u is smooth, the coefficients ofαjmeasure the local polynomial approximation error at scale2−jwhich decays very fast. Therefore, most of the coefficients ofαjcan be set to zero in this case. It is also clear how to invert the transformation by adding the coefficients ofαjto the polynomial imputation of the coarser samplingf2−j. We can iterate this construction and arrive at a decompositionu2−J⇝(u2−J0,αJ0+1,…,αJ),with a minimal scaleJ0. Here is a fundamental result regarding the decay properties of the coefficientsαj:Theorem 3.1Assume thatu∈Cswiths<D. Then we have‖αj‖∞≲2−sjforj∈N.Several signals which appear in practice are not globally smooth but only piecewise smooth. Due to the local nature of the wavelet construction, also piecewise smooth functions can be approximated efficiently.Theorem 3.2Assume that u is supported on[−1,1]and thatu|[−1,0),u|[0,1]∈Cswith a possible discontinuity of u in 0 ands<D. Then, by keeping only N wavelet coefficients, one can get an approximationuNof u such that‖u−uN‖2≲N−s.ProofWe only sketch the proof. Assume w.l.o.g. thatN=2j. Then we can set all wavelet coefficients of scale >j to zero except for those which contribute to the singularity. For any fixed scale J, there is a finite number of such coefficients. We also keep these coefficients up to scale2sj. Thus, all in all we keep order N coefficients, and all the others are set to zero. By Theorem 3.1, this approximation gives an error of orderN−sin theL∞norm (and thus also in theL2-norm) away from an interval I of size∼2−2sjaround the singularity. It remains to estimate the error on the interval I. Here, theL2-error is also of orderN−s, due to the small size of the interval. □Theorem 3.2 is of tremendous importance. It states that a piecewise smooth signal can be encoded up to an errorN−sby using about N coefficients. This is as good as if the singularity were not present at all! Also, it can be shown that this result is optimal in the sense that essentially no better compression strategies for piecewise smooth functions exist (Donoho, 1993).Our first goal is to extend the constructions and results of Section 3.1.1 to the manifold-valued setting. To some extent this has been done in Donoho (2001), Rahman et al. (2006), Grohs and Wallner (2009). Our construction is somewhat different from the one in Rahman et al. (2006) in that we use another notion of geometric mean, namely the one developed in Section 2.1. This is a minor difference for the nonlinear construction of interpolating wavelets, but it will make a difference for the construction of nonlinear average interpolating wavelet transforms which we study below in Section 3.2.The starting point is the fact that the mappingS:u2−j↦uˆ2−(j+1)has a very simple structure:(23)Su2−j(2k)=u2−j(k),Su2−j(2k+1)=∑i=−⌊D/2⌋⌊D/2⌋lD(i)u2−j(k−i),with some sequence l satisfying(24)∑i=−⌊D/2⌋⌊D/2⌋lD(i)=1.In the literature, the operator S is usually called the Deslauriers–Dubuc subdivision scheme (Deslauriers and Dubuc, 1989). It is now immediate to generalize the prediction operator to the nonlinear setting. Assume we have a pair(f,g)satisfying (4), (5) and (6). Then by Proposition 2.2, the operator S can also be defined in the M-valued setting: For a functionu:R→Mwe again use the notationuhMto denote uniform sampling of meshwidthh>0. Then we can define(25)uˆ2−(j+1)M(2k):=u2−jM(k),uˆ2−(j+1)M(2k+1):=avg((lD(i))i=−⌊D/2⌋⌊D/2⌋,(u2−jM(k−i))i=−⌊D/2⌋⌊D/2⌋).Similar to the linear case, the wavelet coefficientsαjMare defined as the prediction error arising from this procedure. However, in contrast to the linear case we cannot simply subtract two points. Nevertheless the function g gives us a notion of difference vector between points. Using this notion we can define the wavelet coefficients as follows:αjM(k):=g(uˆ2−(j+1)M(2k+1),u2−(j+1)M(2k+1))∈Tuˆ2−(j+1)M(2k+1)M,k∈Z.For a functionu:R→Mwe get a decompositionu2−JM⇝(u2−J0M,αJ0M,…,αJ−1M).The reconstruction procedure is obvious, for instance to go from(u2−jM,αj+1M)tou2−(j+1)Mwe first computeuˆ2−(j+1)Mfromu2−jMusing (25). Then we have(26)u2−(j+1)M(2k)=u2−jM(k),u2−(j+1)M(2k+1)=f(uˆ2−(j+1)M(2k+1),αj+1M(k)).Observe however that the reconstruction procedure constitutes a nonlinear operation in each step which makes it much more difficult to analyze theoretically.Remark 3.3Analogous pyramid decompositions can be defined by replacing S above by an arbitrary interpolatory subdivision scheme (Cavaretta et al., 1991). The definition of our manifold-valued generalization using the geometric mean defined in Definition 2.4 immediately implies that all symmetries of a linear subdivision scheme carry over to the manifold-valued case. This is not necessarily the case with the construction of Rahman et al. (2006) where the nonlinear average construction relies on the choice of a suitable basepoint.Nevertheless we mention that one can also replace the exponential and logarithm mappings in the construction of Rahman et al. (2006) by an arbitrary retraction pair and all approximation results which we show in Section 3.3 below remain valid.If we are dealing with data which is free of noise, the interpolating wavelet transform is an appropriate choice. However, in practice one often has to work with noisy data which, due to the instability of point evaluations, presents difficulties for the interpolating transform (Rahman et al., 2006). A remedy to this problem is to work not with point values but with averages of the function over dyadic boxes. In Donoho (1994) such constructions are given. We briefly describe the construction which is a special case of the biorthogonal wavelet transform framework introduced in Cohen et al. (1992).The idea is to discretize not by point evaluation but by integration over dyadic boxes. The resulting discretization operator for a functionu:R→Ris defined viau¯h=(∫[hk,h(k+1)]u(t)dt)k∈Z.Again we need an operator to get fromu¯2−jto a predictionu¯ˆ2−(j+1)∼u¯2−(j+1). This is done in a similar way as for the interpolating wavelet transform by picking for each k a polynomialπ¯k,jof degree D, D even, which interpolates the averages of f over dyadic boxes of size2−jaround k. Then we defineu¯ˆ2−(j+1)(2k)=∫[2−jk,2−j(k+1/2)]π¯j,k(t)dtandu¯ˆ2−(j+1)(2k+1)=∫[2−j(k+1/2),2−j(k+1)]π¯j,k(t)dt.Observe the redundancies(27)12(u¯ˆ2−(j+1)(2k)+u¯ˆ2−(j+1)(2k+1))=u¯2−j(k)for allk∈Zand(28)12(u¯2−(j+1)(2k)+u¯2−(j+1)(2k+1))=u¯2−j(k)for allk∈Z.Again we can define wavelet coefficientsβj:=(αj(k))k∈Z:=(u¯2−(j+1)(2k+1)−u¯ˆ2−(j+1)(2k+1))k∈Z.Using the redundancies (27), (28) we can reconstruct the values ofu¯2−(j+1)fromu¯2−jandαjviau¯2−(j+1)(2k+1)=u¯ˆ2−(j+1)(2k+1)+βj(k),u¯2−(j+1)(2k)=u¯2−j(k)−(u¯2−(j+1)(2k+1)−u¯2−j(k)),wherek∈Z. Note that also for this wavelet transform Theorems 3.1 and 3.2 hold true (Donoho, 1994).Average interpolating wavelet transforms can also be defined in the manifold-valued context, at least to some extent. Several problems arise, mostly due to the fact that it is not clear how to define an averaging operation of an N-valued function over a dyadic box. Certainly, such operations exist in a geometric setting, see Karcher (1977), but the problem with these constructions is that in general the redundancy relation (28) fails.At least for practical purposes a partial remedy is to start from a midpoint pyramid(29)u¯JM,u¯J−1M,…,u¯J0Msatisfying the consistency condition(30)u¯jM(k)=avg((12,12),(u¯j+1M(2k),u¯j+1M(2k+1))),for a minimal scaleJ0and a maximal scale J andJ0⩽j<J. In practice, one is given a sequence of datau¯JM, say of length2J. Then one can define a midpoint pyramid (29) by taking (30) as a definition.Next we note the fact that in the linear case the prediction operator which mapsu¯2−jtou¯ˆ2−(j+1)can be written as(31)u¯ˆ2−(j+1)(2k+1)=∑i=−D/2+1D/2rD(i)u¯2−j(k−i)for some sequencerDsatisfying∑i=−D/2+1D/2rD(i)=1.Motivated by (31) and (27) we put(32)u¯ˆj+1M(2k+1)=avg((rD(i))i=−D/2+1D/2,(u¯jM(k−i))i=−D/2+1D/2)and(33)u¯ˆj+1M(2k)=f(u¯jM(k),−g(u¯jM(k),u¯ˆj+1M(2k+1))).By (33) we get the redundancy relation(34)u¯jM(k)=avg((12,12),(u¯ˆj+1M(2k),u¯ˆj+1M(2k+1))).One should compare the relations (34) and (30) with the linear relations (27) and (28).We can now define wavelet coefficientsβjM(k):=g(u¯ˆj+1M(2k+1),u¯j+1M(2k+1))∈Tu¯ˆj+1M(2k+1)M,k∈Z.The reconstruction procedure proceeds similarly to the linear one. Given(u¯jM,βjM)we first computeu¯ˆj+1Musing (32) and (33). Then we get the reconstruction of the odd indices viau¯j+1M(2k+1)=f(u¯jM(k),βjM(k)).To reconstruct the even indices we make use of (30) to find that, byg(u¯jM(k),u¯j+1M(2k))=−g(u¯jM(k),u¯j+1M(2k+1)),we get the reconstruction formulau¯j+1M(2k)=f(u¯jM(k),−g(u¯jM(k),u¯j+1M(2k+1))).We have constructed a way to transform a midpoint pyramid into a wavelet decomposition(u¯J0M,βJ0M,…,βJ−1M)and vice versa.Remark 3.4Our construction easily extends to the regular multivariate case by defining the notion of midpoint pyramid as a uniformly weighted average of (in the case of bivariate data) four points. For each point at a coarse scale, in the bivariate case one then has to store three wavelet coefficients – the fourth one can be computed from the first-order condition (13) which defines the geometric average. Therefore, in view of ensuring a representation of minimal redundancy, the use of an averaging procedure of the form introduced in this paper is crucial.Note that in Rahman et al. (2006) a different generalization to the multivariate case is given which e.g. in the bivariate setting first applies a wavelet transform in the vertical direction and then in the horizontal direction. Doing this one runs into trouble when computing the wavelet coefficients corresponding to the diagonal direction which requires the computation of a univariate wavelet transform of tangent vectors. We think that our approach of computing two dimensional transforms using geometric averages is more natural.We now turn to the theoretical properties of the manifold-valued wavelet constructions. The average interpolating construction described in Section 3.2.2 starts from a midpoint pyramid, which in practice can be constructed from a discrete sequence of data via (30). However, it is not clear how to associate to a given functionu:R→Man infinite midpoint pyramid satisfying the downsampling relation (30), or a similar one. This fact makes an asymptotic analysis of the decay of the wavelet coefficients impossible. For this reason we focus on the interpolating wavelet transform for manifold-valued data described in Section 3.1.2.In the remainder of this section we once and for all fix an odd integerD>0and associate to an arbitrary functionu:R→Mits nonlinear interpolating wavelet transform(u2−J0M,αJ0M,…),as in Section 3.1.2.Before we formulate our main results we need to introduce some notation.For a sequence α of vectors(α(k))k∈Z,α(k)∈TMwe use the symbol‖α‖∞=supk∈Z‖α(k)‖M,where‖⋅‖Mindicates the Riemannian metric on M. Further, we want to be able to compare two sequencesu=(u(k))k∈Z,v=(v(k))k∈Z,u(k),v(k)∈M. Here we use the notationdistM(u,v)∞:=supk∈ZdistM(u(k),v(k)),wheredistMdenotes the geodesic distance in M. Finally, with α as above and β of the same form, we also want to compare these two sequences. Denoting for two vectorsγ∈TxM,δ∈TyMdistTM(γ,δ):=distM(x,y)+‖γ−Ptyx(δ)‖M,Ptyxdenoting parallel transport from y to x (DoCarmo, 1992) we writedistTM(α,β)∞:=supk∈ZdistTM(α(k),β(k)).Then the following analog of Theorem 3.1 holds:Theorem 3.5Assume thatu∈Csfors<D. Then we get the following decay rate for the wavelet coefficients:(35)‖αjM‖∞≲2−sjwith the implied constant uniform on compact sets. Conversely, if(35)holds for the wavelet decomposition of u, then for allε>0we haveu∈Cs−ε.This has been proven in Grohs and Wallner (2009). □Theorem 3.5 tells us something about the decay of the wavelet coefficients associated with a smooth function. The potential use of this result is clear: by setting all small coefficients to zero, one should expect to still obtain a very good approximation of the original function. This intuition turns out to be correct, but it is far from trivial in the nonlinear case. The key issue lies in the stability properties of the reconstruction procedure. More specifically, one needs to address the question of how a change of the wavelet coefficients affects the reconstruction. To this end we consider the reconstruction operatorRj:(u2−J0M,αJ0M,…,αj−1M)↦u2−jM,using (26).The following result holds.Theorem 3.6Assume that(f,g)satisfy(6),(5)and(4). Assume we are given two wavelet decompositionsU:=(u2−J0M,αJ0M,…,αj−1M)andV:=(v2−J0M,βJ0M,…,βj−1M).Then there exists a constant C independent of j such thatdistM(Rj(U),Rj(V))∞⩽C(distM(u2−J0,v2−J0)∞+∑i=J0j−1distTM(αiM,βiM)).ProofThis has been shown in Grohs (2010b). □Using the above stability result, we can now prove a nonlinear analog of Theorem 3.2. In order to avoid technical complications we assume thatu:[−1,1)→Mis periodic.Theorem 3.7Assume thatu:[−1,1)→Mis a periodicCsfunction(s<D)apart from a discontinuity at 0. Then, using N wavelet coefficients, one can obtain an approximationuNsuch that∫[−1,1]distM(u(t),uN(t))2dt≲N−2s.ProofUsing Theorem 3.6, the proof proceeds with similar arguments as the proof of the linear version, Theorem 3.2. Without loss of generality, we putN=2j. Let(u2−J0M,αJ0M,…)be the wavelet decomposition of f. Assume we have the full datau2−2sjat hand. We first examine what happens if we build an approximationu˜of u fromu2−2sjby setting all wavelet coefficients of higher scale to zero. By Theorem 3.6, Theorem 3.5 and the locality of the wavelet reconstruction, we observe that, away from an interval I of size∼2−2sjaround 0, we get an approximation errorsupt∈[−1,1)∖Idist(u(t),u˜(t))≲2−2s2j.See also Xie and Yu (2011), Grohs (2010a) for more details. Therefore we get that(36)∫[−1,1)∖IdistM(u(t),u˜(t))2dt≲2−4s2j.On the other hand, the errordistM(u(t),u˜(t))is bounded on I. Therefore we have(37)∫IdistM(u(t),u˜(t))2dt≲∫I1dt≲2−2sj.Putting together (36) and (37) we arrive at the estimate(38)∫[−1,1)distM(u(t),u˜(t))2dt≲2−2sj.Eq. (38) gives the right magnitude of approximation error but clearly way too many coefficients are needed foru˜, which requires order22sjnonzero wavelet coefficients. We now perform a further compression step, reducing the number of nonzero wavelet coefficients to order2j. To do this, we observe that by Theorem 3.6 it would be sufficient to threshold the wavelet coefficientsU:=(u2−J0M,αJ0M,…,α2sj−1M)toV:=(u2−J0M,βJ0M,…,β2sj−1M),such that(39)distM(R2sj(U),R2sj(V))∞≲2−sj,and such that the number of nonzero wavelet coefficients inVis of order N. To do this we note that at each scale k, only a fixed finite number2L+1of wavelet coefficients(αkM(−L),…,αkM(L))gets affected by the singularity at 0. All the other wavelet coefficientsαkM,smooth:=(…,αkM(−L−2),αkM(−L−1),αkM(L+1),αkM(L+2),…)are only affected by the smooth part of u and therefore (by the locality of the construction and Theorem 3.5) decay of order2−sk. We now obtain our compressionVby setting to zero all coefficientsαkM,smoothfor all scalesj,j+1,…,2sj. Due to the decay properties ofαkM,smoothand Theorem 3.6, we get (39). It remains to count the number of nonzero coefficients inV. There are order2jcoefficients up to scale j. For the scalesj+1,j+2,…,2sjwe have2L+1coefficients which amounts to a total of order2jcoefficients. This proves the theorem. □Theorem 3.7 confirms that the nonlinear wavelet transform indeed satisfies the same important properties as its linear counterpart.Remark 3.8We already mentioned that an asymptotic analysis of the nonlinear average interpolating wavelet construction is not possible. Nevertheless, one could still analyze the heuristic construction of Section 3.2.2 for finite data. We postpone this for future work. Note also that the possibilities for constructing more general wavelet transforms in manifolds are limited since a perfect reconstruction property can in general not be achieved, cf. Grohs and Wallner (2011). In future work we aim to study whether redundant wavelet transforms such as the undecimated wavelet transform (Starck et al., 2007) are more amenable to a generalization to manifold-valued data.We return to the specific case of the Stiefel manifold and present some computational experiments, starting first with synthetic data. For our experiments we have chosen the pair(f,g)constructed in 2.2.3 based on closest point projection. For the plots of the wavelet coefficients in Figs. 3 and 4we made use of the plotting routines of the Symmlab100 MATLAB package, available from http://www.stanford.edu/~slansel/SymmLab/. We are grateful to the authors of this package for making their software codes publicly available.Our first experiment deals with a smooth matrix curve (and its svd)(40)A(t)=U(t)Σ(t)V(t)⊤=sin(tE)+noise∈R10×10,with some random10×10matrix E. We have used the first 3 columns ofU(t)as a curve inSt(10,3)and performed a nonlinear average interpolating wavelet transform of this curve withD=4. We performed a thresholding operation by setting all wavelet coefficients to zero which exceed the threshold of 0.01. The results can be seen in Fig. 3. We can observe two things: First, the thresholding operation serves as a denoising operation. We remark that this is in general not the case with the interpolating wavelet transform. Second, if the noise level is set to zero, almost all wavelet coefficients are negligible, resulting in huge compression rates.Our second experiment deals with a matrix curve which possesses a point singularity. We chose the curve(41)A(t)=U(t)Σ(t)V(t)⊤=sin(tE)+(1−t2)1/2cos(tF)∈R10×10,E, F random10×10matrices, and again used the first 3 columns ofU(t)as a curve inSt(10,3). This time our analysis was performed using the interpolating wavelet transform withD=3. The results are shown in Fig. 4. We can nicely observe that all coefficients which do not correspond to the point singularity are almost negligible and only those around the singularity must be kept. The wavelet transform finds pointwise singularities by itself. On the right of Fig. 4, a thresholding operation has been performed which results in overall savings of more than ninety percent.After some preliminary experiments on synthetic data in Section 4.1 we now apply our methods to a real problem in imaging science namely the compression of hyperspectral images. In our numerical experiments on real data we have always used the average interpolating construction from Section 3.2.2 due to its noise insensitivity. We stress once again that this is just one selected application, there are many other situations where smooth curves of low-rank matrices appear naturally, for instance latent semantic indexing, dynamic graph compression or linear operators depending on a parameter.In our experiments we mainly focused on data compression. Since wavelet methods are capable of detecting large temporal variations in data, several other tasks such as feature detection, inpainting, classification, … could be tackled. We postpone a more detailed study to future work. Before we describe our application we give a brief introduction to hyperspectral imaging.A hyperspectral image is an image where intensities corresponding to many different wavelengths are recorded separately (Chang, 2003). The result is a parameterized familyA(λ)∈RN×Nof matrices, λ being the wavelength. We would like to emphasize that, when the wavelength λ varies continuously we get a smooth curveA(λ). In practice, only a finite number of wavelengths is recorded, typically at the order of hundreds or more. A related imaging technique is inferometry, where even more frequency bands are recorded. It is to be expected that in the near future technology will advance to develop sensors capable of recording many more spectral bands, in the order of many thousand or even millions. This technique is called ultraspectral imaging (Meigs et al., 2008). For completeness we also mention the term multispectral imaging which refers to only a few frequency bands.Due to its capacity to record information across the full range of the spectrum and recent technological advances, the scope of applicability of hyperspectral imaging is tremendous. We restrict ourselves to mentioning only a few important ones: In mineralogy, hyperspectral imaging is used for the detection of oil fields, while the US military uses this technique for the surveillance and detection of humans. A less controversial application is the monitoring of the constitution of plants and crops in agriculture.Usually, the size of a hyperspectral image can be quite large which calls for efficient means to compress such data. Since a hyperspectral image can be interpreted as dense samples of a smooth matrix curve, it is reasonable to use a compression strategy which takes advantage of this fact.In Algorithm 4we propose a simple strategy for the compression of a hyperspectral imageAi∈RN×N,i=1,…,T,corresponding to recordingsA(λi)at wavelengthλi.Remark 4.1There is nothing exceptional about our choice of blocksize or the rank of the low-rank approximation in each block in Algorithm 4, any other choice would do. There is of course a lot of room for improvement; for instance the rank of the low-rank approximation could be chosen differently for each block by an adaptive procedure. We confine ourselves to our simple algorithm since the primary goal of this work is to demonstrate the usefulness of manifold-valued wavelet transforms, especially for the Stiefel manifold.Remark 4.2Note that the more frequency bands are recorded, the better the performance of the wavelet compression is to be expected. This is because one needs at least 8–16 sampling points at the lowest scale (depending on D) in order to render the wavelet scheme well-defined. This limits the achievable compression rate. In our experiments, almost all wavelet coefficients could be discarded and we conjecture that it would be possible to achieve much higher compression rates for ultraspectral images with more frequency bands using our algorithm.Fig. 5shows an example of Algorithm 4 applied to real hyperspectral image data. The data is taken from http://earthexplorer.usgs.gov/ and represents a sample of the Dehradun region in India, recorded with the Hyperion sensor aboard NASAʼs Earth Observation-1 (EO1) satellite. The sensor records wavelengths in the range of 355–2577 nm at a sampling interval of about 10 nm. The dataset we used is an image cube of size144×144×128.The resulting compression only requires about7%of the original data, compared to about30%by only using the low rank approximation without wavelet thresholding. In view of the present article, one should compare the difference between the low rank approximation and its wavelet compression, which is barely noticeable, see Fig. 5. This represents a compression of the low-rank approximation by a factor about1:5, essentially without any noticeable degradation.Remark 4.3So far we have not specified how we actually carry out Step 2 in Algorithm 4. We will comment on this issue later in Section 4.3 below. In addition we would like to point to the recent work (Studer et al., 2012) where sensing devices are constructed which directly sense a sparse representation without going through the usual procedure of first recording the full image and then throwing away almost all of the data. It seems conceivable that in the future sensors capable of directly recording a low rank approximation of an image can be constructed, although this is just speculation.Remark 4.4We reiterate that we do not claim that the compression scheme for hyperspectral images presented in this work is superior to state-of-the-art methods which are currently in use. The point is to demonstrate the usefulness of our flexible compression scheme for curves of low rank matrices. Of particular importance is the fact that our scheme provably achieves (asymptotically) optimal compression rates for piecewise smooth families of low-rank matrices. In fact, we are not aware of any other method in the literature which satisfies a similar property. Another distinctive property of the approach presented in this paper is that features such as point singularities can be detected automatically and non-adaptively simply by looking for the largest coefficients.In all our analysis the underlying assumption is that the curve of low-rank matrices is smooth, or at least piecewise smooth. This seems like a problem since usually low-rank approximations are computed via svd which is nonunique, therefore one should not expect low-rank approximation curves computed via svd to be smooth.Fortunately, a remedy exists. In Koch and Lubich (2008) a method is given to obtain smooth dynamical low-rank approximations to matrix curvesA(t)∈Rn×n,t>0.The idea is to not compute a low-rank approximation for each matrix but to solve the ODE(42)X˙(t)=minξ∈TMn,p‖A˙(t)−ξ‖2,X(0)=X0,whereX0is some initial rank p approximation toA(0). The computation of the solution of (42) can be done using geometric ODE integrators (Iserles et al., 2000). This is very efficient since typically the matricesA˙(t)are much sparser than the matricesA(t). Moreover, in Koch and Lubich (2008) it is shown that the approximation quality ofX(t)is equivalent to the best possible approximation by a rank p matrix, at least as long as the first singular values stay well-separated. If singular values interlace, e.g. new singular vectors enter the best approximation, then the algorithm (42) needs to be restarted. By this procedure one obtains a piecewise smooth rank p approximation ofA(t). In Nonnenmacher and Lubich (2008) several important applications of this algorithm are given, each one of them producing piecewise smooth curves on the Stiefel manifold which can be processed by the Stiefel-valued wavelet transform developed in this paper.

@&#CONCLUSIONS@&#
