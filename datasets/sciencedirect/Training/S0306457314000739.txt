@&#MAIN-TITLE@&#
Probabilistic topic modeling in multilingual settings: An overview of its methodology and applications

@&#HIGHLIGHTS@&#
A systematic overview of multilingual probabilistic topic modeling (MuPTM).A tutorial on methodology, modeling, training, output, inference and evaluation of MuPTM.Language-independent and language-pair independent data representations.A model-independent framework and applications in various cross-lingual tasks.A complete MuPTM-based framework for cross-lingual semantic similarity.

@&#KEYPHRASES@&#
Multilingual probabilistic topic models,Cross-lingual text mining,Cross-lingual knowledge transfer,Cross-lingual information retrieval,Language-independent data representation,Non-parallel data,

@&#ABSTRACT@&#
Probabilistic topic models are unsupervised generative models which model document content as a two-step generation process, that is, documents are observed as mixtures of latent concepts or topics, while topics are probability distributions over vocabulary words. Recently, a significant research effort has been invested into transferring the probabilistic topic modeling concept from monolingual to multilingual settings. Novel topic models have been designed to work with parallel and comparable texts. We define multilingual probabilistic topic modeling (MuPTM) and present the first full overview of the current research, methodology, advantages and limitations in MuPTM. As a representative example, we choose a natural extension of the omnipresent LDA model to multilingual settings called bilingual LDA (BiLDA). We provide a thorough overview of this representative multilingual model from its high-level modeling assumptions down to its mathematical foundations. We demonstrate how to use the data representation by means of output sets of (i) per-topic word distributions and (ii) per-document topic distributions coming from a multilingual probabilistic topic model in various real-life cross-lingual tasks involving different languages, without any external language pair dependent translation resource: (1) cross-lingual event-centered news clustering, (2) cross-lingual document classification, (3) cross-lingual semantic similarity, and (4) cross-lingual information retrieval. We also briefly review several other applications present in the relevant literature, and introduce and illustrate two related modeling concepts: topic smoothing and topic pruning. In summary, this article encompasses the current research in multilingual probabilistic topic modeling. By presenting a series of potential applications, we reveal the importance of the language-independent and language pair independent data representations by means of MuPTM. We provide clear directions for future research in the field by providing a systematic overview of how to link and transfer aspect knowledge across corpora written in different languages via the shared space of latent cross-lingual topics, that is, how to effectively employ learned per-topic word distributions and per-document topic distributions of any multilingual probabilistic topic model in various cross-lingual applications.

@&#INTRODUCTION@&#
Probabilistic latent topic models such as probabilistic Latent Semantic Analysis (pLSA) (Hofmann, 1999b, 1999a) and Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003b) along with their numerous variants are well studied generative models for representing the content of documents in large document collections. They provide a robust and unsupervised framework for performing shallow latent semantic analysis of themes (or topics) discussed in text. The families of these probabilistic latent topic models are all based upon the idea that there exist latent variables, that is, topics, which determine how words in documents have been generated. Fitting such a generative model actually denotes finding the best set of those latent variables in order to explain the observed data. With respect to that generative process, documents are seen as mixtures of latent topics, while topics are simply probability distributions over vocabulary words. A topic representation of a document constitutes a high-level language-independent view of its content, unhindered by a specific word choice, and it improves on text representations that contain synonymous or polysemous words (Griffiths, Steyvers, & Tenenbaum, 2007).Probabilistic topic modeling constitutes a very general framework for unsupervised topic mining, and over the years it has been employed in miscellaneous tasks in a wide variety of research domains, e.g., for object recognition in computer vision (e.g., Li & Perona, 2005; Russell, Freeman, Efros, Sivic, & Zisserman, 2006; Wang & Grimson, 2007), dialogue segmentation (e.g., Purver, Körding, Griffiths, & Tenenbaum, 2006), video analysis (e.g., Wang, Ma, & Grimson, 2009), automatic harmonic analysis in music (e.g., Arenas-Garca et al., 2007; Hu & Saul, 2009), genetics (e.g., Blei, Franks, Jordan, & Mian, 2006), and others.Being originally proposed for textual data, probabilistic topic models have also organically found many applications in natural language processing (NLP). Discovered distributions of words over topics (further per-topic word distributions) and distributions of topics over documents (further per-document topic distributions) can be directly employed to detect main themes1To avoid confusion, we talk about themes when we address the true content of a document, while we talk about topics when we address the probability distributions constituting a topic model.1discussed in texts, and to provide gists or summaries for large text collections (see, e.g., Hofmann, 1999b; Blei et al., 2003b; Griffiths & Steyvers, 2004; Griffiths et al., 2007). Per-document topic distributions for each document might be observed as a low-dimensional latent semantic representation of text in a new topic-document space, potentially better than the original word-based representation in some applications. In an analogous manner, since the number of topics is usually much lower than the number of documents in a collection, per-topic word distributions also model a sort of dimensionality reduction, as the original word-document space is transferred to a lower-dimensional word-topic space. Apart from the straightforward utilization of probabilistic topic models as direct summaries of large document collections, these two sets of probability distributions have been utilized in a myriad of NLP tasks, e.g., for inferring captions for images (Blei & Jordan, 2003), sentiment analysis (e.g., Mei, Ling, Wondra, Su, & Zhai, 2007; Titov & McDonald, 2008), analyzing topic trends for different time intervals in scientific literature, social networks and e-mails (e.g., Wang & McCallum, 2006; McCallum, Wang, & Corrada-Emmanuel, 2007; Hall, Jurafsky, & Manning, 2008), language modeling in information retrieval (e.g., Wei & Croft, 2006; Yi & Allan, 2009), document classification (e.g., Blei et al., 2003b; Lacoste-Julien, Sha, & Jordan, 2008), word sense disambiguation (e.g., Boyd-Graber, Blei, & Zhu, 2007), modeling distributional similarity of terms (e.g., Ritter, Mausam, & Etzioni, 2010; Dinu & Lapata, 2010), etc. Lu, Mei, and Zhai, 2011 examine task performance of pLSA and LDA as representative monolingual topic models in typical tasks of document clustering, text categorization and ad hoc information retrieval. Data representation, i.e., representations of words and documents in all applications presented in this article will be based on those per-topic word distributions and per-document topic distributions.However, all these models have been designed to work with monolingual data, and they have been applied in monolingual contexts only. Following the ongoing growth of the World Wide Web and its omnipresence in today’s increasingly connected world, users tend to abandon English as the lingua franca of the global network, since more and more content becomes available in their native languages or even dialects and different community languages (e.g., the idiomatic usage of the same language typically differs between scientists, social media consumers or the legislative domain). It is difficult to determine the exact number of languages in the world, but the estimations vary between 6000 and 7000 languages and almost 40,000 unofficial languages and dialects.2Source: http://www.ethnologue.com.2It is extremely time-consuming and labor-intensive to build quality translation resources and parallel corpora for each single language/dialect pair. Therefore, we observe an increasing interest in language-independent unsupervised corpus-based cross-lingual text mining from non-parallel corpora without any additional translation resources. High-quality parallel corpora where documents are sentence-aligned exact translations of each other (such as Europarl (Koehn, 2005)) are available only for a restricted number of languages and domains. There has been a recent interest to build parallel corpora from the Web (e.g., Resnik & Smith, 2003; Munteanu & Marcu, 2005, 2006), but the obtained parallel data still typically remain of limited size and scope as well as domain-restricted (e.g., parliamentary proceedings).With the rapid development of Wikipedia and online social networks such as Facebook or Twitter, users have generated a huge volume of multilingual text resources. The user-generated data are often noisy and unstructured, and seldom well-paired across languages. However, unlike parallel corpora, such comparable corpora, where texts in one language are paired with texts in another language discussing the same themes or subjects, are abundant in various online sources (e.g., Wikipedia or news sites). Documents from comparable corpora do not necessarily share all their themes with their counterparts in the other language, but, for instance, Wikipedia articles discussing the same subject, or news stories discussing the same event contain a significant thematic overlap. We could say that such documents in different languages, although inherently non-parallel, are theme-aligned.Multilingual probabilistic topic models (MuPTM-s) have recently emerged as a group of unsupervised, language-independent generative machine learning models that can be efficiently utilized on such large-volume non-parallel theme-aligned multilingual data and effectively deal with uncertainty in such data collections. Due to its generic language-independent nature and the power of inference on unseen documents, these models have found many interesting applications. The knowledge from learned MuPTM-s has been used in many different cross-lingual tasks such as cross-lingual event clustering (DeSmet & Moens, 2009), cross-lingual document classification (De Smet, Tang, & Moens, 2011; Ni, Sun, Hu, & Chen, 2011), cross-lingual semantic similarity of words (Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009; Vulić, DeSmet, & Moens, 2011a; Vulić & Moens, 2012), cross-lingual information retrieval (Vulić, Smet, & Moens, 2011b, 2013; Vulić & Moens, 2013; Ganguly, Leveling, & Jones, 2012) and others.The main goal of this work is to provide an overview of the recently developed multilingual probabilistic topic modeling concept. It aims to model topic discovery from multilingual data in a conceptually sound way, taking into account thematic alignment between documents in document collections given in different languages. We have decided to provide a thorough analysis of the framework of multilingual probabilistic topic modeling because we feel that the current relevant literature lacks a systematic and complete overview of the subject. Moreover, during our tutorials at ECIR 2013 and WSDM 2014 on the subject we realized even more that, after being provided with feedback on our tutorials, it would be extremely beneficial for the IR community to have an extended written overview of the whole subject, along with its formalisms, definitions and modeling perspectives (both conceptual and mathematical), relevant state-of-the-art, a broad relevant references list, and also with standard evaluation procedures, an overview of applications, and suggestions for future work.As a representative example, we choose bilingual LDA, which has been designed as a basic and natural extension of the standard omnipresent LDA model in the multilingual settings where document-aligned articles in different languages are available (e.g., Wikipedia articles about the same subject in multiple languages). We provide a complete and comprehensive overview of that model all the way up from the conceptual and modeling level down to its core mathematical foundations as it could serve as a valuable starting point for other researchers in the field of multilingual probabilistic topic modeling and cross-lingual text mining. Alternative multilingual probabilistic topic models that build upon the idea of the standard pLSA and LDA models are presented in a nutshell. These models differ in the specific assumptions they make in their generative processes as well as in knowledge that is presupposed before training (e.g., document alignment, prior word matchings or bilingual dictionaries), but all these models have the ability to discover latent cross-lingual topics from comparable data such as Wikipedia or news. Additionally, all these models output the same basic sets of probability distributions, that is, per-topic word distributions and per-document topic distributions. Finally, we also demonstrate how to utilize the high-level structured text representations by means of per-topic word distributions and per-document topic distributions from any multilingual probabilistic topic model, and establish knowledge transfer across different languages via the shared space of latent language-independent concepts, that is, cross-lingual topics in several real-life NLP/IR tasks: (1) cross-lingual event-centered news clustering, (2) cross-lingual document classification, (3) cross-lingual semantic similarity, and (4) cross-lingual information retrieval. In this article, we show the results obtained by BiLDA, but the presented solutions are completely topic model-independent.The results reported across all these tasks show the validity of multilingual comparable data as training data, as well as the superiority of MuPTM over monolingual probabilistic topic modeling (MoPTM) and other data-driven modeling paradigms which do not rely on any expensive translation resources. We also expose and discuss an issue present in all current multilingual topic models – a need to set the number of topics in advance before training. Different applications reach their optimal results with different number of topics set a priori and it is often difficult to accurately predict that application-dependent number of topics in advance. We also report on a mismatch between the standard intrinsic evaluation measure of perplexity and the extrinsic evaluation in terms of final scores in the cross-lingual tasks.This section presents and defines the basic concepts and modeling assumptions related to multilingual probabilistic topic modeling, with a special focus on learning from comparable theme-aligned corpora. We also draw an analogy to the broader paradigm of latent cross-lingual concepts, and their relation to latent cross-lingual topics. Following that, the representative bilingual LDA (BiLDA) is presented in its entirety, which includes its generative story, the explanation of the Gibbs sampling training procedure for the model, the output of the model in terms of per-topic word distributions and per-document topic distributions, the procedure to infer the trained model on unseen data, and a qualitative analysis of its output in terms of the top N most important words for some selected topics. We also define several evaluation metrics utilized to compare different topic models. At the very end of the section, alternative multilingual topic models are also presented.Definition 1Multilingual theme-aligned corpusIn the most general definition, a multilingual theme-aligned corpusCofl=∣L∣languages, whereL={L1,L2,…,Ll}is the set of languages, is a set of corresponding text collections{C1,C2,…,Cl}. EachCi={d1i,d2i,…,ddnii}is a collection of documents in languageLiwith vocabularyVi={w1i,w2i,…,wwnii}. Collections{C1,C2,…,Cl}are said to be theme-aligned if they discuss at least a portion of similar themes. Here,dnidenotes the total number of documents in document collectionCi, whilewniis the total number of words inVi. Moreover,djidenotes the j-th document in document collectionCi, andwjidenotes the j-th word in vocabularyViassociated with document collectionCi.A multilingual probabilistic topic model of a theme-aligned multilingual corpusCis a set of semantically coherent multinomial distributions of words with valuesPiwji∣zk,i=1,…,l, for each vocabularyV1,…,Vi,…,Vlassociated with text collectionsC1,…,Ci,…,Cl∈Cgiven in languagesL1,…,Li,…,Ll.Piwji∣zkis calculated for eachwji∈Vi. The probabilitiesPiwji∣zkbuild per-topic word distributions (denoted byϕi), and they constitute a language-specific representation (e.g., a probability value is assigned only for words fromVi) of a language-independent latent cross-lingual concept – topiczk∈Z.Z={z1,…,zK}represents the set of all K latent cross-lingual topics present in the multilingual corpus. Each document in the multilingual corpus is thus considered a mixture of K latent cross-lingual topics from the setZ. This mixture for some documentdji∈Ciis modeled by the probabilitiesPizk∣djithat altogether build per-document topic distributions (denoted by θ). In summary, each language-independent latent cross-lingual topiczkhas some probability to be found in a particular document (modeled by per-document topic distributions), and each such topic has a language-specific representation in each language (modeled by language-specific per-topic word distributions).We can interpret Definition 2 in the following way: each cross-lingual topic from the setZcan be observed as a latent language-independent concept present in the multilingual corpus, but each language in the corpus uses only words from its own vocabulary to describe the content of that concept (see Fig. 1for an illustrative example). In other words, we could observe each latent cross-lingual topic as a set of discrete distributions over words, one for each language. For instance, having a multilingual collection in English, Italian and Dutch and discovering a topic on Soccer, that cross-lingual topic would be represented by words (actually probabilities over words) {player, goal, scorer, …} in English, {squadra (team), calcio (soccer), allenatore (coach), …} in Italian, and {doelpunt (goal), voetballer (soccer player), elftal (soccer team), …} in Dutch. We have∑wji∈ViPiwji∣zk=1, for each vocabularyVirepresenting languageLi, and for each topiczk∈Z. We say that a latent cross-lingual topic is semantically coherent if it assigns high probabilities to words that are semantically related. Definition 2 is predominant in the MuPTM literature (e.g., see DeSmet & Moens, 2009; Mimno et al., 2009; Platt, Toutanova, & Yih, 2010).Zhang, Mei, and Zhai (2010) provide an alternative, more general definition of a multilingual topic model, but we will show that their definition may be brought down to Definition 2 after a partition over the languages is performed. Namely, the whole multilingual corpus is observed as a mixture of latent cross-lingual topics fromZ. They then define a latent cross-lingual topiczk∈Zas a semantically coherent multinomial distribution over all the words in all the vocabularies of languagesL1,…,Li,…,Ll, andPwj∣zkgives the probability of any wordwj∈{V1,…,Vi,…,Vl}to be generated by topiczk. In this case, we have∑i=1l∑wj∈ViPwj∣zk=1. The language-specific representation for languageLiof topiczkis then obtained by retaining only probabilities for words which are present in its own vocabularyVi, and normalizing those distributions. For a wordwji∈Vi, we havePiwji∣zk=Pwji∣zk∑wj∈ViPwj∣zk. After the partition over languages and normalizations are performed, this definition is effectively equivalent to Definition 2. However, note that their original definition is more general than Definition 2, but it is also unbalanced over the languages fromLpresent inC, that is, words from the languages that are more present in the original corpusCmight dominate the multinomial per-topic word distributions. By performing the partition and normalization over the languages, that imbalance is effectively removed.Definition 3Multilingual probabilistic topic modelingGiven a theme-aligned multilingual corpusC, the goal of multilingual probabilistic topic modeling or latent cross-lingual topic extraction is to learn and extract a setZof K latent language-independent concepts, that is, latent cross-lingual topicsZ={z1,…,zK}that optimally describe the observed data, that is, the multilingual corpusC. Extracting latent cross-lingual topics actually implies learning per-document topic distributions for each document in the corpus, and discovering language-specific representations of these topics given by per-topic word distributions in each language (see Definition 2).This shared and language-independent set of latent cross-lingual topicsZserves as the core of unsupervised cross-lingual text mining and cross-lingual knowledge linking and transfer by means of multilingual probabilistic topic models. It is the cross-lingual connection that bridges the gap across documents in different languages and transfers knowledge across languages in case when translation resources and labeled instances are scarce or missing. The trained multilingual probabilistic topic model may be further inferred on unseen documents.Definition 4Inference of a multilingual probabilistic topic modelGiven an unseen document collectionCu, the inference of a multilingual topic model on the collectionCudenotes learning topical representations of the unseen documentsdu∈Cu, that is, acquiring per-document topic distributions for the new documents based on the previous output of the model.Knowledge transfer in general refers to transferring knowledge learned from one corpus to another corpus, which was unavailable during the learning procedure. Cross-lingual knowledge transfer is characterized by the fact that corpora are present in more than one language.Additionally, following the assumptions and general definitions provided in this section, monolingual probabilistic topic models such as pLSA (Hofmann, 1999b, 1999a) and LDA (Blei et al., 2003b) could be interpreted as a degenerate special case of multilingual probabilistic topic models where only one language is involved, and all the definitions and assumptions remain the same (see later the discussion in Section 2.3).The latent cross-lingual topics presented in Section 2.1 constitute only one possibility when the aim is to detect and induce a latent semantic structure from multilingual data, that is, to extract latent cross-lingual concepts that are hidden within the data. Latent cross-lingual concepts may be interpreted as language-independent semantic concepts present in a multilingual corpus (e.g., document-aligned Wikipedia articles in English and Spanish) that have their language-specific representations in different languages. To repeat, for instance, having a multilingual collection in English, Spanish and Croatian, and discovering a latent semantic concept on Basketball, that concept would be represented by words (actually probabilities over words) {player, ball, coach, …} in English, {pelota (ball), jugador (player), partido (match), …} in Spanish, and {trener (coach), razigravač (playmaker), doigravanje (playoff), …} in Croatian.These K semantic concepts span a latent cross-lingual semantic space. Each word w may be represented in that latent semantic space as a K-dimensional vector, where each vector component is a conditional concept probability scorePzk∣w. In other words, each word is actually represented as a multinomial probability distribution over the induced latent cross-lingual semantic concepts. Moreover, each document d, regardless of its actual language, may be represented as a multinomial probability distribution, a mixture over the same induced latent cross-lingual semantic conceptsPzk∣d.The description in this article relies on the multilingual probabilistic topic modeling framework, but we emphasize that all the work described in this article is independent of the actual method used to induce the latent cross-lingual concepts. The reader has to be aware of the fact that the description of how to utilize this latent knowledge in applications is generic and model-independent as they allow the usage of all other models that compute probability scoresP(zk∣w)andP(zk∣d)(obtained from per-topic word distributions and per-document topic distributions). Besides MuPTM, a number of other models may be employed to induce the latent cross-lingual concepts. For instance, one could use cross-lingual Latent Semantic Indexing (Dumais, Landauer, & Littman, 1996), probabilistic principal component analysis (Tipping & Bishop, 1999), LDA (Blei et al., 2003b), or a probabilistic interpretation of non-negative matrix factorization (Lee & Seung, 1999; Gaussier & Goutte, 2005; Ding, Li, & Peng, 2008) on concatenated documents in aligned document pairs. Other more recent models include matching canonical correlation analysis (Haghighi, Liang, Berg-Kirkpatrick, & Klein, 2008; Daumé III & Jagarlamudi, 2011) or other families of multilingual topic models (Fukumasu, Eguchi, & Xing, 2012).Without loss of generality, from now on we deal with bilingual probabilistic topic modeling. We work with a bilingual corpus and present cross-lingual tasks in the bilingual setting. For bilingual corpora we introduce the source languageLS(further with indices S) and the target languageLT(further with indices T). We will show that all the definitions and assumptions may be easily generalized to a setting where more than two languages are available.Bilingual Latent Dirichlet Allocation (BiLDA) is a bilingual extension of the standard LDA model (Blei et al., 2003b), tailored for modeling parallel or, even more importantly, comparable theme-aligned bilingual document collections. An example of such a document collection is Wikipedia in 2 languages with paired articles. BiLDA has been independently designed by several researchers (Ni, Sun, Hu, & Chen, 2009; DeSmet & Moens, 2009; Mimno et al., 2009; Platt et al., 2010). Unlike LDA, where each document is assumed to possess its own document-specific distribution over topics, the generative process for BiLDA assumes that each aligned document pair shares the same distribution of topics. Therefore, the model assumes that we already possess document alignments in a corpus, that is, links between paired documents in different languages in a bilingual (or a multilingual) corpus. This assumption is certainly valid for multilingual Wikipedia data, where document alignment is established via cross-lingual links between articles written in different languages. These links are provided by the nature of the Wikipedia structure. Cross-lingual document alignment for news crawled from the Web is also a well-studied problem. Since the establishing of cross-lingual links between similar documents is not the focus of the research reported here, these algorithms are not elaborated in the article, but we refer the curious reader to the literature (see, e.g., Utiyama & Isahara, 2003; Resnik & Smith, 2003; Tao & Zhai, 2005; Munteanu & Marcu, 2006; Vu, Aw, & Zhang, 2009).Definition 6Paired bilingual corpusA paired bilingual document corpus is defined asC={d1,d2,…,dr}={(d1S,d1T),(d2S,d2T),…,(drS,drT)}, wheredj=(djS,djT)denotes the j-th pair of linked documents in the source languageLSand the target languageLT, respectively. The goal of bilingual probabilistic topic modeling is to learn for a (paired or non-paired) bilingual corpus a set of K latent cross-lingual topicsZ, each of which defines an associated set of words in bothLSandLT.BiLDA can be observed as a three-level Bayesian network that models document pairs using a latent layer of shared topics. Fig. 2shows the graphical representation of the BiLDA model in plate notation, while Algorithm 1 presents its generative story.Algorithm 1Generative story for BiLDABiLDA takes advantage of the assumed topical alignment at the level of linked documents by introducing a single variable θ (see Section 2.1) shared by both documents.θjdenotes the distribution of latent cross-lingual topics over each document pairdj. For each document pairdj, a per-document topic distribution3The correct term here should be per-pair topic distribution for BiLDA and per-tuple topic distribution in case when more than 2 languages are involved, but we have decided to retain the original name of the distribution in order to draw a direct comparison with standard monolingual LDA.3θjis sampled from a conjugate Dirichlet prior4For an introduction to conjugate distributions, priors and Bayesian inference, we refer the curious reader to the excellent Heinrich’s overview (Heinrich, 2008).4with K parametersα1,…,αK. Then, with respect toθj, a cross-lingual topiczjiSis sampled. Each wordwjiSat the position i in the source document of the current document pairdjis then generated from a multinomial distributionϕzjiS. Similarly, each wordwjiTof the target language5Both words (w-s) and topics (z-s) are annotated with a corresponding superscript S or T to denote which language they are used in.5is also sampled following the same procedure. Note that words at the same positions in source and target documents in a document pair need not be sampled from the same latent cross-lingual topic. The only constraint imposed by the model is that the overall distributions of topics over documents in a document pair modeled byθjhave to be the same. The validity of this assumption/constraint is dependent on the actual degree of thematic alignment of two coupled documents, as well as on the chosen topic granularity (e.g., two Wikipedia articles about the same subject may have the same focus and share global topics, but at a finer scale, they might exhibit different sub-focuses and do not share a subset of other more local topics).According to Griffiths et al. (2007), each hyper-parameterαkcould be interpreted as a prior observation count for the number of times topiczkis sampled in a document (or document pair) before having observed any actual words. If one is in possession of a certain prior or external knowledge (e.g., document metadata, main themes of a document collections) about the topic importance and the likelihood of its presence in the data, introducing asymmetric priors gives more preference to a subset of the most important topics, which could in the end lead to a better estimated set of output distributions (Mimno & McCallum, 2008; Jagarlamudi, DauméIII, & Udupa, 2012). However, it is often the case that we do not possess any prior knowledge about themes in a text collection, and then it is reasonable to assume that all topics are a priori equally likely. Therefore, it is convenient to use a symmetric Dirichlet distribution with a single hyper-parameter α such thatα1=⋯=αK=α. Similarly, a symmetric Dirichlet prior is placed on ϕ and ψ with a single hyper-parameter β. β may be interpreted as a prior observation count of the number of times words in each language are sampled from a topic before any observations of actual words. Placing these Dirichlet prior distributions on multinomial distributions θ, ϕ and ψ results in smoothed per-topic word and per-document topic distributions, where the values for α and β determine the degree of smoothing. The influence of these hyper-parameters on the quality of learned latent topics is a well-studied problem in monolingual settings (Asuncion, Welling, Smyth, & Teh, 2009; Lu et al., 2011) and it can be generalized to multilingual settings.A natural extension of BiLDA that operates with more than two languages, called polylingual topic model (PolyLDA) has been presented by Mimno et al. (2009). A similar model has also been proposed by Ni et al. (2009, 2011). Instead of document pairs, they deal with aligned document tuples (where links between documents in a tuple are given), but the assumptions made by their model remain the same. Fig. 3shows the graphical representation in plate notation of the BiLDA model generalized to l languages,l⩾2, with document tuplesdj={dj1,…,djl}and a discrete set of l language-specific per-topic word distributions{ϕ1,…,ϕl}(see Section 2.1).On the other hand, when operating with only one language, BiLDA or (more generally) PolyLDA is effectively reduced to the standard monolingual LDA model (see Fig. 4and compare it with Fig. 2 or Fig. 3) (Blei et al., 2003b), that is, the monolingual LDA model is only a degenerate special case of BiLDA and PolyLDA (see also Section 2.1).The goal of training the BiLDA model is to discover the layer of latent cross-lingual topics that describe observed data, that is, a given bilingual document collection in an optimal way. It means that the most likely values for θ, ϕ and ψ have to be found by the training procedure. In simple words, we need to detect and learn which words are important for a particular topic in each language (that is reflected in per-topic word distributions ϕ and ψ), and which topics are important for a particular document pair (as reflected in per-document topic distribution θ). Similarly to the LDA model, the topic discovery for BiLDA is complex and cannot be solved by an exact learning procedure. There exist a few approximative training techniques which aim at converging to the correct distributions. Variational estimation for the monolingual LDA was used as the estimation technique in the seminal paper by Blei et al. (2003b). Other estimation techniques for the monolingual case include Gibbs sampling (Geman & Geman, 1984; Steyvers & Griffiths, 2007), and expectation propagation (Minka & Lafferty, 2002; Griffiths & Steyvers, 2004).An extension of the variational method to multilingual settings and its complete formulation for BiLDA was proposed and described by the authors (DeSmet & Moens, 2009). Due to its prevalent use in topic modeling literature in both monolingual and multilingual contexts (Boyd-Graber & Blei, 2009; Mimno et al., 2009; Jagarlamudi & Daumé III, 2010; Vulić et al., 2011a), we opt for Gibbs sampling as the estimation technique for the BiLDA models in all applications described in this article. Therefore, we here provide an overview of Gibbs sampling for BiLDA.Gibbs sampling is a Monte Carlo Markov chain (MCMC) estimation technique. MCMC is a random walk over a Markov chain where each state represents a sample from a specific joint distribution. Starting from a random initial state, the next state is repeatedly sampled randomly from the transition probabilities, and this is repeated until the equilibrium state is reached, in which case states are samples from the joint probability distribution. Gibbs sampling considers each word token in a text collection in turn and then samples a topic for that word token, where the probability of generating the current word by each topic is calculated conditioned given all other variables (including all other topics). For BiLDA in specific, the Gibbs sampling procedure follows the steps presented in Algorithm 2.Algorithm 2Gibbs sampling for BiLDA: An overviewAfter the convergence or the equilibrium state is reached, a standard practice is to provide estimates of the output distributions as averages over several samples taken in the equilibrium state.BiLDA requires two sets of formulas to converge to correct distributions: (i) one for each topic assignmentzjiS(a topic assigned to a word position i that generated wordwjiSin a document pairdj) and (ii) one for each topic assignmentzjiT. θ, ψ and ϕ are not calculated directly, but estimated afterwards. Therefore, they are integrated out of all the calculations, which actually leaves topic assignments for each word position,zjiS-s andzjiT-s as the only hidden variables. For the source part S of each document pairdjand each word position i, the probability is calculated thatzjiSassumes, as its new values, one of the K possible topic indices (from a set of K topics), as indicated by variablezk:(1)samplezjiS∼PzjiS=zk∣z¬jiS,zjT,wS,wT,α,β∼∫θj∫ϕPzjiS=zk∣,z¬jiS,zjT,wS,wT,α,β,θj,ϕdϕdθjIn this formula,zjTrefers to all target topic indices for document pairdj, andz¬jiSdenotes all source topic indices indjexcludingzjiS.wSdenotes all source word tokens in the corpus,wTall target words. Sampling for the target side (indices T) is performed in an analogous manner:(2)samplezjiT∼PzjiT=zk∣z¬jiT,zjS,wS,wT,α,β∼∫θj∫ψPzjiT=zk∣,z¬jiT,zjS,wS,wT,α,β,θj,ψdψdθjWe further show the derivation of the Gibbs sampler for BiLDA and explain the notation only for the source side of a bilingual corpus and the source languageLSwith indices S, since the derivation for the target side (with indices T) follows in a completely analogous manner. Starting from Eq. (1), we can further write:samplezjiS∝∫θj∫ϕPzjiS=zk∣z¬jiS,zjT,θ,α·PwjiS∣zjiS=zk,z¬jiS,w¬jiS,ϕ,βdϕdθ∝∫θjPzjiS=zk∣θj·Pθj∣z¬jiS,zjT,αdθj·∫ϕkPwjiS∣zjiS=zk,ϕk·Pϕk|z¬jiS,w¬jiS,βdϕkBoth θ and ϕ have a prior Dirichlet distribution and their posterior distributions are updated with the counter variable n (which counts the number of assigned topics in a document) and the counter variable v (which counts the number of assigned topics in the corpus) respectively (see the explanations of the symbols after the derivation). The expected values∫xf(x)dxfor θ and ϕ become:(3)=EDirichlet(nj,k,¬iS+nj,kT+α)[θj,k]·EDirichlet(vk,wjiS,¬S+β)[ϕkwji]Following Eq. (3), the final updating formulas for both source and target language for the BiLDA Gibbs sampler are as follows:(4)PzjiS=zk|z¬jiS,zjT,wS,wT,α,β∝nj,k,¬iS+nj,kT+αnj,·,¬iS+nj,·T+Kα·vk,wjiS,¬S+βvk,·,¬S+|VS|β(5)PzjiT=zk|z¬jiT,zjS,wS,wT,α,β∝nj,k,¬iT+nj,kS+αnj,·,¬iT+nj,·S+Kα·vk,wjiT,¬T+βvk,·,¬T+|VT|βThe counter variablenj,kSdenotes the number of times source words in the source documentdjSof a document pairdjare assigned to a latent cross-lingual topiczk(with index k), whilenj,k,¬iShas the same meaning, but not counting the currentwjiSat position i (i.e., it isnj,kS-1). The same is true for the target side and the T indices. When a “·” occurs in the subscript of a counter variable, this means that the counts range over all values of the variable whose index the “·” takes. So, whilenj,kScounts the number of assignments of wordswjiSto one latent topiczkindjS,nj,·Sdoes so over all K topics indjS.The second counter variable,vk,wji,¬Sis the number of times a word type whose token appears at position i (wjiS) gets assigned a latent cross-lingual topiczkin the source side of the entire document collection, but not counting the currentwjiS(i.e., it isvk,wjiS-1). Additionally,zjSdenotes all latent topic assignments for the source side of the document pairdj,z¬jiSdenotes all topic assignments for the source side ofdjbut excludingwjiS.vk,·,¬Scounts the total number of occurrences of source language words fromVSassociated with the topiczkin the whole corpus, as it is the sum over all possible source language words (a “·” appears instead of thewjiS). Again, because of the¬symbol in the superscript, the currentwjiSis not counted (i.e., the count is thenvk,·S-1). Finally,∣VS∣and∣VT∣are vocabulary sizes for the source and the target language, respectively.As can be seen from the first term of Eqs. (4) and (5), the document pairs are linked by the counter variablesnjSandnjT, as both sets of assignments:zjiSandzjiTare drawn from the sameθj(see the first term which is exactly the same in the updating formulas described by Eqs. (4) and (5)). The vocabulary counter variables operate only within the language of the word token currently being considered.With formulas (4) and (5). eachzjiSandzjiTof each document pair is sampled and cyclically updated. After a random initialization, usually using a uniform distribution, the sampled values will converge to samples taken from the real joint distribution ofθ,ϕand ψ, after a time called the burn-in period. From a set of complete burned-in Gibbs samples of the whole document collection, the final output probability distributions, that is, per-topic word distributions and per-document topic distributions are estimated as averages over these samples.Language-independent per-document topic distributions provide distributions of latent cross-lingual topics for each document in a collection. They reveal how important each topic is for a particular document. We need to establish the exact formula for per-document topic distributions for documents in an aligned document pair using Eqs. (4) and (5):(6)P(zk|dj)=θj,k=nj,kS+nj,kT+α∑k∗=1Knj,k∗S+∑k∗=1Knj,k∗T+KαThe representationRep(dj)Zof the documentdjby means of latent cross-lingual topicsZis then a K-dimensional vector where the k-th dimension of the vector is exactly the probability of the latent cross-lingual topiczkin the documentdj:(7)Rep(dj)Z=[P(z1|dj),P(z2|dj),…,P(zK|dj)]We may detect from Eq. (6) that two documents from an aligned document pair are enforced to have exactly the same topical representations, that is, the two documents discussing the same themes will be presented as exactly the same mixtures over the induced latent cross-lingual topics. This property is achieved by making the computation ofP(zk∣dj)for both documents in the pair explicitly dependent on the topic assignments counts from the source language document(nj,kS)as well as the target language document(nj,kT). Previous standard approaches to multilingual probabilistic topic modeling (see later Section 2.5) typically trained monolingual LDA on concatenated documents from an aligned document pair, where this property was not taken into account. In other words, unlike BiLDA, monolingual LDA builds a single topical representation for the artificially created concatenated document, and does not enforce this property at all. Comparisons of monolingual LDA trained on concatenated documents forming aligned document pairs (further MixLDA) and bilingual LDA reveal the superiority of the true multilingual approach modeled by BiLDA.Language-specific per-topic word distributions measure the importance of each word in each language for a particular latent cross-lingual topiczk. Given the source language with vocabularyVS, and the target language with vocabularyVT, and following Eq. (4), a probability that some wordwiS∈VSwill be generated by the cross-lingual topiczkis given by:(8)P(wiS|zk)=ϕk,i=vk,wiSS+β∑i∗=1|VS|vk,wi∗SS+|VS|βThe same formula, but now derived from Eq. (5) is used for the per-topic word distributions (ψ) for the target language:(9)P(wiT|zk)=ψk,i=vk,wiTT+β∑i∗=1|VT|vk,wi∗TT+|VT|βIn summary, these per-document topic distributions and per-topic word distributions are in fact mathematical realizations of the high-level intuitions and modeling premises clearly demonstrated in Fig. 1. For a better understanding of these core concepts, we refer the reader to study that figure again.Since the model possesses a fully generative semantics, it is possible to train the model on one multilingual corpus (e.g., multilingual Wikipedia) and then infer it on some other, previously unseen corpus. Inferring a model on a new corpus means calculating per-document topic distributions for all the unseen documents in the unseen corpus based on the output of the trained model (i.e., we effectively learn the MuPTM-based representation of an unseen document, see Definition 4 and Eq. (7)). Inference on the unseen documents is performed only one language at a time, e.g., if we train on English-Dutch Wikipedia, we can use the trained BiLDA model to learn document representations, that is, per-document topic distributions for Dutch news stories, and then separately for English news.In short, we again randomly sample and then iteratively update topic assignments for each word position in an unseen document, but now start from the fixed v counters learned in training, and then cyclically update the probability distributions from which the topic assignments are sampled. Since the inference is performed monolingually, dependencies on the topic assignments from another language are removed from the updating formulas. Hence, similar to Eq. (4), the updating formula for the source languageLSis:(10)P(zjiS=zk|z¬jiS,wS,α,β)∝nj,k,¬iS+αnj,·,¬iS+Kα·vk,wjiSS+βvk,·S+|VS|βLearning a multilingual topic model on one multilingual corpus and then inferring that model on previously unseen data constitutes the key concept of cross-lingual knowledge transfer by means of multilingual probabilistic topic models and that property is extensively utilized in various cross-lingual applications.A simple way of looking at the output quality of a topic model is by simply inspecting top words associated with a particular topic learned during training. We say that a latent topic is semantically coherent if it assigns high probability scores to words that are semantically related (Gliozzo, Pennacchiotti, & Pantel, 2007; Newman, Lau, Grieser, & Baldwin, 2010; Mimno, Wallach, Talley, Leenders, & McCallum, 2011; Stevens, Kegelmeyer, Andrzejewski, & Buttler, 2012; Aletras & Stevenson, 2013; Deveaud, SanJuan, & Bellot, 2013). It is much easier for humans to judge semantic coherence of cross-lingual topics and their alignment across languages when observing the actual words constituting a topic. These words provide a shallow qualitative representation of the latent topic space, and could be seen as direct and comprehensive word-based summaries of a large document collection. In other words, humans can get the first clue “what all this text is about in the first place”.The desirable property of semantic coherence comprises both a strong intra-semantic coherence, that is, words from the same vocabulary grouped together in the same topic are closely semantically related, as well as a strong inter-semantic coherence, that is, words across languages used to represent the same cross-lingual topic are also closely semantically related. Samples of cross-lingual topics extracted by BiLDA trained on aligned Wikipedia articles are provided in Table 1. We may consider this visual inspection of the top words associated with each topic as an initial qualitative evaluation, suitable for human judges.Besides this shallow qualitative analysis relying on the top words, there are other, theoretically well-founded evaluation metrics for quantitative analysis and comparison of different models. In the literature, latent topic models are often evaluated by their perplexity, where the perplexity or “confusion” of a model is a measure of its ability to explain a collectionCuof unseen documents. The perplexity of a probabilistic topic model is expressed as follows:(11)perp(Cu)=exp-∑d∈Culog∏w∈dP(w)∑d∈CuNdwhereNdis defined as the number of words in a documentd,P(w)is word w’s marginal probability according to a specific model, calculated as∑kP(w∣zk,ϒ), where k ranges over all K topics in the model, and ϒ is the set of the corpus independent parameters of the model. For BiLDA, the parameter set isϒ={α,β,ϕ,ψ,K}. A lower perplexity score means less confusion of the model in explaining the unseen data, and, theoretically, a better model. A good model with a low perplexity score should be well adapted to new documents and yield a good representation of those previously unseen documents. Since the perplexity measure defines the quality of a topic model independently of any application, it is considered an intrinsic or in vitro evaluation metric.Another intrinsic evaluation metric for multilingual probabilistic topic models, named cross-collection likelihood, was proposed recently in (Zhang et al., 2010), but that measure also presupposes an existing bilingual dictionary as a critical resource. Additionally, a number of intrinsic quantitative evaluation methods (but for the monolingual settings) are proposed in (Wallach, Murray, Salakhutdinov, & Mimno, 2009). Other studies for the monolingual setting focused more on automatic evaluation of semantic coherence (e.g., Chang, Boyd-Graber, Gerrish, Wang, & Blei, 2009; Newman et al., 2010; Mimno et al., 2011). However, perplexity still remains the dominant quantitative in vitro evaluation method that is predominantly found in the literature.Finally, the best way to evaluate multilingual probabilistic topic models is to test how well they perform in practice for different real-life tasks (e.g., document classification, information retrieval), that is, to carry out an extrinsic ex vivo evaluation. We later investigate whether there exists a mismatch between the intrinsic and extrinsic evaluation in information retrieval (see Section 6).Similarly to LDA in the monolingual setting (for which we have already shown that it is only a special case of BiLDA operating with only one language), we believe that bilingual LDA can be considered the basic building block of this general framework of multilingual probabilistic topic modeling. It serves as a firm baseline for future advances in multilingual probabilistic topic modeling. Although MuPTM is a quite novel concept, several other models have emerged over the last years. All current state-of-the-art multilingual probabilistic topic models build upon the idea of standard monolingual pLSA and LDA and closely resemble the described BiLDA model, but they differ in the assumptions they make in their generative processes, and in knowledge that is presupposed before training (e.g., document alignments, prior word matchings or bilingual dictionaries). However, they all share the same concepts defined in Section2.1, that is, the sets of output distributions and the set of latent cross-lingual topics that has to be discovered in a multilingual text collection.The early approaches (see, e.g., Dumais et al., 1996; Carbonell et al., 1997) tried to mine topical structure from multilingual texts using an algebraic model, that is, Latent Semantic Analysis (LSA) and then use the discovered latent topical structure in cross-lingual information retrieval. Artificial “cross-lingual” documents were formed by concatenating aligned parallel documents in two different languages, and then LSA on a word-by-document matrix of these newly built documents was used to learn the lower dimensional document representation. Documents across languages are then compared in that lower-dimensional space.Another line of work Zhao and Xing (2006, 2007) focused on building topic models suitable for word alignment and statistical machine translation operations. Again inspired by monolingual LDA, they have designed several variants of topic models that operate on parallel corpora aligned at sentence level. The topical structure at the level of aligned sentences or word pairs is used to re-estimate word translation probabilities and force alignments of words and phrases generated by the same topic.However, the growth of the global network and increasing amounts of comparable theme-aligned texts have formed a need for constructing more generic models that are applicable to such large-volume, but less-structured text collections. Standard monolingual probabilistic topic models coming from the families of pLSA and LDA cannot capture and accurately represent the structure of such theme-aligned multilingual text data in a form of joint latent cross-lingual topics. That inability comes from the fact that topic models rely on word co-occurrence information to group similar words into a single topic. In case of multilingual corpora (e.g., Wikipedia articles in English and Dutch) two related words in different languages will seldom co-occur in a monolingual text, and therefore these models are unable to group such pairs of words into a single coherent topic (for examples see, e.g., Boyd-Graber & Blei, 2009; Jagarlamudi & Daumé III, 2010). In order to anticipate that issue, there have been some efforts that trained monolingual probabilistic topic models on concatenated document pairs in two languages (e.g., Dumais et al., 1996; Littman, Dumais, & Landauer, 1998; Carbonell et al., 1997; Chew, Bader, Kolda, & Abdelali, 2007; Xue, Dai, Yang, & Yu, 2008; Cimiano, Schultz, Sizov, Sorg, & Staab, 2009; Roth & Klakow, 2010), but such approaches also fail to build a shared latent cross-lingual topical space where the boundary between the topic representations with words in two languages is firmly established. In other words, when training on concatenated English and Spanish Wikipedia articles, the learned topics contain both English and Spanish words. However, we would like to learn latent cross-lingual topics for which their representation in English is completely language-specific and differs from their representation in Spanish.Recently, several novel models have been proposed that remove such deficiency. These models are trained on the individual documents in different languages and their output are joint latent cross-lingual topics in an aligned latent cross-lingual topical space. The utility of such new topic representations is clearly displayed further in this article (see Sections 3–6). These models require alignments at document level a priori before training, which is easily obtained for Wikipedia or news articles. These document alignments provide hard links between topic-aligned semantically similar documents across languages.Recently, there has been a growing interest in multilingual topic modeling from unaligned text, again inspired by monolingual LDA. The MuTo model (Boyd-Graber & Blei, 2009) operates with matchings instead of words, where matchings consist of pairs of words that link words from the source vocabulary to words from the target vocabulary. These matchings are induced by the matching canonical correlation analysis (MCCA) (Haghighi et al., 2008; Daumé III & Jagarlamudi, 2011) which ties together words with similar meanings across languages, where similarity is based on different features. Matchings are induced based on pointwise mutual information (PMI) from parallel texts, machine-readable dictionaries and orthographic features captured by, for instance, edit distance. A stochastic expectation–maximization (EM) algorithm is used for training, and their evaluation has been performed on a parallel corpus. A similar idea of using matchings has been investigated in (Jagarlamudi & Daumé III, 2010). In their JointLDA model, they also observe each cross-lingual topic as a mixture over these matchings (or word concepts, as they name them), where the matchings are acquired directly from a machine-readable bilingual dictionary. JointLDA uses Gibbs sampling for training and it is trained on Wikipedia data. Although these two models claim that they have removed the need for document alignment and are fit to mine latent cross-lingual topics from unaligned multilingual text data, they have introduced bilingual dictionaries as a new critical resource. These machine-readable dictionaries have to be compiled from parallel data or hand-crafted, which is typically more expensive and time-consuming than obtaining alignments for Wikipedia or news data.Another work that aims to extract latent cross-lingual topics from unaligned datasets is presented by Zhang et al. (2010). Their Probabilistic Cross-lingual Latent Semantic Analysis (PCLSA) extends the standard pLSA model (Hofmann, 1999b) by regularizing its likelihood function with soft constraints defined by an external machine-readable bilingual dictionary. They use the generalized expectation maximization (GEM) algorithm (Mei, Cai, Zhang, & Zhai, 2008) for training. Similar to MuTo and JointLDA, a bilingual dictionary is presupposed before training and it is a critical resource for PCLSA. The dictionary-based constraints are the key to bridge the gap between languages by pushing related words in different vocabularies to occur in the same cross-lingual topics. The same relationship between pLSA and LDA (Girolami & Kabán, 2003) in the monolingual setting is also reflected between their multilingual extensions, PCLSA and BiLDA.In this article, we will present a subset of cross-lingual applications in which any multilingual probabilistic topic model may be utilized. In specific, we show the results obtained by BiLDA and provide an overview of its task performance. The goal of this article is however not to provide a direct comparison of different multilingual probabilistic topic models in various cross-lingual tasks, but to provide a comprehensive and didactic description of a general model-independent framework for building systems that rely on such multilingual probabilistic topic models and MuPTM-based representations of words and documents, and do not exploit any external expensive knowledge resource (e.g., parallel corpora, machine-readable dictionaries, extensive human annotations). Such data-driven unsupervised systems which exploit only internal evidence are essential for languages and language pairs with limited resources. We acknowledge that there exist numerous different techniques proposed for solving the presented tasks. However, our main focus is not to detect the best technique for each cross-lingual task, but to give a “cookbook” on how to exploit the latent cross-lingual topical knowledge as one source of evidence when dealing with these tasks in an unsupervised, language-independent and language pair independent manner.In addition, the reader has to be aware that significant portions of Application I (Section 3), Application II (Section 4), and Application IV (Section 6) contain already published work (DeSmet & Moens, 2009; De Smet et al., 2011; Vulić, DeSmet, & Moens, 2013), but we have decided to retain the essence and have rewritten the previously published work in a systematic and didactic manner in order to better stress the general applicability of text representations by means of latent cross-lingual topics in a variety of cross-lingual tasks, and to provide some new insights from observing all the applications together. Moreover, a significant portion of Application III (Section 5) is novel and previously unpublished.The first task we have chosen to present is cross-lingual event-centered news clustering. In general, event-centered news clustering may be considered an information retrieval task in which it is necessary to group news stories into coherent clusters, where each item (i.e., each news story) in one cluster should report on the same event. A special case is cross-lingual event-centered news clustering where one has to perform the clustering of news stories now written in different languages into groups of stories that describe the same event. Implicitly, that also defines a method for linking news stories across languages. Due to the dynamic and ever-changing nature of news, one needs an unsupervised tool that can coherently capture such dynamics and provide a structured representation of news stories irrespective to their actual language. Such event-centered cross-lingual clustering of related news stories is highly desirable in systems for browsing, categorizing and summarizing large news archives given in multiple different languages (Chen et al., 2000; Pouliquen, Steinberger, Ignat, Käsper, & Temnikova, 2004; Evans, Klavans, & McKeown, 2004; Kabadjov, Atkinson, Steinberger, Steinberger, & der Goot, 2010). Cross-lingual event-centered news clustering may be observed as a special case of the cross-lingual document clustering task (e.g., Montalvo, Martínez-Unanue, Casillas, & Fresno, 2006; Wu & Lu, 2007; Tang, Xia, Zhang, Li, & Zheng, 2011), with an extra constraint which specifies that documents – news stories should be clustered together if and only if they cover the same event.An event is defined as a well-specified happening at a certain moment in time (e.g., a single day or a short period) and space which deals with a certain set of news themes (e.g., a flood and a shortage of drinking water) and involves some named entities. Those named entities are actors of the events (e.g., persons or companies) or locations where the events occurred. Each news story typically reports on a single event, and since different sources can produce several stories on the same event in different languages, cross-lingual event-related clustering of these stories is required.An event can be observed as a mixture of different themes, where some themes are dominant, while others are only marginally present. That phenomenon can be captured by probabilistic topic models – per-document topic distributions will be higher for topics closely related to the themes prominent in a news story. Two news storiessiandsjare considered similar and are most likely discussing the same event if their per-document topic distributions are similar, that is, if the valuesP(zk∣si)andP(zk∣sj)are similar for allzk∈Z, whereZis the set of K latent cross-lingual topics (see Section 2.1). Note that by utilizing the language-independent setZand per-document topic distributions as the news story representation, we are able to perform the cross-lingual event-centered news clustering, i.e., the clustering of stories written in different languages, regardless of the actual language in the story. Previous systems for cross-lingual news clustering either relied on a readily available machine translation system (Montalvo, Martínez-Unanue, Casillas, & Fresno, 2007; VanGael & Zhu, 2007) for feature or document translation, or on the knowledge of shared or cognate named entities (Montalvo et al., 2007). As already proven by DeSmet and Moens (2009), here we stress that the cross-lingual topical knowledge and the representations by means of per-document topic distributions also prove beneficial for this task.

@&#CONCLUSIONS@&#
