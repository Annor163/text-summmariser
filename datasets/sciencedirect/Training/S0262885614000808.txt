@&#MAIN-TITLE@&#
Dynamic–static unsupervised sequentiality, statistical subunits and lexicon for sign language recognition

@&#HIGHLIGHTS@&#
A phonetic modeling approach for unsupervised sequentiality of dynamic-static SUs.Model based sign segmentation and subunit modeling with HMMs.Construction of a sign-to-subunits data-driven lexicon.Comparison of different signers' pronunciations and unseen signer pronunciation compensation.Evaluation on data from three corpora, two sign languages and unseen signers.

@&#KEYPHRASES@&#
Automatic sign language recognition,Data-driven subunits,Sub-sign phonetic modeling,Unsupervised,Segmentation,HMM,

@&#ABSTRACT@&#
We introduce a new computational phonetic modeling framework for sign language (SL) recognition. This is based on dynamic–static statistical subunits and provides sequentiality in an unsupervised manner, without prior linguistic information. Subunit “sequentiality” refers to the decomposition of signs into two types of parts, varying and non-varying, that are sequentially stacked across time. Our approach is inspired by the Movement–Hold SL linguistic model that refers to such sequences. First, we segment signs into intra-sign primitives, and classify each segment as dynamic or static, i.e., movements and non-movements. These segments are then clustered appropriately to construct a set of dynamic and static subunits. The dynamic/static discrimination allows us employing different visual features for clustering the dynamic or static segments. Sequences of the generated subunits are used as sign pronunciations in a data-driven lexicon. Based on this lexicon and the corresponding segmentation, each subunit is statistically represented and trained on multimodal sign data as a hidden Markov model. In the proposed approach, dynamic/static sequentiality is incorporated in an unsupervised manner. Further, handshape information is integrated in a parallel hidden Markov modeling scheme. The novel sign language modeling scheme is evaluated in recognition experiments on data from three corpora and two sign languages: Boston University American SL which is employed pre-segmented at the sign-level, Greek SL Lemmas, and American SL Large Vocabulary Dictionary, including both signer dependent and unseen signers' testing. Results show consistent improvements when compared with other approaches, demonstrating the importance of dynamic/static structure in sub-sign phonetic modeling.

@&#INTRODUCTION@&#
Sign languages are natural languages that manifest themselves via the visual modality in the 3D space. They convey information via visual patterns and serve for communication in parts of Deaf communities [2]. Visual patterns are formed by manual and non-manual cues. The automatic processing of such visual patterns for the Automatic Sign Language Recognition (ASLR) can bridge the communication gap between the deaf and the hearing. Since the early work of [3], there has been progress in visual processing, sign language phonetic modeling, and automatic recognition [1,4,5]. Moreover ASLR may contribute to other disciplines such as linguistics for the study of Sign Languages (SLs), via automated processing of corpora, whereas it is broadly related to human computer interaction.Herein we focus on sign language articulation produced by manual cues. The term “manual cues” refers to the movements and handshapes of both hands, one of which is considered as dominant. The dominant hand articulates the main phonetic parts. The other hand is referred to as non-dominant (ND). The ND hand contributes to symmetric/anti-symmetric movements or as a Place-of-Articulation (PoA). By PoA we refer to the location of the dominant hand in relation to either the body or the non-dominant hand. When the ND hand contributes in sign articulation, it is called active. Handshape, the form of the hand, equally plays a central role.A coarse correspondence of a “word” in spoken language is a “sign” in SL. The phonemes constituting a spoken word are concatenated sequentially across time as the English word “admit” is phonetically transcribed as [әdm'ɪt]. As discussed next, signs make use of both simultaneous [2] and sequential phonetic structure [6]. Signs tend to be monosyllabic [7]. Due to the larger articulators, for instance the hands versus the tongue, this sequential compositionality is transformed into simultaneity via multiple cues accommodating similar amounts of information in the spoken or signed propositions respectively [8]. Take for instance the signs in Fig. 1: articulation parameters such as type of movement, handshape, as well as facial cues may vary in parallel. Yet there are studies on the sequential structure of SL [9], as the seminal work of Liddell and Johnson (L&J) [6]. Varying and non-varying phonetic parts are sequentially stacked across time. We link the terms “varying/non-varying” to the cases of movements/non-movements respectively; for a familiar example refer to the corresponding, in a broad sense, vowel-consonant case in speech. Take for instance the Greek Sign Language (GSL) sign “SAY” in Fig. 1f. This sign is articulated employing the dominant hand. It consists of two different positions, at the mouth, and in the neutral space, before and after the downward movement. These three movements/non-movements parts are stacked in sequence, as “position-mouth”, “downward movement”, and “position-neutral space.” Thus, we conclude that the concept of the “phoneme” in SL is not to be taken for granted as in speech. There is still work in this direction both in the linguistic community [2,6,10], as well as from practical viewpoints such as computational recognition [11–13,1].In this context, the phonetic modeling for automatic recognition is challenging. First, as other authors mention too [14,13], there is a lack of formal dictionaries with sub-sign phonetic transcriptions, based on well-defined phone inventories and on a standard notation system. In automatic speech recognition (ASR), such resources are easily accessible, standard for several spoken languages, and reusable among research teams. For sign languages, the cases that employ sub-sign phonetic level dictionaries are as follows: On the one hand, data-driven approaches define a set of basic units computationally without the need of manual annotation; indicative examples include [11,14,13]. On the other hand, formally defined dictionaries are based on linguistic models such as the Movement–Hold [6], and sign notation systems such as the Stokoe system [2], the Hamburg notation system (HamNoSys) [15], or SignWriting [16]. These dictionaries are constructed by manual phonetic annotation which is time-consuming as in [12], by linguistic dictionary compilation [17], or recently via automatic processing as in [18,19]. In between, one finds approaches [20–23] that incorporate linguistic–phonetic concepts, ranging from Stokoe-driven decomposition to syllable phonetics. However, they do not lead to broadly reusable sub-sign transcriptions according to some known notation system or linguistic model [2,15,16,6]. Finally, there is a lack of phonetically transcribed data since annotation at the phonetic level is highly time consuming. Meanwhile, new SL corpora are being built [24–26], increasing the need for automatic processing. All the above render research in phonetic modeling for ASLR challenging.This paper introduces a novel SL phonetic modeling approach for unsupervised dynamic–static sequentiality with statistical subunits (2-S-U). By “dynamic–static subunit sequentiality” we refer to the sequential stacking of dynamic and static subunits across time. This approach provides by construction both sequential and simultaneous phonetic structure. This is accomplished without any linguistic prior. A valuable result of the above is the construction of an unsupervised data-driven subunit-level lexicon that shares the aforementioned properties. The 2-S-U approach includes first the unsupervised model-based sign segmentation and classification into dynamic and static segments, i.e., movements and non-movements, and then the construction of data-driven statistical dynamic and static subunits (SUs). The latter is implemented in a state synchronous multistream Hidden Markov Model (HMM) framework that encapsulates movement's dynamics. Moreover, it integrates movement and position cues as multiple stream observations. This scheme lets us employ different features and models for the dynamic and the static cases. The HMM-based SUs are the intra-sign primitives that are reused to reconstruct the signs in the lexicon. Although we do not incorporate any linguistic information, our approach is inspired by L&J's work on Movement–Hold [6]. As L&J suggested that signs are formed by movements and non-movements (postures), we explicitly model movements and non-movements. In this way we actually generate a sequential structure of sub-sign models. This sequential structure is considered partially “phonetically meaningful”; this holds in the above explained terms of movements and non-movements. An example of this decomposition into Movements (M) and Holds (H) for sign ADMIT – HMH – is illustrated in Fig. 2. We represent movements and non-movements by different feature cues in each case; these correspond to the above movements and holds. We call these cues “movement–position cues” (M–P), and they are used for the explicit training of the corresponding Dynamic and Static models. Finally, handshape is also incorporated as a parallel information cue.The overall framework is evaluated on data from three corpora and two SLs: Boston University SL corpus (BU400) [27], GSL Lemmas corpus (GSL-Lem) [26] and American Sign Language (ASL) Large Vocabulary Dictionary (ASLLVD) [24]. The experiments address multiple aspects such as exploitation of the M–P cues, integration of handshape information, employment of a single training example per sign, testing on unseen signers, and compensating for unseen pronunciations by employing a few development data. Finally, we present comparisons with three SU-level approaches [14,11,23], one sign-level approach [28] from the state of the art, and one similar approach to 2-S-U, without D/S discrimination (see Section 10). 2-S-U leads to improvements that show the importance of D/S sequentiality in sub-sign phonetic modeling.

@&#CONCLUSIONS@&#
