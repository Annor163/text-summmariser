@&#MAIN-TITLE@&#
ValWorkBench: An open source Java library for cluster validation, with applications to microarray data analysis

@&#HIGHLIGHTS@&#
Cluster validation for microarray data analysis is an essential task in bioinformatics and biomedicine that is not receiving enough attention.The state of the art does not offer software systems able to help the development of much needed new measures.We propose ValWorkBench and describe its internal modules and classes, in order to provide the much needed software platform for the development and testing of new validation measures as well as clustering algorithms.From the programmer point of view, no other platform in the same area offers what ValWorkBench provides.From the user point of view, the measures contained in it have been the object of, rigorous scientific studies, i.e., in BMC Bioinformatics 9 (2008) 462; Algorithms for Molecular Biology 6 (2011) 1; Natural Computing, pages 2014 (in press).

@&#KEYPHRASES@&#
Microarray cluster analysis,Bioinformatics software,Pattern discovery in bioinformatics and biomedicine,

@&#ABSTRACT@&#
The prediction of the number of clusters in a dataset, in particular microarrays, is a fundamental task in biological data analysis, usually performed via validation measures. Unfortunately, it has received very little attention and in fact there is a growing need for software tools/libraries dedicated to it. Here we present ValWorkBench, a software library consisting of eleven well known validation measures, together with novel heuristic approximations for some of them. The main objective of this paper is to provide the interested researcher with the full software documentation of an open source cluster validation platform having the main features of being easily extendible in a homogeneous way and of offering software components that can be readily re-used. Consequently, the focus of the presentation is on the architecture of the library, since it provides an essential map that can be used to access the full software documentation, which is available at the supplementary material website [1]. The mentioned main features of ValWorkBench are also discussed and exemplified, with emphasis on software abstraction design and re-usability. A comparison with existing cluster validation software libraries, mainly in terms of the mentioned features, is also offered. It suggests that ValWorkBench is a much needed contribution to the microarray software development/algorithm engineering community. For completeness, it is important to mention that previous accurate algorithmic experimental analysis of the relative merits of each of the implemented measures [19,23,25], carried out specifically on microarray data, gives useful insights on the effectiveness of ValWorkBench for cluster validation to researchers in the microarray community interested in its use for the mentioned task.

@&#INTRODUCTION@&#
The advent of high throughput technologies for biological and biomedical research, in particular microarrays, has demanded fast progress in many areas of the information sciences, including the development of mathematical and statistical software environments able to “standardize” many of the data analysis pipelines for biological investigation. Well known examples are BioJava [44], BioPerl [50], SeqAn [13], MatLab [32] and R [46]. The first three are software libraries that privilege the “programmer and algorithm engineer point of view”, in the sense that modules and procedures within the entire libraries can be used to develop new tools. The second two privileges the “user point of view”, since the available tools are offered either via a GUI or as a set of implemented functions, easy to use as a black box. However, from the “programmer and algorithm engineer point of view”, there is no direct access to the functions in order to use them as building blocks to implement new methods. That is due to copyright restrictions and/or to the complex structure and constrains of the existing packages.In terms of methodologies, cluster analysis, with its long record of deep mathematical and statistical studies on which it is based [16,33,29,34,30] and well documented success in many applied sciences, e.g., biomedicine [4], is a natural candidate for biological data analysis. As for microarrays, its analysis potential was shown almost immediately after their introduction in a pioneeristic paper by Eisen et al. [15]. Shortly thereafter additional results, mostly related to cancer classification [2,3,14,27,41,42,48], helped in establishing cluster analysis as one of the essential techniques to discover “biologically significant groups” in microarray data [10]. Since then, most of the attention has been devoted to the development of new clustering algorithms, although cluster analysis is a process that goes beyond the mere production of a partition of the data. Indeed, essential to the process is the assessment of the quality of the partition obtained by the algorithm, with the use of specific indices, referred to as validation measures, as well explained in [33]. Such a practice was mostly ignored in microarray data analysis, as well argued by Handl et al. [28] in a study that certainly contributed to the adoption of cluster validation techniques as a common practice in microarray cluster analysis. Indeed, some of the validation methods that have been specifically designed for microarrays have become very popular and seem to be used in many studies, e.g., the methods in [14,40]. Unfortunately, novel and effective validation measures are not easy to come by, in particular for very challenging high dimensional data such as microrrays, and in fact the entire area of cluster analysis for post-genomic studies is still the object of intense research [22,35].Relevant for this contribution is the state of the art regarding cluster validation software libraries In particular, there is no software library that offers a wide range of measures and that can be useful both for data analysis and for research in the development, prototyping and benchmarking of new validation measures. Indeed, with reference to the software environments mentioned above, clustering software is absent in BioJava, BioPerl and SeqAn. MatLab and R, being designed to cater to an audience wider than bioinformatics, computational biology and biomedicine, offer cluster analysis tools. However, as discussed in detail in Section 3, they privilege mostly the “user point of view”, the only notable exception being mosclust[52] that offers full documented access to its modules for program development. Unfortunately, the library consists of two internal validation measures only. Moreover, most of the validation measures present in the mentioned software environments do not allow to be used in conjunction with a clustering algorithm that has been developed outside of their specific programming environment. That is, an algorithm that is external to the system, available as an executable and compatible with the input/output conventions of the validation measures. Such a level of “algorithm independence” would allow the fast validation of novel external clustering algorithms, as discussed in Section 3.Therefore, given the state of the art depicted above, the main contribution of this paper is to fill an important gap in the literature by carrying out the non-trivial task of providing the full software documentation of ValWorkBench, an open source and portable Java library for cluster validation specifically tested on microarray data [23,25,19]. The objective is to grant full access to the wealth of software modules and classes present in the library, which can be used for the fast development, prototyping and testing of new internal validation measures as well as clustering algorithms, therefore privileging here the “programmer and algorithm engineer point of view”. For completeness, we mention ValWorkBench is the result of the accurate and robust comparative experimental analysis mentioned earlier (see [23,25,19] again). However, the primary intent of that line of research is to provide useful information for the choice of a measure in microarray applications, i.e., its precision in identifying the correct number of clusters and its time performance, in order to give the potential user useful insights on the effectiveness of the proposed library. However, as of now, its internal structure is not readily accessible and its wealth of modules and classes cannot be used for algorithm development and experimentation.ValWorkBench is freely and anonymously available at [1]. It is open source and distributed under the GNU licence. Javadocs and instructions on how to install it are available at the website, as well as the clustering algorithms binary executables (e.g. K-means [33], Non Negative Matrix Factorization [38] and Hierarchical [33]). ValWorkBench is tested and runs on any platform that supports Java version 1.6 or higher, in particular for Windows, Linux and Mac OS X.The remainder of this paper is organized as follows. Section 2 offers an overview of the library software structure as well as some background material on clustering, essential for the presentation of ValWorkBench. In particular, Section 2.1 provides the mentioned prerequisites. Section 2.2 presents, at a high level, the main software components of ValWorkBench. The next four sub-sections offer some details of particularly relevant classes. A fully commented account of code details is provided in the Additional Files available at the supplementary material website [1]. For completeness, JavDocs and user manuals are also provided there. Section 3 discusses the main progress, in terms of software design and algorithmic benchmarking, granted by the full documentation reported here of the internals of ValWorkBench. Moreover, such a contribution is also highlighted via a comparison of ValWorkBench with other libraries offering implementations of internal validation measures. The last section contains the conclusions and future steps.Following Handl et al. [28], cluster analysis is seen here as a three step process. The first, usually referred to as preprocessing, consists of data normalization, feature selection and of the choice of a distance function. The state of the art is given in [45] for normalization, in [39] for feature selection and in [21,20,22,43] for the choice of similarity/distance functions. Regarding the other two steps, which consist of the choice of a clustering algorithm and of a validation technique, in what follows we highlight the essential aspects of them, with some emphasis on cluster validation since it is central for this paper. To this end, we need to introduce some notation.Consider a set of n items Σ={σ1, …, σn}, where σi, 1≤i≤n, is defined by m numeric values, referred to as features or conditions, and let Ck={c1, c2, …, ck} be a partition of Σ. Each subset ci, 1≤i≤k, is referred to as a cluster, and Ckis referred to as a clustering solution.Usually, the partition of the items in Σ is accomplished by means of a clustering algorithm A. A recent survey of classic as well as more innovative methods, specifically designed for microarray data, is given in [4,49] and a more in depth treatment can be found for instance in [16,28,29,33,34]. For the convenience of the reader, we recall that clustering algorithms are classified into: partitional and hierarchical.The first type of clustering algorithms take as input Σ and an integer k and give as output a partition Ckof Σ, with |Ck|=k. It is worth pointing out that a partitional clustering algorithm can take as input a partition of the data and use it as an initial clustering solution that the algorithm refines, hopefully improving its quality. In this paper, we refer to this input option as external initialization. The second type of clustering algorithms produce a nested sequence of partitions, i.e. a tree. However, they can be easily adapted to generate a partition of a dataset into k clusters. The details are left to the reader.LetC¯jbe a reference classification for Σ, consisting of j classes. That is,C¯jmay either be a partition of Σ into j groups, usually referred to as the gold solution, or a division of the universe generating Σ into j categories, usually referred to as class labels.An an external measure E is a function that takes as input two partitions Cjand Ckand returns a value assessing how close Ckis to Cj. It is external because the quality assessment of the partition is established via criteria external to the data. Notice that it is not required that j=k. ValWorkBench provides the three most prominent external measures known in the literature: the Adjusted Rand Index [31], the F-Index [47] and the Fowlkes and Mallows Index (FM-Index for short) [18] (see Section 2.4 and Additional File 3 at the supplementary material website [1]).An important aspect of cluster analysis, referred to as model selection, is the determination of the number of clusters in a dataset. Technically, one is interested in the following:•Given: (a) A sequence of clustering solutions C1, …, Cs, obtained for instance via the repeated application of a clustering algorithm A; (b) a function R, usually referred to as a relative measure, that estimates the relative merits of a set of clustering solutions. One is interested in identifying the partitionCk*, among the ones given in (a), providing the best value of R. In what follows, the optimal number of clusters according to R is referred to as k*.It is worth pointing out that, in the specialistic literature, it is usual to refer to relative measures with the term internal. We follow that convention here. For the state of the art on internal measures, the reader is referred to [23,24,28,26]. Some of the most prominent internal measures are based on: (a) compactness; (b) hypothesis testing in statistics; (c) stability-based techniques and (d) jackknife techniques. This also gives a natural division of the main measures that are provided by ValWorkBench:(a)Within Clusters Sum of Square (WCSS for short) [30] and Krzanowski and Lai Index (KL for short) [37].Gap Statistics (Gap for short) [51].CLEST [14], Model Explorer (ME for short) [6], Consensus Clustering (Consensus for short) [40] and Fast Consensus (FC for short) [25].Figure of Merit (FOM for short) [56].Conceptually, ValWorkBench can be thought of as having two layers, referred to as task and service, respectively. A high level diagram of the library architecture is given in Fig. 1. The first layer consists of the measure package, further subdivided into two subpackages, offering all the methods to carry out various validation tasks. The second one consists of five additional packages that offer “basic service” to methods within all packages, and in particular to the ones contained in the task layer. They range from ensuring a uniform handling of data input to graphic routines. A synoptic description of each of them is provided next, grouped by layer.•The service layer packages:–datatypes: it contains classes encapsulating methods and state information for storing data related to the application of validation measures. It is presented in Section 2.3 and detailed in Additional File 1 at the supplementary material website [1].algorithms: it contains classes encapsulating methods and state information for computing clustering partitions of a specific dataset. In this library, only the class of Hierarchical Clustering algorithms is provided. Indeed, it consists of the class HLink.java only, supporting Average, Complete and Single Link cluster merging [33]. This choice is dictated by efficiency. Indeed, the availability of Hierarchical algorithms within the library allows to design measures that interleave the execution of a Hierarchical Clustering algorithm with the computation of the measure itself. That is, the computation proceeds by level of the hierarchical tree being built rather than by starting the entire clustering process again at each iteration of the measure being computed. The algorithms package is presented in Additional File 2 at the supplementary material website [1].graphics: it contains classes encapsulating methods and state information for visualizing results of clustering algorithms and validation measures. It is presented in Additional File 4 at the supplementary material website [1].nullmodels: it contains classes encapsulating methods to generate datasets from null models, those latter being a formalization of the intuition of “no structure” or “randomness” in a dataset. Those “data generation” procedures are central for the computation of many internal validation measures (see for instance [23,25,26]), although their range of application goes beyond those measures [33]. Consequently, internal to ValWorkBench, the ones most essential for data analysis are implemented [33]: Poisson, Principal Components and Permutational. The package is presented in Additional File 5 at the supplementary material website [1].exceptions: it contains classes encapsulating methods and state information for handling exceptions related to the application of validation measures. It is presented in Additional File 6 at the supplementary material website [1].The task layer package(s):–measures: it contains the Measure class that defines abstract and public methods common to all measures. Moreover, it contains two subpackages encapsulating methods and state information that implement external and internal validation measures. In particular, the ones that have been listed in Section 2.1. The two subpackages are defined as follows.external-it contains the following main subpackages:⁎adjustedRandfindexfmindexnullmeasuresThe first three packages naturally correspond to the external measures mentioned in Section 2.1.2. Moreover, it results convenient, for uniformity of notation, to consider the partition of a dataset D into clusters as a task performed by a null measure, whose software is contained in the forth package. Some level of detail about some of those packages is provided in Section 2.4. Full details are given in Additional File 3 at the supplementary material website [1].internal-it contains the following subpackages:⁎WCSSKLGapFOMdiffFOMCLESTConsensusCmodelExplorerEach of those packages naturally corresponds to each of the internal measures mentioned in Section 2.1.2. Again, some level of detail about some of them is provided in Section 2.5. Full details are given in Additional File 3 at the supplementary material website [1].It contains the classes that define the basic data types of the library.•Data Input Matrix. Given a dataset Σ, consisting of n elements, each being an m-dimensional vector, it can be represented in two different ways: as (1) a data matrix D, of size n×m, in which the rows represent the items and the columns represent the condition values; (2) a similarity/dissimilarity matrix S, of size n×n, in which each entry Si,j, 1≤i≠j≤n, quantifies the similarity/dissimilarity of the pair of items (i, j). Specifically, the value of Si,jcan be computed using rows i and j of D. The DataMatrix.java and SimilarityMatrix.java classes allow to store and handle the data matrix D and the similarity matrix S.Gold/Clustering Solution. Given a clustering solution C, the ClusterMatrix.java class allows to store and handle a clustering solution as well as a gold solution, as a matrix, while a linked list representation is managed by the ClusterList.java class. Moreover, C can also be represented by an n×n connectivity matrix MC, in which each entry MC(i, j) is 1 if the items i and j are in the same cluster, and 0 otherwise. The ConnetivityMatrix.java class allows to store and handle MC.Indicator Matrix. Given a dataset represented as a data matrix D, one can define a sampling dataset D′ as a dataset obtained taking n′ rows from D, with 0<n′<n. Let IDbe an n×n indicator matrix in which each entry ID(i, j) is 1 if the items i and j belong to D′, and 0 otherwise. The IndicatorMatrix.java class allows to store and handle ID.Consensus Matrix. LetD1′,D2′,…,Dh′be h sampling datasets of D and let C1, C2, …, Chbe the corresponding partitions into k clusters.A consensus matrix is defined as follows:(1)M(k)=∑hM(h)∑hI(h)where M(i) is the connectivity matrix of Ciand I(i) is the indicator matrix ofDi′, 1≤i≤h. The ConsensusMatrix.java class allows to store and handleM(k).Header Data. The HeaderData.java class allows to store and handle book-keeping information of a computational experiment. Indeed, it maintains all the information about the experimental set-up, e.g., the list of the command-line arguments used for the analysis as well as the running time of the experiment.Measure Vector. The MeasureVector.java class allows to store and handle the results of both external and internal measures.Input Measure. The InputMeasure.java class is an abstract class that encapsulates different state information fields, as well as some of the input data needed to compute an internal/external validation measure. Each measure has, as a parameter, a corresponding input class that is extension of the InputMeasure.java class. Such an organization allows to group together the input that is common to all measures, while delegating specialization to a lower level of implementation. This point is exemplified in the next subsection.The structure and content of this package is depicted in Fig. 2. For brevity, we limit ourselves to discuss with some level of detail only the classes contained in the packages corresponding to the Adjusted Rand Index RAand to the null measure. As anticipated earlier, a full description of the entire package is given in Additional File 3 at the supplementary material website [1].In what follows, for the definition of RA, we assume that one of the two partitions is the gold solutionC¯rwhile the other partition Ctis provided as output by a clustering algorithm, since a generalization of the definition to the case of two arbitrary partition is straightforward. Let ni,jbe the number of items common to bothc¯iand cj, 1≤i≤r and 1≤j≤t. Moreover, let|c¯i|=ni.and |cj|=n.j. We have:RA=∑i,jni,j2−∑ini.2∑jn.j2n212∑ini.2+∑jn.j2−∑ini.2∑jn.j2n2It has a maximum value of one, indicating a perfect agreement between the two partitions, while its expected value of zero suggests a level of agreement due to chance. Moreover, RAcan take values on a larger range than [0, 1] and, in particular, may be negative [17,55]. Therefore, for two partitions to be in significant agreement, RAmust assume a non-negative value substantially away from zero.The adjustedRand package contains two main classes: InputARand.java and AdjustedRand.java. The first is an extension of the abstract class InputMeasure.java and it provides input fields needed to compute the Adjusted Rand Index with the second class (see Fig. 2 again).The AdjustedRand.java class allows to compute the Adjusted Rand Index in two modes: single and iterative. In single mode, the agreement is computed between two given partitions of D stored both in the clusterMatrix data format. In the iterative mode, the agreement is computed between a given partition (e.g. the gold solution) and a set of partitions of D produced by a clustering algorithm, for all k in [kmin, kmax], i.e.,Ckmin,…,Ckmax.In order to grant simplicity and uniformity, the computation of a set of partitions of D can be seen as the computation of a measure that returns nothing, i.e., it is a null measure. It takes as input D, a clustering algorithm A and two integers kmin, kmax (with 1<kmin≤kmax) and it stores in main memory the sequence of partitions of D, i.e.,Ckmin,…,Ckmax, obtained via the repeated application of A. The package implementing this idea contains two main classes: InputNullM.java and NullMeasure.java. The role of the first one is analogous to the InputARand.java class and, as in that case, it is an extension of the InputMeasure.java class. As for the second, it has the following three extensions:•NullMeasureGeneric.java: it takes as one of its parameters the path name of a clustering algorithm binary executable, with input/output formats compatible with ValWorkBench. the clustering is then performed via that algorithm.NullMeasureHierarchical.java: the clustering is performed via Hierarchical Clustering, with an implementation internal to the library. Average, Complete and Single Link cluster merging are supported.NullMeasureHierarchicalInit.java: as pointed out in Section 2.1, the partition quality of some clustering algorithms may be improved by getting an external initialization. This class is analogous to the NullMeasureGeneric.java class in that it takes as a parameter a clustering algorithm external to the library. However, in addition to it, it provides a Hierarchical Clustering initiation to it. This latter feature is built-in.The structure and content of this package is depicted in Fig. 3. With reference to the description given in that figure, it is worth pointing out that, in addition to WCSS, the following classes also have extensions analogous to it: ConsensusClustering.java, GapStatistics.java, KrzanowskiLai.java and FigureOfMerit.java. Those extensions provide a standard implementation of a given measure along with variants that have been designed to be computationally efficient, via heuristics [23]. The advantages of such an approach are discussed and exemplified in Section 3. For brevity, we limit ourselves to discuss with some level of detail only the classes contained in the WCSS package. As anticipated earlier, a full description of the entire internal measures package is given in Additional File 3 at the supplementary material website [1].WCSS measures the “goodness” of a cluster via its compactness, one of the most fundamental indicators of cluster quality. Indeed, for each k∈[2, kmax], the method consists of computing the sum of the square distance between each element in a cluster and the centroid of that cluster. The “correct” number of clusters k* is predicted according to the following rule of thumb. For values of k<k*, the value of WCSS should be substantially decreasing, as a function of the number of clusters k. On the other hand, for values of k*≤k, the compactness of the clusters will not increase as much, causing the value of WCSS not to decrease as much. The following heuristic approach comes out [51]: Plot the values of WCSS, computed on a given set of clustering solutions, in the range [1, kmax]; choose as k* the abscissa closest to the “knee” in the WCSS curve.The WCSS package contains two main classes. Specifically, the InputWCSS.java class is an extension of the abstract class InputMeasure.java that allows to provide input fields needed to compute WCSS by the WithinClustersSumSquares.java class. This latter, is an abstract class for:•WCSSGeneric.java: it takes as one of its parameters the path name of a clustering algorithm binary executable, with input/output formats compatible with ValWorkBench. The computation of WCSS is then performed via that algorithm.WCSSFast.java: it is the implementation of a fast heuristic for the computation of WCSS. The interested reader can find details in [23].WCSSHierarchical.java: the computation of WCSS is performed via Hierarchical Clustering, with an implementation internal to the library. Average, Complete and Single Link cluster merging are supported.WCSSHierarchicalInit.java: this class is analogous to the WCSSGeneric.java class in that it takes as a parameter a clustering algorithm external to the library. However, in addition to it, it provides a Hierarchical Clustering initiation to it. This latter feature is built-in.

@&#CONCLUSIONS@&#
We have presented a new software library with an efficient and generic design that addresses a wide range of problems in cluster analysis. Moreover, it is the very first software development platform for cluster validation analysis that has been specifically designed to provide full usability of all its building blocks. In fact, departing from the current state of the art, the major novelty of ValworkBench is to place the “developer point of view” at a par with the “user point of view”. Those features make it a unique programming platform for cluster analysis, in particular for microarray, in bioinformatics, computational biology and biomedicine.ValWorkBench is under active development and we hope that it will become one of the standard platforms for algorithmic engineering at the interface of cluster analysis. We envision to extend the ValWorkBench framework including additional statistical tests for measuring the quality of a partition (e.g. Bayesian analysis), as well as including new building blocks (e.g. new data generation approaches and robustness analysis).The authors declare that they have no conflict of interest.