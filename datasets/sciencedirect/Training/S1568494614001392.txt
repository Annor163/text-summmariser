@&#MAIN-TITLE@&#
Comparative analysis of statistical and machine learning methods for predicting faulty modules

@&#HIGHLIGHTS@&#
We investigate the performance of the fault proneness predictions using static code metrics.The performance of logistic regression and six machine-learning methods (ANN, SVM, DT, CCN, GMDH and GEP) was evaluated in the study for predicting the fault proneness in the modules.Thus, the aim of this work is to find the best predictor for predicting faulty modules.The validation of the methods is carried out using Receiver Operating Characteristic (ROC) analysis.In order to perform the analysis we validate the performance of these methods using public domain AR1 and AR6 data sets.

@&#KEYPHRASES@&#
Software quality,Static code metrics,Logistic regression,Machine learning,Receiver Operating Characteristic (ROC) curve,

@&#ABSTRACT@&#
The demand for development of good quality software has seen rapid growth in the last few years. This is leading to increase in the use of the machine learning methods for analyzing and assessing public domain data sets. These methods can be used in developing models for estimating software quality attributes such as fault proneness, maintenance effort, testing effort. Software fault prediction in the early phases of software development can help and guide software practitioners to focus the available testing resources on the weaker areas during the software development. This paper analyses and compares the statistical and six machine learning methods for fault prediction. These methods (Decision Tree, Artificial Neural Network, Cascade Correlation Network, Support Vector Machine, Group Method of Data Handling Method, and Gene Expression Programming) are empirically validated to find the relationship between the static code metrics and the fault proneness of a module. In order to assess and compare the models predicted using the regression and the machine learning methods we used two publicly available data sets AR1 and AR6. We compared the predictive capability of the models using the Area Under the Curve (measured from the Receiver Operating Characteristic (ROC) analysis). The study confirms the predictive capability of the machine learning methods for software fault prediction. The results show that the Area Under the Curve of model predicted using the Decision Tree method is 0.8 and 0.9 (for AR1 and AR6 data sets, respectively) and is a better model than the model predicted using the logistic regression and other machine learning methods.

@&#INTRODUCTION@&#
As the size and complexity of the software is increasing day by day, it is usual to produce software with faults. The identification of faults in a timely manner is essential as the cost of correcting these faults increases exponentially in the later phases of the software development life cycle. Hence, testing is a very expensive process and one-third to one-half of the overall cost is allocated to the software testing activities [47]. There are several approaches being proposed to detect the faults in the early phases of the software development [47]. This motivates the development of fault prediction models that can be used in classification of a module as fault prone or not fault prone.Several static code metrics have been proposed in the past to capture various aspects in the design and source code, for example [17,22,35]. These metrics can be used in developing the fault prediction models. The prediction models can then be used by the software organizations during the early phases of software development to identify faulty modules. The software organizations can use these subset of metrics amongst the available large set of software metrics. The metrics are computed from the proposed model to obtain the information about the quality of the software and can be assessed in the early stages of software development. These quality models will allow the researchers and software practitioners to focus the available testing resources on the faulty areas of the software, which will help in producing a improved quality, low cost and maintainable software. The static code metrics have been advocated as widely used measures in the literature [9,36,37].The machine learning (ML) methods have been successfully applied over the years on a range of problem domains such as medicine, engineering, physics, finance and geology. These methods have also started being used in solving the classification and control problems [1,11–13,34,56]. In the existing literature, the researchers have used various methods to establish the relationship between the static code metrics and fault prediction. These methods include the traditional statistical methods such as logistic regression (LR) [8,26,32,39] and the machine learning (ML) methods such as decision trees [16,25,27,29,30,33,42,46,49,52], Naïve Bayes [6,9,10,38,41,49], Support Vector Machines [52,54], Artificial Neural Networks [28,33,38]. However, few studies compare the LR models with the ML models for software fault prediction using the static code metrics. Hall et al. [18] concluded that more quality studies should be conducted for software fault prediction using ML methods and Menzies et al. [36] advocated the use of public data sets for software fault prediction. It is natural for the software practitioners and potential users to wonder, “Which ML method is best?”, or more realistically, “Is the predictive capability of the ML methods comparable or better than the traditional LR methods?”. For this reason, this paper compares the model predicted using the LR method with the models predicted using the widely used ML methods and rarely used ML methods (Cascade Correlation Network (CCN), Group Method of Data Handling Polynomial Method (GMDH), Gene Expression Programming (GEP)). The evidence obtained from the data based empirical studies can help the software practitioners and researchers in obtaining the most powerful support for accepting/rejecting a given hypothesis [2]. Hence, conducting empirical studies to compare the models predicted using the LR and the ML methods is important to develop adequate body of knowledge so that the well formed and widely accepted theories can be produced.The main motivation of the paper is threefold: (1) to show the performance of the ML methods such as Support Vector Machines (SVM), Decision Tree (DT), CCN, GMDH and GEP for software fault prediction; (2) to compare and assess the predictive performance of the models predicted using the ML methods with the model predicted using the LR methods; and (3) to evaluate the performance capability of the ML methods using public data sets, i.e. across two systems.Thus, in this work we (1) build fault proneness models and (2) empirically compare the results of the LR and the ML methods. In this paper, we investigate the below issues:1.How accurately and precisely do the static code metrics predict faulty modules?Is the performance of the ML methods better than the LR method?The validation of the models predicted using the LR and the ML methods is carried out using Receiver Operating Characteristic (ROC) analysis. We use the ROC curves to obtain the optimal cut off point that provides balance between the faulty and non faulty modules. The performance of the models constructed to predict faulty or non faulty modules is evaluated using the Area Under the Curve (AUC) obtained from the ROC analysis [11]. In order to perform the analysis we validate the performance of these methods using public domain AR1 and AR6 data sets. The AR1 and AR6 data sets consist of 121 and 101 modules, respectively. These data sets were developed using C language [35]. The data are obtained from the Promise data repository [35] and collected by Software Research Laboratory (Softlab), Bogazici University, Istanbul, Turkey [19]. Thus, the main contributions of the paper are: first, we performed the comparative analysis of the models using the LR method with the models predicted using the ML methods for prediction of faulty modules. Second, we analyze public domain and industrial data sets, hence analysing valuable data in an important area. Third, we analyze six ML methods and apply ROC analysis to determine their effectiveness.The paper is organized as follows: Section 2 presents the research background that summarizes the static code metrics included and describes the sources from which the data is collected. Section 3 describes the descriptive statistics and the performance measures used for model evaluation. The results of model prediction are presented in Section 4 and the models are validated in Section 5. Section 6 summarized the threats to validity of the models and the conclusions of the work are given in Section 7.In this section we present the dependent and independent variables used in this paper (Section 2.1). We also describe the data collection procedure in Section 2.2.Fault proneness is the binary dependent variable in this work. Fault proneness is defined as the probability of fault detection in a module [2,4,7]. We use the LR method, which is based on probability prediction. The binary dependent variable is based on the faults that are found during the software development life cycle.For this study, we predict fault prone modules from static code metrics defined by Halstead [17], and McCabe [35]. The software metrics selected in this paper are procedural and module based metrics, where a module is defined as the smallest individual unit of functionality. We find the relationship of the static code metrics with fault proneness since they are “useful”, “easy to use”, and “widely used” metrics [36].First, the static code metrics are known as useful as they have shown higher probability of fault detection in past [36]. The results in this study also show that the correctly predicted percentage of faulty modules was high. Second, the metrics like lines of code and the metrics given by Halstead and McCabe [17,35] can be computed easily and at low cost, even for very large systems [30]. Third, many researchers have used the static code metrics in the literature [8,25–30,32,33,38–42,46,49,52–54]. It has been stated in [36] that “Verification and Validation textbooks advise using static code complexity metrics to decide which modules are worthy of manual inspections”. Table 1presents the static code metrics chosen in this study.This study makes use of two public domain data sets AR1 and AR6 available in the Promise data repository [45] and donated by Software Research Laboratory (Softlab), Bogazici University, Istanbul, Turkey [23]. The data in AR1 and AR6 are collected from embedded software in a white-goods product. The data was collected and validated by the Prest Metrics Extraction and Analysis Tool [44] available at http://softlab.boun.edu.tr/?q=resources&i=tools. The data in AR1 and AR6 was implemented in the C programming language. Both the data sets were collected in 2008 and donated by Softlab in 2009. The AR1 system consists of 121 modules (9 faulty/112 non faulty). The AR6 system consists of 101 modules (15 faulty/86 non faulty). Both the data sets comprise of 29 static code attributes (McCabe, Halstead and LOC measures) and 1 fault information (false/true). Table 2summarizes the distribution of faulty modules in the AR1 and AR6 data sets. The table shows that 7.44% of modules were faulty in the AR1 data set and 14.85% of modules were faulty in the AR6 data set.In this section, steps taken to analyze the static code metrics are described.Before further analysis can be carried out, the data set must be suitably reduced by analysing it and then drawing meaningful conclusions from it. Descriptive statistics such as minimum, maximum, mean, standard deviation can be effectively used as measured for comparing different experimental studies. The independent variable having low variance will not differentiate modules very well and hence are unlikely to be useful. Therefore, such variables are to be excluded from the analysis.Outliers are defined as the data points that are located away from the sample space. Outlier analysis is essential for identifying those data points that are over influential and must be analyzed for exclusion from the data set. In this study we found the Univariate and multivariate outliers. We used Mahalanobis Jackknife distance to identify the multivariate outliers. The details on outlier analysis can be found in [3].The performance measures used to assess the quality of the predicted model in this work are described in this section.The correctness of the LR and the ML methods are analyzed using performance measures such as the sensitivity and the specificity. The sensitivity of the model is defined as the percentage of the faulty modules correctly predicted and the specificity of the model is defined as the percentage of the non faulty modules correctly predicted. The sensitivity and specificity should be high in ideal situation. The sensitivity and the specificity as determined by the ROC analysis (see Section 3.2.3 for details) are reported for each model predicted in this paper.The precision is defined as the ratio of number of modules correctly classified to be faulty/non faulty to the total number of modules.The performance of the models predicted in this work is analyzed using the ROC analysis [21]. During the development of the ROC curves, many cutoff points between 0 and 1 are selected and the sensitivity and specificity at each cut off point is calculated.The Area Under the ROC Curve (AUC) is a combined measure of the sensitivity and specificity [14]. The AUC is used for comparing the predictive capability of the models predicted using the LR and the ML methods.The prediction accuracy of the models can be assessed by applying them to different data sets. Hence, we performed k-cross validation of the models developed [51]. In this method the data set is randomly divided into k subsets. Each time k−1 subsets are used as the training sets and one of the k subsets is used as the validation set. Finally, we get the probability of occurrence of faults for each of the module. The results were validated with the value of k equal to 10.In this section we describe the analyses performed to find the effect of the static code metrics and the fault proneness of the modules. We first employed the LR [24] method, which is the widely used statistical method to construct the fault proneness models. We then employed six machine learning methods (ANN, DT, SVM, CCN, GMDH, and GEP) to predict the fault proneness of the modules. These methods have been rarely applied in predicting fault prone/not fault prone modules.In these methods we applied multivariate analysis. Multivariate regression analysis is used to determine combined effect of static code metrics and fault proneness [1].During the multivariate analysis we did not found any of the outlier to be influential. In order to validate our results we used two data sets AR1 and AR6.Tables 3 and 4show “min”, “max”, “mean”, “std dev”, “variance”, “25% quartile” and “75% quartile” for all metrics considered in this study. The data was prepared by removing the outliers and metrics which did not had at least six values from the analysis.The research methodology and the results of analysing the relationship between the static code metrics and the fault proneness using the LR method is presented in this section.The LR method is the most widely used statistical technique [4] in the literature for predicting categorical dependent variable from the set of independent variables (a detailed description is given by [4,24]). In this study, independent variables are the static code metrics and the dependent variable is fault proneness. The LR method is divided into two categories: (i) Univariate LR and (ii) Multivariate LR.The univariate LR is a statistical method determines the relationship between each static code metric and fault proneness. The multivariate LR is used to construct a model by using all the static code metrics in combination for predicting faulty/non faulty modules. In the LR method there are two stepwise selection methods including forward selection and backward elimination [4]. In the forward stepwise method, the entry of each variable is examined stepwise at each step. The backward elimination method enters all the independent variables in the model at a time. The metrics are then removed one at a time from the model until a stopping criteria is satisfied. In this work we have applied the backward elimination method on the static code metrics selected significant in the univariate analysis.For the model predicted, the Coefficients (Ai's), the statistical significance (p-value), Maximum Likelihood Estimation (MLE), and the R2 statistics are reported. The higher value of the R2 statistic depicts that more is the effect of the independent variables on the dependent variable and higher is the prediction accuracy of the model. However, as stated in [6,55], “we should not interpret the value of R2 in logistic regression using the usual heuristics for linear regression R2s since they are built upon very different formulas”. Hence, high value of R2s rarely occur in the LR analysis. The details can be found in [2].A test of multicollinearity was performed on the models predicted in this paper. The presence of the multicollinearity makes the interpretation of the model difficult. Let the covariates of the model predicted beY1,Y2,…Yn. The maximum eigenvalue and the minimum eigenvalue (emax and emin, respectively) is calculated using the principal component analysis method. The conditional number is defined asλ=emax/emin. The multicollinearity of the model is not tolerable if the value of the conditional number is greater than 30 [5,50].In this subsection we find the effect of the independent variables (static code metrics) on the dependent variable (fault proneness). The multicollinearity of the predicted models is acceptable as the conditional number for the models is below 30 (see Section 4.2.1). In Tables 3 and 4, we summarize the coefficient (B), statistical significance (p-value), R2, and correctness. One metric was selected in the model predicted using AR1 (see Table 5). Table 6shows two out of the selected static code metrics are included in the model predicted using AR6 data set. The value of R2 statistic is 0.46.Table 7predicts accuracy of the model containing the static code metrics, using forward selection method. Two metrics Halstead effort and decision count are included in the model. During univariate analysis all the static code metrics chosen in this work were found to be significant.The model is applied to all the system modules to compare the accuracy of predicting faulty and non faulty software modules. The cut off point is selected using the ROC analysis to avoid the selection of arbitrary cut off point in the analysis [21]. This will provide balance between the values of the sensitivity and the specificity. The results of the model predicted using the AR1 data set shows that the sensitivity and specificity are 77.8% and 68.7%, respectively. In case of the AR6 data set, out of 15 modules actually fault prone, 12 modules were predicted to be fault prone. The sensitivity of the model is 80.00% (refer Table 5). Similarly, 65 out of 86 modules were predicted to be not fault prone. Thus, the specificity of the model is 75.58%. This shows that the model accuracy is not low.In this section, we present the results of models predicted using six ML methods.We use Multilayer Feed Forward and back propagation algorithm to train the network using the ANN method. The network constructed using the ANN method consists of three layers, input layer with I nodes, hidden layer with H nodes, and output layer with O nodes. The nodes in the input layer are connected to each node in the hidden layer and there is no direct connection between the input and the output nodes. The ANN adjusts the weights between connection of each nodes repetitively by computing the difference between the predicted output and the actual network. The network learns by searching the connection weights that minimizes the error between the actual and the predicted output on the training data set [46]. In order to perform the ANN analysis, we used min–max normalization is used. The min–max normalization transforms the data in the range of 0–1 [20,50] using the following formula:(1)v′=v−minMmaxM−minMwhere minMand maxMare the minimum and maximum values of an metric M.The ANN model was trained with the learning rate of 0.005, with the aim to continue until the minimum sum of squared error is reached.The principal components (P.C.) analysis was applied on the normalized metrics to produce domain metrics. The aim of the P.C. analysis is to transform the raw static code metrics to the measures which are not correlated to each other [31]. Given an a×b matrix of multivariate training data, where a represents system modules and b represents the static code metrics. The P.C. analysis reduces the column (a×p matrix) where b is reduced to p (p<b).The following domain metrics were given as input to the ANN model using the AR1 data set:•P1: Total operators, halstead length, halstead volume, halstead error, total operandsP2: total operators, halstead length, halstead volume, halstead error, total operandsP3: design density, normalized cyclomatic complexity, cyclomatic density, blank loc, design complexityP4: halstead level, decision density, unique operators, normalized cyclomatic complexity, cal pairs, code and comment loc, cyclomatic density, decision density, formal parameters, comment locP5: formal parameters, code and comment loc, decision density, normalized cyclomatic complexity, condition countP6: blank loc, code and comment loc, decision density, halstead level, formal parametersP7: cyclomatic density, design density, blank loc, halstead difficulty, code and comment locP8: multiple condition count, code and comment loc, decision density, unique operands, comment locP9: decision density, halstead difficulty, formal parameters, halstead level, blank locThe following domain metrics were given as input to the ANN model using the AR6 data set:•P1: Total operators, halstead volume, halstead length, total operands, unique operandsP2: Halstead level, halstead error, cyclomatic density, blank LOC, Branch countP3: Design density, design complexity, call pairs, normalized cyclomatic complexity, comment LOCP4: Design density, unique operators, halstead difficulty, condition count, multiple condition countP5: Formal parameters, normalized cyclomatic complexity, condition count, cyclomatic complexityP6: Formal parameters, normalized cyclomatic complexity, decision density, design complexity, call pairsP7: Normalized cyclomatic complexity, design density, halstead difficulty, comment LOC, unique operandsTable 8presents the architecture of the ANN model for the AR1 and AR6 data sets. The ANN method is non linear in nature, hence the traditional statistical tests applied in the LR method are not applicable here. Instead we used the AUC produced by the ROC analysis to heuristically analyze and assess the importance of the static code metrics (input variables) for model prediction purpose.In this section, we present and assess the results produced by applying the ANN method for the fault prediction. The module with one or more faults was considered to be faulty. The input to the model predicted using the ANN method were the P.C.s given above.Table 9presents the accuracy of the model predicted using the ANN method. The cut off point for the AR1 and AR6 models are 0.05 and 0.11, respectively. In case of the AR1 data set, the sensitivity and specificity of the model is 77.8% and 68.7%, respectively. Out of 9 modules actually fault prone, 7 modules were predicted to be fault prone. The sensitivity of the model is 94.20%. Similarly 13 out of 15 modules were predicted not to be fault prone. Thus, the specificity of the model is 86.50%. This shows that the model accuracy is very high.The SVM is a useful technique for classification of data and has been successfully applied in various modeling applications such as text classification, image analysis and face identification [56]. The SVM are capable of handling complex and large problems. Using the SVM method, an N-dimensional hyperplane is constructed that separates the input domain into binary or multi-class categories. This method constructs an optimal hyperplane which separates the categories of the dependent variable on each side of the plane [48]. The support vectors are present close to the hyperplane. The non linear relationships can be handled using the kernel function. The Radial Basis Function (RBF) maps the non linear relationships into a higher dimensional space so that the categories of the dependent variable can be separated. The k-means clustering is used to find the centers of the RBF network [43]. Fig. 1depicts the architecture of the RBF kernel in the SVM method. The circles depict one category of the dependent variable and the rectangle depicts the other category of the dependent variable. The support vectors are depicted by the shaded portions of the rectangles and circles.Given a set of (ai, bi),…,(am, bm) andbi∈{−1,+1}training samples. α=(i=1,…,m) is a Lagrangian multipliers. K(ai, bi) is called a kernel function and z is a bias. The discriminant function DF of two module SVM is given below [56]:(2)DF(b)=∑i=1mbiαiK(ai,a)+zThen an input pattern a is classified as [56]:(3)a=+1ifDF(a)>0−1ifDF(a)<0In this section, we present results of employing the SVM method to predict the fault proneness of a module. In case of the AR1 model, out of 9 modules actually fault prone, 7 modules were predicted to be fault prone (see Table 10). The sensitivity of the model is 77.78%. Similarly, the specificity of the model is 66.07%. In case of the AR6 model, the sensitivity and specificity of the model is 73.33% and 73.26%, respectively. This shows that the model accuracy is less as compared to model predicted using the LR and ANN approach.In this section, we present the results for predicting faulty and non faulty modules using the DT method.In the DT [42] method, each internal node in the tree represents the value of the independent variables and the terminal node represents the value of the dependent variable. The Classification and Regression Tree (CRT) algorithm splits the data into partitions based on the independent variables. At each step in the CRT algorithm the independent variable having the strongest relationship with the dependent variable is selected. In the CRT method, each parent node is split into only two child nodes. The aim of the CRT method is to maximize the within-node homogeneity.Table 11shows the subset of the metrics included in the model predicted using the AR1 and AR 6 data sets. Table 12presents the predicted correctness of the model developed using the DT method.The sensitivity and specificity of the model predicted with respect to the AR1 data set is 77.7% and 89.3%, respectively. Similarly, the sensitivity and specificity of the model predicted with respect to the AR6 data set is 93.3% and 81.4%, respectively. The results show that the accuracy of the DT model is high as compared to the models predicted using the LR, SVM and ANN methods.We used the correlation based feature selection (CFS) technique to select the uncorrelated and best predictors out of the set of the independent variables present in the training data set using correlation based heuristic [19]. All the possible combinations of the independent variables were searched to find the best combination of the independent variables. The CFS method ranks the metric subsets rather than the individual metric. The CFS is a heuristic that evaluates the predictive ability of the individual variables (static code metrics in our case) for predicting the dependent variable along with the correlation among the independent variables. In case of the AR1 data set, the predicted model shows better accuracy when the CFS technique was applied. However, in case of the AR6 data set, the model predicted showed lesser accuracy when subset of metrics were included using CFS. Hence, we did not use the CFS method in case of predicting model using the AR6 data set.The results of the correctness of predicted fault proneness model using the CCN approach is shown in Table 13. The sensitivity and specificity of the model predicted with respect to the AR1 data set is 88.89% and 76.79%, respectively. Similarly, the sensitivity and specificity of the model predicted with respect to the AR6 data set is 80% and 74.42%, respectively.The GMDH networks are known as self organizing networks that mean the connections between the neurons are not fixed during the training process. In the GMDH network, the number of layers are automatically decided so that an network with maximum prediction accuracy and without overfitting is constructed [48].Table 14shows the output generated for the GMDH polynomial model. Output from neuron j is shown as N(j). The final line shows the output of the network. In this case, the probability of faulty modules being true is predicted.The results of correctness of predicted fault proneness model using the GMDH polynomial network approach is shown in Table 15. The sensitivity and specificity of the model predicted with respect to AR1 data set is 88.89% and 73.21%, respectively. Similarly, the sensitivity and specificity of the model predicted with respect to AR6 data set is 80% and 74.42%, respectively.Genetic algorithms (GA) are search based algorithms that have seen explosion of interest since 1980s. These are natural biological evolution algorithms and are much faster than the exhaustive search procedures. GEP was developed by Candida Ferreira in 1996 and they are 100–60,000 faster than the GA [15]. GEP mutate quickly the valid expressions as they use Karva language to encode the expressions. They use genes that have two sections the head and the tail. The head consist of functions, variables and constants and the tail consist of variables and constants. The terminals in the tail are used if there are not are enough terminal in the head. The following are the major steps for training data set in the GEP:(1)Creation of initial population.Use of evolution to create well fitted individuals using mutation, transposition, inverse and recombination steps.Finding simpler functions.In Table 16, we summarize the parameters to and determined by the GEP method. We used 4 genes per chromosome and addition function to link the genes.The fitness function uses the following function, i.e. the number of faulty and non faulty modules predicted correctly with penalty for analysing the performance of the model. The aim of the GEP method is to maximize the value of the fitness function until the stopping criterion is met.Fitness=TFP+TNFPTFP+FNFP+FFP+TNFPThe results of correctness of predicted fault proneness model using the GEP method is shown in Table 17. The sensitivity and specificity of the model predicted with respect to the AR1 data set is 33.33% and 99.11%, respectively. Similarly, the sensitivity and specificity of the model predicted with respect to the AR6 data set is 53.33% and 97.67%, respectively.The models predicted in the previous section are applied on the same data set from which they are derived. The models predicted should be applied on the different data sets to gain insight about their prediction accuracy. Hence, we used 10-cross validation method on the LR, ANN, SVM, DT, CCN, GMDH, and GEP models following the procedure given in Section 3. For performing the 10-cross validation, the software modules were randomly divided into 10 partitions of approximately equal size (AR1: 9 partitions of 12 data points each and 1 partition of 11 data points, AR6: 9 partitions of ten data points each and 1 partition of 13 data points). Tables 18–24show the accuracy of the models predicted using the ML methods. We present the results of the cross validation of predicted models via the LR, ANN, SVM, DT, CCM, GMDH, and GEP approaches in Tables 25 and 26.As shown in Tables 25 and 26, the results of cross validation of the ML models were better as compared to the cross validation results of the LR model. The basis of comparison is the values of the AUC computed using the ROC analysis. The ROC analysis can be used to obtain the optimal cut off point that provides the balance between the number of faulty and non faulty modules. In Figs. 2 and 3, the ROC curves for the LR, ANN, SVM, DT, CCM, GMDH, and GEP models are presented.In case of the AR1 data set, the AUC of model predicted using the LR method was 0.94 which is much lower than the models predicted using machine learning methods. Similar results were shown when we predicted model using the AR6 data set. The AUC of model predicted using the DT method is 0.865 in case of AR1 and 0.948 in case of the AR6 data set. Thus, the model predicted using the DT method shows the highest values of AUC. In line with the other models predicted in literature, the findings of this study may also be validated externally. The models predicted using the DT method showed the best results. Although the LR models are widely used in construction of software quality models, but the results in this study show that the ML models show better predictive capability as compared to the LR model. Therefore, it appears that the model predicted using the ML methods might lead to construction of the optimum prediction models for developing fault prone models.The empirical studies consist of similar types of limitations which are summarized below:•The degree to which the results can be generalized to other research cannot be determined as we have used medium sized software in this paper.The study does not take into account the level of severity of faults hence the results cannot be used to determine the severity of the modules predicted to be faulty [14].The results in this study cannot be applied to the study having different dependent variable such as maintenance effort, testing effort, etc.Hence, the results in this study can be used for future guidance for predicting faulty modules using the static code metrics. However, the results of this needs to be externally validated.The main goal of our study was to examine the regression (LR), and the ML methods (ANN, SVM, DT, CCN, GMDH and GEP) in order to find the combined impact of the static code metrics on fault proneness. Thus we employed six ML methods (ANN, SVM, DT, CCN, GMDH and GEP) methods to assess the relationship between the static code metrics and fault proneness. The primary contribution of this work is to assess the predictive capability of the ML methods. We also compare and analyze the performance of the models predicted using the LR method with the models predicted using the ML methods. To validate the results we used two public domain data sets AR1 and AR6. The AUC was used to determine the performance of the ML methods.The AUC for the LR model was 0.494 and 0.538 for the models predicted using the AR1 and AR6 data sets, respectively. The AUC for DT model was 0.865 and 0.948, respectively. The models predicted using the ML methods outperformed the models predicted using the LR method for both AR1 and AR6 data sets. Thus, the models predicted using the DT showed the best results. This paper confirms that the models using the ML methods such as ANN, SVM, DT, CCN, GMDH and GEP have predictive ability for predicting faulty and non faulty modules.Hence, the researchers and software practitioners can use the model predicted in this paper to predict faulty or non faulty modules in the early phases of the software development. The software practitioners can focus the available testing resources on the faulty portions of the software.Replicated studies with large sized software should be carried out so that generalized results can be obtained. We may use the evolutionary or hybrid evolutionary algorithms for fault prediction in future.

@&#CONCLUSIONS@&#
