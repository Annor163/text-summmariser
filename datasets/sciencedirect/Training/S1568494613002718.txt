@&#MAIN-TITLE@&#
The role of cardinality and neighborhood sampling strategy in agent-based cooperative strategies for Dynamic Optimization Problems

@&#HIGHLIGHTS@&#
While low-complexity agent-based solvers are common in DOPs, more “intelligent” agents have rarely been studied.This work focuses on the use of cooperative strategies composed by trajectory based search agents for DOPs.We analyze the influence of the number of agents (cardinality) and the neighborhood sampling strategy (NSS).A low cardinality and a heterogeneous NSS composition leads to better and more robust performance, respectively.The best performing cardinality-composition pair improves significantly the state-of-the-art algorithms considered.

@&#KEYPHRASES@&#
Dynamic Optimization Problems,Agent-based optimization,Hybrid metaheuristics,Cooperative strategies,

@&#ABSTRACT@&#
The best performing methods for Dynamic Optimization Problems (DOPs) are usually based on a set of agents that can have different complexity (like solutions in Evolutionary Algorithms, particles in Particle Swarm Optimization, or metaheuristics in hybrid cooperative strategies). While methods based on low-complexity agents are widely applied in DOPs, the use of more “intelligent” agents has rarely been explored. This work focuses on this topic and more specifically on the use of cooperative strategies composed by trajectory-based search agents for DOPs. Within this context, we analyze the influence of the number of agents (cardinality) and their neighborhood sampling strategy on the performance of these methods. Using a low number of agents with distinct neighborhood sampling strategies shows the best results. This method is then compared versus state-of-the-art algorithms using as test bed the well-known Moving Peaks Benchmark and dynamic versions of the Ackley's, Griewank's and Rastrigin's functions. The results show that this configuration of the cooperative strategy is competitive with respect to the state-of-the-art methods.

@&#INTRODUCTION@&#
Dynamic Optimization Problems (DOPs) have attracted the attention of the scientific community in the past decade due to its closeness to real-world situations (trade market prediction, meteorological forecast, robotics motion control, etc. [1–3]). Solving this type of problems is harder than their “static” counterparts because of the presence of time-dependent properties that may affect the fitness function, constraints, number of variables, their domain, etc.It is not our purpose here to provide a literature review on the topic (the interested reader is referred to [4] and the website http://dynamic-optimization.org for more information) but nowadays, most of the publications in this field consider some assumptions for the resolution of DOPs:•Changes in the problem can be detected.Time-dependent features vary gradually.Keeping a good track of the optima is more important than refining the quality of the solutions, especially when the frequency of the changes is high.A population of solutions may better track the changes than a unique solution.Under these assumptions, it is not strange that most of the research is concentrated around population-based methods [2,5] where each member of the population can be considered as an “agent” in a wide sense. For example, an agent could be a very simple entity as a solution (an individual in the case of Evolutionary Algorithms [6,7] or a particle in PSO [8,9]), or a more complex object like a local search operator (mutation operator [10] or a full Tabu Search method [11]).While methods composed by low-complexity agents are widely extended in DOPs, the use of more sophisticated agents has been less explored. The present work focuses on this last topic and concretely on cooperative strategies where the agents implement trajectory-based algorithms.Having in mind the need for more competitive methods for solving DOPs, this paper pursues two main objectives. The first one is to analyze two of the features with a higher influence on the performance of cooperative strategies:•The number of agents composing the strategy, the so called cardinality.The Neighborhood Sampling Strategy (NSS) implemented by the agents.To carry out this study, we depart from the approach proposed in [12] where the authors presented a cooperative strategy composed by a set of Tabu Search agents that are controlled by a rule-based central coordinator. In this contribution, the strategy uses the same type of agent.To evaluate the influence of the cardinality (first feature), we test various configurations with different number of agents. For the second feature, we consider that all the agents can use the same or different NSS's, leading to a homogeneous or a heterogeneous composition, respectively. The analysis is done evaluating different pairs {cardinality, composition} over the well-known Moving Peaks Benchmark [1] and dynamic versions of the Ackley's, Griewank's and Rastrigin's continuous optimization functions.The second objective of the paper is to assess the performance of the best pair {cardinality, composition} by comparing it versus state-of-the-art algorithms for continuous DOPs.It is important to remark that we will only consider DOPs having changes in the objective function.This work extends the research presented in [11–13], where the same cooperative strategy was used. In the first one, the aim was to study the performance of trajectory-based agents in DOPs as well as the benefits of cooperation; the second work focused on the analysis of different coordination schemas with distinct control rules for this cooperative strategy; and the last paper provided a comparison of several state-of-the art algorithms for DOPs that included this cooperative strategy, emphasizing the comparison methodology and the visualization of the results.The paper is structured as follows. Section 2 describes the cooperative strategy, the Tabu Search implemented by the agents and the NSS's proposed. The experimental framework is in Section 3, where the benchmark problems, the state-of-the-art methods, the performance measures and the analysis techniques used are presented. Section 4 contains the results related with the first aim of the paper while Section 5 shows the comparison of the best cooperative strategy obtained versus the state-of-the-art algorithms. Finally, conclusions are given in Section 6.As we stated before, the cooperative strategy used in this paper was previously presented in [11,12]. In this section, we describe the main aspects of this strategy, the Tabu Search method implemented by the agents and the three Neighborhood Sampling Strategies considered. Further details of the strategy and the Tabu Search method can be found in previous references.The strategy consists of a set of agents guided by a central coordinator [12] as shown in Fig. 1. The agents explore the search space using a Tabu Search algorithm, periodically sending performance reports to a coordinator which analyses them and readjusts the behavior of the agents by sending orders to them. The orders indicate the agents the point from which they should restart their search, either to move them into more promising region or to escape from a local minimum. The exchange of data is done using a blackboard model [14]. Concretely, two blackboards are available, one to send performance reports from the agents to the coordinator, and another, to send orders from the coordinator to the agents.Within the cooperative strategy, the coordinator and the agents maintain certain information about the search (e.g. fitness of the current solution, best solution found so far, etc.). When a change is detected by any of the agents, the rest of them should be informed in order to update or discard their search information. The mechanism to detect changes and the system to communicate these changes are described below.An agent detects fitness function changes by reevaluating its best solution found so far and checking whether its fitness has changed or not. This action is done after each neighborhood exploration to ensure early detection of changes. When an agent i detects a change, it increases a counter cci(change counter), that stores the number of changes detected so far (see Fig. 2). Then it will send the ccivalue to the coordinator in the next report. The coordinator also maintains a register, called ccglobal, with the highest ccireceived so far. After receiving a report, the coordinator determines whether the agent has detected a new change (cci>ccglobal) to update ccglobal; or if it has already perceived the current change (cci=ccglobal) or not (cci<ccglobal). In the former case, the coordinator notifies those agents that have missed the change to make all the strategy's components work with current information, as Fig. 2 illustrates.Now, we can describe the cooperative strategy with the help of Fig. 3. For the sake of clarity, we divide the description in agents and coordinator:At the beginning, the agents are initialized and run asynchronously. Each agent first checks if it has received an order or report from the coordinator. This report contains:•Agent identificationsrelocchangeNotificationWhen the received report has changeNotification = True, the agent updates its ccicounter and restarts memories having “obsolete” information as: components of the optimizer (tabu list), the fitness of the current solution, the best solution found so far (sbest) and a list where it keeps the local minima found since the last report. These local minima are solutions that could not be improved by the agent after exploring the corresponding neighborhoods a certain number of times. Before running the optimizer, the agent checks if it has to relocate its search (sreloc≠∅) and in this case, it sets its current solution to sreloc. After a given number of fitness function evaluations, the agent stops the search process, updates the memories, records the performance information in a report and sends it to the coordinator. These performance reports include the following information:•Agent identificationsbestLocal minima found since the last reportccicounterWhen a report from an agent is available in the input blackboard, the coordinator first checks if the agent has not detected the current change (cci<ccglobal). In this case, the agent is notified using an order report. Otherwise, if the agent detected a new change (cci>ccglobal), the coordinator resets its memories and updates ccglobal.In the next step, the coordinator updates some internal information (memories) using the performance information stored in the report. Firstly, if sbestis better than Gbest(the global best solution found so far), then Gbest←sbest.Secondly, the Visited Region List (VRL) is processed. VRL is a structure where the coordinator keeps the history of the local minima found by all agents. Each entryv∈VRL is a local minimum that has associated a hyperspherehypervcentered invwith radius ρ and a counterφvwith the number of times this hypersphere has been visited by any of the agents. ρ is a parameter set by the user and constant along the search process.The VRL is updated with the list of local minima available in the report. For each local minimum x in the reported list, the coordinator checks ifx∈hypervfor anyv∈VRL. If suchvexists, thenφvis incremented. In other case, x is added to VRL.Once the coordinator updates the memories, it applies a set of rules of the formIF condition THEN relocate agent to evaluate the performance of the agent as well as the position to be placed to improve its performance (if needed).In this work we used the best performing rule presented in [11] where the antecedent (condition) was designed using Reactive Search ideas [15] (it was also evaluated in [16] for static domains). Specifically, the control rule implemented by the coordinator is:IFthe last local minimum found by agentibelongs to av∈VRLwithφv>λreactionTHENrelocate agentiin srelocThe threshold λreactionregulates the activation of the rule and srelocis set to the mid-point between Gbestand sbest. The objective of this rule is to move the agent trapped in a local minimum, closer to the best global solution found. If the rule is triggered, the coordinator records the relocation order in a report which is sent to the agent through the output blackboard.As mentioned before, the optimization method used by the agents is a Tabu Search. The details of this method are described in the next subsection.Agents are Tabu Search metaheuristics containing features previously presented in [17,18]. From the former we took the variable move step as well as some parts of its tabu neighborhood exploration method and from the latter, its diversification procedure.Algorithm 1Continuous Tabu Search pseudocodeprocedureContinuousTabuSearchδ←δinitx ← GenerateInitialSolution()xbest←xxcbest←xconsAdvances←0while not stopping condition dox ← ExploreNeighborhood(x,δ)iff(x)<f(xcbest) thenxcbest←xconsAdvances←consAdvances+1iff(x)<f(xbest) thenxbest←xend ififconsAdvances=2 thenδ ←min(δinit, δ·2)consAdvances←0end ifelseδ←δ2end ififδ<resthenδ←δinitif IsNearLocalMinimum(xcbest, LRR) thenx ← Diversification(xcbest)else{xcbestis considered as a new local minimum }AddLocalMinimum(xcbest)end ifxcbest←xconsAdvances←0end ifAn important issue to address in continuous optimization is the step size δ, that is, the maximum distance from the current solution up to which neighborhood solutions are explored. A small step size can lead to an important waste of objective function evaluations, whereas a big step size may avoid finding solutions with enough accuracy. For this reason it is interesting to use a large step size at the beginning of the search to do a better exploration, and then to make it smaller to obtain a better fine tuning of the solution. In this way, the algorithm starts with a step size of length δ=δinitand every time the exploration of the neighborhood of the current solution does not lead to a better position, this size is halved. When two consecutive movements lead to improvements in the current solution, the step size is multiplied by two. The length of the movement is delimited by the interval [res, δinit], where res is a parameter which determines the precision of the algorithm.Algorithm 1 shows the Tabu Search pseudocode. Let us suppose we are dealing with a minimization problem. After an initialization stage, the Tabu Search iteratively proceeds as follows. Firstly, the loop explores the neighborhood of the current solution and checks whether the chosen neighbor is better than xcbest. If so, the search is allocated at this point and the step size is increased if two consecutive improvements have taken place. Otherwise, δ is halved.When the step size δ is lower than res, the current solution is identified as a local optimum. Then, the method evaluates whether it is inside a previously visited local minimum (the distance is lower than a parameter named LRR) or not, that is, if it is inside a tabu region or not, respectively. When it is inside a tabu region, the method applies a diversification mechanism (procedure 2.1 of [18]), to restart the search in a non-explored region; otherwise, the Tabu Search stores xcbestas a new local minimum and continues the search from its current point.Algorithm 2Neighborhood Exploration pseudocodeprocedureExploreNeighborhood(x, δ)r← uniform random number in [0, 1]ifr<Paddthenxnew← ApproximateDescentDirection(x,δ)end ififr≥PaddOR (r<PaddAND f(xnew)>f(x)) thenxnew← ApplyNeighborhoodSearchStrategy(x,δ)end ifreturnxnewThe procedure ExploreNeighborhood is shown in Algorithm 2. The Tabu Search explores the neighborhood in two different ways that are selected according to a probabilistic criterion. The first one, chosen with probability Padd, consists on finding a good descent direction using the Approximate Descent Direction (ADD) method [19] and taking the neighbor at distance δ in that direction. The second one, selected with probability 1−Padd, consists on the generation of 2n solutions by a neighborhood sampling strategy (NSS). A NSS is defined by a neighborhood operator O and a selection operator Γ. The neighborhood operator is an application O:S×P→S, where S is the solution space and P is the space of parameters. This operator generates new solutions from a reference solution X∈S in a non deterministic way, that is, various applications of O over the same solution return different solutions. The selection operator Γ:2S×P→S, receives as argument a set of solutions and returns one of them according to certain criteria and parameters. Given a solution X, the NSS works in the following way: it generates 2n neighbors applying 2n times the operator O over X and then, selects one using Γ. The second exploration mode is also applied every time the ADD mode does not lead to a better solution.We chose three NSS's to induce different behaviors to the Tabu Search algorithm. Their descriptions are given below.Given a solution x={x1, …, xn}, FTSS generates 2n neighbors in the next way: for each coordinate i, two solutions are obtained by adding and by subtracting, only to this coordinate, the current step size δ to xi(e.g. for i=2, the two solutions generated are {x1, x2+δ, …, xn} and {x1, x2−δ, …, xn}). Then, the selection operator returns the best non-tabu solution or the best solution if it fulfills the aspiration level, that is, if it is better than the best solution found so far by the agent. The tabu list is composed by the reverse of the movements previously accepted. A move becomes non-tabu after a given number of iterations defined by the parameter tenure, that is set to 1.In this case, the neighborhood operator generates 2n solutions uniformly distributed within a hypersphere with radius δ and centered at the current solution. The selection operator returns the best generated solution.This NSS explores the neighborhood of the current solution by an operator that generates solutions according to a gaussian distribution N(0, 1). Concretely, this operator calculates the ith component of the new solution by the equationxinew=xi+δr, where xiis the ith coordinate of the current solution and r is a gaussian N(0, 1) distributed random number. The selection operator returns the best of the 2n neighbors.The Tabu Search parameters are displayed in Table 1and were selected according to [17,18]. Regarding the coordinator settings, the radius for the visited regions ρ was set to ρ=0.15·xrange, where xrange is the difference between the variables’ upper and the lower bounds (assumed equal for all the variables).The solution srelocused in the consequent of the control rule is generated as follows: given two solutions x1={x11, …, x1n} and x2={x21, …, x2n}, each component of the resulting solution z is calculated as zi=(x1i+x2i)/2, i=1, …, n.Different experiments have been performed in order to study the influence of the cardinality and the composition on the performance of the cooperative method. In this section we describe the benchmark functions, the implementation details and the experimental set up.The MPB is a classic test benchmark for DOPs originally proposed in [1]. It is a maximization problem consisting in the superposition of m peaks, each one characterized by its own height (h), width (w), and location of its center (p). The fitness function of the MPB is defined as follows:(1)MPB(x)=maxjhj−wj∑i=1n(xi−pij)2,j=1,…,mwhere n is the dimensionality of the problem. The highest point of each peak corresponds to its center, and therefore, the global optimum is the center of the peak with the highest parameter h.Dynamism is introduced in the MPB by periodically changing the parameters of each peak j after a certain number of function evaluations (ω):(2)hj(t+1)=hj(t)+hs·pN(0,1)(3)wj(t+1)=wj(t)+ws·pN(0,1)(4)pj(t+1)=pj(t)+vj(t+1)(5)vj(t+1)=s|r+vj(t)|((1−λ)r+λvj(t))Changes to both width and height parameters depend on a given severity for each of them (wsand hs). Changes to the peak position depend on a shift vector vj(t+1), which is a linear combination of a random vector r and the previous shift vector vj(t) for the peak, normalized to length s (position severity, shift distance, or simply severity). The random vector r is created by drawing random numbers for each dimension and normalizing its length to s. Finally, parameter λ indicates the linear correlation with respect to the previous shift, where a value of 1 indicates “total correlation” and a value of 0 “pure randomness”.One way to obtain artificial dynamic optimization problems is to depart from a “static” function and then add some sort of dynamism. Here, we use standard multimodal functions commonly used in static continuous optimization problems, namely Ackley, Griewank, and Rastrigin, whose expressions are shown below:(6)Ackley(x)=−20exp−0.21n∑i=1nxi2−exp1n∑i=1ncos(2πxi)+20+exi∈[−32,32],x*={0,…,0}∈ℝn(7)Griewank(x)=∑i=1nxi24000−∏i=1ncosxii+1(8)xi∈[−50,50],x*={0,…,0}∈ℝnRastrigin(x)=∑i=1nxi2−10cos(2πxi)+10xi∈[−5,5],x*={0,…,0}∈ℝnwhere n is the dimension of the problem, and x* is the optimum (in all the functions, the optimum is located at the origin of coordinates, the{0,…,0}∈ℝn).The dynamism induced to these functions consists in having the origin of coordinates as a parameter, p, that is changed periodically applying the dynamic model of the MPB described in Eqs. (4) and (5).Now, the function to be optimized becomes:f′(x)=f(x−p)f∈{Ackley,Griewank,Rastrigin}The main reason to also use these functions in the experimentation is the different nature of their multimodality with respect to the MPB. While the multimodality in the MPB is obtained through the composition of several unimodal functions, Ackley, Griewank and Rastrigin are inherently multimodal functions whose composition leads to a very distinct structure, landscape and dynamism of the local optima. For example, whereas in MPB the standard scenarios have up to 100 local optima (the number of peaks), the other three functions present up to thousands. Besides this, in the MPB problem the local optima are independent from each other. In the dynamic versions of Ackley, Griewank and Rastrigin there are some local optima whose movement is linked (those that belong to the same function) and others that is not (those that belong to different functions). In short, the different landscape and dynamism of these functions provide a wide and heterogeneous benchmark that allows us to accomplish a more robust study.According to literature, the dimensionality and the severity of the changes are factors of DOPs that clearly affect the performance of the algorithms. In the experiments conducted in this work, a set of different values for each of the two previous factors were selected and then, every possible combination of values was tested, for every problem, and every algorithm.The problems’ configurations are summarized in Table 2. These settings are based on the configuration parameters of the so called Scenario 2 of the MPB.The severity value depends on the range of the input variables x (all xidimensions have the same range in the benchmarks), but this range is different for each problem. In order to unify test configurations, the severity values is expressed as a percentage of x's range.In order to make our results comparable with other closely related works, the performance measure selected for our experiments is the offline error (eoff) [20]. This measure is the average of the error of the best solution found by the algorithm since the last change, for every function evaluation, and for all changes. Given that the changes in the environment are produced at a fixed rate, its formula is:(9)eoff=1NcNe∑i=1Nc∑j=1Ne(fi*−fij)where Ncis the total number of changes in the environment, Neis the total number of evaluations allowed in each change,fi*is the optimum value of the ith change, and fijis the best value found by the algorithm since the beginning of the ith change up to the jth evaluation.The execution of an algorithm for Ncchanges is defined as a run. In order to obtain statistically meaningful results, each experiment consisted of Nrindependent runs, each one with a different random seed. In this work, we have chosen Nr=50, and the comparison of the algorithms has been performed using those 50 eoffmeasures for each algorithm.The analysis and visualization of results when comparing algorithms over different DOPs, considering different severities and dimensions, is not a trivial task. Here, we resort to a technique named SRCS, introduced in [21]. SRCS focuses on creating a ranking of the algorithms, instead of displaying the absolute performance values of each one.The basic idea is as follows. Let us suppose that a set of algorithms are applied over a problem configuration. If a non-parametric test for multiple comparisons detects significant differences among the algorithms, then pairwise comparison tests are performed. For every algorithm, a rank value is calculated using the number of times that such algorithm was better, worse or equal than the others algorithms. After that, a color key is associated to each rank value in order to visually present the results. With the color scheme proposed in SRCS, a lighter color indicates a higher ranking, and therefore, a better performance. This idea is illustrated in Fig. 4. These rank colors can then be used instead of numerical values to better compress the information to display when presenting multiple problems/multiple configurations/multiple algorithms results.We will display the results of our experiments using the SRCS technique and a two level matrix arrangement where the rows and the columns will have a different meaning.On one hand, we use this two level matrix arrangement to compare the performance of different settings of the cooperative strategy over different configurations of a single problem (Fig. 5A). In this case, the rows and columns of the upper level matrix represent possible values for two settings of the cooperative strategy (e.g composition and cardinality). Each lower level matrix shows the results of a specific setup of the cooperative strategy over the different configurations of the problem at hand.The rows and columns of a lower level matrix represent different configurations for the severity and dimension of the problem, respectively. A cell in this matrix displays the color code returned by the SRCS technique for a set up in a specific problem configuration. To check the performance of a given setting over a single problem configuration, we have to compare the colors of the same positions in every lower level matrix, as Fig. 5A shows with black squares.In this particular example, we may say that {Feature 1-Value 4, Feature 2- Value 1} obtains the best results (white color) whereas configurations {Feature 1-Value 2, Feature 2-Value 3} and {Feature 1-Value3, Feature 2-Value 4} (darkest color) get the worst rankings.We will also use a two level matrix for the SRCS results to compare the performance of a set of methods over different configurations of a set of problems (Fig. 5B). In this case, the rows and columns of the upper level matrix are set to methods and problems, respectively. Every low level matrix shows the results obtained by a method in the configurations of a given problem. The lower level matrices display the same information as above. To compare the results of various algorithms over a single problem configuration, in this case we have to check the corresponding position of those lower level matrices that belong to the same column (as indicated with black squares).The SRCS technique gives statistical information about the ranking of a set of methods in just one configuration of one problem. However, it is also interesting to compare the results of the methods over a set of configurations from one or various problems to have a more comprehensive view. In this sense, García et al. proposed in [22] the directives to carry out this global analysis using paired non-parametric statistical tests. These tests allow to rank the performance of two or more methods over a set of problem configurations and to determine if the differences in performance are significant or not. The significance of the differences is assessed by comparing estimators (usually the mean/median fitness or error value) of the performance of the algorithms over each problem configuration.Matched pairs non-parametric tests allow, for example, to determine the best global method over all the configurations of the MPB problem or over the 36 problem configuration considered in this work. The specific matched pairs non-parametric tests used here were provided by the tool KEEL [23]. We used Friedman's test to determine if there are significant differences between the performance of two or more methods, Holm's test to detect which algorithms are worse than a control algorithm, and Wilcoxon paired test to check if there exist significance differences between the performance of two algorithms.

@&#CONCLUSIONS@&#
This work focused on cooperative methods for DOPs that are based on a set of cooperating agents that implement trajectory-based optimization methods (Tabu Search here).In first place, the contribution presented an analysis of the influence of the number of agents (cardinality) and composition (in terms of the neighborhood sampling strategy) on the global performance of the strategy. After an extensive experimentation over four problems and considering nine different combinations of severity and dimensionality, the main conclusions obtained are the next ones:•The lower the cardinality, the better the performance: strategies with just three agents outperformed those with a higher number of agents.The “best” NSS depends to a larger extent on the problem than on the dimension or severity of the configuration.The heterogeneous compositions showed a more robust behavior, over the four problems, than the homogeneous ones despite not being the best performing option in none of them.The global best performing {cardinality-composition} pair was the heterogeneous composition with 3 agents.This heterogeneous strategy with 3 agents was compared against some of the best performing methods for DOPs. This comparison revealed the cooperative strategy significantly outperforms the seven algorithms considered.The main trend in DOPs is the development of methods that consist of a relatively large number of agents. The wide number of population based Evolutionary Algorithms or Particle Swarm Optimization methods for DOPs found in the literature supports the former claim. However, as it was already pointed out in [11], if the number of agents is reduced but their “intelligence” is increased, very competitive results can also be obtained. The study presented in this work can be considered as a step forward in this direction, since we have shown that only three agents properly coordinated improve significantly some of the best performing algorithms for DOPS.