@&#MAIN-TITLE@&#
Hybrid parallel chaos optimization algorithm with harmony search algorithm

@&#HIGHLIGHTS@&#
Considering the weakness in the COA, parallel chaos optimization algorithm (PCOA) is studied in this paper.To obtain optimum solution accurately, harmony search algorithm (HSA) is integrated with the PCOA to form a novel hybrid optimization algorithm.Different chaotic maps are compared and the impacts of parallel parameter on the proposed hybrid algorithm are discussed.

@&#KEYPHRASES@&#
Chaos optimization algorithm (COA),Parallel chaos optimization algorithm (PCOA),Harmony search algorithm,Hybrid algorithm,Chaotic map,

@&#ABSTRACT@&#
The application of chaotic sequences can be an interesting alternative to provide search diversity in an optimization procedure, named chaos optimization algorithm (COA). Since the chaotic motion is pseudo-randomness and chaotic sequences are sensitive to the initial conditions, the search ability of COA is usually effected by the starting values. Considering this weakness, parallel chaos optimization algorithm (PCOA) is studied in this paper. To obtain optimum solution accurately, harmony search algorithm (HSA) is integrated with PCOA to form a novel hybrid algorithm. Different chaotic maps are compared and the impacts of parallel parameter on the hybrid algorithm are discussed. Several simulation results are used to show the effective performance of the proposed hybrid algorithm.

@&#INTRODUCTION@&#
Chaos has been applied in many scientific fields since the first introduction of canonical chaotic attractor in 1963 by Lorenz [1]. Chaos is a bounded unstable dynamic behavior that exhibits sensitive dependence on its initial conditions [2]. An essential feature of the chaotic systems is that small changes in the parameters or the starting values lead to the vastly different future behaviors. Recently, chaotic sequences have been adopted instead of random sequences to provide the search diversity in an optimization procedure, named chaos optimization algorithm (COA) [2–7]. Due to the non-repetition of the chaos, the COA can carry out overall searches at higher speeds than stochastic ergodic searches that depend on the probabilities. The COA, which has the features of easy implementation, short execution time and robust mechanisms of escaping from the local optimum, is a promising optimization tool for the engineering applications [2–7]. Recently, in most of those literatures, researchers have considered integrated use of chaotic sequences in order to enhance the performance of the meta-heuristics algorithms, such as chaotic harmony search algorithm [8,9], chaotic ant swarm optimization [10–12], chaotic particle swarm optimization [13–21], chaotic genetic algorithms or chaotic evolutionary algorithm [22–26], chaotic differential evolution [27–29], chaotic firefly algorithm [30], chaotic simulated annealing [31,32], hybrid COA with artificial emotion [33].Since the chaotic motion is pseudo-randomness and chaotic sequences are sensitive to the initial conditions, COA's search ability is usually effected by the starting values. So, a kind of parallel chaotic optimization algorithm (PCOA) is proposed in our former study [34,35], which shows its superiority over general COA. In the PCOA, multiple chaotic maps are simultaneously mapped onto one decision variable, so PCOA searches from diverse initial points and can detract the sensitivity of initial condition. Although the hybrid algorithm based on PCOA in [34,35] can reach optimum solutions, the search speed is slow since the employed local search technique – simplex search method has slow efficiency.Harmony search algorithm (HSA) [36–38], first introduced to optimize various continuous nonlinear functions by Geem et al. [36], is one of the latest evolutionary computation techniques, mimicking the musical process of search for a perfect state of harmony. In comparison to other meta-heuristics, the HSA imposes fewer mathematical requirements and can be easily adapted, furthermore, numerical comparisons demonstrated that the evolution in HSA is faster than genetic algorithm [39]. Recently, several improvements have been presented to HSA, such as chaotic harmony search algorithm [8,9] and improved HSA [38–41].In this paper, to obtain optimum solution accurately, HSA is integrated with PCOA to form a novel hybrid optimization algorithm. The proposed hybrid algorithm is a two stages search technique, the first stage is the PCOA with twice carrier wave parallel chaos search for global searching, and the second stage is the HSA for accurate local searching. In the proposed hybrid algorithm, PCOA is conducted until it has converged to a close neighborhood or it has reached its maximum iteration times, then the HSA is conducted for accurate solution. Different chaotic maps are compared and the impacts of parallel parameter on the hybrid algorithm are discussed.The rest of this paper is organized as follows. Section 2 briefly describes chaotic maps. The PCOA approach is introduced in Section 3. Section 4 gives presentation of the proposed hybrid PCOA with HSA. Simulation results showing the effectiveness of the hybrid algorithm in Section 5. Conclusions are presented in Section 6.One dimensional noninvertible maps have capability to generate chaotic motion. In this study, eight well-known one-dimensional chaotic maps in [42] are considered here.This map was introduced by Robert May in 1976, and he pointed out that the logistic map led to chaotic dynamics. Logistic map generates chaotic sequences in (0,1). This map is also frequently used in the COAs [2,14,22,34]. This map is formally defined by the following equation:(1)xn+1=φxn(1−xn),0<φ≤4,xn∈(0,1)Tent chaotic map is very similar to the logistic map, which displays specific chaotic effects. Tent map generates chaotic sequences in (0,1). This map is formally defined by the following equation:(2)xn+1=xn0.7,xn<0.7xn+1=103xn(1−xn),elseChebyshev chaotic map is a common symmetrical region map. It is generally used in neural networks, digital communication and security problems [16]. Chebyshev map generates chaotic sequences in (−1,1). This map is formally defined [43]:(3)xn+1=cos(φcos−1xn),0<φ,xn∈[−1,1]Circle chaotic map was proposed by Kolmogorov. This map describes a simplified model of the phase locked loop in electronics [44]. This map is formally defined [45]:(4)xn+1=xn+ϑ−(τ2π)sin(2πxn)mod(1),xn∈(0,1)Circle map generates chaotic sequences in (0,1). In Eq. (4), xn+1 is computed mod 1.Cubic map is one of the most commonly used maps in generating chaotic sequences in various applications like cryptography. This map is formally defined by:(5)xn+1=ρxn(1−xn2),xn∈(0,1)Cubic map generates chaotic sequences in (0,1) with ρ=2.59.Gauss map is also one of the very well known and commonly employed map in generating chaotic sequences in various applications like testing and image encryption. This map is formally defined by the following equation:(6)xn+1=0,xn=0xn+1=1xnmod(1),xn≠0Gauss map generates chaotic sequences in (0,1).Iterative chaotic map with infinite collapses (ICMIC) [43] is formally defined by the following equation:(7)xn+1=sinαxn,α∈(0,∞),xn∈(−1,1)ICMIC map generates chaotic sequences in (−1,1).Sinusodial map is formally defined by the following equation:(8)xn+1=sin(πxn),xn∈(0,1)Sinusodial map generates chaotic sequences in (0,1).Typical behaviors of two chaos variables (x1, x2) based on different chaotic maps with 200 iterations are shown in Fig. 1. Here the initial points of two chaos variables are: x1=0.152, x2=0.843. Fig. 1(a) is the result of logistic map (φ=4), Fig. 1(b) is the result of tent map, Fig. 1(c) is the result of Chebyshev map (φ=5), Fig. 1(d) is the result of circle map (ϑ1=ϑ2=0.5, τ1=τ2=5), Fig. 1(e) is the result of cubic map, Fig. 1(f) is the result of Gauss map, Fig. 1(g) is the result of ICMIC map (α=70), Fig. 1(h) is the result of sinusodial map, respectively. From Fig. 1, it can be observed that each chaos variables are randomly distributed in their certain bounds. Although 200 iterations are shown, chaos variables have good ergodic properties with the increase of iteration times. This characteristic is very useful for global search, and it is helpful for COA's global optimum. From Fig. 1, it can also be observed that the distribution or ergodic property of different chaotic maps are different. Therefore, the search patterns of different chaotic maps differs with each others in view of convergence rate, algorithm speed and accuracy [14,42,44].In the PCOA proposed in our former study [34,35], multiple chaotic maps are mapped onto one optimization variable simultaneously, and the search result is the best value of parallel multiple chaotic maps. In this way, the PCOA method searches from several different initial points and detracts the sensitivity of initial condition.Consider an optimization problem for nonlinear multi-modal function with boundary constraints as:(9)minP(x)=P(x1,x2,…,xn),xi∈[ai,bi].In the following, i=1, 2, …, n, which represents each optimization variable; j=1, 2, …, N, which represents each optimization variable mapped by multiple N chaotic maps simultaneously.The process of a kind of twice carrier wave PCOA approach is described as follows. The first is the raw searching in different chaotic trace, while the second is elaborate searching by continually reducing the searching space of variable optimized and enhancing the searching precision.Step1: Initialize the maximum iteration times S1 in the first carrier wave chaos search, random initial value of chaotic maps0<γij(0)<1.Step2: Set iteration times l=0, parallel optimumPj*=∞, and global optimum P*=∞.Step3: Map chaotic mapsγij(l)onto the variance range of the optimization variables by the following equation:(10)xij(l)=ai+γij(l)(bi−ai)wherex(l)=x11(l)x12(l)…x1N(l)x21(l)x22(l)…x2N(l)…………xn1(l)xn2(l)…xnN(l)Step4: Compute the objective function value for each decision variable and update the searching results. IfPj(x(l))≤Pj*, thenxj*=xj(l)and parallel optimumPj*=Pj(x(l)). IfPj*≤P*, then global optimumP*=Pj*, andx*=xj*. This means that the search result is the best value of parallel multiple chaotic maps.Step5: Generate next values of chaotic maps by a chaotic map function (M) as in Eqs. (1)–(8):(11)γij(l+1)=M(γij(l))Step6: If l≥S1 or∥Pj*−P*∥<PH, stop the first carrier wave search process; otherwise l⟵l+1, go to Step 3.Step1: Initialize the maximum iteration times S2 in the second carrier wave chaos search, set iteration times l′=0, random initial value of chaotic maps0<γij(l′)<1.Step2: Compute the second carrier wave by the following equation:(12)xij(l′)=xj*+λi(γij(l′)−0.5)Step3: Compute the objective function value for each decision variable and update the searching results. IfPj(x(l′))≤Pj*, then and parallel optimum. IfPj*≤P*, then global optimumP*=Pj*, andx*=xj*.Step4: Generate next values of chaotic maps by a chaotic map function (M) as in Eqs. (1)–(8):(13)γij(l′+1)=M(γij(l′))Step5: If l′≥S2 or∥Pj*−P*∥<PL, stop the second carrier wave search process; otherwise λi⟵tλi, go to Step 2.The λ is a very important parameter and adjusts small ergodic ranges aroundxj*, and t>1. It is difficult and heuristic to determine the appropriate value of λi, initial value of this parameter is usually set to 0.01(bi−ai) [44].Since the PCOA is an effective global search approach and it is not sensitive to its initial conditions, after many iteration times, parallel optimumPj*will converge to a close neighborhood, andPj*will be close to global optimum P*. Here∥Pj*−P*∥<PHor∥Pj*−P*∥<PLmeans whether the parallel search results of PCOA have reached a close neighborhood, and∥Pj*−P*∥<PHis computed as:(14)|P1*−P*|<PHand|P2*−P*|<PH…and|Pj*−P*|<PHIn the same manner,∥Pj*−P*∥<PLis similar to Eq. (14), and PHand PLare pre-set values.Due to the pseudo-randomness of chaotic motion, the motion step of chaotic maps between two successive iterations is always big, which resulted in the big jump of the decision variables in searching space. Thus, even if PCOA has reached the neighborhood of the global optimum, it needs to spend much computational effort to approach the optimum eventually by searching numerous points [4]. Then, although the above mentioned twice carrier wave PCOA has good capacity for global exploring, its accurate search ability is not enough. In order to improve the accurate local search of PCOA, a heuristic algorithm – harmony search algorithm (HSA) is employed here. The HSA can be conducted for accurate local search since numerical comparisons have demonstrated that the evolution in HSA is faster than genetic algorithm [39]. Consequently, by combining HSA with PCOA, a novel hybrid optimization algorithm can be developed with fine capability.The flowchart of hybrid PCOA with HSA is illustrated in Fig. 2. The proposed hybrid algorithm is a two stages search technique, the first stage is the twice carrier wave PCOA for global searching, and the second stage is the HSA for accurate searching. In the proposed algorithm, the PCOA is conducted until it has converged to a close neighborhood or it has reached its maximum iteration times, then HSA will be conducted for accurate solutions. The condition of switch from PCOA to HSA is: the second carrier wave parallel chaos search is completed (l′≥S2) or PCOA has converged to a close neighborhood (∥Pj*−P*∥<PL).As HSA is the successor of PCOA, so the parallel optimumPj*is the starting points of HSA, that is,xj*after PCOA search is the initial harmony memory (HM) for HSA. This section describes the HSA after PCOA search as follows:Step1: Initialize HSA parameters: maximum iteration times S3, harmony memory considering rate (HMCR), pitch adjusting rate (PAR), bandwidth vector (BW).Step2: Set iteration times l″=0, initialize the harmony memory (HM) from the PCOA search results.(15)HM1=x1*,HM2=x2*,…,HMj=xj*Step3: Improvise a new harmony from the HM.x′=(x′1,x′2,…,x′n)is improvised based on the following three mechanisms [36,37]: random selection, memory consideration, and pitch adjustment. In the random selection, the value of each decision variable, in the new harmony vector is randomly chosen within the value range with a probability of (1−HMCR). The HMCR, which varies between 0 and 1, is the rate of choosing one value from the historical values stored in the HM, and (1−HMCR) is the rate of randomly selecting one value from the possible range of values [38].(16)x′i=x′i∈{xi1,xi2,…,xiHMS}withprobabilityHMCRx′i=x′i∈xiwithprobability(1−HMCR)The value of each decision variable obtained by the memory consideration is examined to determine whether it should be pitch-adjusted. If the pitch adjustment decision forx′iis made with a probability of PAR,x′iis replaced withx′i±u(−1,+1)×BW, where BW is an arbitrary distance bandwidth for the continuous design variable, and u(−1, 1) is a uniform distribution between −1 and 1. The value of (1−PAR) sets the rate of performing nothing. Thus, pitch adjustment is applied to each variable as follows [37]:(17)x′i=x′i±u(−1,+1)×BWwithprobabilityHMCR×PARx′i=x′iwithprobabilityHMCR×(1−PAR)Step4: Update the HM according to the objective function value. With the evaluation of the objective function value, if the new harmony vector is better than the worst harmony vector in the HM, the new harmony vector is included in the HM, and the existing worst harmony vector is excluded from the HM.Step5: If∥xi*−x*∥<ξor l″≥S3, HSA search is terminated; otherwise l″←l″+1, Step 3 and Step 4 are repeated.The PAR and BW in HSA search are important parameters, and they are potentially useful in adjusting convergence rate to optimal solutions [39]. The traditional HSA method uses fixed value for both PAR and BW, and variable PAR and BW in [39] is employed:(18)PAR(l″)=PARmin+(PARmax−PARmin)S3·l″(19)BW(l″)=BWmax·expl″·ln(BWmin/BWmax)S3In this paper, large PAR value with small BW value are usually used for the improvement of best solutions in late generations, here PARmin=0.75, PARmax=0.99, BWmin=0.001, BWmax=0.1.The efficiency and performance of the proposed hybrid algorithm with the following six nonlinear functions [2,4,40] is evaluated:(20)f1(x,y)=4−2.1x2+x43x2+xy+(−4+4y2)y2,−200<x,y<200(21)f2(x,y)=0.5−sin2x2+y2−0.5(1+0.001(x2+y2))2,−200<x,y<200(22)f3(X)=∑i=13(xi2−10cos(2πxi)+10),−5<xi<5,i=1,2,3(23)f4(X)=14000∑i=130xi2−∏i=130cos(xii)+1,−5<xi<5,i=1,2,…,30(24)f5(X)=130∑i=130(xi4−16xi2−5xi),−10<xi<10,i=1,2,…,30(25)f6(X)=∑i=130((xi+1−xi2)2+(xi−1)2),−10<xi<10,i=1,2,…,30Function f1 is the Camel function as illustrated in Fig. 3(a), which has six local minima and two global minima x*=(−0.0898, 0.7126), x*=(0.0898, −0.7126), and optimal objective function value f*=−1.03162845. Function f2 is the Schaffer's function as illustrated in Fig. 3(b), which has infinite local maximum and one global maximum x*=(0, 0), and f*=1.0. Function f3 is the Rastrigin's function as illustrated in Fig. 3(c), which has many local minima and one global minimum x*=(0, 0, 0), and f*=0. Function f4 is the Griewank's function with 30 variables as illustrated in Fig. 3(d), which has several thousand local minima and one global minimum x*=(0, 0, …, 0), and f*=0. Function f5 has 30 variables as illustrated in Fig. 3(e), which has many local minima and one global minimum x*=(2.9051, 2.9051, …, 2.9051), and f*=−78.332314. Function f6 has 30 variables as illustrated in Fig. 3(f), which has several local minima and one global minimum x*=(1, 1, …, 1), and f*=0. These six nonlinear multi-modal functions are often used to test the convergence, efficiency and accuracy of optimization algorithm [2,4,40]. Among them the latter three functions have 30 variables, which is high dimensional problem.The parameters is the hybrid algorithm are: PL=0.1 for f2, f4 while PL=0.5 for f1, f3, f5 and f6, ξ=0.01 for f1−f3 while ξ=0.05 for f4−f6, different multiple chaos variables N is used, here Tent map in Eq. (2) is used as the chaotic mapping function M. Fig. 4shows a typical objective function values in the search procedure with respect to different parallel number N.The simulation results are shown in Fig. 5. The ‘best’ means the best objective function value in 20 times searching, the ‘rate’ means the success rate of reaching the optimum x* with the error less than 0.02. In can seen from Figs. 4 and 5 that, as the increase of parallel number N, the search speed a is faster and the convergence is better. When N is 15, the proposed hybrid algorithm can reach satisfied simulation results for these benchmarks tests functions. So the parallel number N of the proposed hybrid algorithm is usually no less than 15.The performance of the proposed hybrid algorithm using different chaotic maps are shown in Fig. 6, where N=15, while PLis as the same as the former. In can be seen form Fig. 6 that there is some little difference with respect to different chaotic maps, while the Tent map and Circle map are two of the best results.The proposed hybrid algorithm is also compared with other two optimization algorithm based on PCOA in [34,35]. With the similar conditions and parameters, the results of the optimum x* with the error less than 0.02 are shown in Fig. 7, where ‘PCOA+CCIC’ and ‘PCOA+SSM’ means the PCOA algorithms in [34,35], respectively, and ‘PCOA+HSA’ means the proposed hybrid algorithm in this paper. Fig. 7 indicates that the proposed hybrid algorithm have bigger success rate to search the accurate solutions. Fig. 8shows a typical objective function values in the search procedure for different optimization algorithms with N=15.Here the proposed hybrid algorithm with N=30 is also compared with the performances of four widely used evolutionary algorithm: particle swarm optimization algorithm (PSO) [46], covariance matrix adaptation evolution strategy (CMAES) [47], adaptive differential evolution algorithm (JDE) [48], and self-adaptive differential evolution algorithm (SADE) [49]. Owing to stochastic nature, evolutionary algorithms may arrive at better or worse solutions than solutions they have previously reached by chance during their search for new solutions to a problem. Because of such cases, it is beneficial to use statistical tools to compare the problem-solving success of one algorithm with that of another. The simple statistical parameters that can be derived from the results of an algorithm solving a specific numerical problem K times under different initial conditions – i.e. the mean solution (mean), the standard deviation of the mean solution (std) and the best solution (best). The simulation results for f1−f6 with 30 runs using these techniques are shown in Fig. 9, which has also verified the good performance of the proposed hybrid algorithm.In this section, simulation is performed to evaluate the performance of the hybrid algorithm for parameter identification of synchronous generator as in [35]. The nominal values of synchronous generator are: rated power – 176.471 MVA, rated active power – 156.25 MW, rated voltage – 14.4 kV, power factor – 0.85, efficiency – 98.58%, rated speed – 3000 rpm, frequency – 50Hz. The cost function is:minC(pˆ)=∑k=1n[(id−iˆd)2+(iq−iˆq)2+(δ−δˆ)2], where the current id, iqand the power angle δ are the measurable system outputs, the currentiˆd,iˆqand the power angleδˆare computed from the identified system model as in [35]. These variables are calculated using standard per unit values, and the learning data-set with size of n=400 are the same as in [35].The parameters of the proposed hybrid algorithm are chosen as: S1=1000, S2=800, PL=0.5, ξ=0.1, the number of decision variables n=11, the number of multiple chaotic maps N=20.The results in Fig. 10are the best results of each optimization algorithm repeated for 30 times, and the minimal cost function value of COA, PCOA+SSM, PCOA+CCIC, and PCOA+HSA are 5.917, 2.159, 2.087, 1.624, respectively. The results of the COA and the PCOA+SSM are mentioned in [35], and the PCOA+CCIC is just the algorithm in [34]. From the identification results in Fig. 10, it is seen that the relative error using the proposed PCOA+HSA algorithm is about 1%, while the relative error of both PCOA+SSM and PCOA+CCIC algorithm is bigger than 1%. In Fig. 10, the best identified value for each parameter is underlined, it shows that seven parameters among the 11 parameters derive the best value by the PCOA+HSA algorithm. From the minimal cost function value and the relative error in Fig. 10, we can know that the proposed hybrid optimization algorithm has superiority over other PCOA algorithms.

@&#CONCLUSIONS@&#
