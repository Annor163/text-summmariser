@&#MAIN-TITLE@&#
The Dai–Liao nonlinear conjugate gradient method with optimal parameter choices

@&#HIGHLIGHTS@&#
Solutions for an open problem in the conjugate gradient methods are discussed.A singular value study on the Dai–Liao conjugate gradient method is made.Two modified Dai–Liao conjugate gradient methods are suggested.Convergence analyses and numerical comparisons are made.

@&#KEYPHRASES@&#
Nonlinear programming,Large-scale optimization,Conjugate gradient algorithm,Singular value,Global convergence,

@&#ABSTRACT@&#
Minimizing two different upper bounds of the matrix which generates search directions of the nonlinear conjugate gradient method proposed by Dai and Liao, two modified conjugate gradient methods are proposed. Under proper conditions, it is briefly shown that the methods are globally convergent when the line search fulfills the strong Wolfe conditions. Numerical comparisons between the implementations of the proposed methods and the conjugate gradient methods proposed by Hager and Zhang, and Dai and Kou, are made on a set of unconstrained optimization test problems of the CUTEr collection. The results show the efficiency of the proposed methods in the sense of the performance profile introduced by Dolan and Moré.

@&#INTRODUCTION@&#
Conjugate gradient (CG) methods comprise a class of unconstrained optimization algorithms characterized by low memory requirements and strong global convergence properties (Dai et al., 1999) which made them popular for engineers and mathematicians engaged in solving large-scale problems in the following form:minx∈Rnf(x),wheref:Rn→Ris a smooth nonlinear function and its gradient is available. The iterative formula of a CG method is given by(1.1)x0∈Rn,xk+1=xk+sk,sk=αkdk,k=0,1,…,in whichαkis a steplength to be computed by a line search procedure anddkis the search direction defined by(1.2)d0=-g0,dk+1=-gk+1+βkdk,k=0,1,…,wheregk=∇f(xk)andβkis a scalar called the CG (update) parameter.The steplengthαkis usually chosen to satisfy certain line search conditions (Sun & Yuan, 2006). Among them, the so-called strong Wolfe conditions (Wolfe, 1969) have attracted special attention in the convergence analyses and the implementations of CG methods, requiring that(1.3)f(xk+αkdk)-f(xk)⩽δαk∇f(xk)Tdk,(1.4)|∇f(xk+αkdk)Tdk|⩽-σ∇f(xk)Tdk,where0<δ<σ<1.Different choices for the CG parameter lead to different CG methods (Hager & Zhang, 2006b). Based on an extended conjugacy condition, one of the essential CG methods has been proposed by Dai and Liao (2001) (DL), with the following CG parameter:(1.5)βkDL=gk+1TykdkTyk-tgk+1TskdkTyk,where t is a nonnegative parameter andyk=gk+1-gk. Note that ift=0, thenβkDLreduces to the CG parameter proposed by Hestenes and Stiefel (1952). Also, the CG parameter proposed by Hager and Zhang (2005) (HZ), i.e.(1.6)βkHZ=gk+1TykdkTyk-2‖yk‖2dkTykgk+1TdkdkTyk,can be viewed as an adaptive version of (1.5) corresponding tot=2‖yk‖2skTyk, where‖·‖denotes the Euclidean norm. Similarly, the CG parameter suggested by Dai and Kou (2013) (DK), i.e.(1.7)βk(τk)=gk+1TykdkTyk-τk+‖yk‖2skTyk-skTyk‖sk‖2gk+1TskdkTyk,in whichτkis a parameter corresponding to the scaling factor in the scaled memoryless BFGS method (Sun & Yuan, 2006), can be considered as another adaptive version of (1.5) corresponding tot=τk+‖yk‖2skTyk-skTyk‖sk‖2. In (Dai & Liao, 2001), it has been shown that a CG method in the form of (1.1) and (1.2) withβk=βkDLis globally convergent for uniformly convex functions.The approach of Dai and Liao has been paid special attention to by many researches. In several efforts, modified secant equations have been applied to make modifications on the DL method. For example, Yabe and Takano (2004) used the modified secant equation proposed by Zhang, Deng, and Chen (1999). Also, Zhou and Zhang (2006) applied the modified secant equation proposed by Li and Fukushima (2001). Li, Tang, and Wei (2007) used the modified secant equation proposed by Wei, Li, and Qi (2006). Ford, Narushima, and Yabe (2008) employed the multi-step quasi-Newton equations proposed by Ford and Moghrabi (1994). Babaie-Kafaki, Ghanbari, and Mahdavi-Amiri (2010) applied a revised form of the modified secant equation proposed by Zhang et al. (1999) and the modified secant equation proposed by Yuan (1991). Furthermore, in several other attempts, the modified versions ofβkDLsuggested in (Babaie-Kafaki et al., 2010; Ford et al., 2008; Li et al., 2007; Yabe & Takano, 2004; Zhou & Zhang, 2006) have been used to achieve descent CG methods. Examples include the studies made by Narushima and Yabe (2012), Sugiki, Narushima, and Yabe (2012), and Livieris and Pintelas (2012).Here, based on a singular value study on the DL method, two nonlinear CG methods are proposed. The remainder of this work is organized as follows. In Section 2, the methods are suggested and their global convergence analysis is discussed. In Section 3, they are numerically compared with the CG methods proposed by Hager and Zhang, and Dai and Kou, and comparative testing results are reported. Finally, conclusions are made in Section 4.Based on Perry’s point of view (Perry, 1976), it is notable that from (1.2) and (1.5), search directions of the DL method can be written as:(2.1)dk+1=-Qk+1gk+1,k=0,1,…,whereQk+1=I-skykTskTyk+tskskTskTyk.So, the DL method can be considered as a quasi-Newton method in which the inverse Hessian is approximated by the nonsymmetric matrixQk+1. SinceQk+1presents a rank-two update, its determinant can be computed by (Sun & Yuan, 2006, chap. 1)(2.2)det(Qk+1)=t‖sk‖2skTyk.Hence, ift>0and the line search guarantees thatskTyk≠0, thenQk+1is nonsingular.It is remarkable that numerical performance of the DL method is very dependent on the parameter t for which there is no any optimal choice (Andrei, 2011). Motivated by this, here, based on a singular value study, two upper bounds for the condition number of the matrixQk+1are obtained and then, two optimal values for the parameter t are suggested. Now, we briefly discuss the singular value decomposition (SVD). For a more detailed discussion, see (Watkins, 2002, chap. 4).Theorem 2.1Watkins, 2002, chap. 4(Geometric SVD Theorem) LetA∈Rn×mbe a nonzero matrix with rank r. Then,Rmhas an orthonormal basisv1,…,vm,Rnhas an orthonormal basisu1,…,un, and there existσ1⩾σ2⩾⋯⩾σr>0such thatAvi=σiui,i=1,2,…,r,0,i=r+1,…,m,,andATui=σivi,i=1,2,…,r,0,i=r+1,…,n.The scalars{σi}i=1rintroduced in Theorem 2.1 are called the singular values of A.Based on Theorem 2.1, for an arbitrary matrixA∈Rn×mwith rank r it can be seen that(2.3)‖A‖F2=σ12+⋯+σr2,where‖·‖Fstands for the Frobenius norm. Also, ifr=m=n, then(2.4)|det(A)|=σ1×σ2×⋯×σn.One essential factor which plays an important role in the sensitivity analysis of a numerical problem related to a matrix, is the matrix condition number. For an arbitrary nonsingular matrix A, the scalarκ(A)defined byκ(A)=‖A‖‖A-1‖,is called the condition number of A. The matrix A with a large condition number is called an ill-conditioned matrix since the computations related to this matrix are potentially very sensitive to changes in the other data. Here, the following theorem is needed.Theorem 2.2Watkins, 2002, chap. 4LetA∈Rn×nbe a nonsingular matrix with the singular valuesσ1⩾σ2⩾⋯⩾σn>0. Then(2.5)κ(A)=σ1σn.The condition numberκ(A)computed by (2.5) is called the spectral condition number.In our analysis, we need to find the singular values of the matrixQk+1. Hereafter, we assume thatskTyk>0, as guaranteed by the strong Wolfe conditions (1.3) and (1.4).Firstly, note that sinceskTyk≠0, there exists a set of mutually orthonormal vectors{uki}i=1n-2such thatskTuki=ykTuki=0,i=1,2,…,n-2,which leads toQk+1uki=Qk+1Tuki=uki,i=1,2,…,n-2.That is,Qk+1hasn-2singular values equal to 1. Next, we find the two remaining singular values ofQk+1namelyσk-andσk+.Since‖Qk+1‖F2=tr(Qk+1TQk+1), from (2.3) we get(2.6)σk-2+σk+2=t2‖sk‖4(skTyk)2+‖sk‖2‖yk‖2(skTyk)2.Also, from (2.2) and (2.4) we have(2.7)σk-σk+=t‖sk‖2skTyk.Now, from (2.6) and (2.7), after some algebraic manipulations we obtain the singular valuesσk-andσk+as follows:(2.8)σk±=12(t‖sk‖2+skTyk)2+‖sk‖2‖yk‖2-(skTyk)2skTyk±12(t‖sk‖2-skTyk)2+‖sk‖2‖yk‖2-(skTyk)2skTyk.The following lemmas are now immediate.Lemma 2.1For the singular valueσk-defined by(2.8), we have(2.9)σk-≤1.Note that ift‖sk‖2skTyk<1, then from (2.7) and sinceσk-⩽σk+, we haveσk-<1. On the other hand, ift‖sk‖2skTyk⩾1, then from (2.8) and Cauchy–Schwarz inequality we have(2.10)σk+⩾12t‖sk‖2+skTykskTyk+12t‖sk‖2-skTykskTyk=t‖sk‖2skTyk,which together with (2.7) yieldsσk-≤1. □For the singular valueσk+defined by(2.8), we have(2.11)σk+⩾1.As shown in the proof of Lemma 2.1, ift‖sk‖2skTyk⩾1, then from (2.10) we haveσk+⩾1. On the other hand, ift‖sk‖2skTyk<1, then from (2.8) and Cauchy–Schwarz inequality we haveσk+⩾12t‖sk‖2+skTykskTyk+12skTyk-t‖sk‖2skTyk=1,which completes the proof. □Now, from (2.9) and (2.11) we getκ(Qk+1)=σk+σk-.Next, we explain the effect of ill-conditioning ofQk+1on the iterative method (1.1) with the search directions (2.1).For a vectoru∈Rn, letfl(u)=(fl(u1),fl(u2),…,fl(un))Tbe a vector inRnin whichfl(ui),i=1,2,…,n, is the nearest floating point number toui. From (2.1), we can writefl(dk+1)=-Qk+1fl(gk+1),k=0,1,…,and consequently, we have(2.12)‖fl(dk+1)-dk+1‖=‖-Qk+1fl(gk+1)-gk+1‖⩽‖Qk+1‖‖fl(gk+1)-gk+1‖.Considering (2.2), ift≠0, thenQk+1is nonsingular and so, from (2.1) and (2.12) we have‖Qk+1‖‖fl(gk+1)-gk+1‖‖gk+1‖⩾‖fl(dk+1)-dk+1‖‖-Qk+1-1dk+1‖⩾‖fl(dk+1)-dk+1‖‖Qk+1-1‖‖dk+1‖,which leads to the following inequality between the relative errors ofdk+1andgk+1:(2.13)‖fl(dk+1)-dk+1‖‖dk+1‖⩽κ(Qk+1)‖fl(gk+1)-gk+1‖‖gk+1‖.Inequality (2.13) shows that if the matrixQk+1is ill-conditioned, then, even for small values of the relative error ofgk+1, the relative error ofdk+1may be large. In other words, whenκ(Qk+1)is large, system (2.1) is potentially very sensitive to perturbations ingk+1. Thus, in such situation numerical instability is probable in the iterative method (1.1).In what follows, in order to achieve more numerical stability, two optimal values for the parameter t are suggested by minimizing two upper bounds of the condition number of the matrixQk+1.Note that for arbitrary nonnegative constants a and b, we havea2+b2⩽a+b. Therefore, if we leta=t‖sk‖2+skTykandb=‖sk‖‖yk‖, then from (2.8) we have(2.14)σk+⩽(t‖sk‖2+skTyk)2+‖sk‖2‖yk‖2-(skTyk)2skTyk⩽(t‖sk‖2+skTyk)2+‖sk‖2‖yk‖2skTyk⩽1+t‖sk‖2skTyk+‖sk‖‖yk‖skTyk.Also, from (2.7) and (2.14) we haveσk-=1σk+t‖sk‖2skTyk⩾t‖sk‖2skTyk1+t‖sk‖2skTyk+‖sk‖‖yk‖skTyk.Thus,(2.15)κ(Qk+1)=σk+σk-⩽1+t‖sk‖2skTyk+‖sk‖‖yk‖skTyk2t‖sk‖2skTyk.So, if we define the functionh(t)as:(2.16)h(t)=‖yk‖skTyk+skTyk‖sk‖1t+‖sk‖skTykt,then inequality (2.15) ensures that the condition number of the matrixQk+1is bounded above byh2(t). Hence, the minimizer ofh(t), i.e.(2.17)tk1∗=skTyk‖sk‖2+‖yk‖‖sk‖,can be considered as an optimal value for the parameter t since it makes an upper bound ofκ(Qk+1)small to the extent possible.To propose another upper bound forκ(Qk+1), from (2.6) and (2.7) we have(2.18)κ(Qk+1)=σk+σk-=σk+2σk-σk+⩽σk+2+σk-2σk-σk+=t‖sk‖2skTyk+1t‖yk‖2skTyk.So, if we define the functiong(t)as:(2.19)g(t)=t‖sk‖2skTyk+1t‖yk‖2skTyk,then inequality (2.18) ensures that the condition number of the matrixQk+1is bounded above by the functiong(t). Hence, the minimizer ofg(t), i.e.(2.20)tk2∗=‖yk‖‖sk‖,can be considered as another optimal value for the parameter t since it makes an upper bound ofκ(Qk+1)small to the extent possible.Remark 2.2Assume that the level setL={x|f(x)⩽f(x0)}, withx0to be the starting point of the iterative method (1.1) and (1.2), is bounded and also, in a neighborhoodNofL,fis continuously differentiable and its gradient is Lipschitz continuous. Consider a CG method with the parameterβkDLdefined by (1.5) in which for allk⩾0,t=tk1∗ort=tk2∗, withtk1∗andtk2∗respectively defined by (2.17) and (2.20). It can be shown that there exists a positive constant B such that0⩽tk2∗<tk1∗⩽B. Hence, if the search directions are descent directions and the steplengths are determined to satisfy the strong Wolfe conditions (1.3) and (1.4), then Theorem 3.3 of (Dai & Liao, 2001) ensures the global convergence of the method for uniformly convex objective functions. Furthermore, if the CG parameter is modified as follows:βkDL+=maxgk+1TykdkTyk,0-tgk+1TskdkTyk,witht=tk1∗ort=tk2∗, for allk⩾0, and the search directions satisfy the sufficient descent condition, then Theorem 3.6 of (Dai & Liao, 2001) ensures the global convergence of the method for general objective functions.Here, we present some numerical results obtained by applying C++ implementations of the CG methods in the form of (1.1) and (1.2) in whichβk=βkDLdefined by (1.5) with the suggested two optimal choicest=tk1∗defined by (2.17) andt=tk2∗defined by (2.20), here respectively called M1 and M2, the HZ method with the CG parameter (1.6), and the DK method with the following optimal choice forτkin (1.7), suggested by Dai and Kou (2013):(3.1)τk=skTyk‖sk‖2.The results are extended by further study on the outputs obtained from the C++ implementations of the CG_Descent algorithm (Hager & Zhang, 2006a) which applies the following restricted form ofβkHZ:(3.2)β¯kHZ=maxβkHZ,-1‖dk‖min{η1,‖gk‖},whereη1is a positive constant, and another CG method proposed by Dai and Kou (2013), here called DK+, in which the CG parameter is computed by(3.3)βk+(τk)=maxβk(τk),η2gk+1Tdk‖dk‖2,whereη2∈[0,1)is a constant andβk(τk)is given by (1.7) with the optimal choice (3.1) forτk. Here, we setη1=0.01in (3.2) andη2=0.5in (3.3), as suggested in (Hager & Zhang, 2006a & Dai & Kou, 2013), respectively.The codes were run on a computer with 3.2gigahertz of CPU, 1gigabytes of RAM and Centos 6.2 server Linux operation system. Furthermore, the experiments were performed on a set of 145 unconstrained optimization test problems of the CUTEr collection (Gould, Orban, & Toint, 2003), with default dimensions as given in Hager’s home page: ‘http://www.math.ufl.edu/∼hager/’, in which the C++ code of CG_Descent version 5.3 is also available freely. The test problems data have been clarified in Table 1.For the methods of M1, M2, HZ, DK and DK+, similar to the CG_Descent algorithm, we used the approximate Wolfe conditions proposed by Hager and Zhang (2005) in the line search procedure, with the same parameter values as considered in CG_Descent version 5.3. Although the descent property may not always hold for the methods of M1 and M2, uphill search direction seldom occurred in our experiments; when encountering, we restarted the algorithm withdk=-gk. In addition, all attempts to solve the test problems were terminated when‖gk‖∞<10-6(1+|f(xk)|).The CG_Descent algorithm successfully solved all the test problems while the methods of M1, M2, HZ, DK and DK+ solved 144 out of 145 test problems. More exactly, these five methods failed to solve the VIBRBEAM function because of jamming, i.e. generating many short steps without making significant progress to the solution. Although these failures may cause the methods of M1, M2, HZ, DK and DK+ to seem slightly unpromising, it is interesting that for the VIBRBEAM function the CG_Descent algorithm achieved a solution with cost (objective function value) 5.19E+00 and precision (gradient infinite-norm) 9.47E−07, while the methods of M1, M2, HZ, DK and DK+ achieved five different solutions respectively with costs 4.63E+00, 2.72E+00, 1.75E+00, 1.56E−01, and 3.32E−01, and precisions 1.35E−05, 2.90E−05, 2.88E−05, 9.68E−05, and 1.20E−04. That is, with respect to the objective function value, for VIBRBEAM the methods of M1, M2, HZ, and in particular DK and DK+ achieved better solutions in contrast to the CG_Descent algorithm.Efficiency comparisons were made using the performance profile proposed by Dolan and Moré (2002), on the running time and the total number of function and gradient evaluations, here denoted byNTand defined byNT=Nf+3Ng, whereNfandNgrespectively denote the number of function and gradient evaluations. Performance profile gives, for everyω⩾1, the proportionp(ω)of the test problems that each considered algorithmic variant has a performance within a factor ofωof the best. Figs. 1–4show the comparisons results.As shown by Fig. 1, in the perspective of the total number of function and gradient evaluations, M1 outperforms M2, HZ and DK while M2 slightly outperforms HZ, and HZ is slightly preferable to DK. Also, with respect to the CPU time, Fig. 2 shows that M1 is preferable to M2 and these two methods are top performer versus the methods of HZ and DK which are approximately competitive. Furthermore, Fig. 3 shows that with respect to the total number of function and gradient evaluations, M1 is often preferable to M2, CG_Descent and DK+ while DK+ slightly outperforms CG_Descent which is often preferable to M2. Also, with respect to the CPU time, Fig. 4 shows that M1 outperforms M2 while M2 and DK+ are approximately competitive and these two methods are often preferable to CG_Descent.

@&#CONCLUSIONS@&#
Based on a singular value study on the matrix which generates the search directions of the Dai–Liao nonlinear conjugate gradient method, two modified conjugate gradient methods have been suggested. Global convergence of the methods has been briefly discussed. Numerical comparisons have been made between the implementations of the proposed methods, and the conjugate gradient methods proposed by Hager and Zhang, and Dai and Kou, on a set of 145 unconstrained optimization test problems of the CUTEr collection. The results showed the efficiency of the proposed methods in the sense of the performance profile introduced by Dolan and Moré.