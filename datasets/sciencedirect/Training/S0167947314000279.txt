@&#MAIN-TITLE@&#
A frame based shrinkage procedure for fast oscillating functions

@&#HIGHLIGHTS@&#
Totally automatic procedure for denoising fast oscillating functions.The procedure is valid for any frame operator.The procedure is a generalization of the empirical Wiener filter.

@&#KEYPHRASES@&#
Frames,Non-parametric regression,Rational dilatation wavelet transform,

@&#ABSTRACT@&#
In non-parametric regression analysis the advantage of frames with respect to classical orthonormal bases is that they can furnish an efficient representation of a more broad class of functions. For example, fast oscillating functions as audio, speech, sonar, radar, EEG and stock market are much more well represented by a frame, with similar oscillating characteristic, than by a classical orthonormal basis. In this respect, a new frame based shrinkage estimator is derived as the Empirical Regularized version of the optimal Shrinkage estimator generalized to the frame operator. An analytic expression of it is furnished leading to an efficient implementation. Results on standard and real test functions are shown.

@&#INTRODUCTION@&#
In the literature optimal results for non-parametric regression analysis of low frequency functions have been obtained in orthonormal basis, e.g. see Antoniadis et al. (2001) for a complete review on wavelet estimators; while, for more general functions and for overcomplete dictionaries literature is still growing in different directions: spatially adaptive methods as Hotz et al. (2012) and Portilla et al. (2003), Bayesian methods as Wolfe et al. (2004), variational methods as Candes and Tao (2007) and Chen et al. (1999), to cite only few. This paper proposes another possible direction. The problem is that of recovering a deterministic functionf∈Rn, from the set of its contaminated samplesxi=fi+ηi,i=1,…,n, withη∼N(0,σ2In), knowing thatfhas a fast oscillating behavior.When the noise varianceσ2is known, the non-parametric function estimation problem is addressed in two steps:•choice of the transformation which better separates the noise from the functionchoice of the function coefficients recovering method.Hereafterdiag(⋆)is the diagonal of matrix⋆and vectors are always vertical arrays. In the space of discrete functions of lengthn, a collection ofNvectorswi∈Rn,i=1,…,N, which together form matrixW∈RN×n, forms a frame if there exist two positive constantsAandBsuch that, for anyf∈Rnone has(1)A‖f‖2≤f∗W∗Wf≤B‖f‖2,whereW∗is the transpose ofW. The latter guarantees that eigenvalues of matrixW∗Ware bounded above and below and, therefore,W∗Wis invertible.N/nis the redundancy factor of the frame operator. If frame bounds are equal to each other and normalized,A=B=1, then the frame is called normalized tight. In this case the generalized Parseval’s identity holds andW∗W=In. See Christensen (2003) for more details. Consider the following problem of recovering vectorf∈Rnfrom its noisy equispaced observations(2)x=f+η,η∼N(0,σ2In).Apply frame transformWto both sides of Eq. (2),(3)y=θ+ϵ,ϵ∼N(0,σ2U)wherey=Wx,θ=Wf,ϵ=WηandU=WW∗∈RN×N. Process data frame coefficientsyto reduce noise obtaining vectorθˆand, subsequently, estimatefby(4)fˆ=(W∗W)−1W∗θˆ=W+θˆ,whereW+=(W∗W)−1W∗is the pseudo-inverse (Moore–Penrose) of matrixW. In the case of tight frameW+=W∗. In classical non-parametric regression analysis, reduction of noise is achieved by shrinking or thresholding the empirical frame coefficients. Among these procedures the best linear diagonal estimator is obtained by minimizing the Risk function (i.e. the mean-squared error of time domain data fit), which by the Gauss–Markov theorem is an optimal estimator when noise is Gaussian. The next statement provides its analytic expression. We assume that the vector of frame coefficientsθis estimated byθˆ=ΓywhereΓis a fixed diagonal matrix inRN×N.Theorem 1Let datayfollow model   (3), letWbe a tight frame and letfˆ=W∗Γybe an estimator off, then if vectorγ=diag(Γ)is data-independent, the minimum MSE is reached for(5)γ=(θθ∗∘U+σ2U∘U)−1(θθ∗∘U)eN,whereU=WW∗,eNis the vertical vector with all entries equal  1  and∘denotes the Hadamard (i.e. element-wise) matrix product.See De Canditiis et al. (2013) for proof. Note that, in the case of orthonormal basis,Wis a unitary matrix,U=I, there is no redundancy (N=n) and expression (5) reduces to the well known Wiener filter gain vector(6)γ=(θθ∗∘In+σ2In∘In)−1(θθ∗∘In)en,which element-wise isγi=θi2/(θi2+σ2), e.g. see Ghael et al. (1997).It is worth to observe that each elementγiof the shrinkage vector (5) is inversely proportional to the informative content (variance) of data coefficientyiand depends on a set ofθi-neighbor coefficients. Then, the estimator is an overlapping block shrinkage procedure where the length of the block is automatically decided by the correlation inherits from the frame operator. In De Canditiis et al. (2013) the advantage of considering shrinkage vector of type (5) instead of classical expression of type (6) is widely proved and explored. Finally, note that matrix(θθ∗∘U+σ2U∘U)is symmetric positive-semidefinite since it is the Hadamard product of two symmetric positive-semidefinite matrices (see Theorem (Schur) 3.1 in Styan (1973)).The shrinkage estimator proposed in (5) is an oracle estimator since it requires the knowledge of the true function. Hence, in real applications an empirical version of it is proposed. In particular, sinceE(yy∗)=θθ∗+σ2U, the matrixθθ∗̂=yy∗−σ2Uis an unbiased estimator of matrixθθ∗, then it can be plugged into expression (5) to obtain an empirical shrinkage estimator. Let us first deal with the case of orthonormal basis, thenθθ∗̂=yy∗−σ2Iis plugged into expression (6) obtaining the following expression:(7)γˆ=(yy∗∘In)−1(yy∗∘In−σ2In∘In)en,where the inversion of matrix(yy∗∘In)−1is obtained by a regularization procedure. In fact, only valuesyi2>σ2are used for the inversion of the diagonal matrixyy∗∘In; this is formally equivalent to use only the eigenvalues ofyy∗∘Inwhich are greater than eigenvalues ofσ2In∘In. The shrinkage regularization procedure is then expressed byγˆi=yi2−σ2yi2Ind(yi2>σ2)=(yi2−σ2)+yi2,whereIndis the indicator function. In engineering, when considering the Fourier orthonormal basis, this estimator is well known as spectral noise subtraction method, see Berouti et al. (1979).In the case of frame operator, plug expressionθθ∗̂=yy∗−σ2Uinto formula (5) and obtain the empirical shrinkage estimator(8)γˆ=(yy∗∘U)−1(yy∗∘U−σ2U∘U)eN,which coincides with the optimal shrinkage estimator obtained in De Canditiis et al. (2013) by a direct minimization of SURE (Stein Unbiased Risk Estimator). As it is, estimator (8) is ill-conditioned being matrix(yy∗∘U)condition number very high. For that reason the potentiality of this estimator has not been properly studied in De Canditiis et al. (2013) where its implementation was obtained solving the associated quadratic form with diagonal correction matrix10(−4.5)INsubject to the constrains0≤γi≤1. In this paper an alternative regularization of estimator (8) is proposed. Specifically, mimicking the spectral noise subtraction method, we propose to regularize estimator (8) by a spectral cut-off regularization procedure where inversion of matrixyy∗∘Uis obtained using only the eigenvaluesλiwhich are greater than eigenvalues of matrixσ2U∘U. Letyy∗∘U=VΛV∗andU∘U=QΔQbe spectral decompositions, then regularize expression (8) by(9)(yy∗∘U)+−1=VΛ+−1V∗,wherediag(Λ+−1)=(λ[1]−1,λ[2]−1,…,λ[M]−1,0,…,0), beingM=maxi{i:λ[i]>σ2δ[i]}whereλ[i]andδ[i]are the descending-sorted eigenvalues of respectively matrixyy∗∘UandU∘U. Let us note thatMis expected to beM≪Nbeing the Karhunen–Loeve decomposition of the empirical covariance matrix expected to be concentrate into a smaller dimensional space. Finally, it is straight to see how this regularization reduces to the classical one when an orthonormal basis is considered instead of frame, i.e. whenλi=yi2andδi=1.In order to give a closed form expression for the proposed estimator, the following proposition is proved.Proposition 1Let datayfollow model in   (3)   and letWbe a tight frame, then the following identity holds(10)(yy∗∘U−σ2U∘U)eN=y2−σ2diag(U),wherey2is the vertical vector with elementsyi2anddiag(U)is the vertical vector of diagonal entries ofU.ProofBy definition, since the frame is tight,W∗W=Inand∑j=1Nwjhwjk=δhk, whereδhk=0ifh≠kandδhk=1ifh=k, forh,k=1,…,n. The proposition is proven evaluating the two following identities: theith entry of vector(yy∗∘U)eNyi∑j=1Nyjuij=yi∑j=1N∑h=1nxhwjh∑k=1nwikwjk=yi∑h=1nxh∑k=1nwik∑j=1Nwjhwjk︸δhk=yi∑h=1nxh∑k=1nwikδhk=yi∑h=1nxhwih=yi2and theith entry of vector(U∘U)eN:∑j=1Nuij2=∑j=1N(∑k=1nwikwjk)2=∑j=1N(∑k=1nwik2wjk2+2∑k=1n∑h>knwikwjkwihwjh)=∑k=1n(wik2∑j=1Nwjk2+2∑j=1Nwikwjk∑h>knwihwjh)=∑k=1nwik2︸uii+∑k=1n∑h>kn∑j=1Nwjkwjh︸δhkwikwih.□Using regularized inverse matrix in (9) and Proposition 1 the analytic expression for the proposed shrinkage vector is(11)γˆ=VΛ+−1V∗(y2−σ2diag(U)).It is now clear why the proposed estimator here and after is refereed as ERS (Empirical Regularized Shrinkage) estimator.In this section the numerical algorithm is schematically presented. Given datax, frame correlation matrixUand spectrumδofU∘U,STEP Iobtain frame data coefficientyas in formula (3)evaluate•matrixyy∗∘Uits spectral decompositionvectorγˆby formula (11)θˆ=γˆ∘yobtain estimatorfˆby formula (4).From a practical point of view, when implementing the spectral decomposition step, in order to preserve the numerical positive-definiteness of matrixyy∗∘Uthe following diagonal correction matrixσ2INis added. This correction does not change the eigenmatrixVin Eq. (9), while it shifts the spectrumdiag(Λ)by a constant factorσ2, which is taken into account when selecting theMeigenvalues greater thanσ2δ.Let us comment on the computational cost of the proposed procedure. STEP I and III cost as a matrix multiplicationO(Nn), however, according to the frame choice, these steps can be obtained using fast analysis and synthesis algorithms respectively. In STEP II the most heavy computation is the spectral decomposition of a symmetricN×Nmatrix, which costsO(N3). However, according to the frame choice, matrixyy∗∘Ucan have a sparse structure which can be used to improve the computation of its spectral decomposition. It is also worthwhile to observe that ERS (11) has an analytic closed form expression which can be advantageous with respect to a non-quadratic minimization procedure as classically proposed in literature, see experimental results for details.

@&#CONCLUSIONS@&#
Actually the majority of frame based denoising procedures are carried out directly pursuing sparseness on the coefficients. Here a component reduction in the computation of SURE estimator is proposed, it shrinks coefficients without pursuing sparsity on them. Sparsity is implicitly used into the regularization step where only a small number of eigenvalues–vectors are considered. The proposed estimator can be competitive with some of the proposed alternative in literature. Future research is devoted to the improvement of the algorithm based on specific frame structure.