@&#MAIN-TITLE@&#
Orthonormal dictionary learning and its application to face recognition

@&#HIGHLIGHTS@&#
A discriminative orthonormal dictionary learning method is proposed for low-rank representation with fast computation.Two kinds of discriminative information is used for learning the dictionary.Experiments on face recognition from both image and video demonstrate the effectiveness of the proposed method.

@&#KEYPHRASES@&#
Orthonormal dictionary learning,Low-rank representation,Face recognition,

@&#ABSTRACT@&#
This paper presents an orthonormal dictionary learning method for low-rank representation. The orthonormal property encourages the dictionary atoms to be as dissimilar as possible, which is beneficial for reducing the ambiguities of representations and computation cost. To make the dictionary more discriminative, we enhance the ability of the class-specific dictionary to well represent samples from the associated class and suppress the ability of representing samples from other classes, and also enforce the representations that have small within-class scatter and big between-class scatter. The learned orthonormal dictionary is used to obtain low-rank representations with fast computation. The performances of face recognition demonstrate the effectiveness and efficiency of the method.

@&#INTRODUCTION@&#
Dictionary learning has received attentions in recent years owning to its wide range of applications, such as face recognition [1,2,3], visual tracking [4,5], image classification [3,6,7], and person re-identification [8,9]. The existing methods can be roughly divided into two categories: unsupervised [10,11] and supervised [2,12,13,14]. Dictionaries learned by supervised methods are more discriminative and obtain better performances in classification tasks. But most supervised methods failed to take the relationship between dictionary atoms into account, which might yield similar atoms. A redundant dictionary with lots of similar atoms will result in the ambiguity of representations and high computation cost, and Fig. 1(a) shows the representation ambiguity problem brought by similar dictionary atoms.In order to alleviate these problems, we present an orthonormal dictionary learning method. The dictionary is enforced to be orthonormal to eliminate similar atoms. Having a compact dictionary, both the ambiguities of representations and computation cost can be reduced. Our work is similar to [13] which achieved good performances on both classification and clustering tasks. Different from promoting incoherence between class-specific dictionaries in their work, our method endows the whole dictionary with the orthonormal property, which implies that all the atoms are as independent as possible whether they are in the same class or not. Fig. 1(b) shows the effect of the orthonormal property of the dictionary. Since the dimensionality of the feature vector is usually much larger than the number of dictionary atoms, forcing orthonormal constraint on the dictionary is feasible.To get a more discriminative dictionary, we push the class-specific dictionary to have good ability to well represent samples from the associated class and also suppress the ability of representing samples from other classes. [13,14,15,16] explicitly modeled the idea and optimized the class-specific dictionaries one by one, which may neglect the relationship between class-specific dictionaries during the optimization procedure. Different from these work, we propose a concise discriminative regularization term only restricting on the representation. In our method, all the class-specific dictionaries are obtained simultaneously, and the relationship between them can well hold. Moreover, we encourage the representations to have small within-class scatter and big between-class scatter, which is implemented by using the Fisher discriminative criterion. The discriminative power of the representation is able to propagate to the dictionary since they are coordinately solved in a unified optimization framework.Using the learned orthonormal dictionary, we are able to acquire low-rank representations with fast computation. Shu et al. [17] proved that the rank of the reconstruction data matrix is upper bounded by the number of non-zero rows of the representation matrix when the dictionary is orthonormal. According to this theorem, the nuclear norm in the low-rank representation can be replaced by its upper bound, and the representations can be obtained by the magnitude shrinkage function instead of the singular value shrinkage function which needs the SVD operation [18]. The solution procedure is much faster than solving the traditional low-rank representation problem [19,20].As an application of the proposed method, face recognition from both images and videos are conducted. In still image datasets, faces from the same individual vary slightly and are highly correlated. They are assumed to lie in a linear subspace, so the reconstruction data of the faces are expected to be low-rank. What's more, the orthonormal dictionary is beneficial for reducing the ambiguities of the representations, and thus able to represent all the faces in the dataset faithfully and discriminatively. The results of various experiments on Extended Yale B and AR datasets demonstrate the effectiveness and efficiency of the proposed method in recognizing faces from still images. Compared with face image recognition, the task of face recognition from videos is more difficult since the videos are acquired under non-ideal conditions where illuminations, poses, or expressions vary large. The proposed dictionary learning method aims to learn a discriminative and compact dictionary which is quite robust to the variations in videos. We conduct experiments on Honda/UCSD and YouTube Celebrities datasets and achieve comparable results with the state-of-the-art methods.The main contributions of this paper are as follows:•We present an orthonormal dictionary learning method which can learn a discriminative and compact dictionary for low-rank representation.The learned orthonormal dictionary is used to obtain low-rank representations with fast computation and comparable performances to the state-of-the-art methods.The proposed method achieves excellent performance on the application of face recognition from both still image and video datasets.The rest of this paper is organized as follows. We elaborate the formulation of the orthonormal dictionary learning method in Section 2, and detail the optimization and the initialization in Section 3. In Section 4, the fast low-rank representation method is presented. The experiments and discussions are in Section 5, and Section 6 concludes this paper.Given the observation sample matrixY∈RD×N, our goal is to learn the dictionaryD∈RD×K, the representation matrixX∈RK×N, and the noise matrixE∈RD×Nto satisfy Y=DX+E. A redundant dictionary have lots of similar atoms, which results in high computation cost and ambiguity in corresponding representations. In order to alleviate these problems, we introduce the orthonormal constraint of the dictionary into our model. The orthonormal dictionary learning is given by(1)minD,X,ELRDX+SNE+DCXLs.t.Y=DX+E,DΤD=I,where the functionLR⋅measures the low-rankness of the reconstruction data matrix DX, theSN⋅measures the sparsity of the noise matrix E, and theDC⋅is the discriminative term for improving the discriminative power of the learned dictionary. The L is the label matrix of the samples Y, and the I is the identity matrix.In classification task, training samples from the same class are highly correlated and expected to form a low-dimensionality subspace, so the training samples without noises, i.e. the reconstruction data matrix represented by DX, should be low-rank. We formulate the low-rank term as(2)rankDX=rankZ,where rank(Z) denotes the rank of the matrix Z. The minimization of Eq. (2) is an NP-hard problem and difficult to solve due to the discrete nature of the rank(⋅) function. Fortunately, Fazel [21] proved that the nuclear norm function ∥Z∥⁎ ( i.e. the sum of the singular values of Z) is the convex envelope of the rank function rank(Z) on the set of {Z|∥Z∥2<1}, and Candès et al. [22,23] proposed to minimize the nuclear norm function instead of the rank function. Accordingly, our low-rank term can be rewritten as ∥DX∥⁎. As demonstrated in [17], ∥DX∥⁎ is upper bounded by∥XΤ∥2,1=∑i=1K‍∥xi∥2=∑i=1K‍∑j=1N‍xij2under the constraint of DΤD=I, where xirepresents the i-th row of X, and xijdenotes the element in the i-th row and j-th column of X. Our low-rank term is finally given by(3)LRDX=∥XΤ∥2,1.Furthermore, the 2,1-norm can be interpreted as the group sparsity of face representations, which is demonstrated to be quite effective on the face recognition with complex variances in [24]. Eq. (3) can be minimized efficiently as described in Sec.3.1.Real-word data are often noisy or corrupted due to illumination variation, occlusion, and pixel corruption. The classifier trained with these data may overfit and the classification performance may degrade. Motivated by the low-rank recovery [17,22], the corrupted data matrix Y is decomposed into two parts: a low-rank component DX and a sparse noise component E to alleviate the problem. Here, we denote the noisy term as(4)SNE=∥E∥2,1.The ∥E∥2,1 is used since it encourages the sum of l2-norm of all columns in E to be zero, which reflects the assumption that some training samples are corrupted and the others are not. In addition, E=Y-DX measures the reconstruction error so the minimization ofSNEencourages the dictionary D to well represent the observation data.In order to enhance the discriminative power of the dictionaryD, we propose the discriminative regularization term ∥X⊙S∥F2 where ⊙ means the element-wise multiplication operator, ∥⋅∥Fdenotes the Frobenius norm of a matrix. TheS∈RK×Nis defined as(5)Sij=0,ifdiandyjbelongtothesameclass1,otherwise,where diis the i-th column of the dictionary D, andyjis the j-th column of the observation data matrix Y. This term enhances the ability of the class-specific dictionary to well represent samples from the associated class and suppress the ability of representing samples from other classes. In [13,14,15,16], the idea is explicitly modeled by imposing constraints on both D and X as Eq. (A.2) in the Appendix A. Proposition 1 shows the equivalence of the proposed term and the explicit modeling formulation. In the explicit modeling formulation, the class-specific dictionaries are optimized one by one, which may cause the loss of the relationship among class-specific dictionaries. By using our discriminative regularization term, all the class-specific dictionaries can be solved simultaneously.The most similar work is the ideal representation Q=U-S in [25], whereU∈RK×Nis a matrix whose elements are all 1. In their work, the code-ideal term represented by ∥X-Q∥F2 encourages X to be close to Q. However, it potentially means that all the atoms in a class-specific dictionary provide the same contributions, which may bring negative effects to the representations of samples. The malpractice is dislodged by using the element-wise operator in our discriminative regularization term.To further enhance the discriminative power of the dictionary D, we introduce the Fisher discrimination criterion tr(SW(X))/tr(SB(X)) where the tr(⋅) denotes the trace a square matrix, the SW(X) is the within-class scatter matrix, and the SB(X) is the between-class scatter matrix. For simplicity, the trace-difference form, tr(SW(X))-tr(SB(X)), is used in our method. As the trace form is not convex to X, a regularization term η∥X∥F2 is added where η is a trade-off parameter and set as 1 in our model. The Fisher criterion is rewritten as(6)trSWX−trSBX+∥X∥F2=∥XI1∥F2−∥XI2∥F2+∥X∥F2,where I1=I-IW,I2=IW-IB, and I is the identity matrix inRN×N, IWand IBare constant symmetric matrixes inRN×Ndefined as(7)IWij=1/Nt,ifxi,xj∈Xt0,otherwiseIBij=1/N.Accordingly, the I1 and I2 are symmetric matrixes inRN×Nand can be calculated by the label matrix L.Fig. 2shows the distributions of the representations learned by Eq. (1). As shown in the figure, the discriminative regularization term aims to let the class-specific dictionary has the ability to represent samples from the associated class and suppress the ability to represent samples from other classes. The Fisher criterion encourages samples from the same class are close to each other and distant from samples from other classes. They use the class label information of the training samples in different ways, and both of them are beneficial for propagating the discriminative power of the representations to the dictionary. The overall discriminative term is(8)DCXL=∥XI1∥F2−∥XI2∥F2+∥X∥F2+∥X⊙S∥F2.Incorporating Eqs. (3), (4), and (8) into Eq. (1), we formulate the discriminative orthonormal dictionary learning model as(9)minD,X,E∥XΤ∥2,1+λ1∥E∥2,1+λ2∥XI1∥F2−∥XI2∥F2+∥X∥F2+λ3∥X⊙S∥F2,s.t.Y=DX+E,DΤD=Iwhere λ1, λ2, and λ3 are the parameters.In order to optimize Eq. (9), an auxiliary variable H is introduced, and the optimization problem is rewritten as(10)minD,X,E,H∥XΤ∥2,1+λ1∥E∥2,1+λ2∥HI1∥F2−∥HI2∥F2+∥H∥F2+λ3∥H⊙S∥F2s.t.Y=DX+E,DΤD=I,X=H.The augmented Lagrangian function of Eq. (10) is(11)LDXEHAB=∥XΤ∥2,1+λ1∥E∥2,1+λ2∥HI1∥F2−∥HI2∥F2+∥H∥F2+λ3∥H⊙S∥F2+<A,Y−DX−E>+<B,X−H>+<C,DΤD−I>+μ2∥Y−DX−E∥F2+∥X−H∥F2+∥DΤD−I∥F2where <A,B>=tr(ABΤ) denotes the trace of ABΤ, A and B are Lagrange multipliers, and μ is the positive penalty parameter. The linearized alternating direction method with adaptive penalty (LADMAP) [19] is used to solve Eq. (11) with the following iterative four steps:1.The dictionary D is updated with the fixed X(i),H(i), and E(i).The representation X, auxiliary variable H, and the noise matrix E are updated with other variables keeping fixed.The Lagrange multipliers are updated as A(i+1)=A(i)+μ(i)(Y-D(i+1)X(i+1)-E(i+1)) ,B(i+1)=B(i)+μ(i)(X(i+1)-H), and C(i+1)=C(i)+μ(i)(D(i+1)ΤD-I), and the penalty parameter μ is updated as μ(i+1)=γμ(i), where γ>1 is the magnified factor.Specifically, with fixed X, H, and E, Eq. (10) can be rewritten as(12)Di+1=argminD∥Y−DXi−Ei+Aiμi∥F2+∥DΤD−I+Ciμi∥F2.The Eq. (12) is solved by gradient descend where D is updated according to its gradient:(13)∇D=2DXiXiΤ+2DDΤ−2I−Ci+CiΤ/μi−2Y−Ei+Ai/μiXiΤ.When updating X, the quadratic term of X is replaced by its first order Taylor approximation at the previous iteration step X(i). The representation X is updated by solving(14)Xi+1=argminX∥XΤ∥2,1+μiη2∥X−F∥F2,where F=(1-2/η)X(i)+(DΤ(Y-E(i)+A(i)/μ(i))+H(i)-B(i)/μ(i))/η, and η>0 is a parameter. As shown in [20], X is updated as(15)xji+1=S1/μiηfj,whereSεvis a magnitude shrinkage function for vector v defined asSεv=max1-ε/∥v∥20v, xjand fjrepresent the j-th row of X and F, respectively.Keeping other variables fixed, the auxiliary variable is updated by solving(16)Hi+1=argminHλ2∥HI1∥F2−∥HI2∥F2+∥H∥F2+λ3∥Hi+1⊙S∥F2+μi2∥Xi+1−H+Biμi∥F2Setting the derivation of Eq. (16) with respect to hjto zero, we have(17)hji+1=μixji+1+bjiM−1,M=2λ2I12−I22+I+λ3diagsj+μiIwhere I is the identity matrix inRN×N, sj, hj, and bjrepresent the j-th row of S, H, and B, respectively.When updating the reconstruction error E, Eq. (10) can be rewritten as(18)Ei+1=argminEλ1∥E∥2,1+μi2∥E−N∥F2,where N=Y-D(i+1)X(i+1)+A(i)/μ(i). Similar to X, the E is updated as(19)eji+1=Sλ1/μnj,where ejand njare the j-th column of E and N, respectively.The randomly selected training samples of the i-th class are used to initialize each class-specific dictionary Di, and the whole dictionary is obtained by combing all the sub-dictionaries as D(0)=[D1,D2,...,DC]. After the orthonormalization of the initialized dictionary D(0), the representations X(0) is initialized by the orthogonal matching pursuit [26]. The error E(0) and the auxiliary variable H(0) are initialized as zeros.Using the learned orthonormal discriminative dictionary D, the representations of training and testing samples are computed by(20)minX,E∥XΤ∥2,1+λ1∥E∥2,1s.t.Y=DX+E,where λ1 is the same as in Eq. (9). The alternating direction method (ADM) [19] is used to optimize Eq. (20), and the augmented Lagrange function is(21)L˜=∥XΤ∥2,1+λ1∥E∥2,1+θ2∥Y−DX−E+Jθ∥F2,where J is the Lagrange multiplier and θ is the positive penalty parameter. Similar to updating the noise matrix in Section 3.1, both X and E can be updated easily with the help ofSεv. The algorithm of the fast low-rank representation is summarized in Algorithm 1.The ridge regression model is used to obtain a linear classifier from the training representations X:(22)minW∥L−WX∥F2+ζ∥W∥F2,where ζ is the hyper-parameter,L∈RC×Nis the label matrix of X, and each column [0,...,0,1,0,...0]Τdescribes the label of a sample. The optimal solution of Eq. (22) is W⁎=LXΤ(LXΤ+ζI)-1. With the optimized W⁎, a testing sample x can be predicted by picking the index of the maximum element of W⁎x.Algorithm 1Algorithm of fast low-rank representationImage 1

@&#CONCLUSIONS@&#
