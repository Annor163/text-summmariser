@&#MAIN-TITLE@&#
A method based on PSO and granular computing of linguistic information to solve group decision making problems defined in heterogeneous contexts

@&#HIGHLIGHTS@&#
Information granulation of linguistic information used in group decision making.Granular Computing is used to made operational the linguistic information.Linguistic information expressed in terms of information granules defined as sets.The granulation of the linguistic terms is formulated as an optimization problem.The distribution and semantics of the linguistic terms are not assumed a priori.

@&#KEYPHRASES@&#
Group decision making,Information granules,Consistency,Granular computing,Linguistic information,

@&#ABSTRACT@&#
Group decision making is a type of decision problem in which multiple experts acting collectively, analyze problems, evaluate alternatives, and select a solution from a collection of alternatives. As the natural language is the standard representation of those concepts that humans use for communication, it seems natural that they use words (linguistic terms) instead of numerical values to provide their opinions. However, while linguistic information is readily available, it is not operational and thus it has to be made usable though expressing it in terms of information granules. To do so, Granular Computing, which has emerged as a unified and coherent framework of designing, processing, and interpretation of information granules, can be used. The aim of this paper is to present an information granulation of the linguistic information used in group decision making problems defined in heterogeneous contexts, i.e., where the experts have associated importance degrees reflecting their ability to handle the problem. The granulation of the linguistic terms is formulated as an optimization problem, solved by using the particle swarm optimization, in which a performance index is maximized by a suitable mapping of the linguistic terms on information granules formalized as sets. This performance index is expressed as a weighted aggregation of the individual consistency achieved by each expert.

@&#INTRODUCTION@&#
A Group Decision Making (GDM) problem is defined as a decision problem in which several experts provide their judgments over a set of alternatives. The aim is to reconcile differences of opinions expressed by individual experts to find an alternative (or set of alternatives), which is best acceptable by the group of experts as a whole.Since the process of decision making, in particular of group type, is centered on humans, coming with their inherent subjectivity, imprecision and vagueness in the articulation of opinions, the theory of fuzzy sets, introduced by Zadeh (1965), has delivered new tools in this field for a long time, as it is a more adequate tool to represent often not clear-cut human preferences encountered in most practical cases. As the information provided by the humans is inherently non-numeric, partial evaluations, preferences, judgments, and weights are usually expressed linguistically (Herrera et al., 2009; Montero, 2009). The use of words or sentences rather than numbers is, in general, less specific, more flexible, direct, realistic, and adequate form to express the qualitative aspects of the problem at hand. In such a case, when linguistic information is used, it needs to be made operational in some way.To effectively operate on linguistic information in GDM problems, several linguistic computation models have been proposed in the literature (Herrera et al., 2009): (i) the linguistic computational model based on membership functions (Degani and Bortolan, 1988; Fu, 2008); (ii) the linguistic computational model based on type-2 fuzzy sets (Mendel, 2002); the linguistic symbolic computational models based on ordinal scales (Delgado et al., 1993; Herrera et al., 1996, 1997); and (iv) the 2-tuple linguistic computational model (Herrera and Martinez, 2000), which is a symbolic model extending the use of indexes. Different methods for solving GDM problems using the above linguistic computation models have been developed (see, for example, Cabrerizo et al. (2009, 2010), Fu (2008), Herrera et al. (1996, 1997), and Mata et al. (2009)). In all of them, both the distribution and the semantics of the linguistic terms are established a priori.Granular Computing is an emerging paradigm of information processing (Pedrycz, 2011). It deals with representing and processing of information in form of information granules, which are complex information entities that arise in the process (called information granulation) of abstraction of data and derivation of knowledge from information (Bargiela and Pedrycz, 2003). The process of information granulation and the nature of information granules imply the definition of a formalism that is well-suited to represent the problem at hand. The resulting information granules are afterwards effectively processed within the computing setting pertinent to the assumed framework of information granulation. There are a number of formal frameworks in which information granules can be defined such as sets (Bargiela, 2001) (interval mathematics), fuzzy sets (Zadeh, 1965, 1975a,b,c), rough sets (Slowinski et al., 2002; Greco et al., 2006, 2011), shadowed sets (Pedrycz, 2009), and probabilities (Zadeh, 2002) (probability density functions).Linguistic information may be made operational through information granulation. However, it is not clear both how the linguistic terms have to be translated into the entities and what optimization criterion can be envisioned when arriving at the formalization of the linguistic terms through information granules.When information is provided by individuals, an important issue to bear in mind is that of consistency (Alonso et al., 2008; Chiclana et al., 2008; Herrera-Viedma et al., 2007), which may be used as an optimization criterion. Preference relations are the most common representation of information used for solving GDM problems due to effectiveness in modeling decision processes. The effort to complete pairwise evaluations is far more manageable in comparison to any experimental overhead we need when assigning membership grades to all alternatives of the universe in a single step, which implies that the expert must be able to evaluate each alternative against all the others as a whole, which can be a difficult task. The pairwise comparison helps the expert focus only on two elements once at a time thus reducing uncertainty and hesitation while leading to the higher of consistency. It is obvious that consistent information, which does not imply any kind of contradiction, is more relevant or important than the information containing some contradictions. However, due to the complexity of most GDM problems, experts’ preferences can be inconsistent. Fortunately, the lack of consistency can be quantified and monitored (Cutello and Montero, 1994; Herrera et al., 1997; Herrera-Viedma et al., 2004).The objective of this paper is to develop a new method based on granular computing to solve GDM problems when linguistic information is used. In particular, we focus on GDM situations defined in heterogeneous contexts, that is, situations where the experts have different background and level of knowledge about the problem and, then, importance degrees are provided or associated to them in order to reflect their importance to solve the problem. The novelty of the proposed method is that the distribution and the semantics of the linguistic terms are not assumed a priori. Here, the information granulation offers an operational model of the GDM problem to be used in presence of linguistic information. This information granulation is formulated as an optimization problem in which a performance index, based on experts’ consistency, is optimized by a suitable mapping of the linguistic terms on information granules. The Particle Swarm Optimization (PSO) (Kennedy and Eberhart, 1995) is utilized as an optimization framework, supporting the formation of the information granules. It helps translate linguistic terms into meaningful information granules so that the highest performance index is achieved. We should point out that in this study the granulation formalism being considered concerns intervals (sets). However, it applies equally well to any other formal scheme of information granulation.The paper is set out as follows. In Section 2, we present both the GDM scenario considered in this paper and the method to obtain the consistency achieved by an expert when expressing his/her opinion using preference relations. Section 3 is concerned with information granulation of the linguistic information present in GDM problems in heterogeneous contexts, and its optimization using the PSO framework. To illustrate the method, some experimental studies are shown in Section 4. A discussion among the proposed method and the classical linguistic computation models is carried out in Section 5. Finally, we offer some conclusions and future works in Section 6.In this section, we describe the GDM scenario which is considered in this study. One the one hand, we show both the main characteristics of the GDM problems and the steps which are faced to solve them. On the other hand, we define the concept of consistency and show how it can be calculated.In a classical GDM situation, there is a problem to solve, a solution set of possible alternatives, X={x1,x2,…,xn}, (n⩾ 2) and a group of two or more experts, E={e1,e2,…,em}, (m⩾2), who express their opinions about the set of alternatives and their suitability to achieve a common solution (Greco et al., 2012; Kacprzyk, 1986; Liu et al., 2012). In a fuzzy context, the objective is to classify the alternatives from best to worst, associating with them some degrees of preference expressed in the [0,1] interval.In many cases, it is assumed that to each expert assigned is an importance degree which reflects his/her importance level or knowledge degree about the problem, and in such a case we work in a heterogeneous or non-homogeneous GDM framework (Chiclana et al., 2007; Montero, 1988). This importance degree is interpreted as a fuzzy subset, I, with a membership function, μI:E→D, in such a way that μI(el)∈D denotes the importance degree of the opinion provided by the expert el, and D is the representation domain of the importance degrees. If D is of linguistic nature, then linguistic terms as “Very Important”, “Important”, “Less Important”, could be used.There are several different preference representation formats that can be used by experts to express their opinions (Herrera-Viedma et al., 2002). Among them, preference relations are one of the commonly used because experts have much more freedom when expressing their preferences and they can gain in expressivity. Different types of preference relations can be used according to the domain studied to evaluate the intensity of the preference. This is expressed in the following definition:Definition 1A preference relation P on a set of alternatives X is characterized by a function μP:X×X→D, where D is the domain of representation of preference degrees.When cardinality of P is low, the preference relation may be conveniently represented by the n×n matrix P=(pik), being pik=μP(xi,xk) (∀i,k∈{1,…,n}) interpreted as the preference degree or intensity of the alternative xiover xk. In this case, if D is a linguistic domain, then linguistic terms as “High”, “Medium”, “Low”, could be used.Usually, to solve a GDM problem, two steps are considered (Herrera-Viedma et al., 2007):•Aggregation phase. The aggregation step of a GDM problem consists in combining the experts’ individual preferences into a group collective one in such a way that it summarizes or reflects the properties contained in all the individual preferences. The collective preference,Pc=pikc, is computed by means of the aggregation of all individual preference relations,{P1,P2,…,Pm}:pikc=Φpik1,pik2,…,pikm, with, Φ, being an appropriate aggregation operator (Yager and Kacprzyk, 1997). The general procedure for the inclusion of importance weight values in the aggregation process involves the transformation on the preference values,pikl, under the importance degree, μI(el), to generate a new value,p¯ikl, and then aggregate these new values using the aggregation operator. According to the particular properties to consider, different methods for including importance degrees in the aggregation operators can be used (Chiclana et al., 2007).Exploitation phase. This final step uses the information produced in the aggregation phase to identify the solution set of alternatives. So, we must apply some mechanism to obtain a partial order of the alternatives and thus select the best alternative(s). There are several ways to do this. A usual one is to associate a certain utility value to each alternative (based on the aggregated information), thus producing a natural order of the alternatives. For example, the quantifier guided dominance degree (Herrera-Viedma et al., 2007),QGDDi=ϕpi1c,…,pi(i-1)c,pi(i+1)c,…,pinc, can be used, with, Φ, taken as an appropriate aggregation operator.Definition 1 dealing with a preference relation does not imply any kind of consistency property. However, the study of consistency is crucial for avoiding misleading solutions in GDM.Coherence in preference modeling has been introduced in standard decision making frameworks, taking many different formulations in each context, as a need in order to assure consistent decision making procedures (Garcı́a-Lapresta and Montero, 2006). In the classical numeric (Boolean) context, preferences use to be assumed to be transitive in order to assure consistent behavior. In the fuzzy framework, transitivity plays a crucial role in coherence modeling, since crisp behavior should appear as a particular case. Hence, crisp transitivity has been generalized into fuzzy preference modeling, existing a great variety of fuzzy transitivity properties, each one offering a different consistency assumption (Herrera-Viedma et al., 2004). Alternatively, consistency has been understood by Cutello and Montero (1994) as a rationality measure, therefore allowing degrees of performance. A key argument was that most standard fuzzy transitivity conditions in literature were crisp in nature, i.e., they either hold or not hold. But it is apparent that some situations are extremely intransitive while sometimes we only find small or unexpected transitivity violations that can be in some way bypassed in practice. Consistency in most cases allows different degrees, and it should be measured. The axiomatic approach of Cutello and Montero (1994) was a first proposal in this direction, proposing a particular family of conditions any rationality measure should verify within preference modeling.As the granulation formalism being considered in this study to represent the linguistic information concerns intervals in the unit interval [0,1], we study the fuzzy preference relations and how to characterize their consistency.Definition 2A fuzzy preference relation P on a set of alternatives X is a fuzzy set on the product set X×X, which is characterized by a membership function μP:X×X→[0,1].Every value pikin the matrix P represents the preference degree or intensity of preference of the alternative xiover xk: pik=0.5 indicates indifference between xiand xk(xi∼xk), pik=1 indicates that xiis absolutely preferred to xk, and pik>0.5 indicates that xiis preferred to xk(xi≻xk). Based on this interpretation we have that pii=0.5 ∀i∈{1,…,n} (xi∼xi). Since pii’s (as well as the corresponding elements on the main diagonal in some other matrices) do not matter, we will write them as ‘–’ instead of 0.5 (Herrera-Viedma et al., 2007; Kacprzyk, 1986). Moreover, it is assumed that the matrix is reciprocal, that is pik+pki=1 ∀i, k∈{1,…,n}.To make a rational choice, properties to be satisfied by such fuzzy preference relations have been suggested (Herrera-Viedma et al., 2004). In this paper, we make use of the additive transitivity property which facilitates the verification of consistency in the case of fuzzy preference relations. As it is shown in Herrera-Viedma et al. (2004), additive transitivity for fuzzy preference relations can be seen as the parallel concept of Saaty’s consistency property for multiplicative preference relations (Saaty, 1994). The mathematical formulation of the additive transitivity was given by Tanino (1984):(1)(pij-0.5)+(pjk-0.5)=(pik-0.5),∀i,j,k∈{1,…,n}.Because the additive transitivity implies additive reciprocity (pij+pji=1,∀i,j), it can be rewritten as:(2)pik=pij+pjk-0.5,∀∈i,j,k{1,…,n}.A fuzzy preference relation is considered to be “additive consistent” when for every three options encountered in the problem, say xi, xj, xk∈X their associated preference degrees pij,pjk, pikfulfil Eq. (2).Given a reciprocal fuzzy preference relation, Eq. (2) can be used to calculate an estimated value of a preference degree using other preference degrees. Indeed, using an intermediate alternative xj, the following estimated value of pik(i≠k) is obtained (Chiclana et al., 2008; Herrera-Viedma et al., 2007, 2004):(3)epikj=pij+pjk-0.5.The overall estimated value epikof pikis obtained as the average of all possible valuesepikj:(4)epik=∑j=1;j≠i,knepikjn-2.The value ∣epik−pik∣ can be used as a measure of the error between a preference value and its estimated one (Herrera-Viedma et al., 2007).When information provided is completely consistent thenepikj=pik∀j. However, because experts are not always fully consistent, the assessment made by an expert may not verify (2) and some of the estimated preference degree valuesepikjmay not belong to the unit interval [0,1]. From Eq. (3), it is noted that the maximum value of any of the preference degreesepikjis 1.5 while the minimum one is −0.5. In order to normalize the expression domains in the decision model, the final estimated value of pik(i≠k), cpik, is defined as the median of the values 0, 1 and epik:(5)cpik=med{0,1,epik}.The error assuming values in [0,1] between a preference value, pik, and its final estimated one, cpik, is:(6)εpik=|cpik-pik|.Reciprocity of P=(pik) implies reciprocity of CP=(cpik), therefore εpik=εpki. εpik=0 is interpreted as a situation of total consistency between pik(pki) and the rest of entries of P. Obviously, the higher the value of εpikthe more inconsistent is pik(pki) with respect to the remaining entries of P.This interpretation allows us to evaluate the consistency degree associated to a reciprocal fuzzy preference relation P as follows (Chiclana et al., 2008):(7)cd=∑i,k=1;i≠kn(1-εpik)n2-nWhen cd=1, the reciprocal fuzzy preference relation P is fully consistent, otherwise, the lower cd the more inconsistent P is.In this section, we elaborate on the quantification of linguistic terms present in the reciprocal preference relations provided by the experts in a heterogeneous GDM context. The granulation process of the linguistic terms leads to the operational realization of further processing forming a ranking of alternatives according to the preferences provided by the group of experts. Furthermore, the optimization of the granulation process using PSO as the optimization framework is described. This information granulation is similar to the clustering which can be formulated as an optimization problem and forms one of the most visible conceptual and algorithmic framework of developing information granules (Pedrycz and Bargiela, 2012). In such a way, our method is in some way similar to the clustering of semantically distinct variables proposed in (Bargiela and Pedrycz, 2002; Pedrycz and Bargiela, 2010, 2012). However, in the method proposed here, the information granules (clusters) are represented as sets and they are formed according to the optimization of a performance index based on experts’ consistency.The linguistic terms used in a pairwise comparison of alternatives in a preference relation are expressed linguistically by admitting qualitative terms. For example, as aforementioned, linguistic terms such as “High”, and “Medium”, are in common usage. As there is some apparent linear order among linguistic terms, they can be organized in a linear fashion. However, the linguistic terms themselves are not operational meaning that no further processing can be realized, which involves a quantification of the linguistic terms.The problem of a granular representation or description of linguistic terms is concerned with the formation of a family of information granules over the unit interval. In this study, we consider an interval format of information granulation meaning that the information granules come in the form of intervals [ak,ak+1], that is to say, information granules L1, L2,…,Lcwhere L1=[0,a1), L2=[a1,a2),…,Li=[ai−1,ai),…,Lc=[ac−1,1]. The above intervals form a partition of the unit interval where 0<a1<,…,<ac−1<1. The interval format of granulation of the unit interval is fully characterized by the vector of cutoff points of the granular transformation in the unit interval, a=[a1a2,…,ac−1].The process of arriving at the operational realization of three linguistic terms with the aid of sets (intervals) is shown in Fig. 1.The two important features of such granulation mechanisms are worth noting here: (i) the mapping is by no means linear, that is, a localization of the associated information granules on the scale is not uniform; and (ii) the semantics of the terms allocated in the process of granulation is retained.It is worth stressing that in this study we consider a joint treatment of the linguistic terms coming from the experts engaged in the process of GDM. It allows us to deal with these terms in a unified fashion and reconcile their semantics so that the individual consistencies are made comparable and thus could be aggregated to arrive at the joint view at the performance index. A granulation of the linguistic terms realized at the level of individual experts involved in the GDM may result in results of individual consistencies that are more difficult to aggregate and compare. Furthermore, linguistic terms are also used to indicate the experts’ importance, and both the number and the semantics of the terms can be different from the linguistic terms used by the experts to provide their opinions. Therefore, we consider two different linguistic term sets, one to provide experts’ preferences, and the other one to express the experts’ importance.The question on how to arrive at the operational version of the information granules specified as intervals can be reformulated as a certain optimization problem. In the following, the optimization criterion used in the optimization problem is defined.The formulation of the optimization problem needs to be now specified so that all technical details are addressed. First, the optimization criterion which has to be optimized needs to be defined.As the lack of consistency in the decision making process can lead to a wrong solution, the consistency of the preference relations provided by the experts can be used to obtain the quality of the solution obtained in a GDM problem. If the consistency level of each preference relation is high, the solution obtained will be better than if the consistency level is low. Therefore, for a given vector of cutoff points, we compute its quality by means of a performance index obtained as the weighted aggregation of the consistency levels measured for all preference relations {P1,…,Pm}. Then, the goal is to increase that performance index, which is used as optimization criterion.In light of the form of the optimization criterion, we can consider alternatives such as genetic algorithms or PSO to optimize it. In comparison with genetical algorithms, PSO is especially attractive given its less significant computing overhead (Pedrycz et al., 2012). In addition, this population-based method offers a significant level of diversity of possible objective functions, which play a role of fitness functions.PSO algorithm is a population based stochastic optimization technique inspired by bird flocking and fish schooling originally designed and introduced by Kennedy and Eberhart (1995). It is based on communication and interaction between the members of the swarm, what means that each member of the PSO algorithm, named as particle, determines its position by combining the history of its own best location with those of others members of the swarm.The algorithm flow of the PSO starts with a population of particles whose positions are the potential solutions of the problem, and the velocities are randomly initialized in the problem search space. In each iteration/generation, the search for optimal position (solution) is performed by updating the particle’s velocities and positions based on a predefined fitness function. The velocity of each particle is updated using two best positions, namely personal best position and neighborhood best position. The personal best position is the best position the particle has visited and neighborhood best position is the best position the particle and its neighbors have visited (Daneshyari and Yen, 2012; Fu et al., 2012; Wang and Watada, 2012).The construction of the information granules formalized as sets is realized as a certain optimization problem solved by using the PSO. In the following, we elaborate on the fitness function, its realization, and the PSO optimization along with the corresponding formation of the components of the swarm.One of the essential issues in designing a PSO algorithm is finding an appropriate mapping between problem solution and the particle’s representation. In this study, each particle represents a vector of cutoff points in the [0,1] scale. These cutoff points are used to represent the intervals into which the linguistic terms are translated.For example, in Fig. 2we show a set of five linguistic terms (Very Low (VL), Low (L), Medium (M), High (H), and Very High (VH)) with their respective cutoff points (a1,a2,a3,a4) to express preferences, and a set of three linguistic terms (Less Important (LI), Important (I), and Very Important (VI)) with their respective cutoff points (b1,b2) to quantify the importance degrees. Then we form the following mapping: VL: [0,a1], L: [a1,a2], M: [a2,a3], H: [a3,a4], and VH: [a4,1], and LI: [0,b1], I: [b1,b2], and VI: [b2,1], respectively. If we consider m linguistic values of the linguistic term set used by the experts to express their opinions and n linguistic values in the linguistic term set used to assign the importance degrees, this results in m+n−2 cutoff points. Being arranged in a single vector, they constitute an individual in the swarm of the PSO. In this particular example, an individual is represented as [a1a2a3a4b1b2] (see Fig. 2).The aim of the PSO is the maximization of the values of the performance index by adjusting the positions of the cutoff points in the [0,1] scale. When it comes to the formation of the fitness function, its determination has to take into account a fact that interval-valued entries of the reciprocal preference relations have to return numeric values of the fitness function. This is realized as follows: as we encounter information granules in the form of intervals, a series of their realizations being the entries both of the preference relations and the importance degrees associated to the experts is formed by randomly generating entries coming from the above intervals. To do so, the reciprocal linguistic preference relations {P1,…,Pm} provided by the experts are sampled to obtain the preference relations {R1,…,Rm} where each entry of Rl, l=1,…,m, is represented by a number drawn from the uniform distribution defined over the corresponding subinterval of the [0,1] interval according to the linguistic term of that entry in the linguistic preference relation Pl. In the same way, the importance degree μI(el) associated to each expert elis sampled to obtain a weight ukwhich is represented by a number drawn from the uniform distribution defined over the corresponding subinterval of the unit interval which represents the linguistic term μI(el). According to it, the performance index Q is expressed as follows:(8)Q=∑l=1mwl·cdl,where cdlis the consistency degree associated with the reciprocal preference relation Rland wlis a weight associated to the experts elcalculated aswl=ulS(m), beingS(m)=∑i=lmul. To obtain the consistency degree cdl, the procedure described in Section 2.2 is used.As the components are intervals but we require a numeric value of the fitness function, the reciprocal linguistic preference relations {P1,…,Pm} and the linguistic importance degrees μI(el) associated to each expert are sampled 500 times. The average of the values of the performance index Q computed over each collection of 500 samples is the fitness function, f, associated with the particle formed by the cutoff points:(9)f=1500∑i=1500Qi,A way of the formation of the fitness function is in line with the standard practices encountered in Monte Carlo simulations (Williams, 1991).In the PSO we use the generic form of the algorithm where the updates of the velocity of a particle are realized in the form v( iter+1)=w×v(iter)+c1a·(zp−z)+c2b·(zg−z) where “iter” is an index of the generation and · denotes a vector multiplication realized coordinatewise. a and b are vectors of random numbers generated from a uniform distribution expressed over the unit interval, zprepresents the local best solution and zgrepresents the global best solution positioned in the search space. The next position (in iteration step “iter+1”) of the particle is computed in a straightforward manner: z(iter+1)=z(iter)+v(iter+1). The inertia coefficient (w) is kept constant through the entire optimization process and equal to 0.2 (this value is commonly encountered in the existing literature (Pedrycz et al., 2012)). The progression of the optimization is quantified in terms of the fitness function obtained in successive generations.When it comes to the representation of solutions, the particle z consists of “m+n−2” entries positioned in the [0,1] interval that corresponds to the search space. There is some additional implementation constraint to be addressed in order to arrive at a meaningful solution. We request that the bounds of the intervals are kept sufficiently distinct so that the values of the fitness function is adjusted as follows: if the minimal length of the intervals, length, is less than 0.05 then the fitness function is set to some low value, say −1, that is far above the typical values of this fitness function encountered when optimizing the intervals. In other words, the low value of the fitness function penalizes a situation when the bounds start to overlap and lose their semantics.Finally, one should note that while PSO optimizes the fitness function, there is no guarantee that the result is optimal, rather than that we can refer to the solution as the best one being formed by the PSO.In this section, we illustrate the method presented in Section 3 and highlight its main features by presenting several examples. In all experiments, PSO was used with the following values of the parameters which were selected as a result of intensive experimentation:•The size of the swarm consisted of 100 particles. This size of the population was found to produce “stable” results meaning that very similar or identical results were reported in successive runs of the PSO. Since the search space is quite large, this particular size of the population was suitable to realize a search process.The number of generations (or iterations) was set to 500. It was observed that after 500 generations there were no further changes of the values of the fitness function.The parameters in the update equation for the velocity of the particle were set as c1=c2=2. These values are commonly encountered in the existing literature.Let us suppose that an investment company wants to invest a sum of money in the best industrial sector, from the set of five possible alternatives: {x1=car industry, x2=food industry, x3=computer industry, x4=arms industry, x5=TV industry}.To do this, four experts E={e1,e2,e3,e4} within the company are requested to express their preferences. The experts provide the following reciprocal linguistic preference relations using the set of five linguistic labels S1={VL=Very Low, L=Low, M=Medium, H=High, VH=Very High}:P1=-VLMMHNeg(VL)-MHMNeg(M)Neg(M)-Neg(L)LNeg(M)Neg(H)L-VLNeg(H)Neg(M)Neg(L)Neg(VL)-P2=-Neg(L)Neg(H)Neg(VL)Neg(M)L-Neg(VL)Neg(VL)Neg(VH)HVL-MNeg(VL)VLVLNeg(M)-Neg(M)MVHVLM-P3=-MHLMNeg(M)-MLLNeg(H)Neg(M)-Neg(M)VLNeg(L)Neg(L)M-VHNeg(M)Neg(L)Neg(VL)Neg(VH)-P4=-VLHNeg(VL)MNeg(VL)-VLVLMNeg(H)Neg(VL)-Neg(L)LVLNeg(VL)L-VHNeg(M)Neg(M)Neg(L)Neg(VH)-It is important to note that as the linguistic terms as represented as intervals,Negsi1denotes the complementary qualitative value ofsi1, whose semantics will be determined in our model using the equivalence index characterizingsi1. Suppose thatsi1=M, and M is represented by the interval [0.2,0.4]. If it is sampled as, for example, with the numeric value 0.25, Neg(M) will be equal to 0.75.To assign importance degrees to the experts, different approaches can be used. On the one hand, the importance degrees can be given externally, someone evaluates the experts and assigns them the importance degrees according their knowledge about the problem. On the other hand, the importance degrees can be assigned internally by evaluating how consistent the opinions provided by the experts are, the more consistent is the expert, the more importance degree is assigned to him/her (Chiclana et al., 2007). In this case, we use the first approach. Due to the fact that the individuals involved in the problem have different knowledge level on the five industrial sectors, they received the following importance degrees using the following set of three linguistic labels S2={LI=LessImportant,I=Important,VI=VeryImportant}:μI(e1)=I,μI(e2)=VI,μI(e3)=LI,μI(e4)=VIFig. 3a shows the progression of the optimization quantified in terms of the fitness function obtained in successive generations. The PSO returns the optimal cutoff points of 0.50, 0.58, 0.66, and 0.74, for the linguistic term set S1, and 0.38, and 0.73, for the linguistic term set S2, respectively. Hence, the intervals corresponding to the linguistic terms of the set S1 are: VL: [0,0.50], L: [0.50,0.58], M: [0.58,0.66], H: [0.66,0.74], and VH: [0.74,1], whereas the intervals corresponding to the linguistic terms of the set S2 are: LI: [0,0.38], I: [0.38,0.73], and VI: [0.73,1]. The average value of the performance index Q is equal to 0.836958 with a standard deviation of 0.0233191.The increase of the values of the consistency degree of each individual preference relation in successive generations is shown in Fig. 3b. As we consider a joint treatment of the linguistic terms used by the experts to express their opinions, it can be seen that when the PSO returns new cutoff points for the linguistic terms, the consistency degree associated of each preference relation changes so that the consistency degrees of some preference relations increase and the consistency degrees of some preference relations decrease. However, the fitness function f always increases (see Fig. 3a).To put the obtained optimization results in a certain context, we report the performance obtained when considering a uniform distribution of the cutoff points over the scale, which are equal to 0.20, 0.40, 0.60, and 0.80, for the linguistic term set S1 and 0.33, and 0.67, for the linguistic term set S2, respectively. The average performance index Q assumes the value 0.701907 with a standard deviation of 0.0142151. Comparing with the values obtained by the optimized cutoff points, the performance index Q takes on now lower values. The histogram of distribution of values assumed by the performance index Q in Fig. 4provides a more comprehensive view at the results: with the uniform distribution of the cutoff points there is a visible presence of a longer tail of distribution spread towards lower values of the Q.To obtain the ranking of alternatives from best to worst, both the aggregation phase and the exploitation phase are carried out:•Aggregation phase. From the optimal split of the scales S1 and S2, a collective preference relation is obtained by aggregating all individual preference relations using the weighted average (Montero, 1988) in the following way:pikc=∑l=1mwl·rikl.Pc=-0.440.500.670.570.56-0.470.520.460.500.53-0.510.530.330.480.49-0.540.430.540.470.46-Exploitation phase. The values obtained using the quantifier guided dominance degree, QGDDi, described in Section 2.1, with the average operator as aggregation operator, are the following: {x1=0.54,x2=0.50,x3=0.51,x4=0.46,x5=0.48}.Therefore, the global ranking of alternatives is: x1≻x3≻x2≻x5≻x4, being the car industry the best industrial sector in which the company should invest. The PSO-optimized distribution of the cutoff points allows us to obtain solutions with higher levels of consistency and, therefore, better results.Suppose that the supermarket manager wants to buy 1000 bottles of Spanish wine from among four possible brands of wine or alternatives: {x1=Marqués de Cáceres, x2=LosMolinos, x3=Somontano, x4=René Barbier}.The manager decide to inquire eight experts about their opinions E={e1,…,e8}. The experts provide the following reciprocal preference relations using the linguistic expression domain S1={EL=Extremely Low, VL=Very Low, L=Low, M=Medium, H=High, VH=Very High,EH=Extremely High}:P1=-EHELLNeg(EH)-MHNeg(EL)Neg(M)-VLNeg(L)Neg(H)Neg(VL)-P2=-LELNeg(H)Neg(L)-VLVLNeg(EL)Neg(VL)-Neg(VL)HNeg(VL)VL-P3=-Neg(EL)MELEL-ELEHNeg(M)Neg(EL)-HNeg(EL)Neg(EH)Neg(H)-P4=-ELHNeg(VH)Neg(EL)-VLNeg(EH)Neg(H)Neg(VL)-Neg(L)VHEHL-P5=-Neg(M)Neg(EL)Neg(VH)M-LVLELNeg(L)-VLVHNeg(VL)Neg(VL)-P6=-LMNeg(VL)Neg(L)-ELVLNeg(M)Neg(EL)-HVLNeg(VL)Neg(H)-P7=-EHNeg(M)EHNeg(EH)-Neg(VL)Neg(H)MVL-VHNeg(EH)HNeg(VH)-P8=-EHVLMNeg(EH)-ELVHNeg(VL)Neg(EL)-VHNeg(M)Neg(VH)Neg(VH)-As the experts involved in the problem have different levels of knowledge about wine, they received the following importance degrees provided by the manager using the following set of five linguistic labels S2={LI=Less Important, N=Neutral, I=Important, SI=Somewhat Important, VI=Very Important}:μI(e1)=N,μI(e2)=I,μI(e3)=VI,μI(e4)=LIμI(e5)=LI,μI(e6)=VI,μI(e7)=SI,μI(e8)=LIThe progression of the optimization is quantified in terms of the fitness function obtained in successive generations (see Fig. 5a). The PSO returns the optimal cutoff points of 0.38, 0.43, 0.50, 0.59, 0.65, and 0.70, for the linguistic term set S1, and 0.10, 0.49, 0.61, and 0.72, for the linguistic term set S2, respectively. In this way, the intervals corresponding to the linguistic terms of the set S1 are: EL: [0,0.38], VL: [0.38,0.43], L: [0.43,0.50], M: [0.50,0.59], H: [0.59,0.65], VH: [0.65,0.70], and EH: [0.70,1], whereas the intervals corresponding to the linguistic terms of the set S2 are: LI: [0,0.10], N: [0.10,0.49], I: [0.49,0.61], SI: [0.61,0.72] and VI: [0.72,1]. The average performance index Q takes a value equal to 0.779122 with a standard deviation of 0.0207083.In Fig. 5b we show the progression of the consistency degree of each individual preference relation in successive generations. As in the above example, when the PSO returns new cutoff points for the linguistic terms, the consistency degree associated of each preference relation changes. In this way, some consistency degrees increases and others decreases, but the fitness function f always increases.When considering a uniform distribution of the cutoff points over the scale, which are equal to 0.14, 0.28, 0.42, 0.56, 0.70, and 0.84, for the linguistic term set S1 and 0.20, 0.40, 0.60, and 0.80, for the linguistic term set S2, respectively, the average performance index Q assumes the value 0.696679 with a standard deviation of 0.0112915. As before, the optimized cutoff points obtain higher values of the performance index Q than the values obtained by the uniform distribution of the cutoff points. In the histogram shown in Fig. 6, we can see that in the PSO-optimized cutoff points there is a visible presence of a longer tail of distribution spread towards higher values of the performance index Q.In order to obtain the best wine, both the aggregation phase and the exploitation phase are carried out:•Aggregation phase. From the optimal split of the scales S1 and S2, a collective preference relation is obtained by aggregating all individual preference relations. To do so, we use the weighted average described in the above example. A reciprocal collective preference relation with the higher performance index Q is given below:Pc=-0.660.430.540.34-0.420.540.570.58-0.620.460.460.38-Exploitation phase. Using the quantifier guided dominance degree, QGDDi, described in Section 2.1, with the average operator as aggregation operator, we obtain the following values: {x1=0.54, x2=0.44, x3=0.59, x4=0.43}.The global ranking of alternatives is: x3≻x1≻x2≻x4, and therefore the manager should buy 1000 bottles of Somontano wine. As in the above example, the PSO-optimized distribution of the cutoff points obtains solutions with higher levels of consistency and, therefore, better results.

@&#CONCLUSIONS@&#
In this paper, we have developed a method based on granular computing to solve GDM problems defined in heterogeneous contexts when linguistic information is used. To do so, an information granulation of the linguistic information and its optimization using the PSO framework have been presented. In the future, we propose to continue this research in several directions:•We have used the PSO an optimization framework because it offers a great deal of flexibility. Different fitness functions could be easily accommodated and a multiobjective optimization can be sought. The need for the two-objective optimization becomes apparent in case of a GDM problem where, in addition to the criterion of consistency, one can consider a maximization of the consensus achieved among the experts. The direct application of the aggregation and the exploitation steps can lead sometimes solutions that are not well accepted by some experts in the group (Saint and Lawson, 1994), because they could consider that their opinions have not been taken into account properly to obtain the solution and, hence, they might reject it. To avoid this situation, it is advisable that experts carry out a consensus process (Cabrerizo et al., 2009, 2010; Fu and Yang, 2012; Mata et al., 2009), where the experts discuss and modify their preferences gradually to achieve a sufficient agreement before applying the aggregation and the exploitation steps.We propose the application of the discussed methodology to other formalisms of information granules as fuzzy sets, rough sets, shadowed sets, probabilities and so on. In particular, dealing with probabilistically granulated linguistic terms could help shed light on possible linkages between probabilistic and fuzzy models of decision making along with some possible hybrid probabilistic-fuzzy schemes.