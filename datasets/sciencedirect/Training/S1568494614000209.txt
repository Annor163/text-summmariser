@&#MAIN-TITLE@&#
Solving the balance problem of massively multiplayer online role-playing games using coevolutionary programming

@&#HIGHLIGHTS@&#
We proposed a coevolutionary design method to solve the balance problem of massively multiplayer online role-playing games.We demonstrated the theoretical evidence of why the cooperative coevolution algorithm is usually faster than Simple Genetic Algorithm.By enabling the elitist-preserving strategy, we improved the performance of probabilistic incremental program evolution.

@&#KEYPHRASES@&#
Game balance,Game design,Coevolutionary programming,Cooperative coevolutionary algorithm,Probabilistic incremental program evolution,Multi-objective optimization,

@&#ABSTRACT@&#
In massively multiplayer online role-playing games (MMORPGs), each race holds some attributes and skills. Each skill contains several abilities such as physical damage and hit rate. All those attributes and abilities are functions of the character's level, which are called Ability-Increasing Functions (AIFs). A well-balanced MMORPG is characterized by having a set of well-balanced AIFs. In this paper, we propose a coevolutionary design method, including integration with the modified probabilistic incremental program evolution (PIPE) and the cooperative coevolutionary algorithm (CCEA), to solve the balance problem of MMORPGs. Moreover, we construct a simplest turn-based game model and perform a series of experiments based on it. The results indicate that the proposed method is able to obtain a set of well-balanced AIFs more efficiently, compared with the simple genetic algorithm (SGA), the simulated annealing algorithm (SAA) and the hybrid discrete particle swarm optimization (HDPSO) algorithm. The results also show that the performance of PIPE has been significantly improved through the modification works.

@&#INTRODUCTION@&#
In recent years, massively multiplayer online role-playing games (MMORPGs) become more and more popular all over the world. The global market has risen above $12 billion in 2012 and is forecasted to reach $17.5 billion in 2015. A MMORPG is a multi-player persistent virtual world which consists of several distinct races. Unlike the board games (such as Chess, Shogi, and Go), which are symmetrical systems, MMORPGs are regarded as asymmetrical since each race is designed to hold some unique features. For example: the wizard can perform spells, while the knight specializes in close fighting. Before starting the game, players should first create one character from a certain race, then play the game by either exploring the virtual world or interacting with the other players.In MMORPGs, the concept of balance can be divided into two parts, the balance of Player-versus-Environment (PvE) and the balance of Player-versus-Player (PvP) [1]. In this paper, specifically, the term PvP refers to “1 versus 1” combat because it is the core of the MMORPG's combat systems. Generally, balance of PvE mainly means the difficulty control of the game, while balance of PvP, existing only in asymmetrical games, refers to the power balancing among the races. When designing a MMORPG, the power of a race will be represented by some attributes and a set of skills each of which consists of several abilities. In most cases, the attributes and the abilities must be non-negative integers, and they are functions of the character's level, called the Ability-Increasing Functions (AIFs). So the main task of balancing PvP is to construct a set of AIFs by which there are no dominating strategies in the game world and each race should have the same probability to win unless designers have some special design purposes.Traditionally, game companies solve the balance problem by designer's talent and human testing. For the balance of PvE, such approach can handle very well, even though it is expensive in both time and resources. The challenges of balancing PvE come from the environment rather than other players, so there is only one aim of the tuning process, namely to satisfy the human side. That is why the traditional way succeeded. However, in the case of balancing PvP, the traditional method causes two problems. Firstly, since designers cannot ensure the inexistence of dominating strategies, testers have to take a lot of time to play the games and even human testers cannot explore all strategies. Secondly, once a dominating strategy is identified, all the AIFs need to be modified to ensure the game is in balance, which is too complicated for human testing to handle. For instance (refer to Fig. 1), supposing there are three races (A, B, C) in the game world, designers may modify B's AIFs to get a balanced relationship between A and B, then similarly modify C's AIFs to get the balanced relationship between B and C. After that, the relationship between C and A becomes determinate because all the AIFs have already been constructed, which means that the last relationship is uncontrollable.Tuning game balance is a multi-objective optimization problem which will become hard to solve with the number of races increasing. The number of uncontrollable relationships, denoted by NUR, can be calculated by the following formula:(1)NUR=n2−n+1where n is the number of races.Moreover, since the nonlinearity of the objective functions, it can also be regarded as a nonlinear integer programming problem, which belongs to the class of NP-hard problems [2]. Therefore, there does not exist an exact algorithm, which can solve the problem in time, depending polynomially on the problem input data length or on the problem size. In this circumstance, heuristic algorithms, such as genetic algorithm (GA), become the only viable option because they find approximate solutions but have acceptable time and space complexity.In the game industry, there are two kinds of revenue models, profiting from the game software and profiting from the game services. For most traditional games (such as console-based games, etc.), game companies employ the former one, that is, they earn money by selling the game software. However, in the case of MMO games, the latter one is preferred. The game software of MMO games are free to install, but players should subscribe to the game before they can play. In this revenue model, game companies make money by selling either the game time or the virtual items. Fig. 2shows the sequence diagrams of the revenue models. In the left part, the yellow activation box refers to the life cycle of the game which means the total game time that the player spent in playing the game. But in the right part, the life cycle equals to the summation of all the yellow activation boxes. Since the free game software, it's obvious that the life cycle is much more crucial to the MMO games. If the players stop the subscription, the incomes of the game company will definitely decrease. Therefore, having a long life cycle has become the most important feature of the high-quality MMO games.In the case of MMORPG, beside the user interface and the story line, the game balance also directly affects the life cycle of the game. For example, if we let a crab and a duck to play the rock-paper-scissors game, undoubtedly, the duck will quit the game as quickly as possible because it always loses. Generally, to be enjoyable, the game must be balanced well [3]. In other words, for the individual player, it must be neither too easy nor too hard, and for players competing against each other, fairness is crucial. A game without balance is untidy and ugly, flaws are reflected in the experience of playing it. An unbalanced game is less satisfying. More seriously, from the designer's viewpoint, not only time but also effort has been wasted. If parts of the game are not well-balanced, there will be a risk that some of them will be used rarely, if at all, in which case the time spent developing them has gone to waste.Leigh et al. [4] have presented a solution for balancing a two-player, real-time action game called CaST. They use the competitive coevolutionary algorithm to search the dominating strategies and once found, tune the game rules and parameters. This solution highlights game imbalance as well as provides intuition towards balancing the game. It can be applied to on-line RPGs with some improvements. Alternatively, Olsen [5] has described the outline of an automated testing framework for MMORPGs. In such framework, game designers firstly create all AIFs empirically. Then the in-game combat data, recorded in game log, is used to construct a set of decision-making models by which a set of complex AI systems can be built. Based on those AIs, combats are simulated to verify whether one race is superior to another. If so, the AIFs will be continually tuned, until the game is in balance. Moreover, the decision-making models are refined periodically and after each refinement, the combat simulations as well as the tuning tasks should be redone (refer to Fig. 3). This approach can be regarded as a kind of dynamic balance adjustment in which the dominating strategies are identified dynamically.However, in those two solutions, the AIFs still will be adjusted by hand-tuning which is inefficient and time-consuming. Moreover, since strengthening one race will indirectly weaken the others, the result of tuning operations may, somehow, become worse. Therefore, it is emergent to find a low-cost and efficient solution. In this paper, we propose a coevolutionary method to obtain a set of well-balanced AIFs. The method includes integration with the modified probabilistic incremental program evolution (PIPE) and the cooperative coevolutionary algorithm (CCEA). The merit of the evolutionary algorithms is that we simply need to define a proper fitness function to reach the near-optimal or optimal solution, which will liberate the development team from the endless testing, and eventually improve the work efficiency.The remainder of this paper is organized as follows. Section 2 presents a brief but necessary overview of genetic algorithm and genetic programming. Section 3 explains the CCEA technology, followed by the details of PIPE in Section 4. Section 5 introduces the proposed design method as well as the simplest turn-based MMORPG model. Section 6 demonstrates the experiments and their results. Conclusions and possible future research directions are placed in the last section.Genetic Algorithm (GA) [6] is a probabilistic search algorithm based on the mechanics of natural selection and natural genetics. It iteratively transforms a population of objects, usually a fixed-length binary string and each with an associated fitness value, into a new population of offspring objects by using three operators that are selection, crossover and mutation.The theoretical basis of GA, the schema theorem, formalized by Holland [6] and popularized by Goldberg [7]. A schema, H=[*11*0**], is a template for the 7-length individuals in which 2nd and 3rd locus are ‘1’ while 5th locus is ‘0’. The asterisk ‘*’ is the wild card symbol which matches either a ‘0’ or a ‘1’ at a particular position. Schema order, denoted by o(H), is the number of non ‘*’ genes in schema H. Moreover, schema defining length, denoted by δ(H), is the distance between first and last non ‘*’ gene in schema H. The schema theorem is given as follow:Theorem 1Given a simple GA with proportional selection, single point crossover and gene wise mutation. Then the expected number of schema H at generation t+1 is,E[m(H,t+1)]=m(H,t)·f(H)f¯[1−μ]where0≤μ≤pc·δ(H)l−1+o(H)pmHere, m(H, t) is the number of individuals matching schema H at generation t, f(H) denotes the mean fitness of individuals matching schema H,f¯is the mean fitness of individuals in the population of generation t, pcand pmdenote the crossover rate and the mutation rate, respectively.It means that short, low-order, above-average schemata receive exponentially increasing trials in subsequent generations of a genetic algorithm. If we consider the algorithm without variational operators, then μ=0.Genetic programming (GP) [8–10] is a specialization of GA in which each individual of the population is a tree-structured program. It is a machine learning technique used to automatically solve problems without requiring the user to know or specify the form or structure of the solution in advance. At the most abstract level, GP is a systematic, domain-independent method for getting computers to solve problems automatically starting from a high-level statement of what needs to be done.Fig. 4shows the tree structure of the program: cosx+sin(x*y), where {x, y} is called the terminal set and {cos, sin, +, *} is the function set. Such two sets together form the solutions of the target problem. While GA is usually concerned with modifying fixed-length strings, which associated with parameters of a function, GP is concerned with actually creating and manipulating the (non-fixed length) structure of the program (or function). Consequently, GP is a much more complex and difficult topic.CCEAs [11,12] have been applied to solve large and complex problems, multiagent systems [13–15], rule learning [16,17], fuzzy modeling [18], and inventory control systems [19]. It models an ecosystem which consists of two or more species. Mating restrictions are enforced simply by evolving the species in separate populations which interact with one another within a shared domain model and have a cooperative relationship. The original architecture of the CCEA for optimization can be summarized as follows:(1)Problem Decomposition: Decompose the target problem into smaller subcomponents and assign each of the subcomponents to a population.Subcomponents Interaction: Combine the individual of a particular population with representatives selected from others to form a complete solution, then evaluate the solution and attribute the score to the individual for fitness.Subcomponent Optimization: Evolve each population separately by using a different evolutionary algorithm, in turn.The empirical analyses have shown that the power of CCEAs depends on the decomposition work as well as separate evolving of these populations resulting in significant speedups over the Simple GA (SGA) [20–22]. Here, We give the theoretical evidence of such results with the following two assumptions.(1)The elitists of CCEA populations are chosen as the representatives.There are no variational operators in both the SGA and the CCEA.Definition 1Given schemata: H1, H2, …, Hn, where Hidenotes a schema of the ith CCEA population, the n-expanded schema, denoted byH1n, is the sequential concatenation of the n schemata. For example, let H1=[1*0*], H2=[*1*1], thenH12=[1*0**1*1].Definition 2Let there be n populations in the CCEA. A complete genotype is the sequential concatenation of n individuals selected from n different populations. If all the individuals are representatives, then the complete genotype is the best one.Definition 3Given an individual I of the ith CCEA population, the expanded genotype of I is the best complete genotype in which the ith representative is replaced by I.Definition 4Given a target problem f, the two algorithms, the SGA and the CCEA, are strictly comparable if the population of the SGA consists of all the expanded genotypes in the CCEA.Theorem 2Let a target problem f be decomposed into n subcomponents, ribe the increasing rate of the individuals matching Hiin the ith CCEA population, rCCEAbe the increasing rate of the complete genotypes matchingH1nin the CCEA, and rSGAbe the increasing rate of the individuals matchingH1nin the SGA. If the two algorithms are strictly comparable, then,(1)rSGA<∑i=1nrirCCEA=k·min{r1,r2,…,rn}n, k≥1ProofSince the two algorithms are strictly comparable, in the SGA, the number of individuals matchingH1nat generation t can be calculated by:m(H1n,t)=∑i=1nM(Hi,t)where M(Hi, t) denotes the number of individuals matching Hiat generation t, in the ith CCEA population. Then according to the schema theorem (refer to Theorem 1), we have,E[m(H1n,t+1)]=m(H1n,t)·∑i=1nf(Hi,t)¯∑i=1nF(i,t)¯<m(H1n,t)·∑i=1nriwheref(Hi,t)¯andF(i,t)¯denote the mean fitness of individuals matching Hiand the mean fitness of all individuals, at generation t in the ith CCEA population, respectively.In the case of CCEA, becauseH1nis the conjunction of Hi, the number of the complete genotypes matchingH1nat generation t is given by:M(H1n,t)=∏i=1nM(Hi,t)Again, according to the schema theorem, we obtain the following equation:E[M(Hi,t+1)]=M(Hi,t)·riThen,E[M(H1n,t+1)]=∏i=1nE[M(Hi,t+1)]=∏i=1nM(Hi,t)·ri=M(H1n,t)·k·(min{r1,r2,…,rn})nwherek=∏i=1nrimin{r1,r2,…,rn}≥1. This completes the proof.  □Therefore, if min{r1, r2, …, rn}≫1, with the n increasing,H1nwill receive a much higher increasing rate in the CCEA. However, Theorem 2 does not mean that the CCEA is superior to the SGA, which depends on the target problems. Actually, since the representatives are necessary in calculating the fitness of the individual of an arbitrary population, the relationships between populations impose a great influence on the efficiency of CCEA [23,24]. It has been proved that even with prefect information, infinite population size and no variational operators, the CCEA can be expected to converge to suboptimal solution [24], while the SGA does not suffer from such affliction [25–27]. However, Liviu [28] also has emphasized that the CCEA will settle in the globally optimal solution with arbitrarily high probability, when properly set and if given enough resources.PIPE [29] is a technology for synthesizing programs. Unlike traditional GP, It creates population according to an adaptive probability distribution over all possible programs with respect to a predefined instruction set. In each generation, the distribution is refined by using the information learned from the best program or the best program found so far (elitist) to guide the evolutionary search [30]. It has been shown that PIPE achieved better results than traditional GP in solving function regression problem and 6-bit parity problem [29].PIPE stores the probability distribution in the probabilistic prototype tree (PPT), which is generally a complete n-ary tree with infinitely many nodes, where n is the maximal number of function arguments. Each node Njin a PPT consists of a value Rjwhich is randomly taken from a problem-dependent set of constants and a variable probability vectorPj→for instruction set, which is initialized as follows.(2)Pj(I)=PTl,∀I:I∈TPj(I)=1−PTl,∀I:I∈Fwhere PTis a predefined, constant probability for selecting an instruction from T (the terminal set), l is the total number of terminals in T and k is the total number of functions in F (the function set).As we mentioned above, there are two kinds of learning mechanisms in the PIPE, generation-based learning (GBL) and elitist learning (EL). GBL is the main learning algorithm and the purpose of EL is to use the best program found so far as an attractor. The pseudo-code of PIPE follows:(1)GBLREPEATwith probability PelDO ELotherwise DO GBLUNTIL termination criterion is reachedwhere Pelis a user-defined constant in [0, 1].The process of GBL can be divided into five distinct phases:(a)Creation of program population. A population of programsPROGj(0<j<PS; PS is population size) is generated using the PPT.Population evaluation. Each programPROGjof the current population is evaluated on the given task and assigned a fitness valueFIT(PROGj)according to the predefined fitness function. The best program of the current population is denoted byPROGb. The elitist is preserved inPROGel.Learning from population. Prototype tree probabilities are modified such that the probabilityP(PROGb)of creatingPROGbincreases. This procedure is called adapt_PPT_towards (PROGb) which is implemented as follows.FirstP(PROGb)is computed by looking at all PPT nodes Njused to generatePROGb:(3)P(PROGb)=∏j:NjusedtogeneratePROGbPj(Ij(PROGb))whereIj(PROGb)denotes the instruction of programPROGbat node position j.Then a target probability PTARGETforPROGbis calculated:(4)PTARGET=P(PROGb)+(1−P(PROGb))·lr·ɛ+FIT(PROGel)ɛ+FIT(PROGb)where lr is a constant learning rate and ɛ is a positive user-defined constant. Given PTARGET, all single node probabilitiesPj(Ij(PROGb))are increased iteratively:(1)REPEATPj(Ij(PROGb))=Pj(Ij(PROGb))+clr·lr·(1−Pj(Ij(PROGb)))UNTILP(PROGb)≥PTARGETMutation of prototype tree. All probabilities Pj(I) stored in nodes Njthat were accessed to generate programPROGbare mutated with probabilityPMp:(5)PMp=PMz·PROGbwhere the user-defined parameter PMdenotes the overall mutation probability, z is the number of instructions in instruction set S andPROGbdenotes the number of nodes in programPROGb. Selected probability vector components are then mutated as follows:(6)Pj(I)=Pj(I)+mr·(1−Pj(I))where mr is the mutation rate, another user-defined parameter. All mutated vectorsPj→are finally renormalized:(7)Pj(I)=Pj(I)∑I*∈SPj(I*)∀Pj(I):I∈SPrototype tree pruning. At the end of each generation the prototype tree is pruned. PPT subtrees attached to nodes that contain at least one probability vector component above a threshold TPcan be pruned. The pruning operation results in more concise population than traditional GP.In the EL, on the other hand, adapt_PPT_towards (PROGb) is modified to adapt_PPT_towards (PROGel), that is, PPT is adapted towards the elitist program without creating and evaluating population.We notice that PIPE does not use the Elitist-Preserving Strategy (EPS) in which the individual with the highest fitness survives to be an individual of next generation unconditionally. The EPS is a useful technic which guarantees that the best chromosome generated will not be lost from one generation to the next as a consequence of sampling effects or the application of genetic operators. In some case, it has been shown that the EPS can significantly improve the performance of evolutionary algorithms [31–33]. However, the EPS biases the distributor of trails in favor of those subspaces which have produced the best-performing individual, namely, it suggests that the effect of EPS on performance will improve local search at the expense of global search. Therefore, to avoid such premature problem, when applying the EPS, we make some modifications to the learning mechanism of the PIPE. Referring Formula (4), in the original PIPE, the learning rate is controlled by the following fraction dynamically:(8)v=ɛ+FIT(PROGel)ɛ+FIT(PROGb)Here, we construct a new fractionv′as follow:(9)v′=exp−FIT(PROGb)−SFKFIT(PROGb)>SF1FIT(PROGb)≤SFwhere SF denotes the satisfactory fitness, K is a positive user-defined constant which used to control the pressure of learning. The new fraction is able to control the learning rate according to the distance betweenFIT(PROGb)and SF. It is important to note that once PIPE employs the EPS, the EL will no longer exist.With the assumption that all the dominating strategies have been identified, the task of balancing PvP is reduced to constructing a set of AIFs by which each race has nearly the same probability to win. In order to solve this multi-objective optimization problem, we build a single aggregate objective function by weighted linear sum of the objectives, which means the relationships between the AIFs become cooperative. In the proposed design method, each of the AIFs is assigned to a population in which the individuals are created by the modified PIPE. All those populations are evolved by the CCEA. Fig. 5shows the flow chart of the method.For an arbitrary “1 versus 1” combat group i, we define the residual rijas (pij−0.5), where pijis the win probability of combatant 1 of group i at level j. To be in balance, both the mean the rijand the mean ofrijshould be close to 0. Thus we build the single aggregate objective function as follows:(10)FIT(PROG)=w1e+w2v+w3frule(S)wheree=1m*L∑i=1m∑j=1Lrijv=1m*L∑i=1m∑j=1Lrijwherew1,w2andw3are weighting parameters,m=n2denotes the number of “1 versus 1” groups, n is the number of races, L is the maximum level, frule(S)≥0 denotes how many times the solution S disobeyed the designed rules of AIF. The better individuals will return lower values in evaluation process.In order to test the proposed method, we construct a simplest turn-based MMORPG model as the platform for experiments. The contents of design are listed as follows:(1)There are three distinct races(R1, R2, R3) in the game world with the maximum level of L. So, we have three combat groups, (R1, R2), (R2, R3) and (R3, R1).Each race has only one skill and two attributes which are health and dodge rate. For simplicity, we let the dodge rate as a constant.Each skill includes only one ability, the physical damage.(11)pij=∑g=1TN12·pi(Fg)pi(F′g−1)+pi(F′g)wherepi(Fg)=g−1LT2(i)−1(1−D2)LT2(i)D2i−LT2(i)g≥LT2(i)0g<LT2(i)pi(Fg′)=∑k=0LT1(i)−1gk(1−D1)kD1g−kg≥LT1(i)1g<LT1(i)LT2(i)=h2(i)d1(i)LT1(i)=h1(i)d2(i)Here, we suppose that the combat ends within TNrounds, pi(Fg) denotes the probability that combatant 1 launch the lethal attack at round g,pi(F′g)is the probability that combatant 2 has not launched the lethal attack before round g+1, Dl, LTl, hl, dlrefer to the dodge rate, lifetime, health value and damage value of combatant l, respectively, where l={1, 2}.

@&#CONCLUSIONS@&#
