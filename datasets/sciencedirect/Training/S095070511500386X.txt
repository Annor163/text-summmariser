@&#MAIN-TITLE@&#
Windowing improvements towards more comprehensible models

@&#HIGHLIGHTS@&#
We propose several improvements for the windowing algorithm.We evaluated model performance, interpretability, and stability.Our methodology focuses on the interpretability of the model.Our approach shows differences in terms of interpretability, without harming performance.Our approach may yield better classification models.

@&#KEYPHRASES@&#
Windowing,Decision tree metrics,High dimensional data,

@&#ABSTRACT@&#
The induction of decision tree searches for relevant characteristics in the data which would allow it to precisely model a certain concept, but it also worries about the comprehensibility of the generated model, helping human specialists to discover new knowledge, something very important in the medical and biological areas. On the other hand, such inducers present some instability. The main problem handled here refers to the behavior of those inducers when it comes to high-dimensional data, more specifically to gene expression data: irrelevant attributes may harm the learning process and many models with similar performance may be generated. In order to treat those problems, we have explored and revised windowing: pruning of the trees generated during intermediary steps of the algorithm; the use of the estimated error instead of the training error; the use of the error weighted according to the size of the current window; and the use of the classification confidence as the window update criterion. The results show that the proposed algorithm outperform the classical one, especially considering measures of complexity and comprehensibility of the induced models.

@&#INTRODUCTION@&#
One of the greatest challenges for machine learning (ML) is to build precise models out of high-dimensional data, like gene expression data. Furthermore, in biology and medicine, the comprehensibility and interestingness of the model is also important for the specialists to trust it and have insights about the phenomena being studied, which might lead to creating new knowledge [1,2]. In this paper, we have studied the performance of decision trees (DTs) built from high dimensional data like gene expression, focusing on not only measures like accuracy, but also on those related to the comprehensibility of the classifiers.On the other hand, given the fact that ML algorithms, including DTs, have been designed for a broad spectrum of the human knowledge, they are based on flexible and powerful representations. This flexibility may, in some cases, present the disadvantage of making inducers more susceptible to the training data. DTs, for example, are known to produce very different models with small changes in the data, which may have the opposite effect: the specialists get confused and stop trusting the models [3,4]. This paper has also evaluated how to improve the stability of DTs.Specifically, in gene expression data, the number of genes (attributes) is typically much greater than the number of tissue samples (instances), and only a small subset of the genes is relevant to the task at hand [5,6]. The great advantage of DTs over other learning paradigms, such as function-based and statistical paradigms, is that they possess an embedded process of attribute selection, allowing them to use only those attributes which were considered to be relevant and informative [7]. As a result, the final DT models are smaller, syntactically simpler and more comprehensible. Considering stability again, in general, DTs built from gene expression data tend to include very few attributes in the model; however, given the huge number of attributes, many trees with similar accuracy can be built, each of which with a different subset of attributes [8].Gene expression profiles are obtained via Molecular Biology techniques like Microarray [9] (based on intensity measures of DNA hybridization) and SAGE [10] (based on tag counting). This kind of data helps in the discovery of diagnostic and prognostic methods, diseases treatment, drug development [11], etc. Many research works have concentrated efforts on applying ML algorithms on gene expression analysis [1,2,12–19].In order to deal with the problems described above, we have used windowing [20], a technique whose idea is to find a subsample of a dataset that provides enough information for an inducer to train a classifier. It has results similar to those achieved by training a model from the entire dataset, reducing the complexity of the learning problem [21]. This way, windowing can be seen as a subsampling technique [22], but, unlike other subsampling techniques (e.g., bootstrapping), windowing tend to provide more class balanced and informative samples. The technique was first proposed by [23] in the context of decision trees as a way to deal with memory restrictions imposed by computers in the late 1970’s to some relatively large datasets. The work of [20] argues windowing is interesting for two reasons: i) in some cases, especially those free of noise, it may make the time taken to build the model shorter (e.g., when the dataset is very large and a model that perfectly classifies all training instances is achieved in the first iterations). For most cases, however, windowing makes that time longer; ii) windowing may produce more accurate classifiers, since it explores the solution space a little further. Memory is still an issue, given the existence of huge databases on medical and biological domains.The study performed by [24] states that windowing contributes to better threshold choices for continuous attributes in decision tree induction. In [25], windowing is seen as a way to make unstable learning algorithms more stable. Some other studies have explored windowing in the context of rule induction [26], as they have noted the technique produces better results with that kind of inducer than with decision trees, since rules are learned independently of each other and are less susceptible to changes in the class distribution. In spite of the experience Quinlan had with windowing, those studies also state that the technique is not good to be used with decision trees, especially in noisy domains [27]. Possibly because of that, very little has been researched towards improving the combination between windowing and decision trees, although there still are some work on it [28]. Our research group believes there still are some aspects of windowing that could be explored further and has proposed some changes in the original algorithm, which will be discussed in the next sections. These changes are focused on improving the performance of decision trees built from gene expression data (which are typically noisy).Our implementation of windowing can be applied to any learning algorithm that works with classification. Our group has been focusing on symbolic classifiers, i.e., those which can be written as a set of rules. In this context, one advantage of windowing over other meta-inducers is that its use with symbolic classifiers still provides symbolic classifiers. Instead of just concerning about accuracy, we have used comprehensibility-related measures to assess the models. Such measures are well established when it comes to rule induction, but not when it comes to DTs. We have proposed some of those measures but specifically applied to DTs.The remaining sections are organized as follows: Section 2 gives details about the original version of windowing; Section 3 describes the alterations proposed in windowing and how the experiments were conducted; Section 4 presents the measures used to assess the results; Section 5 presents the experimental setup; results and discussion can be found in Section 6; conclusions are presented in Section 7.The version of windowing used here is that of C4.5 [20], an algorithm of top-down induction of decision trees. Starting from the root of the tree, the algorithm performs a greedy search for attributes to be used in tests in the internal nodes. Each internal node performs a test on only one attribute and has two or more branches, each of which representing a test outcome. The leaf nodes contain class labels. To label a new example: from the root node, tests are performed and, according to their outcomes, the example goes down the tree until it reaches a leaf node, receiving its label. The decision tree inducer has a Java implementation in Weka [29] called J48.This section describes windowing as it is implemented in C4.5 Release 8 [20]. Algorithm 1shows the pseudo-code of windowing, where N represents the number of instances in the training set andx→iand yi(i=1,…,N) represent a vector containing the attribute values and the class label for instance i, respectively. The ∥E∥ operator returns 1, if E is true, or 0, otherwise.Before the learning process begins, a subset of the training set is chosen, forming the initial window (Line 2), from which a model is induced (Line 5); the model is then used to predict the class of all training examples inside (windowErrors) and outside the window (testErrors), which might produce some misclassifications (Lines 6–7); if the errors found are less than the errors of the best classifier so far (initially,N+1), the current classifier is kept as the best one (Lines 9–12); if there were errors outside the window, the window is updated and used to train another classifier; the resulting model is then tested again, and the process is repeated until no misclassifications occur outside the window.The initial window (Line 2) is not actually sampled randomly. First the training set is shuffled; then the algorithm tries to build a window as uniform as possible, i.e., the class distribution gets to be as balanced as possible. Considering c as the actual number of all class values of a dataset andE=W/cas the expected number of instances for each class value in the initial window W, for any given class value, if the number of instances labeled with it is at least equal toE,then it will be represented byEinstances in the initial window; otherwise, all instances representing that class value will be added to the window. This often leads to better results, especially in cases of unbalanced classes [20]; it also contributes to better threshold choices for continuous attributes [24]. In spite of being as uniform as possible, the class distribution and the examples chosen to be in the initial window are subject to a random sampling, which means that one may have different initial windows, leading to different final classifiers. As it can be seen in Algorithm 1, at least half of the misclassified examples outside the window is added to it at each iteration (Lines 13–16), provided that there are enough examples. This is done to make the model converge faster. Lines 15–16 take I misclassified examples from outside the window and adds them to it.The process can be repeated more than once. Each repetition is called a trial and starts with a different initial window, which often generates a different final classifier. By default, C4.5 uses 10 trials. The best tree classifier from all trials is returned as the final output.The revised (extended) windowing is presented in Algorithm 2. The initial window (Algorithm 2, Line 2) is chosen the same way as the original (Algorithm 1, Line 2), as explained at the end of the previous section.In the original implementation, only the best tree of each trial is pruned (Algorithm 1, Line 18), but all intermediary trees within a trial are kept unpruned. In order to decide which tree is the best among all trials, the algorithm takes the unpruned version of the best classifier returned by each trial and calculate their estimated error. The best final classifier is the one with the least estimated error. In Algorithm 2, there is the possibility of not using Line 42 (which corresponds to Algorithm 1, Line 18), but using Algorithm 2, Line 8 (not available in Algorithm 1), which results in the pruning of all trees. The reason for this alteration is simple: the process of pruning trees is known to be important, because it helps the tree improve its generalization error [30,42]. In a previous work [31], we have analyzed this aspect of windowing, and the results showed this change, by itself, made no improvements to the algorithm. In spite of having a greater computational cost, we wanted to verify if this change, by itself or in combination with the other ones, would bring any benefits over the original version of windowing and the traditional decision tree.It is known windowing has its performance deteriorated in noisy domains, because it ends up adding all noisy instances to the window, since they are typically misclassified even by good models [32,33]. In order to treat this problem, another alteration we propose is based on the confidence in the classification performed, and its inspiration was provided by co-training semi-supervised learning [34], which uses the confidence in the classification as the criterion to add unlabeled instances to the set of labeled instances. Following this idea, we implemented the possibility of using the classification confidence criterion to update the window in Algorithm 2, Line 24.For instance, the inducer J48 assigns an array of values to each prediction it performs. Each position of the array corresponds to one of the possible class values and represents the distribution of a given class in the leaf node that was reached. The predicted class will be the one with the greatest such distribution. These values can be seen as a factor that represents how confident the model is in the prediction it has just performed.In a given iteration of the algorithm, there may be misclassified instances outside the window. In the proposed alteration, such instances are sorted according to the confidence with which the model would predict the true class. Keep in mind that, in this case, the predicted class was not the true one, because the instance was misclassified, but the true class also has an associated confidence. These confidence values (of the true class) are put in descending order by our implementation. The heuristic here is the following: although the instance has been misclassified, the greater the confidence the model has in the prediction of the true class, the closer it is to classify the instance correctly. For example: in a certain problem, in which the class attribute can assume values “Yes” or “No”, the model built during an iteration of windowing misclassified two instances outside the window. One of them had “Yes” as the true class, and the model assigned a confidence of 0.40 to value “Yes” and 0.60 to value “No” (the one predicted by the model). The other instance had “No” as the true class, and the model assigned a confidence of 0.05 to value “No” and 0.95 to value “Yes” (the one predicted by the model). In both cases, the model made mistakes, but one could say that, in the first case, the model was closer to a good classification.The algorithm calculates, at each iteration, the size of the increment used to update the window. When the window is updated, the instances to be added are the ones with the greater confidence values. But, when the confidence criterion is used, the algorithm also makes some changes to the increment calculation. As the instances are sorted by their confidence values, the algorithm counts how many confidence values are greater than zero, counter to be referred to as N0 from now on (Algorithm 2, Line 26). N0 is then tested against the calculated increment. If it is greater than half of the calculated increment, the increment is assigned value N0, provided that N0 < increment. Otherwise, the increment gets to be half of itself. This is done for two reasons: i) the fact that N0 is small may be due to the presence of noise, in which case adding too many instances to the window is not interesting; ii) the algorithm is having difficulty in learning the concept, in which case it is better if we fine-tune the increment. This way, the algorithm would converge more slowly, but it would minimize the risk of adding too many instances that won’t bring useful information at the moment.The confidence criterion was also used as a way to stop the algorithm before it finds no misclassified instances outside the window or there are no more instances outside the window, preventing it from continuing the calculations in situations where adding more instances to the window would not improve the classifier: if N0 has had value zero for more than three consecutive iterations, i.e., if the models produced in the last iterations have not had a single confidence value greater than zero, the trial is stopped (Algorithm 2, Line 30).The third change proposed refers to the error calculation of the intermediary trees (those produced during a trial). In the original algorithm, the error of such a tree is given by the sum of the errors found in the window (training error) and those found outside it (unseen instances). However, the error of trees built in the last iterations of a trial are closer to the resubstitution error, since the window gets bigger as the iterations are processed. The resubstitution error is optimistic, making the trees built in the first iterations of a trial be in disadvantage since the window is smaller and the number of unseen instances is greater. In order to make the assessment of the trees fairer, instead of the calculation of the training error, we created the possibility of using the estimated error (Algorithm 2, Line 11). The calculation of this error was proposed by [20] as a pessimistic estimate of the generalization error. This estimate is based on the binomial distribution and takes into consideration the class distribution in the leaf nodes. It is the same error used by the original algorithm to assess the best trees of each trial. Quinlan proposed the calculation of the estimated error following a recursive definition, in which the estimated error of a non-leaf node is given by the sum of the estimated error of each of its descendants; and the estimated error of a leaf node is calculated as follows: considering the leaf node covers M training instances, E of which incorrectly, Quinlan compared this situation with that of the observation of E events out of M trials. If M is seen as a sample, one could calculate a posteriori probability that would give an idea of the error probability of the leaf node. This probability has the form of a confidence interval. Once the confidence level is defined, which is one of the parameters included in the inducer, the confidence interval is given by the one of a binomial distribution. Since we want a pessimistic estimate of the generalization error of the tree, only the upper limit of the interval matters. Such approximation harms many of the concepts in Statistics, but it represents a heuristic that produces good estimates accord-ing to [20].For the same reason presented in the last paragraph, a fourth alteration was implemented: the use of the weighted error in the assessment of the intermediary trees. This weighting is based on the size of the current window. The bigger the window, the more data the algorithm has for training, and the greater it is its chances to make good classifications. We implemented the possibility of “punishing” the trees built from bigger windows, approximating the conditions under which the trees are assessed. In this case the total error is multiplied by1+|window|/N,where |window| is the size of the current window and N is the number of instances in the whole dataset (Algorithm 2, Line 18).Accuracy and the area under the ROC curve (AUC) are two of the most commonly used metrics in the assessment of ML models. However, when one needs an idea of the interpretability degree of a model, or of how useful a model is, other measures have to be used. In the context of decision rules, many of these measures are already defined [35]. Nonetheless, we have found no such measures defined directly to DTs. What researchers typically do is to rewrite the tree as a set of rules, representing each path from the root to the leaf nodes in the form L → R, where L is a conjunction of tests in the attributes and R is the class label of the reached leaf. From that point, those decision rule measures can be used. Rules derived this way present some issues, including redundant tests. If this redundancy is eliminated, the resulting rules get to be dependent of each other, and the order they are applied becomes relevant. Pure decision rules, on the other hand, are normally generated independently of each other [36]. Our research group has been working on some syntactic metrics that could be applied directly to decision trees, such as cohesion, compactness, and cohesion–compactness. Other metrics, such as tree size, tree height, and window size are commonly known in the machine learning literature. All these metrics are presented in the following sections.The size of the final tree produced by an inducer is simply the number of nodes in the tree . This measure is actually already provided by some systems, such as Weka. This is a trivial measure, but it provides us with some idea of one of the aspects related to the interpretability of the model: we assume that, the smaller the tree, the easier it is for the model to be understood by human specialists. In spite of being symbolic models, huge DTs are hard to interpret.The height of a tree is defined as the number of steps between the root node and the deepest leaf node. This path allows only a step from a certain node to one of its direct descendants. This measure also provides an idea of how complex the model is. The taller the tree, the more attributes, and nodes are needed to explain the concept, which makes the tree more complex.This measure is specific to the analysis of models produced by windowing, which may produce good models using only a subset of the original training dataset. The measure is defined as the number of instances present in the window used to generate the final classifier produced by the algorithm. Provided that the other performance measures are not significantly worse, the smaller the window that generated the model, the better the algorithm. In this sense, it is obvious for trees in Fig. 1, it is not possible for us to calculate the size of the window by just looking at the final classifier; for that, we would have to analyze the algorithm execution.Considering c > 1 as the number of different class labels, and f ≥ 1 as the number of leaf nodes in the tree, we define cohesion as:(1)Cohesion=cf−1+cAs can be noted, cohesion values lie in the interval (0, 1]. Considering a tree has at least one leaf node (in the “worst” case, the tree will always predict the majority class), value 1 will be achieved when the model has exactly one leaf node, representing the simplest possible model. As the number of leaves grows, the value of cohesion approaches 0 and the tree becomes more complex because the concept needs more information to be explained (according to the model). A value close to 1 means the model is more cohesive in relation to the possible number of different class labels. The only especial case is the one cited above, in which the model is represented by just one leaf node. In spite of assuming value 1, it would not normally be considered a good model, since it found no relevant information that could be used to explain the concept being studied.This measure represents how compact the tree is in relation to the knowledge involved in the problem at hand. Considering t ≥ 0 as the number of different attributes used by the tree model, and a > 0 as the total number of attributes present in the dataset, we define compactness as follows:(2)Compactness=1−taThe value of compactness lies in the interval [0, 1]. Value 0 means that each attribute in the dataset has been used by the model at least once (t=a); value 1 means no attribute has been used (t=0). Compactness here is related to how many attributes the model considered relevant to describe the concept, in relation to the total number of attributes. A value close to 1 would indicate the knowledge is more compact (according to the model), being the model considered simpler than one whose compactness is close to 0, in which case more attributes were needed to represent the concept.This measure is especially important in gene expression datasets, that typically have thousands of attributes, but only a few of them are really relevant. However, the fact that the tree has used just a few attributes, indicating the knowledge involved is compact, does not mean it has chosen the correct attributes (the most relevant to the problem).One could say that cohesion and compactness are related. If there are only a few leaves in the tree, we would expect few attributes in the model and vice versa. That is, the greater the cohesion, the greater the compactness, and the other way around. One way of combining them is to take the geometric mean betweencohesionandcompactness. Geometrically, the resulting value would be the side size of the square with the same area as the rectangle whose sides are represented by the two components given above. The bigger the square, the simpler the concept could be considered. We define cohesion–compactness as:(3)Cohesion--Compactness=Cohesion×CompactnessIn Fig. 1, three examples of trees are shown, from which we will exemplify how to compute the proposed measures in Table 1.In order to assess the alterations we had implemented, we designed an experiment that considered J48, and different versions of windowing. Such versions resulted from all possible combinations among the following options the algorithm started to offer: whether to use the confidence criterion, whether to use weighted errors, whether to replace the window errors by the estimated errors, and whether to prune the intermediary trees. All other parameters of the algorithm were kept with their default original values. This way, 16 different versions of windowing were evaluated (see Table 2). Whenever feasible, the J48 algorithm (unique tree induced without windowing) was also compared to the different windowing settings (in these cases, 17 different inducers participated in that experiment).Forty-one real gene expression datasets were tested, some coming from microarray studies, others coming from SAGE studies. Their description can be found at the Appendix A.For each inducer applied to each dataset, 10-fold cross-validation [37] was used. The performance measures taken into consideration were: accuracy, AUC, tree height, tree size, size of the window used to build the best classifier found, cohesion, compactness, cohesion/ compactness relation, and training time.For the significance test, we used [38], a non-parametric technique based on ranks, and widely spread in the ML community. For the case of rejecting the null hypothesis (which states that differences in the data are obtained by chance), a post-hoc test is necessary to check in which classifier pairs the differences actually are [39]. Again we chose a non-parametric approach, and the p-Values found for the possible pairs were adjusted according to [40]. The tests considered all possible pairs of inducers, i.e., multiple comparisons with no control. The most important advantage of choosing a non-parametric approach is not making the assumption of any prior probability distribution for the variables being considered (ANOVA, for example, assumes a normal distribution).The experiments reported here were performed in Weka [29]. Instead of C4.5 (C language implementation), we have used J48 (Java implementation), and since Weka does not provide any implementation of windowing, a Weka’s Java class was implemented as a meta-inducer, following its standards. The implementation has all the alterations we are proposing and allows the choice of any inducer available in Weka to be used as the base inducer (Algorithm 1, Line 5 as well as Algorithm 2, Line 6), provided that it works with classification problems. For our purposes, the base inducer was J48.In fact, we have implemented a different version of J48, for two reasons: to make the model provide some performance measures we were proposing; to be able to build an unpruned tree that we could prune later without having to rebuild it from scratch (in Weka, one cannot prune a J48 tree without starting the process over again). Other than that, our reimplementation of J48 is identical to its original version. This inducer was also used with its default parameters.The statistical tests were implemented and performed using The R Project for Statistical Computing.22https://www.r-project.org/ .The analysis presented here considered a significance level of 5%. In Table 2, we show the correspondence between the different configurations of windowing, and the notation adopted to refer to them. For those tables showing all pairwise comparisons using the Friedman and post-hoc test for a given variable (metric) in the next section, only the uppermost right triangle is shown in the matrix, because it is symmetric. The symbol ○ in a cell indicates no difference was found between the inducer at the respective row and the inducer at the respective column; △ (▽) indicates the inducer at the row was better (worse) than the one at the column, but not significantly; ▲ (▼) indicates the inducer at the row was significantly better (worse) than the one at the column.

@&#CONCLUSIONS@&#
