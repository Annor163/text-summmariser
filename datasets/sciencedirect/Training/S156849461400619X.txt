@&#MAIN-TITLE@&#
A new reverse reduce-error ensemble pruning algorithm

@&#HIGHLIGHTS@&#
An interesting RRE pruning algorithm incorporated with the operation of subtraction is proposed in this work.The WSM is chosen and its votes are subtracted from the votes made by those selected components.The backfitting step of RE algorithm is replaced with the selection step of a WSB in RRE.The problem of ties might be solved more naturally with RRE.Soft voting approach is employed in the testing to RRE algorithm.

@&#KEYPHRASES@&#
Neural network ensemble,Machine learning,Pattern recognition,Classifier,Reduce-Error (RE) pruning,Reverse Reduce-Error (RRE) pruning,

@&#ABSTRACT@&#
Although greedy algorithms possess high efficiency, they often receive suboptimal solutions of the ensemble pruning problem, since their exploration areas are limited in large extent. And another marked defect of almost all the currently existing ensemble pruning algorithms, including greedy ones, consists in: they simply abandon all of the classifiers which fail in the competition of ensemble selection, causing a considerable waste of useful resources and information. Inspired by these observations, an interesting greedy Reverse Reduce-Error (RRE) pruning algorithm incorporated with the operation of subtraction is proposed in this work. The RRE algorithm makes the best of the defeated candidate networks in a way that, the Worst Single Model (WSM) is chosen, and then, its votes are subtracted from the votes made by those selected components within the pruned ensemble. The reason is because, for most cases, the WSM might make mistakes in its estimation for the test samples. And, different from the classical RE, the near-optimal solution is produced based on the pruned error of all the available sequential subensembles. Besides, the backfitting step of RE algorithm is replaced with the selection step of a WSM in RRE. Moreover, the problem of ties might be solved more naturally with RRE. Finally, soft voting approach is employed in the testing to RRE algorithm. The performances of RE and RRE algorithms, and two baseline methods, i.e., the method which selects the Best Single Model (BSM) in the initial ensemble, and the method which retains all member networks of the initial ensemble (ALL), are evaluated on seven benchmark classification tasks under different initial ensemble setups. The results of the empirical investigation show the superiority of RRE over the other three ensemble pruning algorithms.

@&#INTRODUCTION@&#
It has been extensively reported in the literature that pooling together complementary classifiers is a desirable strategy to construct robust classification systems with good generalization performance [1–3]. Remarkable improvement in generalization performance has been observed from ensemble learning in a broad scope of applications, for example: face recognition [4], optical character recognition [5], scientific image analysis [6,7], medical diagnosis [8,9], financial time series prediction [6], military purposes [10], intrusion detection [11] etc.Despite its remarkable performance, a main disadvantage of ensemble learning is that, generally, it is necessary to combine a large number of classifiers to make certain that the error converges to its asymptotic value. This brings on large computational requirements, including the training costs, the storage needs and the prediction time. And moreover, a large amount of communication costs are required, when classifiers are distributed over a network. To alleviate these drawbacks, various ensemble pruning algorithms have been proposed [12–20].However, the problem of selecting the subset of classifiers that has the best generalization performance, i.e. ensemble pruning, has proven to be an NP-complete problem [21,22]. Assuming that the generalization capability can be estimated based on some quantity measured on the pruning set, selective ensemble is a combinatorial search problem whose complexity grows exponentially with the size of the initial ensemble, since for an ensemble of T base classifiers, the number of nonempty subsets is 2T−1. Therefore, finding the exact solution using enumerative algorithm is unfeasible for typical ensemble sizes. To solve this problem, it is proposed to use approximate algorithms that, with high probability, select near-optimal subensembles. In particular, genetic algorithms (GAs) [23,24] and semidefinite programming (SDP) [25] have been employed to address the problem of ensemble pruning. Despite the fact that the computational complexities of these algorithms are not exponential in the size of the initial ensemble any more, their computational costs are still rather large [26].Several efficient ensemble pruning algorithms, which are based on greedy selection strategy in the space of subensembles, report good classification results and desirable generalization performances [12–14,17]. These algorithms start with an empty (or full) initial ensemble and explore the space of different subsets by iteratively broadening (or shrinking) the initial ensemble by an individual classifier. The greedy exploration is guided by an evaluation measure in terms of either the classification accuracy or the diversity of the candidate subsets, which is the main ingredient of a greedy ensemble pruning algorithm and differentiates those algorithms that fall into this category. Some evaluation measures that have been successfully employed to guide the greedy selection process include Reduce-Error (RE) pruning [14], Kappa pruning [14], Complementarity Measure (CC) [17], Margin Distance Minimization (MDSQ) [17], Orientation Ordering (OO) [18], Boosting-Based pruning (BB) [27], and Uncertainty Weighted Accuracy (UWA) [28]. A more informative and thorough literature survey about existing ensemble pruning algorithms has been given in Section 2 of this paper.Although greedy algorithms possess relatively high efficiency, as they only consider a very small subspace within the entire solution space, they might receive suboptimal solutions of the ensemble pruning problem [12–14,17,21,27]. Besides, another notable disadvantage of almost all the existing ensemble pruning algorithms, including greedy ones, consists in that they hastily discard all of the classifiers which are not selected into the pruned ensemble, causing a waste of useful resource and information. Since all the models in the initial ensemble are generated through training phase, which requires rather heavy consumptions of computing time and storage space. And moreover, in most cases, the classifiers which are not selected into the pruned ensemble, and therefore will be abandoned, constitute the majority of the component networks in the initial ensemble. From this point of view, the waste of resource and information is impressive and considerable.Aiming at ameliorating the above mentioned drawbacks, an interesting greedy Reverse Reduce-Error (RRE) pruning algorithm incorporated with subtraction operation is proposed in this paper. Different from all the other currently existing ensemble pruning algorithms, in RRE, the classifiers not selected into the pruned ensemble are not simply and hastily abandoned. In contrast, the RRE algorithm makes the best of the defeated candidate networks in a way that, the Worst Single Model (WSM) is selected, which gets the largest pruned error within the set of defeated networks. And then, in the testing phase to the pruned ensemble, the opinion of WSM is considered and taken. In particular, the votes made by the WSM are subtracted from the votes made by those selected components within the pruned ensemble. The rationale of this step of vote subtraction lies in that, for most cases, the WSM might make a wrong predictive decision for the test sample. Therefore, it is expected that subtracting the votes made by WSM from the ones made by those selected members will increase the classification accuracy and enhance the generalization performance of the pruned ensemble furthermore.And in the classical Reduce-Error (RE) pruning algorithm, the final selection operation is implemented according to the desired amount of pruning, where the first υ classifiers in the reordered sequence are selected [14]. While the subensemble selection operation of the proposed RRE algorithm is implemented differently, which is achieved based upon the validated error of all the available sequential subensembles on the pruning set.In the original RE algorithm, backfitting is applied after each step of incorporation of a new component network. Backfitting sequentially attempts to replace one of the selected networks by another network not yet included in the subensemble. A replacement is made if a network that reduces the subensemble validated error is found in the pool of unselected networks. If one or more networks are replaced, then backfitting is applied repeatedly with a limit of 100 iterations. It is reported that, in bagging ensembles, when the training set is used as the pruning set, backfitting does not significantly reduce the generalization error of the pruned ensembles [18]. And another severe defect of backfitting consists in its significant increase in the computational cost of the original RE pruning algorithm [26]. In contrast, in RRE pruning algorithm, backfitting is replaced with the selection step of a WSM based on its performance on the pruning set.In some works, e.g. [26], ties are resolved by discarding the votes of the last classifiers included in the ensemble, one at a time, until the tie is broken. In the proposed RRE algorithm, the votes made by the WSM are subtracted, which might solve the problem of ties more reasonably and naturally.Besides, in the testing phase of RRE algorithm, soft voting technique is employed [29], viz., the voting process is implemented by the summation of the computing results of all the selected models and the subsequent subtraction of the result of the WSM, and the final classification decision is made according to the results of this soft voting.Moreover, we have proposed a novel Ensemble Pruning method based on BackTracking algorithm (EnPBT) in our previous work [30]. In comparison with other pruning methods including greedy algorithms, the pruned ensemble achieved with EnPBT algorithm generally possesses significantly stronger classification and generalization performance. However, we find an obvious defect in EnPBT that, the definition of its solution space contains many redundant solution vectors. And naturally, the solution space tree of EnPBT also contains many redundant solution vectors. This causes a number of redundant explorations of EnPBT algorithm, and finally affects its entire searching efficiency.Aiming at improving the above mentioned defect of EnPBT algorithm, we further proposed a Modified Backtracking Ensemble Pruning Algorithm (ModEnPBT) [30]. In contrast, its solution space is compact with no repeated solution vectors. And naturally, its solution space tree is also concise, having no redundant solution vectors. Therefore, it possesses relatively higher searching efficiency in comparison with EnPBT algorithm.Compared with the RRE pruning algorithm proposed in this work, EnPBT and ModEnPBT algorithm aim at improving classification and generalization performance of some state-of-the-art pruning methods, including greedy algorithms, based on BackTracking algorithm [30], whereas the proposal of RRE pruning algorithm aims at ameliorating those specific drawbacks of the classical RE pruning algorithm. Consequently, their designing motivations and starting points are totally different. With regard to the comparison of their performances, the three algorithms possess similar performances. In some cases, EnPBT or ModEnPBT performs the best [30], while in other cases, RRE pruning algorithm has the optimal performances. We could make choice from these three algorithms according to the actual application problem to be addressed.The remains of this paper are structured as follows. A thorough discussion on the existing literature on the domain of ensemble pruning is provided in Section 2. Section 3 briefly reviews the classical Reduce-Error (RE) ensemble pruning algorithm. Section 4 presents the proposed Reverse Reduce-Error (RRE) pruning algorithm. Section 5 reports the results of experimental study, and from these experimental results, the final conclusions are drawn in Section 6.During the past decade, many effective ensemble pruning approaches have been proposed. Roughly speaking, these approaches can be classified into four categories: (1) ordering-based pruning; (2) clustering-based pruning; (3) optimization-based pruning; (4) other pruning methods [31,32].In the following description about the main characteristics of each category, the original ensemble is denoted asENS≡{nt(x)}t=1T; and the pruning set is denoted as Pr={(xi, yi), i=1, 2, …, NPr}, where xirepresents a vector with attribute values and yidenotes the value of the target variable.Ordering-based pruning refers to those approaches which try to order the base learners according to some standard, and only the learners in the forepart will be selected into the final pruned ensemble [31,32]. The key differentiation among the approaches of this category is the evaluation measure utilized for base learners ordering. Simply using the predictive performance of each individual learner as the evaluation measure usually cannot acquire desirable results [32–34]. Information-theoretic measures were used in [34] for the evaluation of Bayesian learners, obtaining similarly disappointing results.A diversity measure is employed in Kappa pruning for evaluation, ordering all pairs of base learners based on the κ statistic computed based on the training set [14]. Kappa pruning can be generalized easily by accepting a parameter specifying any pairwise diversity measure for either classification or regression models, in stead of the κ statistic. However, it is intuitively plausible that, two diverse pairs of learners do not necessarily lead to one diverse ensemble of four learners [26,35]. Actually, Kappa pruning has been shown to be noncompetitive for pruning ensembles produced with bagging algorithm [26].Orientation ordering ranks the base learners by increasing value of the angle between their signature vector and the reference vector, being one of the most efficient approaches for ensemble pruning, and with comparable predictive performance compared to state-of-the-art ensemble pruning methods [18,26]. Signature vector and reference vector are two crucial points in orientation ordering. For a specific classifier nt, signature vector is a vector of dimension NPr, with elements taking the value +1 if nt(xi)=yi, and −1 if nt(xi)≠yi. Whereas the signature vector of an ensemble equals the average signature vector of all classifiers in the ensemble. It indicates the capability of an ensemble to correctly classify each pruning sample by majority voting. And the reference vector is a vector perpendicular to the ensemble signature vector, which corresponds to the projection of the first quadrant diagonal onto the hyper-plane defined by the ensemble signature vector. Orientation ordering ranks the classifiers by increasing value of the angle between their signature vector and the reference vector. In essence, this method of ordering gives priority to models that could correctly classify those samples that are incorrectly classified by the whole ensemble [32].Another important issue in ordering-based pruning algorithms concerns the determination of the pruning rate. One method uses a fixed user-specified pruning rate. If the goal of pruning is to improve efficiency, then this method can be used so as to acquire the desired pruning rate, which is dictated by constraints of memory or speed in the application problem. A second method dynamically determines the pruning rate according to the predictive performance of ensembles of different size [32].Clustering-based pruning includes those approaches which employ some types of clustering algorithms to determine a number of representative prototype base learners to constitute the pruned ensemble [31,32]. Therefore, the most important issue for the algorithms of this category is the choice of an appropriate clustering algorithm. Clustering algorithms including hierarchical agglomerative clustering [15], k-means [36,37], deterministic annealing [38] etc., have been used in existing researches.Clustering algorithms are based on the notion of distance. Therefore, a second fundamental issue for clustering-based pruning algorithms is the choice of a proper distance measure. Research works in [15] used the possibility that classifiers do not make coincidental errors in an independent pruning set as a distance measure, which actually equals to one minus the double fault diversity measure. Authors of [36,37] used the Euclidean distance in the training set as their distance measure. Virtually, any distance measure fit for nominal or numeric outputs could be utilized [35].The third important issue concerns how to prune each cluster resulted from the adopted clustering algorithm. The method used in [38] trains an individual model for each cluster, using the cluster centers as the target variable values. Another interesting method proposed in [15] chooses from each cluster an individual classifier which is most distant to the remaining clusters, whereas the method used in [36] selects the most precise model from each cluster. The method proposed in [37] iteratively deletes one model from the ensemble, from the least to the most precise, till the precision of the whole ensemble starts to decline.The determination of the proper number of clusters is the fourth noteworthy issue. It could be decided according to the method performance on a validation set [36]. The method used in [37] gradually increases the number of clusters until the variance between the cluster centers starts to drop off.Optimization-based pruning approaches formulate the ensemble pruning problem as an optimization problem which tries to find the subensemble of base learners that maximizes or minimizes an objective function correlated to the generalization capability of the pruned ensemble [31].The Gasen-b method is a kind of optimization-based pruning approach, which performs stochastic search in the space of learner subsets with a genetic algorithm [24]. The ensemble is represented as a bit string, with one bit corresponding to one individual learner. Learners are included in or excluded from the ensemble according to the value of each corresponding bit [24,32]. Gasen-b performs standard genetic operations including mutations and crossovers, and uses default values for the parameters of the genetic algorithm. The fitness function for an individual S⊆ENS is set as the accuracy of S on the pruning set Pr by majority voting.The ensemble pruning problem was formulated as a mathematical problem by Zhang et al. in [25] and semi-definite programming (SDP) techniques were applied to solve the problem, which also belongs to the category of optimization-based pruning approaches. Specifically, the ensemble pruning problem is formulated as a quadratic integer programming problem, which seeks for a fixed-size subensemble of k classifiers with the least classification errors and the most divergence. It is found by the authors that this problem is similar to the max cut with size k problem, and they subsequently solve it approximately with an algorithm based on SDP.Hill climbing search, being another optimization-based pruning approach, greedily selects the next state to visit from the neighborhood of the current state. Similar to ordering-based approaches, the crucial component that differentiates hill climbing ensemble pruning techniques is the evaluation measure, which can be divided into two main categories: performance-focused and diversity-focused [32,39]. For a detailed review about the hill climbing ensemble pruning methods, please refer to literature [32].Other pruning algorithms include those algorithms that do not fall into any one of the previous three categories [32,40,41]. The first type of these algorithms directly selects a subensemble of classifiers based on statistical procedures. For example, Tsoumakas et al. prune an ensemble of heterogeneous classifiers using statistical tests which determine whether the differences in predictive performance among the classifiers of the ensemble are significant. Only the classifiers with significantly superior performance than the rest are retained in the final pruned ensemble [20,42].The second type of algorithms is based on reinforcement learning. For instance, Partalas et al. perform ensemble pruning with a reinforcement learning approach [21,43]. Specifically, the problem of pruning an ensembleENS≡{nt(x)}t=1Tis characterized as an episodic task, where an agent takes T sequential actions, each one corresponding to either the inclusion or exclusion of classifier nt, t=1, …, T from the final pruned ensemble. Then, the Q-learning algorithm [44] is utilized to approximate the optimal policy for this task.Before a detailed presentation of the Reduce-Error (RE) pruning algorithm, it is necessary to introduce some notations. Ensemble methods generate a collection of classifiers during the training phase, which are combined to produce a final decision by either weighted or simple majority voting, stacking, or some other combination approaches in the test phase. The result of combining the decisions of the classifiers in an ensembleENS≡{nt(x)}t=1Tusing simple majority voting is(1)HENS(x)=argmaxy∑t=1TI(nt(x)=y),y∈Ywhere I(·) is an indicator function (I(true)=1andI(false)=0), nt(x) is the class label predicted by the tth member net of the ensemble, and Y={1, 2, …, l} is the set of all possible class labels [26].The Reduce-Error (RE) pruning algorithm was proposed by Margineantu and Dietterich in [14]. The first classifier selected into the subset is the one with the highest classification accuracy estimated on the pruning set Pr={(xi, yi), i=1, 2, …, NPr}. The remaining classifiers are then sequentially selected into the subensemble, one by one, in such a way that the classification accuracy of the intermediate subensemble estimated on the pruning set, is as high as possible. Specifically, the subensemble Suis built by combining into Su−1 the classifier(2)su=argmaxj∑(x,y)∈PrI(HSu−1∪nj(x)=y),y∈Ywhere the index j∈ENS\Su−1 traverses the classifiers that have not already been selected up to that iteration. Consequently, the set of classifiers with original random order n1, n2, …, nTis replaced by an ordered sequence of member netsns1,ns2,…,nsT, where siis the sequence number in the original randomly ordered ensemble of the classifier that occupies the ith position in the newly ordered ensemble resulted from Reduce-Error (RE) pruning algorithm [14,26].At last, depending upon the desired amount of pruning, the first υ classifiers in the sequence are selected. If the goal is to achieve the highest possible accuracy, υ should correspond to the minimum classification error in the test set. Therefore, determining the location of this minimum error using information only from the training set is a rather difficult task because testing and training error minima can occur at different subensemble sizes. However, the minimum observed in the ensemble test error curves is fairly broad, which means that it is easy to improve the results of ensemble by early stopping in the combination procedure of ordered ensembles. A heuristic approach that performs well in ensembles of standard CART trees terminates the combination procedure after 20–40% of the classifiers have been integrated into the ordered ensemble. In their work, ties are solved by casting off the votes of the last classifiers incorporated in the ensemble, one at a time, until the tie is broken [17,26,27].Moreover, in the original RE algorithm proposed in [14], backfitting is applied after each step. Backfitting sequentially attempts to replace one of the selected classifiers by another classifier not yet included in the ensemble. A replacement is made if a classifier that reduces the subensemble error is found in the set of unselected classifiers. If one or more classifiers are substituted then backfitting is applied repeatedly with a limit of 100 times. It is reported that, in bagging ensembles, when the training set is used as the pruning set, backfitting does not significantly improve the generalization performance of the pruned ensemble [18]. And another significant drawback of backfitting is that it dramatically increases the execution time of the algorithm [26].The first stage of Reverse Reduce-Error (RRE) pruning algorithm is identical with the component nets reordering process carried out according to the classical Reduce-Error (RE) pruning algorithm [14]. The original random order of the classifiers in the initial ensemble is reordered so that the classifiers that are expected to perform better when combined are aggregated first. And in the classical RE, the final selection operation is implemented according to the desired amount of pruning, where the first υ classifiers in the reordered sequence are selected [14]. In contrast, the subensemble selection operation of the proposed RRE is implemented differently. The near-optimal solution of pruned ensemble is obtained based upon the validated error of all the available sequential subensembles on the pruning set.In detail, after the implementation of RE pruning algorithm, say, the set of classifiers with original random order n1, n2, …, nTis replaced by an ordered sequence of member netsns1,ns2,…,nsT, where siis the original label in the original randomly ordered ensemble of the classifier that occupies the ith position in the newly ordered ensemble, then the validated errors of all the available sequential subensembles, i.e.,{ns1},{ns1,ns2},…,{ns1,ns2,…,nsi},…,{ns1,ns2,…,nsT}on the pruning set are calculated, and the subensemble with the lowest validated error is chosen as the near-optimal solution of the pruning problem.After the above mentioned near-optimal subensemble has been determined, those unselected component nets of the initial ensemble are not simply and wholly abandoned, which is different from RE algorithm and most of the other classical ensemble pruning algorithms [12–14,17,18,28]. And neither backfitting is applied in the proposed RRE algorithm. In RRE, after the near-optimal subensemble has been selected, the one with the worst classification accuracy on the validation set is chosen as the worst single model (WSM). And then, in the following testing phase, after the regular voting process has been carried out, an additional reverse voting process is implemented, where those votes made by the WSM are subtracted reversely from the votes made by the selected members in the pruned ensemble. It is intuitively expected that, for most cases, the voting decision made by the WSM might be a wrong decision. Therefore, it is expected that subtracting those votes made by WSM will reversely boost the classification accuracy of the pruned ensemble. The schema of RRE ensemble pruning algorithm is illustrated in Fig. 1. Experimental studies carried out in this work verify the validity and effect of the RRE pruning algorithm.According to the descriptions about the backfitting process applied in the original RE algorithm [14] in Section 3, obviously, the selection of a WSM is remarkably more efficient. Moreover, ties could be solved more naturally with the implementation of RRE. If backfitting is not employed, then all the unselected member nets will be simply abandoned in RE pruning algorithm, which is an undesirable waste of useful resource and information. It is clearly that all the member nets, either selected or unselected into the final pruned ensemble, are computational results from networks training procedure.In order to further advance the generalization ability of the pruned ensemble, soft voting technique is applied in the testing phase. Namely, the outputs of all the selected component networks are summed together at first, without taking votes. And then, the output of the WSM is subtracted from this summation. Finally, the classification decisions are reached according to the soft voting results.LetENS≡{nt(x)}t=1Tdenotes the original ensemble to be pruned, and S⊆ENS denotes the pruned ensemble, containing all the component nets which have been selected during the pruning process. Let wsm denotes the network index of the Worst Single Model (WSM) resulted from the RRE ensemble pruning process, then(3)wsm=argmaxj∑(x,y)∈PrI(nj(x)≠y),nj∈ENS\Swhere ENS\S represents the subtraction of set S from set ENS.For each testing sample (xtest, ytest), output vector resulted from soft voting is calculated as:(4)Soft voting output vector(xtest)=∑ni∈Sni(xtest)−nwsm(xtest)And then:(5)[maxValue,maxClassIndex]=max(soft voting output vector(xtest))where the function max takes the output vector of soft voting as input, and outputs the maximum value in the output vector in maxValue, and outputs the index of the item which gets the maximum value in the output vector in maxClassIndex. The final classification decision of the RRE algorithm for the specific testing sample (xtest, ytest) is made as the class of maxClassIndex calculated from the above equations, accordingly.With the implementation of the soft voting technique to our RRE algorithm, we do not firstly transform the networks outputs for each testing sample into class labels, and then take votes according to these resulted class labels. In contrast, the outputs of all the selected component networks are summed together by the simple superposition method at first, without transformation to their corresponding class labels and without taking votes, either. And then, the outputs of the WSM are subtracted from this superposition result, also without transformation of the outputs of WSM to their corresponding class labels. Finally, the classification decisions of the RRE algorithm are reached according to the soft voting results.The calculation process of the soft voting technique is clearly shown in Eq. (4). The reason why such technique comes up with significant advantages could be explained as: in an ensemble system, if we at the first step transform the component networks outputs for each testing sample into class labels, and then take votes according to these resulted class labels, the incorrect outputs of those mistaken networks will be amplified into wrong votes. These wrong votes would consequently cause harm to the classification accuracy of the whole ensemble system. By contrast, the soft voting technique could alleviate this problem.Actually, transforming the outputs of component networks into class labels before taking votes destroys the real distribution of networks outputs. The maximum value of specific output node in the component networks of an ensemble is amplified as one, while the output values of other output nodes are cleared to zeros. It seems that the network output signals are distorted by this step of transformation from real outputs to class labels, and the incorrect outputs of the mistaken networks are amplified.In contrast, the soft voting technique alleviates this problem and maintains the real distribution of networks outputs. That is fundamentally why adopting such soft voting technique comes up with significant advantages.In some works, e.g. [26], ties are resolved by discarding the votes of the last classifiers included in the ensemble, one at a time, until the tie is broken. While in RRE algorithm, the real outputs of all the selected component networks are firstly superposed together, without transformation to their corresponding class labels and without taking votes, either. And then, the real outputs of the WSM are subtracted from this superposition result, either without transformation of the outputs of WSM to their corresponding class labels. And in the end, the classification decisions of the RRE algorithm are reached according to the soft voting results. Therefore, by combining the soft voting technique together with the design idea of WSM, we solve the problem of ties more reasonably and naturally. After the soft voting process and the reversed subtraction process of the real outputs of the WSM, the probability that a tie occurs will be decreased to an extremely low level.The formal procedure of RRE algorithm is displayed in Algorithm 1.Algorithm 1The RRE Ensemble Pruning AlgorithmInput: The initial ensemble, the number of component nets in the initial ensemble, i.e. NumComponentNets, a pruning set Pr and a testing set.Output: The near-optimal pruned ensemble based on the pruning set, i.e. RREpruneEnsemble; the Worst Single Model (WSM) and the test error count RRETestErrorCount.1:BeginSelect the first component net incorporated to the ensemble as the one with the lowest validation error estimated on the pruning set Pr.Select the remaining component nets, which are sequentially incorporated to the ensemble, one at a time, in such a way that the classification error of the partial subensemble, estimated on the pruning set Pr, is as low as possible. {Comments: From the implementation of the above two steps, an ordered sequence of component netsns1,ns2,…,nsTis achieved.}Calculate the validated errors of all the possible sequential subensembles, i.e.,{ns1},{ns1,ns2},…,{ns1,ns2,…,nsi},…,{ns1,ns2,…,nsT}on the pruning set Pr, and the subensemble with the lowest validated error is chosen as the near-optimal solution of the pruning problem, i.e. RREpruneEnsemble.From those unselected member nets of the initial ensemble, the worst single model (WSM) is selected as the one with the worst classification performance on the pruning set Pr. {Comments: The following steps buildup the testing stage.}Initialize SoftClassLabelCount to be zero vector, and RRETestErrorCount=0.For TestSampleIndex=1: NumTestSamplesFor RREpruneEnsComIndex=1: NumRREpruneEnsComNetsCalculate the network output of RREpruneEnsemble[RREpruneEnsComIndex] for TestSample[TestSampleIndex] and assign the value to variable NetworkOutput;SoftClassLabelCount=SoftClassLabelCount+NetworkOutput;End ForCalculate the network output of WSM for TestSample[TestSampleIndex] and assign the value to variable WSMOutput;SoftClassLabelCount=SoftClassLabelCount - WSMOutput;[mValue, ENSlabel]=max(SoftClassLabelCount);If ENSlabel ≠ class label of TestSample[TestSampleIndex]RRETestErrorCount=RRETestErrorCount+1;End IfEnd ForEndAccording to our analysis, the computational complexity of the classical RE algorithm [14,26] in the pruning stage equals:(6)NVP*NCN*c+NCN*NVP*c*∑i=2NCNi−NVP*c*∑i=2NCN(i−1)*i=NVP*NCN*c+16NCN3+12NCN2−23NCN*NVP*c=Θ(NVP*NCN3)where NVP denotes the number of validate patterns in the prune set, NCN denotes the number of component nets in the original ensemble, and c represents the time requirement for a component net to calculate its output result for one pattern, which is a small enough positive number.In comparison, the computational complexity of the proposed RRE ensemble pruning algorithm in the pruning stage equals:(7)NVP*NCN*c+NCN*NVP*c*∑i=2NCNi−NVP*c*∑i=2NCN(i−1)*i+NVP*c*(NCN+NCN2)/2+NVP*O(NCN)*c=NVP*NCN*c+16NCN3+12NCN2−23NCN*NVP*c+NVP*c*(NCN+NCN2)/2+NVP*O(NCN)*c=Θ(NVP*NCN3)Therefore, the increment of computational complexity of RRE compared with RE algorithm in the pruning stage equalsNVP*c*(NCN+NCN2)/2+NVP*O(NCN)*c. Compared to the order of growth of the running time of both the two algorithms, i.e., Θ(NVP*NCN3), the increment of computational time is so small that it can be neglected when the values of NVP and NCN are sufficiently large. And what is more is that, the above computational complexity of the classical RE algorithm does not take into consideration the process of backfitting, which is a rather time consuming process.Moreover, the computational complexity of the classical RE algorithm [14,26] in the testing stage equals:(8)NTP*NSN*c=Θ(NTP*NSN)where NTP denotes the number of test patterns in the test set, NSN denotes the number of selected nets in the final pruned ensemble, and c represents the time requirement for a component net to calculate its output result for one pattern, which is a small enough positive number.In comparison, the computational complexity of the proposed RRE ensemble pruning algorithm in the testing stage equals:(9)NTP*(NSN+1)*c=Θ(NTP*NSN)Therefore, the computational complexity of the proposed RRE ensemble pruning algorithm in the testing stage is also the same order as that of the RE algorithm.With regard to one of the two baseline methods, i.e., BSM [28], which selects the best single model in the initial ensemble, according to the performance of the models on the pruning set, its computational complexity in pruning stage equals Θ(NVP*NCN), and its computational complexity in testing stage equals Θ(NTP). The other baseline method, i.e., ALL [28], retains all member networks of the initial ensemble and does not perform the ensemble pruning process at all. Therefore, it does not require any running time for ensemble pruning. The computational complexity of ALL in testing stage equals Θ(NTP*NCN), which is the most among that of the four algorithms. However, it can be very clearly observed from the experimental results that, the classification performances achieved by BSM and ALL are often remarkably inferior to that achieved by RRE and RE algorithms. Besides, if we adopt the baseline method of ALL as the ensemble learning algorithm, we just get back where we start. The method ALL exactly equals to the original entire ensemble without any pruning operations.One last point about the comparison of computational complexity of RRE, RE [14,26], BSM and ALL, their computational complexities in training stage are obviously identical with each other, as all of them generate an initial ensemble of identical size in their training stage.The dataset was donated by Donato Malerba of Dipartimento di Informatica, University of Bari, Italy. This data set has been used to try different simplification methods for decision trees. A summary of the results can be found in [46]. The problem consists in classifying all the blocks of the page layout of a document that has been detected by a segmentation process. This is an essential step in document analysis in order to separate text from graphic areas. Indeed, the five classes are: text, horizontal line, picture, vertical line and graphic. For a detailed presentation of the problem see [47]. All instances have been personally checked so that low noise is present in the data. The 5473 examples come from 54 distinct documents. Each observation concerns one block. All attributes are numeric. Data are in a format readable by C4.5.The dataset was created by Tactile Srl, Brescia, Italy and donated in 1994 to Semeion Research Center of Sciences of Communication, Rome, Italy, for machine learning research [45]. In the dataset, 1593 handwritten digits from around 80 persons were scanned, stretched in a rectangular box 16×16 in a gray scale of 256 values. Then each pixel of each image was scaled into a bolean (1/0) value using a fixed threshold. Each person wrote on a paper all the digits from 0 to 9, twice. The commitment was to write the digit the first time in the normal way and the second time in a fast way. This dataset consists of 1593 records and 256 attributes. Each record represents a handwritten digit, originally scanned with a resolution of 256 grays scale. Each pixel of the each original scanned image was first stretched, and after scaled between 0 and 1. Finally, each binary image was scaled again into a 16×16 square box.The date calculation task is an assignment to classify dates (e.g., September 21, 2007) into seven classes according to the specific day of the week which they fall on. It is an example of task which easily leads to overfitting in MFNs trained by BP. Norris described this problem in his paper and concluded that BP was not able to learn this task, unless it was decomposed into three easier subtasks beforehand by human experts [48–50]. The dates collected in the data set were chosen from July 1, 1970 to April 1, 2004. The input for the networks is composed of 78 inputs representing the year (35 nodes), the month (12 nodes) and the day of the month (31 nodes).The gene benchmark data set was extracted from the Proben1 benchmark collection [51]. The data set detects intron/exon boundaries (splice junctions) in nucleotide sequences. From a window of 60 DNA sequence elements (nucleotides), we can decide whether the middle is either an intron/exon boundary (a donor), or an exon/intron boundary (an acceptor), or none of these. The data set features 3000 training instances and 190 test instances.E. Alpaydin and Fevzi. Alimoglu create a digit database by collecting 250 samples from 44 writers [45]. They use a WACOM PL-100V pressure sensitive tablet with an integrated LCD display and a cordless stylus. The input and display areas are located in the same place. Attached to the serial port of an Intel 486 based PC, it allows them to collect handwriting samples. The tablet sends x and y tablet coordinates and pressure level values of the pen at fixed time intervals of 100ms. These writers are asked to write 250 digits in random order inside boxes of 500 by 500 tablet pixel resolution. Subjects are monitored only during the first entry screens. Each screen contains five boxes with the digits to be written displayed above. Subjects are told to write only inside these boxes. If they make a mistake or are unhappy with their writing, they are instructed to clear the content of a box by using an on-screen button. The first ten digits are ignored because most writers are not familiar with this type of input devices, but subjects are not aware of this. In their study, they use only (x, y) coordinate information. The stylus pressure level values are ignored. The data file pendigits.tra contains 7494 training instances, while the data file pendigits.tes contains 3498 testing instances. Each instance is composed of 16 input attribute and one class attribute. For each attribute, all input attributes are integers in the range 0.100, while the last attribute is the class code 0.9.Seismic bumps dataset was created and donated by Marek Sikora and Lukasz Wrobel. The data describe the problem of high energy seismic bumps forecasting in a coal mine. Data come from two of long walls located in a Polish coal mine. The dataset contains 2584 instances. Each instance is composed of 18 input attribute and one class attribute. The class distribution is 6.6% for the class of hazardous state, and 93.4% for the class of non-hazardous state.MiniBooNE particle identification data set was created and donated by Byron Roe, department of Physics University of Michigan. It is a multivariate dataset containing 130065 instances in Physical area. This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The submitted file is set up as follows. In the first line is the number of signal events followed by the number of background events. The signal events come first, followed by the background events. Each line, after the first line has the 50 particle ID variables for one event.In our experiments for this work, 5BBC-ICBP-ES, 6BBC-ICBP-ES and 7BBC-ICBP-ES are utilized as the three initial ensembles before pruning, where n-Bits Binary Coding ICBP Ensemble System (nBBC-ICBP-ES) is proposed in our previously published paper [54]. It is constructed by two layers of sub-systems, with the first layer of sub-system consisting of many small ICBP components, and the second layer of sub-system being an aggregation sub-system [54]. Those many small ICBP components of the first layer of our sub-system are constructed utilizing the particular property of ICBP network model [55].Here, the ICBP network refers to the Improved Circular Back-Propagation (ICBP) neural network model developed by us in one of our earlier works [55]. ICBP differentiates from the standard BP network in that an extra anisotropic input node is added to the input layer. On account of the extra added anisotropic input node, ICBP acquires an advantageous property which SBP network does not have, i.e., by assigning different arrangement of values “1” and “−1” to the connecting weights between the extra input node and all the hidden nodes, we can directly develop a set of heterogeneous ICBP networks with diversified hidden layer activation functions [55].Specifically, nBBC-ICBP-ES is built by four steps [54]. In the first step, the number of hidden nodes of the single hidden layer, denoted as Nh, is set to be an appropriate one (e.g., 5, 6 or 7) for an ICBP root model. In the second step, the connection weights between the extra input node of a root ICBP and all its hidden nodes are assigned with every possible arrangement of binary values “1” and “−1” in a binary string of length Nh, each one forming a new ICBP component, thus totally a whole set of 2Nhsmall ICBP models is obtained. All those possible arrangements of binary values “1” and “−1” for the connecting weights between ICBPs extra input node and all the hidden nodes are just like one binary coding system of “1” and “−1”, which is just the reason why the name of n-Bits Binary Coding ICBP Ensemble System (nBBC-ICBP-ES) comes from [54]. In the third step, the whole set of small ICBP models of size 2Nhare employed to build the first layer of sub-system entirely, which are trained based on the training data set, respectively. Finally, in the second layer of aggregation sub-system, the final classification decision is made based on the outputs of those component ICBPs in the first layer of sub-system, following the majority voting rule.The reasons why we have made such a choice are because nBBC-ICBP-ES is easy to implement, and its effectiveness has been demonstrated through experiments on several benchmark classification tasks [54]. And it is expected that the research work about ensemble pruning by RRE Algorithm could improve the performance of the initial nBBC-ICBP-ES furthermore, so that a well-behaved pruned ensemble could be achieved, which is originated from our own research works. Therefore, nBBC-ICBP-ES is adopted as the initial ensemble for this work very naturally.Using the above described n-Bits Binary Coding ICBP Ensemble System (nBBC-ICBP-ES) [54,55] as the initial ensemble before pruning, the proposed RRE algorithm is compared with the classical RE algorithm [14,26] and two baseline methods. The first one selects the best single model (BSM) in the initial ensemble, according to the performance of the models on the pruning set, while the second one retains all member networks of the initial ensemble (ALL), corresponding to the two extreme pruning scenarios [28].Seven groups of experiments on benchmark classification tasks are implemented, including the task of blocks classification [45], the task of semeion handwritten digit recognition [45], the date calculation task [45,48–50], the task of gene detection [45], the task of pen-based handwritten digits recognition [45], the task of high energy seismic bumps forecasting [45,52], and the task of MiniBooNE particle identification [45,53]. More detailed description about the seven benchmark classification tasks has been provided in Section 5.1. The experiments are performed 10 times repetitively for each classification task with a different randomized ordering of its samples. For each trial running, each dataset is initially partitioned into three disjunctive subsets, i.e., training dataset, pruning dataset and testing dataset. The detailed information about the size of each subset and the number of attributes and output classes for each task are displayed in Table 1.

@&#CONCLUSIONS@&#
