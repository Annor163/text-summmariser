@&#MAIN-TITLE@&#
Partially Observable Markov Decision Processes incorporating

@&#HIGHLIGHTS@&#
Epistemic uncertainties affecting the transition matrices in a Markov process.Use of the Dirichlet distribution to describe the epistemic uncertainties.Monte-Carlo simulations in the framework of dynamic programming.Estimating Dirichlet distribution from imperfect inspection results.Estimating Dirichlet distribution from inspections performed at irregular time intervals.

@&#KEYPHRASES@&#
Uncertainty modeling,Markov process,Dynamic programming,Epistemic uncertainty,Decision analysis,Maintenance,

@&#ABSTRACT@&#
The use of Markov Decision Processes for Inspection Maintenance and Rehabilitation of civil engineering structures relies on the use of several transition matrices related to the stochastic degradation process, maintenance actions and imperfect inspections. Point estimators for these matrices are usually used and they are evaluated using statistical inference methods and/or expert evaluation methods. Thus, considerable epistemic uncertainty often veils the true values of these matrices. Our contribution through this paper is threefold. First, we present a methodology for incorporating epistemic uncertainties in dynamic programming algorithms used to solve finite horizon Markov Decision Processes (which may be partially observable). Second, we propose a methodology based on the use of Dirichlet distributions which answers, in our sense, much of the controversy found in the literature about estimating Markov transition matrices. Third, we show how the complexity resulting from the use of Monte-Carlo simulations for the transition matrices can be greatly overcome in the framework of dynamic programming. The proposed model is applied to concrete bridge under degradation, in order to provide the optimal strategy for inspection and maintenance. The influence of epistemic uncertainties on the optimal solution is underlined through sensitivity analysis regarding the input data.

@&#INTRODUCTION@&#
Most countries worldwide face the problem of ageing civil engineering infrastructures. This is especially the case in the developed countries where investment focus is increasingly shifting from extending the infrastructure to the inspection, maintenance and rehabilitation (IM&#38;R) of the existing assets. For example, government statistics in the United States show that the proportion of public non-capital spending for infrastructure increased from 39 percent in 1960 to 57percent in 1994 (CBO, 1999). Although mathematical optimization and operational research methods started to be gradually applied for maintenance optimization since the early 1940s, it is until the last two decades that maintenance optimization became a topic of highest priority for civil engineering infrastructure managers. Several researches emphasized on the usefulness of the characterization of the uncertainties (pertaining to reliability analysis and to risk-based decision problems in engineering) as epistemic or aleatory (Der Kiureghian &#38; Ditlevsen, 2009; Hofer, 1996). The decision process regarding the optimal maintenance policy is often very sensitive to the input statistical parameters (e.g. mean values, standard deviations, Markov transition matrices) of the probabilistic models (Avrachenkov, Filar, &#38; Haviv, 2001; Cranahan, 1988). However, these parameters are estimated by classical inference methods where the available data is often very limited (DeStefano &#38; Grivas, 1998; Madanat, Mishalani, &#38; Wan Ibrahim, 1995; Mishalani &#38; Madanat, 2002). This lack of information leads to second-order uncertainties due to the inevitable difference between the observed sample from which inference is made and the real population. This second-order statistical uncertainty, shrouding the real values of the parameters, could be usually quantified by several means, such as confidence intervals, Bayesian probability distributions and fuzzy sets (Corotis, 2009). Keeping in mind that exact and tractable modelingof reality is a far-fetch if not an impossible goal to achieve, it remains that a desired attribute of a mathematical model is to be able to use, as best as possible, the available information. In fact, a mathematical optimization model using only a portion of the relevant information will produce biased optimal solutions. Parametric uncertainty is one type of information that is seldom taken into account by most of the existing models for IM&#38;R optimization. Therefore, point estimators for the transition matrices in Markov Decision Processes (MDP) are simply used instead of exploiting the additional information that can be available. If, for example, a probability density function is available for the elements of MDP transition matrices, then the use of the whole information contained in such a distribution will lead to more realistic life cycle cost estimation and therefore to better IM&#38;R decisions. It will also be shown that the expected costs usually increase with full uncertainty consideration. Hence, neglecting the epistemic uncertainty leads in general to misleading low life cycle costs.In the next two sections, a literature review on the epistemic uncertainties in the transition matrices and on the dynamic programming of finite horizon MDPs or POMDPs (Partially Observable Markov Decision Processes) are presented. Next, we describe the parameters uncertainties in MDPs. Then, we propose a methodology for incorporating the epistemic uncertainty in MDPs. The paper ends with the presentation of a numerical example that considers IM&#38;R optimization management of a highway concrete bridge deck.Several studies have been suggested in the literature since the early 1950s in order to take into account the effect of uncertainties in the transition matrices. Bellman (1961) proposed an “adaptive control” of Markov chains by recognizing that the problem can be transformed into an equivalent dynamic programming problem with a completely known transition law. The state space of the equivalent problem is the Cartesian product of the original state space by the set of all probability distributions on the parameter set. This approach has the advantage of being adaptive but it suffers, as pointed out by Bellman himself, from the “dimensionality curse”; i.e. the problem becomes quickly intractable as the state space grows exponentially with higher dimensions. A customized version of this approach in the field of IM&#38;R of civil engineering structures was proposed by Durango and Madanat (2008). One can say that most of the research tackling the issue of parametric uncertainty in sequential decision optimization problems focuses on investigating robust optimization techniques (Givan, Leach, &#38; Dean, 2000; Iyengar, 2005; Nilim &#38; El Ghaoui, 2005). In the same framework, ambiguity is investigated in the context of infinite horizon MDP with finite state and action spaces where it is modeled by constraining the transition probability matrix to lie in a pre-specified polytope (Bagnell, Ng, &#38; Schneider, 2001; Satia &#38; Lave, 1973; White &#38; Eldeib, 1994). Such an approach (robust optimization) considers the optimization problem as a game between the decision maker and the nature which is supposed to be malevolent. In other terms, the worst case scenario is assumed, leading to policies which are deemed too conservative by several authors. For this reason, several studies attempted to avoid this drawback. Kuhn and Madanat (2006) used the Hurwicz criterion (Revelle, Whitlatch, &#38; Wright, 1997) which allows the decision maker to set his or her own ’optimism level’, as a number between 0 and 1. This number is used to balance the decisions by interpolating between the best case and the worst case scenarios. Delage and Mannor (2009) proposed a chance constrained MDP with a set of percentile criteria that represent the trade-off between the optimistic and the pessimistic views. However, as for the models of Kuhn and Madanat, a so-called risk of the policy level must be chosen at the discretion of the decision maker. Moreover, these approaches do not use all of available information about the epistemic uncertainty.The authors of the present paper believe that for problems which do not include hard constraints (i.e. constraints which must be met compulsory otherwise the solution found is deemed not feasible), the whole information available about the uncertainty can be taken into account by the optimization model. A particular example of optimization problems, where the constraints are soft, is the management optimization of infrastructure facilities which is subject to limited budget constraints. In this case, constraining the expected direct cost that is to be paid by the manager during each time period, to be less than or equal to the imposed budgets limits is deemed to be sufficient. In this paper, we present an extension of dynamic programming algorithms used to solve finite horizon MDPs or Partially Observable MDPs (POMDPs) in order to take into account second-order epistemic uncertainties. The whole information available about the uncertainty is taken into account by the optimization model. In addition, we propose a methodology based on the use of Dirichlet distributions which avoid, in our sense, much of the controversy dominating the literature about the different methods used to estimate the Markov transition matrices (maximum likelihood estimation, regression using state expectation, regression using state distribution, etc.). We illustrate the methodology by applying it to a Generalized Partially Observable Markov Decision Process (GPOMDP) (Faddoul, Raphael, &#38; Chateauneuf, 2009). A literature review on POMDPs and GPOMDPs is provided in the following section. Although the proposed methodology for including epistemic uncertainties in the decision process is motivated by IM&#38;R optimization, the formulation is quite general and can be relevant to any MDP.The methodology presented in this paper uses dynamic programming to solve finite horizon MDPs. This technique has been used extensively for maintenance optimization(Wang, 2002). However, a legitimate question arises about the suitability of the Markov property to describe the state evolution of civil engineering structures. Conflicting arguments are presented in literature concerning this subject. AlthoughNeves, Frangopol, and Cruz (2006) have criticized the Markov property assumption for civil engineering structures, Orcesi and Cremona (2010) have demonstrated that the homogeneous Markov assumption was justified in the case of the French national bridge stock. Whatever the case may be, a technique proposed by Robelin and Madanat (2007) allows the dynamic programming to take into account the possible non-Markovian effects of actions and/or deterioration processes.A dynamic programming model is said to be deterministic if the application of an action or a degradation process at the beginning of stage n will result in a precisely known state of the system at the beginning of stage n + 1. On the other hand, in probabilistic dynamic programming, the application of an action or a degradation process at the beginning of stage n will lead to a probabilistic distribution of the state of the system at the beginning of the next stage (Pham &#38; Wang, 1996).In a maintenance model, large uncertainties are associated with the results of any maintenance action and/or degradation process, due to two main factors:1imperfectness of maintenance actions;stochastic nature of the degradation process between the application time of the maintenance action and the beginning of the next stage.Due to the abovementioned uncertainties, probabilistic dynamic programming models were used in the majority of maintenance optimization problems.However, classical probabilistic dynamic programming models in maintenance assume perfect inspections at the beginning of each time period. This type of models suffers from two main drawbacks; namely:1The inspections are assumed to be perfect and this is rarely the case;The optimization of inspection planning (which is vital because inspections have usually significant costs) is not possible.The use and development of POMDPs was essentially driven by the goal of dealing with the first one of these two shortcomings of classical probabilistic MDPs.In a POMDP (Eckels, 1968; Monahan, 1982), the state of the system at the beginning of each time period cannot be fully observed. Thus, the manager of the system must rely on a characterization of a partially observed state; i.e. a belief state which is usually described by probabilistic distributions. In a classical POMDP, the “belief state” of the system at the beginning of stage n is defined by the vectorνn=[ν1n,ν2n,…,νkn]where theνinare the probabilities associated with the different possible valuesθin, of the exact state θnof the system at the beginning of stage n, i.e.νn=[Pr(θ1n),Pr(θ2n),…,Pr(θkn)].The effect of a maintenance action or a degradation process can be modeled (as in a probabilistic dynamic programming problem) by the transition matrices Aanand M respectively where the element aij(i = 1,…, k; j = 1,…, k) of matrix Aanrepresents the probability that the system evolves from the stateθinto the stateaθjnif we implement the maintenance action a at the beginning of stage n and where the element mij(i = 1,…, k; j = 1,…, k) of matrix M represents the probability that the system evolves from the stateaθinto the stateθjn+1as a result of the degradation process.The belief stateavnof the system during stage n, after the implementation of a maintenance action, will be equal to the matrix product of the vector vnby the maintenance transition matrix Aan; i.e.avn= vn× Aan. Similarly, the belief state vn + 1 of the system at the beginning of stage n + 1, that is, after the evolution of the system, due to the Markovian degradation process, will be equal to the matrix product of the vectoravnby the Markovian degradation process transition matrix M; i.e. vn + 1 =avn× M. Thus, a maintenance action and/or a Markovian degradation process (having probabilistic consequences on the system degree of degradation) will result in a probability distribution, i.e. an exactly well defined belief state of the system.Similarly to a deterministic dynamic programming problem, solving a finite time horizon POMDP by dynamic programming requires a backward calculation (beginning from the last time period) of the costs associated with each of the belief states νnof stage n by choosing the action that minimizes the total cost c(νn) (Bellman, 1961). This cost is composed of the cost of that action ca(an) and the discounted optimal cost α × *c(νn + 1|a, νn) associated with the forecasted belief state of stage n + 1, knowing that we applied the action a and that we were in the belief state νnat the beginning of stage n. i.e. the recursive relation will be:(1)c(νn)=ca(an)+α×*c(νn+1|a,νn)One can note that Eq. (1) is similar to a deterministic dynamic programming recursive relation but where were replace the state θnby the belief state νn.Extensions and generalizations of the POMDPs described above were proposed by Corotis, Ellis, and Jiang (2005) and Faddoul et al. (2009) through the development of GPOMDPs. A GPOMDP allows for easy mathematical modeling of complex sequence of actions to be undertaken during each time period of the planning horizon. For example, a sequence of actions may consist of two inspection decisions followed by one maintenance decision. This type of modeling is relevant to maintenance problems, as a cheap inspection may be carried out first, in order to decide whether a more precise and costly inspection should be applied before taking the decision relative to the type of the repair technology.The transition matrices relative to degradation processes, maintenance actions and inspections, are usually estimated by using one, or a combination, of the following techniques (DeStefano &#38; Grivas, 1998; Kallen, 2007): (i) expert evaluation based on past experience; (ii) expert evaluation based on analytical reasoning, taking into account the physical and chemical properties underlying the considered evolution processes; (iii) statistical estimation based on historical observation samples of the system itself and/or similar systems; (iv) estimation based on stochastic mathematical models which might be fitted to available historical data, completed by physical knowledge to calibrate some of the model parameters.Whatever the applied estimation methods, uncertainties will usually veil the exact and true values of the estimated matrices. These uncertainties can be typically expressed by: (i) fuzzy sets, (ii) mean value and variance of the estimated parameters, (iii) confidence intervals for a specified significance level, (iv) full probability distributions. In what follows, we present the probabilistic approaches that may be used to describe the epistemic uncertainties to be included in a MDP.Consider the case of an estimation procedure, of a transition matrix T, based on N observations of the true state of the system at the beginning of stage n + 1, knowing that it was at an exactly defined stateθinat the beginning of stage n. Such a situation corresponds to the case where the inspections used for the estimation of T are assumed to be performed at regular time intervals equal to the transition period of the Markov process. Moreover, it is assumed that these inspection techniques are perfect, i.e. they reveal the true state θ of the structure. The cases where the inspections used for the estimation of the transition matrices are imperfect and/or performed at varying time intervals, are treated in Appendices A.1 and A.2.The rows of T can be considered as the vector parameters of multinomial distributions of dimension K (K being the number of states). i.e. the observed states of a sample of size N, that was in stateθinat the beginning of stage n, will be distributed according to the following multinomial probability distribution:(2)f(x1,…,xK;N,pi1,…,piK)=N!x1!×⋯×xK!×pi1x1×⋯×piKxKwhere xiis the number of observations where the system state isθin+1at the beginning of stage n + 1, and pijis the element at row i and column j of the transition matrix.The epistemic uncertainty related to the parameters pij, in a given row of the transition matrix, can be conveniently represented by the Dirichlet multivariate probability distribution since it is the Bayesian conjugate prior to the multinomial distribution.Hence, the epistemic uncertainty related to the parameters pij, in a given row of the transition matrix, can be conveniently represented by the following Dirichlet multivariate probability distribution:(3)Dir(pi1,…,pik−1;α1,…,αk)=Γ(∑j=1kαj)∏j=1kΓ(αj)∏j=1kpijαj−1withparametersα1,…,αK>0where Γ( · ) is the gamma function. As we have mentioned in Section 3, the belief state vectorνn+1=[ν1n+1,ν2n+1,…,νkn+1]at the beginning of stage n + 1 is equal to the matrix product: νn + 1 = νn× T, where T is a transition matrix, it comes:(4)νin+1=∑j=1kνjn×pji∀i=1,…,kIn other terms, the belief state vector vn+1 is a linear combination of the transition matrix rows which are random vectors having Dirichlet distributions. Thus, we have translated the epistemic uncertainty that was shrouding the estimates of the transition matrix elements into a probability distribution of the belief state νn + 1.Several techniques are available for estimating the parameters of Dirichlet distributions from sample data (Minka, 2003; Wicker, Muller, Kalathur, &#38; Poch, 2008). However, a simple and natural technique arises from the fact that the Dirichlet distribution is a conjugate prior to the multinomial distribution. If we take a prior Dir(pi1, …, pik − 1; α1, …, αk) and a multinomial sample x1, …, xk, the posterior will be Dir(pi1, …, pik − 1; α1 + x1, …, αk+ xk). Hence, the parameters of the prior can be thought of as pseudo-counts; i.e. the probability density function of a Dirichlet distribution returns the belief that the probabilities of K rival events are pigiven that each event has been observed (αi− 1) times.If, for example, we are given a sample of evolving structures which has resulted in a matrix S, where an element sijrepresents the number of structures which were in stateθinand evolved to stateθjn+1, the Dirichlet parameters for each row of the transition matrix will simply be represented by a row of a matrix αi1, …, αikwhere an element αij= sij+ 1.Even if an element sijof the sample matrix is equal to zero, the corresponding element pijof the estimated transition matrix may be, due to the statistical uncertainty, different from zero even though it will be usually relatively small. Hence, special attention needs to be paid for transition matrices where there is an impossibility that the structure evolves from some specified states to other specified states, as for example for the deterioration matrix which is usually an upper triangular matrix due to the fact that usually the performance of the structure cannot improve on its own without any maintenance action. For such matrices, the Dirichlet parameter vectors must only include the elements αijfor which a transition is possible, and thus, they may have a lower dimension than the state space of the structure.Even though, the Dirichlet distribution is typically chosen as the probability distribution for the rows of the transition matrices, one can choose any other multivariate distribution that might seem suitable for particular problems. When the available estimation data is given either in the form of estimates of the meanE˜(pij)and the varianceV˜ar(pij), or in the form of an estimated mean valueE˜(pij)and a confidence interval I, a technique for the reparametrization of a Dirichlet distribution is presented in Droesbeke, Fine, and Saporta (2002, chap. 14).Since the belief state vector vn + 1 is equal to the matrix product vn× T, where T is a random transition matrix, then vn + 1 will also be a random vector having a probability distribution.A classical POMDP behaves like a classical deterministic MDP but over the belief states instead of the exact states. In such a model, the results of actions, degradations or inspections allow the probabilistic outcomes in the original state space to have exact outcomes in the belief state space (i.e. space of probability distributions). This is because the outcomes are belief states which are exactly defined and expressed by probability vectors. If we extend the classical POMDP model to the case of a probabilistic POMDP, i.e. making the POMDP behave like a probabilistic MDP but over the belief states rather than the exact states, then any result of action, degradation or inspection will be a probabilistic distribution of the belief states. It is in this second order probabilistic distribution that the epistemic uncertainty will be integrated.In order to calculate the optimal expected cost *c(vn) associated with the belief state vn, the dynamic programming recursive procedure will call, at the beginning of stage n, for the minimum expected costs *c(vn + 1|vn) calculated with the updated belief states at the beginning of stage n + 1. To take into account the uncertainties in estimating the matrices of actions, degradations and inspections, a probabilistic POMDP must evaluate the expected value E[*c(νn + 1|νn)] (rather than *c(vn + 1|νn) which evaluates the optimal expected cost of exactly updated belief states), knowing that the vector vn + 1 has a probability distribution over the belief state space at the beginning of stage n + 1.In our approach, the belief state vector vn + 1|νnis considered as a random vector. It is a function of the transition matrix rows which are random vectors having a known distribution (e.g. Dirichlet distribution). Consequently, the expected cost associated with the probability distribution of νn + 1|νnwill be:(5)E[*c(νn+1|νn)]=∫statespace*c(νn+1|νn)×f(νn+1|νn)where f( · ) is the probability distribution of the belief state νn + 1|νnresulting from epistemic uncertainties, and E[⋅] is the mathematical expectation operator.The minimum expected cost *c(vn + 1|νn) in Eq. (5) is calculated recursively using a dynamic programming algorithm. As it can be seen from Eq. (5), the uncertainties in the transition matrices are accounted for by taking the expectation of the minimum expected costs *c(νn + 1|νn) over the probability density function f(νn + 1|νn) of the belief state νn + 1|νn.By using Eq. (5), we are applying the principle of separating the epistemic and the aleatory uncertainties (Hofer, 1996) related to the state of the system during any of the future periods where the epistemic and the aleatory uncertainties are represented respectively by f( · ) and (νn + 1|νn). It should be emphasized here that once the system has reached the time period n all the uncertainties related to the state of the system are epistemic uncertainties since the system has one exact true state which is unknown to the manager (Hofer, 1996).A convenient approach which could be used for computing Eq. (5) is the Monte Carlo simulation technique. A crucial concern to deal with is the issue of additional computational complexity that Monte Carlo simulations would bring to the problem. Notice however that the dynamic programming framework alleviates drastically this difficulty as follows:If we assume that the deterioration transition matrices Mjare particular occurrences of a random variable M, and if one samples the transition matrix SNmax times for each stage, then the total number of sampled states in stage i will be equal to SNmax power i ((SNmax)i). If for example one samples the transition matrices for each stage three times, then the number of states sampled at the 20th stage will be 320.We present in Fig. 1the proposed algorithm for the case where the deterioration transition matrix is uncertain (the cases where the action and/or inspection matrices are uncertain are straightforward). The grayelements of the flowchart correspond to the added Monte Carlo simulations to the basic dynamic programming algorithm. In the upper part of the flowchart of Fig. 1, we have four nested loops:(i)In the outer loop, the stage index i is decremented form the end of the planning horizon (i = N) to time period (i = 1).The second nested loop considers all the K possible belief statesνkiof the ith stage.The third nested loop considers all the amax alternative actions. In this loop, we minimize the costcs(vki×Aa)+ca(a)+*cfwhere *cf is the average optimal future cost; ca(a) is the cost of the maintenance action a andcs(vki×Aa)is the immediate users cost due to the application of action a.The inner loop corresponds to the sampling of the uncertain transition matrix, the prescribed sampling number being SNmax. At the end of this loop, we calculate the average of the obtained optimal future costs*c(vki×Aa×M).Since there is only one belief state in the first stage, a specific sampling loop is performed for that initial belief state in the lower part of the algorithm.As for the updating of the epistemic uncertainties, we adopt the open-loop feedback control approach in the sense, that after each stage, in the case of the use of Dirichlet matrices, the pseudo-counts of these matrices are updated according to the results of the applied inspections to our structure and others of the same type as well.As for the sampling of the transition matrices, we believe that two cases are relevant:(i)A stationary uncertainty model: In this modeling the same set of sampled matrices is used for every stage and state.A time varying uncertainty model: In such a modeling, a new set of sampled matrices is used for each stage.The first procedure corresponds to the case where there is only one true but unknown transition matrix for all the periods. The second procedure corresponds to the case where the epistemic uncertainty emanates from the influence of an unknown variable which affects the evolution process and which is subject to variation with time; i.e. there is a single known probability distribution from which a different transition matrix is true for each period. The methodology proposed in this paper can be used for both cases.As mentioned above, the underlying principle of the algorithm described in this paragraph is also valid for the transition matrices of the actions and the uncertainties matrices of the inspections in a GPOMDP framework. It should be emphasized here that a probabilistic MDP solved by dynamic programming can be considered as a special GPOMDP for which the results of the actions are uncertain but for which we assume a single perfect inspection to be applied for free at the beginning of all time periods. Therefore, the approach for including epistemic uncertainty presented in this section can be easily extended to probabilistic MDPs.Consider a highway concrete bridge whose deck is made of a single concrete slab; the herein data are for illustrative purposes only. We suppose that the performance of the concrete deck is described using five states as may be shown from Table 1. In this table, the costs cs(aθin) are generally the sum of the costs incurred by the user of the bridge due to malfunctioning of the deck along with the risk of failure which is expressed in monetary units and calculated as the product of the failure probability of the deck by the costs generated by such a failure.The stochastic deterioration of the deck is modeled by a Markov chain with state space Θ = {θ1, θ2, θ3, θ4, θ5} having the following transition matrix:M=θ1n+1θ2n+1θ3n+1θ4n+1θ5n+1aθ1n0.5710.2850.14400aθ2n00.5710.2850.1440aθ3n000.5710.2850.144aθ4n0000.7140.286aθ5n00001For this structure, four imperfect inspection techniques (i0, i1, i2, i3) are available, where i0 means that no inspection is performed (i.e. we entirely rely on the prediction of the degradation model) and its cost is nil. The cost of inspection technique i1 is 20 units and the cost of inspection techniques i2 and i3 is 40 units each. For each inspection technique we assume that there are k possible results rk(k = 1, …, 5). The uncertainties associated with the results of the inspection techniques i1, i2 and i3 are expressed by the probability distributions shown in Table 2.We assume that only four imperfect maintenance actions (a0, a1, a2, a3) can be employed, where a0 means that no maintenance is performed and its cost is nil. The cost of action a1 (preventive maintenance action) is 800 units, the cost of action a2 (corrective maintenance action) is 800 and the cost of action a3 (replacement of the deck) is 3000 units. The probabilities associated with the consequences of the maintenance actions are expressed by the following matrices:A0=aθ1naθ2naθ3naθ4naθ5nθ1n10000θ2n01000θ3n00100θ4n00010θ5n00001Transitionmatrixforactiona0A1=aθ1naθ2naθ3naθ4naθ5nθ1n10000θ2n0.6660.334000θ3n0.40.40.200θ4n00.1670.3330.3330.167θ5n000.2850.2850.43Transitionmatrixforactiona1A2=aθ1naθ2naθ3naθ4naθ5nθ1n0.90.1000θ2n0.1420.7160.14200θ3n0.10.30.50.10θ4n0.40.30.20.10θ5n0.20.40.20.10.1Transitionmatrixforactiona2A3=aθ1naθ2naθ3naθ4naθ5nθ1n10000θ2n10000θ3n10000θ4n10000θ5n10000Transitionmatrixforactiona3The element aijin each of these matrices corresponds to the probability that the deck, which is initially in the stateθin(before the maintenance action), will be after the application of the maintenance action in the stateaθjn(the superscript “a” means that the state into consideration occurs immediately after the maintenance action before any degradation can take place).We suppose that the initial belief state of the deck isv=[0.20.20.30.20.1]. The planning horizon is set to 14 years. The length of each time period is taken to be 2 years long and the discount rate is: α = 0.049.We should mention that in this example we adopted a discretization step of 1/15 in the [0, 1] interval of the probability space describing the belief states. Thus, since the original state space has five possible outcomes, the total number of belief states in each stage is 650,630. For each belief state and for each uncertain transition matrix, Monte Carlo sampling was done by randomly generating transition matrices using Dirichlet distributions.The results were computed using specialized GUI software for GPOMDP that we have developed. It can be noted (see Figs. 2and 3) that the added complexity (that arises from considering the epistemic uncertainty) to the original dynamic programming algorithm is modest owing to the fast convergence of the solution when one increases the sampling size. For a sampling number greater than 20 the coefficient of variation of the estimated minimum expected cost is smaller than 0.0006 with a quasi-constant value of this cost (8024 units).The time required for a sampling number equal to 20 (for which the COV was equal to 0.0006) was about 3 minutes on a core 2 duo Intel computer, when considering the uncertainty of the deterioration transition matrix.Firstly, a simulation analysis is conducted in order to validate the expected costs calculated by our methodology against those obtained by simulation under the assumption that the transition matrices are uncertain. In a second stage, the simulated costs resulting from the use of the IM&#38;R strategies prescribed by our model (i.e. by considering the epistemic uncertainties) and the ones resulting from the IM&#38;R strategies prescribed by the classical POMDPs (i.e. without considering the epistemic uncertainties) are presented and discussed. The aim is to show the great interest of considering the epistemic uncertainties in the analysis. For this purpose, two sets of 100 simulations each (reducing thus the standard deviation by1/100) were executed: In the first set, the IM&#38;R strategy that is prescribed by the epistemic uncertainty approach presented in this paper was applied; while, in the second set, the IM&#38;R strategy that is prescribed by classical POMDP calculations (i.e.without epistemic uncertainty) was applied. An inspection-action scheme where the degradation process is subject to epistemic uncertainty (with an available sample size n = 49) was considered in the analysis. At the beginning of each time period, an adjusted degradation transition matrix was randomly generated from the Dirichlet matrix. Then, using the generated matrix, we randomly generate the evolution of the state of the deck by sampling the multinomial distributions which are the rows of the generated matrix. Using Bayes equation, the results of the inspections were also randomly generated based on the belief state of the deck and the uncertainty characterization of the inspection techniques (Table 2).Although such a simulation is adequate for evaluating specified IM&#38;R strategies, it is not feasible to use it in searching for optimal strategies. This is due to the daunting computational complexity which may arise from such a brute force searching algorithm.By applyingthe IM&#38;R strategy obtained using the approach presented in this paper (which takes into account the epistemic uncertainties), the simulation analysis has led to an average simulated cost of 6560 units and a standard deviation of 700 units. This cost is to be compared to the calculated expected cost (6380) which was obtained using the methodology presented in this paper. The little discrepancy between the two costs is clearly due to the statistical uncertainty related to the limited sample size. A perfect matching would require theoretically an infinite sample size. These findings confirm the accuracy of the results obtained by applying the proposed methodology.The IM&#38;R strategy prescribed by standard POMDPs (i.e. obtained without considering epistemic uncertainties) has led to an average simulated cost of 7340 units (the calculated expected cost using standard POMDP is 5410) and a standard deviation of 1100 units. The significant discrepancy between the simulated cost and the POMDP cost will be explained later in this section.These results show that while algorithmic calculation of classical POMDPs gives costs that are lower than those given by our approach (5410 vs 6380), the application of IM&#38;R strategies that were obtained using classical POMDP results in actual costs that are much higher than those incurred by applying the results prescribed by our approach (7340 vs 6560). Hence, one can conclude that ignoring epistemic uncertainty in the present case will result in a 11.89 percent increase in the actual IM&#38;R costs.It should be remembered here the fact that both simulations (using IM&#38;R strategies obtained respectively by classical POMDPS and our approach) were conducted under the assumption that the true deterioration matrix is not known precisely (i.e.there is epistemic uncertainty). This explains the significant bias between the simulated cost and the POMDP calculated cost for the strategy prescribed by the POMDP. The high cost resulting from applying the IM&#38;R strategy prescribed by classical POMDPs is due to the discrepancy between the assumptions with the simulated reality since dynamic programming in classical POMDP assumes a precisely known deterioration matrix. Notice also that the standard deviation obtained by using the proposed approach is significantly smaller than that obtained by using POMDPs without considering epistemic uncertainties (700 vs 1100).Calculations were made in this section in the cases where the Dirichlet distributions for the deterioration transition matrix were generated from a small size sample, medium size sample, and large size sample (Table 3). Optimal strategies and minimum total expected costs for action-alone scheme and inspection-action scheme (Table 4) are computed for these different sample sizes. Since we are presuming that the epistemic uncertainties emanates from the limited available sample size, we assume in this example the stationary uncertainty model. We are thus assuming that there is one true transition matrix, which we do not know exactly.It can be noted (see Table 4) that the more the epistemic uncertainty is large (smaller sample size) the higher the minimum expected costs will be. Obviously, not considering epistemic uncertainty is equivalent to assuming an infinite sample size. Note that the costs (7300 units for the action alone scheme and 5410 units for the inspection-action scheme) for that case are the smallest ones. Moreover, the optimal maintenance actions and prescribed inspection techniques, for a given initial belief state, are different for each sample size.Similar calculations are performed for the cases where the matrices related to the maintenance actions and inspection techniques were estimated from samples of varying size for the case of an action-inspection scheme (Fig. 4). From this figure, it can be seen that the additional expected cost (due to the joint consideration of epistemic uncertainty for maintenance actions, inspections and degradation) with respect to the case where the epistemic uncertainties are not present (i.e. the case of an infinite sample size) is lower than the sum of additional expected costs due to each of these epistemic uncertainties considered separately. For example, for the case of a sample of unit size (n = 49 i.e.1 on the log2scale), the additional expected cost due to the joint consideration of epistemic uncertainty for maintenance actions, inspections and degradation is equal to (8024 − 5410 = 2614 units). This cost is lower than the sum of additional expected costs due to each of these epistemic uncertainties considered separately, i.e. (7448 − 5410) + (6055 − 5410) + (6013 − 5410) = 3286 units.In Fig. 5, the results corresponding to the probabilistic epistemic uncertainty are presented for the cases where the degradation transition matrices are generated with and without taking into account the physical impossibility to have non-zero elements below the diagonal (cf. Section 4). As one would expect, using an adjusted deterioration transition matrix results in a higher total expected cost. Hence, the probability distributions resulting from Dirichlet matrices (pseudo-counts) can and must be modified when additional information relative to the evolution process is available.The preceding results (Table 4, Figs. 4 and 5) show the large influence of the epistemic uncertainties on the minimum expected total cost. This fact underlines the importance of considering such kind of information in the optimization process. Special care should therefore be given to identify the epistemic uncertainty model.

@&#CONCLUSIONS@&#
