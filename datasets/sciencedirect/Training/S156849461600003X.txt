@&#MAIN-TITLE@&#
Not guaranteeing convergence of differential evolution on a class of multimodal functions

@&#HIGHLIGHTS@&#
We constructed a Linear Deceptive function as the representative of a class of multimodal functions.DE cannot guarantee convergence in probability on the above class of multimodal functions.A random drift model was firstly used to analyze the convergence of a real-coded evolutionary algorithm.DE's mutation operators prefer to search in the aggregating region of the target individuals.

@&#KEYPHRASES@&#
Continuous optimization,Convergence in probability,Differential evolution algorithm,Drift analysis,

@&#ABSTRACT@&#
The theoretical studies of differential evolution algorithm (DE) have gradually attracted the attention of more and more researchers. According to recent researches, the classical DE cannot guarantee global convergence in probability except for some special functions. Along this perspective, a problem aroused is that on which functions DE cannot guarantee global convergence. This paper firstly addresses that DE variants are difficult on solving a class of multimodal functions (such as the Shifted Rotated Ackley's function) identified by two characteristics. One is that the global optimum of the function is near a boundary of the search space. The other is that the function has a larger deceptive optima set in the search space. By simplifying the class of multimodal functions, this paper then constructs a Linear Deceptive function. Finally, this paper develops a random drift model of the classical DE algorithm to prove that the algorithm cannot guarantee global convergence on the class of functions identified by the two above characteristics.

@&#INTRODUCTION@&#
The differential evolution algorithm (DE) proposed by Storn and Price in 1995 [1] is a population-based stochastic parallel evolutionary algorithm. DE emerged as a very competitive form of evolutionary computing [2–4] and has got many practical applications, such as function optimization, multi-objective optimization, classification, scheduling and so on.Since that theoretical studies benefit understanding the algorithmic search behaviors and developing more efficient algorithms, more and more researchers pay attention to the theoretical studies on DE with the popularity in applications. In 2005, Zielinski et al. [5] investigated in theory the runtime complexity of DE for various stopping criteria including a fixed number of generations (Gmax) and maximum distance criterion (MaxDist). From 2001 to 2010, Zaharie [6–11], Dasgupta et al. [12,13] and Wang et al. [14] analyzed the dynamical behavior of DE's population from different perspectives, i.e., the statistics characteristics, the gradient-descent type search characteristics and stochastic evolving characteristics respectively. Recently some convergent DE algorithms [15–19] have developed.This paper focuses on the convergence analyses of the classical DE. Several important conclusions on the convergence have been drawn. In 2005, Xu et al. [20] performed a mathematical modeling and convergence analysis of continuous multi-objective differential evolution (MODE) under certain simplified assumptions, and this work was extended in [21]. In 2012, Ghosh and Das et al. [22] used Lyapunov stability theorem to establish the asymptotic convergence behavior of a classical DE (DE/rand/1/bin) algorithm on a class of special functions identified by the following two properties, 1) the function has the second-order continual derivative in the search space, and 2) it possesses a unique global optimum in the range of search. In 2013, Hu et al. [23] proposed and proved a sufficient condition for global convergence of the modified DE algorithms. In 2014, Hu et al. [24] developed a Markov chain model of the classical DE and proved then that it cannot guarantee global convergence in probability. In a word, the classical DE cannot guarantee global convergence in probability except for some special functions.Along this perspective, this paper does two works as follows:•Firstly, this paper addresses that DE variants are difficult to solve a class of multimodal functions. By abstracting the characteristics of the class of functions, this paper then constructs a Linear Deceptive function which can simplify the theoretical analyses on DE.This paper develops a random drift model of the classical DE algorithm to prove the conclusion that the algorithm cannot guarantee global convergence on a class of functions represented by the Linear Deceptive function.The rest is organized as follows. Section 2 introduces the classical DE algorithm. As the research background of this paper, Section 3 presents a problem that many DE variants are difficult to solve a class of multimodal functions. Section 4 qualitatively analyzes the reason resulting the problem by using distribution characteristics of the trial population, and offers the proof idea of the main conclusion in the following sections. Sections 5 and 6 prove the main conclusion that the classical DE cannot guarantee global convergence on a class of multimodal functions by constructing a Linear Deceptive function and developing a random drift model of the classical DE. Finally the concluding remarks are presented in Section 7.DE is used for dealing with the continuous optimization problem. We suppose in this paper that the objective function to be minimized isf(x→),x→=(x1,…,xn)∈Rn,and the feasible solution space isΨ=∏j=1j=n[Lj,Uj]. The classical DE [1,3,26] works through a simple cycle of operators including mutation, crossover and selection operator after initialization. The classical DE procedures are described in detail as follows.The first step of DE is the initialization of a population of m n-dimensional potential solutions (individuals) over the optimization search space. We shall symbolize each individual byx→ig=(xi,1g,xi,2g,…,xi,ng), for i=1, …, m, where g=0, 1, …, gmaxis the current generation and gmaxis the maximum number of generations. For the first generation (g=0), the population should be sufficiently scaled to cover the optimization search space as much as possible. Initialization is implemented by using a random number distribution to generate the potential individuals in the optimization search space. We can initialize the jth dimension of the ith individual according toxi,j0=Lj+rand(0,1)·(Uj−Lj)where rand(0, 1) is a uniformly distributed random number confined in the [0,1] range.After initialization, DE creates a donor vectorv→igcorresponding to each individualx→igin the gth generation through the mutation operator. Several most frequently referred mutation strategies are presented as follows:DE/rand/1:v→ig=x→r1g+F(x→r2g−x→r3g);DE/best/1:v→ig=x→bestg+F(x→r1g−x→r2g);DE/current-to-best/1:v→ig=x→ig+F(x→bestg−x→ig)+F(x→r1g−x→r2g);DE/best/2:v→ig=x→bestg+F(x→r1g−x→r2g)+F(x→r3g−x→r4g);DE/rand/2:v→ig=x→r1g+F(x→r2g−x→r3g)+F(x→r4g−x→r5g);wherex→bestgdenotes the best individual of the current generation, the indices r1, r2, r3, r4, r5∈Sr={1, 2, …, m} ∖ { i } are uniformly random integers mutually different and distinct from the running index i, and F∈(0, 1] is a real parameter, called mutation or scaling factor.If the jth element ofv→iis infeasible (i.e. out of the boundary), it is reset as the following rule, called Symmetrical Mode Rule.vi,j=2Lj−vi,jifvi,j<Lj2Uj−vi,jifvi,j>UjFollowing mutation, the crossover operator is applied to further increase the diversity of the population. In crossover, the target vectors,x→ig,are combined with elements from the donor vector,v→ig,to produce the trial vector,u→ig,using the binomial crossover,ui,jg=vi,jgifrand(0,1)≤CRorj=jrandxi,jgotherwisewhere CR∈(0, 1) is the probability of crossover, jrandis a random integer in [1,n].Finally, the selection operator is employed to maintain the most promising trial individuals in the next generation. The classical DE adopts a simple selection scheme. It compares with the objective values of the targetx→igand trialu→igindividuals. If the trial individual reduces the value of the objective function then it is accepted for the next generation; otherwise the target individual is retained in the population. The selection operator is defined asx→ig+1=u→ig,iff(u→ig)<f(x→ig)x→ig,otherwise.The pseudocode of the classical DE (DE/rand/1) is illustrated in Fig. 1The analyses in the introduction section demonstrate that the classical DE cannot guarantee global convergence in probability except for some special functions. Next, a question aroused is on which functions DE algorithms are not convergent in probability. We notice the following two test functions are difficult to be solved by using the classical DE and improved DE algorithms.DE Deceptive functionThe DE Deceptive function was given in the reference [29]. It can be formulated as follows:f(x)=−3sinc(2x+10)if−10≤x<0−x·sin(xπ)if0≤x≤10where the function sinc(t) is given by:sinc(t)=1ift=0sin(πt)/(πt)ift≠0The landscape of DE deceptive function is shown in Fig. 2[24]. The global optimum of the function is x=−5.0 with the function value f(x)=−3. There is a deceptive local minimum x=8.5060 with function value f(x)=−2.9160 in this test function.The five common variations of DE are applied to test the DE Deceptive function. The reference [27] showed that the control parameter setting [F=1.0, CR=0.9] in it can maintain the population diversity and make the algorithm more powerful in global exploration. The population size m is generally set to be 5–10 times as the individual dimension n. The parameters of our experiments are set as follows:•Mutation factor, F=1.0;Crossover probability, CR=0.9;Population size, m=8*n;Fixed accuracy, Fix_Acc=10−6.Maximum iteration times, Max_Iter=2000.We run every variation 200 times on the Deceptive function. As shown in Table 1, we report the numbers of achieving Fix_Acc within Max_Iter, the numbers and rates of trapping into local optimal solution set. Fix_Acc and Max_Iter denote a fixed accuracy and a maximum iteration times, respectively. We can see that the rates of trapping into local optimal solution set are, in order, 92.0%, 99.0%, 99.5%, 59.5%, 33.5%. These results demonstrate that the classical DE algorithms are difficult to solve the DE Deceptive function.Shifted Rotated Ackley's function with global optimum on boundsThe Shifted Rotated Ackley's function is one of the test functions for the CEC2005. The mathematical expression and characteristics of the function can refer to the reference [25]. The landscape of the Ackley's function on 2 dimension is shown in Fig. 3(a). Fig. 3(b) is the projective map on YOZ plane of 2-dimension Ackley function. From Fig. 3(b), we can see that its global optimum is near a boundary of the search space and there are a number of local optima.Table 2reports the results of several DE variants on Shifted Rotated Ackley's function. The results of DE/rand/1 are originated from reference [30], while the other results are from reference [31]. We can see that all the mean errors are larger than 20. This means that the function is very difficult for the DE variations.What is the theoretical reason that DE and the variants cannot solve the above two functions well? Some perceptual knowledge may be derived from analyzing the exploration ability of DE algorithm. Since the exploration ability of DE depends on the distribution of its trial population to a great extent, this section qualitatively analyzes the distribution characteristic of DE's trail population.Let us assume that the crossover probability CR equals 0, analyze the relationship between the population distribution of trial vectors and its target population. The following qualitative analyses will give us a perceptual fact that trail vectors prefer to locate into the aggregating region of target vectors.CR=0, that is, the population distribution of trial vectors just depends on the mutation operator and nothing on the crossover operator of DE. Next, DE/rand/1 (v→ig=x→r1g+F(x→r2g−x→r3g)) for example, we analyze the relationship between the population distribution of trial vectors and its target population. As shown in Fig. 4(a), the region [A1, A2] denotes the distribution region of a target population, obviously, the region [B1, B2] is the accessible region of the target population [A1, A2] by using DE/rand/1 operator when F=1.Let us now suppose F=0.9, andx→r1g=A1, a drift towards left will occur ifx→r2g<x→r3gwhile a drift towards right will occur ifx→r2g≥x→r3g. Since the values ofx→r2gandx→r3gare interchangeable because of the generation rule of r2 and r3, the drift probability towards left is equal to the drift probability towards right. So the dot A2 does. And the trial vectors generated by A1 towards right belong to [A1, A2] while the generated vectors also belong to [A1, A2] by A2 towards left. Therefore, if running DE/rand/1 operator 100 times on the target vectors A1, A2 respectively, we may get that the distribution of vectors on the regions [B1, A1], [A1, A2] and [A2, B2] is approximately 50, 100 and 50. From this perspective, it isn’t difficult to find that trail vectors prefer to locate into the aggregating region of target vectors. Fig. 4(b) develops Figure 4(a) on 2-dimension.On the other hand, the analyses of the above paragraph also indicate that the mutation operator of DE prefers to search in the aggregating region of target vectors. So if the global optimum belongs in a target population (as shown in Fig. 5(I)) or the aggregating region of target vectors (as shown in Fig. 5(III)), then this kind of problems will be easily solved by DE algorithms. As shown in Fig. 5, the three dots denote the target individuals of the current population, then the problems corresponding to the subgraph I and III are easy for DE algorithms while those to the subgraph II and IV are comparatively difficult at some degree.The above analyses of trail populations indicate that trail vectors prefer to locate into the aggregating region of target vectors. That is to say, if the selection operator of DE is not considered, the mutation operator prefers to search in the aggregating region of target vectors. So if the global optimum of a function is far away from the aggregating region of a certain target population, the function may be difficult to be solved by using DE at some degree. Thinking along with this route, we give a function, such as, the above DE Deceptive function or Ackey's function, which holds two characteristics as follows: (1) its global optimum is near a boundary of the search space, and (2) in the other side away from the global optimum there are many local optima. Now we imagine an initial population drifts towards the side of the local optima under the influence of the greedy selection operator of DE. It is then easy to form the situation that the aggregating region of a certain target population is far away from the global optimum of the given function. Therefore DE is difficult to solve the class of functions identified by the above two characteristics. Also, considering the fact [24] that any target population of DE cannot escape once traps into a local optimal set because of its greedy selection operator, we speculate that the classical DE algorithm cannot globally converge in probability on the class of functions.A following question aroused is how to rigorously prove the above conjecture that the classical DE cannot guarantee global convergence on the above class of functions. Next, we will prove the conjecture in theory according to the following line. Firstly, we will construct a function, called DE Linear Deceptive function. Similar to the DE Deceptive and the above Ackley's functions, the DE Linear Deceptive function holds two characteristics as follows. One is that its global optimum is near a boundary of the search space. The other is that the function have a larger region on the other side of the global optimum. The region, called Deceptive Optima set Sdec, is a set with two characteristics, that is, (1) any individual cannot escape from the region once traps into it, (2) the region doesn’t include global optimum. Secondly, we will develop a random drift model of the classical DE. Finally, we will give a theoretical proof of the above conjecture on the Linear Deceptive function by proving a fact that the initial population will drift towards the Deceptive Optima set with a probability bigger than 0.In order to give a succinct and rigorous proof, we construct the following Linear Deceptive function by simplifying the DE Deceptive function.f(x)=−kx−1−2/k≤x<−1/kkx+1−1/k≤x<0−x/k+10≤x<k−11/kk−1≤x≤kHere k≥10. The global minimum of the function is −1/k with the function value 0, and a local minimal region is [k−1, k] on which the function value of each point is 1/k.Now, let k equal 10. At this case, the global minimum of this function is −0.1 with the function value 0, while the local minimal set is [9.0, 10] on which the function value of each point is 0.1. Its landscape is shown in Fig. 6.According to the above description of the Deceptive Optima set Sdec, the set depends on reproduction operators of an algorithm and concrete functions. For an algorithm and a function, Sdecmaybe conclude the local optimum set, premature solution set as well as stagnation set of the function. Given the mutation factor F=1, we estimate the Deceptive Optima set Sdecof the Linear Deceptive function for the classical DE algorithm with the mutation operator DE/rand/1v→ig=x→r1g+F(x→r2g−x→r3g). When k=10, as shown in Fig. 6, we suppose that the gthtarget population belongs to the set of [5, 10]. Letx→r1g=5, the smallest difference of(x→r2g−x→r3g)equals to −5, we can getv→ig=5+1·(−5)=0. Obviously, thisv→igis the leftmost one which can be found by using DE/rand/1. Since the function value ofv→igis bigger than that of any target individual of [5, 10],v→igwill failure to survive under the greedy selection operator of the classical DE. On the other hand, any target individual of [5, 10] cannot escape from the set [5, 10] under the reproduction operators of the classical DE. So the set [5, 10] is a Deceptive Optima set of the Linear Deceptive function with k=10 for the DE/rand/1. By this analogy, for arbitrary k≥10 and DE/rand/1, the Linear Deceptive function has a Deceptive Optima set [k/2, k].The five common variations of the classical DE are applied to test the Linear Deceptive function. We run every variation 200 times. As shown in Table 3, we report the number of achieving Fix_Acc within Max_Iter, the number and rate of trapping to local optimal solution set. Fix_Acc and Max_Iter denote a fixed accuracy and a maximum iteration times, respectively. The results show that the five variations of the classical DE have large probability trapping in the optimal solution set of the function. The DE/best/1 (i.e., 100.0%) tops the list of the probabilities, followed by DE/cur-to-best/1 (i.e., 99.5%), DE/rand/1 (i.e., 98.5%), DE/best/2 (i.e., 81.0%) and DE/rand/2 (i.e., 77.0%).All the above algorithms are coded in Visual C++ and the experiments are executed on a ACER 4750G laptop with a 2.30 GHz Intel(R) Core(TM)i5 2410M CPU and 2GB RAM.We can see that the Linear Deceptive function keeps the two characteristics of the two test functions, i.e., the DE Deceptive function and the above Shifted Rotated Ackley's function. One is that its global optimum is near a boundary of the search space. The other is that it has a larger Deceptive Optima set Sdecin the search space. The experimental results also indicate that, like the two test functions, the Linear Deceptive function is difficult to be solved by DE, while its Deceptive Optima set is easier to be estimated than the two test functions. This benefits the following proof of the conjecture.In order to analyze the convergence properties of the classical DE, a convergence definition must be given. There are several differential convergence definitions. The following convergence definition, global convergence in probability, is employed in this paper.Definition 1[24] (Global convergence in probability) Let {x(t), t=0, 1, 2, …} be a population sequence generated by a population-based stochastic algorithm, then the algorithm holds global convergence in probability if and only if, for any initial population x(0):limt→∞P{x(t)∩Bδ*≠ϕ}=1.HereBδ*denotes the optimal solution set expanded,Bδ*={x→||f(x→)−f(x→*)|<δ}, δ is a small positive value.Next, a Random Drift Model will be used to prove the conclusion in theory. Drift analysis reduces the behaviour of algorithms in a higher dimensional population space into a super martingale on the one-dimensional space. Two key points in applying drift analysis are (1) to define a good distance function, and (2) to estimate the mean drift [28].In order to describe how far a population x(t) is away from a Deceptive Optima set Sdec, we define a distance function V(x(t), Sdec) to measure the distance. This function, without confusion, is denoted as V(x(t)) in short. For any population x(t) of DE, we define V(x(t)) in the following way:V(x(t))=:max0≤i<m{|x→i(t)−Sdec|,x→i(t)∈x(t)},where|x→i(t)−Sdec|is the distance between the vectorx→i(t)and the set Sdec.Now we can define an one-step mean drift at the t−th generation,E[V(x(t+1))−V(x(t))|x(t)=X]=V(X)−∫Y∈ΨV(Y)p(X,dY;t).Here p(X, Y;t) denotes the probability that the process will, when in state X at time t, next make a transition into state Y. Ψ denotes the feasible solution space of a function.We then decompose the drifts into two parts: positive and negative drifts as follows,E+[V(x(t+1))−V(x(t))|x(t)=X]=V(X)−∫Y:V(Y)<V(X)V(Y)p(X,dY;t),E−[V(x(t+1))−V(x(t))|x(t)=X]=V(X)−∫Y:V(Y)>V(X)V(Y)p(X,dY;t).Here the positive drift is the expectation of a population towards the deceptive optima set Sdec, while the negative drift is that away from Sdec.Lemma 2Let {x(t), t=0, 1, 2, …} be the population sequence generated by the classical DE, the classical DE cannot guarantee global convergence in probability. That is, for some continuous optimization problems, there exits an initial population x(0), such thatlimt→∞P{x(t)∩Bδ*≠ϕ}<1.Proof(i) Giving the t-th generation populationx(t)=X¯uniformly distributed in [0, k], now we prove the one-step mean drift at the t-th generation towards Sdecis greater than a positive constant.Taking the classical DE/rand/1 (v→ig=x→r1g+F(x→r2g−x→r3g)), for instance, We have the probability that each trial individualu→(t)is in the interval (−2/k, 0) as followingPu→(t)∈−2k,0=43k6F2−2k4F+1k22k2≤F<1F60≤F<2k2The detailed deducing is given in Appendix.SoPu→(t)∈−2k,0≤43k6−2k4+1k2<1k2.Under the greedy selection of the classical DE, we have the probability that at least one individual of the next generation population x(t+1) is in the interval (−2/k, 0) as followingPx(t+1)∩−2k,0≠ϕ<1−1−1k2m.That is to say, the probability of a negative drift's occurrence is less than1−(1−1/k2)m. In fact, the negative drift means that an individual moves to the left and survives to the next population. Let's carefully consider the Linear Deceptive function by using Fig. 6 as a reference. The interval (−2/k, 0) is a neighborhood of the global optimum −1/k. And we can get that an individual moving towards the left maybe survive only when it locates in the interval (−2/k, 0). So the occurrence of a negative drift for a population x(t) means that the intersection of x(t+1) and (−2/k, 0) is non-null.In addition, since r2, r3 are uniformly random numbers,x→r2andx→r3are essentially interchangeable. Ifx→r2−x→r3<0, we havex→r2−x→r3>0after interchanging the values ofx→r2andx→r3. So for the populationx(t)=X¯∈[0,k], if there is a negative drift (i.e.,x→r2−x→r3<0), a corresponding positive drift equaling the negative drift must happen. Therefore, this part of drifts in total is zero. So the occurrence probability of effective positive drifts is greater than 1−2·P{x(t+1)∩(−2/k, 0)≠ϕ}, calledP{ESdec+}, that isP{ESdec+}=2·(1−1/k2)m−1. Obviously, the value is greater than 0 if k is big enough.Let δ be the minimum nonzero difference of any two individuals in the population x(t), we get the one-step mean drift as following:E[V(x(t+1))−V(x(t))|V(x(t)=X¯)]>P{ESdec+}·Fδ=2·1−1k2m−1Fδ.Let MDTs denote themeandrifttimes of the population x(t) trapping in the deceptive optimal set Sdec, the needed MDTs is less than the value of dividing k/2 by the one-step mean drift. Here the k/2 is the maximum distance between the population x(t)∈[0, k] and the deceptive optima set [k/2, k] of the operator DE/rand/1. That isMDTs≤k/2[2·(1−1/k2)m−1]Fδ.(ii) And because the probability of the initial population belonging in [0, k] isP{x(0)∈[0,k]}=k2(k2+2)m,for randomly sampling in the search space x(0)∈[−2/k, k], the probability of the population x(t) trapping in the deceptive optimal set Sdecis not less than(P{ESdec+})MDTs·P{x(0)∈[0,k]}>0.So we getlimt→∞P{x(t)∩Bδ*≠ϕ}≤1−(P{ESdec+})MDTs·P{x(0)∈[0,k]}<1.So, we can draw a conclusion that DE is not an algorithm guaranteeing global convergence in probability. For this kind of functions represented by the Linear Deceptive function, at least, DE cannot guarantee global convergence.Numerous researches have shown that DE variants are difficult to solve the following two multimodal functions, i.e., DE Deceptive function and Shifted Rotated Ackley's function with Global Optimum on Bounds. This paper revealed that those two functions hold two common characteristics. One is that the global optima of the functions are near the boundary of their search spaces, the other is that the functions have larger deceptive optima sets in their search spaces. The distribution analyses for the trial population of DE disclosed that the mutation operators of DE have the property of preferring to search in the aggregating region of the target individuals (when without considering the impact of its selection operator). This preference does not conducive to solve the kind of functions holding the above two characteristics. The discovery suggests that DE's performance may be improved by balancing the preference ability of its mutation operators. Our future works are to quantitatively research on the preference property. The quantitative researches aim to guide algorithms’ improvement.This paper then constructed a Linear Deceptive function, which holds the above two characteristics and also helps us to simply theoretic analyses of DE. Finally, this paper proved that the classical DE cannot guarantee global convergence in probability on a kind of multimodal functions represented by the Linear Deceptive function. The employed mathematical tool of the proof is the Random Drift model. The model is ever used to estimate the computation time of evolutionary algorithms by He and Yao [28]. This is the first time it has ever been used for analyzing the convergence of DE. The proof process of this paper suggested that the random drift model would have an ability to qualitatively analyze the convergence for real-coded evolutionary algorithms.

@&#CONCLUSIONS@&#
