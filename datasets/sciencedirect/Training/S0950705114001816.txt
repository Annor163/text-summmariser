@&#MAIN-TITLE@&#
A knowledge-based platform for Big Data analytics based on publish/subscribe services and stream processing

@&#HIGHLIGHTS@&#
Knowledge-based solution for automatic schema mapping to manage data heterogeneity.Automatic ontology extraction and semantic inference for novel Big Data analytics.Integration with publish/subscribe services for large-scale analytics infrastructures.

@&#KEYPHRASES@&#
Publish/subscribe services,Interoperability,Schema matching,Semantic search,Complex event processing,Big Data analytics,Ontologies,

@&#ABSTRACT@&#
Big Data analytics is considered an imperative aspect to be further improved in order to increase the operating margin of both public and private enterprises, and represents the next frontier for their innovation, competition, and productivity. Big Data are typically produced in different sectors of the above organizations, often geographically distributed throughout the world, and are characterized by a large size and variety. Therefore, there is a strong need for platforms handling larger and larger amounts of data in contexts characterized by complex event processing systems and multiple heterogeneous sources, dealing with the various issues related to efficiently disseminating, collecting and analyzing them in a fully distributed way.In such a scenario, this work proposes a way to overcome two fundamental issues: data heterogeneity and advanced processing capabilities. We present a knowledge-based solution for Big Data analytics, which consists in applying automatic schema mapping to face with data heterogeneity, as well as ontology extraction and semantic inference to support innovative processing. Such a solution, based on the publish/subscribe paradigm, has been evaluated within the context of a simple experimental proof-of-concept in order to determine its performance and effectiveness.

@&#INTRODUCTION@&#
At the state of the art, large and complex ICT systems are designed by assuming a system of systems perspective, i.e., a large number of components integrated by means of middleware adapters/interfaces over a wide-area communication network. Such systems usually generate a large amount of loosely structured data sets, often known as Big Data, since they are characterized by a huge size and an high degree of complexity, that need to be effectively stored and processed [1,2]. Some concrete examples can be taken from the application domains of environmental monitoring, intrusion/anomaly detection systems, healthcare management and online analysis of financial data, such as stock price trends. The analysis of such data sets is becoming vital for the success of a business and for the achievement of the ICT mission for the involved organizations. Therefore, there is the need for extremely efficient and flexible data analysis platforms to manage and process such data sets, sometimes on a online/timely basis. However, their huge size and variety are limiting the applicability of the traditional data mining approaches, which typically encompass a centralized collector, able to store and process data, that can become an unacceptable performance bottleneck. Consequently, the demand for a more distributed approach involving the scalable and efficient management of Big Data is strongly increasing in the current business arena.The well-known MapReduce paradigm [3] has attracted great interest, and is currently considered the winning-choice framework for large-scale data processing. Such a successful adoption both in industry and academia is motivated by its simplicity, scalability and fault-tolerance features, and further boosted by the availability of an open source implementation offered by Apache (namely Hadoop [4]). Despite such a great success and benefits, MapReduce exhibits several limitations, making it unsuitable for the overall spectrum of needs for large-scale data processing. In particular, as described in details in [5], the MapReduce paradigm is affected by several performance limitations, introducing high latency in data access and making it not suitable for interactive use. As a matter of fact, Hadoop is built on top of the Hadoop Distributed File System (HDFS), a distributed file system designed to run on commodity hardware, and more suitable for batch processing of very large amounts of data rather than for interactive applications. This makes the MapReduce paradigm unsuitable for event-based online Big Data processing architectures, and motivates the need of investigating other different paradigms and novel platforms for large-scale event stream-driven analytics solutions.Starting from these considerations, the main aim of this work is to design and implement a flexible architectural platform providing distributed mining solution for huge amounts of unstructured data within the context of complex event processing systems, allowing the easy integration of a large number of information sources geographically scattered throughout the world. Such unstructured data sources (such as Web clickstream data, online social network activity logs, data transfer or phone calls records, and flight tracking logs) usually do not fit into more traditional data warehousing or business intelligence techniques/tools and sometimes require timely correlation/processing triggered on specific event basis (e.g., in case of online analysis solicited by specific crisis conditions or emotional patterns). This implies the introduction of new flexible integration paradigms, as well as knowledge-driven semantic inference features in data retrieval and processing to result in really effective business benefits. Publish/subscribe services [6,7] have been proved to be a suitable and robust solution for the integration of a large number of heterogeneous entities thanks to their intrinsic asynchronous communication and decoupling properties. In fact, these properties remove the need of explicitly establishing all the dependencies among the interacting entities, in order to make the resulting virtualized communication infrastructure more scalable, flexible and maintainable. In addition, despite their inherently asynchronous nature, publish/subscribe services ensure timely interactions, characterized by low-latency message delivery features, between the corresponding parties, being also perfectly suitable in online event-driven data processing systems.Accordingly, we have designed our Big Data analytics architecture by building it on top of a publish/subscribe service stratum, serving as the communication facility used to exchange data among the involved components. Such a publish/subscribe service stratum brilliantly solves several interoperability issues due to the heterogeneity of the data to be handled in typical Big Data scenarios. In fact, most of the large-scale infrastructures that require Big Data analytics are rarely built ex-novo, but it is more probable that they are realized from the federation of already existing legacy systems, incrementally developed over the years by different companies in order to accomplish the customer needs known at the time of realization, without an organic evolution strategy. For this reason, the systems to be federated are characterized by a strong heterogeneity, that must be coped with by using abstraction mechanisms available on multiple layers [8,9]. Therefore, such systems can be easily interconnected by means of publish/subscribe services, with the help of proper adapters and interfaces in order to overcome their heterogeneity and make them fully interoperable on a timely basis. We can distinguish the aforementioned heterogeneity both at the syntactic and semantic level. That is, each system is characterized by a given schema describing the data to be exchanged. Even in domains where proper standards have been issued and progressively imposed, the heterogeneity in the data schema is still seen as a problem. Such heterogeneity limits the possibility for applications to comprehend the messages received from a different system, and hence to interoperate. Specifically, publish/subscribe services use these data schemas to serialize and deserialize the data objects to be exchanged over the network. If the schema known by the destination is different than the one applied by the source, it is not possible to correctly deserialize the arrived message, with a consequent loss of information. Interoperability not only has to resolve the differences in data structures, but it also has to deal with semantic heterogeneity. Each single value composing the data to be exchanged can have a different definition and meaning on the interacting systems. Thus, we propose a knowledge-based enforcement for publish/subscribe services in order to address their limitations in supporting syntactic and semantic interoperability among heterogeneous entities. Our driving idea is to integrate schema matching approaches in the notification service, so that publishers and subscribers can have different data schemas as well as exchange events that are easy to be understood and processed.In order to be processed online, in a fully distributed (and hence more scalable) way, Big Data are filtered, transformed and/or aggregated along the path from the producers to the consumers, to allow consumers to retrieve only what they are interested in, and not all the data generated by the producers. This allows avoiding the performance and dependability bottlenecks introduced by a centralized collecting and processing unit, and guarantees a considerable reduction of the processing latency as well as of the traffic imposed on the communication network (since processing is placed closer to the event producers), with considerable benefits in terms of network resource usage. For this purpose, we introduced on top of the publish/subscribe service an event stream processing layer [10], which considers data as an almost continuous stream of events. This event stream is generated by several producers and reaches its destinations by passing through a series of processing agents. These agents are able to apply a series of operations taken from the available complex event processing techniques portfolio [10] to filter parts of the stream, merge two or more distinct streams, perform queries over a stream and to persistently store streams. Hence, the first step of our work consisted in the definition and implementation of several primitive stream-processing operators specialized as data processing agents and in the engineering of a model-based prototype to assist Big Data analysts to easily create a stream processing infrastructure based on publish/subscribe services.Furthermore, we also observed that traditional solutions for performing event stream processing are affected by two main problems limiting their applicability to Big Data analytics:•Stream Interoperability, i.e., users are exposed to the heterogeneity in the structures of the different event streams of interest. In fact, users have to know the details of the event types in order to properly define query strings based on the stream structures, and to write different queries for streams whose structure varies.Query Expressiveness, i.e., events composing the streams are considered as a map of attributes and values, and the typical queries on event streams are structured in order to find particular values in the events.The construction of our platform on top of a publish/subscribe service model, empowered with a knowledge-based solution for interoperability among heterogeneous event types, allows us to easily resolve Stream Interoperability issues, leaving only the Query Expressiveness as an open problem. Recent research on event-driven systems, such as the works described in [11,12], is speculating on the introduction of semantic inference in event processing (by realizing the so-called Semantic Complex Event Processing (SCEP) [13]), in order to obtain a knowledge-based detection of complex event patterns that goes beyond what is possible with current solutions. To this aim, we designed an agent that dynamically builds up a Resource Description Framework (RDF) [14] ontology, based on the incoming events, and applies queries expressed in the SPARQL query language [15] for semantic inference. Such dynamically-built ontology can be integrated with external knowledge, related to the domain or to the specific application within which the stream processing platform is used.This contribution is structured as follows. In the next section, we provide a description of the fundamental problems addressed. Section 3 provides some background to support the proposal: in the first part it introduces publish/subscribe services, while in the second part it presents details on event stream processing. Section 4 describes in details the proposed solution for dealing with data heterogeneity and semantic inference in stream processing network infrastructures supporting Big Data analytics in a fully distributed scenario. Starting from the unsuitability of tree-based exchange formats, such as XML, we describe a knowledge-based solution to develop a flexible notification service. In addition, we show how it is possible to implement a stream processing network on top of a publish/subscribe service stratum. We conclude with details on how RDF ontologies can be dynamically built, and how SPARQL queries can be executed during event stream processing. Section 5 illustrates a proof-of-concept prototype used to assess our solution, as well as the outcomes of some performed experiments. Section 6 concludes the work by presenting some final remarks.The aim of this section is to describe in detail the two problems ofdata integrationandsemantic processingwithin the context of a platform for Big Data analytics.Federating legacy systems, built by different companies at different times and under different regulation laws, requires the resolution of the interoperability issues imposed by the high potential heterogeneity among the systems belonging to a federated organization.The first degree of heterogeneity is related to the programming environments and technologies used to realize the legacy systems to be federated. Nowadays, this heterogeneity is not felt as a research challenge anymore, but it is simply a matter of realizing the artifacts needed to bridge the gap introduced by the different adopted technologies. The literature is rich of experiences on designing and implementing software for this technological interoperability, which have been summarized and formalized in the widely-known Enterprise Integration Patterns (EIP) [16], documenting the different communication ways according to which the systems are integrated and discussing how messages can be delivered from a sender to the correct receiver, by changing the information content of a message due to different data and information models and describing the behavior of messaging system end-points.As a practical example, let us consider two distinct systems, one implemented with CORBA and another with JBoss. In order to make them interoperable, i.e., the messages exchanged on CORBA are also received by destinations in the JBoss system and vice versa, a mediation/integration entity, such as a Messaging Bridge is needed, providing one instance for each system to be integrated. As clearly illustrated in Fig. 1, such a component has a set of Channel Adapters, each in charge of sending and/or receiving messages on a particular platform (e.g., CORBA or JBoss), and an integration logic, responsible to map from one channel to the other ones by transforming the message format characterizing each communication channel into the other ones. For more details on integration issues and solutions we refer interested readers to [17–19].The second possible degree of heterogeneity is on the structural schema adopted for the exchanged data (referring to the organization of data in specific complex and simple data types), and raises the so-called Data Exchange Problem[20]. Specifically, let us consider an application, namelyAsource, which is a data source characterized by a given schema, indicated asSsource, for the produced data, and another one, namelyAdest, which is a data destination and is characterized by another schema, indicated asSdest. When the two schemas diverge, a communication can take place only if a mapping M between the two schemas exists. This allows the destination to understand the received message contents and to opportunely use them within its application logic. When the two schemas are equal, the mapping is just an “identity”. On the contrary, when several heterogeneous legacy systems are federated, it is reasonable to have diverging data schemas, and the middleware solution used for the federation needs to find the mapping M and to adopt proper mechanisms to use it in the data dissemination process. Moreover, the communication pattern adopted in collaborative infrastructures is not one-to-one, but one-to-multi or multi-to-multi. So, during a single data dissemination operation, there is no single mapping M to be considered, but several of them, i.e., one per each destination. If we consider, for example, the position on the Earth of an aircraft within an Air Traffic Control framework, we can have different coordinate systems, all based on the concepts of latitude, longitude and altitude. A given system may measure latitude and longitude with a single number (i.e., expressing decimal degrees), or with triple numbers (i.e., expressing degrees, minutes and seconds). National institutions and/or standardization bodies have tried to find a solution to this problem by specifying a standard schema for the data exchanged in certain application domains. Let us consider, for example, two explicative scenarios: one in the context of aviation and the other one form the healthcare sector. EuroControl, the civil organization coordinating and planing air traffic control in Europe, has specified the structure of the flight data exchanged among Area Control Centers, called ATM Validation ENvironment for Use towards EATMS (AVENUE).3www.eurocontrol.int/eec/public/standard_page/ERS_avenue.html.3Health Level Seven International (HL7), the global authority on standards for interoperability of health information technology, has issued a standardized application protocol for distributing clinical and administrative information among heterogeneous healthcare systems. These two standards have not resolved the syntactic heterogeneity in their domains of reference. In fact, a standard data format is likely to be changed over time for two main reasons. In the first case, it has to address novel issues by including more data. In fact, in the last three years AVENUE has been updated several times increasing the number of structures composing its schema. On the other hand, it has to evolve by adopting a different approach. In fact, there are two versions of the HL7 standard (i.e., HL7 version 3 and HL7 version 2.x), with the most recent one adopting an object-oriented approach, while a well-defined mapping between them is missing [21]. Not all the systems may be upgraded to handle new versions, so systems with different versions have to co-exist. This brings back the Data Exchange Problem when a publisher produces events with a certain version of the standard data structure and subscribers can accept and comprehend only other versions.Beyond the ability of two or more systems to be able to successfully exchange data, semantic interoperability is the ability to automatically interpret the data exchanged meaningfully and accurately as defined by the end users. For a concrete example, an integer value for the temperature of a transformer can have different interpretations, and cause different control decisions, if we consider it being expressed in Celsius, Fahrenheit or Kelvin degrees. Let us assume that a control device receives a message from the temperature monitoring device where the field Temperature has the value 86. According to the IEEE C57.12.00-2000 standard [22], the transformer temperature should not exceed 65°C above ambient temperature when operated at its rated load (KVA), voltage (V), and frequency (Hz). Knowing that the ambient temperature is 20°C , if the value of Temperature is expressed in Celsius degrees, the control device has to alarm system operators of a temperature limit violation in the transformer (i.e., the reported value of 86°C is greater than the threshold of 85°C). However, if it is expressed in different scale, such as Kelvin, an alarm should not be triggered, since the value of Temperature is equal to −187.15°C . As a different example, in aviation the altitude can have several meanings, e.g., (i) True altitude, i.e., the measure using the Mean Sea Level (MSL) as the reference datum; (ii) Absolute altitude or Height, i.e., the height of the aircraft above the terrain over which it is flying; or (iii) Flight Level, i.e., the value computed assuming an International standard sea-level pressure datum of 1013.25hPa. Parties exchanging altitude information must be clear on which definition is being used.Complex Event Processing (CEP) consists of collecting a series of data from multiple sources about what is currently happening in a certain system or environment (i.e., events) and analyzing such events in a proper manner (e.g., by looking for certain values or inferring specific event patterns) in order to detect the occurrence of certain situations or to generate new information by aggregating the available one. The data processing required by CEP is typically realized by writing computing rules based on the values exhibited by certain attributes of the received events. This may not be enough to spot the occurrence of complex critical situations, whose detection needs more data than the one carried by the exchanged events. Let us consider, for example, the following event raised by an aircraft:event AircraftState {ID=01512fg5;Type=Boeing 757;Current_Position=(41 degrees, 54′ North, 12 degrees, 27′ East);Destination=Paris;Origin=Athens;Remaning_Fuel=1000 gallons}If we limit our analysis to the values assumed by the event attributes, we cannot notice that there is a serious trouble with this aircraft. In fact, the current position exposed by the aircraft is over the city of Rome. If we consider that a Boing 757 consumes 3 gallons of fuel per mile, then the total amount of fuel needed to cover the distance between Rome and Paris (i.e., about 687 miles) is equal to 2061 gallons, which is greater than the amount available in the aircraft. This simple example tells us that certain situations cannot be detected if we do not have the domain knowledge properly formalized and available to the processing agents in charge of analyzing the exchanged events. In our example, such a domain knowledge consists in the latitude and longitude of the main cities in Europe, and the fuel consumption of the main aircraft types flying over Europe.Such a kind of semantic inference may also be needed when correlating events of different streams. Let us consider the case of the pilot noticing that the fuel is not suitable to reach its given destination, i.e., Paris, and asking for an authorization to land in a nearby local airfield, instead of a larger international one. Then, such a local airfield may publish this event:event LandingAuthorization {ID=P14254J;ICAO_ARC=1;Position=(42 degrees, 25′ North, 12 degrees, 6′ East);Date=X Month 2013;Situation=Emergency }From this event it is not possible to assume that such an authorization is for the previous aircraft in a serious danger. However, if we infer that the position in the second event refers to the town of Viterbo (Italy), and we relate the fact that Viterbo is in the air sector of Rome (the same of the considered aircraft) we can infer that the pilot of the aircraft in danger decided to make an emergency landing in the local airfield of Viterbo. However, the minimum landing runaway length for a Boing 757 should be of 3000ft, but the airfield of Viterbo is classified as 1 in the ICAO Aircraft Reference Code (given to airports with a landing runaway with a length smaller than 800m). This allows the air traffic management system to rise an alert that the landing runaway is too small, and there is a high probability that an accident may occur. Such an alert should trigger the preparation for a rescue team to be ready at the place so as to provide assistance and save some lives.These rather simple examples help to notice that the traditional data processing approaches, based on the values assumed by the event instances, are not sufficient to detect complex situations and/or obtain the advanced aggregate information needed by current applications. To this aim, such approaches have to be empowered by combining them with a semantic inference framework that can consider some knowledge on the domain and the semantics of the exchanged events.

@&#CONCLUSIONS@&#
Considering the Emerging Technologies Hype Cycle in 2012 by Gartner [42], we notice that the area of Big Data is approaching its peak of expectation. This means that such a technology is not yet mature and there are several possible research challenges that have to be still faced with. Specifically, there is a demand for a suitable platform to collect and process the data generated within the different heterogeneous systems/components of large-scale infrastructures.When integrating multiple systems to construct large-scale infrastructures by means of publish/subscribe services, it is possible to encounter some troubles due to the different data schema known by the legacy systems to be fudged. This is a major issue, since it can affect the successful notification of events towards subscribers using different schema than the publishers. In this paper we have shown how syntactic and semantic heterogeneity in publish/subscribe services can be treated by embedding a schema matching mechanism within the NS. We have experimentally proved that our solution is able to make the publish/subscribe service completely flexible in terms of data representation schema, while containing the latency degradation caused by such a flexibility.Furthermore, when considering the event exchange scenario on the resulting Big Data processing infrastructure, we also observed that traditional event processing methods compute queries based on the values that certain attributes assume at the event instances received by the processing agents. We have illustrated with concrete examples that such an approach is not able to detect and manage complex situations where domain knowledge is required. We have dealt with such an issue by proposing a method to dynamically build ontologies based on the received events, and computing semantic inferences by combining such ontologies with properly formalized domain knowledge. Such a mechanism, based on a properly crafted semantic reasoner has been experimentally evaluated, by observing a satisfactory performance and functional behavior. The presented technological framework may become the core of an open source solution supporting the analysis/processing of huge data volumes across multiple heterogeneous systems scattered throughout the Internet.