@&#MAIN-TITLE@&#
A spline-based tool to assess and visualize the calibration of multiclass risk predictions

@&#HIGHLIGHTS@&#
Generic calibration tools for multiclass risk models are proposed.Calibration plots visualize the reliability of the predicted risks.Estimated calibration index (ECI) quantifies the lack of calibration.More attention for calibration in machine learning research is required.

@&#KEYPHRASES@&#
Risk models,Probability estimation,Machine learning,Logistic regression,Calibration,Multiclass,

@&#ABSTRACT@&#
When validating risk models (or probabilistic classifiers), calibration is often overlooked. Calibration refers to the reliability of the predicted risks, i.e. whether the predicted risks correspond to observed probabilities. In medical applications this is important because treatment decisions often rely on the estimated risk of disease. The aim of this paper is to present generic tools to assess the calibration of multiclass risk models.We describe a calibration framework based on a vector spline multinomial logistic regression model. This framework can be used to generate calibration plots and calculate the estimated calibration index (ECI) to quantify lack of calibration. We illustrate these tools in relation to risk models used to characterize ovarian tumors. The outcome of the study is the surgical stage of the tumor when relevant and the final histological outcome, which is divided into five classes: benign, borderline malignant, stage I, stage II–IV, and secondary metastatic cancer. The 5909 patients included in the study are randomly split into equally large training and test sets. We developed and tested models using the following algorithms: logistic regression, support vector machines, k nearest neighbors, random forest, naive Bayes and nearest shrunken centroids.Multiclass calibration plots are interesting as an approach to visualizing the reliability of predicted risks. The ECI is a convenient tool for comparing models, but is less informative and interpretable than calibration plots. In our case study, logistic regression and random forest showed the highest degree of calibration, and the naive Bayes the lowest.

@&#INTRODUCTION@&#
For medical applications, prediction models that provide probabilistic (risk) estimates of an event of interest are useful for clinical decision support, personalized healthcare, and shared decision making. Prior to the implementation of such tools in clinical practice, validation with respect to discrimination and calibration is required [1–6]. A model should be able to distinguish between different possible outcome categories (discrimination). This can be evaluated using the area under the receiver operating characteristic curve (AUC) or multiclass extensions of this approach. Calibration assessment is often overlooked, but is of importance for several applications where risk models may be used. Such applications include decisions whether or not to treat a patient [7], start preventive action, or to inform the choice of treatment [8]. Calibration is also relevant when informing patients about risk [9], when comparing hospitals with respect to quality of care (e.g. benchmarking based on mortality risk) [10], and when identifying high risk patients for inclusion in clinical trials [11]. The optimal use of risk models in these situations relies on reliable risk estimation. For example, a classic result from decision analysis states that the adopted risk threshold to decide whether or not to take further action implies specific misclassification costs [12]: the odds of the risk threshold equals the ratio of the harm of a false positive test result to the benefit of a true positive result. For example if a risk threshold of 10% is adopted, the assumption is that 1 true positive is worth 9 false positives. If a poorly calibrated risk model is then used to assess whether patients exceed the planned threshold, inappropriate decisions may be taken.For binary outcomes, the relationship between predicted and observed probabilities can be visualized by means of a calibration plot [1,13,14]. Observed probabilities are sometimes obtained by computing event rates within groups of patients with similar predicted probabilities (e.g. decile split). However, often flexible smoothing methods such as local regression (loess) or splines are used to link predicted probabilities to estimated observed probabilities [1].Recently, our group extended binary calibration plots to multiclass models based on multinomial logistic regression (MLR) [15]. We proposed two frameworks, one parametric and one non-parametric. Logistic regression is a common algorithm to build binary and multiclass clinical prediction models, and naturally works with risk estimates. However, machine learning algorithms are also used for clinical risk prediction [16–21], and are very frequently used in high dimensional and/or “large p, small n” prediction studies (i.e. a large number of predictors and a small number of patients) [22–24]. Moreover, although using machine-learning approaches for classification problems is often less suited to probability estimation, methods do exist to facilitate this [25–30]. The calibration performance of risk models is an issue that is often neglected, and it is not surprising that with a few exceptions this is frequently the case for models based on machine learning algorithms [13,26,27,30–32].The aim of this paper is to introduce a non-parametric framework to evaluate the calibration of multiclass risk models irrespective of the modeling technique used. Based on this framework we also derive a calibration measure to quantify and compare calibration performance between models. We illustrate these methods with a case study looking at the classification of ovarian tumors. We develop and validate risk models to diagnose tumor pathology based on logistic regression, support vector machines, k-nearest neighbors, random forest, naive Bayes and nearest shrunken centroids.Our group developed calibration tools for risk models based on multinomial logistic regression (MLR) [15]. Assume an MLR or ‘baseline-category logit’ model [33] with m predictorsx1toxmfor an outcome with J (j=1,…,J) categories. If category 1 is chosen as the reference category, the model is written as(1)logPY=2PY=1=α2+∑l=1mβ2lxl=lp21logPY=3PY=1=α3+∑l=1mβ3lxl=lp31…logPY=JPY=1=αJ+∑l=1mβJlxl=lpJ1and the multiclass risks are obtained as(2)PY=1=p1=11+exp(lp21)+exp(lp31)+⋯+exp(lpJ1)=11+∑j=2Jexp(lpj1)PY=2=p2=exp(lp21)1+exp(lp21)+exp(lp31)+⋯+exp(lpJ1)=exp(lp21)1+∑j=2Jexp(lpj1)…PY=J=pJ=exp(lpJ1)1+exp(lp21)+exp(lp31)+⋯+exp(lpJ1)=exp(lpJ1)1+∑j=2Jexp(lpj1)Letlpˆ21,…,lpˆJ1denote the estimated linear predictors andpˆ1,…,pˆJthe estimated multiclass risks. The non-parametric recalibration framework for such models relates the multiclass outcome Y on the estimatedJ-1linear predictorslpˆ21,…,lpˆJ1from the MLR risk model through a vector spline [34] MLR analysis [15]:(3)logP(Y=2)/P(Y=1)=a2+∑j=2Jb2j·s2lpˆj1logPY=3/PY=1=a3+∑j=2Jb3j·s3lpˆj1…logPY=J/PY=1=aJ+∑j=2JbJj·sJlpˆj1withs(·)=s2(·),s3(·),…,sJ(·)a vector spline smoother applied to each linear predictor [15,34]. This vector spline smoothers(·)is a natural extension of the cubic spline smoother to vector responses and consists ofJ-1natural cubic B-splinessj(·)[34,35]. Similarly as the multiclass risks in (2) are obtained from (1), this framework can be used to estimate the observed probabilitiesoˆ1,…,oˆJ(Appendix A) [15].If we now consider a generic multiclass risk model (i.e. irrespective of how it was developed) yielding risk estimatespˆ1,…,pˆJ. In order to use the nonparametric recalibration model we first calculatelogpˆj/pˆ1=zˆj1, with j=2,…,J if category 1 is used as reference category. These quantities are used as predictors in the vector spline MLR model (3) to estimate the observed probabilities:(4)logP(Y=2)/P(Y=1)=a2+∑j=2Jb2j·s2zˆj1logP(Y=3)/P(Y=1)=a3+∑j=2Jb3j·s3zˆj1…logP(Y=J)/P(Y=1)=aJ+∑j=2JbJj·sJzˆj1The observed probabilitiesoˆjare obtained by using the vector spline MLR model (4) and then applying the equations like (2) (Appendix A). Calibration plots are obtained by plotting the predicted probabilitiespˆjvs the observed probabilitiesoˆjfor each outcome categoryj(j=1,…,J)[15]. Note that there is no one-to-one relationship betweenpˆjandoˆj. The reason is that, for a specific value ofpˆj, predicted probabilities for the other J−1 categories can vary and this will lead to different values foroˆj. Therefore, spline smoothers are plotted to assess the trend of the scatter plot for each category [15].To quantify the lack of calibration with a single measure, the correspondence betweenpˆjandoˆjcan be assessed through their squared difference averaged over J categories and N observations:∑n=1N∑j=1Jpˆnj-oˆnj2N·J. If we multiply the average squared difference by 100J/2 we obtain a measure that we refer to as the estimated calibration index (ECI) because it is based on estimates of the observed probabilitiesoˆ1,…,oˆJ. The multiplication factor 100J/2 ensures that the ECI has a theoretical range between 0 and 100 (Appendix B). The ECI has similarities to the Brier score. However, the Brier score represents the average squared difference between the actual outcomesynand the predicted probabilities such that it is an overall performance measure that captures discrimination and calibration: Brier score =∑n=1N∑j=1Jynj-pˆnj2N·J[36]. In contrast, the ECI is the average squared difference of the predicted probabilitiespˆnwith the estimated observed probabilitiesoˆninstead of the actual outcomes.The accurate diagnosis of ovarian tumors prior to surgery is crucial when choosing appropriate patient management and referral. As different types of malignancies require different management, we aimed to develop a model to predict the risk that an ovarian tumor is benign, borderline malignant, a stage I cancer, a stage II–IV cancer, or secondary metastatic cancer. We used data from the International Ovarian Tumor Analysis (IOTA) consortium [37–39]. These data are derived from 5909 patients collected between 1999 and 2012 at 24 centers in 10 countries [40], and were randomly divided into a development and validation set stratified for the final outcome. The outcome is based on the histo-pathological diagnosis of the mass after surgical removal by laparotomy or laparoscopy as well as the stage of the tumor (when relevant) as assessed by the surgeon. Pathologists were blind to the data collected for the study and the results of any risk models we developed. The following predictors or features were considered for model development without further selection: age (years), serum CA125 (U/ml), oncology referral center (yes/no), maximum diameter of the lesion (mm), proportion of solid tissue (between 0 and 1), presence of more than 10 cyst locules (yes/no), number of papillary projections (0, 1, 2, 3 and more than 3), presence of acoustic shadows (yes/no) and presence of ascites (yes/no) (Table 1). The proportion of solid tissue is defined as the ratio of the maximum diameter of the largest solid component and the maximum diameter of the lesion. These nine variables are the predictors from the multinomial risk prediction model ADNEX [40]. Some values for serum CA125 were missing and were imputed in order not to lose these records. We used predictive mean matching regression [41] using variables that were either related to the level of CA125 itself, or the unavailability of CA125 via an indicator as to whether the CA125 level was missing or not [42].We develop and validate various binary and multiclass risk models to diagnose ovarian tumors. The models are based on logistic regression and machine learning algorithms (see Appendix C). In the first part of the study, we develop binary models to distinguish between benign and malignant tumors. We then discuss multiclass problems by developing models to distinguish between benign, borderline malignant, stage I invasive, stage II–IV invasive and secondary metastatic ovarian tumors. The available data were split into a development set and a validation set. The split was random, using a 1:1 ratio with stratification according to the multiclass outcome. R version 3.0.3 (www.r-project.org) was used for the statistical analysis.

@&#CONCLUSIONS@&#
