@&#MAIN-TITLE@&#
Neighbors’ distribution property and sample reduction for support vector machines

@&#HIGHLIGHTS@&#
A novel method is proposed to detect boundary regions of data set.The samples locating in boundary regions take place of overlap regions to represent training set during pre-processing of SVMs.The proposed method can avoid assuming that there are overlap regions between different classes.

@&#KEYPHRASES@&#
Neighbors’ distribution property,Sample-neighbor angle,Cosine sum,Sample reduction,SVM,

@&#ABSTRACT@&#
For data pre-processing of SVMs, many scholars tried to find those samples, which would become support vectors. Generally, support vectors locate in the overlap regions, which are between different classes. But overlap region does not always exist. In this paper, a new method is proposed to find the boundary regions of each class instead of overlap regions. This method could deal with the dataset without overlap regions. Summing the cosine of the sample-neighbor angle, the sum ranges from 0 to k. When the sample locates in the boundary region of data distribution, the sum would be close to k; when the sample locates in the interior of the data distribution, the sum would be close to 0. Using cosine sum, the samples locating in the interior of each class can be disposed before SVMs training. Experimental results show that the proposed method can solve the problem, which the methods based on finding overlap regions cannot deal with.

@&#INTRODUCTION@&#
Support vector machine is a classical machine learning technology [1,2]. It has been widely used in many fields such as face recognition [3–5], gene selection [6], text categorization [7], and novelty detection [8]. Since Support vector machine needs to solve a quadratic programming [1,2]. The time complexity and space complexity are both very high. This limits the application of the support vector machine in large-scale datasets. In order to solve this problem, many scholars researched it and proposed many methods. Osuna proposed the decomposition algorithm, which fixed the working set size [9]. Platt proposed SMO algorithm, which only preserved 2 samples in the working set [10]. It can be deemed as a special case of decomposition algorithm. Tsang proposed core vector machine, which is based on minimum core-set [11]. The aim of those algorithms is to reduce the size of working set. Fung G replaced the inequality constraints of quadratic programming with equality constraints in order to avoid solving quadratic programming [12]. Joachims applied cutting plane methods to SVM [13]. Keerthi proved that solving the problem of maximum margin is equivalent to solving the problem of the minimum distance between two convex polytopes [14]. Only the samples on the convex hull influence on solving the problem of the minimum distance between two convex polytopes.Another solution is to select a subset of training set. It can reduce the complexity of the original problem. A novel method, finding boundary regions of each class is proposed in this paper. Using boundary regions of each class to represent original set could avoid the assumption that there are overlap regions in training set, which proposed by Shin [19].The remaining part of this paper is organized as follows. Related work will be introduced in Section 2. A novel method for reducing training set is proposed in Section 3. Section 4 provides the experimental results of the proposed method. Discussion and conclusive remarks are provided in Section 5.

@&#CONCLUSIONS@&#
In this paper, a new sample reduction method for SVM is proposed. The proposed method is to find the boundary regions of each class instead of overlap regions between different classes. It can cope with the training set without overlap regions. The experiments performed on artificial synthetic datasets and real-world benchmark datasets demonstrate that the proposed method can avoid the deficiency of the methods based on finding overlap regions.The methods based on finding overlap regions require two classes at least. So it could not be applied to one-class problem. The proposed method does not relate to the number of classes. It is also suitable for one-class problem. In addition, the proposed method also can be used in the algorithms, which only focus on the boundary of the data distribution. In these algorithms, it is sufficient to preserve the samples in the boundary regions of each class.