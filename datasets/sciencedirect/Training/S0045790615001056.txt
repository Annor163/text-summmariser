@&#MAIN-TITLE@&#
A hierarchical Dirichlet process mixture of generalized Dirichlet distributions for feature selection

@&#HIGHLIGHTS@&#
A statistical framework based on hierarchical Dirichlet processes and generalized Dirichlet distribution is developed.The framework simultaneously performs model parameters estimations as well as model complexity determination.The learning of the model is done via variational Bayes inference.The efficiency of the proposed algorithm is validated via challenging applications.

@&#KEYPHRASES@&#
Clustering,Hierarchical Dirichlet process,Variational learning,Face detection,Facial expression recognition,Human gesture recognition,

@&#ABSTRACT@&#
This paper addresses the problem of identifying meaningful patterns and trends in data via clustering (i.e. automatically dividing a data set into meaningful homogenous sub-groups such that the data within the same sub-group are very similar, and data in different sub-groups are very different). The clustering framework that we propose is based on the generalized Dirichlet distribution, which is widely accepted as a flexible modeling approach, and a hierarchical Dirichlet process mixture prior. A main advantage of the adopted hierarchical Dirichlet process is that it provides a principled elegant nonparametric Bayesian approach to model selection by supposing that the number of mixture components can go to infinity. In addition to capturing the structure of the data, the combination of hierarchical Dirichlet process and generalized Dirichlet distribution is shown to offer a natural efficient solution to the feature selection problem when dealing with high-dimensional data. We develop two variational learning approaches (i.e. batch and incremental) for learning the parameters of the proposed model. The batch algorithm examines the entire data set at once while the incremental one learns the model one step at a time (i.e. update the model’s parameters each time new data are introduced). The utility of the proposed approach is demonstrated on real applications namely face detection, facial expression recognition, human gesture recognition, and off-line writer identification. The obtained results show clearly the merits of our statistical framework.

@&#INTRODUCTION@&#
In the last few years, we have seen an explosion of on-line data in digital form. It is crucial then to develop approaches and techniques to index and understand this content, and to represent it in compact forms. This is the case, for instance, of many forensics applications where one of the main goals is to automatically analyze or predict criminal incidents (see, for instance, [1]) which is vital for law enforcement agencies. Examples include intrusion detection, video-surveillance, user authentification (i.e. the task of confirming or denying the identity claimed by a given person) via face or fingerprint recognition to name a few [2]. Statistical approaches in general and graphical models in particular have been widely used for data (i.e. text, image, video) interpretation, understanding and modeling. Mixture models, as a particular case of graphical models, with their strong theoretical roots are known to be excellent statistical approaches for data analysis by offering a formal approach to unsupervised learning problems [3]. These models are at the heart of many challenging applications from different domains, but their deployment requires the resolution of some problems namely the choice of the components densities, the estimation of the parameters and model selection (i.e. determination of the number of components or model’s complexity). Concerning the choice of components densities, apart from some exceptions most mixture-based approaches consider Gaussian distributions [4]. Recently, however, several works have shown that this choice is not appropriate in many real-life applications and that other distributions such as the generalized Dirichlet may offer better modeling capabilities especially when dealing with proportional data [5]. Thus, in this work we consider the generalized Dirichlet as the parent distribution of our models. Concerning model selection, several approaches have been proposed in the past and are deeply discussed in [5]. The majority of the used approaches consider information theory-based criteria such as minimum description length and minimum message length [5]. Yet, these approaches are computationally expensive since one needs to run the learning algorithm for every candidate number of components. Unlike these conventional approaches which assume a finite number of components, we do not confine ourselves to this assumption. Indeed, infinite mixtures are considered, where a new data point can be affected to a new cluster that was not previously seen, via the adoption of a nonparametric Bayesian approach based on hierarchical Dirichlet process prior. We are mainly motivated by the excellent results obtained recently thanks to Dirichlet processes which avoid elegantly model selection problems [6]. For a comprehensive treatment of nonparametric Bayesian approaches and Dirichlet process the reader is referred to [7] and references therein.One major limitation of the implementation of mixture models in real-life scenarios is dealing with high-dimensional data which causes sparsely populated spaces. Feature selection techniques have been widely used in the past to tackle this problem and allow generally to attain significant generalization improvement, to unseen data, in many applications such as text classification, image retrieval and annotation. Indeed, it is crucial to take into account the fact that features are generally not equally important (or relevant) for a given task (e.g. clustering) and that irrelevant features may even compromise modeling capabilities (e.g. by blurring clusters). Feature selection is a difficult problem especially in unsupervised settings, characterized by the absence of labeled data that could guide the selection process, which is actually our case since we are considering mixture models. In this context, features have been generally assumed to be independent (e.g. by considering Gaussians with diagonal covariance matrices) [4], to reduce complexity and the number of parameters to learn, which is not the case in several real-life applications [8]. However, the authors in [8] have shown that this assumption could be avoided in the case of the generalized Dirichlet distribution thanks to some interesting mathematical properties that allow the independence between features to become a fact, via a simple geometric transformation, as it will be explained in the next section.A main drawback each time nonparametric Bayesian approaches have been considered is the fact that learning has been typically performed using Markov Chain Monte Carlo (MCMC) techniques namely Gibbs sampling and Metropolis–Hastings which is clearly time consuming [9]. There is a rich tradition in machine learning of studying inference techniques. The design of a good inference algorithm is particularly difficult. Recently, the variational approach has received a lot of attention by offering a compromise between deterministic approaches and purely Bayesian ones. Indeed, it can be viewed as deterministic approximation to Bayesian inference that allows to incorporate prior information in a principled way. The main idea is to approximate the model posterior distribution by minimizing the Kullback–Leibler divergence between the exact (or true) posterior and an approximating distribution which have had some impressive successes in learning complex statistical models (see, for instance, [10–12] and references therein). Thus, an important contribution of this work is the development of a variational algorithm to learn our hierarchical Dirichlet process mixture of generalized Dirichlet distributions. In order to face the fact that data extracted from real-life applications are generally dynamic, we extend our batch learning algorithm to online setting where data are supposed to arrive sequentially which is crucial in several real-life applications.The rest of this paper is structured as follows. The paper begins with a presentation of the hierarchical Dirichlet process mixture of GD Distributions in Section 2, followed by the development of our batch and online learning algorithms in Section 3. Section 4 presents the experimental results in the context of four challenging real-life applications namely face detection, facial expression recognition, human gesture recognition, and off-line writer identification. Section 5 concludes the paper with some final remarks.The hierarchical Dirichlet process framework is particularly useful in problems for modeling grouped data where observations are organized into groups allowed to remain statistically linked by sharing mixture components [13]. In this section, we develop a hierarchical Dirichlet process mixture model of generalized Dirichlet distributions with an unsupervised feature selection scheme.The formal definition of the Dirichlet process [14] is as following: let H be a distribution over some probability space Θ and γ be a positive real number, then a random distribution G is distributed according to a Dirichlet process with the base distribution H and concentration parameter γ, denoted asG∼DP(γ,H), if(1)(G(A1),…,G(At))∼Dir(γH(A1),…,γH(At))where(A1,…,At)is the set of the finite partitions of Θ, andDir(γH(A1),…,γH(At))is a finite-dimensional Dirichlet distribution with parameters(γH(A1),…,γH(At)). Recently, hierarchical Dirichlet process [13] has been developed as a hierarchical nonparametric Bayesian model and has shown promising results to the problem of model-based clustering of grouped data with sharing clusters. It is built on the Dirichlet process which involves a Bayesian hierarchy where the base measure for a Dirichlet process is itself distributed according to a Dirichlet process. Now, let us introduce the general setting of a two-level hierarchical Dirichlet process: Assume that we have a grouped data set, in which each group is associated with an infinite mixture model (a Dirichlet processGj). This indexed set of Dirichlet processes{Gj}shares a base distributionG0, which is itself distributed as a Dirichlet process:(2)G0∼DP(γ,H)Gj∼DP(λ,G0)foreachj,j∈{1,…,M}where j is an index for each group of data. In this work, we represent a hierarchical Dirichlet process in a more intuitive and straightforward form through two stick-breaking constructions which involves a global-level and a group-level construction [15,16]. In the global-level construction, the global measureG0is distributed according to a Dirichlet processDP(γ,H)and can be described using a stick-breaking representation asψk′∼Beta(1,γ)Ωk∼Hψk=ψk′∏s=1k-1(1-ψs′)G0=∑k=1∞ψkδΩkwhere{Ωk}is a set of independent random variables distributed according toH,δΩkis an atom atΩk. The random variablesψkare known as the stick-breaking weights that satisfy∑k=1∞ψk=1, and are obtained by recursively breaking a unit length stick into an infinite number of pieces such that the size of each successive piece is proportional to the rest of the stick. SinceG0is the base distribution of Dirichlet processesGjand has a stick-breaking representation, the atomsΩkare shared among allGjand differ only in weights. In this work, we apply the conventional stick-breaking representation as mentioned in [10] to construct each group-level Dirichlet processGjasπjt′∼Beta(1,λ)ϖjt∼G0πjt=πjt′∏s=1t-1(1-πjs′)Gj=∑t=1∞πjtδϖjtwhereδϖjtare group-level atoms atϖjt,{πjt}is a set of stick-breaking weights that satisfy∑t=1∞πjt=1. Sinceϖjtis distributed according to the base distributionG0, it takes on the valueΩkwith probabilityψk. This can also be represented using a binary latent variableWjtkas an indicator variable, such thatWjtk∈{0,1},Wjtk=1ifϖjtmaps to the base-level atomΩkwhich is indexed by k; otherwise,Wjtk=0. Then, we can haveϖjt=ΩkWjtk. As a result, group-level atomsϖjtdo not need to be explicitly represented which further simplifies the inference process as it shall be clearer later. The indicator variableW→=(Wjt1,Wjt2,…)is distributed according toψ→in the form(3)p(W→|ψ→)=∏j=1M∏t=1∞∏k=1∞ψkWjtkSinceψ→is a function ofψ→′according to the stick-breaking construction,p(W→)can then be represented as(4)p(W→|ψ→′)=∏j=1M∏t=1∞∏k=1∞ψk′∏s=1k-1(1-ψs′)WjtkThe prior distribution ofψ→′is a specific Beta distribution according to Eq. (3) as(5)p(ψ→′)=∏k=1∞Beta(1,γk)=∏k=1∞γk(1-ψk′)γk-1One of the most significant applications of hierarchical Dirichlet process is as a nonparametric prior over the factors for grouped data. Specifically, let i indexes the observations within each group j, we assume that each variableθjiis a factor corresponding to an observationXji, and the factorsθ→j=(θj1,θj2,…)are distributed according to Dirichlet processGj, one for each j. The likelihood function can then be defined as(6)θji|Gj∼GjXji|θji∼F(θji)whereF(θji)represents the distribution of the observationXjigivenθji. Next, we introduce a binary latent variableZjit∈{0,1}as an indicator variable. That is,Zjit=1ifθjiis associated with component t and maps to the group-level atomϖjt; otherwise,Zjit=0. Therefore, we haveθji=ϖjtZjit. Sinceϖjtalso maps to the global-level atomΩk, we then haveθji=ϖjtZjit=ΩkWjtkZjit. The indicator variableZ→=(Zji1,Zji2,…)is distributed according toπ→as(7)p(Z→|π→)=∏j=1M∏i=1N∏t=1∞πjtZjitAccording to the stick-breaking construction of the Dirichlet process in Eq. (4),π→is a function ofπ→′. We then have(8)p(Z→|π→′)=∏j=1M∏i=1N∏t=1∞πjt′∏s=1t-1(1-πjs′)ZjitThe prior distribution ofπ→′is a specific Beta distribution as shown in Eq. (4) as(9)p(π→′)=∏j=1M∏t=1∞Beta(1,λjt)=∏j=1M∏t=1∞λjt(1-πjt′)λjt-1In this paper, we focus on a specific form of hierarchical Dirichlet process mixture model where each observation within a group is drawn from a mixture of generalized Dirichlet (GD) distributions. Since Dirichlet process mixture models are often considered as infinite mixture models, we refer to the proposed model as the hierarchical infinite GD mixture model. The motivation of focusing on GD mixture is due to its superior performance in modeling high-dimensional proportional data (i.e. normalized histograms) that naturally arise by many applications, such as text, image and video modeling [11,12]. Moreover, the GD distribution has a more general covariance structure than Dirichlet distribution which makes it to be more practical and useful.Assume that we have a D-dimensional random vectorY→=(Y1,…,YD)which is drawn from a GD distribution with parametersα→=(α1,…,αD)andβ→=(β1,…,βD)(10)GD(Y→|α→,β→)=∏l=1DΓ(αl+βl)Γ(αl)Γ(βl)Ylαl-11-∑f=1lYfγlwhere∑l=1DYl<1and0<Yl<1forl=1,…,D,αl>0,βl>0,γl=βl-αl+1-βl+1forl=1,…,D-1,γD=βD-1, andΓ(·)is the gamma function. Based on an interesting mathematical property of the GD distribution which is thoroughly discussed in [8], we can transform the original data pointY→using a geometric transformation into another D-dimensional data pointX→with independent features in the form of(11)GD(X→|α→,β→)=∏l=1DBeta(Xl|αl,βl)whereX→=(X1,…,XD),X1=Y1andXl=Yl1-∑f=1l-1Yfforl>1, andBeta(Xl|αl,βl)is a Beta distribution defined with parameters{αl,βl}. As a result, the estimation of a D-dimensional GD distribution is transformed to D estimations of one-dimensional Beta distributions which may facilitate the inference process for multidimensional data [8]. Moreover, this property is important since the independence between the features now becomes a fact rather than an assumption as considered in previous unsupervised feature selection based on Gaussian mixtures.Now let us consider a data setXcontaining N random vectors that are separated into M groups, then each vectorX→ji=(Xji1,…,XjiD)is represented in a D-dimensional space and is drawn from a hierarchical infinite GD mixture model. Then, the corresponding likelihood function of the proposed model with latent variables can be written as(12)p(X)=∏j=1M∏i=1N∏t=1∞∏k=1∞∏l=1DBeta(Xjil|αkl,βkl)ZjitWjtkIn practice, not all the features are important, some of them may be irrelevant and may even degrade the clustering performance. Therefore, feature selection technique is important and is adopted here as a tool to choose the “best” feature subset. The most common feature selection technique, in the context of unsupervised learning, defines an irrelevant feature as the one having a distribution independent from class labels [4]. Therefore, in our work the distribution of each featureXlcan be defined by(13)p(Xjil)=Beta(Xjil|αkl,βkl)ϕjilBeta(Xjil|αl′,βl′)1-ϕjilwhereϕjilis a binary latent variable that represents the feature relevance indicator, such thatϕjil=0means that the feature l of group j is irrelevant (i.e. noise) and follows a Beta distribution:Beta(Xjil|αl′,βl′); otherwise, the featureXjilis relevant. The prior distribution ofϕ→is defined as(14)p(ϕ→|∊→)=∏j=1M∏i=1N∏l=1D∊l1ϕjil∊l21-ϕjilwhere eachϕjilis a Bernoulli variable such thatp(ϕjil=1)=∊l1andp(ϕjil=0)=∊l2. The vector∊→=(∊→1,…,∊→D)represents the features saliencies (i.e. the probabilities that the features are relevant) such that∊→l=(∊l1,∊l2)and∊l1+∊l2=1. Furthermore, a Dirichlet distribution is chosen over∊→as(15)p(∊→)=∏l=1DDir(∊→l|ξ→)=∏l=1DΓ(ξ1+ξ2)Γ(ξ1)Γ(ξ2)∊l1ξ1-1∊l2ξ2-1By incorporating this unsupervised feature selection scheme into our hierarchical infinite GD mixture model, we then have the following likelihood function(16)p(X|Z→,W→,θ→,ϕ→)=∏j=1M∏i=1N∏t=1∞∏k=1∞∏l=1DBeta(Xjil|αkl,βkl)ϕjilBeta(Xjil|αl′,βl′)(1-ϕjil)ZjitWjtkwhereθ→={α→,β→,α→′,β→′}. In the next step, we need to introduce the prior distributions for parametersα→,β→,α→′andβ→′, of Beta distributions. Although Beta distribution belongs to the exponential family and has a formal conjugate prior, it is analytically intractable. Thus, Gamma distribution is adopted to approximate the conjugate prior by assuming that these Beta parameters are statistically independent as(17)p(α→)=G(α→|u→,v→)=∏k=1∞∏l=1DvkluklΓ(ukl)αklukl-1e-vklαklp(β→)=G(β→|g→,h→)=∏k=1∞∏l=1DhklgklΓ(gkl)βgkl-1e-hklβkl(18)p(α→′)=G(α→′|u→′,v→′)=∏l=1Dvl′ul′Γ(ul′)αl′ul′-1e-vl′αl′p(β→′)=G(β→′|g→′,h→′)=∏l=1Dhl′gl′Γ(gl′)βl′gl′-1e-hl′βl′Variational inference [17] is a deterministic approximation technique that is used to find tractable approximations for posterior distributions of a variety of statistical models. In this section, we first propose a Batch variational inference method for learning the proposed model. Then, we introduce an online learning extension which is useful in applications involving large-scale or streaming data. In order to simplify notations, we defineΘ=(Ξ,Λ)as the set of latent and unknown random variables, whereΞ={Z→,ϕ→}andΛ={W→,∊→,ψ→,π→′,α→,β→,α→′,β→′}.In variational inference, the goal is to find an approximationQ(Θ)to the true posterior distributionp(Θ|X)by maximizing the lower bound oflnp(X)which is defined by(19)L(Q)=∫Q(Θ)ln[p(X,Θ)/Q(Θ)]dΘIn this work, we adopt factorial approximation which is commonly used in variational inference [17] to factorizeQ(Θ)into disjoint tractable distributions. Moreover, we apply a truncation technique as in [18] to truncate the variational approximations of global- and group-level Dirichlet process at K and T, such that(20)ψK′=1,∑k=1Kψk=1,ψk=0whenk>K(21)πjT′=1,∑t=1Tπjt=1,πjt=0whent>TThe truncation levels K and T are variational parameters which can be freely initialized and will be optimized automatically during the learning process. By using the truncated stick-breaking representations and the factorization assumption, the approximated posterior distributionq(Θ)can be fully factorized into disjoint distributions as(22)q(Θ)=q(Z→)q(W→)q(ϕ→)q(π→′)q(ψ→′)q(α→)q(β→)q(α→′)q(β→′)q(∊→)In our work, variational inference is performed based on a natural gradient method as introduced in [19] which is motivated by the readily extension of this gradient-based approach to online learning. The main idea is that, since our model has conjugate priors, the functional form of each factor in the variational posterior distribution is known. Therefore, the lower boundL(q)can be considered as a function of the parameters of these distributions by taking general parametric forms of these distributions. The optimization of variational factors is then obtained by maximizing the lower bound with respect to these parameters. In our case, the functional form of each variational factor is the same as its conjugate prior distribution, namely Discrete forZ→andW→, Bernoulli forϕ→, Dirichlet for∊→, Beta forψ→′andπ→′, and Gamma forα→,β→,α→′andβ→′:(23)q(Z→)=∏j=1M∏i=1N∏t=1TρjitZjitq(W→)=∏j=1M∏t=1T∏k=1KϑjtkWjtkq(ϕ→)=∏j=1M∏i=1N∏l=1Dφjilϕjil(1-φjil)1-ϕjil(24)q(∊→)=∏l=1DDir(∊→l|ξ→∗)q(π→′)=∏j=1M∏t=1TBeta(πjt′|ajt,bjt)q(ψ→′)=∏k=1KBeta(ψk′|ck,dk)(25)q(α→)=∏k=1K∏l=1DG(αkl|ũkl,ṽkl)q(β→)=∏k=1K∏l=1DG(βkl|g̃kl,h̃kl)(26)q(α→′)=∏l=1DG(αl′|ũl′,ṽl′)q(β→′)=∏l=1DG(βl′|g̃l′,h̃l′)Consequently, the parameterized lower boundL(q)can be obtained by substituting Eqs. (23)–(26) into Eq. (19). Maximizing this bound with respect to these parameters then gives the required re-estimation equations developed in Appendix A. The batch variational inference for our model can be considered as an EM-like algorithm and is summarized in Algorithm 1. The convergence of this batch learning algorithm is guaranteed and can be monitored through inspection of the variational lower bound.Algorithm 1Batch variational learning of the model.1: Choose the initial truncation levels K and T.2: Initialize the values for hyperparametersλjt,γk,ukl,vkl,gkl,hkl,ul′,vl′,gl′,hl′,ξ1andξ2.3: Initialize the value ofρjitby K-Means algorithm.4: repeat5:The variational E-step:6: Estimate the expected values in Eqs. (52)–(58), use the current distributions over the model parameters.7:The variational M-step:8: Update the variational solutions for each factor using Eqs. (23)–(26) and the current values of the moments.9: until Convergence criterion is reached.In this section, an online variational inference approach is developed, using the framework presented in [19], to learn our model. In contrast with batch learning algorithms, online algorithms are more efficient when dealing with large-scale or streaming data. In our case, let r denotes the amount of observed data that we currently have. Then, the current lower bound for the observed data can be calculated by(27)L(r)(q)=Nr∑i=1r∫q(Λ)dΛ∑Z→iQ(Ξ→i)lnp(X→i,Ξ→i|Λ)q(Ξ→i)+∫q(Λ)lnp(Λ)q(Λ)dΛwhereΞ={Z→,ϕ→}andΛ={W→,∊→,ψ→,π→′,α→,β→,α→′,β→′}. The main idea of the online variational learning algorithm is to successively maximize the current variational lower bound as in Eq. (27) with respect to each variational factor. Assume that we have already observed a data set{X1,…X(r-1)}. Then, after obtaining a new observationXr, we can maximize the current lower boundL(r)(q)with respect toq(ϕ→r)while other variational factors remain fixed to their(r-1)th values. Thus, the variational solution toq(ϕ→r)can be updated as(28)q(ϕ→r)=∏j=1M∏l=1Dφjrlϕjrl(1-φjrl)1-ϕjrl(29)φjrl=exp(φ∼jrl)exp(φ∼jrl)+exp(φ^jrl)(30)φ∼jrl=∑t=1T∑k=1K〈Zj(r-1)t〉〈Wjtk(r-1)〉[R∼kl(r-1)+(α¯kl(r-1)-1)lnXjrl+(β¯kl(r-1)-1)ln(1-Xjrl)]+〈ln∊l1(r-1)〉(31)φ^jrl=R∼l′(r-1)+〈ln∊l2(r-1)〉+(β¯l′(r-1)-1)ln(1-Xjrl)+(α¯l′(r-1)-1)lnXjrlThe updating of the variational solutions with respect to the other factors is detailed in Appendix B.Since the hyperparameters ofq(r)(π→′),q(r)(ψ→′),q(r)(α→)q(r)(β→),q(r)(α→′)are independent from each other, they can be updated in parallel. This online variational inference procedure is repeated until all the variational factors are updated with respect to the current arrived observation. According to [19], the online variational algorithm can be defined as a stochastic approximation method [20] for estimating the expected lower bound and the convergence is guaranteed if the learning rate satisfies the following conditions:(32)∑r=1∞ζr=∞,∑r=1∞ζr2<∞The proposed online learning algorithm is much more computationally efficient than its batch counterpart. This is due to the fact that the batch algorithm updates the variational factors by using the whole data set in each iteration, and thus its estimation quality is improved much more slowly than in the case of the online one. The online variational inference for hierarchical infinite GD mixture model with feature selection is summarized in Algorithm 2.Algorithm 2Online variational learning of the model.1: Choose the initial truncation levels K and T.2: Initialize the values for hyperparametersλjt,γk,ukl,vkl,gkl,hkl,ul′,vl′,gl′,hl′,ξ1andξ2.3: forr=1→Ndo4:The variational E-step:5: Update the variational solutions toq(ϕ→r)andq(Z→r), sequentially using Eqs. (28) and (59).6:The variational M-step:7: Compute learning rateζr=(η0+r)-ς.8: Calculate natural gradientsΔϑjtk(r)andΔξ→∗(r)using Eqs. (64) and (65).9: Update variational factorsq(r)(W→)andq(r)(∊→)as shown in Eqs. (62) and (63).10: Calculate the natural gradients of the remaining hyperparameters using Eqs. (79)–(88).11:Update variational factorsq(r)(π→′),q(r)(ψ→′),q(r)(α→),q(r)(β→),q(r)(α→′)andq(r)(β→′)through Eqs. (70)–(72).12:Repeat the variational E-step and M-step until new data is observed.13: end for

@&#CONCLUSIONS@&#
