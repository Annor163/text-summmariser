@&#MAIN-TITLE@&#
Single frame correction of motion artifacts in PMD-based time of flight cameras

@&#HIGHLIGHTS@&#
Motion Artifacts (MoArt) are a non-systematic error in Time of Flight (ToF) imaging.MoArt emerge when motion appears during the integration time.MoArt cause distorted ToF measurements (depth and amplitude).We propose a single frame correction of ToF measurements with motion flow estimation.We present quantitative and qualitative results in challenging scenarios.

@&#KEYPHRASES@&#
Time of flight (ToF),Motion artifacts (MoArt),Photonic mixer devices (PMD),Motion flow,Phase-shift algorithm,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Time of flight cameras provide “per-pixel” distance between the camera and the target scene. ToF cameras obtain depth from the phase shift between two signals: i) square periodic signal used for infrared light modulation and ii) light reflected from the scene, that camera pixels convert into an electrical signal (see [1]). Current ToF technologies can be classified into three categories: i) pulsed modulation, ii) continuous wave modulation and iii) pseudo-noise modulation. The most established architecture is based on continuous wave modulation and Photonic Mixer Device (PMD) technology. PMD ToF cameras are based on the so-called “4 phase-shift” system where each pixel of the ToF sensor includes a 2-well CMOS architecture. Each well integrates samples of the correlation function between the incoming reflected infrared signal and the reference signal used for light modulation. Current PMD ToF cameras sample the correlation function at 4 different phase shifts. With these 4 samples one can obtain depth while removing gain and offset variations in the captured signal.Each ToF “smart pixel” integrates charge during hundreds of nanoseconds on each stage, which provides sub-centimeter accuracy at a high frame rate. PMD ToF technology has thus become a competitive alternative to other depth sensing technologies, such as stereo vision, laser or structure light projection (see [2] for an overview).However, PMD ToF technology suffers from error sources that still limit its accuracy (see [1]). These errors can be classified into systematic and non-systematic errors.Among the systematic errors we highlight the following: i) wiggling error due to the use of non-ideal sinusoidal signals, ii) pixel-related error arising from a Fixed Pattern Noise (FPN), iii) amplitude-related errors due to the non-uniformity of the infrared (IR) illumination and reflectivity variations, iv) photon shot noise that influences the number of collected electrons during the integration time and v) temperature-related error due to its influence in the semiconductor properties. These error sources have been discussed widely in the literature (see [3] for a comprehensive survey). Most of commercial cameras include nowadays hardware compensation for many of these errors.Correction of non-systematic errors is more challenging as their influence depends on the target scene and cannot be calibrated (see [4]). We highlight: i) flying pixels, ii) depth ambiguity, when depth range involves more than one period of the modulation signal, iii) light scattering within the lens and the sensor, iv) multipath interference and v) motion artifacts. In this paper we study motion artifacts (MoArt) phenomena (see [5]) and we propose a solution to compensate them in PMD based cameras.High motion dynamics in the scene causes significant distortion in depth estimation, specially in the vicinity of depth discontinuities and points with strong texture changes. If motion happens during the integration time Tint(30–2000μs in commercial cameras), a pixel integrates incoherent signals, which leads to remarkable artifacts in the depth image. To illustrate the problem, we show in Fig. 1an example of both the effect of motion artifacts and our correction. Left column of Fig. 1 shows depth and amplitude images of a moving planar object, registered with a commercial ToF camera. Right column of Fig. 1 shows the result of our algorithm, where the correct shape is recovered.This paper proposes a method that removes motion artifacts from a single frame captured by a PMD ToF camera without altering current hardware. Our method exploits spatial and temporal redundancy present in current PMD pixel pipeline to correct depth measurements caused by the motion. We show in the paper that as a result of the proposed correction method, a single PMD ToF image can be used to measure motion vectors along occluding contours of the scene in the image. Our method is very accurate, recovering depth in very challenging scenarios under high dynamics. It outperforms the state of art based on using several frames for correction. In addition, our method runs in real time in a single core CPU processor, does not require high amount of memory and can be integrated in camera hardware.This article is structured as follows: previous works are reviewed in Section 2. A description of time of flight fundamentals and motion artifacts phenomena is detailed in Sections 3 and 4, respectively. Our single frame approach and the motion estimation algorithm are explained in Sections 5 and 6, respectively. Quantitative and qualitative results obtained using a commercially available PMD-based ToF camera are shown in Section 7. Finally, conclusions are discussed in Section 8.Several studies show the impact of motion artifacts in ToF imaging [5–7]. Some of them propose methods to remove the artifacts. Lottner et al. [5] combine a ToF camera and a color camera to accurately detect the edges of the scene, removing motion artifacts. Schmidt [6] analyzes the origin and effects of motion artifacts in PMD-based ToF cameras and proposes a method to detect pixels affected by motion. He also proposes a new hardware architecture to reduce motion artifacts that consist of a 4-well CMOS sensor that gets simultaneously 4 correlation samples. In that manner, the total integration time is reduced as it uses a single stage. Schmidt's [6] architecture is nowadays prohibitive and sensor manufacturers mainly produce 2-well sensors. More recently, Lee et al. [8,9] study the physical phenomena that generates motion artifacts and propose some methods to detect them. Furthermore, they propose a simple method to correct the artifacts that require user interaction.Recent works [10,7,11,8,9,12] detect and remove automatically motion artifacts by exploiting consistency between phase images in PMD-based architectures. These works can be separated into single frame and multi frame approaches.Single frame approaches exploit the fact that in existing PMD cameras phase images are sequentially obtained, revealing motion. Hussmann et al. [7] detect motion artifacts from discrepancies between phase images. Motion compensation is proposed only for lateral movements in a conveyor belt. This method requires photometric calibration of the camera, is restricted to a very close range (distances below 100cm) and is vulnerable to intensity changes by different pixels. Lindner et al. [10] distinguish between lateral and axial motion. In the former, artifacts are compensated by computing dense optical flow between consecutive “intensity phase images”. Intensity phase images are obtained from raw CMOS measurements of each pixel that need to be accessible from the camera (e.g. phase images are computed from the differences of raw images). Optical flow is then used to correct phase images, removing motion artifacts. To correct axial motion, Lindner et al. [10] need two consecutive frames that are individually corrected from lateral motion artifacts. This proposal is time demanding and it has to deal with the inherent problems in optical flow estimation (normalization of phase images, aperture problem…). Lefloch et al.'s proposal [13] is an improvement of Lindner et al.'s [10] proposal that reduces the number of flow calculations. As it is stated by the authors, optical flow computation is a very time demanding process that requires GPU hardware to be real-time. Very recently Hoegg et al. [12] present a flow like algorithm which uses particular parts of Lindner et al. [10] (flow field) and Lefloch et al. [13] (binarization of the motion area with an experimental threshold). The proposal simultaneously estimates the motion field in motion areas and corrects the artifacts by applying the computed flow field directly to the raw phase values of each individual CMOS channel. As in Lindner et al. [10] the approach does require a phase normalization step in order to apply the algorithm. The motion direction estimation is based in local consistency and it has a time complexity ofOn2that restricts the local neighborhood radius search to achieve real time. To overcome this problem, they propose to use a previous frame to initialize the search direction assuming linear motion. A median filter is applied optionally to remove outliers in the flow field optimization. All existing single frame methods have in common that they require to have access to each individual channel of the ToF CMOS sensor.In multiple frame methods, temporal consistency between frames is exploited to correct motion artifacts. Schmidt [11] detects inconsistent motion events between the different stages of the “4 phase-shift” pipeline using two consecutive frames. The main assumption is that only one motion event can occur within 2 consecutive frames. This assumption is violated easily in many cases considering fast movements in the scene. In practice this method has some limitations and raises important issues about reading gaps between consecutive frames, that in many cases can be much larger than the integration time. Methods considered as single frame propose also to use previous frames to improve efficiency [12] or to correct axial motion artifacts [10].To conclude, the literature counts with several solutions to detect and to correct motion artifacts. Some of them require hardware modifications or special setups. Single frame methods are the most promising approaches, as they can correct motion artifacts under fast and uncontrolled motion. However, despite existing single frame methods provide accurate results (specially [12]) they are computationally expensive and they all require access to raw CMOS channels in the sensor. Following manufacturers trend, existing low-cost PMD ToF cameras (e.g. PMD CamBoard nano) only give access to 4 phase images (differences of raw CMOS channels), which makes existing single frame methods not applicable on those cameras. We propose a single frame method that reduces motion artifacts and estimates motion of occluding contours in real time using a single PMD frame as an input, where only phase images are available. Our proposal works for general motions, it does not require user interaction nor hardware improvements and it has been validated for low cost commercial PMD ToF cameras and very high motion dynamics.ToF technology obtains depth from the phase difference between the emitted and the received signal after colliding with the scene. We define um(t) as the T periodic square signal used to modulate light:(1)umt=1,if0≤t−n⋅T≤T/20,ifT/2≤t−n⋅T≤T⋅n=0,1,2,…The received signal r(t), reflected from the scene, is represented with the following continuous wave function:(2)rt=a0⋅sin⁡ωt+β+O,where aois the signal amplitude, ω=2π/T and β=ω⋅TLis the phase shift caused by the time of flight TL. Depth is then computed asd=c⋅TL2, with c the speed of light in the medium (i.e. vacuum). The offset O is composed of two additive terms:(3)O=Obg+Odc,where Obgis due to background illumination and Odcappears as the DC component of the modulated infrared light source.PMD technology is based on sampling the correlation function between r(t) and um(t) to obtain depth:(4)ρτ=Kint⋅1T∫0Tumt+τ⋅rtd⁡t=Kint⋅a0π⋅cos⁡β+τ+O2,where a0, β and O are the unknown. Kinttakes into account the total number of periods T integrated over the integration time of the sensor Tint, which is thousand times bigger than T.Each PMD “smart pixel” contains 2 CMOS wells that accumulate voltage that corresponds to samples of the correlation function ρτand ρτ+π(see [14] for further details). We refer to the voltage accumulated on each well as UτAand UτB. Their relationship with ρτcan be approximated by the following linear function:(5)UτA=GA⋅ρτ+OAUτB=GB⋅ρτ+π+OB,where GAand GBare the conversion gains that translate optical energy into voltage and OAand OBare the offsets. In general GA≠GBand OA≠OBdue to non-isotropic material properties that appear during sensor manufacturing. Lindner [15] gives a detailed description of these variations called Fixed Pattern Noise (FPN). Despite being considered as noise, it is a systematic error that can be calibrated.Taking the difference UτA−UτBwe get:(6)φτ=UτA−UτB=GFPN⋅Kint⋅2⋅a0π⋅cos⁡β+τ+Ocal=a⋅cos⁡β+τ+Ocalwhere GFPN is the FPN gain and Ocalis the offset term which depends on the background illumination O and can be calibrated:(7a)GFPN=GA+GB2(7b)Ocal=O2⋅Kint⋅GA−GB+OA−OB.There are thus a, β, GFPN and Ocalas unknowns, whereas only β contains depth information.Current PMD devices implement the “4 phase-shift” architecture, which consists on a 4-stage pipeline where φτis computed at τ={0,π/2,π,3π/2} in 4 sequential stages. In Fig. 2we show a summarized cross-section of a PMD-based “smart pixel” with the acquisition pipeline (notice that the capture of every phase image is followed by a readout gap). QAand QBare the accumulated charges in each of the two well.From Eq. (6) we can observe that the following relationships are satisfied:(8)φ0=−φπφπ/2=−φ3π/2.This temporal redundancy between φ0−φπand φπ/2−φ3π/2 is used in the “4 phase-shift algorithm” to cancel the effect of FPN. Usually the value of φτis accessible from the sensor instead of UτAand UτB.Depth d and amplitude a are then obtained irrespective of FPN gain and offset variations:(9a)d=c2⋅ω⋅arctan⁡φ3π/2−φπ/2φ0−φπ(9b)a=φ3π/2−φπ/22+φ0−φπ22.State of the art methods propose calibration techniques of the terms GA, GB, OA, OBwhich can reduce the number of phase images (φτ) from the initial 4 samples to 2 or even 1 sample (see [16] and [17], respectively). Calibration of FPN usually requires special lighting conditions (see [16]) or patterns. We show next that only calibration of Ocalis needed to get depth from only two phase images. We present a calibration method without patterns or special lighting conditions, just by taking a short sequence of images from a static scene in complete darkness.Based on [16], if the offset Ocalis known, the depth and amplitude values can be obtained with only two samples of φτ:(10a)d=c2⋅ω⋅arctan⁡φτ2−Ocalφτ1−Ocal(10b)a=φτ1−Ocal2+φτ2−Ocal2,where τ2=τ1+π/2 with τ1∈{0,π} (note that in Eq. (10a) the sign ofφτ12changes accordingly to the sign of the corresponding correlation sample). Eq. (10a) shows that we can use only two consecutive steps (φτ1andφτ1+π/2) of the “4 phase-shift” pipeline to get depth.We propose a calibration method that requires a number NFof views from a static scene (i.e. camera is still with respect to the scene and objects in the scene are not moving). For every frame in the sequence we get 4 raw channel measurements of a single pixel:(11)φ0i,φπ/2i,φπi,φ3π2ii=1NF.We sample Ocalfrom every two channels separated by π:(12)Oical=12⋅φτi+φτ+πi.Based on Mufti et al. [18] and the recent work of Hussmann et al. [19], we can assume that, if enough photons are collected, Ocalfollows a Gaussian distributionNO^calσO, that we estimate from the samples obtained with Eq. (12).We show in Fig. 3the computed normal distribution of Ocalfor two random pixels in an image taken by a commercial ToF camera.From Eq. (7b), Ocalis composed of two terms. We assume in practice that (GA−GB)≈0, being the second term (OA−OB) the leading factor in Ocal. Unlike the first term, that depends on O and Kint, the offset term (OA−OB) does not vary with depth, integration time, background illumination or light source offset. Calibration of Ocalcan thus be performed offline and stored. We validate this assumption experimentally in Section 7.When the scene moves with respect to the camera, each stage of the “4 phase-shift” pipeline captures different depths, producing errors when they are combined to get depth. We exploit the fact that in PMD cameras the 4 stages of the pipeline are run sequentially to detect motion inconsistency at every pixel. As each stage is run at a fraction of Tint, we consider in the paper that the motion inside each stage is very small.In Fig. 4we represent the sampled phase images in a pixel that belongs to the left occluding contour of the object. On that pixel we show that while φ0 and φπ/2 are properly obtained from the foreground, at φπthere is a sudden change of depth from foreground to background. In last stage (φ3π/2), only background depth is detected.If we use Eqs. (9a) and (9b), the resulting depth appears distorted due to the inconsistency of the different samples (distorted image in Fig. 1). We propose to detect motion artifacts in a single pixel with the following criterion:A pixel is said to be affected by motion artifacts if:(13)φ0+φπ−φπ/2+φ3π/2>γ,where γ is a fixed threshold obtained by the following calibration process.To establish a threshold γ we model statistically the distribution of:(14)η=φ0+φπ−φπ/2+φ3π/2in the case of images where the scene is static with respect to the camera. We assume that η follows a Gaussian distributionNη¯σηthat can be estimated by sampling. We then propose γ=3.326⋅ση, that contains the 99.97% of the distribution when no motion artifacts are present. In Section 7.1 we show the estimation of the inconsistency threshold for several values of Tintin a commercial PMD camera.We propose a method that removes motion artifacts from a single frame (i.e. 4 phase images). Our method is based on two basic assumptions: i) after calibration of the offset, depth can reliably be obtained from two consecutive phase images and ii) neighboring pixels are a strong cue to complete information that is lost due to motion artifacts.We can distinguish 3 main steps in our proposal:1.Calibration: is the responsible for performing the offset calibration (see Section 3.1) and determining the inconsistency threshold (see Section 4.1) automatically with no user interaction. After these two steps we can detect pixels affected with motion artifacts and we can obtain depth using the pseudo 4 phase-shift algorithm.Event detection: in this step we characterize the event in terms of: i) the phase image where the event appears and ii) the direction of the event (rising or falling edge). Hence, we have a discrete label for each pixel affected by motion that includes both temporal and depth change information.Depth correction: in this stage we correct the depth value of every pixel affected by motion artifacts. We combine the information available in the phase images with neighbor pixels.As explained in Section 3 we remove the influence of Fixed Pattern Noise in order to compute depth measurements using only two of the four phase images provided by the sensor (the so called “pseudo 4 phase-shift algorithm” explained in [16]). In addition, we apply a statistical method to find threshold γ, explained in Section 4.1. Both processes are done only once at the beginning. Calibration requires to show the camera a static scene during several seconds. During calibration we avoid regions of the image with strong depth discontinuities, saturation and low reflective surfaces.When a pixel is affected by motion it receives light from multiple depths during Tint. In occluding contours, motion produces abrupt depth transitions, mainly affecting a single phase of the pixel. We detect events by monitoring sudden changes between the 4 phase images labeling them accordingly.We use a discrete labeling cel (coarse event location) that represents the phase image containing the event and the direction (i.e. rising edge or falling edge) of the depth change caused by the motion. We use the following set of 9 labels:(15)cel∈−4,−3,−2,−1,0,+1,+2,+3,+4,where |cel| represents the phase image affected by a depth transition and its sign characterizes the depth gradient. (e.g. cel=+1 means a rising edge event in φ0 and cel=−2 means a falling edge event in φπ/2). The absence of events is represented by cel=0.Event labeling is divided into two steps:1Detection of the event location: in this step we localize the phase image that is distorted due to motion, i.e. |cel|.Detection of the event direction: we characterize depth transitions with two classes: {rising, falling} events, i.e. sign of cel.Our method for detecting the phase affected by a motion event is based on the following two main assumptions:•An event is generated by the transition of two depths ({β1, β2}).There is only one depth transition affecting a single phase image during integration time.From Eq. (13), if there is no event in the current pixel (|cel|=0) the following inequalities hold: i) |φ0+φπ|<γ and ii) |φπ/2+φ3π/2|<γ, where γ is a small threshold that has been previously calibrated. Otherwise, we detect the position of the events by checking the following tests in sequential order:(16)a)|cel|=1⇔|φ0+φπ|>γand|φπ/2+φ3π/2|<γb)|cel|=4⇔|φ0+φπ|<γand|φπ/2+φ3π/2|>γc)|cel|=2or|cel|=3⇔|φ0+φπ|>γand|φπ/2+φ3π/2|>γ.To distinguish between events in |cel|=2 or |cel|=3, we first write the equations of all phases from Eq. (6), once we have calibrated the offset term Ocal(see Section 3.1):(17)φ0=+Kint⋅a′⋅cos⁡βφπ/2=−Kint⋅a′⋅sin⁡βφπ=−Kint⋅a′⋅cos⁡βφ3π/2=+Kint⋅a′⋅sin⁡βwherea′=GFPN⋅2⋅a0π.If an event appears in |cel|=2, φπ/2 contains the mixture of two phase shifts {β1, β2}. Hence, we can express Eq. (17) considering the phase mixture as:(18)φ0=+Kint⋅a1′⋅cos⁡β1φπ/2=−K1⋅a1′⋅sin⁡β1−K2⋅a2′⋅sin⁡β2φπ=−Kint⋅a2′⋅cos⁡β2φ3π/2=+Kint⋅a2′⋅sin⁡β2where a1′, a2′ are the corresponding amplitude values of the two depths involved in the generation of the artifact and Kint=K1+K2.We use Eq. (18) to express φ0+φπand φπ/2+φ3π/2 as follows:(19)φ0+φπ=Kint⋅a1′⋅cos⁡β1−a2′⋅cos⁡β2φπ/2+φ3π/2=K1⋅a2′⋅sin⁡β2−a1′⋅sin⁡β1.We propose to detect events in the second interval (|cel|=2) by checking the following condition:(20)φ0+φπ>φπ/2+φ3π/2.Finally, the identification of an event in φπ(|cel|=3) is defined as the event that does not satisfy the statements of |cel|={1,4,2}, in that order. As a consequence of following the sequential pipeline described in Eq. (16) to detect the event location, the labeling error is determined by the detection of |cel|=2. To test when inequality (20) holds, in Fig. 5a we plot the number of incorrectly labeled events depending on the value of K1 for every possible combination of {β1, β2} (sweeping both terms over the (0, 2π) interval) in the depth range 0–5m. In Fig. 5b we fix β1 to be in a specific depth quadrant Qi(see Eq. (21)), sweeping β2 over all possible values. To create Fig. 5 we use Eq. (18), assuming that amplitudes a1′ and a2′ are inversely proportional to the square of the corresponding depth.We observe in Fig. 5 that the percentage of erroneous event classification increases with the factor K1. However, when K1≈Kint, the event is closer to be happening in the transition between |cel|=2 and |cel|=3. Those mistakes are thus not very relevant as there is no practical difference between considering the event to be classified as |cel|=2 or |cel|=3. As shown in Fig. 5, if we only consider those classification errors committed when K1/Kint<0.66, the detection of events in |cel|=2 remains in average below 35% of error. This corresponds with the results we obtain in real sequences (see Section 7), where despite these classification errors, the event detection allows us to effectively detect and compensate motion artifacts. The complete event detection pipeline is resumed in Algorithm 1 (Appendix A).Given that we know the phase image affected by a motion artifact, we identify the direction of the event as follows:•Falling edge: when there's a transition from foreground to background (β2>β1).Rising edge: when there's a transition from background to foreground (β2<β1).So that we can make a proper substitution (using local neighborhood) of the erroneous measurements and correct the artifacts.Let's divide the maximum depth range of the camera into its corresponding depth quadrants Qis.t.(21)Qi=i⇔β∈i−1⋅π2,i⋅π2,i∈1234.Then, for each pixel affected by motion we can easily obtain the involved depth quadrants (Qbg, Qfg) by clustering the depths of each pixel in a local neighborhood (we use the 2 depths per pixel using the pseudo four-phase shift algorithm).The direction of an event is detected by simply checking the sign of the phase image difference shifted by π, according to the involved phase quadrants. The whole labeling method is detailed in Appendix A and Appendix B. In Fig. 6we show the detection of the event direction in several cases where multiple depth quadrants are involved. In the experiments of Fig. 6 the background depth varies along the whole range while the foreground varies from motion in Q1 (first two columns) to motion in Q2 (last two columns). Note that, due to the weak illumination power, uncertainty is high for depths above 1.5m approximately (Q2 and higher), generating undesirable effects.Finally, in Fig. 7we show the events detected in the example shown in Fig. 1, using a color palette that represents the value of cel. We codify every pixel of the object with a color (i.e. from dark blue to light pink) that represents the stage {φ0,φπ/2,φπ,φ3π/2} in the “4 phase shift” pipeline which is affected by the transition from foreground to background (i.e. blue colors) and from background to foreground (i.e. red colors). Fig. 8illustrates the detection of a rising edge event in φπ/2. We define {φbg, φfg} as the phase images associated with background (low charge) and foreground (high charge) depths respectively.Our depth correction algorithm is based on two basic rules: i) we reconstruct the depth detected at the beginning of the integration time and ii) we reconstruct depth using two previous consecutive phases to the phase affected by the motion event. If less than two consecutive phase images are available we search in neighboring pixels to complete data.According to rule i), we obtain local foreground depth on falling edge events (from foreground to background) and the local background depth on rising edge events (from background to foreground).From rule ii), events in φπor φ3π/2 allow us to recover depth directly from the phases φ0 or φπ/2 using Eq. (10a). If the event is detected in any of the first two phases, we search in the local neighborhood to recover depth. We show in Table 1the case studies depending the position of the detected event. We also point out the required phase images needed to recover the corresponding depth and the cel labeling we look for in the neighbors to get a replacement. As depicted from Table 1, we need to search for neighbors in half of the cases.The strategy to find neighbors involves two methodologies: i) to look for the closest pixels that have the same event type (rising or falling edge) but complementary stable phase images and ii) to look for the nearest points that have no motion artifact and compatible stable phase images. In Fig. 9we show an example of our proposal to replace missing data with spatial information of a pixel affected by motion characterized with a falling edge in the second phase image (φπ/2).Once we have found the most suitable neighbors, we sort them, identifying the median phase value. It's well known in the literature that the median filter is a robust and effective method for outlier removal. In a compromise between robustness and computational efficiency, we replace the corresponding phase images with the mean of the 3 closest candidates around the median. ToF measurements are then recovered using only 2 phase images. In the example shown in Fig. 9, where only one phase image is needed for substitution, the replacement would be computed in the following 2-step procedure:1.Find the most suitable neighbors within the neighbor set ℛias the closest points to the pixel of interest with stable phase images in the required intervals. We search neighbors in a spiral loop until enough pixels are found (i.e. a minimum of Nn=7 is required to increase accuracy) or we exceed a reasonable distance (we fix this internal parameter with a radius distance of 20pixels). In practice a 20pixel radius distance can be in fact very conservative. We found that a radius distance between {2.5 and 4.9} pixel is enough in our camera after averaging the radius needed over several frames in a wide variety of dynamic scenarios.Sort the previous data set and identify the median value. Finally, we compute the mean of the 3 closest points to the median:(22)φi,π/2fg=13∑j=medi−1medi+1φ^j,π/2fg,where φ(i,τ)zrefers to the phase image φτat pixel i, identified as background or foreground (i.e. z={bg, fg}) and mediis the median value index for the neighbors dataset of pixel i ordered in the previous stepφ^j,π/2fg.Additional post-processing of depth and amplitude measurements (like outliers removal, bilateral filtering…), as it is done in [10] and [12], could improve the results.We show that, as a consequence of our depth correction algorithm, we can actually measure motion flow near occluding contours of the scene using a single ToF frame. We first define a method to accurately obtain the instant where a motion event is detected on each pixel, that we call pel (precise event location). We then use the gradient of pel image to obtain motion flow.Let assume that we find an event in phase τ of current pixel using the proposed coarse event detection. We define φτerras the erroneous value of the phase image in phase τ. We model the event as a change between two depth values (e.g. background and foreground). We define φτfgand φτbgas the foreground and background phase images corresponding to τ. We assume that photon generation in the CMOS photo gates is proportional to exposure time and we define that φτerras a linear combination of φτfgand φτbg. Depending the direction of the event we define:(23)φτerr=α⋅φτfg+1−α⋅φτbg,ifcel<0α⋅φτbg+1−α⋅φτfg,ifcel>0,where α is the fraction of the integration time in phase τ before the event appears (see Fig. 10). Using the algorithm from Section 5.3, we can obtain φτfgand iτbgfrom neighbor pixels and from phase images not affected by the event in the pixel. In Table 2we show all possible cases to get φτfgand φτbg.Using Eq. (23) we obtain α:(24)α=φτerr−φτbgφτfg−φτbg,ifcel<0φτerr−φτfgφτbg−φτfg,ifcel>0.We define pel as the fraction of Tintwhere the motion event takes place:(25)pel=cel−1+α4,where we assume that the integration time of each phase image is Tint/4.We show in Fig. 11pel computation for the running example used in Fig. 1.During motion, neighboring pixels near occluding contours are affected by the motion event at different pel values. The spatial (i.e. image coordinates) distribution of pel values depends on magnitude and direction of motion. We denote as Ipelthe image arrangement of pel values (see right part of Fig. 11). The image gradient of ∇Ipelgives the distribution of time differences between pixels. The magnitude of ∇Ipelis proportional to the inverse of motion flow:(26)∇Ipel∝1v,where v is the speed magnitude. The direction of ∇Ipelgives the direction of speed in the image. Fig. 12shows Ipeland its gradient for an object following horizontal motion in the image.Eq. (26) is only valid when either we detect an event in the pixel (pel≠0) or the speed of the object produces events in neighboring pixels that are separated by less than the integration time.Axial motion is corrected properly as movements along the viewing direction also generate phase image displacement in occluding contours. However, our motion field estimator doesn't provide information about this kind of movement.

@&#CONCLUSIONS@&#
