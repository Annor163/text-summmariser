@&#MAIN-TITLE@&#
Distributed feature selection: An application to microarray data classification

@&#HIGHLIGHTS@&#
Feature selection is indispensable when dealing with microarray data.A new method for distributing the filtering process is proposed.The data is distributed by features and then merged in a final subset.The method is tested on 8 microarray datasets.The classification accuracy is maintained and the time considerably shortened.

@&#KEYPHRASES@&#
Feature selection,Distributed learning,Microarray data,

@&#ABSTRACT@&#
Feature selection is often required as a preliminary step for many pattern recognition problems. However, most of the existing algorithms only work in a centralized fashion, i.e. using the whole dataset at once. In this research a new method for distributing the feature selection process is proposed. It distributes the data by features, i.e. according to a vertical distribution, and then performs a merging procedure which updates the feature subset according to improvements in the classification accuracy. The effectiveness of our proposal is tested on microarray data, which has brought a difficult challenge for researchers due to the high number of gene expression contained and the small samples size. The results on eight microarray datasets show that the execution time is considerably shortened whereas the performance is maintained or even improved compared to the standard algorithms applied to the non-partitioned datasets.

@&#INTRODUCTION@&#
Over the last decade, feature selection, which consists of detecting the relevant features and discarding the irrelevant ones [1,2], has been an active research area due to the high dimensionality of the datasets. Feature selection methods usually come divided into three types: filters, wrappers and embedded methods. While wrapper models involve optimizing a predictor as part of the selection process, filter models rely on the general characteristics of the training data to select features independently of any predictor. The embedded methods generally use machine learning models for classification, and then an optimal subset of features is built by the classification algorithm. In the last few years, ensemble methods represented a new type of methods for feature selection. They aim to cope with the instability issues observed in many techniques for feature selection when the characteristics of the data change [3–5]. In previous works, several feature selection methods are applied and the obtained features are merged into a more stable subset of features prior to classification or the different predictions obtained with the different subsets of features are somewhat combined [6,7]. However, as stated in [8], even when the subset of features is not optimal, filters are preferable due to their computational and statistical scalability, so they will be the focus of this research.The filter approach is commonly divided into two different subclasses: individual evaluation and subset evaluation [9]. Individual evaluation is also known as feature ranking and assesses individual features by assigning them weights according to their degrees of relevance. On the other hand, subset evaluation produces candidate feature subsets based on a certain search strategy. Each candidate subset is evaluated by a certain evaluation measure and compared with the previous best one with respect to this measure. While the individual evaluation is incapable of removing redundant features because these are likely to have similar rankings, the subset evaluation approach can handle feature redundancy with feature relevance. However, methods in this framework can suffer from an inevitable problem which is caused by searching though the possible feature subsets. This stage, required in the subset generation step, usually increases the computational time. Having said that, a revision of the existing literature showed that the subset evaluation approach outperformed the ranking methods [9–12].Feature selection is usually applied in a centralized manner, i.e. a single learning model is used to solve a given problem. However, if the data is distributed, feature selection may take advantage of processing multiple subsets in sequence or concurrently. The need to use distributed feature selection can be two-fold. On the one hand, with the advent of network technologies data is sometimes distributed in multiple locations and may consequently be biased. On the other hand, most of the existing feature selection algorithms do not scale well and their efficiency significantly deteriorates or even becomes inapplicable when dealing with large-scale data. In order to increase efficiency, learning can be parallelized by distributing the subsets of data to multiple processors, learning in parallel and then combining them. There are two main techniques for partitioning and distributing data: vertically, i.e. by features, and horizontally, i.e. by samples. Distributed learning has been used to scale up datasets that are too large for batch learning in terms of samples [13–15]. While not common, there are some other developments that distribute the data by features [16,17]. In this study, the data are distributed vertically in order to have the feature selection process distributed. After having the data distributed in small feature subsets and selecting the relevant features from each subset, a merging procedure is performed which updates the feature subset according to improvements in the classification accuracy.Although this approach can be applied to any feature-abundant classification problem, it is especially suitable for application to microarray data. This type of data has become very popular in the past decade since it poses a difficult challenge for machine learning researchers due to its high number of features and its small sample size. In this domain, features represent gene expression coefficients corresponding to the abundance of mRNA – messenger ribonucleic acid – in a sample (e.g. tissue biopsy), for a number of patients. Although there are usually very few samples, the number of features in the raw data ranges from 6000 to 60,000. A typical classification task is to separate healthy patients from cancer patients based on their gene expression “profile”. By applying the proposed distributed methodology to this domain, we will be able to deal with subsets with a more balanced feature/sample ratio and avoid overfitting problems. The experimental results from eight different databases demonstrate that our proposal can improve the performance of original feature selection methods and show important savings in running times.The remainder of the paper is organized as follows: Section 2 describes the state of the art on distributed feature selection and feature selection methods on microarray data, Section 3 introduces our distributed approach, Section 4 reveals the experimental setup and Section 5 visualizes the experimental results. Finally, Sections 6 and 7 provide the discussion and conclusions, respectively.The literature about feature selection for microarray data is abundant. Different feature selection strategies have been proposed over the last years for feature/gene selection. Ferreira and Figueiredo [18] proposed combining unsupervised feature discretization and feature selection techniques which have improved previous related techniques over several microarray datasets. In [19] a new framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem is presented and tested over microarray data, showing promising results. Wang et al. [20] also proposed a novel filter framework to select optimal feature subsets based on a maximum weight and minimum redundancy criterion. Hybrid methods have been recently tested on this type of data [21,22] obtaining high classification accuracies. Embedded methods have also been proposed, such as in [23], where the authors introduced an algorithm that simultaneously selects relevant features during classifier construction by penalizing each feature's use in the dual formulation of support vector machines. On the other hand, trying to overcome the problem that a weakly ranked gene could be relevant within an appropriate subset of genes, Sharma et al. [24], introduced an algorithm that first distributes genes into relative small subsets, then selects informative smaller subsets of genes from a subset and merges the chosen genes with another gene subset to update the final gene subset. Their method showed promising classification accuracy for all the test datasets.As we have said, many filter approaches were applied to successfully classify microarray data [8,12,25], however according to the authors’ knowledge, there has been no attempt in the literature to tackle this problem with distributed feature selection, apart from the aforementioned proposal of Sharma et al. [24]. In fact, feature selection in distributed environments is a poorly explored field and very few references were found in the literature, most of which undertaken over the few last years. It is worth mentioning the research of Das et al. [26], where an algorithm is presented which, based on a horizontal partition (by samples), performs feature selection in an asynchronous fashion with a low communication overhead by which each peer can specify its own privacy constraints. A vertical partition of the data (by features) to generate the diverse components of an ensemble [27] is also present in the literature. However in these cases, feature selection is not applied to the different partitions of the data and therefore the model may be constructed based on irrelevant features. More recently, Banerjee and Chakravarty [28] proposed a distributed feature selection method evolved from a method called virtual dimension reduction, where the partition of the data can be done both vertically or horizontally. Zhao et al. [29] presented a distributed parallel feature selection algorithm based on maximum variance preservation. The algorithm can read data in a distributed form and perform parallel feature selection in both symmetric multiprocessing mode via multithreading and massively parallel processing.Still, DNA microarray data prevents the use of horizontal partitioning because of the small sample size. The distributed methods mentioned above based on vertical partitioning have not been designed specifically for dealing with microarray data, so they do not tackle the particularities of this type of data, such as the high redundancy present among the features. The method proposed in [24] does address these issues, but it has the disadvantage of being computationally expensive by interacting with a classifier to select the genes in each subset. In this work we will propose a distributed filter method, suitable for application to microarray data and with a low computational cost.Distributed feature selection has not been deeply explored yet. So, in this paper we present a distributed filter approach trying to improve upon previous accuracy results over microarray data as well as reducing the running time. Our proposal consists of performing several fast filters over several partitions of the data, combined afterwards into a single subset of features. Thus, we divide each dataset D into several small disjoint subsets Di. The filter is applied to each of them, generating a corresponding selection Si. After all the small datasets Dihave been used (which could be done in parallel, as all of them are independent of each other), the combination method builds the final selection S as the result of the filtering process. To sum up, there are three main steps in this methodology:1.Partition of the datasets.Application of filtering to the subsets.Combination of the results.The partition of the dataset consists of dividing the original dataset into several disjoint subsets of approximately the same size that cover the full dataset (see Fig. 1). As mentioned in Section 1, in this research the partition is made vertically. Two different methods are used for partitioning the data: (a) performing a random partition and (b) ranking the original features before generating the subsets. The second option was introduced to try to improve the performance obtained by the first one. By having an ordered ranking, features with similar relevance to the class will be in the same subset, which will facilitate the task of the subset filter which will be applied later. These two techniques for partitioning the data will generate two different approaches for the distributed method: distributed filter (DF) with the random partition and distributed ranking filter (DRF) associated to the ranking partition.After this step, the data is split by assigning groups of k features to each subset, where the number of features k in each subset is half the number of samples, to avoid overfitting. When opting for the random partition (DF), the groups of k features are constructed randomly, taking into account that the subsets have to be disjointed. In the case of the ranking partition (DRF), the groups of k features are generated sequentially over the ranking, so features with a similar ranking will be in the same group. Notice that the random partition is equivalent to obtaining a random ranking of the features (by shuffling the features) followed by the same steps as with the ordered ranking. Fig. 2shows a flow chart which reflects the two algorithms proposed, DF and DRF.Algorithm 1Pseudo-code for the distributed filter algorithmD(m×s):=training dataset with m samples and s featuresn:=number of subsets of k features1.Select the partition method:• DF → Shuffle features from dataset D to obtain a random ranking R or• DRF → Apply a ranker method over D to obtain an ordered ranking R2.for i=1 to n do(a) Ri= first k features in R(b) R = R\Ri(c) Di=D(m×|Ri|)3.for i=1 to n do(a) Si= subset of features obtained after applying the chosen filter over Di4.S = S15.baseline = accuracy obtained by classifying subsetD(m×|S1|)with classifier C6.for i=2 to n do(a) Saux= S ∪ Si(b) accuracy = accuracy obtained by classifying subsetD(m×|Saux|)with classifier C(c) if accuracy > baselinei. S = Sauxii. baseline = accuracy7.Build classifier C with D(m×|S|)8.Obtain prediction PAfter having several small disjoint datasets Di, the filter method will be applied to each of them, returning a selection Sifor each subset of data. Finally, to combine the results, a merging procedure is necessary. In this work we have opted for a merging procedure involving a classifier, since simpler methods, such as the union, result in a huge number of features (probably containing redundancy, and perhaps overfitting), as will be seen in Section 5.4. In the method we propose, the first selection S1 is taken to calculate the classification accuracy, which will be the baseline, and the features in S1 will always become part of the final selection S. For the remaining selections, the features in Si, i=2…n will become part of the final selection S if they improve the baseline accuracy, as can be seen in more detail in Algorithm 1. Combining the features in this manner is expected to remove redundancy, since a redundant feature will not improve the accuracy and hence will not be added to the final selection. At the end, this final selection S is applied to the training and test sets in order to obtain the ultimate classification accuracies. It must be noted that this algorithm can be used with any feature subset filter.This section will present the microarray datasets chosen for testing the distributed approaches, as well as the ranker method and filters which will carry out the feature selection process. For testing the adequacy of DF and DRF, four well-known supervised classifiers, of different conceptual origin, were selected to evaluate the proposal. All the classifiers and filters are executed using the Weka tool [30], with default values for their parameters. Experimentation is performed on an Intel(R) Xeon(R) CPU W3550 @ 3.07 QUAD-CORE with 12GB RAM.The performance of the distributed filter will be tested over DNA microarray data. These types of datasets pose a huge challenge for feature selection researchers due to the high number of gene expression data contained and the small samples size. Eight well-known binary microarray datasets11Datasets available on http://datam.i2r.a-star.edu.sg/datasets/krbd/.are considered, listed in Table 1. Those datasets originally divided into training and test sets were maintained, whereas, for the sake of comparison, datasets which come originally with only a training set were randomly divided using the common rule 2/3 for training and 1/3 for testing. This division introduces an interesting scenario, since in some datasets, the distribution of the classes in the training set differs from the one in the test set. Table 1 depicts the number of attributes and samples and also the distribution of the binary classes, i.e. the percentage of binary labels in the datasets, showing if the data are unbalanced.As can be seen in Table 1, Leukemia dataset presents the so-called imbalance problem. A dataset is considered unbalanced when the classification categories are not approximately equally represented in the training set. This situation is very common in microarray datasets, when most instances correspond to “normal” patterns while the main goal consists of identifying the “abnormal” patterns.Some techniques are available to overcome the imbalance problem, however some of them are not appropriate for microarray data [31]. For this sake, and following the recommendations in [31], we will use random oversampling to deal with the Leukemia dataset, which consists of replicating, at random, elements of the under-sized class until it matches the size of the other classes. Notice that, when dealing with a real-life dataset (especially if it involves people's health), it is necessary to take into account the specialists’ opinion before taking any action to modify the data.There are two different types of ranker feature selection: univariate and multivariate. Univariate methods are fast and scalable, but ignore feature dependencies. On the other hand, multivariate filters model feature dependencies, but at the cost of being slower and less scalable than univariate techniques [32].As representatives of univariate ranker filters we have chosen the well known Information Gain [33] and ReliefF [34] methods. As for multivariate ranker filter, minimum-Redundancy-maximum-Relevance (mRMR) was chosen, which has proven to be very suitable for application to microarray data [12].Because of their nature, the rankings returned by Information Gain and ReliefF place the more relevant features on the top of the ranking (even when they are redundant) whilst mRMR is able to detect the redundant features and move them to lower positions in the ranking. Feature/gene redundancy is an important problem in microarray data classification. In the presence of thousands of features, researchers noticed that it is common that a large number of features are not informative because they are redundant. Empirical evidence showed that along with irrelevant features, redundant features also affect the speed and accuracy of mining algorithms [35].In light of the above, the behavior of mRMR might seem to be an advantage in removing redundancy, but it is not the case in this research. Preliminary experiments with these three ranker filters (not included for the sake of brevity) revealed that it was better to have the redundant features in the same packet of features, so the filter applied afterwards could remove the redundant features at once. On the contrary, if the redundant features are split in different packets, there are more chances to keep them and degrade the performance. Moreover, multivariate ranker filters are slower than univariate ones, and to deal with the entire set of features (in the order of thousands) it is important to reduce this processing time. Between Information Gain and ReliefF, the former obtained more promising results in those preliminary experiments, so it was chosen for this research. This univariate filter provides an ordered ranking of all the features where the worth of an attribute is evaluated by measuring the Information Gain with respect to the class. Theoretically, the most relevant features will be placed in the first partition and so on. In this manner, it will be easier to remove redundancy, since features with similar relevance to the class would be together in the same partition.One of the particularities of the Information Gain filter is that, if a feature is not relevant to the prediction task, its Information Gain with respect to the class is zero. For this reason, we will also try an approach which consists of eliminating the features with Information Gain zero from the initial ranking.The distributed methods proposed herein can be used with any feature selection filter. In this work, three well-known subset filters and two popular ranking filters were chosen for testing our proposal. The ranking methods return an ordered ranking of the features, so it is necessary to establish a threshold in order to obtain a subset of features. In this case we have opted for retaining 10% and 25% of the top features. Notice that although most of the filters work only over nominal features, the discretization step is done by default by Weka, working as a black box for the user.•Correlation-based Feature Selection (CFS) is a simple multivariate filter algorithm that ranks feature subsets according to a correlation based heuristic evaluation function [36]. The bias of the evaluation function is toward subsets that contain features that are highly correlated with the class and uncorrelated with each other, so irrelevant and redundant features should be screened out.The Consistency-based Filter[37] evaluates the worth of a subset of features by the level of consistency in the class values when the training instances are projected onto the subset of attributes. The algorithm generates a random subset S from the number of features in every round. If the number of features of S is less than the current best, the data with the features prescribed in S is checked against the inconsistency criterion. If its inconsistency rate is below a pre-specified one, S becomes the new current best. The inconsistency criterion, which is the key to the success of this algorithm, specifies to what extent the dimensionally reduced data can be accepted. If the inconsistency rate of the data described by the selected features is smaller than a pre-specified rate, it means the dimensionally reduced data is acceptable.The INTERACT algorithm [38] is based on symmetrical uncertainty (SU) [39], which is defined as the ratio between the Information Gain (IG) and the entropy (H) of two features, x and y: SU(x, y)=2IG(x|y)/[H(x)+H(y)], where the Information Gain is defined as IG(x|y)=H(y)+H(x)−H(x, y), being H(x) and H(x, y) the entropy and joint entropy, respectively. Besides SU, INTERACT also includes the consistency contribution (c-contribution). C-contribution of a feature is an indicator about how significantly the elimination of that feature will affect consistency. The algorithm consists of two major parts. In the first part, the features are ranked in descending order based on their SU values. In the second part, features are evaluated one by one starting from the end of the ranked feature list. If c-contribution of a feature is less than an established threshold, the feature is removed, otherwise it is selected. The authors stated that this method can handle feature interaction, and efficiently selects relevant features.The Information Gain filter [33] is one of the most common univariate methods of evaluating attributes. This filter evaluates the features according to their Information Gain and considers a single feature at a time. It provides an orderly classification of all the features, and then a threshold is required to select a certain number of them according to the order obtained.The filter ReliefF[34] is an extension of the original Relief algorithm [40]. The original Relief works by randomly sampling an instance from the data and then locating its nearest neighbor from the same and opposite classes. The values of the attributes of the nearest neighbors are compared to the sampled instance and used to update relevance scores for each attribute. The rationale is that a useful attribute should differentiate between instances from different classes and have the same value for instances from the same class. ReliefF adds the ability of dealing with multiclass problems and is also more robust and capable of dealing with incomplete and noisy data. This method may be applied in all situations, has low bias, includes interaction among features and may capture local dependencies which other methods miss.Since we are dealing with real datasets, the relevant features are not known a priori. Therefore, it is necessary to use a classification algorithm to evaluate the performance of the feature selection, focusing on the classification accuracy. Unfortunately, the class prediction depends also on the classification algorithm used, so when testing a feature selection method, a common practice is to use several classifiers to obtain results as classifier-independent as possible. In this research the following classification algorithms has been used:•C4.5 is a classifier developed by Quinlan [41], as an extension of the ID3 algorithm (Iterative Dicotomiser 3). Both algorithms are based on decision trees. A decision tree classifies a pattern by filtering it in a descending way until it finds a leaf, that points to the corresponding classification. One of the improvements of C4.5 with respect to ID3 is that C4.5 can deal with both numerical and symbolic data. In order to handle continuous attributes, C4.5 creates a threshold and depending on the value that the attribute takes, the set of instances is divided.A naive Bayes classifier [42] is a simple probabilistic classifier based on applying Bayes’ theorem with strong (naive) independence assumptions. This classifier assumes that the presence or absence of a particular feature is unrelated to the presence or absence of any other feature, given the class variable. A naive Bayes classifier considers each of the features to contribute independently of the probability that a sample belongs to a given class, regardless of the presence or absence of the other features. Despite their naive design and apparently oversimplified assumptions, naive Bayes classifiers have worked quite well in many complex real-world situations. In fact, naive Bayes classifiers are simple, efficient and robust to noise and irrelevant attributes.k-Nearest neighbor (k-NN) [43] is a classification strategy that is an example of a “lazy learner”. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common amongst its k nearest neighbors (where k is some user specified constant). If k=1 (as is the case in this paper), then the object is simply assigned to the class of that single nearest neighbor. This method is more adequate for numerical data, although it can also deal with discrete values.A Support Vector Machine (SVM) [44] is a learning algorithm typically used for classification problems (text categorization, handwritten character recognition, image classification, etc.). More formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training data point of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. In its basic implementation, it can only work with numerical data and binary classes.

@&#CONCLUSIONS@&#
In this research we have proposed a new method for distributing the feature selection process applied to a complex scenario such as microarray data classification. These data are characterized by having a much larger number of features than of samples, so the idea was to distribute the features into disjoint subsets and then combine the results of the light feature-selectors into a final set of features.The main goal was to design a method that would be able to successfully distribute the feature selection process. The experiments on eight microarray datasets showed that our proposal was able to reduce the running time significantly with respect to the standard (centralized) filtering algorithms as well as to the number of input features. In terms of execution time, the behavior was excellent, this fact being the most important advantage of the proposed method. Furthermore, with regard to classification accuracy, the distributed approach was able to match, and even in some cases improve, the standard algorithms applied to the non-partitioned datasets.The extensive experiments performed herein included a comparison with well-known and highly recommended feature selection algorithms, involving also the wrapper approach. Results demonstrated that our method achieved a similar performance than this suite of standard algorithms, while shortening the running time impressively.Finally, it is worth mentioning that the proposed method can be used with any feature selection algorithm without any modifications, so it could be seen as a general framework for distributed feature selection.