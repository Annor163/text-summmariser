@&#MAIN-TITLE@&#
Semantic video labeling by developmental visual agents

@&#HIGHLIGHTS@&#
Developmental Visual Agents are life-long learning systems for video understanding.The proposed agents continuously process videos and receive supervisions by humans.The architecture goes from unsupervised feature extraction up to the symbolic level.Motion-based coherence constraints allow to exploit even few supervisions per class.

@&#KEYPHRASES@&#
Learning from constraints,Life-long learning,Scene understanding,Motion estimation,Deep learning,

@&#ABSTRACT@&#
In the recent years, computer vision has been undergoing a period of great development, testified by the many successful applications that are currently available in a variety of industrial products. Yet, when we come to the most challenging and foundational problem of building autonomous agents capable of performing scene understanding in unrestricted videos, there is still a lot to be done. In this paper we focus on semantic labeling of video streams, in which a set of semantic classes must be predicted for each pixel of the video. We propose to attack the problem from bottom to top, by introducing Developmental Visual Agents (DVAs) as general purpose visual systems that can progressively acquire visual skills from video data and experience, by continuously interacting with the environment and following lifelong learning principles. DVAs gradually develop a hierarchy of architectural stages, from unsupervised feature extraction to the symbolic level, where supervisions are provided by external users, pixel-wise. Differently from classic machine learning algorithms applied to computer vision, which typically employ huge datasets of fully labeled images to perform recognition tasks, DVAs can exploit even a few supervisions per semantic category, by enforcing coherence constraints based on motion estimation. Experiments on different vision tasks, performed on a variety of heterogeneous visual worlds, confirm the great potential of the proposed approach.

@&#INTRODUCTION@&#
Computer vision systems have nowadays shown outstanding results in several specific tasks that range from object recognition, detection, localization, to segmentation and tracking. The whole research field of computer vision is facing a great development, and related technologies can be found today in a variety of low-cost commercial devices such as cameras, tablets, and smartphones, as well as in highly advanced systems, as in the case of autonomous vehicles, augmented reality environments, medical diagnosis assistants, video surveillance controllers.Despite this notable achievements, the general problem of constructing an automatic agent capable of performing visual scene understanding in unrestricted domains is far from being solved. As a matter of fact, the basic task of video semantic labeling (or scene parsing), which consists in assigning a semantic label to each pixel of a given video stream, has mostly been carried out only at the frame level, as the outcome of well-established pattern recognition methods working on images [1–5]. Conversely, we maintain that there are strong arguments to start exploring the more challenging problem of semantic labeling in unrestricted video streams, by developing automatic visual systems which can continuously interact with the environment and improve their skills, in a lifelong process which in principle never ends. Rather than exploiting huge amounts of labeled examples at once [6], which could be extremely costly to obtain in case of videos, we argue that these agents should be capable of using even only a few pixel-wise supervisions per semantic category, but exploiting the intrinsic information coming from motion in order to virtually extend supervisions, as well as to enforce coherence in predictions. Roughly speaking, once a pixel has been labeled, the constraint of motion coherent labeling virtually offers tons of other supervisions, that are essentially ignored in most machine learning approaches working on big databases of labeled images. This process resembles the visual interaction experienced in their own life by humans, who progressively acquire knowledge and competence, and can perform scene understanding after receiving just a few supervisions.Following this idea, in this paper we introduce Developmental Visual Agents. DVAs continuously develop their visual skills by processing videos coming from any kind of source, and by interacting with users, from which they can ask for and receive supervisions. These agents implement a lifelong learning mechanism which proceeds asynchronously with respect to the processing of the input stream and the acquisition of external information. We aim at devising these systems from bottom to top, starting from the low-level feature extraction process, up to the symbolic layer where interaction with users occurs, and scene parsing predictions are shown. On top of the representation gained by motion coherence, the mapping to linguistic descriptions is dramatically simplified.In this scenario, the learning framework indeed plays a crucial role. The work described in this paper is rooted on the theory of learning from constraints[7] that allows us to model the interaction of intelligent agents with the environment by means of constraints on the tasks to be learned. The notion of constraint is very well-suited to express both visual and linguistic granules of knowledge. In the simplest case, a visual constraint is just a way of expressing the supervision on a labeled pixel, but the same formalism is used to express motion coherence, as well as complex dependencies on real-valued functions. In principle one could also include abstract logic formalisms, such as First-Order-Logic (FOL) formulae [8,9].The main contributions of the paper can be summarized as follows:•DVAs are introduced as general-purpose visual agents for semantic video labeling in unrestricted domains, in a complete bottom-to-top pipeline from feature extraction up to the symbolic layers;A lifelong learning paradigm for on-line video processing is defined, which exploits motion and temporal coherence throughout the life of the agent;An incremental development of the system is proposed, so that DVAs continuously interact with external users, who provide new supervisions as the learning process asynchronously proceeds;In order to perform real-time responses, motion coherence is widely exploited, and time budgets are introduced for handling and processing the input video stream.A few ideas at the basis of the DVA architecture and the acronym “DVA” have been introduced in our previous works [10–12], yet they have been limited to a preliminary feature extraction process only, and have never been applied to video semantic labeling. The focus of this paper is on semantic predictions based on visual patterns and motion dynamics. In order to move closer to a real understanding of the scene, higher level reasoning mechanisms should be added to relate the predictions on the frame pixels. While these mechanisms are out of the scope of this work, the selected grounding theory of learning from constraints[7] is generic enough to naturally embed new types of knowledge into the DVA architecture.The next Section of the paper will shortly describe the whole architecture of a DVA, while the subsequent Sections 3 to 4 will describe in detail the computational blocks regarding feature extraction and the symbolic levels, respectively. Section 5 will relate the DVA paradigm to existing works in the literature, while in Section 6 several experiments will show the performance of the proposed approach.The software for running experiments with DVAs can be downloaded at the website of the project:http://dva.diism.unisi.ittogether with several videos and other supplementary material which illustrate the behavior of DVAs in different scenarios.A DVA is a system designed to perform semantic labeling in unrestricted domains, by living in its own environment and by continuously processing videos, following a lifelong learning paradigm. The agent is devised so as to implement all the levels of a truly on-line vision system, starting from feature extraction up to the symbolic layers where interaction with users occurs, and predictions on semantic categories are attached to visual patterns. The system architecture is depicted in Fig. 1.Given a video streamV,we indicate withVtthe video frame at time t. The first element of the DVA pipeline consists of computational blocks hierarchically organized into multiple layers, that will be described in Section 3. In each layer, a set of features are progressively learned and extracted from each pixel x ofVt. The features of theℓ+1th layer are built upon the ones extracted at layer ℓ, and they pretty much resemble the responses to convolutional filters, with a local support that is limited to a small area around x, also referred to as receptive field[13], indicated with a small grid in Fig. 1. Filter responses on x are encoded and collected into a feature vector (a group of colored boxes in Fig. 1), and the response to the same feature for all pixels is referred to as feature map (Fig. 1).DVAs parametrize receptive fields by considering also their transformed instances under the class of affine transformations, paired by a criterion that allows us to get an affine invariant representation of the data covered by the field. This choice allows DVAs to compactly represent such data by a fixed-length vector called receptive input (Section 3.1). As the on-line video processing advances, motion estimation is yielded by matching receptive inputs among consecutive frames (Section 3.2). Given the receptive inputs ofVobserved up to time t, a set of features are learned in an unsupervised setting, following information theoretical principles of Minimal Entropy Encoding [14] (Section 3.3). Features inherit motion coherence by the aforementioned matching scheme, and, within a given layer, they can be grouped to encode different properties of the input. Before being fed as input to the next layer, features are projected onto a space of lower dimensionality, by estimating the principal components over a time window with the NIPALS (Non-linear Iterative Partial Least Squares) algorithm [15]. The deep structure allows higher layers to virtually cover larger areas of the input frame (Section 3.4).The second portion of the DVA pipeline of Fig. 1 will be described in Section 4, and it begins with an aggregation process to partition the input frameVtinto homogeneous regions, sometimes also named super-pixels. To this aim, we extend the graph-based region-growing algorithm of [16], enforcing motion coherence between consecutive frames into the aggregation procedure (Section 4.1). The partitioning obtained for frameVtcontains a set of regions that can be described in terms of the features extracted on the pixels belonging to them. In particular, DVAs average the features among the pixels of each region, to build a unique feature histogram. During the agent’s life, descriptors are progressively accumulated as vertices (nodes) of a graph, named Developmental Object Graph (DOG) (see Fig. 1). Two vertices in the DOG can be linked by an edge if either they are spatially similar, or if the agent collected evidence that motion estimation is relating them (Section 4.2).Descriptors stored in the DOG correspond to visual patterns, and supervisions can be attached to them by external user interaction. In particular, users can provide two kinds of supervisions: (i) strong, which associates one or more tags to a specific coordinate x in a given frame, and therefore to a specific region and node within the DOG; (ii) weak, which only provides the information on the presence of a certain object class within the frame, yet without precisely indicating its position (see Fig. 2). The use of weak supervisions is motivated by real-time user interaction, for example in scenarios where users can only provide spoken supervisions through a microphone: clearly, it is reasonable to employ this kind of labeling only after the concepts have already been learned and developed for some time. DVAs are also designed to ask for supervisions on their own initiative, on those DOG nodes which are most frequently observed and for which a label is either not assigned nor predicted (e.g., unknown objects that are frequently seen and that differ from what the agent is able to recognize).The last computational block of the DVA architecture of Fig. 1 involves the symbolic decision mechanism. DVAs learn a set of ω classifiers on the space of DOG nodes, where ω is the number of classes for which an external supervision was received up to the considered time instant t. The jth classifier models the class membership of a given node to the jth symbolic class, wherej∈{1,…,ω}. Since pixels are associated to regions, and regions to nodes, we get ω pixel-wise predictions onVt. Learning is semi-supervised, since it is based on the few supervisions received up to t, and on the relationships among all the nodes (supervised and not-supervised), represented by the previously introduced edges of the DOG graph (Section 4.3).Both the just mentioned symbolic classifiers and the previously introduced multilayer hierarchical features are grounded on the theory of learning from constraints[7]. Such theory defines a generic learning framework that is centered around the parsimony principle and on the concept of constraint to represent any kind of knowledge on the learning environment, on the learning tasks and on their interactions. The classical regularization framework of kernel machines is naturally extended to the case in which the agents interact with a richer environment, heading to more sophisticated tools called Support Constraint Machines (SCMs). In the case of DVAs, the hierarchical features are constrained to maximize the mutual information between their output values and the receptive inputs, while the symbolic classifiers are subject to supervision constraints on a few nodes, and to motion and spatial coherence constraints between pairs of nodes connected by a DOG edge. The constraints enrich their expressiveness with the progressive exposition to the video so as to follow a truly lifelong learning paradigm. As a matter of fact, as time passes, new receptive inputs are observed, new supervision constraints are introduced by user interaction, and new motion-based relationships are discovered.DVAs are designed so as to continuously interact with the environment, while making predictions at any time. However, there are two major “budgets” that DVAs must take into account: memory budget and time budget. The memory budget is due to the limited storage capability of the host machine, that cannot memorize all the observed receptive inputs (to learn hierarchical features), nor all the region descriptors, i.e., DOG nodes (to learn the symbolic classifiers). DVAs introduce internal storage of tunable sizes, and metrics to compare receptive inputs or region descriptors, so that the ones computed on the current frameVtare associated to the most similar entries in the current storage. A predefined sampling resolution allows the agents to memorize only those elements that are not too similar to the already stored ones, exploiting popular tools in metric spaces that are called ϵ-nets [17]. A removal policy is provided for those data that are not observed by a certain amount of time. Differently, the time budget defines the maximum time to spend on a frame, and it is needed to perform predictions close to real-time. The metric-based comparisons are made fast by exploiting motion coherence, and they can be easily early stopped if the required time budget is overtaken. On the other hand, extracting a large set of features, or computing predictions for a large set of symbolic classes can quickly become cumbersome and time consuming. For this reason, we assume that DVAs operate in a transductive environment, so that features are extracted only from the stored receptive inputs, and semantic predictions are performed only on the stored DOG nodes. The lifelong learning procedure operates so as to cache feature values and the outputs of the symbolic classifiers as the optimization asynchronously evolves: in this way the agent will be able to continuously make predictions by simple lookup operations on the cached data, while learning can proceed in background with slower dynamics (Sections 3.3, 4).The first computational block in the DVA architecture (Fig. 1) is the feature extraction module. Sections 3.1–3.3 will consider the case of a single layer extractor that, given an input frameVt,extracts a vector of d features for each pixel. The vector components are the responses of a small area around x (the so called receptive field[13]) to a bank of d filters that are learned in an unsupervised manner. Equivalently, this can be seen as applying a set of convolutional filters toVt,as in any feature learning scheme based on convolutional networks [18]. The extension to a multilayer hierarchy of extractors will be discussed in Section 3.4.We model a receptive field of x withNGaussians gk,k=1,…,N,centered nearby the pixel at coordinatesxk+x,and with variance η2. The number of Gaussians, their position, and their variances are design choices, and in this work we assume the means to be located on a square gridN×Nof unitary edge, centered in x. We define receptive input of x the vectorξ(x,Vt):=[ξ1(x,Vt),…,ξN(x,Vt)]′,where the kth component is the outcome of convolving the kth Gaussian with the input frame,ξk(x,Vt):=gk⊗Vt∝∫Vte−∥xk+x−y∥22η2Vt(y)dy.In other words, the receptive inputξ(x,Vt)is a filtered representation of the neighborhood of x, with a degree of image detail that depends on the number of GaussiansNand on their variance η2.We can extend the notion of receptive field by allowing it to scale, rotate, or, more generally, to incur in affine transformations. Any 2D affine map A can be rewritten as the composition of three 2D transformations and a scale parameter,A=σRφ1Uφ2Rφ3,where σ > 0 and Rφ, Uϑ areRφ=[cosφ−sinφsinφcosφ],Uϑ=[1cosϑ001],with φ1 ∈ [0, 2π],φ2∈[0,π2),and φ3 ∈ [0, π) [19]. These continuous intervals are discretized into grids Φ1, Φ2, Φ3, and, similarly, we collect in Σ a set of discrete samples of σ (starting fromσ=1). The domainT=Σ×Φ1×Φ2×Φ3collects all the possible transformation tuples (or simply tuples).Following this new notion of receptive field, and given a tupleT∈T,we can redefine the receptive input to depend on T as well, introducingξ(x,T,Vt),whose kth component is(1)ξk(x,T,Vt)∝∫Vte−∥σRφ1Uφ2Rφ3xk+x−y∥22σ2η2Vt(y)dy.Notice that the value of σ affects both the width of the Gaussians and their centers, and that only the transformation parameter σ affects the “shape” of the Gaussian bells. For this reason, computing the receptive input for all the pixels x and for all the transformations inTonly requires to perform |Σ| convolutions per-pixel, independently of the number of centersNand on the size of the grids Φ1, Φ2, Φ3.11The non-uniform scaling ofUφ2should generate anisotropic Gaussians (see [19]), that we do not consider here both for simplicity and to reduce the computational burden.The receptive input can include invariance to local changes in brightness and contrast, that we model by normalizingξ(x,T,Vt)to zero-mean and unitary L2 norm.For each pixel x ofVtwe can compute|T|receptive inputs, by varying the transformation tupleT∈T. That is, T can be considered as a hidden variable depending on pixel x of frameVt. DVAs associate a unique receptive input to each pixel, by determining the most appropriate tupleTxt. The selection criterion is based on similarity matching with respect to the video processed up to the current time instant. We aim at “warping” the receptive field by the tupleTxtfor which the receptive inputξ(x,Txt,Vt)is more “similar” to one of the other receptive inputs that DVA observed up to now, indicated withξxt. Let Q be the collection of receptive inputs up to time t, and dist( ·, ·) a metric on Q22We do not explicitly indicate the dependance of Q on the frame and pixel indices to keep the notation simpler.. Givenx∈Vtwe solve(2)(Txt,ξxt)=argminT∈T,ξ∈Qdist(ξ,ξ(x,T,Vt))to determine the optimal transformation tupleTxt,and to associate x to its nearest neighbor in Q (that isξxt), as depicted in Fig. 3(full search). Then, the new receptive inputξ(x,Txt,Vt)is added to Q, and we move to the next pixel, repeating the procedure.It is clearly unfeasible to store in Q all the receptive inputs of every pixel of the video. On the other hand, since Eq. (2) aims at moving a new input closer to one of those already on Q, we introduce a tolerance ϵ to avoid storing near-duplicates. We addξ(x,Txt,Vt)to Q only if its distance fromξxtis larger than ϵ. Thus, ϵ determines the sampling resolution. The criterion of Eq. (2) is also well-suited for creating dense regions of similar receptive inputs in Q, which is what we are going to exploit to generate compact codes using density-based encoding schemes (as we will describe in Section 3.3).The data in Q are distributed on a(N−2)-sphere of radius 1, because of the L2 normalization and the mean subtraction. For this reason, when dist(·, ·) is chosen as the Euclidean distance, a similarity measure based on the inner product ⟨·, ·⟩ can be equivalently employed to compare receptive inputs.33Due to the L2 normalization, dist( ·, ·) depends only on the dot product of its arguments,dist(ξi,ξj)=∥ξi∥2+∥ξj∥2−2〈ξi,ξj〉=2−2〈ξi,ξj〉.The set Q is an ϵ-net of the subspace ofIRNthat contains all the observed receptive inputs. Such nets are standard tools in metric spaces, and they are frequently exploited in searching problems because of their properties [12]. For instance, it can be easily shown that there exists a finite set Q for any processed video stream. Finding a solution to Eq. (2) in an ϵ-net, for all pixels in a given frame, can be speeded up by a pivot-based mechanism [12], that avoids searching over the entire setT×Q.WhileQ=∅at the beginning of the agent’s life, it is progressively populated as long as time passes. The transformationTxtof the first pixel of the video is clearly not-unique, thus we arbitrarily set it to [1, 0, 0, 0] and add the receptive input to Q. More generally, the creation of Q turns out to be strongly based on the very early stages of life, in which the population in Q is very small. This does not seem to be an appropriate developmental mechanism in principle, so we propose using a blurring scheme such that Q ends up into a nearly stable configuration only after a certain visual developmental time. We initially set the variance factor η of the Gaussian filters of Eq. (1) to a large value, and progressively decrease it with an exponential decay that depends on t. This mechanism produces initial frames (input layers) strongly blurred, so that only a few receptive inputs are added to Q for each frame (since they are almost all near-duplicates, due to the blurring of the image). As η is decreased, the number of items in Q grows until a stable configuration or a maximum amount of budget memory are reached. This resembles somehow curriculum learning [20], where examples are presented to learning systems following an increasing degree of complexity. When the memory budget for Q is set, we propose using a removal policy of those elements that are less frequently solutions of Eq. (2).Despite the use of efficient schemes to find a solution to Eq. (2) for all pixels in a given frame, the procedure still quickly becomes computationally intractable when the resolution of the video and the number of transformation tuplesTget larger. Strict time requirements in real-time settings can be met by partitioning the search space into mini-batches, and by accepting sub-optimal solutions within a pre-defined time budget. However, the quality of the result can be unsatisfactory for practical usages.While(Txt,ξxt)in Eq. (2) indicates the optimal solution of the problem, here we relax the notation by using the same symbols to also refer to potentially suboptimal solutions. The quantityDxtis the value of the objective function of Eq. (2) computed for the given(Txt,ξxt).We exploit the inherent coherence of video sequences to define a heuristic technique which performs quick local searches and that can provide good approximations of the problem stated by Eq. (2), while greatly speeding up the computation. The key idea is that the scene smoothly changes in subsequent frames and, therefore, we can “track” receptive inputs betweenVt−1andVt,also yielding a motion estimation for the pixels of the frame. We use the already computed pairs(Txt−1,ξxt−1)and the distancesDxt−1to compute the new pairs(Txt,ξxt)and the newDxt,following [12]. For each pixel location x ofVt,we look for a location y inVt−1that can be associated to x, such that: (1) the coordinates y belong to a neighborhood of x, since the smoothness assumption implies a small translation betweenVt−1andVt; (2) the tupleTxtis “close” toTyt−1,since the smoothness assumption also implies small changes in the scene; (3) the receptive input computed at x in frameVtmust be similar to the one that was computed at y in frameVt−1. The last requirement can be reformulated by constraining the distance between the receptive input computed at x in frameVtandξyt−1to be similar toDyt−1.In detail, we indicate withT˜yt−1the set of tuples that are equal or close44We considered 9 cases forT˜yt−1,by increasing/decreasing the 4 transformation parameters ofTyt−1one at a time, or none of them.toTyt−1. Given the pixel x of the current frame and a search radius r, we scan the pairs(Tyt−1,ξyt−1)with y satisfying|x−y|=r. For iteratively increasing values of r (starting withr=0), we look for a pixel y such thatdist(ξ(x,T˜yt−1,Vt),ξyt−1)≤ζ·Dyt−1,where ζ ≥ 1, considering all the tuples inT˜yt−1∈T˜yt−1. When the maximum radius is reached without finding a solution, then full searches are performed, as described in Section 3.1 (see also Fig. 3). Otherwise, we haveTxt=T˜yt−1,ξxt=ξyt−1,and we setDxt=min(Dyt−1,dist(ξ(x,T˜yt−1,Vt),ξyt−1)),to avoid progressive increments of the estimatedDxt. Finally, the found associationy∈Vt−1→x∈Vtis an instance of motion estimation between x and y, that will be exploited also in the last stage of the DVA pipeline of Fig. 1.So far, we have described how DVAs determine, for each pixel in the video stream, the transformation parameters (Txt) and the “best” representative (ξxt) in Q. In order to extract and encode the set of d features from the streamVup to time t, it is natural to exploit the data in Q, as it is a subsampling of the receptive inputs processed up to t. Once we have extracted features from Q, we can propagate them to each pixel of the video, using the aforementioned association “pixel → representative in Q”.The goal is to associate a code out of d symbols,p(ξi)=[p1(ξi),…,pd(ξi)],to each ξi∈ Q, such that similar ξi’s will share similar codes. The vector p(ξi) is the feature vector extracted from ξi(Fig. 1). The set Q is built using a criterion that favors the creation of dense regions (Section 3.1), so that the notion of similarity can be translated into the idea of assigning a similar code to all data grouped into dense regions, and placing separation surfaces between different codes in low-density regions. Minimal Entropy Encoders (MEEs) [14] implement this criterion in the context of kernel machines, showing good performances in clustering (encoding) tasks. MEEs learn a set of d functionsf(ξi)=[f1(ξi),…,fd(ξi)],that are placed in a probabilistic relationship by the softmax function,(3)pj(ξi)=efj(ξi)∑k=1defk(ξi).The feature vectorp(ξi)=[p1(ξi),…,pd(ξi)]is a collection of probabilistic scores (it sums to 1), enforcing the functions in f(ξi) to compete during their development. The rationale behind MEEs is that of getting close-to-one-hot configurations of the probabilities p(ξi) for all the ξi∈ Q (i.e., only one feature probability is “high” in the feature vector) and of exploiting the whole available set of features (i.e., all the d features are used in Q, on average), by means of functions f(·) that are smooth on Q (so that the separation between different one-hot configurations will be placed in low density regions).MEEs are instances of the theory of learning from constraints[7]. Under such a framework, MEEs can be seen as Support Constraint Machines that minimize a parsimony principle (the norm of f(·)) under a (soft) constraint that enforces the maximization of the mutual information (MI) between the space of receptive inputs (X) and the space of the output features (Y). Formally, their objective is(4)minf−MI(Y(f),X)+λ∥f∥=minfH(Y(f)|X)−H(Y(f))+λ∥f∥where H(·) and H(· | ·) are the entropy and the conditional entropy, respectively. In the case of DVAs, the entropy functions can be computed with X being the discrete sample Q, and Y(f) the space of output features whose activation probabilities are given by Eq. (3).When the parsimony principle is casted into the Reproducing Kernel Hilbert Space (RKHS) of a given kernel function K( ·, ·), the solution becomes a kernel expansionfj(ξ)=∑k=1|Q|mkjK(ξk,ξ),with ξk∈ Q. In the case of a linear kernel function ⟨·, ·⟩, we getfj(ξ)=〈uj,ξ〉. Then, the problem of Eq. (4) can be solved with respect to the variables mjor uj(j=1,…,d),respectively [7,14].55In the DVA implementation, the dth feature is not learned by MEEs, but automatically triggered in presence of almost constant receptive inputs (standard deviation under a given threshold). These inputs are not added to Q.In the linear case, each fj(·) can be seen as the response to a convolution (correlation) operation between a local image portion (ξi) and a linear filter (uj). In the non-linear case, fj(·) models a more complicated non-linear relationship between the input and the filter response.Since DVAs implement an instance of transductive learning (as described in Section 1), the learning procedure and the prediction of the feature vectors are performed on the same domain, Q. DVAs are able to extract features for all the pixels by a simple lookup operation, exploiting the association “pixel → representative on Q → feature vector”, that is particularly well suited in the case of non-linear kernels, in which the kernel expansion may involve several terms. Moreover, since the described motion estimation scheme enforces coherence on receptive inputs among consecutive frames, feature vectors inherit such coherence.Considering the feature extraction and the invariance mechanisms of Section 3.1 as a whole, an important property must be remarked. There are no repeated instances of the same input under different affine transformations in the set Q, by construction. As a result, the MEE will not learn multiple features that correspond to the same input under affine transformations. For example, suppose that the jth feature responds to an edge-like pattern. When a DVA processes edges at different orientations, it will always encode them using the jth feature. The representation capabilities of each feature in the learning algorithm is enhanced by the invariance mechanism, without increasing the computational burden of the learning scheme. On the other hand, we lose any transformation-related information (e.g., the orientation of the edge in the previous example, or, more generally,Txt). In some real-world recognition tasks, it might be important to encode also the way the pattern is geometrically oriented into the feature vector. For this reason, a transformation unrolling mechanism can be activated in DVAs, in which the feature vector is augmented to include transformation related data. Each feature is represented|T|times in the augmented vector. Only the representation corresponding to the index of the tupleTxtwill be set to the original value of the feature (the other repetitions will be set to zero). The size of the augmented feature vector will bed×|T|.The feature extraction scheme described up to this point allows DVAs to associate a vector of d features to each pixel x of the frameVt. We use the symbolp(x,Vt)to indicate such vector, overloading the previously used notation (to avoid making an explicit reference to the receptive inputs, keeping the notation simpler). DVAs process the input frameVtat resolution w × h and produces(w−b)×(h−b)feature vectors of length d, where the quantity b is the number of pixels that we have to discard due to border effects (we cannot compute receptive fields on image border, due to the finiteness of the image). We can represent such data with a tensor sized(w−b)×(h−b)×d. The input frame can be a grayscale image, as well as an RGB representation, a concatenation of RGB and grayscale, or other combinations. More generally, the input can be a w × h × q tensor (e.g.,q=1for grayscale images,q=3for RGB data). The proposed feature extraction by means of receptive inputs allows DVAs to handle generic tensorial data without affecting the described computations. The centers of theNGaussians are simply replicated for the q input channels, leading to receptive inputs withq·Ncomponents. We can build richer features by replicating the feature extractor into two possibile “directions”: vertical and horizontal, respectively. Fig. 4illustrates an example with the notation what we are going to introduce in the rest of this Section.In the vertical case, we build a hierarchy of extractors by stacking them into L layers, so that the features learned/extracted in layer ℓ are the input of the features that will be learned/extracted in layerℓ+1. Formally, we indicate with wℓ × hℓ × qℓ the size of the input for layer ℓ, while bℓ is the border to be discarded on such layer. The output tensor will be sized(wℓ−bℓ)×(hℓ−bℓ)×dℓ,where dℓ is the number of features of layer ℓ. The higher level features are the outcome of a composition of feature extractors, thus modeling more complicated non-linear functions (due to the softmax operator), as commonly done in deep learning algorithms. Moreover, as long as we move toward the higher layers, the receptive-field-based feature extraction virtually involves larger image portions around each pixel.In principle, due to the stacking mechanisms, the dimensions of the input tensor of layer ℓ arewℓ=(wℓ−1−bℓ−1),hℓ=(hℓ−1−bℓ−1),andqℓ=dℓ−1. A main issue of this bare stacking is that computing receptive inputs for layers ℓ > 1 can be very cumbersome, due to the size of the input tensor that depends on the number of features extracted from the layer below (i.e.,qℓ=dℓ−1). While in image classification it is quite common to pool and subsample the tensor to reduce its size in the first two dimensions, in the case of DVAs we do not subsample it, to keep a pixel-wise feature extraction that will be needed for the final goal of semantic labeling. However, some of thedℓ−1features may be correlated, or some of them could be triggered very rarely or never. For those reasons, it can be useful to project thedℓ−1features into a smaller dimensional space before feeding it to the next layer, so thatqℓ<dℓ−1. DVAs apply stochastic iterations of the NIPALS (non-linear iterative partial least squares) algorithm [15], to roughly compute qℓ principal components over a set of feature vectors (sizeddℓ−1>qℓ) that are randomly taken during video processing in a given time window.At each layer of the deep net, the feature learning is based on the same principles and algorithms. However, we use developmental stages based on learning layers separately, so as upper layers activate the learning process only when the features of the lower layers have already been learned, which can be measured by the stability of the MEE objective function, and of the Q set.Feature extractors can be replicated also horizontally. In layer ℓ we can implement Cℓ feature extractors, each of them independently contributing to learn a subset of the dℓ features. We indicate the number of features on each subset withdcℓ,withc=1,…,Cℓ(we generally select them to be roughly of the same size). Each group ofdcℓfeatures is what we refer to as a category of features. Different categories are characterized by the different portions of the input taken into account for their computation. In other words, each of the Cℓ “horizontal” feature extractors processes a subset of the input tensor for layer ℓ. In particular, the third dimension, qℓ, is partitioned into Cℓ disjoint groups, generating Cℓ input tensors. In the case of the first layer, each category can operate on a different input channel or on different combinations of the channels.We indicate withpcℓ(x,Vt)the feature vector, sizeddcℓ,extracted on the cth category of the ℓth layer. The pixel-based features that are developed in the whole architecture are used for the construction of higher-level representations that are involved in the prediction of symbolic functions.In order to build high-level symbolic classifiers for semantic labeling, we first aggregate homogenous regions (superpixels) ofVt,as it will be described in Section 4.1. This reduces the computational burden of pixel-based processing, but it requires to move from the pixel-based descriptorspcℓ(x,Vt)to region-based descriptorsszt,where ztis the index of a region ofVt. Section 4.2 will show how these region-based descriptors can be stored into a Development Object Graph, and Section 4.3 will describe how a classifier can be built over such graph, yielding to semantic labeling.The aggregation procedure generates R regions66In the following description we drop the dependence of the region variables on time t to keep the notation simple.(superpixels) rz,z=1,…,Rfor a given frame, where each rzcollects the coordinates of the pixels belonging to the zth region. We extended the algorithm in [16], a simple graph-based segmentation algorithm, that has already been exploited to perform segmentation over time (e.g., see [1]). The algorithm progressively merges pixels according to a dissimilarity function based both on color similarity (as in [16]) and on motion coherence. Other approaches, such as the recent streaming hierarchical video segmentation [21] (still a graph-based approach), could be used as well. Differently from that work, we do not make explicit use of voxels in the aggregation stage, while we introduce motion information from the optical flow in the pixel-wise similarity measure, getting an instance of video segmentation.77Our approach also generalizes the case of convolutions over time, leading to 3D voxels in the feature extraction stage.Our algorithm first computes a dissimilarity score between the RGB triplets of neighboring pixels.88We also tried L*a*b, but we did not observe significant difference in the overall performance of the system.Then, such measure is decreased (increased) for those pixels whose estimated motion directions are (are not) coherent, by a customable factor. In this way, neighboring pixels which are locally moving in the same direction will more likely belong to the same region. The dissimilarity is also decreased for pixels belonging to the same static region (i.e., a region whose pixels are estimated to not move) in the previous and current frames. The obtained regions correspond to visual patterns that the user can tag with supervisions, and they can be described in terms of the features extracted on the pixels belonging to them. DVAs compute the average feature vector to build a feature histogram on each region, repeating the process for all the layers and categories, finally stacking the results to generate the region descriptor sz. We also append to szthe color histogram of rz, considering 4 equally spaced bins for each channel of the considered color space. Finally, we normalize szto sum to 1, giving the same weight to the feature-based portion of szand to the color-based one (more generally, the weight of each portion is a customizable parameter). The length of szis then∑ℓ=1L∑c=1Cℓdcℓ+43.Region descriptors and their relations are stored as vertices (or “nodes”) and edges of a graph, that we refer to as Developmental Object Graph (DOG). Following the same approach described for receptive inputs and the set Q in Section 3, nodes are stored into the set V, and each region descriptorsztis either mapped to its nearest neighbor vj∈ V (if their distance is below a sampling resolution τ), or it is added to V (otherwise). In the first case, we say thatszthits node vj. We exploit the χ2 distance, since it is well suited for comparing histograms. Also in this case the search procedure can be efficiently performed by partitioning the search space, and by tolerating sub-optimal mappings. A pre-defined time budget is defined, and we return the best response within such time constraint.We can also use region-based motion coherence to strongly reduce the number of full searches required to retrieve the exact (or good approximation of the) nearest neighbors on V for the region descriptors ofVt. We partition the image into M × M rectangular areas, and we associate each region to the area containing its barycenter. We are given the mappings between region descriptors and their closest DOG nodes at frameVt−1. For each regionrztin frameVt,belonging to a given M × M area, we compute the descriptorszt. Then, we search the same area inVt−1(and the neighboring areas) for a regionrht−1whose descriptorsht−1is similar toszt. If such a region is found, thenszthits the same DOG node vjthat was associated torht−1in the previous frame (see Fig. 5). In this case,rztis associated to vjwithout the need to perform a full search to retrieve its nearest neighbor in V (as in the case of Section 3, this might be a suboptimal solution). A memory budget is also employed to limit the maximum number of stored DOG nodes, and a dedicated policy is used to decide which nodes can be removed during the life of the agent.DOG edges are of two different types, spatial and motion-based, and their weights are namedwijsandwijm,respectively. Spatial connections are built by assuming that close descriptors represent similar visual patterns. Only nodes that are closer than a predefined threshold γs> τ are connected, leading to a sparse set of edges. Weights are computed by the χ2 Gaussian kernel, aswijs=exp(−χ2(vi,vj)/2στ2).It is worth remarking that nodes that represent regions with similar appearance may not be actually spatially close, due to slight variations in lighting conditions, occlusions, or due to the suboptimal solutions of the receptive input matching process described in Section 3. The motion between framesVt−1andVtcan be used to overcome this issue, and, for this reason, we introduce links between nodes that are estimated to be the source and the destination of a motion flow. The weights are initialized aswijm=0att=0,for each pair (i, j), and then they are estimated by a two-step process. First we compute the likelihood Pt(va, vb) that two DOG nodes va, vb∈ V are related in two consecutive framesVtandVt−1due to the estimated motion. Then the weightwa,bmof the edge between the two corresponding DOG nodes is updated. Pt(va, vb) is computed by considering the motion vectors that connect each pixel inVtto another pixel ofVt−1(Section 3.2). For each pair of connected pixels, one belonging to regionrzt⊂Vtand the other torht−1⊂Vt−1,we consider the DOG nodes vaand vbto whichrztandrht−1are respectively associated. The observed event gives an evidence of the link between vaand vb, and, hence, the frequency count for Pt(va, vb) is increased by a vote, scaled by|rzt|to avoid penalizing smaller regions. Moreover, in the computation we consider only votes involving regions of comparable size, i.e.|rzt|/|rht−1|∈[0.8,1.25]Finally, since a DOG node v corresponds to all the region descriptors that hit it, the total votes accumulated for the edge between vaand vbare also scaled by the number of distinct regions ofVtthat contributed to the votes. Similarly to the spatial case, a sparse connectivity is favored by pruning the estimates below a given threshold γm, in order to avoid adding weak connections due to noisy motion predictions. Weights are computed by averaging through time the computed probabilities: aswa,bm=1t+1∑u=0tPu(va,vb). This step can be done with an incremental update that does not require to store the likelihood estimates for all the time steps.So far, we described how a pixel x is univocally associated to a region of the current frame, whose descriptor is associated to a DOG node, with nodes related by spatial and motion-based connections. External supervisions can also be exploited and handled within this framework, by learning a set of classifiers predicting the labels (tags) associated to each DOG node → to each region ofVt→ to each pixel ofVt,that is the final goal of semantic labeling.While the video stream is processed, humans can interact with DVAs providing custom strong supervisions for a selected coordinate x of the frame they are observing (Section 2). Classes are not known in advance, and users can provide both positive (+1) and negative (−1) supervisions. Let ω be the number of classes for which the DVA received at least one supervision up to time t (ω=0at the beginning of the agent’s life). The DVA learns a set of ω classifiers on the space of DOG nodes V,l(vi)=[l1(vi),…,lω(vi)],vi∈V,following the theory of learning from constraints[7], as already done in the case of MEEs (Section 3.3). In particular, we look for smooth functions l(·), that are required to fulfill a set of constraints in a soft manner. Smoothness is modeled by minimizing the norm ‖l‖, while the constraints are of two types: supervision constraintsμS(1),and coherence constraintsμM(2). Formally, we solve(5)minl{μS(1)(l)+μM(2)(l)+λR∥l∥},where λR> 0 is a scalar weight. We remark that the aforementioned theory does not make any restriction on the source of the involved constraints, so that we plan to add new constrains in future work (relationships among different classes, hierarchical organization, first order logic clauses [8,9]).The supervision constraints enforce the approximation of labelsyi,k∈{−1,+1}on some DOG nodes vi∈ V and for some functions lk. For each lk, the supervised nodes are collected into the setSk={(vi,yi,k),i=1,…,nk},and(6)μS(1)(l)=∑k=1ω∑(vi,yi,k)∈Skβikmax(0,1−yi,klk(vi))2.The scalar βik> 0 is the belief[7] of each point-wise constraint. When a new supervision for class k is given on node vi, its belief is set to a fixed value. Then, βikis increased if the user provides the same supervision multiple times or decreased in case of mismatching supervisions, and it is normalized to keep∑iβik=1. This allows the agent to better focus on those supervisions that have been frequently provided, and to give less weight to noisy and incoherent labels. When the user provides a supervision for class k without indicating the exact position to which it should be attached (weak supervision, Section 2), DVAs uniformly include it into Eq. (6) if they determine that there exists a node associated to the current frame for which lk(·) is above a predefined threshold. DVAs can also ask for supervision on those nodes that are hit more frequently and for which l(·) is small, meaning that no confident predictions are performed on them. When a frame that includes a region associated to one of these nodes is processed, it is saved and shown to the user, whose reply is treated as strong supervision.The coherence constraints enforce a smooth prediction over connected vertices of the DOG,(7)μM(2)(l)=∑k=1ω∑i=1|V|∑j=i+1|V|wij(lk(vi)−lk(vj))2,leading to an instance of manifold regularization [22]. The task is semi-supervised, since such constraints operate on all DOG nodes (both supervised and unsupervised). In this case, the belief of each point-wise constraint (wij) is a linear combination of the aforementioned edge weightswijsandwijm,(8)wij=λM(αM·wijs+(1−αM)·wijm).HereλM>0defines the global weight of the coherence constraints whileαM∈[0,1]can be used to tune the strength of the spatial-based connections w.r.t. the motion-based ones.When casted into the framework of RKHS, the solution of Eq. (5) is a kernel expansion in the DOG nodes,lk(v)=∑i=1|V|oikK(vi,v),and the learning problem can be solved with respect to the variables oik. We also assume that the agent is biased towards negative predictions, adding a fixed bias term equal to−1to each lk(·). This choice, paired with a kernel with local support, allows DVAs to learn from positive examples only. For this reason, we selected the χ2 exponential kernel [23].Coherently to the feature extraction of Section 3.3, DVAs implement an instance of transductive learning also in the case of the class-predictions l(·) over the set V. The motivations are exactly the same: the learning can proceed in background, asynchronously with respect to the video stream, and the values of l(·) on V can be buffered. The predictions on each pixel can be performed by a simple lookup operation, exploiting the association “pixel → region → node ∈ V → buffered predictions”. The set of DOG nodes V progressively grows as the video stream is processed, up to a predefined maximum size (due to the selected memory budget). As V reaches the maximum allowed size, the adopted removal policy selects those nodes with a small number of hits, and that are neither involved by any supervision constraint, nor been recently hit by any descriptor.

@&#CONCLUSIONS@&#
We introduced Developmental Visual Agents (DVAs) for semantic labeling in unrestricted videos. The key idea in the DVA architecture is to implement a lifelong learning paradigm, so that each agent can progressively develop its visual ability, by a continuous and never-ending interaction with the external environment. The approach is completely different from most of the current state-of-the-art systems in computer vision, which typically work on images, by relying on huge labeled data sets. DVAs, on the contrary, can exploit even a few supervisions per semantic category, by implementing motion coherence constraints that virtually spread supervision information. The experimental results presented in this paper show very promising results for this highly challenging task, describing a complete bottom-to-top experience.The DVA architecture can be extended in different ways. At the symbolic level, several different solutions can be conceived in order to plug contextual information into the model. For example, we are currently investigating some mechanisms which exploit the co-occurrence between semantic categories, as well as between DOG nodes, to perform a sort of collective classification for scene parsing. Moreover, the proposed theoretical framework also allows to model generic logic constraints into the Support Constraints Machines, thus allowing to perform high level cognitive tasks such as spatial-semantic reasoning. Another intersting direction of research we are undertaking is to extend the low-level filters through the temporal dimension, in order to extract spatio-temporal features which, integrated in the framework of learning from constraints, may provide very powerful features for action recognition.Conflict of Interest: The authors declare that they have no conflict of interest.