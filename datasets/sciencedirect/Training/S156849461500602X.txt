@&#MAIN-TITLE@&#
Control strategy PSO

@&#HIGHLIGHTS@&#
An evaluation index called “Control Strategy PSO” is developed.It can be applied to other intelligent algorithms.We present a detailed theoretical and empirical analysis.

@&#KEYPHRASES@&#
Particle swarm optimization,Parameter selection,Evaluation index,Control theory,

@&#ABSTRACT@&#
Many variants of particle swarm optimization (PSO) both enhance the performance of the original method and greatly increase its complexity. Motivated by this fact, we investigate factors that influence the convergence speed and stability of basic PSO without increasing its complexity, from which we develop an evaluation index called “Control Strategy PSO” (CSPSO). The evaluation index is based on the oscillation properties of the transition process in a control system. It provides a method of selection parameters that promote system convergence to the optimal value and thus helps manage the optimization process. In addition, it can be applied to the characteristic analyses and parameter confirmation processes associated with other intelligent algorithms. We present a detailed theoretical and empirical analysis, in which we compare the performance of CSPSO with published results on a suite of well-known benchmark optimization functions including rotated and shifted functions. We used the convergence rates and iteration numbers as metrics to compare simulation data, and thereby demonstrate the effectiveness of our proposed evaluation index. We applied CSPSO to antenna array synthesis, and our experimental results show that it offers high performance in pattern synthesis.

@&#INTRODUCTION@&#
Motivated by the social behavior of flocks of birds and schools of fish, particle swarm optimization (PSO) is a stochastic process that is an efficient, robust, and simple optimization algorithm for finding optimal regions of complex search spaces. PSO uses a set of particles that represent the potential solutions needed to solve an optimization problem. The particle moves toward an optimal solution based on its present velocity and its individual best position found at each iteration, while also incorporating the globally best solution found by its companion particles. The position and the velocity relationship after the kth iteration are obtained by the following update formula:(1)v[k+1]=ω⋅v[k]+cp⋅rp[k]⋅(p[k]−x[k])+cg⋅rg[k]⋅(g[k]−x[k])(2)x[k+1]=x[k]+v[k+1]where ω is a constant in the range (0, 1) called the inertia weight; cpand cgrepresent the acceleration factors, which denote the cognition and social learning factors, respectively; rpand rgare two independent uniform random numbers, different from each other, and generally distributed between 0 and 1; p[k] is the best previous position of x[k]; and g[k] is the best overall position achieved by a particle within the entire population.To gain deeper insight into the mechanism of PSO, many theoretical analyses have been conducted on the algorithm, and most of these works have focused on the behavior of a single particle in PSO, analyzing the particle's trajectory or stability using deterministic or stochastic methods [1–6]. To view PSO from a new perspective, we constructed a relationship between the dynamic process of PSO and the transition process of a control system in order to identify factors that influence the convergence speed and stability of basic PSO without increasing the algorithm's complexity.The outline of this paper is organized as follows. In Section 2, we summarize significant previous developments regarding the original PSO method. In Section 3, we propose a theoretical analysis for convergence in a standard PSO according to control theory. A comparative analysis of our control strategy PSO, which incorporates the opinions of Jiang and Fernández-Martínez, is presented in Section 4. The experimental environment and data analysis are described in Section 5. Section 6 discusses the application of this new strategy to antenna array pattern synthesis. Finally, we draw conclusions and describe our plans for future research in Section 7.It is well known that both exploration and exploitation activities are necessary to achieve optimization using the PSO algorithm. In practice, exploration and exploitation contradict each other, and therefore premature convergence will occur when these activities are not balanced properly. So, to achieve good performance with PSO, many theoretical analyses and improved algorithms have been proposed. Shi and Eberhart [7] first incorporated the concept of inertia weight into the original PSO algorithm to balance local and global searching during the optimization process. They concluded that the PSO with an inertia weight in the range [0.9, 1.2] will have a better performance on average. Some researchers have also attempted to simulate particle trajectories by directly sampling the particles using a random number generator with a certain probability distribution. For instance, Clerc and Kennedy [1] mathematically analyzed the stochastic behavior of the PSO algorithm in stagnation and introduced a PSO variant with a constriction factor. Later, they compared the performance of the PSO with inertia weight and a constriction factor [3]. In addition, Kennedy [8] proposed a type of PSO where the usual velocity formula was replaced by samples from a Gaussian distribution. To avoid premature convergence of PSO, Chen et al. [9] developed a novel hybrid optimization method, called the hybrid PSO–EO algorithm, which combines PSO with extremal optimization (EO). Sun et al. [10] proposed quantum-behaved particle swarm optimization (QPSO) as well as additional algorithms that improved QPSO. Although similar variants were continually devised [11–14], most of the improved PSO methods increased the complexity of the algorithm. This need not be the case, however, and Pedersen and Chipperfield [15] presented a simplified PSO called many optimizing liaisons (MOL) which is similar to “social-only” PSO: the only difference was that the search range would be decreased for all dimensions simultaneously by multiplying with a factor for each failure to improve the fitness.Accelerating the convergence speed and avoiding the local optimal solution are two main goals in PSO research. Many factors affect the convergence properties and performance of the PSO algorithm, such as population size, velocity clamping, position clamping, topology of the neighborhood, synchronous or asynchronous updates. Among these factors, the values of the inertia weight and the acceleration coefficient may significantly impact the efficiency and reliability of the PSO. Properly selecting these two parameters can improve the convergence rate of PSO using a smaller number of particles as well as increase the operation speed.Some theoretical analyses of particle trajectories have provided insight into how the particle swarm system works. Eberhart and Shi [3] empirically found that an inertia weight of 0.729 and an acceleration coefficient of 1.496 are good parameter choices that led to convergent trajectories. Trelea [4] analyzed the dynamic behavior and convergence of the standard PSO algorithm using standard results from discrete-time dynamical system theory and provided a parameter set in the algorithm convergence domain. Jiang et al. [6] studied the stochastic convergence properties of the standard PSO algorithm and came up with a condition that ensures stochastic convergence of the particle swarm system. Then, according to the results of their analysis, Jiang et al. [5] suggested a set of PSO parameters. In their study, Fernández-Martínez et al. [16] proposed some promising parameter sets and established a range for the inertia value and acceleration coefficients after investigating the properties of the variance and covariance of second-order moments. In subsequent studies, Chen and Jiang [17] proposed a statistical interpretation of particle swarm optimization in order to capture the stochastic behavior of the entire swarm. They suggested an acceleration coefficient that combines the effects of both the inertia weight and the common acceleration coefficient for the neighborhood of the global best position.The aforementioned reports use mathematical analyses to provide insight into how a particle swarm system works. The oscillation properties of PSO also influence the optimization process, but few reports have focused on analyzing these properties from a control theory perspective. Both the No Free Lunch Theorem [18] and the Optimal Contraction Theorem [19] indicate that no optimization method will be optimal for arbitrary problems, and a balance between exploitation and exploration in PSO is desirable for all problems. To enhance the searching ability of PSO and to accelerate its convergence, we performed a detailed theoretical and empirical analysis and propose a parameter selection scheme based on control theory.To improve convergence performance, it is necessary to analyze the behavior of particle trajectories. By substituting Eq. (1) into Eq. (2), the following nonhomogeneous recurrence relation is obtained:(3)x[k+1]+(cp⋅rp[k]+cg⋅rg[k]−1−ω)⋅x[k]+ω⋅x[k−1]=cp⋅rp[k]⋅p[k]+cg⋅rg[k]⋅g[k]Eq. (4) is obtained by applying the expectation operator to both sides of Eq. (3):(4)Ex[k+2]+cp+cg2−1−ω⋅Ex[k+1]+ω⋅Ex[k]=cp⋅p[k]+cg⋅g[k]2According to the z-transform of Eq. (4), the corresponding characteristic equation is(5)z2+cp+cg2−ω−1z+ω=0which has the complex eigenvalues given by(6)z1,2=1+ω−((cp+cg)/2)±j4ω−(((cp+cg)/2)−ω−1)22where j is the imaginary unit, and the stability condition is given by(7)|z1,2|<1According to the time-domain analysis of linear control systems, the complex eigenvalues of Eq. (6) in the z-plane can be expressed as(8)z1,2=e−ξωn±jωn1−ξ2where ωn is the natural frequency and ξ (0<ξ<1) is the damping ratio of the control system. Both parameters are characteristics of the PSO algorithm itself. The decay rate and amplitude of the transient component depend on ωn and ξ. The maximum overshoot M denotes the maximum peak value of the response, which is decided by a system's damping degree, such that the greater the value of ξ, the smaller the maximum overshoot.(9)M=e−πξ1−ξ2×100%The characteristics of Eqs. (8) and (9) can be summarized as follows:(1)Lower |z1,2| values lead to faster convergence, while higher values result in unstable motion and slower convergence. The convergence rate becomes slower when the value of |z1,2| approaches 1 and continuous oscillation occurs when |z1,2|=1. Thus, higher |z1,2| values enable the swarm to cover a wider region of the search space. Lower |z1,2| values are beneficial in the later stages of searching, when faster convergence is preferable.The dynamic characteristic Ex(k), which is decided by the complex eigenvalues, is a sinusoidal oscillation with a natural frequency of damped oscillation equal to ωn. Too small a value of ωn promotes premature convergence, whereas if ωn is too large there is excessive oscillation, potentially making convergence impossible when there is a limited time period for optimization.The parameter ξ determines the way particles oscillate over attractor points. When ξ equals 1, a particle approaches its attractor in a non-oscillatory fashion. By adjusting ξ to a value between 0 and 1, a desired behavior can be obtained. The maximum overshoot is only a function of the damping ratio ξ, as shown in Eq. (9). The greater the maximum overshoot, the greater the oscillation amplitude. But large M values do not favor fast stabilization of the system, while small M values do not promote system optimization.From these analyses, we conclude that the maximum overshoot M and the natural frequency of damped oscillation ωnare the primary influences on optimization behavior. When the value of ωnincreases, the maximum overshoot M will decrease. Conversely, the maximum overshoot M will increase as ωndecreases. Based on engineering experience and corresponding experimental results, we propose a new parameter selection strategy called “Control Strategy PSO” (CSPSO). A detailed discussion of the validity of the strategy follows in Section 3.(10)M×ωn=1M∈(0.65,0)According to Eq. (10), the PSO algorithm will achieve better balance between exploration and exploitation, and the optima will be found with higher probability. The values of ω and c can be obtained from the relationship between M and ωnin Eq. (10) as follows. From Eq. (9), ξ can be easily obtained:(11)ξ=ln(1/M)2ln(1/M)2+π2The model for the complex eigenvalues can be obtained from Eqs. (6) and (8):(12)|z1,2|=1+ω−((cp+cg)/2)±j4ω−((cp+cg)/2)−ω−122=ω=e−ξωnSo, the relationship between ω, cp, cg, ωn, and ξ can be expressed as:(13)ω=e−2ξωn(14)180±arctan4ω−((cp+cg)/2)−ω−121+ω−((cp+cg)/2)×π180(rad)=ωn1−ξ2The acceleration factors cpand cgare used to determine the weight of the cognitive component and the social component, respectively. Parameter cppulls the particles toward their best local position, whereas cgpulls them toward the best global position. A relatively high value of the cognitive component, compared with the social component, causes individuals to roam through a wide search space. On the other hand, a relatively high social component value prematurely leads particles to a local optimum. Both acceleration factors are set at a value of 2 in classic PSO. In the literature, parameter selection for the PSO algorithm favors cp=cg=c, so in this special case Eq. (14) becomes(15)180±arctan4ω−(c−ω−1)21+ω−c×π180(rad)=ωn1−ξ2The solution process can be summarized as follows:Step 1.Choose an M value in the range of 0.5–0.8, according to control requirements.Calculate the damping ratio ξ using Eq. (11), and then determine the natural frequency of damped oscillation ωnusing Eq. (10).Calculate the inertia weight ω and acceleration factor c according to Eqs. (13) and (15), respectively.In our analysis, the corresponding values of ω and c were calculated according to Eq. (10) for different values of M. Eight different values of M were selected arbitrarily, and the detailed data are listed in Table 1. In the next section, we compare the performance of our proposed control strategy with that of other reported selection strategies.Jiang et al. [6] analyzed the convergence of the standard particle swarm system and concluded that its convergence is based on probability. The authors offered sufficient conditions, shown in Theorem 1, that ensure convergence.Theorem 1Given ω, cp, cg≥0, if 0≤ω<1, cp+cg>0,0<f(1)<(cg2(1+ω))/6, wheref(1)=−(cp+cg)ω2+16cp2+16cg2+12cpcgω+cp+cg−13cp2−13cg2−12cpcg, are all satisfied together, the standard particle swarm system determined by parameter tuple (ω, cp, cg) will converge in the mean square to the global optimalP⇀g(i.e.,limt→∞E|x⇀i(t)−P⇀g|2=0, wherex⇀i(t)is the position of the ith particle in the swarm at time t).The conditions described in Theorem 1 are rewritten in detail as follows when cp=cg=c,(16)(i)2.017<c<40≤ω<1,0<c<2(1+ω)(17)(ii)1.608<c<2.0175c−25c2−336c+57624<ω<5c+25c2−336c+576240≤ω<1,0<c<2(1+ω)(18)(iii)0<c<1.6085c−25c2−336c+57624<ω<5c+25c2−336c+57624ω>c+c2−24c+366orω<c−c2−24c+3660≤ω<1,0<c<2(1+ω)The convergence ranges of ω are listed in Table 1 (Set 1–Set 8). The calculated values of ω according to Eq. (10) all fall within the convergence range: that is, our proposed parameter selection strategy for PSO follows Jiang et al.’s Theorem 1[5] and will converge (in the mean square metric) to the global optimum.To explain how the new parameter selection strategy compares with other strategies, the sets of parameters proposed in different studies are also listed in Table 1, e.g., ω=0.729, c=1.494 [3]; ω=0.6, c=1.7 [4]; and ω=0.715, c=1.7 [6]. The performance of the PSO algorithm using those parameter sets is promising, and their resulting values of M and ωnalmost satisfy the new parameter strategy described in Eq. (10). In the context of the convergence properties of expectation and the variance sequence of a particle's position, Jiang et al.[5] mention that the most important factor in balancing exploration and exploitation is λmaxD, which can be calculated as follows:Based on the iteration equation, the characteristic equation for the variance sequence {Dxt} is(19)λ3−(ψ2+R−ω)λ2+ω(ψ2−R−ω)λ−ω3=0where ψ=1+ω−((cp+cg)/2),R=(1/12)cp2+cg2, given that λD1, λD2, and λD3 are the three characteristic roots andλmaxD=max||λD1||,||λD2||,||λD3||<1. The larger λmaxDis, the stronger the exploration ability of the PSO algorithm will be. In this way, promoting the exploration ability of particles is a direct way to drive particles out of their local minima. As a result, in order to search the solution space more thoroughly, it is necessary to determine parameters as λmaxD≈1. In response to this, Jiang et al. proposed a set of parameters: ω=0.715 and c=1.7 (corresponding to λmaxD=0.995).The values of λmaxDcorresponding to CSPSO are listed in Table 1. It shows that all the values of λmaxDapproach 1 and larger than that of Ref. [4]. That is, according to the suggested value of λmaxDproposed by Jiang et al. [5], the CSPSO is able to balance exploration and exploitation more appropriately than the methods proposed in the existing studies we are aware of.Fernández-Martínez and Gonzalo [16] presented a generalized form of the particle swarm optimization algorithm (abbreviated GPSO) and analyzed the effect of stochasticity on the stability of GPSO trajectories. They proposed that the borderline of the GPSO second-order stability region could be calculated as follows:(20)RGPSO=(ω,c)1−2Δt<ω<1,0<c<ch(w,α,Δt)wherech(w,α,Δt)=ch(ω,1,1)=12(1−ω2)7−5ωwhen α=1(cp=cg) and Δt=1.They believed, as a general rule, that the parameters (ω, c) for standard PSO (Δt=1) should be chosen so that the inertia value ω lies in the range of 0.5–0.8, and the value of the total acceleration coefficient satisfies Eq. (20). Therefore, from the ω values listed in Table 1, the acceleration coefficients were calculated based on Eq. (20). Consequently, all the calculated values for parameter c of CSPSO fell into the desired ranges. Overall, the parameter sets obtained via CSPSO fit the limit range of the second-order stability.Chatterjee et al. [20] sought to obtain an improvement in the parameter choice of the PSO algorithm by investigating the dynamics of the leading particle in stagnation. They indicated that the most likely convergence region of the PSO algorithm can be divided into two sub-regions designated S1∪S2 and S3∪S4 as follows:(21)S1={(ω,C1,C2)|−1<ω<0,0<C1+C2≤1+ω}S2=(ω,C1,C2)|0≤ω<1,0<C1+C2≤(1+ω)2−(ω,C1,C2)|ω=0,C1+C2=1S3=(ω,C1,C2)−1<ω<0,1+ω<C1+C2<2(1+ω)S4=(ω,C1,C2)|0≤ω<1,(1−ω)2<C1+C2<2(1+ω)They found that if the parameters fall into S3∪S4, then the direction xi(K)−xi(K+1) was orthogonal to the gradient of the objective function at the stagnation point xi(K) and there was a greater probability of obtaining better results. They indicated that the parameter choice (ω, C1, C2)∈(S3∪S4) would help design both effective and efficient PSO models after a large number of simulations. Therefore, comparing their conclusion with CSPSO here will further verify its effectiveness. As all ω>0 in CSPSO, we calculated the corresponding range of c (C1=C2=c/2) according to S4. The results are shown in Table 1. According to CSPSO described in Eq. (10), all the corresponding values of ω and c satisfy the parameter choice (ω, C1, C2)∈S4 proposed by Chatterjee et al. when M∈(0.65, 0.8).In addition to the detailed comparison above, we also calculated M×ωnfor the parameter sets described in previous reports (Refs. [3,4,6]), which are approximately equal to 1. The effectiveness of these three parameter sets has been proven, which further validates our proposed evaluation index of PSO.Table 2shows the 15 benchmark functions we employed for performance evaluation, which consists of the brief descriptions of their formulas, their feasible search range RG, the fitness value of their global minimum Fmin, and their accuracy level ɛ. The employed benchmarks are divided into three classes, namely, (1) conventional problems, (2) rotated problems, and (3) shifted problems.These problems consist of different characteristics that allow us to examine the algorithm's capabilities in various criteria. Some of them are used in several established works [1,3,4] and also in follow-up work [21–24] on PSO by different researchers. For example, function F1 is used to test the algorithm's convergence speed because it can be solved easily. Meanwhile, functions F2–F6 are multimodal functions that consist of huge number of local optima in high dimensional case. These functions are used to evaluate the algorithm's ability to escape from the local optima. The rotated problems (F7–F10) are developed by multiplying the original Xivariable with an orthogonal matrix M to produce a rotated variable Zi, that is Zi=M×Xi. For shifted problems (F11–F15), a vector o=[o1, o2, …, oD] is defined to adjust the global optimum of a traditional problem to the new location, that is Zi=Xi−o.We employ eight different kinds of parameter combination based on CSPSO, which we denote Set 1–Set 8, to compare with Refs. [3,4,6]. All the PSO algorithms are set with the same population size of 50, and the termination criterion allowed in each run is a maximum number of 2.5E+05 function evaluations (FEs). The calculations are also stopped if the exact solution is found. We perform the evaluation with the same dimension (D) of 30 and each algorithm is tested 30 times independently on each function to reduce statistical error.We measure the performance of PSO on the basis of three criteria, that is, searching accuracy, searching reliability and efficiency through the mean fitness value (Fmean), success rate (SR), and function evaluation (FE), respectively.The Wilcoxon test (h) is employed to rigorously compare CSPSO with other parameter selection strategies of PSO. The test is conducted at the 5% significance level (i.e., α=0.05), and the values of h are reported in Table 3. The h values indicate if the performance of CSPSO and other parameter selection strategies of Refs. [3,6] are better than (i.e., h=“+”), insignificantly different from (i.e., h=“=”), or worse than (i.e., h=”−”) the performance of Ref. [4] at the 5% significance level. Results of the mean value (Fmean), standard deviation (SD), the best value ever reached (Best), and the t-test achieved by all the involved parameter selection strategies are listed in Tables 3 and 4, where the best results are marked in boldface. We summarize Fmean and h comparison results among CSPSO and other parameter selection strategies of PSO as “+/=/−” in the last row of the table. “+/=/−” gives the number of functions that CSPSO and other parameter selection strategies of PSO perform significantly better, almost the same as, and significantly worse than Ref. [4], respectively.As reported in Table 3, Set 1–Set 8 have the similar performance for all test functions with those of Refs. [3,4,6]. To be specific, Set 1–Set 8 achieve better results than Ref. [6]. For most test functions (F2–F4, F6–F9 and F11–14), the mean values produced by CSPSO (Set 1–Set 8) obtain the same performance with those of Refs. [3,4,6]. Set 1–Set 8 achieve the same accuracy of the mean values with Refs. [3,4]. But, Ref. [6] performs significantly better on F5 (Ackley), F10 (Rotated Ackley) and F15 (Shifted Ackley). This observation is further validated by the t-test results using the h values reported in Table 3, which is largely in line with the mean values. It is apparent that CSPSO (Set 1–Set 8) has the same searching accuracy as Refs. [3,4,6] in most cases when comparing on traditional, rotated, and shifted problems except for F5 (Ackley), F10 (Rotated Ackley) and F15 (Shifted Ackley).From the last row of Table 3 (the row with “+/=/−”), we observe that the solutions generated by CSPSO (Set 1–Set 8) are better than or similar with those of other three references except for Ref. [6] on F5, F10, and F15. It shows that, on one hand, CSPSO via Eq. (10) is valid for most test functions, but on the other hand, it is necessary to readjust M and ωnwhen using CSPSO on some special problems.In order to investigate the reliability and the search speed of control strategy PSO, we compared it with Refs. [3,4,6] in terms of success rate (SR, the percentage of the successful runs in which acceptable solutions are obtained) and the mean FEs required to get acceptable solutions in the successful runs. The results are reported in Table 4. We observed that Set 1–Set 8 achieve 100% success rates on F1–F4, F7, F11, and F12. In comparison with Refs. [3,4,6], one or two of Set 1–Set 5 yield slightly worse or similar solutions on F6 (Non-continuous Rastrigin), F8 (Rotated Griewank), F9 (Rotated Non-continuous Rastrigin) and F14 (Shifted Non-continuous Rastrigin). All the parameter selections including Set 1–Set 8 and the three cited references failed to achieve 100% success rates on F10 (Rotated Ackley) and F13 (Shifted Rastrigin). The convergence rate of Ref. [4] is faster than others but its success rate is low on complex problems. The success rate of Ref. [6] is higher than others especially on F5 (Ackley), F10 (Rotated Ackley) and F15 (Shifted Ackley), but the optimization time is often the longest. FE analysis, as reported in Table 4, reveals that a smaller maximum overshoot leads to a faster convergence and a lower convergence rate. It is consistent with the engineering practice and the control theory.Both the results of Tables 3 and 4 manifest the validity of the control strategy PSO. Although it demonstrates a similar performance of Set 1–Set 8 with Refs. [3,4,6] in solving most test functions, it still captured our attention that Ref. [6] showed remarkable performance (SR=100%) on F5 (Ackley) and F15 (Shifted Ackley) while Set 1–Set 8, Ref. [3], and Ref. [4] achieve low success rates. Given that Ackley's function is an exponential function with a cosine wave of moderate amplitude, which has a narrow optimum basin and hides the global optimum in a bad fitness area, it is hard to avoid falling into the trap of local optima. Due to its peculiarity, we readjust the maximum overshoot M and the natural frequency of damped oscillation ωnaccording to CSPSO in Section 5.3.It is efficient to prevent premature convergence by increasing the maximum overshoot according to CSPSO. In order to improve the convergence precision and get a higher convergence rate, we increase the maximum overshoot M and accordingly enhance the natural frequency of damped oscillation ωn. Five sets of parameter selection are then presented as Set 9–Set 13 shown in Table 5.Table 5 shows that all the values of λmaxDcorresponding to Set 9–Set 13 approximately equal to 1. That means Set 9–Set 13 are able to balance exploration and exploitation according to the suggested value of λmaxDproposed by Jiang et al. [5]. Meanwhile, the values of c according to Set 9–Set 13 also coincide with which proposed in Ref. [20].To ensure fair assessment between Set 9–Set 13 and Set 1–Set 8, all the simulation experiments were carried out under the same conditions defined in Section 5.2. Boldface text in Tables 6 and 7indicates the best results on test function F1–F15. In considering the previous simulation results shown in Tables 4 and 5 that Ref. [6] has the best search accuracy (1.87E−14) and convergence rate (SR=100%) on F5 (Ackley) and F15 (Shifted Ackley), we calculated the t-test and the values of h shown in Table 6. The h values indicate if the performance of Set 9–Set 13, Refs. [3,4] are better than (i.e., h=“+”), insignificantly different from (i.e., h=“=”), or worse than (i.e., h =”−”) the performance of Ref. [6] at the 5% significance level.Simulation results reveal that Set 9–Set 13 achieve the same search accuracy as Ref. [6] on all test functions. It is observed that Set 9–Set 13 generate smaller local optimal values than Ref. [6] especially on F5 (Ackley) and F15 (Shifted Ackley). That means Set 9–Set 13 are able to search the corresponding solution regions thoroughly and successfully avoid premature convergence. As shown in Table 7, Set 9–Set 13 also perform well in terms of SR and FEs. The convergence rate (SR) on F5 (Ackley) and F15 (Shifted Ackley) greatly improved to 100%. The convergence rate (SR) on F10 (Rotated Ackley) is also better than that of Set 1–Set 8.In this section, we applied control strategy PSO to antenna array pattern synthesis. The purpose of antenna array synthesis is to find an appropriate excitation vector and layout of the array that produces the radiation pattern closest to the desired pattern.The array factor AF(θ) of a linear and zero-phased symmetrical array of 2M elements can be written as [25](22)AF(θ)=∑i=12M2ωicos(2i−1)λπdsinθwhere θ represents the angular shift with respect to the normal direction, ωidenotes the excitation amplitude of the array elements, λ is the wavelength, and d is the equal inter-element spacing distance.The objective function in array synthesis is usually defined as the weighted sum of the side-lobe level and the null depth level, which are two important performance indices in antenna array pattern synthesis. The objective function is usually defined as(23)f=α|MSLL−SLLd|+β∑θNull|NULL−NULLd|MSLL=maxθ∈S{AF(θ)}where α and β are weight coefficients, MSLL is the maximum side-lobe level, SLLdis the desired side lobe level, θNull is the desired null position, NULL denotes the null depth level at the desired null position, NULLdis the desired null depth level at the desired null position, and S is the side lobe region excluding the main beam.Generally, the current distribution is more sensitive to the null depth level than side-lobe level. To speed up the search function, priority is given to the objective function with lower sensitivity when determining the optimization goal. In order to reduce the side-lobes, steer nulls and shape beams, we choose the set of parameters α=0.9, β=0.1, according to the test results.To validate the effectiveness of CSPSO, we examined a 20-element half-wavelength-spaced linear array. The array was designed with lower side-lobe level (SLL) suppression in the region [−100°, 100°], with the maximum side lobe level reduced by −35dB relative to the main beam width 20°, with predicted nulls at 10°, 20° and 30° of NULLd−65dB, −80dB, −70dB respectively.In order to evaluate the effect of CSPSO in antenna array pattern synthesis, we compared the performance of CSPSO (Set 2) with four other fixed-value methods (Refs. [3,4,6,20]). The detailed parameter settings are shown in Table 8.The simulation results of linear antenna array synthesis using five different parameter strategies of PSO are presented in Fig. 1.The maximum side-lobe level (MSLL), the null depth level (NULL) at 10°, 20°, and 30°, and the iteration number for each algorithm with the corresponding operation time measured in seconds are shown in Table 9.Table 9 shows all the parameter selection strategies are able to meet the target. Control strategy PSO (Set 2) requires fewer runs than the others. The null depth levels of the pattern at 20° and 30° obtained by CSPSO (Set 2) are much lower than those obtained by other three (Refs. [3,6,20]) fixed-value methods. The maximum side-lobe level of the pattern obtained by CSPSO (Set 2) is lower than the other fixed-value methods(Refs. [3,6,20]), and only a little higher than Ref. [4]. It shows that CSPSO has significant advantages in the evolutionary rate, accuracy, and processing of complex functions.

@&#CONCLUSIONS@&#
