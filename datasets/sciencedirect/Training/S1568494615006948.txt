@&#MAIN-TITLE@&#
An influence power-based clustering approach with PageRank-like model

@&#HIGHLIGHTS@&#
Influence power is introduced to measure points’ relation in and between clusters.PageRank-like algorithm is adopted to estimate influence power evolving over time.Clustering is performed in a tree-growing fashion based on influence power.The proposed method outperforms on seven complex and non-isotropic datasets.Alzheimers disease and race walking recognition datasets validates the applicability.

@&#KEYPHRASES@&#
Clustering,Influence power,PageRank,Pattern recognition,

@&#ABSTRACT@&#
In this paper, we present a clustering method called clustering by sorting influence power, which incorporates the concept of influence power as measurement among points. In our method, clustering is performed in an efficient tree-growing fashion exploiting both the hypothetical influence powers of data points and the distances among data points. Since influence powers among data points evolve over time, we adopt a PageRank-like algorithm to calculate them iteratively to avoid the issue of improper initial exemplar preference. The experimental results show that our proposed method outperforms four well-known clustering methods across seven complex and non-isotropic datasets. Moreover, our simple clustering method can be easily applied to several practical clustering problems. We evaluate the effectiveness of our algorithm on two real-world datasets, i.e. an open dataset of Alzheimers disease protein–protein interaction network and a dataset for race walking recognition collected by ourselves, and we find our method outperforms other methods reported in the literature.

@&#INTRODUCTION@&#
Data clustering is a decisive aspect of pattern recognition and machine learning, and generally used in data mining, image segmentation, bioinformatics, etc. [37]. It is difficult to determine the optimal number of clusters, and most of the clustering problems are shown to be NP-hard [24]. Although many heuristic methods have been developed, most of them are not generic enough and can only be used on particular datasets.The widely used clustering methods are K-means [33], probabilistic mixture model [9] and their derivative algorithms, because of their simplicity and fast speed. Yet these methods often suffers from the issues of identifying the initial partitions and the number of clusters. Moreover, the use of centroid to represent a cluster inherently limits K-means family to compact and isotropic clusters, while probabilistic mixture model family suffers from the issues of local optimum and slow convergence rate. As such, their ability for dealing with complex cluster structure or large scale of dataset is limited. For instance, when the sample space is not convex, these algorithms may be trapped in a local optimum [36]. The family of spectral methods has been proposed to solve these issues [35], such as affinity propagation [13,52] that can automatically find the number of clusters using eigenvectors of an affinity matrix derived from the data. However, they may not work well on non-globular clusters and are often time intensive. Besides, it may fail to work properly with an improper initial exemplar preference. This is because both K-means and spectral clustering families are to minimize the squared error from all the data points to their nearest cluster centroids [24,13]. Fuzzy clustering method, adopting a membership function [4], allows one data point belongs to more than one cluster, but it suffers from the same problem of K-means: the clustering initialization and the local minimum. The above algorithms are sensitive to initial conditions, that is to say, they may get different local maximums under different initial conditions.Normally, the distance between two data points is measured statically by a distance function, e.g. Euclidean distance or Chebyshev distance. The distance cannot be changed once it is calculated. A clustering algorithm may divide data points from the same cluster into several clusters when the cluster centroid is not obvious. Such measurement makes the algorithm easy to fall into local optimal solutions. For instance, in Fig. 1, most existing clustering methods divide the datasets into three clusters by the misleading information induced from the ringlike cluster whose centroid is hard to define. This is because the distance d1 is much longer than d2 for the point A. As a result, the point C2 is chosen to be in the same cluster as A. After several iterations, such misleading information will spread across clusters.To address such issue, we consider that the distance between two data points can change if their environment changes. To this end, we adopt the concept of influence power to help us score each point's significance in clusters. As shown in Fig. 1, the data point B has the influence power d3 on the data point A. When the influence of B on A changes, the influence of C2 on A will change accordingly, and the influence of C1 on A will change as well. If C1 has more influence on B, the influence of C1 on B will increase. Notice that here d does not mean a static distance. It represents a dynamic influence power. The values of influence powers among data points evolve until a balance state is reached. It seems that a boundary point may have smaller influence power than a centroid if it is obvious in a cluster. In some slim or non-isotropic shapes whose centroids are not obvious, such as the ringlike shape, points may get similar influence powers.In recent years, the influence power model has attracted a lot of attention in social network computing. Li [28] applied the influence power model to the excavations in social networks. Yuan et al. [50] proposed users influence index model based on Sina11Sina is the name of a website, its available URL is http://www.sina.com.cn/.micro-blog platform and found that the model can reduce the false fans interference and reflect the users real influence ability. Liang et al. [30] improved the fuzzy clustering algorithm and obtained good results by adding a influence factor. Zhang [51] applied the influence power model to reveal the hidden relationships in management science.In this paper, we propose a novel influence power-based clustering method called clustering by sorting influence power (CSIP for short). First, we define the influence power model in clustering, and use a PageRank-like algorithm to measure the influence powers among data points. Then we present the CSIP algorithm to cluster data points according to the influence powers. It is able to determine the number of clusters by sorting influence powers. To evaluate our influence power-based approach for clustering, we first compare our influence power-based approach for clustering on seven open datasets with other four well-known clustering approaches. Also, we apply our approach to the protein–protein interaction network of human Alzheimer's disease and the discrimination of race walking from normal walking and running. Experimental results show that our approach is promising, and can effectively identify a variety of complex situations, such as clusters of different shapes, densities, sizes, and noises.The rest of this paper is organized as follows. Section 2 describes the influence power model. Section 3 introduces a PageRank-like algorithm for calculating influence powers. Section 4 presents the CSIP algorithm and discusses parameter selections for the CSIP. Section 5 shows the experiments and the results. Section 6 discusses existing clustering approaches related to our work. Section 7 concludes this paper and gives future research directions.Influence power (IP for short) is the ability that a point influences other points in a dataset, or the obedience degrees of other points to it. In this paper, we defineIPjias the influence power of point i on point j, where0≤IPji≤1.The influence power measures a point's degree of influence.IPji=1indicates that j accepts the whole influence i exerts on it. In contrast,IPji=0indicates that i has no influence on j. If0<IPji<1, it means that j partially accepts the influence from i.The cumulative influence power of point i, denoted as IPi, is the sum of the influence power of i to all other points in a dataset, as follows:(1)IPi=∑j≠iIPjiIPiis the cumulative influence power of point i. The higher the value, the greater the influence. A point's influence power is due to its neighbors’ influence powers exerting on it. Because of interactions among different points, the influence power of each point changes over time. More generally, we can specify a neighborhood weightwijto control the significance of the influence power of point j exerting on point i, i.e.IPi=∑j≠iwijIPji.The influence power of point i during (t+1)-period is equal to its influence power during t-period plus the changing value:(2)IPi(t+1)=IPi(t)+ΔIPi(t)where IPi(t) represents the influence power of point i during t-period, and ΔIPi(t) is the variation of the influence power of i during t-period.The cumulative influence power of point i during (t+1)-period can be updated as follows:(3)IPi(t+1)=∑j≠iwijIPji(t+1)=∑j≠iwij(IPji(t)+ΔIPji(t))whereΔIPji(t)represents the variation of the influence power of point i on j during t-period. The cumulative influence power of i during (t+1)-period changes if the influence power i exerts on any other point changes during t-period.The above formulae describe the factors that affect the value of influence power. However, the incremental influence powerΔIPji(t)depends on the influence power of j on other points. In order to facilitate the calculation, the influence power of point i on point j is concerned in connection with the distance between i and j, e.g. Euclidean distance. The Euclidean distance is used for all datasets in our experiment. For high dimensional datasets, more proper measurements such as the cosine distance or Chebyshev distance [37] can be used instead. In our model, different distance measurements can be used for estimating the distance between two points. We also use the techniques for dimension reduction (e.g. hubness-based method [42]) in order to avoid the high dimensionality of data space that can lead to the curse of dimensionality in clustering. More details are described in our experiment (see Section 5.3.3).We use PageRank-like algorithm to calculate the influence power of data points. The relationship between points in the influence power model is similar to the links between pages in the classic PageRank algorithm. Therefore, we utilize the traditional PageRank algorithm to calculate the cumulative influence of each point.The PageRank algorithm [6] is originally designed to determine the importance of web pages. It is successfully used in Google search engine. The basic idea behind PageRank algorithm is “a node has a high rank if the sum of the ranks of its backlinks is high. This covers both the case when a node has many backlinks and when a node has a few highly ranked backlinks” [38]. The concept is to estimate the importance of nodes based on hyperlink structure analysis. A node mostly referred by other important nodes is more important. Since we intend to rank data points to determine clusters by influence power, the PageRank algorithm can be easily and naturally wielded in scoring influence power.For a set of n data points p1, p2, …, pn, letIP→be an n-dimensional column vector representing the influence powers of those points, i.e.IP→=[IP1,IP2,…,IPn]T.Let A be an (n×n)-adjacency matrix that represents the neighborhood weights among the data points. Each entrywijin A is defined as(4)wij=zj·1Dist(i,j)·ℓjDist(i,j)<δ0otherwisewhere Dist(i, j) is the distance between piand pj, and δ is a predefined threshold. LetNjbe the set of points whose distances to point pjare less than δ. ℓjis the size of the setNj. For each j, zjis a normalization constant such that∑i=1nwij=1. That is to say, the elements of each column in A sum up to 1, so the matrix is a stochastic matrix. In graphical view, a link exists between piand pjif and only ifpi∈Nj.wijis the weight on the link.We define that the influence power of piduring (t+1)-period can be updated as follows(5)IPi(t+1)=∑j≠iwijIPj(t)=∑pj∈NizjIPj(t)Dist(i,j)·ℓjA data point pjdivides and exerts its influence power on any of its neighbors. Point piderives a proportion of influence power from point pjthat is due to both the number of neighbors pjhas and the distance between piand pj. If a point contains a large number of neighbors, each of its neighbors can obtain a small proportion of its influence power only. In addition, the far a point is, the smaller influence its neighbor gets from it.Since a large number of zero weights may often exist in the adjacency matrix, especially when the dataset is high-dimensional, we can add artificial links with uniform probability from fully connected points to solve the computation problems against correctness and convergence. Let E be an (n×n)-matrix in which all entries are 1/n. Then, the modified adjacency matrix A′ is(6)A′=d×A+(1−d)Ei.e. for eachwijin A,w′ij=d·wij+(1−d)(1/n). The coefficient d∈[0, 1], called damping factor, determines the probability that the iteration at any period will continue. Any point's influence power is derived in large part from the influence powers of other points. The damping factor adjusts the derived value downward. In practice, it is usually set to 0.85 [6].The influence power values are the entries of the dominant left eigenvectorIP→of the modified adjacency matrix. Combining the Eqs. (5) and (6), the influence power of piduring (t+1)-period using the modified can be updated as follows(7)IPi(t+1)=d∑j≠iwijIPj(t)+(1−d)1n=d∑pj∈NizjIPj(t)Dist(i,j)·ℓj+(1−d)1nThe influence power vector can be updated as follows(8)IP→(t+1)=d×A×IP→(t)+(1−d)1nn×1As shown in Algorithm 1, we use the Eq. (8) to update influence power eigenvector in each iteration. We assume each point has equal initial influence, i.e.IP→(0)=[1/n]n×1. An alternative way is to take the potential value among points as the initial IP values. The influence power vector is repeatedly calculated until its residual error is less than a predefined stopping threshold ɛ. Notice that·represents the magnitude of a vector. In addition, the calculated IP values in each iteration can be normalized to keepIP→=1.Algorithm 1The PageRank-like algorithm for influence power measurement.Input:A – adjacency matrix, d – damping factor, ɛ – stopping threshold;Output:IP→– influence power vector;PR(A, d, ɛ)1:IP→(0)=1nn×1,t=0;2:repeat3:IP→(t+1)=d×A×IP→(t)+(1−d)1nn×1;4:t=t+1;5:untilIP→(t+1)−IP→(t)<ɛ;6:returnIP→(t+1);Example 3.1Consider a dataset containing 6 data points: P1, P2, P3, P4, P5 and P6 located at (1.0, 2.0), (1.5, 2.5), (3.5, 3.0), (4.0, 1.5), (5.5, 2.0), and (6.0, 1.5), respectively. We set the δ=2.5, d=0.85 in this case. Then, we obtain the adjacency matrixA=00.74500001.000.3100000.25500.3580.1790000.40400.2540.261000.2860.35800.7390000.2830.5670.Initially, each point's IP value is set to16, i.e.IP→(0)=16,16,16,16,16,16T. All the IP values are updated by Algorithm 1 iteratively, e.g.IP→(1)=0.124,0.218,0.132,0.153,0.23,0.141T.Several benefits are associated with the PageRank-like implementation of the influence power model, although other methods can also be used for the evolving calculation, e.g. evolutionary algorithm, neural network, etc. First, the implementation of the PageRank-like algorithm is simple. Also, the definition of neighborhood weights can be changed without re-implementation of the algorithm (e.g. in Eq. (4),wij=(1/ℓj)). Second, the PageRank algorithm has been proved fast convergence theoretically [27] and practically [38]. A rough estimate of the number of iterations needed to converge to a stopping threshold is log10ɛ/log10d (when n→∞), which is irrelevant to either the size of dataset or the dimension of data points. For d=0.85 and ɛ=10−8, the algorithm can converge in roughly −8/log100.85=114 iterations. Third, the feature space in high-dimensional dataset is usually sparse, making it difficult to distinguish high-density regions from low-density regions [21]. The PageRank-like algorithm can overcome this limitation and handle high-dimensional dataset by adding artificial links with weights controlled by the damping factor. Finally, the PageRank-like algorithm can be efficiently implemented to simple and fast distributed algorithms (e.g. MapReduce) for large datasets that may not fit in memory. The existing variations of the PageRank algorithm (e.g. VisualRank, EigenTrust) and many toolkits supporting these algorithms allow our influence power model to be applied in various scenes.As in webpage ranking field, a webpage with a higher PageRank value can be viewed as an indication that it is linked or voted by more other webpages. Use of such centroid, which is the most powerful, to represent a cluster is the most popular scheme in clustering. It works well when the clusters are compact or isotropic, but often fails when the clusters are elongated or non-isotropic (e.g. ring-like shape) [22]. The use of boundary points in a cluster may capture its shape well. In our case, since a data point's influence power is due to its neighbor points’ influence powers, the IP values of boundary points in a cluster are often low. Also, a data point with a lower IP value may be more likely isolated from a cluster if it is far from all the points in the cluster. Unlike center-based clustering algorithms in which each cluster has to be represented by a centroid, the decision whether a point is in a certain cluster in our method is due to its distances to the boundary points with lower IP values in the cluster.As shown in Algorithm 2, after computation of the influence powers, all the data points are sorted in ascending order according to their IP values. The sorted index is stored in a queue sortedIndex[1…n]. The first data point sortedIndex[1] with the lowest IP value is selected as the root of the first cluster. The root of a cluster is a data point with the lowest IP value in the cluster. For each following data point sortedIndex[i] in the queue, its parent is defined as the point selected from sortedIndex[1…i−1] with the shortest distance to sortedIndex[i]. The distance between the point sortedIndex[i] and its parent is then compared with a given distance bandwidth parameterB. If the distance is greater thanB, the point sortedIndex[i] is assigned to a new cluster; otherwise it is assigned to the same cluster as its parent, i.e. they have the same root. This process continues until all the data points are visited and assigned to clusters.Algorithm 2The CSIP algorithm.Input:D={pi}i=1n,pi∈Rdim– n data points of dim dimensions;B– distance bandwidth, d – damping factor, ɛ – stopping threshold;Output:{C1, C2, ⋯, Ck}-the clustering results;CSIP(D,B, d, ɛ)1:   Dist ← compute the distance matrix of n data points in D;2:   A ← compute the adjacency matrix of n data points in D;3:IP→← CALL PR(A, d, ɛ);4:   sortedIndex ← the sorted indices of data points according to their IP values in ascending order;5:   root[sortedIndex[1]]=sortedIndex[1];6:   fori=2 tondo7:p=sortedIndex[i];8:parent=sortedIndex[1];9:minDist=Distp,parent;10:forj=2 toi−1 do11:ifDistp,sortedIndexj]]<minDistthen12:parent=sortedIndexj;13:minDist=Distp,parent;14:end if15:end for16:ifminDist≤Bthen17:rootp=rootparent;18:else19:rootp=p;20:end if21:   end for22:   {C1, C2, ⋯, Ck} ← group clusters according to root[1…n];23:   return {C1, C2, ⋯, Ck};When the algorithm finishes, the array root[1…n] contains the roots of all the n data points. The points in the same cluster have the same root in the array. For example, point i belongs to the cluster rooted at point root[i].Example 4.2Continue the Example 3.1, Fig. 2shows a simple illustration for the algorithm. The distance bandwidthBis set to 2.1.After sorting by the computed IP values, the order of the points is given as [P1, P3, P4, P2, P6, P5]. The first point P1 with the lowest IP value becomes the first cluster root. Then P3 is selected from the sorted queue. Because the distance between P1 and P3 is 2.69, which exceeds the required bandwidthB, P3 becomes the root of a new cluster. Next, P4 is selected from the queue and finds P3 as its parent. Then P2 is taken from the queue. Although any of the distances from it to P1 and P3 are withinB, the nearest one P1 is chosen as its parent. P6 is not within the bandwidth of P3, but P4 whose root is P3. Similarly, P5 selects P6 as its parent because their distance is the shortest. Finally, all the data points are grouped into two clusters: P1, P2 and P3, P4, P5, P6 rooted at P1 and P3, respectively.The distance bandwidthBneeds to be determined in the CSIP algorithm. A large or a small distance bandwidth may result in trivial clustering that is either the one-clustering that consists of only one cluster or the singleton clustering in which every data point forms its own cluster. Apparently, the number of clusters may decrease with the increment of the bandwidth. A simple way for choosingBcan be derived from the distance matrix directly. In order to avoid the trivial clustering, each point shall have at least one neighbor point within the bandwidth. So, the following are proposed for determiningB:(9)MinD(i)=minj≠i(Dist(i,j))(10)B=maxi=1…n(MinD(i))The Eq. (9) avoids a selection of a large distance bandwidth that may lead to one-clustering issue. The Eq. (9) saves the algorithm from the singleton clustering.An alternative is to use an objective function to automatically tune the parameter. Since data points are not labeled in clustering scenarios, a well-known index, the Davies–Bouldin index (DBI) [34] can be taken as the objective function for parameter estimation. It is defined as(11)DBI=1k∑i=1kmaxj≠iSi+SjDist(ci,cj)whereSi=1Ci∑p∈CiDist(p,ci)dim1/dimwhereCiis the cardinality of Ciand ciis the centroid of Ci. Since no centroid is defined in the CSIP, we view the data point with the highest influence power in the cluster is the centroid. Usually a lower DBI value means the ratio of the low intra-cluster scatter and the high inter-cluster separation, which indicates a better clustering result. Hence, the objective of clustering is to minimize the DBI for estimating a proper value ofB.Algorithm 3The parameter tuning algorithm.Input:θ – DBI threshold,ΔB– iterative increment;Output:B– distance bandwidth;1:B=maxi(MinD(i)), minDBI=+∞;2:     repeat3:   {C1, C2, ⋯, Ck} ← call CSIP usingB;4:   currentDBI ← calculate DBI by the Eq.((11));5:   ifcurrentDBI<minDBIthen6:     minDBI=currentDBI;7:best_B=B;8:   end if9:B=B+ΔB;10:     untilminDBI<θorB>maxi,j(Dist(i,j));11:     returnbest_B;As shown in Algorithm 3, the initial value ofBstarts frommaxi(MinD(i)). This is because it is the minimum value that can avoid the trivial clustering. The DBI value is calculated after clustering by the CSIP using current distance bandwidth as input in each iteration. The algorithm terminates until the required DBI reaches orBexceeds the longest distance among data points. The increment ofBin each iteration can be set toΔB=mini,j(Dist(i,j)). In this way all the possible distance bandwidths that may make DBI change can be tested. In order to accelerate the speed of this algorithm, especially for large datasets, we can also setΔB=maxi,j(Dist(i,j))−maxi(MinD(i))n. The DBI value will not change after n iterations because all the data points are within the distance bandwidth whenB≥maxi,j(Dist(i,j)); that is, only one cluster is created.Notice that only the distance between two points is used as the partition metric for clustering in the CSIP. The difference of IP values between two points is not concerned in this algorithm. However, in some scenarios the data points in the same cluster may have similar influence powers but not distance, e.g. slim ring-like cluster. The utilization of influence power gap will be considered as a supplementary for clustering in the future work. Another parameter δ in the Eq. (4) is often set to the same value asB.To validate the CSIP algorithm, we choose eight datasets that contain clusters with different shapes, densities, sizes, and noises. We also compare the clustering performance of the CSIP with four commonly used clustering methods, i.e. a centroid-based clustering (K-means [33]), a fuzzy clustering (fuzzy c-means [4], FCM for short), a probabilistic mixture model-based clustering (Gaussian Mixture Model with expectation-maximization [9], GM_EM for short) and a graph-based clustering (K-affinity propagation [52], K-AP for short).The CSIP algorithm is implemented in Matlab R2010a. The implementation of other clustering methods are collected from Matlab toolbox and Matlab Central website22http://www.mathworks.com/matlabcentral/.. All the experiments are conducted in a single PC with an Inter(R) Pentium(R) CPU G620 @2.60GHz and 4GB RAM. The number of clusters, the runtime, and the evaluation metrics are reported in Tables 4–10. Because K-means, FCM and GM_EM are all non-deterministic algorithms, they are executed 100 times for each dataset. The maximum and the average values of the evaluation metrics performed by K-means, FCM and GM_EM are shown in these tables. The deterministic algorithms, K-AP and CSIP, are tested once only. For CSIP, we choose a distance bandwidth for each dataset that results in the best DBI using Algorithm 3. In this way CSIP is able to find the number of clusters accordingly. The number of clusters required as a necessary input parameter by other algorithms is set to the actual number of clusters in each dataset if available.Seven synthetic datasets are used in our experiments as benchmarks. The Aggregation dataset consists of seven perceptually distinct groups of points and the total number of these points is 788. In fact, the dataset contains features that are known to create difficulties for the selected algorithms, e.g. narrow bridges between clusters, uneven-sized clusters, etc. It was also used by [16]. The Circular-Gaussianblob dataset consists of 130 points, including one circular plus three Gaussian blobs [17]. The Three-Dimensional dataset is selected, which contains eight clusters with 225 points for each. The Spiral dataset consists of three spirals with 312 points in total [8]. The Flame dataset contains of two complex clusters with 240 points in total [14]. The Uoo is composed of one U-shape cluster and two O-shape clusters with a total number of 266 points. The Uniformly-Points dataset is used for representing a special case of two clusters containing 441 uniformly distributed points, one regularly grid positioned in a rectangle (−10≤x≤2, −10≤y≤10) and another (4≤x≤10, −10≤y≤10). The characters of these seven synthetic datasets are shown in Table 1.We also compare the CSIP with four specific clustering approaches in the application of Alzheimers disease protein–protein interaction to discover the interaction between proteins. Alzheimers disease is a typical degenerative diseases of central nervous system. The dataset collected from IntAct [19] contains protein–protein interaction data of the degenerative diseases of central nervous system. Proteins potentially associated with the pathology of Alzheimers disease were gathered into a database, and can be mapped into a protein interaction network [23].As shown in Fig. 3, each node represents a protein. If there is an interaction between two proteins, they are connected with a link. All interactions form a protein–protein interaction network. There are 90 protein nodes in this protein interaction network. The purpose of clustering is to find clusters in which proteins are more likely in the same group. The proteins clustered in the same group perform common functions on Alzheimers disease.Race walking began in 1932, and became a formal sport event of Olympic in 1992. In today's race walking competition, the determination of whether an athlete fouls is mainly affected by a referee's subjective judgment, leading to a high possibility of misjudgment. To this end, we intend to determine whether race walking can be automatically recognized by sensors embedded in mobile devices. In our experiment, acceleration data are collected by a smartphone app developed by ourselves. More details about the system implementation can be found in our previous work [32,48,53]. Unfortunately, a huge challenge for sensor-based supervised human activity recognition task is the collection of annotated or labeled training data, especially for sport activity recognition where actions or movements are often instantaneous. Ground truth annotation is often an expensive and tedious task. As such, it is more practicable to use clustering methods for detecting the fouls for an athlete during race walking competition or for an amateur during race walking practice.In our experiment, we asked four volunteers, one professional athlete and three amateurs, to perform three activities including walking, race walking and running. For each volunteer, each activity is continuously recorded in a 400m long athletic track for each time. The data were recorded by a smartphone with one three-dimensional accelerometer, which was attached on the volunteers’ waist, as shown in Fig. 4. The sampling rate is 30Hz. The data are divided into several segments using a predefined window length of 1s. We use 75%-overlapping sliding windows to produce instances. For each instance, a total number of 36 features are extracted, such as mean, standard deviation, variance, skewness, kurtosis, etc. The competing clustering approaches are compared to group race walking, normal waling and running by these features.To evaluate the clustering algorithms, we choose four evaluation metrics: the Adjusted Rand Index (ARI) [20], the Normalized Mutual Information metric (NMI) [47], the silhouette value [43], and the Q value [46]. Notice that the Q value is a specific metric for Alzheimers disease dataset.Let D be a set of n data points. Given two partitions of D, namelyX={X1,X2,…,XX}withXclusters andY={Y1,Y2,…,BY}withYclusters, the information about A and B can be represented in the form of aX×Ycontingency table (CT) shown in Table 2. nijdenotes the number of data points that are common in clusters Xiand Yi. Every data point in D contributes to the cell of the corresponding clusters in both X and Y. Focusing on the pair wise agreement, the information in the CT can be further condensed in a mismatch matrix shown in Table 3.The elements a, b, c and d represent counts of unique entity pairs. Explicit formulae for calculating them in the mismatch matrix can be constructed using entries in the CT:(12)a=12∑i=1X∑j=1Ynij(nij−1)b=12n2+∑i=1X∑j=1Ynij2−(∑i=1Xni·2+∑j=1Yn·j2)c=12∑j=1Yn·j2−∑i=1X∑j=1Ynij2d=12∑i=1Xni·2−∑i=1X∑j=1Ynij2wherea+b+c+d=n2=n(n−1)2where ni· is the number of points belonging to the cluster i of partition X, and n·jis the number of points belonging to the cluster j of partition Y.The ARI is used to access the global congruence of two typing methods. It gives the overall concordance of two methods taking into account that the agreement between partitions could arise by chance alone.(13)ARI=a+d−nca+b+c+d−ncwherenc=n(n2+1)−(n+1)∑ni·2−(n+1)∑n·j2+∑∑nij2n2(n−1)The maximum value of the ARI is 1, which means that the two clustering results are exactly the same. When the two partitions are picked at random which corresponds to the null model, the ARI is 0.The NMI is based on the notion of entropy that has its origin in information theory. The NMI is a mutual information metric whose range is normalized to0,1. It is also proposed as similarity measurement between partitions X and Y, defined as follows(14)NMIXY=−2∑i=1X∑j=1Ynijlognijnni·n·j∑i=1Xnilognin+∑j=1YnjlognjnA high NMI value indicates that the clustering and true cluster labels match well [10].Both ARI and NMI can be used only in the synthetic datasets where cluster labels are available.The silhouette value ranges from−1,+1for each point is to measure the similarity between points in the same cluster and that in other clusters. The silhouette value Sifor the point i is defined as(15)Si=(βi−αi)max(αi,βi)where αiis the average distance from point i to all the other points within the same cluster, and βiis the minimum average distance from point i to points in another different cluster. As αiis a measure of how dissimilar point i is to its own cluster, a small value means it is well matched. Furthermore, a large βiimplies that point i is badly matched to its neighbor cluster. Thus Siclose to one means that a point is appropriately clustered. If Siis close to negative one, then point i would be more appropriate if it was clustered in its neighbor cluster. An Sinear zero means that the point is on the border of two natural clusters. The average of a cluster is a measure of how tightly all the points in the cluster are grouped. Thus the average of the entire dataset is a measure of how appropriately the points are clustered [43].To evaluate the result of our method solving the problem of the protein–protein interaction of human's Alzheimer's disease associated proteins whose standard labels are unknown, we introduce the Q value.Given a partition of a network, which divides its vertices into k clusters, the modularity is defined as(16)Q=∑i=1k(eiine−(eine)2)where eiiis the number of edges with both vertices within group i, eiis the number of edges with one or both vertices in group i, and ne is the total number of edges.Therefore, the Q value measures the fraction of edges falling within groups, subtracted by what one would expect if the edges were randomly placed. If the number of within-cluster edges is no better than random, we get Q=0. Values approaching Q=1, which is the maximum, indicating networks with strong cluster structure. A larger Q value means stronger cluster structures.We use different combinations of colors (red, green, blue, cyan, magenta and black) and symbols (+, *, ×, ∘, ♦ and •) to represent different clusters in our experimental results. The points with the same color and shape are considered as in the same cluster.Dataset aggregation. From Fig. 5, it can be seen that only the CSIP perfectly identifies all the shapes in this complex dataset, while all the other methods fail to identify the optimal result. Through the analysis of the Table 4, we derive that although the other four clustering approaches can get high ARI and NMI values, the precision is not ideal. The maximum ARI and NMI values produced by the CSIP are 1, which illustrate the effectiveness of our method for clustering the seven complex shapes in this dataset.Fig. 6describes the three stages of each point's IP value in the CSIP, i.e. the initial stage, the stages after sorting and after clustering. It can be seen that these points are distributed randomly at the beginning. From the view of the state after being clustered by the CSIP, we can see that the points with similar IP values are clustered into the same group.The silhouette values show that each cluster is very reasonable except the sixth and the seventh clusters in Fig. 7. The sixth cluster is the crescent shape (upper left, represented in blue color and ∘ symbol in CSIP) and the seventh cluster is the ellipse (lower middle, represented in black color and + symbol in CSIP). This phenomenon can be easily explained using the definition of the silhouette values. As to the sixth cluster, the distance from any end of the crescent to its cluster center is too far. Both ends of the crescent are relatively close to the cluster in its right. To the seventh cluster, it is near to the two clusters in its left. The separation of the sixth cluster or the seventh cluster from their neighbor clusters in distance may result in a small silhouette value. Hence, the silhouette values may be only suitable to evaluate the result on convex spherical sample space. It is not very useful for evaluating complex clustering such as non-convex clusters.Dataset circular-Gaussianblob. From Fig. 8, we can easily find that the algorithms except the CSIP cannot recognize the blob cluster inside the circular in the left and the two blob clusters in the most right. The CSIP can identify any part of this dataset that is consistent with our subjective feeling. As shown in Table 5, the ARI and NMI values are not high in the other four algorithms. Although the max values of GM_EM reach 0.653 and 0.719, respectively, there is still much room for improvement.Fig. 9shows that our clustering result is correct. However, in terms of silhouette value, the second cluster (i.e. the blob cluster inside the circular) might be more appropriate if it is merged with the circular cluster. According to the definition of the silhouette value, the circular should be more likely grouped with the blob, because the center of the circular is located in the blob. As a result, negative values are produced if the two clusters are separated. Thus, we can get that this metric may not be very suitable in this case.Dataset three-dimensional. For the three-dimensional dataset, from Fig. 10and the Table 6, we can get that all method except K-AP achieve good results. K-AP is almost ineffective for this dataset. It costs much longer time for clustering than other algorithms. The K-means and GM_EM may get different degrees of errors. The average values of the ARI and the NMI of K-means is greater than GM_EM. In some runs, GM_EM can achieve the best results, that is, its max values of the ARI and NMI is 1. The FCM and CSIP achieves perfect clustering results for this dataset.As shown in Fig. 11, it can be seen that these points are distributed randomly in this dataset. The points with similar IP values are clustered into the same group. This clearly illustrates that similar influential points are more likely to be clustered in the same cluster. From Fig. 12, we can see that for each point Si=1 (i=1, 2, ⋯, 1800), which means that the data points are appropriately clustered.Dataset Spiral. For the 3-Spiral dataset containing three clusters of the same size, the results are shown in Fig. 13and Table 7. We can see that the clustering results created by K-means, FCM, GM_EM, and K-AP are vastly different from the actual clustering results. The obtained ARI and NMI values are very low. The CSIP can achieve very good results for this dataset. Because the cluster shape is far beyond globular and compact, the silhouette value is not a good quality criterion in this case.Dataset flame. It can be seen from Fig. 14and Table 8that K-means, FCM, GM_EM and K-AP cannot recognize the bottom cluster of the flame, because the centroid is far away from some boundary points that leads to the algorithms cannot identify the boundary of the flame very well. According to the ARI and NMI values, K-means and FCM may produce better results than GM_EM and K-AP sometimes.The CSIP is able to identify the two clusters. Since the bottom cluster of the flame is not isotropic, the silhouette values of the upper boundary points in this cluster are negative, as shown in Fig. 15.Dataset Uoo. From Fig. 16, it can be seen that K-means, FCM and K-AP are difficult to recognize the circular cluster. Although GM_EM produce poor clustering results on occasion, it is the best clustering algorithm for this dataset except the CSIP. GM_EM gets the best result indicated by the highest ARI and NMI indices among the four algorithms, as shown in Table 9. We summarize that GM_EM is not a stable algorithm in terms of the ARI and NMI. The CSIP is superior to other algorithms. It can perfectly identify all the three clusters of this dataset. Its ARI and NMI values are 1.It is noticed from Fig. 17that in terms of silhouette values, the first and second clusters (i.e. the two O-shape clusters) is reasonable, but several points of the third cluster (i.e. U-shape cluster) should be assigned to its neighbor O-shape cluster. This is inconsistent with the actual clustering results. The main reason is that the upper points of the U-shape cluster is close to the centroid of the other two O-shape clusters, which results in a negative silhouette value if these points are not clustered into the two clusters. This also indicates that the silhouette value may not be an appropriate criterion for non-isotropic and elongate clusters.Dataset uniformly-points. For this dataset, the distance bandwidth is set to 1.50 in the CSIP, which gets the best DBI value in our experiments. Fig. 18shows that only the CSIP is able to separate the points of the square in the left from the points of the rectangle in the right. The experiment also shows the limitation of other algorithms in dealing with uniformly distributed data points. They are often easy to fall into local optimum in a local region where the densities are similar everywhere.Table 10shows that the ARI is 1 for the CSIP, which indicates a perfect match with the benchmark. In addition, its NMI value is also 1 that further illustrates the effectiveness of the CSIP. K-means, FCM and GM_EM can get high ARI and NMI values. However, enormous difference exists among different runs. The stability of these algorithms is poor compared with the CSIP.As shown in Fig. 19, we conclude that in terms of the silhouette value, a small number of points in the first cluster (i.e. the left square) should be assigned to the second clusters (i.e. the right rectangle). The silhouette values becomes smaller as the points are closer to the other cluster. The negative value arises because the boundary points between the two clusters are close to each other. The internal contact of the first cluster is not tightly because its scope is wide. The boundary points are more likely to be clustered to the other cluster. The uniformly distributed points that are far from the cluster center make the silhouette value decreasing. The datasets like the uniformly distributed data points do not have a good cohesion strength.This section shows the performances of different clustering algorithms on protein-to-protein interaction network. Unlike synthetic datasets, there are no standard clusters in the Alzheimers disease dataset. In order to evaluate the effectiveness of the CSIP, the Q value is adopted as an evaluation criteria introduced in section 5.2.4. As we know, a larger Q value means stronger group structures. We control the number of clusters by adjusting the ranges ofBto get the best Q value instead of the DBI in Algorithm 3. Table 11shows that the Q value increases when the number of clusters decreases. The Q value reaches 0.9690 as a maximum when the number of clusters is 7.We compare the clustering results with other three reported algorithms in the literature [31], i.e. Korbel, AAVM-K-means and Edge-based algorithms. Table 12shows that in terms of the Q value, the CSIP outperforms others in this real dataset of Alzheimer disease protein–protein network.Since 32 features are extracted in this experiment, we need to reduce the dimension of feature space before clustering to avoid the curse of dimensionality in clustering. Radovanović et al. [42] explored the affection of the distribution of k-occurrences (i.e. the number of times a feature point x appears among the k nearest neighbors of other points in a dataset, denoted by Nk(x)) on high dimensionality, called hubness. According to their experimental results, they claimed that this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent popular nearest neighbors. In our experiment, we selected those feature points x with((Nk(x)−μNk)/(σNk))>2as hubs, that is, Nk(x) more than two standard deviations higher than the mean, whereμNkandσNkare the mean and standard deviation of Nk, respectively. Then, we selected the nhpoints with the lowest k-occurrences, where nhis the number of hubs selected. Finally, we randomly select nhpoints from the remaining points as the selected features.We compare the CSIP with other clustering method, as shown in Table 13. It can be seen that the CSIP performs the best in terms of both ARI and NMI. K-AP performs well on ARI, but gets low value of NMI and requires long runtime. The clustering time cost by the CSIP is acceptable for recognizing race walking in practice. K-Means spends the least time for clustering, but gets the worst ARI. Since the dimension reduction procedure spends the same time for all the competing approaches, we exclude its runtime from our comparison. The time complexity of hubs-based dimension reduction method is analyzed in [42]. Overall, we claim that the CSIP outperforms other competing clustering approaches in race walking recognition.The computation time is a very important indicator of the quality of an algorithm. In this section, we evaluate the CSIP empirically and theoretically. We evaluated the runtime of all the clustering algorithms on the synthetic datasets. As shown in Fig. 20, the speed of CSIP is quite acceptable. In the sense of computation time, it is close to the simple and fast algorithms, i.e. K-means and FCM. It is much faster than the probabilistic model GM_EM and graph-based algorithm K-AP.In order to evaluate the CSIP for processing large-scale datasets, we compare the runtime of all the algorithms when the number of data points increase. Fig. 21illustrates that the runtime of K-AP dramatically increases as the number of data points grows. The CSIP performs well on small datasets. The CSIP performs much better than K-AP when the dataset size grows. It is quite acceptable, although it is not the best algorithm in time complexity. Therefore, one of our future work will focus on how to improve CSIP's time performance on large datasets, such as developing distributed algorithm.Theoretically, the CSIP consists of three parts, i.e. the PR algorithm, the main CSIP algorithm and the parameter tuning algorithm. The PR algorithm converges after about log10ɛ/log10d iterations [27]. The time complexity of the matrix operations for updating IP values isO(n2). As such, the time complexity of the IP algorithm isO((log10ɛ/log10d)·n2). Obviously, the time complexity of the sorting and the clustering in the CSIP isO(n2). If a fixed distance bandwidth is used, the total time complexity of the CSIP isO((log10ɛ/log10d)·n2). In the case of using the parameter tuning algorithm that normally ends after n iterations, the time complexity of the entire CSIP algorithm isO((log10ɛ/log10d)·n3).Clustering remains a difficult problem because of the inherent vagueness in the definition of a cluster [21]. Kleinberg [26] also provides an impossibility theorem for clustering formally, showing that there is no clustering algorithm satisfying all the three properties, i.e. scale-invariance, richness and consistency. Consequently, thousands of clustering algorithms have been proposed in the literature. We refer interested readers to the reviews [21,22,49] and books [1,15] about clustering. We list some of the major approaches related to our work.In spite of the fact that K-means was proposed over 50 years ago, K-means [33] and its derivations are still widely used because of its simplicity and easiness to implement in many practical problems. However, K-means suffers from the issues of identifying the initial partitions and the number of clusters. There is no efficient and universal method to overcome these obstacles, although many variants of K-means have appeared, to name a few, ISODATA [2], kd-tree [40], K-medoid [25], Kernel K-means [44], X-means [41], etc. In addition, the use of centroid to represent a cluster inherently limits K-means to compact and hyperspherical clusters. Fuzzy c-means (FCM) [11] is one of the most popular fuzzy clustering algorithm that can be regarded as a generalization of ISODATA by relaxing the only one cluster assignment to all of the clusters with a certain degree of membership. Like its counterpart, FCM also suffers from the difficulty to identify the initial partitions. Our experiments also show that K-means and FCM are difficult to cluster complex shapes whose centroids are not very highly distinguishable, even the number of clusters are known in advance.A number of probabilistic models have been developed for clustering that assume the data is generated from a mixture distribution, including Gaussian model [7], Latent Dirichlet Allocation [5] and Pachinko Allocation model [29]. The expectation-maximization (EM) algorithm [9] is the most popular to infer the parameters in mixture models. The major disadvantages of these methods, similar to K-means family, are the sensitivity to the selection of initial model parameters and the number of clusters. Also, the use of EM algorithm for estimating parameters suffers from the issues of the convergence possibility to a local optimum and the slow convergence rate [12]. Our experiments also show that GM_EM seems to perform better than K-means and FCM on clustering complex shapes, but it may produce vastly different results among different runs even with the same settings.Graph theoretic clustering, also referred to as spectral clustering sometimes, represents the data points as nodes in a weighted graph. As opposed to K-means and probabilistic mixture models which always result in clusters with convex geometric shape, the family of spectral clustering, e.g. Ratio Cut [18], Normalized Cut [45], Laplacian Eigenmap [3], Pair-wise Clustering [39], K-AP [52], etc., can be used in more complex scenarios, such as intertwined spirals, or other arbitrary nonlinear shapes [1]. The disadvantage of spectral clustering family is its intensive computational expense on constructing an adjacency matrix and calculating the eigen-decomposition of the corresponding Laplacian matrix, which have time complexities of O(n2m) and O(n3), respectively. It is an unreasonable burden for large-scale datasets. This is also illustrated in our experiments that K-AP costs much more time than other algorithms in clustering.

@&#CONCLUSIONS@&#
