@&#MAIN-TITLE@&#
View invariant action recognition using projective depth

@&#HIGHLIGHTS@&#
We propose the use of projective depth for view-invariant action recognition.Body points are decomposed into a set of projective depths.Similarity of two actions is measured by the motion of projective depths.Projective depths are shown to be invariant to camera parameters.We explore different ways of extracting planes used to calculate projective depth.

@&#KEYPHRASES@&#
View invariance,Action recognition,Projective depth,

@&#ABSTRACT@&#
In this paper, we investigate the concept of projective depth, demonstrate its application and significance in view-invariant action recognition. We show that projective depths are invariant to camera internal parameters and orientation, and hence can be used to identify similar motion of body-points from varying viewpoints. By representing the human body as a set of points, we decompose a body posture into a set of projective depths. The similarity between two actions is, therefore, measured by the motion of projective depths. We exhaustively investigate the different ways of extracting planes, which can be used to estimate the projective depths for use in action recognition including (i) ground plane, (ii) body-point triplets, (iii) planes in time, and (iv) planes extracted from mirror symmetry. We analyze these different techniques and analyze their efficacy in view-invariant action recognition. Experiments are performed on three categories of data including the CMU MoCap dataset, Kinect dataset, and IXMAS dataset. Results evaluated over semi-synthetic video data and real data confirm that our method can recognize actions, even when they have dynamic timeline maps, and the viewpoints and camera parameters are unknown and totally different.

@&#INTRODUCTION@&#
Pose and action recognition plays a crucial role in various applications such as surveillance, human–computer interaction (HCI), and ergonomics. For this reason, it has attracted a lot of attention from researchers in recent times [1–3].Action can be regarded as a collection of 4D space–time data observed by a perspective video camera. Due to image projection, the 3D Euclidean information is lost and projectively distorted, which makes action recognition rather challenging, especially for varying viewpoints and different camera parameters. Another source of challenge is the irregularities of human actions due to a variety of factors such as age, gender, and circumstances, and the large degrees of freedom of articulated bodies [4]. The timeline of action is another important issue in action recognition. The execution rates of the same action in different videos may vary for different actors or due to different camera frame rates. Therefore, the mapping between same actions in different videos is usually highly non-linear.A number of attempts have been made to address these issues. Very often simplifying assumptions are made on one or more of the following aspects: (1) camera model, such as scaled orthographic [5] or calibrated perspective camera [6]; (2) camera pose, i.e. little or no viewpoint variations; (3) anatomy, such as isometry [7], and coplanarity of a subset of body points [7].Action recognition algorithms normally assume a model of the human body and proceed by recognizing the body pose and motion over time. The different human models used include silhouette, body points, stick model, etc. Space–time features are essentially the primitives that are used for recognizing actions, e.g. photometric features such as the optical flow [8–10] and the local space–time features [11,12]. These photometric features can be affected by luminance variations due to camera zoom or pose changes, and often work better when the motion is small or incremental. On the other hand, salient geometric features such as silhouettes [13–17] and point sets [7,18] are less sensitive to photometric variations, but require reliable tracking. Silhouettes are usually stacked in time as 2D [15] or 3D object [13,17], while point sets are tracked in time to form space–time curves. Some existing approaches are also more holistic and rely on machine learning techniques, e.g. HMM [19], SVM [11], etc. As in most exemplar-based methods, they rely on the completeness of the training data, and to achieve view-invariance, are usually expensive, as it would be required to learn a model from a large dataset.As we mentioned, many action recognition methods adopt simplified camera models or assume fixed viewpoints or simply ignore the effect of viewpoint changes. Practically, this is not a valid assumption. A reliable action recognition system has to be invariant to the camera parameters or viewpoint changes. View-invariance is, thus, of great importance in action recognition, and has started receiving more attention in recent times. This is the main issue we address in this paper.There are different approaches to tackling view invariance. One approach to tackle view-invariant action recognition has been based on using multiple cameras [19–21,6]. Campbell et al. [22] use stereo images to recover a 3D Euclidean model of the human subject, and extract view invariance for 3D gesture recognition; Weinland et al. [6] use multiple calibrated and background-subtracted cameras, and they obtain a visual hull for each pose from multi-view silhouettes, and stack them as a motion history volume, based on which Fourier descriptors are computed to represent actions. Ahmad and Lee [19] build HMMs on optical flow and human body shape features from multiple views, and feed a test video sequence to all learned HMMs. All these methods require the setup of multiple cameras, which is quite expensive and restricted in many situations such as online video broadcast, HCI, or monocular surveillance.A second line of research is based on a single camera and is motivated by the idea of exploiting the invariants associated with a given camera model, e.g. affine, or projective. For instance, Rao et al. [23] assume an affine camera model, and use dynamic instants, i.e. the maxima in the space–time curvature of the hand trajectory, to characterize hand actions. The limit with this representation is that dynamic instants may not always exist or may not be always preserved from 3D to 2D due to perspective effects. Moreover the affine camera model is restrictive in most practical scenarios.A more recent work reported by Parameswaran and Chellappa [7] relaxes the restrictions on the camera model. They propose a quasi-view-invariant 2D approach for human action representation and recognition, which relies on the number of invariants in a given configuration of body points. Thus a set of projective invariants are extracted from the frames and used as action representation. However, in order to make the problem tractable under variable dynamics of actions they introduced heuristics, and made simplifying assumptions such as isometry of human body parts. Moreover, they require that at least five body points form a 3D plane or the limbs trace planar area during the course of an action. Ref. [24] described a method to improve discrimination by inferring and then using latent discriminative aspect parameters.Another interesting approach to tackle unknown views has been suggested by Ref. [25], who use virtual views, connecting the action descriptors extracted from source view to those extracted from target view. Ref. [26], used a bag of visual-words to model an action.A more promising approach is based on exploiting the multi-view geometry. Two subjects in the same exact body posture viewed by two different cameras at different viewing angles can be regarded as related by the epipolar geometry. Therefore, corresponding poses in two videos of actions are constrained by the associated fundamental matrices, providing thus a way to match poses and actions in different views. The use of fundamental matrix in view invariant action recognition is first reported by Syeda-Mahmood et al. [27] and later by Yilmaz and Shah [17,18]. They stack silhouettes of input videos into space–time objects, and extract features in different ways, which are then used to compute a matching score based on the fundamental matrices. A similar work is also presented in [28], which is based on body points instead of silhouettes. A recent method [29] uses probabilistic 3D exemplar model that can generate 2D view observations for recognition.This work is an extension of Ref. [30], which introduced the concept of characteristic vectors for view-invariant action recognition. Whereas [30] uses the concept of using projective depth with respect to the ground plane, we exhaustively investigate the different ways of extracting planes including (i) body-point triplets, (ii) planes in time, and (iii) planes extracted from mirror symmetry. We analyze these different techniques and analyze their efficacy in action recognition. The most interesting case and efficient method is extracting planes using mirror symmetry. We demonstrate that it is possible to use mirror symmetry to extract planes even when the action may be asymmetric.The rest of the paper is organized as follows: In Section 2, we introduce the concept of projective depth, and investigate the different ways of using it for action recognition including (i) Ground Plane, (ii) Triplets, (iii) Planes in time, and (iv) Mirror symmetric planes. We present our results on three different datasets including the CMU MoCap dataset, Kinect dataset, and IXMAS dataset in Section 4. Finally we discuss our method and conclude in Section 5.We propose to use the concept of the projective depth[31] for use in action recognition. Since the image sequence is acquired from a camera, we lose the depth information. However, given a 3D plane viewed by two camera, it is possible to find the projective depth of a given point relative to this plane. Let us first look at the concept of projective depth:A world pointX=(xT,ρ)Tis imaged atxin the first image and at(1)x′=(1-ρ)Hx+ρe′in the second image, whereHis the homography between the planes ande′is the epipole in the second image.This world point introduces a parallax relative to the plane as illustrated in Fig. 1. Sincex′,e′, andHxare collinear, the scalarρis the parallax relative to the planeπ, which can be expressed as:(2)ρ=x′-Hxe′-Hxρis 0 implies the point is on the plane. Otherwise the sign ofρindicates which side of the planeπthe pointXis. However, in the absence of oriented projective geometry the sign of a homogenous object, and the side of a plane have no meaning. To solve this, instead of using the projective depth directly, we use the scaled absolute value of the difference of depths as our invariant:Definition 1Canonical poseWe shall call the image pointspi=1,…,kof a set of fixed points in a stationary cameraP, a canonical pose of the k points.Note that the definition does not impose constraints such as points in general position or non-coplanarity. This pose would be used to extract projective depths from a sequence.Definition 2Letmi=1,…,kbe a set of image points in a cameraP1that are in one-to-one correspondence with the points in the canonical pose and a homographyH1that is consistent with the fundamental matrixFbetween the set of points and the points in the canonical pose. Let alsomi=1,…,k′be the images of these points after moving to new locations and a homographyH2that is consistent with the fundamental matrixFbetween the set of moved points and the points in the canonical pose we define the “characteristic vector” of the k moving points as(3)t=b1⋮bkwhere(4)bi=mi-H1pixe′-H1pix-mi′-H2pixe′-H2pix(5)=mi-H1piye′-H1piy-mi′-H2piye′-H2piy,(·)xand(·)ydenote the x and y coordinates of the argument vector, ande′is the epipole in the second image.This concept is shown in Fig. 2: Given a person viewed by two cameras, we can take the first pose as the canonical pose. For the subsequent poses, we can calculate the characteristic vector, which is the scaled absolute difference of the pose and the canonical pose. As we will demonstrate, the characteristic vector is invariant to camera internal and external parameters.Proposition 1Invariance of Characteristic VectorAssume two sets of freely moving points that are in one-to-one correspondence with the points in the canonical pose are observed by two distinct camerasP1andP2. If the motion of the two sets of points differ up to similarity, then the associated characteristic vectors would differ up to scale.An important constraint in the definition of the characteristic vector is the consistency ofHwith the fundamental matrixF. This was established by Viéville and Luong [32] as the condition thatHTFhas to be skew symmetric. The latter implies thatHTF+FTH=0.Proposition 1 implies that the projective depths of two semantically similar actions viewed by two cameras with different internal parameters, and unknown rotation and translation would differ up to a scale. For instance, Fig. 3shows the characteristic vectors of two persons performing the watch-time sequence. The change in projective depths is very similar for both actions, even the two cameras have different internal parameters, and unknown rotation and translation.A key issue is thus, how can one find a set of 4 or more point correspondences that yield a homographyH, which is consistent with the epipolar geometry and can be used to extract projective depths. This issue is of practical interest in our problem, because in practice it would be impossible to find corresponding planes between two actions performed by two different subjects at totally different locations viewed by two different cameras.As we will demonstrate in this section, there are multiple ways of extracting projective depths including using the ground plane, body-point triplets, planes in time, and mirror-view symmetry.This was investigated in [30]. The main thesis was that the ground plane can be used to estimate the depth of each body point. Then we would have exactly 11 projective depth corresponding to a 11 point body model per frame. With the exception of the foot points, the ground plane is always far away from the body points and hence, we can be sure that the projective depths are large enough to be meaningful. The problem of action recognition would then translate to matching curves, as we would have 11 curves corresponding to each action. However, in this case, we have a much smaller set to work with (Only 11 projective depths per frame). A key issue faced in [30] was in extracting the ground plane. The method of doing this is reproduced below:Letm1andm2be two arbitrary points in a cameraP1and in correspondence withp1andp2in the canonical pose. Let alsom3be any arbitrary point inP1. Then the corresponding pointp3in the canonical camera must satisfy the epipolar constraintp3TFm3=0. This provides a one parameter family of solutions in the form ofp3(α)forp3. Taking the epipoles as the fourth corresponding points, defines a one-parameter family of homographiesH(α)that map the four pointsm1,m2,m3andetop1,p2,p3ande′. The optimal parameterαthat would impose the consistency condition is then found using(6)α∗=argmintrace(H(α)TFFTH(α))subject to∥HT(α∗)F+FTHT(α∗)∥is minimizedThis is a constraint minimization of a polynomial cost function, for which there is a closed form solution.For a given test sequence, Ref. [30] first calculate the characteristic vectors with respect to a canonical pose of a human subject over all frames in the sequence. This basically yields a time series of characteristic vectors. The canonical pose may be for instance a person simply standing right up. The canonical pose is not important because we look at the temporal evolution of the projective depths rather than the projective depths themselves. If two actions are ‘similar,’ then the temporal evolution of the projective depths would also be similar without regards to the canonical pose. In the experiments, the canonical pose was usually taken as the first pose of the video to experimentally demonstrate that it does not really matter. We could also use dynamic programming to choose a canonical pose which gives a high similarity score for the entire sequence similar to [33].If we regard the characteristic vectortas a random, scaled vector, then given a setR={r1,…,rM}of M different series of reference characteristic vectors corresponding to M different actions, our goal is to findrmthat best matches the test sequence. For the time being assume that the test sequence and all reference sequences are of the same length of K and are aligned. Assuming a normal distribution of noise and errors with varianceσ2, the probability of the characteristic vector of an unknown actiontto matchrmis given by(7)p(t¯∣r¯m)∝exp-∥t¯-r¯m∥22σ2wheret¯=∣t∣∥t∥andr¯m=∣rm∣∥rm∥.Assuming conditional independency over time, we can solve the problem by minimizing the following negative log-likelihood function:(8)m∗=argminm=1,…,M∑K=1,…,K∥t¯k-r¯mk∥2wherem∗is the estimated optimal index for the matched sequence in the database. In practice one may attempt to improve upon this formulation by constraining the fact that the motion of a given point in time must be smooth. However, as seen in the experimental section this maximum likelihood solution is sufficient for providing good results.The 3D body structure of a human can be divided into triplets of body points, each of which determines a plane in the 3D space when the points are not collinear. The problem of comparing articulated motions of human body thus transforms to comparing rigid motions of body planes (triplets) as shown in [34,33]. We can divide the body points into a set of triplets. The 3 point triplet along with the epipole define a plane. We can use these planes to calculate the projective depth of every other point. To match two poses, it would be necessary to match their projective depths.Given a body model with 11 body points, we have113=165triplets and for every triplet, we have11-3=8projective depths. The total projective depths would equal the number of triplets times the number of projective depths for each triplet or165×8=1320in our case. This is a lot of data to work with. Fig. 4shows an example of a body-point triplet. As just described, we calculate the projective depth of every point not included in the triplet. And we do this for every possible body-point triplet. The projective depths of these similar poses differ up to a scale.We still need a fourth point to calculate the homography. This is provided by the epipoles since the epipoles are in every plane. Hence given an observed pose transitionIi→Ijfrom sequence{It}, and a second oneJmk→Jnkfrom sequence{Jtk}. WhenIi→Ijcorresponds toJmk→Jnk, one can regard them as observations of the same 3D pose transition by two different camerasP1andP2, respectively. There are two instances of epipolar geometry associated with this scenario:1.The mapping between the image pair〈Ii,Ij〉and the image pair〈Jmk,Jnk〉is determined by the fundamental matrixF[31] related toP1andP2. The projection of the camera center ofP2inIiorIjis given by the epipolee1, which is found as the right null vector ofF. Similarly the image of the camera center ofP1inJmkorJnkis the epipolee2given by the right null vector ofFT.The other instance of epipolar geometry is between transitioned poses of a triplet of body points in two frames of the same camera, i.e. the fundamental matrix induced by a moving body point-triplet, which we denote asF. We call this fundamental matrix the inter-pose fundamental matrix, as it is induced by the transition of body point poses viewed by a stationary camera.LetΔbe a triplet of non-collinear 3D points, whose motion lead to different image projections onIi,Ij,JmkandJnkasΔi,Δj,ΔmkandΔnk, respectively:Δi=〈x1,x2,x3〉,Δj=〈x1′,x2′,x3′〉,Δmk=〈y1,y2,y3〉,Δnk=〈y1′,y2′,y3′〉,ΔiandΔjcan be regarded as projections of a stationary 3D point triplet〈X1,X2,X3〉on two virtual camerasPi′andPj′.〈X1,X2,X3〉defines a world planeπ, which induces a homographyHijbetweenPi′andPj′. It is known that a homography may be computed from four corresponding image points. In this case, the four points can be the image pointsx1,…,x3andx1′,…,x3′together with the epipoles inPi′andPj′. Letei′andej′be these epipoles. Ifei′andej′are known, thenHijcan be computed, and hence the pprojective depths induced byΔiandΔjcan be determined using (2). The difficulty is that the epipolesei′,ej′,em′anden′are unknown, and cannot be computed directly from the triplet correspondences. Fortunately, however, the epipoles can be closely approximated as described below.Proposition 2If the exterior orientation ofP1is related to that ofP2by a translation, or by a rotation around an axis that lies on the axis planes ofP1, then under the assumption:(9)ei′=ej′=e1,em′=en′=e2,we have:(10)E(F^1,F^2)=0.Under more general motion, the equalities in (9) become only approximate. However, we found that this approximation is inconsequential in action recognition for a wide range of practical rotation angles.Degenerate triplets: A homography cannot be computed from four correspondences if three points are collinear. Even when three image points are close to collinear the problem becomes ill-conditioned. We call such triplets as degenerate, and simply ignore them in matching pose transitions. This does not produce any difficulty in practice, since with 11 body point representation used in this paper, we obtain 165 possible triplets, the vast majority of which are in practice non-degenerate.A special case is when the epipole is close to or at infinity, for which all triplets would degenerate. We solve this problem by transforming the image points in projective space in a manner similar to Zhang and Loop [35]. The idea is to find a pair of projective transformationsQandQ′, such that after transformation the epipoles and transformed image points are not at infinity. Note that these transformations do not affect the projective equality in Proposition 1.Another option is to use the planes in time. As the person moves in time, we have more points to use. However, this leads to a an extreme amount of data since we are effectively choosing 3 points from the number of body points times the number of frames. Assuming the number of body points is 11 and the length of the video is 60, then this amounts to a total of60×113=47698420, which is huge and is not practical to work with. For this reason, we did not pursue this course of research.This work builds on the work of Ref. [36], which analyzed the idea of 3D reconstruction from a single perspective view of a mirror symmetric scene. The work demonstrated that the mirror view is equivalent to the observing the same scene with two cameras. Let’s first quickly review their work since we are going to build on that. In particular, it would be helpful to look at Lemma 1 from [36], which is reproduced here:Lemma 1The image of a scene that is symmetric with respect to an unknown plane, formed by an arbitrary projective camera, is identical to the image of the scene formed by the (virtual) projective camera symmetric of the first one with respect to the scenes 3-D (unknown) symmetry plane.Assume we have an image of a symmetric shape. We can place the originOof the world on the symmetry plane. LetXdenote a world point represented by the vector[xyz1]Tand let x denote the corresponding homogenous 3-vector[UVW]. Let the camera be defined by the3×4matrixP=M[I∣-C∼], whereM=KRwhereKis the3×3calibration matrix, andRis the3×3rotation matrix from the world coordinate system to the camera coordinate system; andC∼is the inhomogenous3×1vector of the camera center coordinates in the world coordinate system. A world pointXis mapped to thexby this relation:(11)x=PX=M[I∣-C∼]XThe world point symmetric toXwith respect to the symmetry plane isX‾=ZX, where:(12)Z=-1000010000100001and we note:(13)Z∼=-100010001The image pointx¯of the world pointX‾seen by the camera at centerCis:(14)x¯=M[I∣-C∼]ZXNow consider a virtual camera, which is symmetric to camera at centerCwith respect to the object’s symmetric plane. Hence its center would beC‾=ZC, and it would project a world pointXaccording to this relation:(15)x′=M‾[I∣-C‾∼]XwhereM‾=MZ∼. Substituting symmetric elements by their expression:(16)x′=MZ∼[I∣-Z∼C∼]X=M[I∣-C∼]ZX=x¯Similarly:(17)x¯′=MZ∼[I∣-Z∼C∼]ZX=M[I∣-C∼]ZZX=xThis means that the image of a pair of symmetric points viewed by a real camera is equivalent to the case of a virtual camera viewing the same symmetric points being reversed in the real and virtual view.Our goal can be stated as follows: Given a 3D pose viewed by two camerasC1andC2, we want to extract planes from the scene to estimate the projective depths of body points relative to the plane. This information can then be used for pose-recognition and extended to action recognition.Applying mirror view symmetry would relateC1and its mirror view, andC2and its mirror view only. Furthermore, this would assume that the action is symmetric, which is not the case with most of the actions. But recall that we already know the epipolar geometry betweenC1andC2and therefore, we can use this information. Furthermore, we are interested in a mirror view of the person, which can be used to extract co-planar points.Let us refer to an example to illustrate this concept: Consider the case of an asymmetric pose and the hand pointsXlefthandandXrighthandare viewed by the two camerasC1andC2and the corresponding image points are:xlefthand, andxrighthandandxlefthand′, andxrighthand′, respectively. Let us first consider cameraC1: If the pose is symmetric, in the mirror view,x¯lefthand=xrighthandandx¯righthand=xlefthand. However, if the pose is not symmetric, this would not be true. But we can think of the virtual body point that would have been there had it been a symmetric pose:x¯lefthand=Z1′xrighthand, where(18)Z1′=-10t1010001wheret1corresponds to some translation. Similarly,x¯righthand=Z1′xlefthand. We can think of cameraC2, wherex¯lefthand′=Z2′xrighthand′andx¯righthand′=Z2′xlefthand′, where(19)Z2′=-10t2010001So we have two unknownst1andt2, which are the unknown translations. Now consider another 3 point, let’s say the left shoulder point,Xleftshoulder, which is viewed in cameraC1andC2asxleftshoulderandxleftshoulder′, respectively.The virtual mirror symmetric points would bex¯leftshoulder=Z1′xleftshoulderandx¯leftshoulder′Z2′xleftshoulder′. Bothxleftshoulderandx¯leftshoulderwould have the same ‘depth’ relative to the plane defined by the points,xlefthand,xrighthand,x¯lefthand, andx¯righthandin cameraC1andxlefthand′,xrighthand′,x¯lefthand′, andx¯righthand′in cameraC2(Refer to Fig. 5).LetHbe the homography relating the points,xlefthand,xrighthand,x¯lefthand, andx¯righthandin cameraC1andxlefthand′,xrighthand′,x¯lefthand′, andx¯righthand′in cameraC2, we have:(20)ρ1=xleftshoulder′-Hxleftshoulderxleftshoulder′-e′=x¯leftshoulder′-Hx¯leftshoulderx¯leftshoulder′-e′=Z2xleftshoulder′-HZ1xleftshoulderZ2xleftshoulder′-e′Similarly, in the other direction we have:(21)ρ2=xleftshoulder-H-1xleftshoulder′xleftshoulder-e=x¯leftshoulder-H-1x¯leftshoulder′x¯leftshoulder-e=Z1xleftshoulder-H-1Z2xleftshoulder′Z1xleftshoulder-eSo we have two equations to solve for the two unknowns,t1andt2. Solving these equations, we gett1=-2exandt2=-2ex′.Till now we have described different techniques to extract projective depths. Once the body posture has been decomposed into a set of projective depths, a number of techniques can be used. As shown in [30], we can use a few examples of an action for view-invariant action recognition. Or, if we have many examples, we can use machine learning techniques, e.g. HMM [19], SVM [11], etc. for view-invariant action recognition.We use a technique similar to [37]. For action recognition, we store the depths in a volume, which we call the Projective Depth Volume, or PDV for short. Thus, when we are using ground plane, we have a 3D volume of4×Nbodypoints×F, whereNbodypointsare the total number of body points andFare the total number of frames. Here the first4×Nbodypointsis the characteristic vector (we use both x and y coordinates and the characteristic vector in both dimensions), and the characteristic vector is calculated for every frame in the sequence. Similarly, when we use triplets, the volume has dimensions of4×Nbodypoints×F×Nbodypoints3. This is because in each frame, we get a set ofNbodypoints3planes, and we calculate the characteristic vector for each of these frames for the entire sequence. Similarly, using mirror symmetry, we have a4×Nbodypoints×F×Nbodypoints2dimensional volume.This volume is characteristic of the action. Our objective is to approximate this volume into compact vectors for use in action recognition. We use rank-1 decomposition described in [37] to generate compact representations of the volume, which is then used for action recognition. The algorithm for rank-1 approximation of 3-rd order tensor PDV is described in Algorithm1.Algorithm 1PDV rank-1 decompositionGiven two motion sequencesmiandmj, we can obtain the corresponding discriminant vectors,vi={DTi,DFi,DRi}andvj={DTj,DFj,DRj}. Then the similarity of the two motion sequences can be calculated using∥vi-vj∥.In this section, we present results on three categories of data including the CMU MoCap dataset, Kinect dataset, and IXMAS dataset [6].We selected 5 classes of actions from CMU’s MoCap dataset: walk, jump, golf swing, run, and climb. Each action class is performed by 3 actors, and each instance of 3D action is observed by 17 cameras, as shown in Fig. 7. The focal lengths were changed randomly in the range of1000±300. Fig. 6shows an example of a 3D pose observed from 17 viewpoints.Our dataset consists of totally 255 video sequences, from which we generated a reference action Database (DB) of 5 video sequences, i.e. one video sequence for each action class. The rest of the dataset was used as test data, and each sequence was matched against all actions in the DB and classified as the one with the highest score. For each sequence matching, 10 random initializations were tested and the best score was used. The first frame was chosen as the canonical pose (our method is not sensitive to the choice of the canonical pose, and using a different pose does not make a difference). We used leave one out cross validation. The classification results are shown in Tables 1–3. The overall classification accuracy for our method is 95%, 90%, and 96%, using ground plane, triplets, and mirror symmetry, respectively. The results are remarkably good despite the extreme viewpoint changes and variations in camera intrinsic parameters.We evaluated our method on Kinect dataset, which was collected in [38]. The dataset was collected using the Microsoft Kinect Sensor and OpenNI platform. There are a total of 19 actions recorded on 16 different actors. We used 16 actions to evaluate our methods. The results are shown in Tables 4–6. Using the Kinect Sensor gives the 3D coordinates of the body points. For our method, we only used the 2D body-point information. The overall classification accuracy for our method is 75.1%, 87.9%, and 82.3%, using ground plane, triplets, and mirror symmetry, respectively.We evaluated our method on IXMAS dataset [6]. We tested our method on 10 actions consisting of “watch time,” “cross arms,” “scratch head,” “sit down,” “stand up,” wave,” “punch,” “kick,” “point,” and “pick up.” This would correspond to testing on10×3×5×10=1500different videos. Some examples from IXMAS dataset are shown in Fig. 8. We used leave one out cross validation to test our results.Body joints are required in IXMAS. However, as we have argued, we treat these points as inputs to our algorithm. For IXMAS, we used articulated object tracking techniques very similar to [39], but with some manual tinkering (we also made use of the calibration matrices of the cameras). We plan to make these tracking data available for use to other researchers.In our experiments, we chose the first frame of “watch time” in “cam0” view of “Amel” as our canonical pose (our method is not sensitive to the choice of the canonical pose, therefore using a different pose of a different person from another viewpoint does not make a difference). The results are shown in Tables 7–9. The overall recognition rates are 81.4%, 87.3%, and 90.5% using ground plane, triplets, and mirror symmetry, respectively. For comparison, Table 10gives a summary of the existing methods for view-invariant action recognition and their respective recognition rates in % for IXMAS dataset.In this work, we investigated the use of projective depths for use in view-invariant action recognition. The challenge is finding planes in the scene, which can be used to extract projective depths. We looked at three different strategies for extracting planes between two frames: (i) Ground Plane: Finding the ground plane and using it to extract the projective depth for all body-points; (ii) Triplets: Using body point triplet planes and for each plane extracting the projective depth of each other point; and (iii) Mirror View: a novel method of using the mirror view of a person so that any line segment and its mirror counterpart can be used as a plane. Using the ground plane can be roughly thought of a subset of using the mirror person because the two feet points and their mirror counterparts are very closely related to the ground plane. Hence, the lower recognition rate using ground plane with respect to using mirror symmetry is understandable.The difference between using triplets and mirror person is easier to analyze when we consider their counterparts for 3D points. The triplets correspond to the plane formed by the triplet while the mirror person is equivalent to taking the mirror view of the person and using each line segment and its mirror view. The difference essentially lies in the planes extracted. For the triplets, the accuracy seems to be low compared to mirror symmetry because the planes extracted from triplets are always very close to the body points. In fact, for useful information to be extracted, it is essential that some of the body points move really far away from the body. Otherwise, all the projective depths map to zero. Mirror symmetry has none of these issues, and therefore, it is not surprising that mirror symmetry gives the best performance in most cases.One may ask the significance of this work given that many other methods are able to give comparable or better results. We are tackling a very hard problem and there are two facets of this problem: (i) View invariance: We assume that the test action and the examples in the dataset may be from very different viewpoints; and (ii) the number of examples are very limited. Both of these factors are very important in that usually the methods in the literature, which address view-invariance, assume they have a huge set of actions from different view points so that they are able to train a classifier, which is able to give good results for different view points. But what would happen if these methods had a handful of videos? We on the other hand, are tackling a much harder problem, which is what if only a few instances of the action were given. The end goal is that a user or animator may be able to define a new action simply by capturing a single video. In this context, our findings are that geometric invariants are indispensable in that they provide us with geometric properties of the object, which are invariant to different viewpoints and intrinsic parameters of the camera.The idea of estimating projective depth, stacking them in a volume and decomposing them is directed more towards motion retrieval. Large motion capture datasets have become commonplace owing to their importance in realistic animation of human motion. With this development, it has become increasingly important to develop methods for an animator to search for similar motions from a given dataset. Since, the depth vector is only dependent on the sequence, we can find the volumes for the database of actions and decompose them offline and store the characteristic vectors. Hence, given a query action, we need to decompose it and compare its characteristic vector with a set of other vectors, which would prove very fast and efficient.In summary, the major contributions in this paper are: (i) We investigate the different ways of extracting projective depths for use in view-invariant action recognition including using the ground plane, body-point triplets, and mirror-view symmetry. (ii) We compare transitions of two poses, which encodes temporal information of human motion while keeping the problem at its atomic level. (iii) We propose to use low rank decomposition for converting motion sequence volumes into compact lower dimensional representation, without losing the non-linear dynamics of the motion manifold. (iv) We provide extensive experiments to rigorously test our method on 3 different datasets, including the CMU MoCap dataset, Kinect dataset, and the IXMAS dataset.

@&#CONCLUSIONS@&#
