@&#MAIN-TITLE@&#
Robust growth mixture models with non-ignorable missingness: Models, estimation, selection, and application

@&#HIGHLIGHTS@&#
Four non-ignorable missingness models are proposed.Three robust models to deal with outliers are proposed.A full Bayesian method is implemented.Model selection criteria are proposed in a Bayesian context.Three simulation studies and one real data case study are conducted.

@&#KEYPHRASES@&#
Growth mixture models,Non-ignorable missing data,Robust methods,Bayesian method,Model selecting criteria,

@&#ABSTRACT@&#
Challenges in the analyses of growth mixture models include missing data, outliers, estimation, and model selection. Four non-ignorable missingness models to recover the information due to missing data, and three robust models to reduce the effect of non-normality are proposed. A full Bayesian method is implemented by means of data augmentation algorithm and Gibbs sampling procedure. Model selection criteria are also proposed in the Bayesian context. Simulation studies are then conducted to evaluate the performances of the models, the Bayesian estimation method, and selection criteria under different situations. The application of the models is demonstrated through the analysis of education data on children’s mathematical ability development. The models can be widely applied to longitudinal analyses in medical, psychological, educational, and social research.

@&#INTRODUCTION@&#
Mixture models offer natural models for unobserved population heterogeneity. The importance of mixture models, their enormous developments, and their frequent applications are not only remarked by a number of recent books but also by a diversity of journal publications. For example, Computational Statistics & Data Analysis has published two special issues on mixture models (Bohning and Seidel, 2003; Bohning et al., 2007) and the current issue is a new one. Latent growth models are used to study individuals’ latent growth trajectories by analyzing the variables of interest on the same individuals repeatedly through time (e.g.,  Bollen and Curran, 2006; McArdle and Bell, 1999; Meredith and Tisak, 1990). These models are very popular in biological, psychological, educational, and social sciences (e.g.,  Collins, 1991; Fitzmaurice et al., 2004; Singer and Willett, 2003). By combining latent growth models and finite mixture models (e.g.,  McLachlan and Peel, 2000), growth mixture models (GMMs, see, e.g.,  Lubke and Muthén, 2005; Muthén, 2004; Muthén et al., 2011), therefore, provide researchers with a flexible set of models for growth data with latent population heterogeneity.However, with the increase in complexity of model specification comes an increase in difficulties estimating GMMs. First, missing data are almost inevitable (e.g.,  Little and Rubin, 2002; Yuan and Lu, 2008), especially in longitudinal studies (e.g.,  Jelicic et al., 2009; Roth, 1994). Little and Rubin (2002) distinguished ignorable and non-ignorable missingness mechanisms. Non-ignorable missingness is a crucial and serious concern, because not attending to it may result in severely biased statistical estimates, standard errors, and associated confidence intervals (e.g.,  Little and Rubin, 2002; Schafer, 1997; Zhang and Wang, 2012). However, most of the literature on the problems of missing data focuses on ignorable missingness (e.g.,  Schafer and Graham, 2002). Second, data may have outliers (e.g.,  Hoaglin et al., 1983), particularly in social and behavioral sciences (e.g.,  Micceri, 1989). The consequences of applying a normal distribution assumption to such data include unreliable parameter estimates (e.g.,  Pan and Fang, 2002), unreliable standard errors and confidence intervals, and misleading statistical tests and inference (e.g.,  Yuan and Bentler, 1998). Third, for complex models such as GMMs with missing data and outliers, maximum likelihood methods might fail or provide biased estimates (e.g.,  Yuan and Zhang, 2012). Most of the previous estimations have relied on maximum likelihood methods for parameter estimation and have carried out inferences through conventional likelihood procedures (e.g.,  Song et al., 2014). Fourth, even with effective estimation methods, model selection in such complex situations becomes extremely difficult. Traditional criteria for model selection, including Akaike’s Information Criterion (AIC,  Akaike, 1974), Bayesian Information Criterion (BIC,  Schwarz, 1978), consistent Akaike’s Information Criterion (CAIC,  Bozdogan, 1987), sample-size adjusted Bayesian Information Criterion (ssBIC,  Sclove, 1987), and Deviance Information Criterion (DIC,  Spiegelhalter et al., 2002), are not uniformly effective due to latent effects and missing data (e.g.,  Celeux et al., 2006).Few studies have discussed how to address these common problems in longitudinal research in the framework of GMMs. Lu et al. (2011) discussed GMMs with non-ignorable missing data using Bayesian methods. However, they (1) considered only one type of non-ignorable missingness, (2) assumed data are normally distributed without any outlier, and (3) did not propose any model selection criterion.This article extends the study of Lu et al. (2011) and addresses these challenges in GMMs: missing data, outliers, estimation, and model selection. Regarding missing data, we propose new types of non-ignorable missingness in GMMs and investigate their influences on model estimation under different situations. Regarding outliers, we use robust models (e.g.,  Lange et al., 1989) to minimize the effects of contaminated data. Because convenient robust methods often lead to other problems such as under-estimation of standard errors (e.g.,  Poon and Poon, 2002), we adoptt-distributions to deal with heavy-tailed data (Lin et al., 2004; Zhang et al., 2013). Regarding estimation methods, as Bayesian methods provide many advantages of estimating complex models (e.g.,  Dunson, 2000), we propose a full Bayesian approach, which is flexible enough to estimate a variety of models with different missing data mechanisms, contaminated data, and mixture structure. Regarding model selection, we propose several selection criteria in the Bayesian context. The performances of these criteria are investigated under different situations.In the next section of this article, Section  2, we propose GMMs with different types of missing data and outliers. In Section  3, we present Bayesian estimation methods. In Section  4, we propose Bayesian model selection criteria. In Section  5, we conduct three simulation studies on Bayesian GMMs under different conditions. In Section  6, we demonstrate the application of the GMMs and the Bayesian method by analyzing real education data on children’s mathematical ability development. In Section  7, we draw conclusions. The Appendices present the technical details of our analyses.The density function of a growth mixture model is(1)f(yi)=∑k=1Kπkfk(yi),whereπkis the invariant class probability (or weight) for classk,(k=1,…,K), satisfying0≤πk≤1and∑k=1Kπk=1(e.g.,  McLachlan and Peel, 2000), andfk(yi)is the density for thekth class, in whichyiis aT×1vector of outcomes for participanti(i=1,…,N)following a latent growth model(2){yi=Ληi+ei,ηi=β+ξi,whereηiis aq×1vector of latent effects,Λis aT×qmatrix of factor loadings forηi,eiis aT×1vector of residual or measurement errors,βis aq×1vector of fix-effects, andξicaptures the variation ofηi.In the Extended Growth Mixture Models (EGMMs,  Muthén and Shedden, 1999),πkis not invariant any more for all individuals in classk. It is allowed to vary individually depending on covariates, so it is expressed asπik(xi). In this study, a probit link function is used(3){πi1(xi)=Φ(Xi′φ1),πik(xi)=Φ(Xi′φk)−Φ(Xi′φk−1),(k=2,3,…,K−1)πiK(xi)=1−Φ(Xi′φK−1),whereΦ(⋅)is the cumulative distribution function (CDF) of the standard normal distribution, andXi=(1,xi′)′with anr×1vector of observed covariatesxi. Note thatΦ(Xi′φk)=∑j=1kπij(xi)andΦ(Xi′φK)≡1.In both cases, a dummy variablezi=(zi1,zi2,…,ziK)′is used to indicate the class membership. If individualicomes from groupk,zik=1andzij=0(∀j≠k).ziis multinomially distributed (McLachlan and Peel, 2000, p. 7).To build models with non-ignorable missingness, we use selection models (Glynn et al., 1986; Little, 1993, 1995) instead of pattern mixture models (Little and Rubin, 2002), in part, because substantively it is more natural to consider the behavior of the response variable in the full target population of interests, rather than in sub-populations defined by missing data pattern (e.g.,  Fitzmaurice et al., 2008). For individuali, the complete-data likelihood function (see,  Celeux et al., 2006) of a selection model with auxiliary latent variables is expressed as(4)Li=∏k=1K[πik(xi)fk(ηi)fk(yi|ηi)fk(mi|zi,ηi,yi,xi)]zik,wheremi=(mi1,mi2,…,miT)′is a missing data indicator foryi, withmit=1whenyitis missing and 0 when observed. Letτit=p(mit=1)be the probability thatyitis missing, thenmit∼Bernoulli(τit).τitdepends on the non-ignorable missingness mechanisms. Lu et al. (2011) proposed Latent-Class-Dependent (LCD) missingness (see Fig. 1panel (a)) in whichτitis assumed to depend on latent class membership and observed covariates,(5)τit=Φ(zi′γzt∗+xi′γxt),whereγzt∗=(γzt1∗,γzt2∗,…,γztK∗)′is the coefficient vector for the class membershipzi, andγxt=(γxt1,γxt2,…,γxtr)′is ther×1coefficient vector for covariates. For LCD, the missingness is ignorable within each latent class.In reality, however, the missingness mechanism in each class may also be non-ignorable. Lu et al. (submitted for publication) illustrated some possible missingness. For example, in a given latent class, students may miss a test when they have few prior knowledge of that course (i.e., low latent initial level), or when their scores did not get much improved during the semester (i.e., small latent slope). In these cases, the missingness within a class actually depends on some random effects. We may call it Latent-Class-Random-Effect-Dependent (LCRED) missingness. According to different situations under consideration, LCRED can be further divided into more specific sub-types: Latent-Class-Intercept-Dependent (LCID) missingness, and Latent-Class-Slope-Dependent (LCSD) missingness. Strictly speaking, LCD is another special case of LCRED when the dependency on random effect is not significant. In addition to random effects, the missingness may also depend on potential outcomes that may be missing. For example, a student who feels he is not doing well in a test may be more likely to give up the test. By considering all these cases, therefore, we build three more non-ignorable missingness models in the framework of EGMMs: LCID, LCSD, and the Latent-Class-Outcome-Dependent (LCOD) missingness. They are illustrated in Fig. 1: (b)–(d).(1) For LCID,τitis a function of latent class, covariates, and latent individual initial levels, so we model it as follows.(6)τIit=Φ(zi′γzt∗+IiγIt+xi′γxt),whereIiis the latent initial levels for individuali,γItis the coefficient forIi, andγztandγxtare the same as in (5). A special case of the LCID is the Latent-Intercept-Dependent (LID) missingness in whichτitdoes not depend on the latent class.(7)τIit=Φ(γ0t+IiγIt+xi′γxt).(2) For LCSD,τitis a function of latent class, covariates, and latent individual slopes of growth, so it can be modeled as(8)τSit=Φ(zi′γzt∗+SiγSt+xi′γxt),whereSiis the latent slope for individuali, andγStis the coefficient forSi. Similarly, a special case is the Latent-Slope-Dependent (LSD) missingness.(9)τSit=Φ(γ0t+SiγSt+xi′γxt).(3) For LCOD,τitis a function of latent class, covariates, and potential outcomes that may be missing. We expressτitas(10)τYit=Φ(zi′γzt∗+yitγyt+xi′γxt),whereyitis the potential outcomes for individualiat timet, andγytis the coefficient foryit. And a special case of the LCOD is the Latent-Outcome-Dependent (LOD) missingness.(11)τYit=Φ(γ0t+yitγyt+xi′γxt).The effects of outliers can be reduced by using robust components. In this study we adopt thet-distribution, which is a natural replacement of normal distribution especially when data have outliers and heavy-tails (Zhang et al., 2013). As the model in Eqs. (1) and (2) has two levels, we propose three robust models as in Fig. 2.(1)t-Normal (TN) model in which the measurement errors aret-distributed and the latent random effects are normally distributed,(12){ei∼MtT(0,Θ,ν),ξi∼MNq(0,Ψ),whereMtT(0,Θ,ν)is aT-dimensional multivariatet-distribution with a scale matrixΘand degrees of freedomν, andMNq(0,Ψ)is aq-dimensional multivariate Normal distribution with a varianceΨ.(2) Normal-t(NT) model in which the measurement errors are normally distributed but the latent random effects aret-distributed,(13){ei∼MNT(0,Θ),ξi∼Mtq(0,Ψ,u).(3)t–t(TT) model in which both the measurement errors and the latent random effects aret-distributed,(14){ei∼MtT(0,Θ,ν),ξi∼Mtq(0,Ψ,u).By combining different missingness and distributions, the proposed models are flexible enough to cover a series of GMMs with a variety of missing data mechanisms and contaminated data.In this section we describe a full Bayesian approach. First, we utilize the data augmentation method (Tanner and Wong, 1987) to obtain the joint likelihood function of the selection model. The observed datayiobsare augmented with the missing datayimis, the latent random effectsηi, and the class membershipzi. The detailed likelihood functions are shown in Appendix A. Second, prior distributions are adopted. Table 1lists the priors and their hyper-parameters. We use uninformative priors so that all the hyper-parameters carry little information. Third, posterior distributions for unknown parameters are calculated. We use conditional posterior distributions instead of the joint posterior because the integrations of marginal posterior distributions of the parameters are hard to obtain explicitly for high-dimensional data. Appendix B lists the detailed posterior distributions. Fourth, with conditional posterior distributions, Markov chains for unknown parameters are generated by implementing a Gibbs sampling algorithm (Geman and Geman, 1984). Fifth, after burn-in periods, convergence tests (e.g., Geweke’szstatistics,  Geweke, 1992) are conducted to test the convergence of generated Markov chains. Sixth, if the chains pass convergence tests, they are viewed as from the joint distribution, and then statistical inference is conducted. Letθdenote an unknown parameter in the model, and(θ(1),θ(2),…,θ(S))be the converged Markov chains. Then acrossSMarkov iterations the posterior estimatesθˆ=∑s=1Sθ(s)/Swith a standard errors.e.(θˆ)=∑s=1S(θ(s)−θˆ)2/(S−1). Both percentile intervals and the highest posterior density intervals (HPD,  Box and Tiao, 1973) are provided. Seventh, model selection criterion is used to compare competing models and identify the best-fit model. The details are described in Section  4. Finally, the results obtained from the selected model are interpreted.In the Bayesian context, there are two versions of deviance, which are the Monte Carlo estimation of the deviance,D(θ)¯=Eθ|y[−2log(p(y|θ))]+C, and the estimate plugged-in deviance,D(θˆ)=−2log(p(y|Eθ|y[θ]))+Cfor some constantC. The difference betweenD(θ)¯andD(θˆ)comes from Jensen’s inequality (Casella and George, 1992). WhenD(θ)is convex, thenD(θ)¯≥D(θˆ), and whenD(θ)is concave, thenD(θ)¯≤D(θˆ). In the detailed framework of Bayesian GMMs with missing data, these two versions can be approximated byD(θ)¯=−2[1S∑s=1S{∑i=1N∑k=1Kzik(s)∑1=tT[(1−mit)likt(s)(y)+likt(s)(m)]}]andD(θˆ)=−2{∑i=1N∑k=1Kzˆik∑t=1T[(1−mit)likt(y|θˆ)+likt(m|θˆ)]},whereSis the number of iterations for converged Markov chains, andzik(s)is the class membership estimated at thesth iteration,zˆikis the posterior mode of class membership,likt(s)(y)andlikt(s)(m)are conditional loglikelihood functions (see,  Celeux et al., 2006) ofyitandmit, respectively. In particular, the loglikelihood function for the missing data indicatormitislikt(m)=mitlog(τit)+(1−mit)log(1−τit), whereτitis defined by Eqs. (5)–(10).Based on the two versions of deviance and Lu et al. (submitted for publication)’s article, new model selection criteria are proposed in the framework of Bayesian GMMs with missing data, and their definitions are shown in Table 2.In this section, three simulation studies are conducted to (1) demonstrate the accuracy of the estimates (including point estimates, standard error estimates, and confidence intervals) of the proposed robust GMMs with missing data using Bayesian methods, and (2) evaluate the performance of the proposed model selection criteria under different situations.The detailed design is presented in Table 3. Three simulation studies are designed such that the model complexity increases from study 1 to study 3. Within each study, complete data are generated first and then missing data are created on each occasion. Simulation factors include sample size, missingness pattern, measurement errors distribution, random effect distribution, and class separation (or distance,  Anderson and Bahadur, 1962). Study 1 focuses on robust GMMs in which class probabilities are fixed and the non-ignorable missingness do not depend on latent class membership. The true model is the “TN-XS” GMM witht-distributed measurement errors and missingness depending on both covariates and latent slope. In total, there are16×2=32conditions. Study 2 is designed for the robust Extended GMMs (EGMMs) in which class probabilities are not fixed and may depend on values of covariates. Also, the missingness may depend on latent class membership. The true model is the “TN-CXS” EGMM witht-distributed measurement errors and missingness depending on observed covariates, the latent slope, and latent class membership. Based on the findings in study 1, 5 competing models (TN-CXS, TT-CXS, NN-CXS, TN-CX, NN-CX) are selected to fit the data. In addition to the factors in study 1, two levels of class separation are considered, which are 2.7 (medium) and 1.7 (small). In total, there are5×2×2=20conditions. Study 3 focuses on the number of classes. GMMs with different classes are compared. Based on the performance in the previous two studies, we focus on the robust part with correct missingness. In total, there are 3 models × 4 numbers of classes = 12 conditions.In each of the simulation cases, parameter estimates are summarized based on 100 converged replications. We have also tried 1000 replications for a small set of conditions and found no noticeable differences in the results. Each replication generates at least 10,000 iterations for a burn-in period and Markov chains with 50,000 iterations for convergence testing and statistical inference.All the 32 summary tables in study 1 (Tables 1–32) are uploaded to the website of http://nd.psychstat.org/research/csda2013. As an example, the summary table of the true mode “TN-XS” whenN=1500is shown in Table 4, from which one can see that, except for the estimates of both degrees of freedom which are inflated due to mis-classification, the true model recovers parameters accurately. For true models, we further summarize all results in Table 5.Misspecified models perform not as good as the true model (shown in Tables 3–32). With estimates of slope (S) significantly lower than the true value 3, misspecified models may reject the true value with a high chance, and then conclusions will be severely misleading. Also, the misspecified measurement errors distribution leads to low coverages forvar(e),var(I), and class proportion. Note that among misspecified models, the model TT-XS performs similarly to the true model TN-XS because the only difference is between a normal distribution and atdistribution withdf≥50. This is because thetdistribution approach the normal distribution with the increase of degrees of freedom.Next, model selection proportions for 10 indices are listed in Table 6. The performances of the criteria in study 1 are ranked, from high to low, as CAIC, BIC, ssBIC, AIC, and DIC.First, the 20 summary tables in study 2 (Tables 33–52) are shown on http://nd.psychstat.org/research/csda2013. Again, for the true model the results are further summarized in Table 7, from which one can tell that the true model performs well in study 2. With the increase of the sample size or the distance of classes, both the point estimates and standard errors get more accurately estimated, and the statistics across all parameters are improved.Misspecified models cause biased estimates, part of which is shown in Fig. 3. The true value of the slope is 3, but when models are incorrectly assumed as CX missingness, our classification analysis shows the slopes of more than 50% individuals are incorrectly estimate as less than 1.5 for class separation = 1.7 and less than 2 for class separation = 2.7. When the missingness is misspecified, the slope coverages are very low, with the largest value 6% for class 2 in Table 50. Also, when the distribution is not correctly modeled, the coverages ofvar(e)are very low for both classes. For most misspecified models, the convergence rates are low. As in study 1, the model TT-CXS performs almost identically to the true model.Second, the selection proportions for each criterion are listed in Table 8. Most criteria can correctly identify the true model with high certainty.As all misspecified models with 4 classes do not converge well in study 3, we did not summarize the results. In addition to the cases with 2 classes (Tables 1, 3, and 7), the remaining 6 summary tables (Tables 53–58) are uploaded to the website. For the misspecified models with a single class, the intercept is estimated to be around the average of the true intercepts of two classes, and the variance of intercept is estimated much bigger than the true value. Their convergence rates are high, though, because the number of parameters are less than that of other GMMs with multi-class. For the misspecified GMMs with 3 classes, intercepts are mistakenly estimated to be around 5, 4, and 1. And the convergence rates for are very low, especially for TN-XS or TT-XS the rate is only 4%.Next, the selection proportions of criteria to pick the best-fit model are listed as in Table 9. According to Table 9, the Dhat group performs better than the Dbar group. Within the Dhat group, the order of correct selection proportions is, from high to low, CAIC, BIC, ssBIC, AIC, and DIC.The following conclusions can be drawn from the simulation studies. First, the Bayesian method can recover model parameters well as indicated by the small relative biases and the close to 95% average coverage probabilities. Second, with the increase of the sample size or distance between classes, (1) the relative biases get smaller, which means that estimates get closer to their true values, and (2) the average Bayesian SEs get closer to the empirical SEs, which means that standard errors become more accurate. Third, the small difference between the empirical SE and the average Bayesian SE demonstrates the Bayesian method used in the study can estimate the standard errors accurately. Fourth, the estimates of degrees of freedom are inflated due to the mis-classification in estimation. Fifth, incorrectly modeling the distribution will also lead to incorrect conclusions. Sixth, with the correct number of classes, almost all the criteria can correctly identify the true model with high certainty, with an order of correct selection proportion, CAIC, BIC, ssBIC, AIC, from high to low. When models having different numbers of classes, Dhat.CAIC, Dhat.BIC, Dhat.ssBIC, and Dhat.AIC can be good model selection criteria. Seventh, non-convergent Markov chains might be a sign of a misspecified model. The simulation studies also verify that ignoring the non-ignorable missingness will cause severely misleading conclusions which has been illustrated by previous literature (e.g.,  Little and Rubin, 2002; Zhang and Wang, 2012).In this section, we illustrate the application of the Bayesian robust GMMs with missing data through real data analysis. The same sample that has been analyzed in Lu et al. (2011) is used here. It is a mathematical ability growth sample from the NLSY97 survey (Bureau of Labor Statistics, US Department of Labor, 1997), including data collected fromN=1510adolescents yearly from 1997 to 2001 when each adolescent was administered the Peabody Individual Achievement Test (PIAT) Mathematics Assessment to measure their mathematical ability. Table 10shows the summary statistics for the data. Overall, the means of mathematical ability increased over time with a roughly linear trend. The missing data rates range from 4.57% to 9.47%, and the raw data show the missing pattern is intermittent. About half of the sample is female.Lu et al. (2011) assumed the data are normally distributed without any outliers. But the histograms drawn in Fig. 4indicate that there are outliers (marked by red circles) at all five grades. So robust methods are used in this study. Also, different non-ignorable missingness mechanisms are considered.The analysis is conducted following the steps in Table 11. In step 1, a tentative model (the TT-ignorable model) is fitted to the data. Gender is a covariate. The estimates of degrees of freedom oftfor both classes are 2.342 and 3.263 for measurement errors and 75.65 and 50.96 for random effects, which indicates that measurement errors aretdistributed while random effects are approximately normally distributed (i.e., a TN model). And then in step 2, 8 TN models with different missingness are fitted to the data. During estimation we use uninformative priors which carry little information for model parameters. A burn-in period is run first to make sure all the Markov chains are converged. For testing convergence, the history plot is examined and the Geweke’szstatistic (Geweke, 1992) is checked for each parameter. Two selected history plots are presented in Fig. 5. The Geweke’szstatistics for all the parameters are smaller than 1.96, which indicates converged Markov chains. To make sure all the parameters are estimated accurately, the next 50,000 iterations are then saved for data analysis. The ratio of Monte Carlo error (MCerror) to standard deviation (S.D.) for each parameter is smaller than or close to 0.05, which indicates parameter estimates are accurate (Spiegelhalter et al., 2003). The results are given in Tables 59–66 on the web site. Step 3, to compare models with different number of classes, two more models are fitted to the data, 3-class NN-X and 4-class NN-X. The results are shown in Tables 67 and 68 on the web site. Step 4, model selection criterion are used to compare the ten models. The indices are listed in Table 12. Suggested by Dhat.CAIC, Dhat.ssBIC, Dhat.BIC, and Dhat.AIC, without further substantive information, the TN-CXY model can be a good candidate of the best-fit model.Table 13provides the results of TN-CXY GMM model. It can be interpreted that (1) class 1 has a higher average initial level but a smaller average slope; (2) class 2 has larger variations for initial levels and slope; (3) the residual variance of class 2 is much larger than that of class 1; (4) in class 1 the initial level and the slope are significantly negatively correlated at the confidence level of 95%; (5) the missingness is not related to gender because none of the coefficients of gender are significant at theαlevel of 0.05; (6) at grade 11 adolescents in class 2 are more likely to miss tests than those in class 1 because the probit coefficient of class membership for grade 11 is significantly positive; and (7) at grades 8 and 10 students with higher potential scores are more likely to miss tests than the students having lower scores because the probit coefficients of the potential outcomesyat the two grades are significantly negative.

@&#CONCLUSIONS@&#
