@&#MAIN-TITLE@&#
Fitting multiple projective models using clustering-based Markov chain Monte Carlo inference

@&#HIGHLIGHTS@&#
Assignment of projective models becomes a problem of probabilistic inference through clustering.A Markov network formulation that models data points in terms of projective relationships in two views is introduced.An algorithm that fits multiple varieties to data points is specified using MCMC based inference.Use of a global energy measure to capture the quality of convergence.Comparative results indicate less susceptibility to parameter tuning and increased accuracy of convergence.

@&#KEYPHRASES@&#
Multiple model fitting,Clustering,Markov chain Monte Carlo,Two-view geometry,Markov random field,

@&#ABSTRACT@&#
An algorithm for fitting multiple models that characterize the projective relationships between point-matches in pairs of (or single) images is proposed herein. Specifically, the problem of estimating multiple algebraic varieties that relate the projections of 3 dimensional (3D) points in one or more views is predominantly turned into a problem of inference over a Markov random field (MRF) using labels that include outliers and a set of candidate models estimated from subsets of the point matches. Thus, not only the MRF can trivially incorporate the errors of fit in singleton factors, but the sheer benefit of this approach is the ability to consider the interactions between data points.The proposed method (CSAMMFIT) refines the outlier posterior over the course of consecutive inference sweeps, until the process settles at a local minimum. The inference “engine” employed is a Markov Chain Monte Carlo (MCMC) method which samples new labels from clusters of data points. The advantage of this technique pertains to the fact that cluster formation can be manipulated to favor common label assignments between points related to each other by image based criteria. Moreover, although CSAMMFIT uses a Potts-like pairwise factor, the inference algorithm allows for arbitrary prior formulations, thereby accommodating the needs for more elaborate feature based constraints.

@&#INTRODUCTION@&#
The detection of multiple projective varieties has significant practical ramifications in the fields of computer vision, robotics, pattern recognition and architecture. A significant category of such varieties are lines, conics and projectivities (homography tensors) [1,2] which can lead to the discovery of planar surfaces and objects within scenes [3–5]. In terms of motion, multifocal tensors [6,7] can be used to detect multiple moving objects, camera movement and degenerate configurations [8,9]. The detection of such models in one or more views becomes an arduous task afflicted not only by the quality (or the lack thereof) of the point matches but also by the lack of prior knowledge on the nature of the outliers which may be attributed to a number of reasons (occlusions, multiple motions, changes in lighting, etc.). Several methods involving least squares fitting or iterative optimization of cost functions have been proposed for the estimation of the parameters of these varieties. For a thorough treatment on these methods, the reader is deferred to Hartley and Zisserman [8].Parameter estimation yields an optimal solution in terms of some distance measure with respect to all data points. This however implies that all data points indeed belong to the chosen model, an assumption which is usually not true. Thus, the task of model fitting typically includes inlier detection, given that a certain type of model exists in the data. By far, the most popular tool to achieve this is random sampling consensus (RANSAC) [10]. The algorithm has been widely employed, not only because of its simplicity, but also because of its remarkably accurate results in a wide variety of model fitting applications.One of the early works that attempt to capture the best model amongst the set of point matches is Phillip Torr's MLESAC [11], a RANSAC variant that attempts to find the single optimal subset of matching point-pairs which minimize the average squared error over the entire set of matches. In the same spirit, but with a rather different approach to candidate model computation, Ondrej Chum proposed DEGENSAC [12], yet another RANSAC variant trying to find a best-fit homography for a scene that contains a dominant plane. The algorithm uses RANSAC to compute the homography of the dominant plane with respect to the two views and thereby estimates the fundamental matrix using the plane induced (virtual) parallax trick [13].While both MLESAC and DEGENSAC silently assume the existence of some unique optimal model fitting the correspondences, a method proposed by Tong [9] is not constrained by any such assumption, as it attempts to discover multiple fundamental matrices for an arbitrary set of correspondences by propagating information through tensor fields in 4D. To compute the models however, RANSAC is employed in a cascaded manner: Each model is calculated by using the set of outliers of the previous RANSAC as input to a new RANSAC execution. Cascaded RANSAC execution (also referred to as “sequential RANSAC” in literature) has been widely employed in order to cope with the existence of multiple models [9,12,14].The MultiRANSAC algorithm, an actual generalization of RANSAC for multiple models, was introduced by Zulliani et al. [15]. As the name implies, the algorithm generalizes the RANSAC core concept to fitting a predetermined number of models to the data. MultiRANSAC expands the minimal sampling sets (MSS) in a cascaded manner, but the consensus sets (CS) are computed based not only on the expanded MSSs but also in terms of the CSs estimated in the previous sets. The authors present results therein superior to the ones obtained with cascaded RANSAC executions.An algorithm that builds on the MultiRANSAC concept by introducing a clustering procedure amongst the data points following the initial determination of consensus sets is J-linkage by Roberto Toldo and Andrea Fusiello [16]. J-linkage determines the initial consensus sets without excluding overlaps. In other words, data points may belong to more than one consensus sets. Multiple participants (data points) are joined to form the initial clusters and the process thereafter evolves by unifying clusters with the smallest Jackard distance, thereby yielding new model parameters from their respective supports (i.e., the sets of points used to estimate the model parameters). Unlike MultiRANSAC, J-linkage can automatically conclude with the actual number of models throughout successive clustering-and-merging steps.Recently, Isack and Boykov presented PeARL, an algorithm that minimizes an energy function comprising the errors of fit and simple pairwise interactions for multiple geometric models [17]. To the best of our knowledge, this is the closest analog to the method introduced in this paper.In PeARL, Isack and Boykov argue based on their results, that greedy approaches to model updates via the estimation of their respective consensus sets used in J-linkage clustering, RANSAC, MultiRANSAC, or even Hough transform [18] with mean-shift [19] leave plenty of margin for model misclassifications, especially for increased levels of noise in the data. Instead of the rather greedy approaches for refinement and/or unification of consensus sets, they propose the minimization of the following energy function:(1)EX=∑rerXr+T∑sϵNrwr,sJXr≠Xswhere eris some error function regarding data point r and its respective assigned model label Xr, T is a positive constant,Jis the indicator function,Nris the Markov blanket (neighborhood) of r, wr,sis a distance-related constant such that, wr,s=exp(−‖r−s‖2/σ2) and ‖r−s‖ is the distance between r and s. The algorithm samples random groups of points and estimates the initial parameters of a number of candidate models. It then performs repeated energy minimization sweeps by re-estimating model parameters from the minimization results. The energy function is minimized using graph-cuts [20], a method for efficient approximation of local minima. The results reported by the authors are clearly in favor of the method as opposed to J-linkage, MultiRANSAC and mean-shift in Hough space.The energy minimization in PeARL implies probabilistic inference over a Markov network in which pairwise interactions are obtained by imposing a graph structure to the data points. In such a framework, one is able to incorporate several factors that characterize not only the distribution of the error of fit, but also the interactions between the point matches, such as distance, local gradient histogram similarities, etc., in order to make inference more “knowledgeable” of other sorts of aspects of visual content. It is worth noting here that a key limitation of graph-cuts is that they can be applied only to specific types of pairwise terms (specifically, ones that are metrics or semi-metrics). This clearly excludes a wide variety of priors which are likely to significantly improve optimization.In this paper, we generalize the notion of energy minimization by restating the problem in terms of obtaining the joint assignment that maximizes the following probability:(2)PXM∝∏rφrXr∏sϵNrψr,sXrXswhereMis a set of candidate (or proposed) models, Xris the label of data point r, φris a singleton factor associated with r and ψr,sa pairwise factor associated with r and s. The label Xrassumes values inM∪{o} where o denotes an outlier. Please note here that, unlike PeARL, in our formulation outliers are a valid label just like any candidate model. The existence of outliers in the label set can generally prevent the inference algorithm from overestimating “bad” models. As will be shown in the following sections, the singleton factor φris modeled in such a way as to approximate the posterior of Xrgiven the set of ground-truth models (i.e., the most suitable set of models for the data). When this approximation is not realistic, outliers should appear in greater numbers. Finally, we introduce a slightly more elaborate prior than the indicator function which penalizes non-uniformity of labels by considering the distance between points in the context of all edge lengths in the MRF (whereas PeARL weighs the prior with a function of absolute distance).Since the proposed distribution can be fairly generic especially in terms of choice of prior, we propose the generalized Swendsen–Wang MHMCMC algorithm (henceforth, GSW) by Barbu and Zhu [21] as the preferred inference “engine”. The results provided in their paper suggest that GSW performs generally better than graph-cuts. Most importantly, GSW is a clustering algorithm at its core and inference progresses by reorganizing the connectivity on the edges of an underlying graph structureG=V,E, whereVis a set of vertices corresponding to data points andEis a set of edges. An activation probability pηis assigned to each edge η∈E. Thus, the graph not only provides a neighborhood system for the MRF, but also a framework of topological connections for the GSW algorithm. Formally, our sampling strategy is equivalent to sampling from the joint distribution of (2), augmented with a set of Boolean variables, U={uη: uη=J(η is "ON"), η∈E}, corresponding to the activation status of the edges (“ON” or “OFF”) and imposing a hard constraint for same label assignment to all variables connected with an edge that is switched “ON”:(3)PX,UM∝∏rφrXr∏sϵNrψr,sXrXs∏η=ij∈E1+uηJΧi=Χj−1pηJΧi=Χj1−pη1−JΧi=Χjwhere 〈i,j〉 denotes the edge between the ithand jthvertices (data points). It can be easily seen that the marginal over U yields back the joint distribution of (2). Note here that the separate product indexes on the right in (3) allow for different structures between the MRF and the graph.

@&#CONCLUSIONS@&#
