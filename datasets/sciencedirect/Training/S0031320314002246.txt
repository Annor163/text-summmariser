@&#MAIN-TITLE@&#
Incremental partial least squares analysis of big streaming data

@&#HIGHLIGHTS@&#
We propose a two-stage Incremental PLS (IPLS) dimension reduction method.IPLS has low time complexity, linear with both the numbers of samples and features.Empirical results show IPLS performs better than some state-of-the-arts methods.

@&#KEYPHRASES@&#
Feature extraction,Incremental learning,Large-scale data,Partial least squares,Streaming data,

@&#ABSTRACT@&#
Incremental feature extraction is effective for facilitating the analysis of large-scale streaming data. However, most current incremental feature extraction methods are not suitable for processing streaming data with high feature dimensions because only a few methods have low time complexity, which is linear with both the number of samples and features. In addition, feature extraction methods need to improve the performance of further classification. Therefore, incremental feature extraction methods need to be more efficient and effective. Partial least squares (PLS) is known to be an effective dimension reduction technique for classification. However, the application of PLS to streaming data is still an open problem. In this study, we propose a highly efficient and powerful dimension reduction algorithm called incremental PLS (IPLS), which comprises a two-stage extraction process. In the first stage, the PLS target function is adapted so it is incremental by updating the historical mean to extract the leading projection direction. In the second stage, the other projection directions are calculated based on the equivalence between the PLS vectors and the Krylov sequence. We compared the performance of IPLS with other state-of-the-art incremental feature extraction methods such as incremental principal components analysis, incremental maximum margin criterion, and incremental inter-class scatter using real streaming datasets. Our empirical results showed that IPLS performed better than other methods in terms of its efficiency and further classification accuracy.

@&#INTRODUCTION@&#
The rapid development of large-scale data acquisition and storage techniques means that there are increasing requirements for mining streaming data [1–4]. Streaming data differ from conventional data in the following ways: (1) the data are not all available at once because they arrive as continuous streams; (2) the order in which the data elements arrive cannot be controlled; (3) the data streams are potentially unending; (4) an element from a data stream cannot be retrieved repeatedly, i.e., each sample can be processed only once; and (5) the scale of the data is vast. These characteristics mean that the processing of streaming data is a challenging problem. Feature extraction has been one of the main techniques used to facilitate the processing of large-scale data. However, normal feature extraction methods cannot meet the requirements of streaming data because they require that all the samples are loaded into memory.To address this problem, the most common method is to modify feature extraction algorithms into incremental approaches [5]. For example, the extension of traditional principal components analysis (PCA) into incremental PCA (IPCA) has been studied widely in the last few decades [6,7]. Many types of IPCA algorithms have been proposed, where the main difference is the incremental representation of the covariance matrix. Most are based on singular value decomposition (SVD), which is time consuming because SVD updating is required when each single instance arrives. In addition to IPCA, other incremental methods have been proposed. For example, Pang et al. proposed an incremental linear discriminant analysis (ILDA) algorithm for online face classification where the scatter matrices are updated incrementally [8]. Hiraoka et al. designed a gradient descent incremental linear discriminant analysis (GDILDA) algorithm, which is based on the theory of neural networks [9]. Lu designed an incremental complete linear discriminant analysis (ICLDA) algorithm for face recognition, where incremental QR decomposition is used to obtain the orthonormal bases of the range and the null spaces of the within-class scatter matrix [10]. Some nonlinear incremental feature extraction methods have also been reported. Law and Jain proposed an incremental nonlinear mapping algorithm by modifying the ISOMAP algorithm [11]. Guan et al. reported an online nonnegative matrix factorization (ONMF) with robust stochastic approximation [12].All of these methods are suitable for application to small-scale streaming data where the model is incrementally built as the training samples arrive, thus the feature dimension is not a problem. However, they cannot meet the requirements of large-scale streaming data applications because their computation complexity is relatively high compared with the number of features.In the last few years, a few highly efficient incremental feature extraction methods have been proposed. A well-known fast version of IPCA with convergence proof is called candid covariance-free IPCA (CCIPCA), which is not based on SVD and it does not need to reconstruct the covariance matrix at each iteration step [13]. Yan et al. [14] proposed an incremental maximum margin criterion (IMMC) algorithm based on the maximum margin criterion (MMC) [15], which computes the difference in the between-class and within-class scatter matrices incrementally. Based on previous work [14], Yan et al. designed an incremental inter-class scatter (IIS) algorithm that only optimizes the between-class distances [16,17].CCIPCA, IMMC, and IIS satisfy the requirements for processing streaming data, but their time complexity is not linear with the numbers of instances and features. However, there are few highly efficient models and more feature extraction methods are still needed because the performance of existing methods is not satisfactory for streaming data in terms of the efficiency and further classification accuracy.Partial least squares (PLS) is a wide class of methods for modeling the relations between sets of observed variables using latent variables [18]. PLC comprises regression tasks and dimension reduction techniques. As a feature extraction method, PLS is known to be effective for classification [19–23]. For example, Barker and Rayens demonstrated the superiority of PLS to PCA as a dimension reduction method in a formal statistical manner [24], Liu and Rayens also illustrated its superiority using real datasets [25]. PLS has also been compared with some of state-of-the-art dimension reduction methods [26,27]. All of these studies have demonstrated the outstanding performance of PLS. However, the application of PLS-based dimension reduction to streaming data is still an open problem. In this study, we propose an incremental partial least squares (IPLS) algorithm and we compare the performance of our proposed method with some state-of-the-art incremental feature extraction methods using real streaming data.It should be noted that Helland et al. [28] proposed an incremental PLS regression method called recursive partial least squares (RPLS). Furthermore, improved methods based on RPLS have been developed to solve the online regression problem [29,30]. However, RPLS-based methods are not suitable for dimension reduction because the projection directions are not maintained during the incremental learning process.This paper is organized as follows. Section 2 briefly introduces some related methods, such as IPCA, IMMC, IIS, and PLS. Our proposed method is explained in detail in Section 3. The experimental settings, empirical results, and discussion are presented in Section 4. Finally, our conclusions are given in Section 5.At present, linear feature extraction approaches are used widely in real tasks such as document classification and face recognition. These methods aim to find a projection matrix that can efficiently project the data from the original high-dimensional feature space to a much lower-dimensional representation under a particular criterion. Different criteria will yield different subspace learning algorithms with different properties. PCA and linear discriminant analysis (LDA) are two of the most widely used linear subspace learning approaches. Recently, PLS analysis, which is an efficient and robust subspace learning approach, has also been applied to many real tasks, with excellent performance.PCA is the most popular feature extraction method and it is an unsupervised technique that tends to find a set of orthonormal basis vectors that maximizes the variance over all the data [31–34]. Suppose that the data sample pointsx(1),x(2),…,x(n)are p-dimensional centralized vectors. The goal of PCA is to find a subspace where the basis vectors correspond to the directions with the maximal variances. Let us denoteC=1/n∑i=1nx(i)x(i)Tas the covariance matrix of sample data. We define the objective function as(1)J(W)=trWTCW.Then, PCA aims to maximize the objective function J(W) in a solution spaceHp×K={W∈Rp×K,WTW=I}. It has been proved that the column vectors of W are the K leading eigenvectors of the covariance matrix C. PCA projects the original data into a K-dimensional (K⪯¡p) subspace. The new low-dimensional feature vector is computed directly as WTx. Although PCA is the optimal solution in terms of the minimum reconstruction error, it is not optimal for classification because no class information is used when computing the principal components.The computational cost of PCA is attributable mainly to SVD [31] processing, which has a time complexity ofO(η3), whereη=min(p,n). Thus, it is difficult or even impossible to perform PCA with a large-scale dataset using high-dimensional representations.PCA is a batch algorithm, thus it does not meet the requirements of many real-world problems. IPCA algorithms have attracted much attention in recent decades [6,7,13,35]. Many SVD-based IPCA algorithms have been proposed where the main difference is in terms of the incremental representation of the covariance matrix [6,7]. However, these IPCA methods are not suitable for applications with huge streaming data because SVD-based updating is too time consuming when instances arrive at high speed [17].A rapid and practical version of IPCA with a proof of convergence is called CCIPCA, which is not based on SVD and it does not need to reconstruct the covariance matrix at each iteration step (thus it is covariance-free) [13]. CCIPCA was motivated by the concept of statistical efficiency (the estimate has the smallest variance given the observed data). It maintains the number of samples and directly updates the eigenvectors incrementally, which yields efficient estimates for some well-known distributions (e.g., Gaussian). The time complexity of CCIPCA is linear with the number of samples and the number of features, and it converges rapidly on high-dimensional data.Like traditional PCA, CCIPCA requires that the mean value of the training samples is fixed or that a zero-mean is an inherent feature of the dataset. In applications, CCIPCA usually adopts an approximate centralization process to meet the zero-mean requirement, where only the current sample is centered correctly, whereas all the historical data are not. In most cases where the inherent mean value of the training samples converges, the approximate centralization process works sufficiently well. The detail deduction process used by CCIPCA is not given here due to space limits, but the detailed algorithm is given in Algorithm 1. Note thatx˜(n)denotes the nth training sample and x(n) is the corresponding centered sample.Algorithm 1CCIPCA algorithm.Input:x˜(n),n=1,2,…// streaming dataK // target dimensionl // amnesic parameterOutput: Projection matrix W1: begin2:x¯(1)=x˜(1),u1(1)=x˜(1);3: forn=2,3,…do4:x¯(n)=n−1nx¯(n−1)+1nx˜(n);5:x1(n)=x˜(n)−x¯(n);6:U(n)=[];7:fori=1,2,…,min(n,K)do8:ifi=nthen9:ui(n)=xi(n);10:else11:ui(n)=n−1−lnui(n−1)+1+lnxi(n)xi(n)Tui(n−1)∥ui(n−1)∥;12:xi+1(n)=xi(n)−xi(n)Tui(n)∥ui(n)∥ui(n)∥ui(n)∥;13:end if14:U(n)=[U(n),ui(n)];15:end for16: end for17: W=normalized U(n);18: endLDA is used to find a lower-dimensional space that can allocate samples to different classes. LDA aims to maximize the Fisher criterion, i.e., an objective function:(2)J(W)=|WTSbW||WTSwW|Sb=∑i=1cpi(mi−m)(mi−m)TSw=∑i=1cpiEx∈ci{(x−mi)(x−mi)T},where Sband Sware called the interclass scatter matrix and intraclass scatter matrix, respectively. E denotes the expectation andpi=ni/nis the prior probability that a sample belongs to class i. W is obtained by solvingW⁎=argmaxJ(W)in solution spaceHp×K={W∈Rp×K,WTW=I}, i.e., the following generalized eigenvalue decomposition problem:Sbw=λSww.Some ILDA algorithms have been proposed that meet the requirements of streaming data [8,9]. LDA and ILDA explicitly utilize the label information of the samples, which is suitable for classification problems. However, there are at mostc−1nonzero eigenvalues, thus the upper bound of K isc−1, and at leastp+csamples are required to ensure that Swis not singular, which limits the applications of LDA and ILDA.MMC [15] is a supervised feature extraction algorithm, which is similar to LDA. Based on the same representation as LDA, MMC aims to maximize the following target function:(3)J(W)=tr(WT(Sb−Sw)W),whereW∈Rp×K,WTW=I.MMC is easier to compute than LDA because it does not include an inverse matrix operation. The projection matrix W is obtained by solving the eigenvalue decomposition problem:(Sb−Sw)w=λw.Similar to other batch feature extraction approaches, MMC is not efficient for large-scale data or streaming data problems. The IMMC [14] algorithm was proposed to solve this problem, but IMMC is not stable because the criterion matrix is not determined as nonnegative in some cases. IMMC borrowed the idea of the CCIPCA algorithm proposed in [13] and it directly updates the eigenvectors incrementally, which makes IMMC run very rapidly. Based on IMMC, Yan et al. designed the IIS algorithm, which only optimizes the interclass matrix Sb[16,17]. IMMC and IIS both have linear time complexity with the number of instances and features.PLS is a class of techniques for modeling the relations between blocks of observed variables using latent variables. The underlying assumption of PLS is that the observed data are generated by a system or process that is driven by a small number of latent (not directly observed or measured) variables. Therefore, PLS aims to find uncorrelated linear transformations (latent components) of the original predictor variables that have high covariance with the response variables. The latent components of PLS are similar to the principal components of PCA, both of which are regarded as the extraction of linear features. Based on these latent components, PLS predicts the response variablesy, i.e., the regression task, and reconstructs the original matrix X, i.e., the data modeling task, at the same time.A key point of PLS is the construction of component t by projecting X on the weight w as t=Xw. The classical criterion of PLS aims to sequentially maximize the covariance between the responseyand latent components. Some variant PLS approaches have been proposed to solve this problem [18]. By ignoring the minor differences among these algorithms, we next provide a brief description of the most frequently used approach, PLS1 [36].PLS1 determines the first latent componentt1=Xw1by maximizing the covariance betweenyandt1under the constraint of∥w1∥=1. The corresponding objective function is(4)w1=argmaxwTw=1(Cov(Xw,y)).We then introduce a Lagrange function as(5)L(w,λ)=wTXTy−λ(wwT−1),where λ is a Lagrange multiplier. At the saddle point, the derivatives of L must vanish, thereby leading to(6)XTy=2λw.Thus, the exact solution of w is given as(7)w1=XTy∥XTy∥.Note that the original data X and y are both assumed to have been centered in this subsection.To extract other latent components, PLS1 models the residual matrices, which could not be modeled by previous latent variables, as the new X andysequentially. To obtain the residuals, PLS1 deflates the matrices X andyby subtracting their rank-one approximations based ont1:(8)E1=X−t1(t1Tt1)−1t1TX,where E1 is used as the new X to extract the next latent variable t2. Note that the deflation of y is not compulsory (since the results are the same) [18,36], thus we do not include it in our illustration.The extraction and deflation processes are used alternatively, thus some information is represented and one latent component is extracted during each PLS iteration. The iteration time K is also the number of components and it is the only PLS parameter that is fixed by the user, or it is determined by the model selection method. In general, the maximal value of K is the rank of matrix X, which has non-zero covariance withy. The iteration process of PLS dimension reduction is summarized in Algorithm 2.Algorithm 2PLS1 dimension reduction.Input:Feature set XTarget variableyTarget dimension KOutput:Projection matrix W1:E0=X,W=[];2: fori=1 to Kdo3:wi=Ei−1Ty/∥Ei−1Ty∥;4:W=[W,wi];5:ti=Ei−1wi;6:Ei=Ei−1−ti(tiTti)−1tiTEi−1;7: end for8: Output the projection matrix W.Clearly, traditional PLS operates as a batch process and it is not practical for real-world streaming data. In this section, we propose a highly scalable incremental feature extraction algorithm based on the PLS algorithm, which we call the IPLS algorithm.As in the above section, suppose that the sample vectors are acquired sequentially,{x˜(1),y1},{x˜(2),y2},…, possibly infinite. Eachx˜(n),n=1,2,…, is a p-dimensional vector and p may be 10,000 or higher, and ynis its corresponding class label, the value of which is 1 for a positive sample and−1for a negative sample. Unless stated otherwise, a variable S at step n is denoted by S(n) in the remainder of this study.PLS assumes thatx˜(n)and ynhave zero means across samples, but the means change dynamically as huge volumes of data are received. As mentioned above, the deflation of y is not technically a requirement during the iterations of PLS1 and neither is the centralization of y. Thus, we focus on the centralization ofx˜(n)in the present study. Letx¯(n)denote the mean ofx˜(n)at step n, then the ith centralized samplexn(i)at step n is given as(9)xn(i)=x˜(i)−x¯(n)x¯(n)=1n∑i=1nx˜(n),wherex¯(n)is computed incrementally as(10)x¯(n)=n−1nx¯(n−1)+1nx˜(n).A critical problem of centralization across samples with streaming data is that the arriving instances should be centralized by the current mean, but all of the historical centralized samples also need to be updated. However, the update is difficult to perform since there are too many historical streaming instances to be stored. This is why many incremental methods, such as CCIPCA, adopt an approximate centralization process to meet the zero-mean requirement. In most cases where the inherent mean value of the training samples converges, the approximate centralization process works sufficiently well. However, exact historical centralization is obviously more suitable when the intrinsic data model is not stable or the sample number is small.Thus, we propose a novel method for updating the feature extraction model without explicitly re-centralizing the historical data. Based on the solution of PLS1 in (6), we define a new variablev=XTy=2λw, and the estimate of v during step n is given as(11)v(n)=X(n)Ty(n)=∑i=1n−1yixn(i)+ynxn(n)=∑i=1n−1yi(xn−1(i)−Δ(n))+ynxn(n)=v(n−1)−(n−1)y¯(n−1)Δ(n)+ynxn(n),whereΔ(n)is defined as the increment of the mean vectorx¯(n)andy¯(n)is the mean of y at step n:(12)Δ(n)=x¯(n)−x¯(n−1),y¯(n)=1n∑i=1nyi.At initialization, we setv(0)=0. After the value of v(n) is determined, the projection direction w(n) is computed directly asw(n)=v(n)/∥v(n)∥.Eq. (11) only estimates the first projection direction, thus we need to find a different method to compute the other higher order projection directions incrementally. Based on similar incremental feature extraction methods, such as CCIPCA [13], IMMC [14], and IIS [16,17], we propose a convenient method for generating “observations” only in a complementary space to compute the higher order projection directions, which is deduced from the property that the projection directions must be orthogonal to each other.To compute the j+1th projection direction, we simply need to subtract the projection of x(n) on the estimated jth latent component from the data, which yields a new x(n) (the residual) for the next iteration.(13)xj+1(n)=xj(n)−xj(n)Tvj(n)∥vj(n)∥vj(n)∥vj(n)∥,wherex1(n)=x(n). This avoids the time-consuming orthonormalization process and the orthogonality ofvj(n),j=1,2,…,Kis always ensured.However, this less costly method is not suitable for our incremental PLS algorithm. Eq. (13) is regarded as a type of deflation based on the projection direction w. The deflation of PLS in (8) is based on the latent variable t, where t=Xw. Both (8) and (13) guarantee the orthogonality ofwj,j=1,2,…,K, but the residual matrices of X are different. Therefore, the projection directions obtained are also different.Unfortunately, we cannot perform exactly the same deflation as (8) using a one-pass algorithm because the dimension of t is the number of instances, which may be infinite with streaming data. Therefore, we need to design a novel method that differs from other incremental feature extraction methods.It is well-known that PLS1 is closely related to the Krylov sequence, which has been demonstrated previously [36]. It has been established that the following equivalences exist between the spaces spanned by the PLS vectors and the spaces spanned by the Krylov sequence:(14){W}={s,Cs,C2s,…,CK−1s},wheres=XTyis the cross-product vector andC=1/nXTXis the covariance matrix. The exact PLS projection directions are computed from a Gram-Schmidt orthonormalization of the Krylov sequence [37]. We represent this as(15)W=[s,Csâ§¹s,C2sâ§¹{s,Cs},…,CK−1sâ§¹{s,Cs,…,CK−2s}],where the notionfâ§¹{g,h,⋯}refers to the components of f that are orthogonal to the space spanned by{g,h,⋯}, i.e., the residual of f after multiple regression on{g,h,⋯}.Based on (15), only the covariance matrix C and the leading vector v1 are required to compute the whole projection direction sequence. This computation is not related directly to the original streaming data, which reduces the computational costs greatly because only the variables C and v1 need to be updated when each new sample arrives. The overall projection direction sequence is computed when necessary.Although the time complexity of incremental updating of C is low, the storage requirements for C are too great if the original feature space is huge. In our experiment, the size of C is more than 3GB since the feature dimension is more than 20k. Fortunately, we can reconstruct the approximate covariance matrix using a few PCA principal components, which can be computed efficiently with the CCIPCA algorithm. In this manner, the time and space complexity are suitable.Given L leading PCA principal components uj,j=1,2,…,Land the first projection direction v1, the second projection direction v2 is computed directly as(16)v2=Cv1≈1n∑j=1L1∥uj∥uj(n)uj(n)Tv1,v2=v2−v2Tv1∥v1∥v1∥v1∥,where∥uj∥is the corresponding eigenvalue of uj. When the number L of principal components in the PCA model is of full rank, v2 computed by (16) is identical to that derived by the traditional PLS1 algorithm. Our experiments showed that the top 20 PCA principal components are sufficient.By embedding the CCIPCA algorithm naturally into our IPLS algorithm, we obtain the overall computational process. The detailed IPLS algorithm is shown in Algorithm 3.Algorithm 3IPLS algorithm.Input:{x˜(n),yn},n=1,2,…// streaming dataK // number of dimensions extractedL // number of PCA componentsl // amnesic parameter of CCIPCAOutput:Projection matrix W1: begin2:x¯(1)=x˜(1),y¯(1)=y1;3:v1(1)=0,u1(1)=x˜(1);4: forn=2,3,…do5:x¯(n)=n−1nx¯(n−1)+1nx˜(n);6:y¯(n)=n−1ny¯(n−1)+1nyn;7:x1n(n)=x˜(n)−x¯(n);8:Δ(n)=x¯(n)−x¯(n−1);9:v1(n)=v1(n−1)−(n−1)y¯(n−1)Δ(n)+ynx1n(n);10:fori=1,2,…,min(n,L)do11:ifi=nthen12:ui(n)=xin(n);13:else14:ui(n)=n−1−lnui(n−1)+1+lnxin(n)xin(n)Tui(n−1)∥ui(n−1)∥;15:xi+1n(n)=xin(n)−xin(n)Tui(n)∥ui(n)∥ui(n)∥ui(n)∥;16:end if17:end for18: end for19:V(n)=[v1(n)];20:w=v1(n);21: fori=2,3,…,Kdo22:w=1n∑j=1L1∥uj(n)∥uj(n)uj(n)Tw;23:vi(n)=w;24:forj=1,2,…,i−1do25:vi(n)=vi(n)−vi(n)Tvj(n)∥vj(n)∥vj(n)∥vj(n)∥;26:end for27:V(n)=[V(n),vi(n)];28: end for29: W=normalizedV(n);30: endThe IPLS algorithm can be divided into two rough stages, i.e., the online processing stage and the finalization stage. In the online processing stage, a preprocessing procedure is conducted initially, where the means, the centered vector, andΔ(n)are updated as each instance arrives. Next, the leading PLS projection vector v1 is computed incrementally as (11). Finally, the PCA eigenvectors uiare extracted using the CCIPCA algorithm, wherei=1,2,…,L. The finalization stage is needed only when the projection matrix is required. IPLS extracts the overall K projection vectors sequence from the leading vector v1 and the top L PCA eigenvectors as (16).The time complexity for IPLS to train N input samples isO(NLp+K2p), where p is the dimension of the original data space, L is the PCA principal component number, and K is the target dimension, which is linear with the original data dimension N and p. In most cases, the time complexity is only O(NLp) sinceK⪯¡N. Furthermore, when handling each input sample, IPLS only needs to maintain the learned leading projection direction, some PCA projection weight vectors, and several first-order statistics of the previous samples, such as the mean and sample number. Thus, IPLS can handle large-scale streaming data.It should be note that PLS is a supervised feature extraction method, so it is not suitable when all of the samples belong to the same class, because the covariance of the label vector y relative to any other variable is zero. Therefore, the projection directions computed by IPLS are invalid in this situation.

@&#CONCLUSIONS@&#
Incremental feature extraction is an effective technique that facilitates data mining from large-scale streaming data. PLS is known to be a powerful feature extraction technique for the classification of traditional data. Therefore, we propose the IPLS method to exploit the advantages of both PLS and incremental feature extraction, thereby improving the generalization performance of streaming data analysis. The leading projection direction extracted by IPLS agrees with that using traditional PLS because exact historical centralization is applied by IPLS. The extraction of high order projection directions by a deflation scheme is a distinguishing feature of PLS, which is difficult to modify incrementally. Based on the equivalence that exists between the space spanned by the PLS vectors and the space of the Krylov sequence, we relate the deflation process to the leading projection direction and the covariance matrix, rather than the original streaming data, because the covariance matrix can be estimated by IPCA of the streaming data. CCIPCA is embedded naturally into the IPLS algorithm, which has a high discriminatory capacity and low time complexity, where it is linear with both the number of samples and the number of features.We compared the performance of IPLS with state-of-the-art methods such as CCIPCA using real streaming data in handwritten digit recognition and text classification tasks. Our empirical results showed that IPLS outperformed other methods in terms of its efficiency and further classification accuracy.The rapid emergence of large-scale streaming data applications means that increasing amounts of streaming data from scientific applications will have nonlinear properties and they may shift with time, thus linear methods such as IPLS and CCIPCA have limitations when handling this type of data. Therefore, highly efficient and nonlinear incremental feature extraction methods will be in great demand in the future.None declared.