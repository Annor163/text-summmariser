@&#MAIN-TITLE@&#
A link-bridged topic model for cross-domain document classification

@&#HIGHLIGHTS@&#
We propose a Link-Bridged Topic model for cross-domain document classification.LBT utilizes an auxiliary link network to discover the co-citation relationship.LBT combines the content information and link structures into a graphical model.LBT outperforms both multi-view learning and single-view transfer baselines.

@&#KEYPHRASES@&#
Cross-domain,Document classification,Transfer learning,Auxiliary link network,

@&#ABSTRACT@&#
Transfer learning utilizes labeled data available from some related domain (source domain) for achieving effective knowledge transformation to the target domain. However, most state-of-the-art cross-domain classification methods treat documents as plain text and ignore the hyperlink (or citation) relationship existing among the documents. In this paper, we propose a novel cross-domain document classification approach called Link-Bridged Topic model (LBT). LBT consists of two key steps. Firstly, LBT utilizes an auxiliary link network to discover the direct or indirect co-citation relationship among documents by embedding the background knowledge into a graph kernel. The mined co-citation relationship is leveraged to bridge the gap across different domains. Secondly, LBT simultaneously combines the content information and link structures into a unified latent topic model. The model is based on an assumption that the documents of source and target domains share some common topics from the point of view of both content information and link structure. By mapping both domains data into the latent topic spaces, LBT encodes the knowledge about domain commonality and difference as the shared topics with associated differential probabilities. The learned latent topics must be consistent with the source and target data, as well as content and link statistics. Then the shared topics act as the bridge to facilitate knowledge transfer from the source to the target domains. Experiments on different types of datasets show that our algorithm significantly improves the generalization performance of cross-domain document classification.

@&#INTRODUCTION@&#
Traditional machine learning approaches make a basic assumption that the training and test data should be drawn from the same feature space and follow the same distribution. In many real-world applications, however, this independent and identically distributed (i.i.d.) assumption does not hold. It has been extensively demonstrated in the literatures that traditional leaning models perform drastically worse when the i.i.d. assumption no longer holds (Dai, Yang, Xue, & Yu, 2007; Pan & Yang, 2010). In contrast, transfer learning allows the domains, distributions, and feature spaces used in training being different from those in testing. It utilizes labeled data available from some related (or source) domain in order to achieve effective knowledge transformation from it to the target domain, which plays an important role in the areas of machine learning and data mining. If done successfully, knowledge transfer would greatly improve the performance of learning by avoiding tremendously expensive data annotation effort. Many examples in knowledge engineering justified that transfer learning can be generally beneficial for different applications, such as document classification (Sarinnapakorn & Kubat, 2007), sentiment classification (Blitzer, Dredze, & Pereira, 2007; Blitzer & Kakade, 2011), collaborative filtering (Pan, Xiang, & Liu Nathan, 2010), and Web search ranking (Gao, Cai, Wong, & Zhou, 2010).With the prosperity of the Internet, more and more text document collections become available which contain rich textual contents that are interconnected via complex hyperlinks or citations, such as encyclopedia websites (e.g., Wikipedia), research paper archives (e.g., CiteSeer), and user-generated media (e.g., blogs and microblogs). Such kind of data is characterized as analogous structures where each article describes a topic (or concept), which contains a title, abstract, content and some references. A typical example is the Wikipedia page on Support Vector Machine.1http://en.wikipedia.org/wiki/Support_vector_machine.1Compared to the documents in traditional information management, these types of data contain links in addition to content. The hyperlinks (or citations) among articles capture their semantic relations and provide additional insights about their relationships.Most state-of-the-art transfer learning algorithms for document classification treat documents as plain text and ignore the structure of links (or citations). However, link structures provide important information regarding the properties of documents and their relationships. Since the links imply the inter-dependence among the documents, the usual i.i.d. (i.e., independent and identically distributed) assumption of documents does not hold any more (Zhu, Yu, Chi, & Gong, 2007). From this point of view, the existing cross-domain document classification methods that ignore the link structure may fail to capture the dependency and would be unable to fully mine the common knowledge between different domains. It turns out that the transfer of cross-domain information could be seriously hindered due to the incompleteness of the mined common knowledge.In this paper, we propose a novel approach to combine content and link information simultaneously for cross-domain document classification. The basic idea is that documents in different domains may share some common topics from the point of view of both content information and link structure, which could be used to mutually reinforce the identification of common topics, thus to enhance the classification knowledge across related but distinct domains. However, there are two essential problems that challenge the procedure of integrating link structures with the shared topics. First, the link data are usually very sparse and the common parts indirectly connected between domains cannot be fully discovered and utilized. For this reason, we utilize an auxiliary link network to strengthen the co-citation relationship among documents by embedding the background knowledge into a graph kernel. Our method cannot only enrich the document representation by reducing the data sparseness, but also enlarge the dimensional feature space by introducing new features shared by the documents, which would help fill the gap across domains. Secondly, it is difficult to come up with a unified model that combines the two types of information simultaneously because the learned decompositions of topics must be consistent with content and link statistics as well as the training and test data from different domains, following the basic principle of multi-view transfer learning regarding how to model the domain commonality and difference from the perspective of multiple views. To deal with this problem, we propose a probabilistic Link-Bridged Topic model (LBT) based on Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) for cross-domain knowledge transfer using a multi-view approach. LBT correlates the domain-specific features and encodes the domain commonality and distinction, as well as view consistency and difference, into the shared topics. Then the shared topics act as a bridge which helps knowledge transfer from the source to target domain. We derive the log-likelihood objective function for LBT and use EM algorithm for its optimization. Experimental results based on two types of datasets demonstrate that our method outperforms state-of-the-art baselines including the semi-supervised learning algorithm Transductive SVM (Joachims, 1999), the traditional multi-view algorithm Co-Training (Blum & Mitchell, 1998), the large-margin-based multi-view transfer learner MVTL-LM (Zhang, He, Liu, Si, & Lawrence, 2011) and the content-based transfer learning algorithm TPLSA (Xue, Dai, Yang, & Yu, 2008).The rest of the paper is organized as follows: Section 2 reviews the related work; Section 3 presents the proposed LBT model for cross-domain document classification; Section 4 discusses the experiments and analyzes the results; finally, we conclude in Section 5 with discussions on future work.

@&#CONCLUSIONS@&#
