@&#MAIN-TITLE@&#
A neural-AdaBoost based facial expression recognition system

@&#HIGHLIGHTS@&#
The study improves expression recognition rate and execution time.Average recognition rates in JAFFE and Yale databases are 96.83% and 92.22%, respectively.The execution time for processing 100×100 pixel size is 14.5ms.Best recognitions are happy, surprise, and disgust and the poorest is neutral.The general results are very encouraging when compared with others.

@&#KEYPHRASES@&#
Facial expression recognition,Bessel transform,Gabor feature,AdaBoost,MFFNN,

@&#ABSTRACT@&#
This study improves the recognition accuracy and execution time of facial expression recognition system. Various techniques were utilized to achieve this. The face detection component is implemented by the adoption of Viola–Jones descriptor. The detected face is down-sampled by Bessel transform to reduce the feature extraction space to improve processing time then. Gabor feature extraction techniques were employed to extract thousands of facial features which represent various facial deformation patterns. An AdaBoost-based hypothesis is formulated to select a few hundreds of the numerous extracted features to speed up classification. The selected features were fed into a well designed 3-layer neural network classifier that is trained by a back-propagation algorithm. The system is trained and tested with datasets from JAFFE and Yale facial expression databases. An average recognition rate of 96.83% and 92.22% are registered in JAFFE and Yale databases, respectively. The execution time for a 100×100 pixel size is 14.5ms. The general results of the proposed techniques are very encouraging when compared with others.

@&#INTRODUCTION@&#
Facial expression is the explicit transformation of the human face due to the automatic responses to the emotional instability. In most situations it is spontaneous and uncontrollable. The automatic facial expression involves the application of an artificial intelligent system to recognize the expressions of the face under any circumstance. Today, the studies of facial expressions have gained keen interest in pattern recognition, computer vision and its related fields. Mainly, such facial expressions are the seven prototypical ones, namely; anger, fear, surprise, sad, disgust, happy, and neutral.Research into automatic facial expression recognition is very important in this modern society of technological age. For instance, the technology is applied in a wide variety of contexts, including robotics, digital signs, mobile applications, and medicine. It is reported that “some robots can operate by first recognizing expressions” of humans (Bruce, 1993). The AIBO robot for instance is a biologically-inspired robot that can show its emotions via an array of LEDs located in the frontal part of the head (Breazeal & Scassellati, 2002). In addition to this, the robot can also display ‘happiness’ feeling when it detects a face. In behavioral sciences and medicine for instance, expression recognition is effectively applied for intensive care monitoring (Morik, Brockhausen, & Joachims, 1999). Currently, there are developing systems that are capable of making routine examinations of facial behavior during pain in clinical settings. In infants the Neonatal Facial Coding System (NFCS) has been employed for real-time assessment within 32 to 33week post-conceptional age infants who are undergoing a heel lance. The technology is being used in more advanced settings to reduce accidents through the implementation of automated detection of driver drowsiness in public transports. This system relays information about the drivers’ emotional states to observers for effective surveillance leading to necessary awareness.The hallmark of every facial expression system is accuracy and to some extent the speed of execution. However most of the existing systems produce poor performances in terms of accuracy; as for execution speed, most of the systems are even silent to give a hint. Some few examples; Franco and Treves (2001) proposed a neural based facial expression recognition system that used principal component analysis (PCA) to reduce the feature vectors. The features were fed into a feed-forward neural network that was trained by a back-propagation network. In this system an average recognition of 84.5% was reported on the Yale facial expression database – an achievement which is not very encouraging. Kumbhar, Jadhav, and Patil (2012) described a neural network classification facial expression recognition system that employs Gabor feature extraction and feature reduction by PCA to distinguish 7-class facial expression recognition on the JAFFE database. In this system they specified 20 inputs, 40 to 60 hidden layers and seven output feed-forward neural networks. Again, the 60–70% recognition accuracy they obtained by their procedure is not encouraging to befit the expectations of a real-time system. Recently, Londhe and Pawar (2012) extracted features of the face using Affine Moment Invariants and performed the classification using feed-forward neural network. The expression recognition obtained was 93.8% on the JAFFE database. Tai and Chung (2007) extracted the facial features using a Sobel filter. In their experiment they reserved the maximum connected component to reduce the wrinkles and noises and conducted 7-class classification on JAFFE facial expression database through the application of Elman network with two hidden layers, each layer containing fifteen neurons. With this approach the average accuracy of automatic facial expression recognition is 84.7%. Zhang and Tjondronegoro (2009) extracted the expressive face by using Gabor filters, feature reduction by PCA and expression classification by neural network. In this method an average facial recognition of 93.4% was recorded in the JAFFE facial expression database. Dailey and Cottrell (1999) also extracted facial features by Gabor techniques and reduced the features by PCA. The expression classifier was neural network and the average expression recognition was 94.5%±0.7 on the seven prototypical facial expressions, however the facial expression database was not mentioned.Most of these studies advocate the use of Neural Network as the expression classifier and extracted the facial features by Gabor filters and reduced the features via PCA. The displeasing thing is that all the results were not very encouraging.This study persists in exploring the potentials of neural networks to execute this kind of assignment, trying to esteem some biological constraints, utilizing the capabilities of modular systems.Though many techniques have been used to extract the facial features, Gabor feature extraction remains a high-quality choice; there are other alternatives but they are not very promising. Just a few examples: Satiyan and Nagarajan (2010) utilized the Haar technique to extract facial features which were used as input to the neural network for classifying 8 facial expressions. The Haar wavelet extraction is very fast (Satiyan & Nagarajan, 2010; Van, 2008), however the wavelets are too huge to result to effective classification when used as input to classifiers in facial expression recognition (Cemre, 2008); in other words it is a potential cause to misclassifications and poor performance. Distance-based feature extraction methods are also one of the largely applied techniques used for feature extraction in both 2D and 3D static faces. The idea behind these procedures is that the muscle deformations which are the major causes of changes in facial expression from normal expression results in variations of the Euclidean distances between facial landmarks or points. These points, as well as their distances, have been widely employed for static facial expression analysis (Sha, Song, Bu, Chen, & Tao, 2011; Soyel & Demirel, 2007; Tang & Huang, 2008). Among the most successful ones is feature extraction based on the Bhattacharyya distance (Choi & Lee, 2003). However, despite some advantages of this method, the degree of computational complexity is unacceptably high. The matching of even a small model shape with a normal image can take half an hour on an eight-processor Sun SPARCServer 1000 (Rucklidge, 1997; Zhang & Lu, 2004). The Patch based feature extraction method is another alternative widely exploited for facial expression biometrics. Maalej, Amor, Daoudi, Srivastava, and Berretti (2010) for instance represented extracted patches from facial surfaces by sets of closed curves and then applied a Riemannian framework to obtain the shape analysis of the extracted patches. However, the patch-based features also have numerous drawbacks. First, particular representations cannot be applied to other solutions without major modifications: the majority of the techniques have only been utilized to a single class. Also, most methods do not exploit the large amounts of available training data (Aghajanian et al., 2009).Thus on the basis of these we still considered Gabor features as the best approach, not because it does not have drawbacks, but the drawbacks can be easily managed. The Gabor filter is a superior model of simple cell receptive fields in cat striate cortex (Jones & Palmer, 1987), and it grants exceptional basis for object recognition and face recognition (Lades et al., 1993; Wiskott, Fellous, Kruger, & vonderMalsburg, 1997). Again, the Gabor methods are superior to all the above-mentioned methods because it extracts the maximum information from local image regions (Deng, Jin, Zhen, & Huang, 2005), and it is invariant against, translation and rotations (Al Daoud, 2009).In this work, the data were reduced in dimensions by Bessel transform (Ganga, Prakash, & Gangashetty, 2011) and then after extraction of the face by Gabor methods, the features were further reduced via an AdaBoost-based (Freund & Schapire, 1995) feature reduction technique. The selected features which represented the facial deformation patterns were then fed into a 3-layer feed-forward neural network that is trained by a back-propagation algorithm. It is interesting to note that Bessel down-sampling techniques have never been adopted for facial expression recognitions. Again, the combinations of Bessel down-sampling and the formulated AdaBoost-based algorithm is an innovation that reduces the expression dataset to enhance accuracy and speed. Finally, the construction of the feed-forward neural network is influential to bring about successful results.The rest of the work is arranged as follows. Section 2 discusses face detection and image down-sampling. Section 3 discusses Gabor feature extraction. Section 4 discusses feature selection. Section 5 discusses the multilayer feed-forward neural network (MFFNN). Results and analysis are presented in Section 6. The final conclusions of the work are drawn in Section 7.The face detection component was implemented by the adoption of Viola and Jones system (Viola & Jones, 2004). Fig. 1shows sample face detection by the Viola–Jones classifier.The size of the image is rescaled to a window of size 20×20 pixels by the use of Bessel down-sampling. Methods like bilinear interpolations have been utilized by several authors for this task in particular but interpolations are prone to aliasing problems (Munoz, Blu, & Unser, 2001). Bessel down-sampling reduces the size of the image and preserves the details and perceptual quality of the original image (Ganga et al., 2011). The down-sampling image signal xd(t1,t2) is expressed as:(1)xd(t1,t2)=∑n1=1p∑n2=1qc(n1,n2)J0αn1p-rt1J0αn2q-st2where p and q refer to the respective image size, p−r and q−s are the required reduced size of the image; r and s are positive integers that represent the reduction values, n is the number of low-frequency DCT coefficients,J0(αn1)andJ0(αn2)are zero order Bessel functions, c(n1,n2) are Bessel coefficients computed from the first order Bessel function, t1 and t2 are chosen such that0⩽t1⩽p-rand0⩽t2⩽q-s. Interested readers are referred to (Al Daoud, 2009).The 2-D Gabor filters are spatial sinusoids localized by Gaussian window, and they can be created to be selective for orientation, localization, and frequency as well. It is very flexible to demonstrate images by Gabor wavelets because the details about their spatial relations are preserved in the process.(2)G(x,y,θ,u,σ)=12πσ2exp-x2+y22σ2exp{2πi(R1+R2}where i is a complex number representing the square root of −1.R1=uxcosθ and R2=uysinθ, u is the spatial frequency of the band pass, θ is the spatial orientation of the function G, (x, y) specify the position of light impulse in the visual field, σ is the standard deviation of 2-D Gaussian envelop. In this Gabor family, we chose eight orientations0,π8,2π8,…,7π8and five scales4,42,8,82,16.In order to give added robustness to illumination we turned the Gabor filter to zero DC (direct current) by the expression(3)G̃(x,y,θ,u,σ)=G(x,y,θ,u,σ)-1q∑i=-nn∑j=-nnG(x,y,θ,u,σ)where, q is the size of the filter, given by q=(2n+1)2. Fig. 2shows the Gabor filter image.The sample points of the filtered image is coded to two bits, real bit x1 and imaginary bit x2 such that,(4)G1=x1=1,ifRG̃(x,y,θ,u,σ)∗I⩾0x1=0,ifRG̃(x,y,θ,u,σ)∗I<0(5)G2=x2=1,ifIG̃(x,y,θ,u,σ)∗I⩾0x2=0,ifIG̃(x,y,θ,u,σ)∗I<0where I is subimage of the expressional face,RandIare the real and the imaginary parts of each Gabor kernel, * is the convolution operator. With this coding, only the phase information in the facial expressions image is stored in the feature vector of size 256 bytes. The final magnitude response which is used to represent the feature vectors is computed by(6)G=G12+G22Fig. 3shows the magnitude response of a template image.Due to the large size of the Gabor wavelets, it is not practically possible to use all the wavelets as input to our classifier, for fear of misclassification and possible system crash. The AdaBoost feature reduction algorithms have special speed advantage in increasing classification process (Shen & Bai, 2004). Thus we formulated an AdaBoost-based algorithm to select a few deserving portions of the wavelets.Assuming the extracted Gabor features are represented by a total of i∊(1,2,...,N) appearance features. Then the image I is represented asΦi={(xn,yn)}n=±1Nconfigured by the parameters z, μ, v. The positive sets ϕ(+) and the negative sets ϕ(−) are denoted byϕ(+)={(xn,yn)}n=1N⊂RJ×(±1)andϕ(-)={(xn,yn)}n=-1N⊂RJ×(±1)respectively, where xnis the nth data sample containing J features, and ynis its corresponding class label. To train the vectors ||G||, which is denoted by ϕ(u,v,z) over a distribution D, we simply determined the weights of all the feature vectorsΦi={(xn,yn)}n=±1N=ϕ(+)+ϕ(-).This gives us a thresholdλwhich indicated the decision hyperplane.λis computed as:(7)λ=∑∀i∈ϕ(+)D(i).ϕ(μ,v,z)||∑∀i∈ϕ(+)D(i).ϕ(μ,ν,z)||+∑∀i∈ϕ(-)D(i).ϕ(μ,v,z)||∑∀i∈ϕ(-)D(i).ϕ(μ,v,z)||A sample is positive or client if it is located at the positive half ofλ(which is the majority decision), otherwise it is a negative or an imposter. The status is reversed if the minority of the positive instances is rather located in the positive half space. Let c be denoted by clients and p be the imposters. For a given training dataset containing both positive and negative samples, where each sample is (Si,yi); y∊{±1} represents the corresponding class label, the feature selection algorithm is formulated as follows:•Initialize sample distribution D0 by weighting every training sample equally such that the initial weight w1,i=1/2c,1/2p for y=1 and −1, respectively.For the iteration t=1, 2,...,T, where T is the final iteration, do:Normalize the weights,wt,i←wt,i∑i=1Nwt,i, where wtis a probability distribution and N is the total number of features.Train a weak classifier htfor feature j, which uses a single feature. The training error ξtis estimated with respect to wtsuch that:(8)ξt=∑twt,i|ht(xi)-yi|2Select the hypothesisht1with the most discriminating information, that is to say, the hypothesis with the least classification errorξt1,on the weighted samples.Compute the weight ωtthat weightsht1by its classification performance as:(9)ωt=12ln1ξt1-1The weight distribution is then updated and normalized by:(10)wt+1,i≈wt,i.e-ωtyiht1(St1)The final feature selection hypothesis H(S) which is a function of the selected features is denoted by:The selected features represent samples of the facial deformation patterns of the expressive face. The datasets which were images from the JAFFE and Yale databases were partitioned into training and testing by leave-one-out cross validation (Wu, Brubaker, Mullin, & Rehg, 2008).The selected features are fed into the constructed neural network to train it to identify the seven universal facial expressions. The architecture is a 3-layer feed-forward neural network and trained by a back-propagation algorithm (Bouzalmat, Belghini, Zarghili, Kharroubi, & Majda, 2011; Londhe & Pawar, 2012). The back propagation algorithm basically replicates its input to its output via a narrow conduit of hidden units. The hidden units extract regularities from the inputs because they are completely connected to the inputs. Every network was trained to give the maximum value of 1 for exact facial expression and 0 for all other expressions. The construction is shown in Fig. 4. The input layer has 7 nodes, each for each facial expression whiles the hidden layer had 49 neurons; each expression for 7 neurons. We chose 7 neurons to compensate for the target output of seven facial expressions. This was the case for the seven prototypical facial expressions, which was validated by the use of the JAFFE facial expression database. Since the experiment was also validated using the Yale database, where four expressions were used there was a slight modification in the construction of the network for this application. Here, the hidden layer neurons were settled at 16 each facial expression dedicated for 4 neurons and 4 neurons in the output layer.The input vectors of the network represented by X=[x1,x2,...,xl]T. The output layers are denoted by Y=[y1,y2,...,yk]T. The optimization model is formulated as X:h→Y. The output dataset of each layer of the network is denoted byy1j,...,yk-1j,ykj,j=1,2,...,k-1,k,where k−1 corresponds to the total hidden layers and k represents the total output layers. We denote the target datasets and its additive white noise by (t1,t2,...,tK) and η=(e1,e2,...,eK), respectively. The variable K represents the total patterns of the network. The corresponding vectors of the hidden units are denoted by V=(v1,v2,...,vk−1).The sigmoid activation function h=(h1,h2,...,hk) of each layer is h1,h2,...,hk−1. The weights of the network are updated by w1,w2,...,wk. The training epochs are 1000 and the target of error is 0.001. The training algorithm is modeled as:(12)minh1,h2,v1,w1,w2∑j=1K(tj-y2j)2Subject to the constraints(13)y1j=h1(w1xj),w1∈Rv1×M,y1j∈Rv1,xj∈RMy2j=h2(w2,y1j),w2∈R1×v1,y2j∈R1The process of training involves weight initialization, calculation of the activation unit, adjustment, weight adaptation, and testing for convergence of the network. Assuming vjirepresents the weight between the jth hidden unit and ith input unit; and wkjrepresents the weight between the kth output and the jth hidden unit. The activation unit is then calculated sequentially, starting from the input layer. The activation of hidden and output unit is calculated as:(14)yj(p)=hyj(p)∑i=1Ivjizi-vjo(15)ok(p)=hok(p)∑j=1Jwkjyj-wkowhereyj(p)is the activation of the jth hidden unit andok(p)is the activation of the kth output unit for the pattern, p. h is a sigmoid function. k is the total number of output units, I is the total number of input units and J is the total number of hidden units. vjois the weight connected to the bias unit in the hidden layer, zo=−1 and yo=−1. We adjusted the weights, starting at the output units and recursively propagated error signals to the input layer. The detected outputok(p)is compared with the corresponding target valuetk(p)which is a facial image, over the entire training set using the sigmoid function to express the approximation error in the network’s target functions.(16)E(p)=12∑k=1Ktk(p)-ok(p)2The minimization of the error E(p), requires the partial derivative of E(p) with respect to each weight in the network to be computed. The change in weight is proportional to the corresponding derivative.(17)Δvji(t+1)=-η∂E(p)∂vji+αΔvji(t)(18)Δwkj(t+1)=-η∂E(p)∂wkj+αΔwkj(t)where, η is the learning rate, normally between 0 and 1, we set it to 0.9. The function α is also set to 0.9. The last term is a function of the previous weight change.(19)∂E∂vji=∂yj∂vji∑k=1K-(tk-ok)ok(1-ok)yjwkj∂yj∂vji=yj(1-yj)zi.Therefore,(20)Δvji=η∑k=1K(tk-ok)ok(1-ok)yjwkjyj(1-yj)ziThe weights are updated by,(21)wkj(t+1)=wkj(t)+Δwkj(t+1)(22)vji(t+1)=vji(t)+Δvji(t+1)where, t is equal to the current time step. Δvjiand Δwkjare the weight adjustments. We repeated the process once from the equation (14) in order to achieve the desired output.

@&#CONCLUSIONS@&#
This study employs many advanced techniques to improve the recognition rate and execution time of facial expression recognition system. Face detection was carried out by the application of Viola–Jones descriptor. Detected faces were down-sampled by the Bessel transform. This approach reduced the image dimensions and preserved the perceptual quality of the original image. An AdaBoost based algorithm was formulated to select a few hundreds of Gabor wavelets from the several thousands of the extracted features to reduce the computational cost and to avoid misclassification as well. The selected features were fed into a well-designed multilayer feed-forward neural network classifier. The network is thus trained with sample datasets from both JAFFE and Yale facial expression databases. The remaining datasets from the two databases and some images from the World Wide Web were used to test for the system. The execution time for a pixel of size 100×100 is 14.5ms; the average recognition rate in JAFFE database is 96.83% and that in Yale is 92.22%. The proposed method is compared with several methods and the performance is outstanding. The results of the study also show that automatic expression recognitions are very accurate in surprise, disgusts and happy; about 100%. Mild expressions like sad, fear and neutral have lower accuracies. However fear can be very accurate when it is at the peak because accuracies in recognitions largely depend on the magnitude of facial deformations around the mouth and eyes. To advance towards 100% efficiency we believe the development of natural databases would be of more help since many artificial databases have many confused scenarios among facial expressions in sad, neutral and mild anger. Again future improvements of recognition accuracies will look at the possibility of increasing the number of hidden neurons to expressions that recorded lower values.