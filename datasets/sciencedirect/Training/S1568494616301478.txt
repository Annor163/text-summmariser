@&#MAIN-TITLE@&#
An incremental algorithm for attribute reduction with variable precision rough sets

@&#HIGHLIGHTS@&#
Two Boolean row vectors are introduced to characterize the disdernibility matrix and reduct.Rather than the whole discernibility matrix, minimal elements are incrementally computed.The attribute reduction process is studied to reveal how to add and delete attributes.Our incremental algorithm is developed by the adoption of the attribute reduction process.The experimental results show our method can handle datasets with large samples.

@&#KEYPHRASES@&#
Variable precision rough sets,attribute reduction,incremental algorithm,dynamic datasets,

@&#ABSTRACT@&#
Attribute reduction with variable precision rough sets (VPRS) attempts to select the most information-rich attributes from a dataset by incorporating a controlled degree of misclassification into approximations of rough sets. However, the existing attribute reduction algorithms with VPRS have no incremental mechanisms of handling dynamic datasets with increasing samples, so that they are computationally time-consuming for such datasets. Therefore, this paper presents an incremental algorithm for attribute reduction with VPRS, in order to address the time complexity of current algorithms. First, two Boolean row vectors are introduced to characterize the discernibility matrix and reduct in VPRS. Then, an incremental manner is employed to update minimal elements in the discernibility matrix at the arrival of an incremental sample. Based on this, a deep insight into the attribute reduction process is gained to reveal which attributes to be added into and/or deleted from a current reduct, and our incremental algorithm is designed by this adoption of the attribute reduction process. Finally, experimental comparisons validate the effectiveness of our proposed incremental algorithm.

@&#INTRODUCTION@&#
Rough set theory (RS) [16,17], as one generalization of set theory, has emerged as a powerful mathematical tool for modeling and tackling uncertainty, vagueness and indiscernibility in data analysis. It has amply been demonstrated that this theory has its usefulness and versatility in successfully solving a variety of problems [19,32]. The basic assumption of RS is that the rigid inclusion relation without permitting errors is taken account into approximation operators of RS formalism. More precisely, RS is used to perform the complete classification of samples belonging to a specified decision class. As a result, RS is sensitive to misclassification and noise in data. To improve the error-tolerance capability of RS, Ziarko [30,31] proposed a robust model, i.e., variable precision rough set model (VPRS), which allows the partial classification. By introducing an inclusion degree, VPRS relaxes the strict inclusion in approximation operators of RS to the majority inclusion, thereby showing certain robustness to misclassification and noise in data.One primary application of VPRS is attribute reduction which aims to search for a compact and informative subset of condition attributes from a given dataset. As a pioneering work, Ziarko originally proposed the concept of β-reduct based on VPRS [30,31]. By keeping the dependency function, this type of reduct preserves the sum of samples in β-lower approximations of all decision classes [13]. However, it has been pointed out in Refs. [2,34,35] that the dependency function is not monotonic with the change of attributes so that the algorithm for finding a β-reduct may not be convergent. Furthermore, the derived decision rules from β-reduct may be in conflict with the ones from the original system [13]. Considering the drawback, the concepts of β-lower and upper distribution reducts based on VPRS were presented in Refs. [13] to preserve β-lower and upper approximations of each decision class, and thus the decision rules derived from β-lower and upper distribution reducts are compatible with the ones derived from the original system. In view of the superiority of β-lower and upper distribution reducts, the focus of this paper is on such type of reduct based on VPRS.Recently, much effort has been made to develop the algorithms for finding β-lower and upper distribution reducts based on VPRS. In Ref. [13], Mi et al. proposed the discernibility matrix based procedure, by which Boolean discernibility functions can be constructed and all β-lower and upper distribution reducts are also obtained with the minimal disjunctive form. Although this approach provides a solid mathematical foundation for the research on β-lower and upper distribution reducts, it has to compute and examine each element in the discernibility matrix. It has been observed in Ref. [27] that other elements are always absorbed by the minimal elements in the discernibility matrix when simplifying the discernibility function, and only minimal elements in the discernibility matrix are sufficient to find β-lower and upper distribution reducts. On the basis of these facts, Yang et al. [27] developed an algorithm for finding one β-lower and upper distribution reduct based on the minimal elements, and experimentally demonstrated the effectiveness of this algorithm. Moreover, an algorithm based on the set covering was proposed in Ref. [10] to calculate a β-lower distribution reduct by converting the reduct problem to the set-covering problem. However, the previous algorithms which fall into the category of the non-incremental algorithms, are not designed to deal with dynamic datasets where data present themselves in successive samples. At the arrival of new samples, these non-incremental algorithms have to compute the whole dataset from scratch due to lacking the scheme of fully utilizing the previous data information. As a consequence, a huge amount of computational time is needed for some re-computations. Besides, these non-incremental algorithms are incapable to deal with large datasets due to the limitation of the computation capability and the memory size. One possible solution is that such datasets can be cut to bulk and added in succession [33], which can boil down to how to deal with dynamic datasets. In a word, it is always desirable to develop an efficient approach to handle dynamic datasets, so that the time complexity of the non-incremental algorithms can be enhanced.The incremental approach, as a state-of-the-art technique of handling dynamic datasets, has been introduced into attribute reduction with rough sets [1,3,4,11,25,29]. Some incremental algorithms for attribute reduction have been proposed from the perspective of the following three variations: attribute set [20,24,33], attribute values [21,23] and samples [12,26]. With the variation of the attribute set, Zeng et al. [33] analyzed the updating mechanisms of attribute reduction and proposed incremental algorithms for feature selection with fuzzy rough sets. When new attributes are dynamically added, Wang et al. [24] developed a dimension incremental strategy for attribute reduction based on the incremental computation of three measures of information entropy. Shu et al. [20] proposed an efficient algorithm for updating attribute reduction based on the incremental computation of the positive region in incomplete decision systems when the attribute set is dynamically varied. Besides, when attribute values vary dynamically, Wang et al. [23] developed an incremental algorithm for attribute reduction based on the incremental computation of three measures of entropy. Based on the incremental computation of the positive region, Shu et al. [21] developed two incremental feature selection algorithms for single sample and multiple samples with varying attribute values.With the variation of samples, some incremental algorithms for attribute reduction have been developed in the framework of rough sets. For example, an incremental attribute reduction algorithm was proposed in Ref. [9] to find the minimal reduct, but it is only applicable for information systems without decision attribute. For a decision table, two incremental algorithms were presented in Refs. [15,18] respectively, but experimental results in Ref. [7] show that both of them have high time complexity. To improve the efficiency of the two methods in [15,18], Hu et al. [7] presented an incremental attribute reduction algorithm based on the positive region, which was shown experimentally to be more efficient than the two algorithms in Refs. [15,18]. Based on the modified discernibility matrix, Hu et al. [8] proposed an incremental algorithm for finding all reducts when adding a new sample into the current dataset. For adding a new sample into a decision table, Yang [26] updated the discernibility matrix and proposed an incremental attribute reduction algorithm. To handle the case when the number of samples increases dynamically, Guan [6] proposed an incremental algorithm for updating all reducts based on the discernibility matrix. When adding a new sample into a current dataset, Feng et al. [5] employed the incremental computation for computing attribute core to improve the efficiency of computing one reduct. Shu et al. [22] presented an incremental attribute reduction algorithm to compute one reduct for a dynamically-increasing incomplete decision system. To allow a group of samples to be added into a current decision table, Liang et al. [12] developed an efficient group incremental algorithm for attribute reduction by introducing incremental mechanisms for three measures of information entropy including Shannon's entropy, complementary entropy and combination entropy.To the author’s best knowledge, the incremental algorithm for attribute reduction with VPRS has not yet been studied so far. The focus of our paper is on developing an incremental algorithm to find one β-upper (lower) distribution reduct based on VPRS. In the framework of VPRS, the formalisms of the discernibility matrix and reduct are characterized in the form of a set, which is inconvenient to carry out the incremental computation of attribute reduction. So this paper first introduces two Boolean row vectors to equivalently characterize β-lower and upper distribution discernibility matrices and reducts, and thus presents a minimal element based algorithm for finding one β-upper (lower) distribution reduct which is viewed as the preprocessing step of our proposed incremental algorithm in this paper. At the arrival of a new sample, we then employ the incremental process of the two Boolean row vectors, in order to incrementally find the minimal elements in the discernibility matrix of the new decision table. Based on the incremental computation of minimal elements, we gain a deep insight into the attribute reduction process that reveals which attributes to be added into and/or deleted from a current reduct. By the adoption of the attribute reduction process, our incremental algorithm is furthermore developed. Finally, experimental comparisons are performed to demonstrate the effectiveness of our proposed incremental algorithm.The remainder of this paper is organized as follows. In Section 2, the basic concepts of VPRS are briefly reviewed. In Section 3, a minimal element based algorithm for one β-upper (lower) distribution reduct is presented as the preprocessing step of our incremental algorithm. In Section 4, the incremental algorithm for finding one β-upper (lower) distribution reduct is developed. In Section 5, experimental results are presented and analyzed. This paper is ended with conclusions.In this section, some basic concepts in VPRS are briefly introduced.An information system is a pair(U,A)with a mappinga(x):U→Vafor∀a∈A, whereU={x1,x2,⋯,xn}is a non-empty finite set of samples,A={a1,a2,⋯,am}is a non-empty finite set of attributes, andVais the domain of the attribute a. With every non-empty subsetB⊆A, we associate a binary relationIND(B), which is called the B-indiscernibility relation and defined asIND(B)={(x,y)∈U×U:a(x)=a(y),∀a∈B}. Then,IND(B)is an equivalence relation and can partitionUinto a family of disjoint subsetsU/IND(B)={[x]B:x∈U}, where[x]Bdenotes the equivalence class ofIND(B)includingx.ForX⊆UandB⊆A, a pair of lower and upper approximation operators ofXare defined as follows:B̲X={x∈U:[x]B⊆X}B¯X={x∈U:[x]B∩X≠ϕ}The pair(B̲X,B¯X)is referred to as Pawlak rough set ofXwith respect to (w.r.t.)B.As noted in Refs. [34,35], Pawlak rough set model (RS) is sensitive to noise and errors in data. As an important extension of RS, the VPRS model was proposed in Refs. [34,35] to improve the error-tolerance ability of RS by giving a less rigorous definition of the inclusion relation in the above approximation operators of RS. In the VPRS model, for a given inclusion degree thresholdβ∈(0.5,1], we denote,B̲β(X)=∪{[x]B:P(X|[x]B)≥β}B¯β(X)=∪{[x]B:P(X|[x]B)>1−β}B̲β(X)andB¯β(X)are called β-lower and upper approximations ofX, respectively. Here,P(X|[x]B)=|X∩[x]B|/|[x]B|represents the proportion of samples in[x]Bwhich are classified to the setX, and|•|is the cardinality of a set.A decision table (DT) is an information system(U,A∪D)withA∩D=ϕ, whereAis the set of condition attributes andD={d}is the decision attribute set on which a mappingd(x):U→Vdis defined. Here,Vdis the domain of the decision attributed.U/IND(D)={D1,D2,⋯,Dr}is the decision partition ofU.Let(U,A∪D)be a DT, andU/IND(D)={D1,⋯,Dr}. ForB⊆A, we denoteLBβ=(B̲β(D1),⋯,B̲β(Dr))andHBβ=(B¯β(D1),⋯,B¯β(Dr)). For∀x∈U, we denoteGBβ(x)={Dj:x∈B̲β(Dj)}andMBβ(x)={Dj:x∈B¯β(Dj)}. The β-lower and upper distribution reducts are defined as follows.Definition 2.1[13]Let(U,A∪D)be a DT andB⊆A, thenBis said to be a β-lower (upper) distribution reduct, ifLBβ=LCβ(HBβ=HCβ) andLB−{a}β≠LCβ(HB−{a}β≠HCβ) for∀a∈B.From Definition 2.1, we can see that a β-lower (upper) distribution reduct preserves the β-lower (upper) approximation of each decision class. Thus, the decision rules derived from each β-lower (upper) distribution reduct are compatible with the ones derived from the original system. As the commonly used procedure for computing all the β-lower (upper) distribution reducts, the discernibility matrix based approach is briefly stated as follows.Definition 2.2[13]Let(U,A∪D)be a DT,U/IND(A)={C1,⋯,Cn}. We denoteD1*β={(Ci,Cj)=([x]A,[y]A):MAβ(x)≠MAβ(y)},D2*β={(Ci,Cj)=([x]A,[y]A):GAβ(x)≠GAβ(y)}.a(Ci)is the attribute value of samples inCion the attributea. Then,The β-upper distribution discernibility matrix is denoted byD1β=(D1β(Ci,Cj))n×n, in whichD1β(Ci,Cj)is defined byD1β(Ci,Cj)={{a∈A:a(Ci)≠a(Cj)},(Ci,Cj)∈D1*βϕ,otherwise;The β-lower distribution discernibility matrix is denoted byD2β=(D2β(Ci,Cj))n×n, in whichD2β(Ci,Cj)is defined byD2β(Ci,Cj)={{a∈A:a(Ci)≠a(Cj)},(Ci,Cj)∈D2*βϕ,otherwise.In this paper, we useDlβ, (l=1,2) to denote the β-lower and upper distribution discernibility matrices, respectively. In accordance with the case of RS, the β-upper and lower distribution discernibility matricesDlβ(l=1,2) also use the matrix-like technique to discern the sample pairs in the decision table. That is, each attribute in the discernibility attribute setDlβ(Ci,Cj)can discern the pair(Ci,Cj). By Definition 2.2,Dlβ(Ci,Cj)≠ϕholds for∀(Ci,Cj)∈Dl*βwhich implies it is necessary to discern the pairs inDl*β;Dlβ(Ci,Cj)=ϕholds for∀(Ci,Cj)∉Dl*βwhich means it is unnecessary to discern the pairs which do not belong toDl*β. Therefore, one only needs to discern the pairs inDl*β(l=1,2). Moreover,Dlβ(l=1,2) are clearly symmetric, i.e.,Dlβ(Ci,Cj)=Dlβ(Cj,Ci), andDlβ(Ci,Ci)=ϕ. Hence, it is sufficient to only computeDlβ(Ci,Cj), (i>j), and the pairs(Ci,Cj)and(Cj,Ci)are treated as the same one in this paper.The Boolean functionsflβ(A∪D)=∧{∨Dlβ(Ci,Cj):(Ci,Cj)∈Dl*β}, (l=1,2) are referred to as the β-upper and lower distribution discernibility functions, respectively. By applying the distribution and absorption laws as many times as possible, the minimal disjunctive normal form offlβ(A∪D)is computed asflβ(A∪D)=∨i=1t(∧Ali), whereAliis a prime implicant of the discernibility function [19]. According to the literature [13], the set of all prime implicants of the β-upper (lower) discernibility function is just the set of all β-upper (lower) distribution reducts, denoted asREDlβ={Al1,Al2,⋯,Alt}, (l=1,2), whereAli, (l=1,2) are the ith β-upper and lower distribution reducts, respectively. However, as noted in Ref. [19], the problem of finding all reducts is equivalent to the one of transforming the discernibility function to a reduced disjunctive form, which in fact is NP-hard, i.e., a nondeterministic polynomial time problem. In many real applications, one β-lower (upper) distribution reduct is enough. The following theorem provides the basis of finding one β-lower (upper) distribution reduct.Theorem 2.1[13]Let(U,A∪D)be a DT,B⊆AandDlβ=(Dlβ(Ci,Cj))n×n, (l=1,2) be the β-upper (lower) distribution discernibility matrix. Then,Bis a β-upper (lower) distribution reduct, if it satisfies the following conditions:For allDlβ(Ci,Cj)≠ϕ,B∩Dlβ(Ci,Cj)≠ϕ.For∀a∈B, there existsDlβ(Ci,Cj)≠ϕsatisfying(B−{a})∩Dlβ(Ci,Cj)=ϕ.In Theorem 2.1, the first condition implies a β-upper (lower) distribution reduct is sufficient to discern the pairs inD1*β(D2*β). The second one indicates that each attribute in a β-upper (lower) distribution reduct is necessary, i.e., for∀a∈Bthere always exists a pair(Ci0,Cj0)inD1*β(D2*β) which can be only distinguished by the attributea, but cannot be distinguished byB−{a}. In a word, a β-upper (lower) distribution reduct is a minimal subset of condition attributes that discern all pairs inD1*β(D2*β). So, Theorem 2.1 provides a convenient approach to test whether a subset of attributes is one β-upper (lower) distribution reduct.To facilitate the implementation of our incremental algorithm in Section 4, this section focuses on developing a minimal element based algorithm to find one β-lower (upper) distribution reduct based on VPRS. First, we define two Boolean row vectors to characterize the relationships between each sample and β-lower and upper approximations of each decision class, respectively. Furthermore, we employ the defined Boolean row vectors to equivalently describe β-lower (upper) distribution reduct and β-lower (upper) distribution discernibility matrix. Based on the minimal elements in the β-lower (upper) distribution discernibility matrix, we finally develop the algorithm of finding one β-lower (upper) distribution reduct, which will be regarded as the preprocessing step of our proposed incremental algorithm in Section 4.Definition 3.1Let(U,A∪D)be a DT,U/IND(D)={D1,⋯,Dr}andB⊆A. Forx∈U,lBβ(x)=(l1B,l2B,⋯,lrB)is referred to as β-lower distribution vector of the samplexw.r.t.B, whereliB=1ifx∈B̲βDi, andliB=0otherwise.uBβ(x)=(u1B,u2B,⋯,urB)is referred to as β-upper distribution vector of the samplexw.r.t.B, whereuiB=1ifx∈B¯βDi, anduiB=0otherwise.Clearly, both β-upper and lower distribution vectors are Boolean row vector, in which each element is 0 or 1. By Definition 3.1,x∈B̲βDiis clearly equivalent toliB=1forx∈U, andx∈B¯βDiis equivalent touiB=1forx∈U. The following theorem is given to show the properties of the two Boolean row vectors.Theorem 3.1Let(U,A∪D)be a DT andB⊆A, thenlBβ(x)=lBβ(y)anduBβ(x)=uBβ(y)hold for∀y∈[x]B;For∀y∈[x]B,x∈B̲βDi⇔y∈B̲βDiandx∈B¯βDi⇔y∈B¯βDi, which implylBβ(x)=lBβ(y)anduBβ(x)=uBβ(y).Theorem 3.1 illustrates that two samples with the same condition descriptions also have the identical distribution vectors.Theorem 3.2Let(U,A∪D)be a DT andB⊆A, thenBis a β-lower distribution reduct iffBis a minimal subset of condition attributes satisfyinglBβ(x)=lAβ(x)for∀x∈U.Bis a β-upper distribution reduct iffBis a minimal subset of condition attributes satisfyinguBβ(x)=uAβ(x)for∀x∈U.(1) Combining the two facts thatx∈B̲βDi⇔liB=1forx∈U, andx∈A̲βDi⇔liA=1forx∈U, we can proof the conclusion of (1).(2) Combining the two facts thatx∈B¯βDi⇔uiB=1forx∈U, andx∈A¯βDi⇔uiA=1forx∈U, we can proof the conclusion of (2).Theorem 3.2 indicates that β-lower and upper distribution reducts can be equivalently described by the two Boolean row vectors.Theorem 3.3Let(U,A∪D)be a DT,U/IND(D)={D1,⋯,Dr}andB⊆A, thenDi∈GBβ(x)⇔liB=1forx∈U;Di∈MBβ(x)⇔uiB=1forx∈U.(1) Combining the factsDi∈GBβ(x)⇔x∈B̲βDiandx∈B̲βDi⇔liB=1forx∈U, we can get the conclusion of (1).(2) Combining the factsDi∈MBβ(x)⇔x∈B¯βDiandx∈B¯βDi⇔uiB=1forx∈U, we can have the conclusion of (2).According to Theorem 3.3, we haveD1*β={([x]A,[y]A):uAβ(x)≠uAβ(y)}andD2*β={([x]C,[y]C):lCβ(x)≠lCβ(y)}. Thus, we develop the following corollary to illustrate the equivalent descriptions of β-lower and upper distribution discernibility matrices.Corollary 3.1Let(U,A∪D)be a DT,U/IND(A)={C1,⋯,Cn}.a(Ci)is the attribute value of samples inCion the attributea. We denoteD1*β={(Ci,Cj)=([x]A,[y]A):uAβ(x)≠uAβ(y)}andD2*β={(Ci,Cj)=([x]A,[y]A):lAβ(x)≠lAβ(y)}. Then,The β-upper distribution discernibility matrix is denoted asD1β=(D1β(Ci,Cj))n×n, in which the elementD1β(Ci,Cj)is defined byD1β(Ci,Cj)={{a∈A:a(Ci)≠a(Cj)},(Ci,Cj)∈D1*βϕ,otherwise;The β-lower distribution discernibility matrix is denoted asD2β=(D2β(Ci,Cj))n×n, in which the elementD2β(Ci,Cj)is defined byD2β(Ci,Cj)={{a∈A:a(Ci)≠a(Cj)},(Ci,Cj)∈D2*βϕ,otherwise.LetI1β(I2β) be the intersection of all β-upper (lower) distribution reducts, thenI1β(I2β) is clearly contained in each β-upper (lower) distribution reduct. Hence,I1β(I2β) can be regarded as the starting point of finding one β-upper (lower) distribution reduct. The following theorem shows an approach to findIlβ, (l=1,2).Theorem 3.4Let(U,A∪D)be a DT,Dlβ=(Dlβ(Ci,Cj))n×n, (l=1,2) be the β-upper and lower distribution discernibility matrices respectively, then we haveIlβ={a∈A:Dlβ(Ci,Cj)={a}}.On the one hand, for∀a∈Ilβ, supposea∉{a∈A:Dlβ(Ci,Cj)={a}}, then according to the statement (1) of Theorem 2.1, there exists a β-upper (lower) distribution reductBsatisfyinga∉B, which contradicts witha∈Ilβ. Hence,a∈{a∈A:Dlβ(Ci,Cj)={a}}holds, which impliesIlβ⊆{a∈A:Dlβ(Ci,Cj)={a}}.On the other hand, for∀a∈{a∈A:Dlβ(Ci,Cj)={a}}, supposea∉Ilβ, then there exists a β-upper (lower) distribution reductBsatisfyinga∉B, which impliesDlβ(Ci0,Cj0)={a}∩B=ϕ. This fact contradicts with Theorem 2.1. So,a∈Ilβholds, which implies{a∈A:Dlβ(Ci,Cj)={a}}⊆Ilβ.To locate other elements of one β-upper (lower) distribution reduct, we develop the approach to compute one reduct based on the minimal elements in the discernibility matrix. Here, the minimal element is defined as follows.Definition 3.2[27]ForDlβ(Ci0,Cj0)∈Dlβ, if there does not exist another element inDlβas its proper subset, thenDlβ(Ci0,Cj0)is called a minimal element inDlβ.For a minimal elementDlβ(Ci0,Cj0),∀Dlβ(Ci,Cj)⊃Dlβ(Ci0,Cj0)implies thatDlβ(Ci,Cj)can be absorbed byDlβ(Ci0,Cj0)when simplifying the discernibility function. That is, other elements in the β-distribution discernibility matrix are always absorbed by the minimal elements, since they contain a minimal element in the discernibility matrix [27]. Thus, it is enough to find and store all minimal elements in the discernibility matrix. A minimal element can be determined by some pairs inDl*β. By the definition of the minimal element, the discernibility attribute set with the minimum number of attributes is a minimal element, so that we can determine the first minimal element. Then, we search for the pairs that can determine the minimal element and delete other discernibility attribute sets including this minimal element. By continuing this process until the discernibility matrix is empty, we can obtain all minimal elements and their corresponding pairs. According to the above steps, we develop the following algorithm to search for all minimal elements and their corresponding pairs in the discernibility matrix.Algorithm 3.1Finding all minimal elements in the discernibility matrix.Considering the facts that the discernibility matrix may take up a huge amount of memory space and only these minimal elements in the discernibility matrix are sufficient to find reducts, Algorithm 3.1 only stores all minimal elements in the discernibility matrix and their corresponding pairs. Although Algorithm 3.1 is a blind searching algorithm, these minimal elements and their corresponding pairs obtained by Algorithm 3.1 will play an important role in the incremental computation of Section 4 and will serve as the preprocessing step of our incremental algorithm in this paper. The time complexity of Algorithm 3.1 isO(|U|2(|A|+|Mlβ|)). By only using all minimal elements in the discernibility matrix, a heuristic algorithm for one reduct is developed as follows.Algorithm 3.2Minimal element based algorithm for β-upper and lower distribution reduct (Non-Incremental).At each loop of the Algorithm 3.2, the attribute with the most frequency is selected, since this attribute can discern the most pairs. The time complexity of Algorithm 3.2 isO(|U|2(|A|+|Mlβ|)). Next, we develop an example to clearly illustrate Algorithms 3.1 and 3.2.Example 3.1Let(U,A∪D)be a DT (see Table 1), whereU={x1,x2,⋯,x8}is the set of samples,A={a1,a2,a3,a4,a5}is the set of condition attributes andD={d}is the decision attribute set.The condition classes ofUareC1={x1,x6,x7},C2={x2,x8},C3={x3},C4={x4},C5={x5}. The decision classes ofUareD1={x1,x2,x3,x4},D2={x5,x6,x7,x8}. The 0.6-lower distribution vectors of each sample w.r.t.Aare calculated aslA0.6(x1)=lA0.6(x6)=lA0.6(x7)=(0,1),lA0.6(x2)=lA0.6(x8)=(0,0),lA0.6(x3)=(1,0),lA0.6(x4)=(1,0),lA0.6(x5)=(0,1). Thus, we haveD2*0.6={(C1,C2),(C1,C3),(C1,C4),(C2,C3),(C2,C4),(C2,C5),(C3,C5),(C4,C5)}. By Definition 2.2, we can computeD20.6(C1,C2)={a4},D20.6(C1,C3)=D20.6(C2,C3)={a3,a4,a5},D20.6(C1,C4)={a1,a2},D20.6(C2,C4)={a1,a2,a4},D20.6(C2,C5)={a5},D20.6(C3,C5)={a3,a4},D20.6(C4,C5)={a1,a2,a4,a5}. According to Algorithm 3.1, we determine the first minimal element that isc21={a4}. We then letD20.6(C1,C3)=D20.6(C2,C3)=D20.6(C2,C4)=D20.6(C3,C5)=D20.6(C4,C5)=ϕ, since they contain this minimal elementc21={a4}. The pair set corresponding toc21={a4}isp21={(C1,C2)}. Continuing this step of Algorithm 3.1, we can determine other minimal elementsc22={a5}, andc23={a1,a2}. Their corresponding pair sets arep22={(C2,C5)}andp23={(C1,C4)}, respectively. By Algorithm 3.2, we can obtain a 0.6-lower distribution reductred20.6={a1,a4,a5}.In the same way, we can calculateuA0.7(x1)=uA0.7(x6)=uA0.7(x7)=(1,1),uA0.7(x2)=uA0.7(x8)=(1,1),uA0.7(x3)=(1,0),uA0.7(x4)=(1,0),uA0.7(x5)=(0,1). Thus, we haveD1*0.7={(C1,C3),(C1,C4),(C1,C5),(C2,C3),(C2,C4),(C2,C5),(C3,C5),(C4,C5)}. By Definition 2.2, we can computeD10.7(C1,C3)=D10.7(C2,C3)={a3,a4,a5},D10.7(C1,C4)={a1,a2},D10.7(C1,C5)={a4,a5},D10.7(C2,C4)={a1,a2,a4},D10.7(C2,C5)={a5},D10.7(C3,C5)={a3,a4},D10.7(C4,C5)={a1,a2,a4,a5}. According to Algorithm 3.1, we can compute all minimal elements which arec11={a3,a4},c12={a5}, andc13={a1,a2}respectively. The set of sample pairs corresponding toc11={a3,a4}isp11={(C3,C5)}, similarly the set of sample pairs corresponding toc12={a5}isp12={(C2,C5)}, and the sample pair set corresponding toc13={a1,a2}isp13={(C1,C4)}. By Algorithm 3.2, we can obtain a 0.7-upper distribution reductred10.7={a1,a3,a5}.However, Algorithm 3.2 can only be applicable to static datasets. It has no mechanisms of dealing with dynamic datasets with increasing samples, i.e., it is incapable to effectively use the previous data structures and results from the original dataset. At the arrival of an incremental sample, Algorithm 3.2 has to be run from scratch on the whole new dataset. As a result, it often consumes a huge amount of computational time and even may be impractical for dealing with dynamic datasets with large samples. In order to improve the time complexity of Algorithm 3.2, we will develop an incremental algorithm for updating one β-lower (upper) distribution reduct in the next section.In order to handle dynamic datasets with increasing samples, this section researches on the incremental mechanisms of finding one β-lower (upper) distribution reduct. As previously mentioned, a key procedure for finding one β-lower (upper) distribution reduct is to compute β-lower (upper) distribution vector of each sample, so we first employ an incremental manner to compute β-lower (upper) distribution vector of each sample at the arrival of an incremental sample. Considering the facts that the discernibility matrix often takes up a huge amount of memory space and the minimal elements in the discernibility matrix are sufficient to find one reduct, we then incrementally compute and store the minimal elements rather than all elements in the β-lower (upper) distribution discernibility matrix of the new decision table. Based on the incremental computation of minimal elements, we further have an insight into the incremental mechanisms of β-lower (upper) distribution reduct, which reveal how to add and delete attributes in the current reduct. By the adoption of the incremental attribute reduction process, we propose an incremental algorithm to find one reduct.Let(U,A∪D)be an original DT with the reductredlβ, (l=1,2) obtained by Algorithm 3.2. The decision partition ofUis denoted byU/IND(D)={D1,⋯,Dr}. For∀xi∈U,lBβ(xi)anduBβ(xi)are the β-lower and upper distribution vector ofxiw.r.t.B⊆A.Mlβ={cl1,⋯,clk}, (l=1,2) are denoted as the minimal element set in β-upper and lower distribution discernibility matrices of(U,A∪D).Plβ={pl1,⋯,plk}are the family of the pairs corresponding to each minimal element inMlβ, wherepliis the set of the pairs corresponding to the minimal elementcli(i=1,⋯,k). Now, at the arrival of a new samplex, the decision table is changed into(U∪{x},A∪D). For∀xi∈U∪{x},lBβ′(xi)anduBβ′(xi)are the β-lower and upper distribution vectors ofxiw.r.t.B⊆A. In the new decision table, the samplexmay have the same descriptions onAandredlβwith some samples inU, and we denote the two equivalence classes by[x]Aand[x]redlβrespectively. Starting fromredlβ, we incrementally obtain a β-upper and lower distribution reduct of(U∪{x},A∪D)and denote it byredlβ,x.NMlβ={ncl1,⋯,ncls}are the sets of all minimal elements in the β-upper and lower distribution discernibility matrices of(U∪{x},A∪D).NPlβ={npl1,⋯,npls}are the families of the pairs corresponding to each minimal element inNMlβ, wherenpliis the set of the pairs corresponding to the minimal elementncli(i=1,⋯,s).For better understanding the abbreviations and symbols in our paper, we summarize the abbreviations and symbols with their explanations in Appendix A and Appendix B, respectively.This subsection investigates the incremental variations of β-upper and lower distribution vectors in two cases.Case 1[x]D={x}.In this case, there are no samples ofUwhich have the same decision values with the incremental samplex. Therefore,U∪{x}/IND(D)={D1′,D2′,⋯,Dr′,Dr+1′}is the decision partition of(U∪{x},A∪D), whereDr+1′={x},Di′=Di,i=1,⋯,r.Theorem 4.1For[x]D={x}, the following statements about β-lower distribution vector hold:lBβ′(xi)=(l1B′,l2B′,⋯,lrB′,lr+1B′)holds forxi∈[x]B, whereljB′=1if|[x]B∩Dj′||[x]B|≥βforDj′∈U∪{x}/IND(D), and otherwiseljB′=0.lBβ′(xi)=(lBβ(xi),0)holds forxi∉[x]B, i.e., the first r elements oflBβ′(xi)are identical to the corresponding elements oflBβ(xi)and the last one is assigned to 0.The proof is straightforward by Definition 3.1.For the β-upper distribution vector, we have the similar conclusions.Theorem 4.2For[x]D={x}, the following statements about β-upper distribution vector hold:uBβ′(xi)=(u1B′,u2B′,⋯,urB′,ur+1B′)holds forxi∈[x]B, whereujB′=1if|[x]B∩Dj′||[x]B|>1−βforDj′∈U∪{x}/IND(D), and otherwiseujB′=0.uBβ′(xi)=(uBβ(xi),0)holds forxi∉[x]B, i.e., the first r elements ofuBβ′(xi)are identical to the corresponding elements ofuBβ(xi)and the last one is assigned to 0.{x}⊂[x]D.In this case, the incremental samplexhas the same decision values with some samples ofU. Therefore, the decision partition of(U∪{x},A∪D)isU∪{x}/IND(D)={D1′,⋯,Dk′,⋯,Dr′}, whereDk′=Dk∪{x}=[x]D,Di′=Di,i=1,⋯,k−1,k+1,⋯,r.Theorem 4.3For{x}⊂[x]D, the following conclusions about the β-lower distribution vector hold:lBβ′(xi)=(l1B′,l2B′,⋯,lrB′)holds forxi∈[x]B, whereljB′=1if|[x]B∩Dj′||[x]B|≥βforDj′∈U∪{x}/IND(D), and otherwiseljB′=0.lBβ′(xi)=lBβ(xi)holds forxi∉[x]B.In this case, we also have the similar conclusions about the β-upper distribution vector.Theorem 4.4For{x}⊂[x]D, the following conclusions about the β-upper distribution vector hold:uBβ′(xi)=(u1B′,u2B′,⋯,urB′)holds forxi∈[x]B, whereujB′=1if|[x]B∩Dj′||[x]B|>1−βforDj′∈U∪{x}/IND(D), and otherwiseujB′=0.uBβ′(xi)=uBβ(xi)holds forxi∉[x]B.By Theorems 4.1–4.4, we can incrementally compute β-lower and upper distribution vectors of each sample in the new decision table. Clearly, the following corollary holds.Corollary 4.1ForB⊆A,IflBβ(xi)=lAβ(xi)holds for∀xi∉[x]B, thenlBβ′(xi)=lAβ′(xi)holds for∀xi∉[x]B.IfuBβ(xi)=uAβ(xi)holds for∀xi∉[x]B, thenuBβ′(xi)=uAβ′(xi)holds for∀xi∉[x]B.The proof is straightforward according to (2) of Theorems 4.1–4.4.At the arrival of an incremental samplex, there are clearly two possibilities:{x}⊂[x]Aand[x]A={x}. In terms of the two possibilities, we incrementally compute the minimal elements in the β-upper and lower distribution discernibility matrices of the new decision table.When a samplexsatisfying{x}⊂[x]Aenters the original dataset(U,A∪D), we first need to determine which elements inMlβmay not be the minimal elements in the β-upper (lower) distribution discernibility matrices of(U∪{x},A∪D). Then, we need to locate new minimal elements by considering the discernibility information between the new samplexand the samples inU. Combining with the above two aspects, we can incrementally compute the minimal elements for adding a sample satisfying{x}⊂[x]A.According to Theorem 3.3, it is unnecessary for(U∪{x},A∪D)to discern the pairs inω1β={([x]A−{x},[xi]A):uAβ(x)=uAβ(xi),xi∉[x]A},ω2β={([x]A−{x},[xi]A):lAβ(x)=lAβ(xi),xi∉[x]A}. However, it is possible for(U,A∪D)to discern these sample pairs inωlβ(l=1,2), which may correspond to some minimal elements inMlβ. Letpli*=pli−ωlβfor∀pli∈Plβ, then the pairs inpli*can be still determined by the elementcli∈Mlβin(U∪{x},A∪D). Clearly,pli*=ϕimplies that it is unnecessary to discern the pairs ofpliin(U∪{x},A∪D), i.e., the elementcliinMlβis not a minimal element in the discernibility matrix of(U∪{x},A∪D). LetMl*=Mlβ−{cli∈Mlβ:pli*=ϕ}, then the elements inMl*can be also determined by some sample pairs in(U∪{x},A∪D), and these elements may be the minimal elements in the discernibility matrix of(U∪{x},A∪D).To further locate whether elements inMl*are still the minimal elements in the β-upper and lower distribution discernibility matrices of(U∪{x},A∪D), we need to consider the discernibility information between the new sample and the samples inU. Thus, in(U∪{x},A∪D), we should computeDlβ([x]A,[xi]A)for∀xi∉[x]A. In a similar manner to Algorithm 3.1, the minimal elements of{Dlβ([x]A,[xi]A):∀xi∉[x]A}can be computed as{cl1**,⋯,clt**}, andpli**are the set of pairs corresponding tocli**(i=1,⋯,t). Obviously,Ml*∪{cl1**,⋯,clt**}contains the minimal elements in the discernibility matrix of(U∪{x},A∪D), i.e.,NMlβ⊆Ml*∪{cl1**,⋯,clt**}. In order to locateNMlβ, we develop the following theorem by the definition of the minimal element.Theorem 4.5For∀cli**, the following statements hold:If∃clj∈Ml*such thatclj⊂cli**, thencli**∉NMlβholds for(U∪{x},A∪D);If∃clj∈Ml*such thatcli**⊂clj, thenclj∉NMlβholds for(U∪{x},A∪D);For∀clj∈Ml*,cli**⊄cljandclj⊄cli**imply thatcli**,clj∈NMlβhold for(U∪{x},A∪D);If∃clj∈Ml*such thatcli**=clj, thenclj∈NMlβhold for(U∪{x},A∪D)and its corresponding pairs set ispli**∪plj*.Statement (1) indicatescli**can be absorbed byclj∈Ml*, and thuscli**is not a minimal element; Statement (2) indicatesclj∈Ml*can be absorbed bycli**, and thusclj∈Ml*is not a minimal element; and Statement (3) indicates bothcli**andcljare the minimal elements; Statement (4) indicates thatcli**is equal toclj.Next, the detailed procedures of incrementally computingNMlβ, (l=1,2) are listed as follows.Step 1: ComputeMl*=Mlβ−{cli∈Mlβ:pli−ωlβ=ϕ}, whereω1β={([x]A−{x},[xi]A):uAβ(x)=uAβ(xi),xi∉[x]A},ω2β={([x]A−{x},[xi]A):lAβ(x)=lAβ(xi),xi∉[x]A}.Step 2: ComputeDlβ([x]A,[xi]A)={a∈A:a(x)≠a(xi)}foruAβ(x)≠uAβ(xi)andlAβ(x)≠lAβ(xi), and then compute their minimal elements{cl1**,⋯,clt**}and their corresponding pairs{pl1**,⋯,plt**}.Step 3: By Theorem 4.1, compute the minimal elementsNMlβand their corresponding pairsNPlβ.An illustrative example is given to show the incremental computation of minimal elements at the arrival of the new sample satisfying{x}⊂[x]A.Example 4.1Consider Example 3.1. Supposex=[1,1,1,2,1,2]is an incremental sample, then we have[x]A={x,x1,x6,x7}and[x]D={x}. By Theorem 4.1, 0.6-lower distribution vectors of all samples in(U∪{x},A∪D)can be calculated aslA0.6′(x)=lA0.6′(x1)=lA0.6′(x6)=lA0.6′(x7)=(0,0,0),lA0.6′(x2)=lA0.6′(x8)=(0,0,0),lA0.6′(x3)=lA0.6′(x4)=(1,0,0),lA0.6′(x5)=(0,1,0). Thus, we haveω20.6={([x]A−{x},[x2]A)}, which impliesp21*=ϕ,p22*=p22,p23*=p23andM2*={c22,c23}. Furthermore, we computeD20.6([x]A,[x3]A)={a3,a4,a5},D20.6([x]A,[x4]A)={a1,a2}andD20.6([x]A,[x5]A)={a4,a5}. By the definition, we havec21**={a1,a2},c22**={a4,a5},p21**={([x]A,[x4]A)}, andp22**={([x]A,[x5]A)}. By Theorem 4.5, the minimal element set in the 0.6-lower distribution discernibility matrix of(U∪{x},A∪D)isNM20.6={nc21,nc22}, wherenc21={a5},nc22={a1,a2}. Their corresponding sample pair family isNP20.6={np21,np22}, wherenp21={([x2]A,[x5]A)},np22={([x]A,[x4]A)}.Similarly, by Theorem 4.2, 0.7-upper distribution vectors of all samples in(U∪{x},A∪D)can be calculated asuA0.7′(x)=uA0.7′(x1)=uA0.7′(x6)=uA0.7′(x7)=(0,1,0),uA0.7′(x2)=uA0.7′(x8)=(1,1,0),uA0.7′(x3)=uA0.7′(x4)=(1,0,0),uA0.7′(x5)=(0,1,0). We can easily calculateM1*={c11,c12,c13}. Moreover, we can computeD10.7([x]A,[x2]A)={a4},D10.7([x]A,[x3]A)={a3,a4,a5},D10.7([x]A,[x4]A)={a1,a2}. By the definition, we havec11**={a4},c12**={a1,a2},p11**={([x]A,[x2]A)}, andp12**={([x]A,[x4]A)}. According to Theorem 4.5, the minimal element set of the 0.7-upper distribution discernibility matrix in(U∪{x},A∪D)isNM10.7={nc11,nc12,nc13}, wherenc11={a4},nc12={a5},nc13={a1,a2}. Their corresponding sample pair family isNP10.7={np11,np12,np13}, wherenp11={([x]A,[x2]A)},np12={([x2]A,[x5]A)},np13={([x]A,[x4]A)}.Next, we analyze the incremental computation of minimal elements at the arrival of an incremental sample satisfying[x]A={x}. In this case,xwill not combine with any sample ofUw.r.t.A. Thus, in the new decision table,Mlβis the set of the minimal elements in the discernibility attribute sets corresponding to the sample pairs in{([xi]A,[xj]A):xi,xj∈U}. In order to obtain the minimal elements in the discernibility matrix of the new decision table, we also need to computeDlβ({x},[xi]A)for∀xi∈U. By the definition, the minimal elements of{Dlβ({x},[xi]A):∀xi∈U}are computed as{cl1**,⋯,clt**}, andpli**is the set of pairs corresponding tocli**(i=1,⋯,t). Obviously,Ml*∪{cl1**,⋯,clt**}contains the minimal elements in the discernibility matrix of(U∪{x},A∪D), i.e.,NMlβ⊆Ml*∪{cl1**,⋯,clt**}. According to Theorem 4.5, we develop the following steps to compute the minimal elements for adding a sample satisfying[x]A={x}.Step 1: Compute{Dlβ({x},[xi]A):∀xi∈U}, and then compute their minimal elements{cl1**,⋯,clt**}and their corresponding pairs{pl1**,⋯,plt**}.Step 2: By Theorem 4.5, compute the minimal elementsNMlβand their corresponding pairsNPlβ.The following example is given to illustrate the incremental computation for adding a sample satisfying[x]A={x}.Example 4.2Consider Example 3.1. Supposex=[1,0,1,2,1,0]is an incremental sample, then we have[x]A={x}and[x]D={x,x5,x6,x7,x8}. By Theorem 4.3, 0.6-lower distribution vectors of each sample in(U∪{x},A∪D)are calculated aslA0.6′(x)=(0,1),lA0.6′(x1)=lA0.6′(x6)=lA0.6′(x7)=(0,1),lA0.6′(x2)=lA0.6′(x8)=(0,0),lA0.6′(x3)=lA0.6′(x4)=(1,0),lA0.6′(x5)=(0,1). We can computeD20.6([x]A,[x2]A)={a2,a4},D20.6([x]A,[x3]A)={a2,a3,a4,a5},D20.6([x]A,[x4]A)={a1}. By the definition, we havec21**={a1},c22**={a2,a4},p21**={([x]A,[x4]A)},p22**={([x]A,[x2]A)}. According to Theorem 4.5, the minimal element set of the 0.6-lower distribution discernibility matrix of(U∪{x},A∪D)isNM20.6={nc21,nc22,nc23}, wherenc21={a1},nc22={a4},nc23={a5}. Their corresponding pair family isNP20.6={np21,np22,np23}, wherenp21={([x]A,[x4]A)},np22={([x1]A,[x2]A)},np23={([x2]A,[x5]A)}.By Theorem 4.4, 0.7-upper distribution vectors of each sample in(U∪{x},A∪D)are calculated as,uA0.7′(x)=(0,1),uA0.7′(x1)=uA0.7′(x6)=uA0.7′(x7)=(1,1),uA0.7′(x2)=uA0.7′(x8)=(1,1),uA0.7′(x3)=uA0.7′(x4)=(1,0),uA0.7′(x5)=(0,1). We can computeD10.7([x]A,[x1]A)={a2},D10.7([x]A,[x2]A)={a2,a4},D10.7([x]A,[x3]A)={a2,a3,a4,a5},D10.7([x]A,[x4]A)={a1}. By the definition, we havec11**={a1},c12**={a2},p11**={([x]A,[x4]A)}andp11**={([x]A,[x1]A)}. According to Theorem 4.5, the minimal element set of the 0.7-upper distribution discernibility matrix in(U∪{x},A∪D)isNM10.7={nc11,nc12,nc13,nc14}, wherenc11={a1},nc12={a2},nc13={a3,a4},nc14={a5}. Their corresponding sample pair family isNP10.7={np11,np12,np13,np14}, wherenp11={([x]A,[x4]A)},np12={([x]A,[x1]A)},np13={([x3]A,[x4]A)},np14={([x2]A,[x5]A)}.Based on the incremental computing of the minimal elements, this subsection focuses on the insight into the incremental mechanisms of β-lower (upper) distribution reduct.When inserting an incremental sample into(U,A∪D),red1β(red2β)either satisfiesured1ββ′(xi)=uAβ′(xi)(lred2ββ′(xi)=lAβ′(xi))for∀xi∈U∪{x}, or does not meet. That is, there are two possibilities: (1)ured1ββ′(xi)=uAβ′(xi)(lred2ββ′(xi)=lAβ′(xi))holds for∀xi∈U∪{x}; (2)ured1ββ′(xi)≠uAβ′(xi)(lred2ββ′(xi)≠lAβ′(xi))holds for∃xi∈U∪{x}. Furthermore, according to Corollary 4.1 we can know thatured1ββ′(xi)=uAβ′(xi)(lred2ββ′(xi)=lAβ′(xi))holds for∀xi∉[x]red2β(∀xi∉[x]red1β). Thus, we have the following possibilities: (1)ured1ββ′(xi)=uAβ′(xi)(lred2ββ′(xi)=lAβ′(xi))holds for∀xi∈[x]red1β(∀xi∈[x]red2β); (2)ured1ββ′(xi)≠uAβ′(xi)(lred2ββ′(xi)≠lAβ′(xi))holds for∃xi∈[x]red1β(∃xi∈[x]red2β). By Theorem 3.1, two samples with the same descriptions onredlβ, have the identical distribution vectors w.r.t.redlβ. Therefore, the above two possibilities are equal to the following ones: (1)ured1ββ′(x)=uAβ′(x)(lred2ββ′(x)=lAβ′(x)); (2)ured1ββ′(x)≠uAβ′(x)(lred2ββ′(x)≠lAβ′(x)). Next, we study the incremental mechanisms of β-upper (lower) distribution reduct in terms of the two possibilities.Possibility 1ured1ββ′(x)=uAβ′(x)(lred2ββ′(x)=lAβ′(x));For Possibility 1, we haveured1ββ′(xi)=uAβ′(xi)(lred2ββ′(xi)=lAβ′(xi))for∀xi∈U∪{x}.According to Corollary 4.1, we haveured1ββ′(xi)=uCβ′(xi)(lred2ββ′(xi)=lCβ′(xi))for∀xi∉[x]red1β(∀xi∉[x]red2β). Combining with the fact that is Possibility 1, we haveured1ββ′(xi)=uAβ′(xi)(lred2ββ′(xi)=lAβ′(xi))for∀xi∈U∪{x}.According to Theorem 4.6,red1β(red2β) is still a β-upper (lower) distribution reduct of(U∪{x},A∪D), or properly contains a β-upper (lower) distribution reduct of(U∪{x},A∪D). In order to obtain a reduct starting fromred1β(red2β), we employ Theorem 2.1 to develop the following strategy of deleting attributes fromred1β(red2β).The First Attribute Deletion StrategyThe attributea∈redlβ(l=1,2) can be deleted fromredlβif the following statement holds:(redlβ−{a})∩ncli≠ϕfor∀ncli∈NMlβ.The First Attribute Deletion Strategy indicates thatredlβ−{a}can still discern the pairs that need to be discerned in the new decision table, so that the attributea∈redlβcan be deleted fromredlβ. If the strategy does not hold for∀a∈redlβ, then by Theorem 2.1redlβis also a reduct of the new decision table. Otherwise, if the strategy holds fora∈redlβ, then any pair that is discerned byacan also be discerned byredlβ−{a}, which implies that the attributeacan be deleted fromredlβ. We can continue the strategy of deleting attributes until the strategy does not hold. Thus, we can obtain the reductredlβ,xof the new decision table.Possibility 2ured1ββ′(x)≠uAβ′(x)(lred2ββ′(x)≠lAβ′(x)).In Possibility 2,red1β(red2β) does not satisfy the condition thatured1ββ′(xi)=uAβ′(xi)(lred2ββ′(xi)=lAβ′(xi))holds for∀xi∈U∪{x}. Therefore, we need to add some condition attributesBl⊆A−redlβintored1β(red2β) untiluBl∪red1ββ′(xi)=uAβ′(xi)(lBl∪red2ββ′(xi)=lAβ′(xi))holds for∀xi∈U∪{x}. According to Theorem 2.1, we have the following strategy of adding attributes.Attribute Addition StrategyAny attributeainncli∈NMlβcan be added intoredlβ, (l=1,2), if the following statement holds:ncli∩redlβ=ϕfor∃ncli∈NMlβ.By Attribute Addition Strategy, if there existsncli∈NMlβsatisfyingncli∩redlβ=ϕ, then we can add any attributea∈ncliintoredlβ. We can continue the addition strategy until the addition attribute setBl⊆A−redlβsatisfies(Bl∪redlβ)∩ncli≠ϕfor∀ncli∈NMlβ. AlthoughBl∪redlβsatisfies (1) of Theorem 2.1, it may not satisfy (2) of Theorem 2.1. That is, there may be redundant attributes inBl∪redlβ, i.e.,redlβ,x⊆Bl∪redlβ. In order to obtain a β-upper (lower) distribution reductredlβ,xof the new decision table, we develop the following strategy of deleting attributes.The Second Attribute Deletion StrategyThe attributeacan be deleted fromBl∪redlβ, if the following statement holds:((Bl∪redlβ)−{a})∩ncli≠ϕfor∀ncli∈NMlβ.If the strategy of deleting attributes does not hold for∀a∈Bl∪redlβ, thenBl∪redlβis a β-upper (lower) distribution reduct of the new decision table according to Theorem 2.1. If the strategy holds for∃a∈Bl∪redlβ, then the attributeashould be deleted fromBl∪redlβaccording to Theorem 2.1. By continuing The Second Attribute Deletion Strategy, we can obtain the reductredlβ,x, which is the set of the remaining attributes after deleting redundant attributes fromBl∪redlβ.To sum up, if an incremental sample is in Possibility 1, we can use The First Attribute Deletion Strategy to obtain the reductredlβ,x; if an incremental sample is in Possibility 2, we can first use Attribute Addition Strategy to add the attribute setBl⊆A−redlβtoredlβ, and then use The Second Attribute Deletion Strategy to delete redundant attributes fromBl∪redlβ. The simple flowchart of the incremental mechanisms of attribute reduction is shown in Fig. 1.The following example is given to show the above incremental mechanisms of β-lower and upper distribution reduct.Example 4.3Consider Example 4.1. Forred20.6={a1,a4,a5}, we have[x]red20.6={x,x1,x6,x7}. By Theorem 4.1, we can computelred20.60.6′(x)=(0,0,0), from which we can obtainlred20.60.6′(x)=lA0.6′(x). This fact implies that the incremental sample is in Possibility 1. By The First Attribute Deletion Strategy,a4∈red20.6can be deleted fromred20.6, and other attributes cannot be deleted fromred20.6. Thus, we can obtain a 0.6-lower distribution reduct of(U∪{x},A∪D), which isred20.6,x={a1,a5}.In the same manner, forred10.7={a1,a3,a5}, we have[x]red10.7={x,x1,x2,x6,x7,x8}. By Theorem 4.1, we can computeured10.70.7′(x)=(0,0,0), by which we can obtainured10.70.7′(x)≠uA0.7′(x). This fact indicates that the incremental sample is in Possibility 2. By Attribute Addition Strategy,B1={a4}can be added tored10.7such that(B1∪red10.7)∩nc1i≠ϕfor∀nc1i∈NM10.7. Then, by applying The Second Attribute Deletion Strategy, the attributea3∈B1∪red10.7can be deleted fromB1∪red10.7, while other attributes inB1∪red10.7cannot be deleted. Thus, we can obtain the 0.7-upper distribution reduct of(U∪{x},A∪D), which isred10.7,x={a1,a4,a5}.In this subsection, we first propose the algorithm of incrementally computing β-upper (lower) distribution vectors of each sample in the new decision table. Then, we propose the incremental algorithm for computing minimal elements in the β-upper (lower) distribution discernibility matrix of the new decision table. Based on the above two algorithms, we finally develop our incremental algorithm for β-upper (lower) distribution reduct.Algorithm 4.1Incremental algorithm for β-upper (lower) distribution vectors of all samples.Algorithm 4.1 is an incremental algorithm for updating β-upper (lower) distribution vectors of all samples in the new decision table. The time complexity of Algorithm 4.1 isO(|U||A|).Algorithm 4.2Incremental algorithm for minimal elements.Algorithm 4.2 updates incrementally minimal elements in the β-upper (lower) distribution discernibility matrix of the new decision table. Its time complexity isO(|U|(|U|+|Mlβ|)).Algorithm 4.3Incremental algorithm for β-upper (lower) distribution reduct (Incremental).Algorithm 4.3 is an incremental algorithm for finding one β-upper (lower) distribution reduct. Step 1 incrementally computes the β-upper (lower) distribution vectors of each sample w.r.t. all condition attributes and a current reduct in the new decision table by using Algorithm 4.1, and its time complexity isO(|U||A|). Step 2 performs the incremental computing of the minimal elements in the β-upper (lower) distribution discernibility matrix of the new decision table by employing Algorithm 4.2, and its time complexity isO(|U|(|U|+|Mlβ|)). Steps 3–8 are in Possibility 1, and only perform The First Attribute Deletion Strategy to delete attributes from a current reduct. The time complexity isO(|A||NMlβ|). Steps 10–20 are in Possibility 2, and need to first perform Attribute Addition Strategy and then The Second Attribute Deletion Strategy. The time complexity isO(|A||NMlβ|). To be brief, the time complexity of Algorithm 4.3 isO(|U|(|U|+|Mlβ|)).In this section, we conduct some experimental comparisons to evaluate the effectiveness of our incremental algorithm, Incremental. The experiments primarily focus on two aspects. The first one is to compare the Non-Incremental with Incremental in terms of the running time and the classification quality under a given parameter. The second one is to investigate the influence of parameters on the running time and the classification quality of the two algorithms. In the two parts of the experiments, Non-Incremental-U and Non-Incremental-L denote the algorithm Non-Incremental for computing β-upper and lower distribution reducts respectively, while Incremental-U and Incremental-L denote the algorithm Incremental for computing β-upper and lower distribution reducts respectively.The experiments in this subsection are set up as follows.Windows 7PC and Intel (R) Xeon (R) CPU E5-2620 0 @ 2.00GHz, 2.00GHz and 80 GB memory.Matlab 2013b.Eight datasets from University of California, Irvine (UCI) Machine Learning Repository [14] are used (see Table 2) in our experimental comparisons.The fuzzy C-mean clustering algorithm proposed in Ref. [28] is used to discretize real-valued condition attributes.We employ the K-nearest neighbor classifier (K is set to 3) to test the classification quality of the selected attribute subsets of two algorithms. 10-fold cross validation is applied on the eight datasets. We randomly divided the samples of each dataset into 10 parts of equal size, and use nine of them as the training set and the rest one as the testing set. After 10 rounds, we compare the average value that is the ratio of the testing samples classified correctly and all samples in the testing set, as the final classification quality.To test the effectiveness of our proposed incremental algorithm, each dataset in Table 2 is divided into the most original dataset and the incremental dataset (see the 4th and 5th columns of Table 2). Here the first m samples of each dataset in Table 2 are chosen as the most original dataset, and the remaining set of samples is regarded as the incremental dataset which is divided into five parts of equal size. The first part is viewed as the 1st incremental dataset to be added into the original dataset (the current dataset), and the combination of the 1st incremental dataset and the original dataset is updated as the current dataset; the second part is viewed as the 2nd incremental dataset to be added into the updated current dataset, and the combination of the 2nd incremental dataset and the updated current dataset is updated as the current dataset; and so on.The algorithm Non-Incremental is first used to compute the original reduct, minimal elements, their corresponding pairs and β-distribution vectors of all samples in the most original dataset (i.e., the first m samples in the 4th column of Table 2). At the arrival of an incremental dataset, both Non-Incremental and Incremental are used to update a current reduct. Non-Incremental updates the current reduct by re-computing the whole new dataset after adding an incremental dataset, since Non-Incremental has no mechanisms of using the previous data information. As an incremental algorithm, Incremental can fully utilize the previous minimal elements, their corresponding pairs, the distribution vectors of samples to update the current reduct. When adding the first incremental dataset into the most original dataset, Incremental can employ the obtained results of the most original dataset to update the current reduct, minimal elements, their corresponding pairs and the distribution vectors of samples. With the successive addition of the incremental datasets, Incremental uses the updated current results to obtain the new reduct. We can continue this process until all incremental datasets are added, so that we can obtain a reduct from each dataset in Table 2.In this subsection, we show the effectiveness of our algorithm Incremental in terms of the runtime and the classification quality by comparisons with Non-Incremental on the eight datasets. Here, the parameter is set as 0.85. The experimental results are shown in Fig. 2, Tables 3 and 4. In Fig. 2, the x-coordinate is the index of the incremental dataset and the y-coordinate is the running time of Non-Incremental and Incremental.Fig. 2 shows the running time of the two algorithms Non-Incremental and Incremental with the arrival of each incremental dataset. The experimental results indicate the algorithm Incremental is consistently much faster than Non-Incremental with the arrival of each incremental dataset. The main reason is that Non-Incremental has no mechanisms of fully employing the previous data information. Once an incremental dataset enters a current dataset, Non-incremental needs to be run from scratch to update the reduct of the new dataset. As a result, it consumes a huge amount of the computational time. Also, it is clear from Fig. 2 that the curves of Non-Incremental-L and Non-Incremental-U increase monotonically while the ones of Incremental-L and Incremental-U are stable. This is because the algorithm Non-Incremental takes an amount of computational time to re-compute the whole new dataset at the arrival of new samples. With the increasing of samples, the curves of Non-Incremental-L and Non-Incremental-U grow monotonically. In contrast, Incremental performs the computation on the incremental dataset rather than the whole new dataset at the arrival of new samples. Thus, the curves of Incremental-L and Incremental-U appear basically stable at the arrival of incremental datasets with equal size. Moreover, we can see from Fig. 2 Non-incremental may be impractical for large datasets under the current software and hardware environments. For example, on datasets ‘Shuttle’ and ‘Magic’, although Non-incremental can be run on the original datasets, it is out of memory when adding the first incremental dataset.Table 3 shows the comparisons of the total running time between Non-Incremental and Incremental on the eight datasets. In Table 3, the running time of Non-Incremental is obtained on the whole dataset, while the one of Incremental is obtained by the sum of the running time on the original dataset and each incremental dataset. From Table 3, it is clear that the algorithm Non-Incremental is impractical for large datasets under the current software and hardware environment. For example, on ‘magic’ and ‘shuttle’, Non-Incremental cannot find reduct in the current software and hardware environments, while Incremental can obtain reduct. This fact implies that it is possible to deal with large datasets by using Incremental. Moreover, we can easily observe from Table 3 that compared with Non-Incremental, Incremental greatly reduces the running time of finding one reduct. For example, on ‘Soybean’, the runtime of Incremental-L is 54.41% of that of Non-Incremental-L and the runtime of Incremental-U is 57.23% of that of Non-Incremental-U; on ‘Credit’, the runtime of Incremental-L is 37.43% of that of Non-Incremental-L and the runtime of Incremental-U is 36.85% of that of Non-Incremental-U; on ‘Spam’, the runtime of Incremental-L is 26.9% of that of Non-Incremental-L and the runtime of Incremental-U is 28.13% of that of Non-Incremental-U.Table 4 summarizes the classification quality of the selected attribute subset obtained by the two algorithms Non-Incremental and Incremental. In table, ‘raw data’ represents the classification quality of the whole attribute set, ‘Non-Incremental-L’ and ‘Non-Incremental-U’ are the classification quality of selected attribute sets obtained by Non-Incremental for β-lower and upper distribution reducts, and ‘Incremental-L’ and ‘Incremental-U’ denote the classification quality of selected attribute sets obtained by Incremental for β-lower and upper distribution reducts. From Table 4, we can observe that the classification quality of Incremental is higher than or close to that of Non-Incremental. This is because both of them compute a reduct based on the minimal elements.To be brief, we can draw the following conclusions. The first one is that compared with Non-Incremental, the algorithm Incremental can obtain a reduct in a much shorter time. This is because Incremental which is an incremental algorithm, can fully utilize the previous obtained data results to update a current reduct with avoiding some re-computations, while Non-Incremental needs to re-compute the whole new dataset at the arrival of an incremental dataset. The second one is that Non-Incremental is impractical to deal with large datasets while it is possible for Incremental to handle a large dataset. The third one is that the classification quality of Incremental is higher than or close to that of Non-Incremental. These facts indicate that Incremental can efficiently find a reduct with a comparable classification quality.In this subsection, we show the influence of the parameters on the running time and the classification quality of the two algorithms Non-Incremental and Incremental on the eight datasets. In order to observe the variation of the running time with β, we set the value of β to vary from 0.55 to 1 with the step of 0.05. The experimental results are shown in Figs. 3 and 4and Table 5. The x-coordinate pertains to the parameters. The y-coordinate denotes the running time in Fig. 3, while the y-coordinate is the classification accuracy of the selected attribute sets in Fig. 4. Here, it should be noted that the total running time in Fig. 3 is obtained on the whole dataset under different parameters, and the running time in Table 4 is obtained by the mean of the total running time under all given parameters.Fig. 3 shows the influence of the parameters upon the total running time on the eight datasets. It is clear from Fig. 3 that the algorithm Incremental is more efficient than Non-Incremental under different parameters. In addition, on datasets ‘Magic’ and ‘Shuttle’, there are two curves which correspond to Incremental-U and Incremental-L respectively. This is because Non-Incremental cannot compute one reduct in the current software and hardware environments. From Fig. 3, we can see that although there is different runtime under different parameters on each selected dataset and there are no unified rules on all selected datasets, Incremental is always much faster than Non-Incremental under all parameters on each selected dataset.Table 5 shows the average running time of Non-Incremental and Incremental under all given parameters on the eight datasets. From Table 5, it is clear that Incremental is much faster than Non-Incremental. For example, on ‘Soybean’, the runtime of Incremental-L is 66.5% of that of Non-Incremental-L and the runtime of Incremental-U is 58.15% of that of Non-Incremental-U; on ‘Credit’, the runtime of Incremental-L is 39.38% of that of Non-Incremental-L and the runtime of Incremental-U is 63.8% of that of Non-Incremental-U; on ‘Letter’, the runtime of Incremental-L is 16.74% of that of Non-Incremental-L and the runtime of Incremental-U is 24.1% of that of Non-Incremental-U. Furthermore, Table 5 also shows the algorithm Non-Incremental is impractical for large datasets under the current software and hardware environment. For example, on ‘magic’ and ‘shuttle’, Non-Incremental cannot find reduct in the current software and hardware, while Incremental can obtain reduct. This fact implies that it is possible to deal with large datasets by using Incremental.Fig. 4 shows the comparisons of the classification quality of selected attribute sets obtained by the two algorithms Incremental and Non-Incremental. In Fig. 4, there are five curves corresponding to the classification accuracies of all attributes, the four selected attribute sets obtained by Non-Incremental-L, Non-Incremental-U, Incremental-L and Incremental-U. It is clear from Fig. 4 that the classification accuracies obtained by Incremental-L and Incremental-U are higher than or close to the ones of Non-Incremental-L and Non-Incremental-U under each parameter. Besides, the classification accuracy of Incremental-U is higher than that of Incremental-L. Its possible reason is that the β-upper distribution reduct obtained by Incremental-U contains more attributes than β-lower distribution reduct obtained by Incremental-U.To sum up, we can reach the following conclusions from the sequence of experiments. The first one is that our proposed incremental algorithm Incremental can greatly reduce the running time of finding one reduct under every parameter. The second one is that it is possible to handle datasets with large samples by exploiting our Incremental under every parameter. The third one is that compared with Non-Incremental, Incremental can compute one reduct with a comparable classification quality in a much shorter time. Therefore, in comparison with Non-Incremental, our incremental algorithm Incremental is a better option of finding a reduct from the selected datasets, even some real-world applications.

@&#CONCLUSIONS@&#
