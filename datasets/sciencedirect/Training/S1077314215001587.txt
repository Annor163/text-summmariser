@&#MAIN-TITLE@&#
An efficient multimodal 2D + 3D feature-based approach to automatic facial expression recognition

@&#HIGHLIGHTS@&#
We propose a feature-based 2D+3D multimodal facial expression recognition method.It is fully automatic benefit from a large set of automatically detected landmarks.The complementarities between 2D and 3D features are comprehensively demonstrated.Our method achieves the best accuracy on the BU–3DFE database so far.A good generalization ability is shown on the Bosphorus database.

@&#KEYPHRASES@&#
Facial expression recognition,Local texture descriptor,Local shape descriptor,Multimodal fusion,

@&#ABSTRACT@&#
We present a fully automatic multimodal 2D + 3D feature-based facial expression recognition approach and demonstrate its performance on the BU–3DFE database. Our approach combines multi-order gradient-based local texture and shape descriptors in order to achieve efficiency and robustness. First, a large set of fiducial facial landmarks of 2D face images along with their 3D face scans are localized using a novel algorithm namely incremental Parallel Cascade of Linear Regression (iPar–CLR). Then, a novel Histogram of Second Order Gradients (HSOG) based local image descriptor in conjunction with the widely used first-order gradient based SIFT descriptor are used to describe the local texture around each 2D landmark. Similarly, the local geometry around each 3D landmark is described by two novel local shape descriptors constructed using the first-order and the second-order surface differential geometry quantities, i.e., Histogram of mesh Gradients (meshHOG) and Histogram of mesh Shape index (curvature quantization, meshHOS). Finally, the Support Vector Machine (SVM) based recognition results of all 2D and 3D descriptors are fused at both feature-level and score-level to further improve the accuracy. Comprehensive experimental results demonstrate that there exist impressive complementary characteristics between the 2D and 3D descriptors. We use the BU–3DFE benchmark to compare our approach to the state-of-the-art ones. Our multimodal feature-based approach outperforms the others by achieving an average recognition accuracy of 86.32%. Moreover, a good generalization ability is shown on the Bosphorus database.

@&#INTRODUCTION@&#
Affect recognition aims to determine an individual’s emotion by detecting and measuring the emotion related physiological (e.g., bodily symptoms), psychological (e.g., feelings) or behavioral (e.g., facial expression) characteristics [1,2]. As an easily detectable, collectible, and measurable emotion component, facial expression is ideal for affect recognition and for human-computer interaction related applications [3]. However, Facial Expression Recognition (FER) is a very challenging problem mainly because of the diversity and hybridity of human expressions among different subjects in different cultures, genders and contexts.In the past decades, a large number of FER approaches have been proposed. They can be categorized from three perspectives, namely the data modality, expression granularity, and temporal dynamics. From the first perspective, they are classified into 1) 2D FER (which uses 2D gray or color face images), 2) 3D FER (which uses 3D range images, point clouds, or meshes of faces), and 3) multimodal 2D + 3D FER (which uses both 2D and 3D facial data). From the second perspective, they are divided into 1) six basic facial expression (i.e., anger, disgust, fear, happiness, sadness, and surprise) recognition, 2) facial Action Unit (AU, e.g., brow raiser, lip tightener, and mouth stretch) detection and recognition. From the third perspective, they are categorized into static (still images) and dynamic (image sequences) FER. In this paper, we focus on the problem of recognizing the six basic facial expressions using multimodal 2D + 3D static images.Appearance-based 2D FER has been widely investigated since 1990s [3]. The main research topics lie in three aspects: face detection, expression related feature extraction and classification. Comprehensive surveys of 2D FER approaches are given in [3,4]. They are mainly classified into two categories, i.e., template-based and feature-based [4]. Template-based approaches usually fit a holistic face model to the input image or track it in the input image sequence. Active appearance model [5], point distribution model [6], mixture of probabilistic PCA [7], and topographic modeling [8] are some typical examples. Feature-based approaches generally localize the features of an analytic face model in the input image or track them in the input sequence. Gabor wavelets [9] and Local Binary Patterns (LBP) [10] based face representations are two popular representatives. Although considerable advancements have been achieved, 2D FER is still very challenging mainly due to its sensitivity to illumination, pose variations, and possible occlusions [3,4].Recently, with the rapid development of 3D imaging and scanning technologies, it becomes more and more popular to capture 3D face scans. Comparing with 2D face images, 3D face scans contain precise geometric shape information of facial surfaces, which is robust to illumination and pose variations, but more sensitive to facial expression changes. Thus, shape-based 3D FER has attracted increasing attentions. Similar to 2D, 3D FER approaches can also be categorized into template-based and feature-based. Template-based approaches usually build a parametric deformable face model first, and then extract the model parameters as expression features for recognition. 3D morphable model [11], bilinear deformable model [12], shape deformation model [13], and statistical feature model [14] are some famous examples. The main drawback of template-based approaches lies in that they require to establish one-to-one correspondence between 3D face scans, which is still a very challenging issue. Meanwhile, time consuming procedures like dense 3D face registration and model fitting are indispensable. Feature-based approaches generally extract 3D expression cues around facial landmarks using different facial surface geometric or differential quantities. For example, the distances between 3D facial landmarks are widely used in [15–17], and [18]. Moreover, 3D facial curves [19], facial geometry images and normal maps [20,21] facial conformal images [22], facial surface normal [23,24] and curvatures [23–25], and local depth-SIFT features [26] are some popular expression features. Feature-based approaches generally perform better than template-based ones. However, the bottleneck of feature-based approaches lies in accurate and robust 3D facial landmark localization, which is still a very difficult task [27]. More detailed surveys of 3D facial expression recognition are given in [28,29].Although the effectiveness of multimodal 2D + 3D face recognition has been well presented as in [30,31], the investigation of multimodal 2D + 3D FER is very limited. Wang et al. [25] compared the FER accuracy of 3D primitive surface feature distribution based approach with 2D Gabor-wavelet and Topographic Context based ones on the BU–3DFE database, and found that 3D shape based approach is superior to 2D ones, especially for non-frontal faces. However, the effectiveness of combing 3D and 2D approaches was not discussed. Zhao et al. [14] used both 2D features (RGB values and LBP) and 3D features (3D coordinates and shape index values) in the 3D statistical feature model for prototypical expression recognition. But the results using only 2D features or 3D features were not reported, and thus the complementarity between 2D and 3D features was also not studied. In [32], the authors used both 2D and 3D dynamic data for real-time facial action and expression recognition. More precisely, they first extended the active shape model to handle 3D data for facial feature tracking. Then, they extracted numerous geometric measurements (e.g., the distances between landmarks and the boundary shape of lips) and surface deformation measurements (e.g., image gradient and surface curvature descriptors). Finally, the Rule Classifier was used for recognizing a subset of 11 important AUs and 4 facial expressions (i.e., happy, sad, surprise, disgust) on a dataset consisting of 832 sequences of 52 participants. Their experimental results demonstrated that the proposed 2D+3D algorithm performed much better than the 2D appearance-based algorithm (i.e., 2D ASM + Gabor filters + LDA) for recognizing the four facial expressions. This is a very illuminating approach for 2D+3D multimodal FER. However, they did not report the performance of each modality under their own framework. The importance of each modality is still unclear. Savran et al. [33] utilized multimodal 2D + 3D face data for facial AU detection. They found that 3D data generally perform better than 2D data, especially for lower AUs. Moreover, the fusion of two modalities can improve the detection rates from 93.5% (2D) and 95.4% (3D) to 97.1% (2D+3D). Except for facial AU detection and expression recognition, Wang et al. [34] quantified facial expression abnormality in Schizophrenia by combining 2D and 3D features. Their experimental results demonstrated that the combined features better characterized facial expressions than either individual 3D geometric or 2D texture features.The above studies have preliminarily proved the fact that the combination of 2D and 3D data is better than either of the single 2D or 3D modality for expression characterization and AU detection, but deep analysis of the superiority for multimodal 2D+3D FER is still missing. An advantage of using 2D data is that it can be used to accurately localize a large set of facial landmarks on 2D face images and further on their 3D face scans due to the 2D–3D correspondence, which is the first contribution of this paper. More precisely, we propose to explore the incremental Parallel Cascade of Linear Regression (iPar–CLR) algorithm [35] to automatically localize 49 landmarks for each 2D face image and its corresponding 3D mesh scan. This large set of expression related landmarks are then used for extracting local texture and shape descriptors for expression classification. To the best of our knowledge, this is the first work which uses such large number of automatically detected landmarks for 2D and 3D multimodal FER. In contrast, the majority of existing feature-based 3D FER approaches reported their results on the BU–3DFE benchmark based on a large set of (typically 83) 3D facial landmarks manually localized by the database providers [15–19,23,25,26]. Therefore, the proposed framework presents a promising way to these landmark-based approaches so that they can be made automatic using the iPar–CLR algorithm in 2D and 3D multimodal face space.The second contribution of this paper is that a novel second-order image gradient based local texture descriptor (HSOG), a novel first-order mesh gradient (i.e., surface normal) based local shape descriptor (meshHOG), as well as a second-order mesh gradient (i.e., surface curvature) based local shape descriptor (meshHOS) are adapted in FER to comprehensively encode the expression variations in both the 2D and 3D modalities. According to our previous work [36], most of existing popular local image descriptors, such as HOG, LBP, and SIFT, only employ the first-order gradient information related to the slope and the elasticity, i.e., length, area, etc. when the image is regarded as a surface, and thereby partially characterize its geometric properties. By contrast, HSOG captures the curvature related cues of the surface, i.e., cliffs, ridges, summits, valleys, basins, and so on. Thus, HSOG can be applied to describe facial expression deformations (e.g., mouth stretch, lip stretcher, brow raiser). Moreover, in that paper, it was also demonstrated that HSOG outperformed the first-order gradient based local image descriptors (i.e., HOG, LBP, SIFT) when there were not severe scale variations, as in the applications of local image matching and scene classification. In this paper, we give another evidence of the effectiveness and generalization ability of HSOG for FER. Similarly, as general local shape descriptors, meshHOG and meshHOS provide a compact description of the facial surface normal and curvature information, and they have proved very efficient for 3D face identification in our previous works [37,38]. In this paper, we interested in exploring their generalization abilities in 3D FER.During the FER stage, both the early fusion (i.e., feature-level) and late fusion (i.e., score-level) strategies of 2D descriptors, 3D descriptors, as well as 2D and 3D descriptors are comprehensively demonstrated and their complementary characteristics are well revealed, which is our third contribution. The important findings behind the fusion results can be summarized as: 1) The second-order gradient based local texture or shape descriptor (HSOG or meshHOS) generally have stronger discriminative power than the first-order gradient based ones (SIFT or meshHOG). Moreover, different order 2D or 3D descriptors are complementary in encoding local texture or shape cues. 2) There exist large complementary characteristics between 2D and 3D descriptors of the same order (SIFT and meshHOG, HSOG and meshHOS), different order (SIFT and meshHOS, HSOG and meshHOG), as well as multiple orders (all four 2D and 3D descriptors).Overall, we present an efficient multimodal (2D and 3D) and multiple-order (first and second) feature-based fully automatic FER approach, and validate it trough comprehensive experiments on the BU–3DFE database. Considerable complementary characteristics between the features of different orders and different modalities are highlighted either by early fusion or late fusion of 2D, 3D, as well as 2D and 3D descriptors. The generalization capability of our approach is further evaluated on the Bosphorus database.This paper is an extension of our work presented in [23] and is organized as follows. Section 2 introduces the iPar–CLR based 2D and 3D facial landmark localization procedure. Section 3 and 4 give the construction details of the HSOG, meshHOG and meshHOS descriptors. Section 5 lists and compares the accuracies of each single 2D and 3D descriptor, and the ones of their fusion. The generalization capability of the proposed approach is discussed in Section 6. Finally, we conclude the paper and point out the limitations and future directions.To extract expression related features, a set of key landmarks are required. In this paper, we introduce the incremental Parallel Cascade of Linear Regression (iPar–CLR) [35] for face landmarking in the 2D modality. iPar–CLR is an incremental and parallel version of the Sequential Cascade of Linear Regression (Seq–CLR) algorithm [39]. Given a set of training face images Iiassociated with p 2D landmarksxi∈R2p×1. f is a feature extraction function (e.g., SIFT) andf(xi)∈R128p×1in the case of extracting SIFT features. During training, one assumes that p corrected landmarks are known for each Ii, and denoted asx*i. To reproduce the testing scenario, one runs the face detector on the training images to provide an initial configuration of the p landmarksx0i,which corresponds to an average shape. In this setting, the Seq–CLR algorithm is formulated as:(1)argminWk,bk∑Ii∑xki∥x*i−xki−Wkf(xki)−bk∥2.In practice, W0 and b0 are first estimated usingx*i,x0i,andf(x0i). Then, a sequence of regressions are computed to updatexkiand make it converge tox*istep by step. iPar–CLR improves Seq–CLR by introducing a parametric 3D shape model for the configuration of p landmarks, and solving Eq. (1) in the parameter space. By assuming that the distribution of the perturbations of shape parameters is Gaussian, iPar–CLR is well suited for the task of incremental update. That is, it can incrementally update the pre-trained shape model according to the newly added face images.When used for joint 2D and 3D facial landmark localization, the texture map is projected from each textured 3D face scan into a 2D regular grid domain using the interpolation techniques. Then, we apply iPar–CLR [40] to each projected 2D texture face image, outputting 49 2D landmarks (see Fig. 1). These 2D landmarks are then transferred to 3D texture face space by the inverse of the above projection. Note that since all these 2D landmarks are located at the frontal part of the projected 2D face texture, the one-to-one correspondence between 3D texture and 2D texture can be approximately preserved during the projection mapping. Finally, the corresponding 3D landmarks are directly determined by the one-to-one correspondence between 3D texture and 3D geometry of the 3D face model. We evaluate iPar–CLR on the whole BU–3DFE database and the expressive samples in Bosphorus, and find that it can precisely localize all the pre-defined 49 facial landmarks for all samples even with variations in expression, ethnicity, gender and age etc. (see Fig. 1 for some sampled results).We extract the SIFT [41] descriptor of each projected 2D texture face image at the locations of the detected 2D landmarks within 16 × 16 patches. The SIFT feature based facial representation of a 2D texture image is generated by concatenating all the SIFT features at the 49 landmarks according to the pre-defined order, resulting in a128×49=6,272dimensional feature vector. This vector is further normalized to the unit length for the following processing.The HSOG descriptor was originally proposed in [36] and proved very efficient for local image matching, object categorization, and scene classification. In this paper, we explore HSOG for 2D facial expression description. The construction of HSOG is composed of three steps:(1) Computation of the first order Oriented Gradient Maps (OGMs): The input of HSOG is a R × R image patch around each localized 2D facial landmark. For each image patch I(x, y), it outputs a number of Oriented Gradient Maps (OGMs){Jo(x,y)}o=1Lby computing the Gaussian convolution of the positive orientation gradient maps, described as:(2)Jo(x,y)=GΣ*max(∂I(x,y)∂o,0),o=1,2,…,L,where o represents a quantized direction, and GΣis a Gaussian kernel with standard deviation Σ, which is proportional to the size of image patch R.(2) Computation of the second order gradients: Once these first order OGMs of all quantized directions are generated, they are used as the inputs for computing the second order gradients. Precisely, for each OGM Jo(x, y), we calculate its gradient magnitude mago(x, y) and orientationθo(x, y) at every pixel location. The orientation value θo(x, y) is then re-scaled from the range of[−π/2,π/2]to [0, 2π], and quantized into L dominant orientations. After quantization, the entry noof each orientation θois calculated as:(3)nθo(x,y)=mod(⌊θo(x,y)2π/L+12⌋,L),o=1,2,…,L.(3) Spatial pooling: Daisy-style spatial pooling strategy is used in HSOG as illustrated in Fig. 2. It is easy to find that there are four parameters that determine the HSOG descriptor, i.e., the size of the patch (R); the number of quantized orientations (L); the number of concentric rings (CR); the number of circles on each ring (C). The total number of the divided circles can be calculated asT=CR×C+1. Within each circle CIRj, and for each OGM Jo, a second order gradient histogram is constructed by accumulating the gradient magnitudes magoof all the pixels with the same quantized orientation entry no.(4)hoj(i)=∑(x,y)∈CIRjδ(nθo(x,y)==i)*mago,wherei=0,1,…,L−1;o=1,2,…,L,j=1,2,…,T,and δ is the characteristic function. Then, for each first order OGM Jo, its second order gradient histogram hois generated by concatenating all the histograms from T circles:(5)ho=[ho1,ho2,…,hoT]T,whereo=1,2,…L. Finally, the HSOG descriptor is obtained by concatenating all L histograms of the second order gradients as in Eq. (6). Each histogram hois normalized to a unit norm vectorh^obefore concatenation.(6)HSOG=[h^1,h^2,…,h^L]T.Similar to SIFT, the HSOG feature based expression representation of a 2D texture image is generated by concatenating all HSOG features of the localized landmarks and then normalized to the unit length. In this paper, we setR=25,L=8,CR=3,C=4as in [36]. Thus, the dimension of the final HSOG feature vector for a face image isT×L×8×49=13×8×8×49=40,768.The meshHOG and meshHOS descriptors were originally proposed in our previous work [37,38] and proved efficient in 3D face identification. In this paper, we employ them in 3D FER. Similar to HSOG, meshHOG and meshHOS are built by the following three steps:(i) Computation of facial surface normal and curvature: Each 3D facial surface is represented by a triangular meshT=(F,V),whereFandVare the face and vertex sets. We compute the unit normal vector of each face by the cross product of its two edge vectors. Then the unit normal of each vertexnv=[nxv,nyv,nzv]Tis achieved by averaging the normal vectors of its one-ring faces. The mesh gradient magnitude magvand orientation θvat each vertex are calculated as:(7)magv=(nxv/nzv)2+(nyv/nzv)2,θv=arctan(nyv/nxv).According to [42], the principal curvatures kmax  and kmin  are computed by fitting a cubic-order surface:(8)f(x,y)=A2x2+Bxy+C2y2+Dx3+Ex2y+Fxy2+Gy3and its normal vectors(fx(x;y),fy(x;y),−1)using both the 3D coordinates and the normal vectors of the associated local neighbor points (two-ring). Once we have two principle curvatures, the shape index values, which describe different shape classes by a single number ranging from 0 to 1, is calculated as:(9)SI=12−1πarctan(kmax+kminkmax−kmin).Fig. 3 shows the shape index maps of sampled 3D faces with six prototypical expressions.(ii) Canonical orientation(s) assignment: Similar to the SIFT feature, to achieve rotation invariance, one or more local coordinate systems (i.e., canonical orientations) should be determined at each localized 3D landmark. This can be accomplished by the following three steps: First, we build an initial local coordinate system, where the landmark v and its normal nvare the origin and the positive z axis, respectively. And two perpendicular vectors in the tangent plane of v are randomly chosen as the x axis and y axis, respectively. Then, the gradient magnitudes and orientations of the vertices around the landmark with a given geodesic distance r0 are computed, Gaussian-weighted by their corresponding gradient magnitudes, and put in a histogram of 360 bins. Dominant gradient orientations, that is, peaks in the histogram, are used to assign one or more canonical orientations to the landmark. Finally, the initial local coordinate system is rotated in the local tangent plane, making each canonical orientation as new x axis, and the new y axis is computed by the cross product of the z axis (i.e., normal vector nv) and the new x axis (i.e., canonical orientation). Once the canonical orientations are determined, all the neighbor vertices and their normal vectors are transformed to the new local coordinate system for the following processes.(iii) Spatial pooling: Similar to the HSOG feature, a simplified daisy-style spatial pooling strategy is also used for meshHOG and meshHOS. However, the pooling strategy here is performed on the tangent plane of each 3D landmark and on the local coordinate system determined by the assigned canonical orientations. As illustrated in Fig. 2, for the 3D descriptors, there is only one concentric ring associated with eight circles, resulting in nine sequential circles. Within each circle CIRj, a mesh gradient histogram and a shape index histogram are constructed respectively. The histogram of mesh gradient is constructed by accumulating the gradient magnitudes magvof all vertices with the same quantized orientation entry nθ(v) as:(10)gahogj(i)=∑v∈CIRjδ(nθ(v)==i)*magv,wherei=0,1,…,7;j=1,2,…,9,nθ(v) is entry of the quantized gradient orientation computed the same asnθo(x,y)in (3). The histogram of shape index is constructed by accumulating the Gaussian weights GΣ(v) of all vertices with the same quantized shape index value nSI(v)(11)sahosj(i)=∑v∈CIRjδ(nSI(v)==i)*GΣ(v),wherei=0,1,…,7;j=1,2,…,9,nSI(v) is the quantized shape index values. Then, for each 3D landmark, its 3D descriptors are generated by concatenating all the histograms from nine circles in a clockwise direction,(12)HOG=[gahog1,gahog2,…,gahog9]T,HOS=[sahos1,sahos2,…,sahos9]T.Each sub-histogram (hogior hosi) is normalized to the unit length before concatenation to eliminate the influence of non-uniform mesh sampling. Note that, intuitively, HOG describes the point-level bending pattern of the local shape around a landmark while HOS indicates the distribution of different shape categories. The expression representation (based on meshHOG or meshHOS) of a 3D face surface is generated by concatenating all HOG or HOS features of the localized 3D landmarks and then normalized to the unit length. Following [37], the geodesic radius r0 is set to 22.50 mm, the radius of each circle is set to 10 mm, and the distance between the center of the centric circle and the one of each rounding circle is set to 15 mm. As a result, the dimension of the final meshHOG or meshHOS feature is9×8×49=3,528.

@&#CONCLUSIONS@&#
