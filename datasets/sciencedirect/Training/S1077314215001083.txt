@&#MAIN-TITLE@&#
Semi-supervised learning and feature evaluation for RGB-D object recognition

@&#HIGHLIGHTS@&#
We propose a semi-supervised learning method for RGB-D object recognition.We propose CNN-SPM-RNN to extract powerful RGB-D features.An unbiased feature evaluation for recent RGB-D features are introduced.

@&#KEYPHRASES@&#
RGB-D,Object recognition,Feature representation,Feature evaluation,Semi-supervised learning,

@&#ABSTRACT@&#
With new depth sensing technology such as Kinect providing high quality synchronized RGB and depth images (RGB-D data), combining the two distinct views for object recognition has attracted great interest in computer vision and robotics community. Recent methods mostly employ supervised learning methods for this new RGB-D modality based on the two feature sets. However, supervised learning methods always depend on large amount of manually labeled data for training models. To address the problem, this paper proposes a semi-supervised learning method to reduce the dependence on large annotated training sets. The method can effectively learn from relatively plentiful unlabeled data, if powerful feature representations for both the RGB and depth view can be extracted. Thus, a novel and effective feature termed CNN-SPM-RNN is proposed in this paper, and four representative features (KDES [1], CKM [2], HMP [3] and CNN-RNN [4]) are evaluated and compared with ours under the unified semi-supervised learning framework. Finally, we verify our method on three popular and publicly available RGB-D object databases. The experimental results demonstrate that, with only 20% labeled training set, the proposed method can achieve competitive performance compared with the state of the arts on most of the databases.

@&#INTRODUCTION@&#
Recently, RGB-D data has attracted great interest in computer vision and robotics community with the advent of new depth sensors, such as Kinect. The Kinect-style depth cameras are capable of providing high quality synchronized images or videos of both color and depth, which represent an opportunity to dramatically improve the performance of many vision problems, e.g., object recognition [5,6], detection [7–9], tracking [10–12], SLAM [13,14] and human activity analysis [15,16]. This is mainly because that depth information has many extra advantages: being invariant to lighting and color variations, allowing better separation from the background and providing pure geometry and shape cues. Furthermore, many RGB-D datasets [5,6,13–15,17,18] have been published for public use to promote the development of such research areas.This paper mainly focuses on object recognition, which is a fundamental problem in computer vision and pattern recognition. Although many methods [1–4,19] have been presented to promote RGB-D object recognition, they chiefly aim at extracting effective features from the novel RGB-D data and using a supervised learning model to achieve good classification performance. However, supervised learning models always require for large amount of manually labeled data. The collection of enough labeled training set is an expensive and difficult task. Thus, it is important to get rid of this problem by utilizing relatively plentiful and convenient unlabeled RGB-D data.With the ability to handle the unlabeled data, a semi-supervised learning framework is proposed in this paper by considering the two distinct views of RGB-D data effectively. Although there are many successful semi-supervised learning algorithms in the literature, e.g., self-training [20–22], co-training [23,24] and graph-based methods [25–27], we are especially interested in the co-training method because of its unique advantages over the RGB-D data: co-training was theoretically proved to be very successful in combining the labeled and unlabeled data under two strong assumptions (including “conditional independence given the label”) in [23], then the work [24] proved that much weaker assumptions were sufficient to guarantee co-training, when given appropriately strong PAC-learning algorithms on each view. Intuitively, the two weaker assumptions can be described as follows: (1) Each example contains two distinct views, and each view provides sufficient information to determine the label of the example; (2) The two views should not be too highly correlated. It means that, there should exist some examples which can be confidently recognized by one view but not by the other view (or vice versa) to make the co-training algorithm work effectively. RGB-D data meets the two assumptions very well. Firstly, RGB-D data contains two distinct views, RGB and depth. Both of them can provide useful cues for object recognition: RGB images can describe rich color, texture and appearance information for the object, while depth maps can sketch pure geometry and shape cues. Secondly, the image capturing modes of RGB (e.g., RGB cameras) and depth (e.g., infrared cameras) are very different, guaranteeing the independence of the two views.Given two distinct views (RGB and depth) for each example, the key to the success of co-training is to obtain effective feature representation for each view. Thus a powerful feature CNN-SPM-RNN will be proposed in this paper based on the feature CNN-RNN [4]. CNN-RNN combines a single convolutional neural network (CNN) and multiple recursive neural networks (RNN [28]) to learn high-level features for each RGB-D object. Since learning the optimal structure of each RNN tree from the raw data is highly time consuming as described in [28], CNN-RNN utilizes fixed-tree RNN structure to hierarchically aggregate the CNN responses very efficiently. However, the fixed-tree RNN requires for the fixed-size of the inputs by simply cropping or warping all the images, which may degrade the recognition performance after such artificial processing. Inspired by the pioneer work [29], which applied a spatial pyramid pooling layer (SPM [30]) to the supervised deep learning model [31] to adapt the model for arbitrary sizes of inputs, we extend its core idea to the unsupervised CNN-RNN feature learning model and design a new feature learning structure, termed CNN-SPM-RNN. Towards SPM layer, the main differences between CNN-SPM-RNN and the work [29] are twofold: (1) A pooling layer with different pyramid partitions in [29] is sufficient to guarantee the success of the supervised deep learning model, benefiting from the back-propagation of errors and the fine-tuning of filters. However, we empirically find that a single pooling layer can even make the unsupervised CNN-RNN model worse, probably because the low-level convolutional responses cannot capture local object structures very effectively after pooling. Thus a feature coding layer is added to encode the convolutional responses and high-level feature responses are obtained to represent local information powerfully. Then the pooling layer is performed to result in fixed-scale feature maps for the fixed-tree RNNs. (2) Compared to the 2D spatial pyramid pooling for the RGB modality in [29], 3D spatial pyramid pooling is utilized for the depth modality to effectively capture shape cues of objects. The details are given in Section 2.2. Furthermore, we find that two additional modalities, grayscale images and surface normals, can largely benefit view representation, as shown in Fig. 1. We introduce a unified feature evaluation framework by combining RGB and grayscale to capture visual appearance (i.e., the RGB view), while depth and surface normals to capture shape cues (i.e., the depth view) for all the representative features, including KDES [1], CKM [2], HMP [3], CNN-RNN [4] and CNN-SPM-RNN.An early version of our work was presented in [32] to explore co-training for RGB-D object recognition. In this paper, we extend [32] in the following aspects: developing a more powerful feature termed CNN-SPM-RNN based on [4,32], introducing a unified framework to fairly evaluate all the representative features, and presenting a wide array of experiments to demonstrate the effectiveness of the proposed semi-supervised method with powerful RGB-D features.The major contributions of this paper are summarized as follows.•Propose a complete and systematic semi-supervised learning framework for RGB-D object recognition using co-training. We theoretically analyze that the framework can take full advantage of the characteristics of the new RGB-D data, and significantly benefit object recognition by learning from large amount of unlabeled RGB-D data.Present a novel feature CNN-SPM-RNN to effectively represent RGB-D data. To the best of our knowledge, this is the first work to successfully apply SPM to the unsupervised deep learning model to address the problem of cropping or warping. The core idea is inspired from the pioneer work [29], which utilized SPM layer in the supervised deep learning model and yielded impressive results.Analyse and evaluate most representative RGB-D features in an unbiased way by utilizing four data modalities, including RGB, grayscale, depth and surface normals, which can provide a meaningful guideline how to best represent the new RGB-D data.The rest of this paper is organized as follows: Section 2 proposes our semi-supervised learning framework for RGB-D object recognition, including the semi-supervised learning method based on co-training, the feature CNN-SPM-RNN, and a unified framework for feature evaluation. Section 3 empirically evaluates and ranks all the representative features in an unbiased way, and shows the comparison of our semi-supervised method with the state of the arts on several public RGB-D object databases. Finally, Section 4 concludes the paper and discusses the future work.As shown in Fig. 2, our semi-supervised learning framework is proposed for RGB-D object recognition. There are three modules in the framework: (1) feature representation for the RGB view; (2) feature representation for the depth view; and (3) exploiting co-training to utilize a small set of labeled data and large amount of unlabeled data. The core idea of the framework is to improve the two classifiers trained on the two distinct feature sets iteratively by co-training. Thus how to extract effective feature representation for each view is the fundamental step.In the following subsections, we first prove that co-training can succeed in learning from unlabeled RGB-D examples, and propose the co-training algorithm for RGB-D object recognition. Then we introduce our feature CNN-SPM-RNN, followed by a unified framework to evaluate recent state-of-the-art features for RGB-D objects.We employ co-training as our semi-supervised learning method to learn from the unlabeled RGB-D data, as shown in Fig. 2. Firstly, Two assumptions to guarantee the success of learning with co-training are introduced. Then a specific co-training algorithm for RGB-D object recognition is proposed.Some notations are given as follows: Let D denote the distribution over the feature spaceF=F1×F2,where F1 and F2 correspond to two different views of an example. AssumeF+andF−are the positive and negative regions of F respectively (for simplicity we consider binary classification here), and Let c be the target function. Then for i ∈ {1, 2}, we defineFi+={fi∈Fi:ci(fi)=1}andFi−=Fi−Fi+. In order to bootstrap co-training, an initial labeled set for the two viewsS10⊆F1+andS20⊆F2+are provided. During the iterative learning procedure of co-training, a hypothesis hiis devised as a subset of Fi, where fi∈ himeans that hiis confident that fiis positive, and fi∉himeans that hihas no opinion.The research [24] proved it was sufficient for co-training to succeed, when given the two assumptions on the underlying data distribution:•The learning algorithm for each view is able to learn from positive data only.The marginal distributionD+isϵ-expanding (ϵ>0).The first assumption means that,∀Di+overFi+,given access to examples fromDi+,each learning algorithm is able to produce a hypothesis hisuch thatPr(errorDi+(hi)≤ϵ)≥1−δ,whereϵ,δ>0. This can be thought of as predicting the examples either “positive with confidence” or “has no opinion”. According to [24], this assumption is easy to fulfill in practice if the positive class is cohesive and the negative class is not. The second assumption can be interpreted as the following definition:DefinitionD+isϵ-expanding if for anyS1⊆F1+,S2⊆F2+,we havePr(S1⊕S2)≥ϵmin[Pr(S1∧S2),Pr(S¯1∧S¯2)].where Pr(S1∧S2) denotes the probability mass on examples that are confidently predicted as positive region by both views, and Pr(S1⊕S2) denotes the probability mass on examples for which we are confident about just one view. Note thatϵ-expanding is necessary to guarantee co-training will succeed, because if S1 and S2 are confident sets and do not expand, then we might never see the expected situation that examples for one hypothesis could help the other.An intuitive interpretation of co-training is as follows: Firstly, two initial classifiers over the respective views are trained on a small labeled sample. Then each classifier is used to label the confident examples for the other classifier, for which these examples can be seen as random training instances. In this case, each classifier can benefit from the additional examples by the other one and improve its classification accuracy in every rounding training.We propose our co-training algorithm for RGB-D object recognition (multi-class classification) in Algorithm 1. Firstly, we extract feature representations FRGBand Fdepthfor the RGB and depth view respectively. Then we train two linear SVM classifiers CRGBand Cdepthbased on a small set of initial labeled examples L using the two feature sets. CRGBand Cdepthare applied to predict the examples from the unlabeled training sets U separately. For each classifier, |nj| most confidently predicted instances of each class whose scores are higher than a threshold will be transferred from U to L in every iteration. Generally, we assign |nj| a small value and keep it the same for all the classes. The algorithm runs until the iteration number reaches the given maximum threshold or all the unlabeled examples in U are labeled. The outputs of the algorithm are the updated classifiers CRGBand Cdepth.At the inference time, CRGBand Cdepthare combined to predict the category of the given example based on their classification scores:(1)c=argci∈χMax(αSCRGBci+(1−α)SCdepthci)where χ is the label set of all the categories,SCRGBciandSCdepthciare predicted scores of category cifor an given example, and α is the coefficient to control the contribution of each view.CNN-SPM-RNN is built on the unsupervised feature learning structure of CNN-RNN [4]. CNN-RNN mainly consists of three steps: resizing all the images to the same scale, extracting low level feature for each image by a single convolutional layer, and finally applying multiple fixed-tree RNNs to learn high order feature representation based on the low level feature responses. Although CNN-RNN can learn powerful features from the raw data, such artificial processing of the first step, i.e., resizing all the images to the same scale by simply cropping or warping the images, may degrade the performance of the learned features. In order to adopt CNN-RNN model for images of arbitrary sizes, we replace the first step of CNN-RNN by a SPM layer, which is composed of three steps: feature coding, spatial pyramid pooling and re-organization, as showed in Fig. 3. To fairly compare CNN-SPM-RNN with CNN-RNN, the parameters of the single-layer CNN and multiple RNNs are kept the same as the work [4], i.e.,k1=128filters with 9 × 9 size are learned for the single-layer CNN, the input fixed-scale feature maps for each RNN are 27 × 27 × 128-dimensional (w=27,h=27,k2=128). Now we describe the details of each step of the proposed SPM layer.The goal is to learn high-level local features to represent objects more powerfully, compared with the low-level convolutional descriptors. First, a codebook{c1,c2,…,ck2}(k2=128,ci∈Rk1=128) is learned by k-means clustering over the sampled convolutional descriptors. Second, each convolutional descriptorx∈Rk1=128is encoded by the codebook with triangular voting [33]:(2)f(x)=(fc1(x),fc2(x),…,fck2(x)),s.t.fci(x)=max(0,μ−∥x−ci∥22),μ=1k2∑i=1k2∥x−ci∥22.wheref(x)∈Rk2=128.2D and 3D spatial pyramid pooling are employed for the RGB and depth modality, respectively. For the 2D spatial pyramid pooling, the partitions are constrained in the two-dimensional image space. While for the 3D spatial pyramid pooling, the partitions are performed in the three-dimension depth space. See Fig. 3 for an intuitive understanding. The work [34] also showed that 3D spatial pyramids were necessary to represent the depth modality. In this paper, we set the number of the pyramid bins as27×27=729,in order to obtain the same size of the fixed-scale feature maps as [4] for a fair comparison. For each bin, max pooling is used to aggregate the neighboring features to a 128-dimensional feature vector.After spatial pyramid pooling, the convolutional responses with arbitrary sizes are transformed to a fixed number of feature vectors. We re-organize all the feature vectors to a 3D feature map (∈R27×27×128) with a fixed order. Finally, the 3D feature map is input to multiple RNNs to learn the global feature representation as [4].We employ CNN-SPM-RNN to extract features for each modality of RGB (2D spatial pyramids), grayscale (2D spatial pyramids), depth (3D spatial pyramids) and surface normals (2D spatial pyramids), respectively. For each object, the RGB feature and grayscale feature are concatenated to represent the appearance information, while depth feature and surface normal feature are combined to capture shape cues.Various features have already been developed for RGB-D object recognition. In this section, we introduce four state-of-the-art features: kernel descriptors (KDES) [1], convolutional k-means descriptors (CKM) [2], hierarchical matching pursuit (HMP) [3], and convolutional-recursive neural networks (CNN-RNN) [4], which are more discriminative and robust than the popular orientation histogram features, such as SIFT [35] and spin images [36]. In order to compare these features with the CNN-SPM-RNN, we analyse the characteristics of them first, and then propose a unified framework to extract all these features effectively for an unbiased evaluation.The four representative features: KDES [1], CKM [2], HMP [3] and CNN-RNN [4], employ very different methods to extract features from the raw data, compared to the handcrafted features utilized in the baseline work [6]. Furthermore, they take advantage of different data modalities among RGB images, grayscale images, depth maps and surface normals to capture cues for object recognition, as shown in Table 1. The analysis of the above features is shown as follows.The baseline work [6] extracts a set of handcrafted features to represent the two distinct views of RGB-D objects. To capture the visual appearance of the RGB view, they extract SIFT descriptors over grayscale images, text on histograms  [37] and color histograms over the RGB images. The shape of the depth view is represented by spin images computed from depth maps and surface normals. Regardless of their effectiveness, these well tuned handcrafted features are hard to design and only can capture a small set of recognition cues from raw data. For example, SIFT is able to capture some sort of edge information while ignores color information; spin images are extended to 3D objects analogous to SIFT over 2D images, but also has limited capability to capture useful shape or geometry information.KDES [1] provides a generalized way to extend orientation histogram features like SIFT to a broad class of similar feature patterns. The previous work [38] has already shown that the well-designed SIFT features are equivalent to a certain type of match kernel over image patches. Thus, it is very convenient to design a set of kernel descriptors on top of various attributes, including 3D shape, physical size, edges, gradients, etc.CKM [2] adapts single-layer feature learning networks based on k-means clustering for 2D images [33] to RGB-D data. To keep the feature learning process as effective as [33], CKM takes the depth channel as the fourth channel of the RGB channels and directly learns features from the four channels. By using the state-of-the-art image pre-processing and feature encoding of  [33], CKM can obtain useful translational invariance of low-level features from raw data such as edges, and can be robust to small deformations of objects. However, without information of grayscale images and surface normals, the performance of CKM is restricted a lot for object recognition.HMP [3] constructs a two-layer architecture to generate features over complete RGB-D images based on sparse coding. It can discover low-level structures such as edges at the first layer, and high-level structures such as shapes and object parts at the second layer. HMP learns features from each data modality, then combines the RGB and grayscale features to represent the visual appearance of the RGB view, and captures the shape cues by concatenating the depth and surface normal features.CNN-RNN [4] is a deep feature learning model based on a combination of convolutional and recursive neural networks. The single CNN layer can learn low-level translationally invariant features which are assembled by multiple RNNs [28] to construct high order representation. Similar to CKM, CNN-RNN only makes use of RGB images and depth maps for object recognition.Both the baseline work and KDES utilize manually designed features, while CKM, HMP, CNN-RNN and CNN-SPM-RNN belong to unsupervised feature learning methods.To obtain an unbiased evaluation for all the above features, we propose a unified framework to represent RGB-D objects by adapting them to the four data modalities, as shown in Fig. 4. For each type of features, the RGB and grayscale images are used to capture visual appearance of the RGB view, and the depth and surface normal images are exploited to capture shape cues of the depth view. We can obtain more powerful view representation by utilizing additional information of grayscale and surface normals, compared to those only based on RGB and depth in their original papers.Specifically, we are capable of extracting CKM descriptors from each data modality respectively following the same process of [2] over RGB-D images. Then, we learn the image-level features for each data modality using a bag-of-words model with spatial pyramid pooling [30]. Finally, following the framework, CKM features from the four data modalities will be combined to represent the RGB view and depth view respectively. To distinguish our CKM features from the original paper [2], we call them enhanced CKM. Similarly, we employ CNN-RNN to learn features not only from the RGB and depth images but also from the grayscale and surface normal images. Then the RGB and grayscale features are combined to describe the RGB view, while the depth and surface normal features are concatenated to represent the depth view. We call our CNN-RNN features enhanced CNN-RNN. Since the baseline work, KDES, HMP and CNN-SPM-RNN have already taken advantage of the four data modalities for object recognition, we keep them the same with the original papers.

@&#CONCLUSIONS@&#
