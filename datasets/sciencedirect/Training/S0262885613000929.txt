@&#MAIN-TITLE@&#
Motion-based segmentation of objects using overlapping temporal windows

@&#HIGHLIGHTS@&#
We divide a video sequence on overlapping subsequences.On each subsequence we perform over-segmentation, handling efficiently asynchronous trajectories.The number of moving objects is automatically estimated.The segmentation results are aggregated into a final segmentation.Our method is tested on the Berkeley motion segmentation benchmark.

@&#KEYPHRASES@&#
Motion segmentation,Overlapping windows,Ranking,Local subspaces,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
The volume of video data recorded and stored has increased exponentially in the past years. Hence, there is a growing demand for video processing algorithms that can efficiently analyze and interpret the content of a video sequence. An important preliminary step in this direction, is the detection and separation of individual objects in a video. As studies on the human visual system have shown [1,2], motion is an important perceptual cue for the segmentation of objects. Consequently, much of computer vision research has been devoted to motion segmentation algorithms that try to imitate this attribute, by segmenting the objects in a video according to their motion.Motion segmentation algorithms usually take as input the trajectories of a set of feature points tracked throughout a video. There is a large family of motion segmentation methods, influenced by the factorization algorithm [3], that adopt the affine camera model due to the geometric constraints the trajectories have to satisfy. One such approach is Generalized Principal Component Analysis (GPCA) [4] which uses a set of polynomials of degree n to fit a union of n subspaces. The derivatives of the polynomials at a point, produce a vector normal to the subspace the point belongs. The Local Subspace Affinity (LSA) [5] computes an affinity matrix between subspaces locally fitted to each point and then applies spectral clustering. Another algorithm, Agglomerative Lossy Compression (ALC) [6], assumes that data are drawn from a mixture of degenerate Gaussians. The algorithm proceeds by searching for the segmentation that minimizes the overall coding length of the segmented data. An approach relevant to ours is presented in [7]. In this algorithm, for each trajectory a ranking of a set of putative subspace hypotheses is computed and used to obtain the Ordered Residual Kernel. Next, KPCA and spectral clustering are applied on the produced kernel matrix. Another method is SCC [8] which uses polar curvature to compute an affinity between pairs of trajectories. More recent algorithms are Sparse Subspace Clustering (SSC) [9,10], Spectral Clustering (SC) [11], Low Rank Recovery (LRR) [12] and its expansion Latent LRR (LatLRR) [13]. SSC builds on the theory of sparse representation, utilizing the fact that every trajectory can be written as a sparse linear combination of trajectories of the same motion. Using the sparse representation of each trajectory, an affinity matrix is computed which is used for spectral clustering. The SC algorithm finds the dimension of the ambient space of the data and then applies spectral clustering. The LRR method, given a set of data vectors, finds the lowest rank matrix that can express the vectors as linear combinations of a dictionary. When used for motion segmentation both the dictionary and the original data vectors coincide with the matrix of trajectories. LatLRR is an enhanced version of LRR that uses both observed and unobserved data for the dictionary.Most of the aforementioned algorithms have been tested on the Hopkins155 dataset [14] and some of them have shown exceptional results. This dataset has certainly helped researchers to evaluate their algorithms, however it is more suitable for generic subspace clustering methods and lacks some of the inherent challenges of motion segmentation. These challenges include long video shots, occlusions between objects and asynchronous as well as erroneous trajectories. To address this issue the authors of [15] have expanded the Hopkins155 dataset and introduced the more realistic Berkeley Motion Segmentation dataset along with an evaluation kit. Many algorithms have been tested on this benchmark including the one presented in [15]. Their method defines an affinity between trajectories even when they have small temporal overlap and then performs spectral clustering with model selection. The algorithm presented in [16] uses the previous method to produce clusters of point trajectories and computes a dense segmentation by exploiting color and edge information. The same authors in [17] adopt a higher order motion model that includes rotation and scaling. Then they perform motion segmentation using spectral clustering on hypergraphs. A different approach is presented in [18] where motion segmentation is modeled as a graph partitioning problem, where each node corresponds to a trajectory. Repulsive weights are set between trajectories that belong to different connected components in their duration. In [19] instead of performing spectral clustering, the discontinuities of spatially neighboring trajectories are detected. Another approach presented in [20] combines image segmentation and optical flow algorithms on GPU to perform fully dense motion segmentation even on long video sequences. The work in [21] follows a Bayesian framework constructing a graph from region trajectories for different frames.While the latter approaches handle occlusions and asynchronous trajectories most of them become intractable for long video sequences. This stems from the large number of trajectories present in a long or rapidly changing video that can render the respective affinity matrix intractably large. The algorithm proposed in this paper overcomes this difficulty by temporally segmenting a video sequence into overlapping windows where the number of trajectories is manageable. Motion segmentation is performed on each temporal window and the individual results are aggregated into a final segmentation. Beyond that, our algorithm detects automatically the number of moving objects in a video sequence and handles effectively asynchronous trajectories.In Fig. 1we show a graphical abstract of the method. In the next section we present a mathematical formulation of the motion segmentation problem under the affine camera model. Section 3.1 explains how the proposed algorithm performs motion segmentation on each temporal window while Section 3.2 describes how the individual segmentations are combined to yield the final aggregate segmentation result. Extensive experiments demonstrating the efficacy of the algorithm have been included in Section 4. The same section contains also a detailed comparison of the proposed method against the state of the art methods proposed in [15 and 17].A tracking algorithm applied on a video sequence of F frames, produces a set Q={ti}i=1..Lof point trajectories. Each trajectory tihas at most 2F elements and contains the 2D image coordinates of a tracked feature point. A trajectory may have missing entries throughout the duration of the video due to occlusions, the camera's limited field of view and mistracking caused by illumination changes. However, for the theoretical analysis of this section we assume that the trajectories {ti}i=1..Lare complete. This constraint will be relaxed in the following section.Under the affine camera model the trajectories of a rigidly moving object, regardless of their duration, lie in an affine subspace of dimension at most 3 [3]. To prove this claim, suppose that M pointsXii=1..MXi∈R3following a rigid motion are tracked for F frames. At frame j an affine camera projects {Xi}i=1..Mon points {xij}i=1..Mof the camera plane according to equation:(1)x1j⋯xMj⏟2×M=Ωj⏟2×3X1…XM⏟3×M+dj…dj⏟2×M.By concatenating the respective equations over F frames the following equation emerges:(2)x11⋯xM1⋮x1F⋯xMF⏟U:2F×M=Ω1⋮ΩF⏟Ω:2F×3X1⋯XM⏟X:3×M+d1⋯d1⋮dF⋯dF⏟D:2F×Mwhere the ith column of U represents trajectory ti. Hence, for a single moving object the column space of U is an affine subspace of dimension at most 3. The dimension is 2 if points {Xi}i=1..Mlie on a plane and 1 if they are on a line. As a consequence, when X has full rank, any trajectory tican be expressed as an affine combination of 4 others following the same motion:(3)ti=∑j=14ajtkj,where∑j=14aj=1,i≠kjj∈1..4.Consequently, in the general case a 4-tuple of trajectories, belonging to a rigidly moving object, forms a basis for all the trajectories of the object. Our algorithm relies heavily on this observation, since it samples 4-tuples of trajectories. We assume that the sampled 4-tuples will include bases for all the available trajectories. Next, our algorithm computes for each trajectory a ranking vector by sorting these 4-tuples according to the respective reprojection error. If the affine camera model is a valid approximation, the entries in the ranking vector of a trajectory having the lowest error, correspond to 4-tuples following the same motion as the trajectory.When the available data are corrupted by noise or the affine model is inadequate, Eq. (3) will be only approximately satisfied. In this case, the vector a=[a1,a2,a3,a4]Tof affine coefficients will be obtained by minimizing(4)Ja=ti−∑j=14ajtkj2,subjecttoaT1→=1with respect to a, where1→is the vector of all ones. The minimum of Jais the residual of embedding tiin the affine subspace spanned bytkjj=1..4.Our algorithm assumes that a tracker has already been applied on a video shot of F frames and a set of trajectories Q={ti}i=1..Lhas been extracted. The duration of each trajectory ranges from 2 up to F frames. The number of trajectories L can be very large depending on the density of the tracking algorithm, the duration of the shot as well as its content. Most existing methods model motion segmentation as a graph partitioning problem where each node represents a trajectory and an affinity between nodes is computed. The graph is usually partitioned using spectral clustering.However, if L is large enough, spectral clustering can become inapplicable both in terms of computational complexity and memory storage. To avoid this difficulty, the proposed algorithm divides temporally each shot in N overlapping windows {wi}i=1..Nof f frames, as Fig. 2illustrates. Subsequently, the algorithm processes each window separately. As we explain in the following section, for every window withe algorithm uses a small number of complete trajectories throughout the window. In the unlikely case where all the trajectories in wiare incomplete the length of the window is reduced.At this level of the algorithm, our goal is not to produce a final segmentation. Instead we want to detect segments corresponding to a single object even if objects are oversegmented. Therefore, we defer to impose a hard constraint on the number of objects to Section 3.2.3.The knowledge of the number of moving objects in each time window wiis essential for the subsequent steps of the algorithm. A rough estimate of this number is initially obtained by considering the set of motion vectors associated with the complete trajectories in wi. Motion between the first and the last frame of wiis considered. The algorithm uses these motion vectors, to extract the number of moving objects in wiand to create for each object a small set of trajectories. These sets will be next used to sample putative subspace hypotheses for the following step of the algorithm.We employ the center-based clustering algorithm proposed in [23] that returns the number A(wi) of different motions and a set sj(wi)(j=1..A(wi)) of representative trajectories for every detected object in wi. As Fig. 3a illustrates, some objects may have much more motion vectors compared to others. In this case the method presented in [23] may fail to detect small objects. To rectify this imbalance, a 2D histogram is computed where the axes correspond to the magnitude and angle of the motion vectors while the range for every bin is the same. A constant number of vectors is drawn from each bin. Using this simple scheme a more balanced set of motion vectors is acquired as the example in Fig. 3b shows. In Fig. 3c the clustering algorithm used, successfully separates the selected motion vectors.This scheme may underestimate the number of objects when they follow similar motions. Therefore, when a window is processed, the algorithm does not impose a hard constraint on the number of objects, assuming that two objects under similar motions in a time window might follow more easily distinguishable motions in another. To this end, in Section 3.2.3, where spectral clustering is performed on a graph corresponding to an oversegmented video sequence, the number of objects A is set to(5)A=maxiAwi.Furthermore, in the case of non-translational motions the number of moving objects may be overestimated, since motion vectors of the same object can be quite different. To alleviate this problem, we set the duration of each time window to a small value, where the translational approximation is adequate for most motions. In addition the clustering algorithm used [23] has a penalty parameter for the creation of new clusters. When the value of this parameter is relatively high, one can expect that motion vectors with small differences will be assigned to the same cluster.At this point we assume that each set of trajectories sj(wi), that was previously computed, roughly corresponds to a moving object in window wi. From each set sj(wi), B 4-tuples of trajectories are randomly sampled. Consequently, there will be (A(wi)×B) 4-tuples for window wi. Theoretically, as explained in Section 2, the span of the trajectories in each 4-tuple, coincides with the affine subspace spanned by the trajectories of the respective moving object. Therefore each such 4-tuple is referred to as a motion hypothesis. Due to tracking noise and limitations of the affine camera model, the residual of Eq. (4) will be nonzero even for trajectories of the same object. Moreover, if misclassification errors occur in the clustering of the motion vectors, there can be erroneous motion hypotheses containing trajectories of different objects. Therefore, the residual should not be directly used to correlate a trajectory with a motion hypothesis, since its value can be relatively high or low depending on the motion.The algorithm takes these limitations into consideration and does not relate directly a trajectory with a motion hypothesis using their residual. Instead, a ranking vector is computed for every trajectory in wi, consisting of the motion hypotheses' indices sorted according to their residual. Concretely, the error vectorewitkis formed for trajectory tkby computing the residuals for every motion hypothesis in window wi:(6)ewitk=e1witke2witk⋯eAi×BwitkThen by sorting the elements ofewitkin descending order, vectore˜witkemerges:(7)e˜witk=eλ1witkeλ2witk⋯eλAi×Bwitkwhereeλjwitk≤eλj+1witkj∈1,…,Ai×B−1finally the ranking vector of trajectory tkis:(8)rtk=λ1λ2⋯λAi×B.It is roughly expected that the indices of motion hypotheses containing trajectories under the same motion as tkwill concentrate on the left part of r(tk), even if their residual is nonzero. Intuitively, one can also assume that the ranking vectors of trajectories belonging to the same object will be relatively similar, even if the object was not detected and no motion hypotheses from its trajectories were drawn.It should also be emphasized that a ranking vector can be computed, even if a trajectory is incomplete in the current window. In order to compute the residual of a motion hypothesis for an incomplete trajectory, the algorithm uses only the entries with the same timespan as the trajectory. Using this methodology even trajectories with small or no temporal overlap can be compared through their ranking vectors.The algorithm uses the ranking vectors to compute an affinity between trajectories. Trajectories are assigned to the same cluster if they are connected with high affinities. Oversegmentation is acceptable at this point as long as each cluster corresponds to a single object.Various metrics can correlate rankings [24–26], but we use the ORK, since it has successfully been applied on other motion segmentation algorithms [7,27]. The ORK K(m,n) for a pair of trajectories (tm,tn) is defined as:(9)Kmn=1Z∑n=1D/h1n⋅k∩nrtm,rtnwhereZ=∑n=1D/h1n, h is a step parameter, and k∩n(r(tm),r(tn)) represents the Difference of Intersection Kernel [28]. The algorithm computes the ORK between all pairs of trajectories in wi, and forms a symmetric L(wi)×L(wi) affinity matrix K, where L(wi) is the number of trajectories in window wi.We use the elements of K as the weights of an undirected graph of L(wi) nodes, where each one represents a trajectory in window wi. The graph is partitioned by repeatedly selecting a node at random and finding all the nodes that connect to it through a path of high affinities. The algorithm concludes when all nodes are assigned to a subgraph. This procedure is described in Algorithm 1.More sophisticated partitioning algorithms, such as spectral clustering [29] can be used. Nonetheless, we preferred Algorithm 1, because it does not assume knowledge of the number of subgraphs as a parameter and can detect a subgraph even if its nodes are not fully connected with high affinities. Algorithm 1 only requires that a transitive path exists between the nodes. This attribute is crucial in detecting large objects, since spatially neighboring trajectories of an object tend to have higher affinities compared with more distant ones. Consequently, an algorithm detecting fully connected subgraphs could fragment large objects as the example in Fig. 4illustrates.Algorithm 1 will often oversegment the objects in the window. However, most of the segments will comprise a small number of trajectories and larger ones can absorb them, following the agglomerative scheme described in Algorithm 2. We measure the compatibility between segments using the error metric proposed in Section 3.2.2.At this point the algorithm has for each temporal window {wi}i=1..Nan oversegmentation of its trajectories. To obtain a final segmentation we propose a mechanism to combine the results of consecutive windows and a metric of similarity between segments.Algorithm 1Graph partitioningMerge Small SegmentsThe algorithm has processed each temporal window wi,i∈{1,⋯,N} and produced a separate segmentation for their trajectories. While temporally adjacent windows will often have similar segmentations they cannot be directly merged, since trajectories, and consequently segments, between windows do not have a bijective relation. For example, new objects can enter a scene or segments may vanish due to occlusion.To associate segments of different windows, we first devise a method that fuses the segmentation choices of two successive windows. On each window we create a new segment containing trajectories that are present only on the other window. For example consider windows w′ and w″ in Fig. 5a. Algorithms 1 and 3 have so far identified segmentsS′1andS′2in w′ and segmentS″1in w″. The initial step of the aggregation if to artificially insert segmentsS′3andS″2, even though they contain trajectories that are not tracked for w′ and w″ respectively. With these insertions the union of the segments will be the same set of trajectories for both windows. The next step of the algorithm computes the intersection between segments of different windows. This produces new finer segments with the timespan of both windows as illustrated in the examples of Figs. 5b and 6.The algorithm continues by merging elements of this new set of segments that is now common to both w′ and w″ in order to obtain larger segments that resemble rigid objects. The proposed merging scheme relies on the use of the dissimilarity score presented in Section 3.2.2.A vital component of our algorithm is the motion dissimilarity score used to determine the affinity between two segments in Algorithms 2 and 4. Let Spand Sqbe any two such segments that contain trajectoriest1p,t2p,…,tLppandt1q,t2q,…,tLqqrespectively. Although Spand Sqare spatially disjoint they may have an overlapping timespan. Using this overlap, we judge regarding their motion similarity in order to decide whether they correspond to the same solid motion, following the scheme described in Algorithm 3.Concretely, for Spwe compute the affine fundamental matrix Fk(Sp) for every pair of consecutive frames k and k+1, using the Mkpavailable trajectories at each transition as shown in Fig. 7. Then, for Sqwe compute at each frame transition the Sampson distance [30] for the available Mkqcorrespondences using the previously computed Fk(Sp). These distances form the error vector E(Fk(Sp),Sq) that has Mkqelements. Concatenating the error vectors for all frame transitions, the algorithm produces a motion dissimilarity vector E(Sp,Sq) where:(10)ESpSq=⋯EFk−1Sp,SqEFkSp,Sq⋯.Similar to [15], we assume that the peaks of this vector correspond to the time instants that the motions of the segments are most dissimilar. Hence, these maxima are very informative in order to decide whether two segments should remain separated or be merged. To illustrate this point, consider the scenario where two segments follow different motions only for a small number of frames. If we use the peaks of the dissimilarity vector, we could easily distinguish their different motions, while an averaging scheme would fail to keep the segments separated. In addition to this, we must take into consideration that measurements can be erroneous due to mistracking or noisy trajectories especially when few correspondences are available.To satisfy these observations, we assume that if the number of correspondences Mkpused to calculate Fk(Sp) is large its computation is more accurate. Hence, the influence of the respective error vector should increase. Under this assumption each error vector E(Fk(Sp),Sq) is replicated proportionally to Mkpand vectorE˜FkSp,Sqemerges. Since the maximum of this vector can be affected by outliers, we employ a high percentile ofE˜FkSp,Sqas the final dissimilarity score. For our experiments we use the 90th percentile.Using the metric defined in Section 3.2.2 for every pair of segments, we create a dissimilarity matrix. The algorithm merges segments with a dissimilarity below a predefined threshold. Algorithm 4 describes in detail how we merge the segmentation results of two consecutive windows. It should be emphasized that this mechanism can be applied independent of the other modules of the algorithm. It is useful, whenever parts of a video sequence are processed separately and we want to combine the segmentation results.Various merging schemes are possible depending on the application, including the schemes illustrated in Fig. 8. The windows are combined in pairs in Fig. 8a while in Fig. 8b we merge the first window with the second. This last scheme would be suitable for instance, if the segmentation algorithm runs parallel to the tracking.In most cases after all windows are merged into a single one, the number of segments will be greater than the number of objects computed by Eq. (5). Hence, to obtain a final segmentation with the number of segments specified in Eq. (5), spectral clustering [29] is applied. We compute the affinity matrix for spectral clustering using the RBF kernel, where the distance is given by the metric defined in Section 3.2.2.Algorithm 3Dissimilarity MetricAggregate successive windows

@&#CONCLUSIONS@&#
In this paper, we proposed a novel and modular approach to motion segmentation where video sequences are divided in time windows. Initial segmentation is carried out within a time window level and an aggregation mechanism for fusing multiple windows' segmentations has been introduced. Our algorithm has shown promising results and is suitable even for long video sequences.In future work we plan on experimenting with different affinity metrics, introducing cues complementary to motion and with more challenging videos. Finally, a mechanism where the window size adapts to the content of the sequence will be a promising addition.