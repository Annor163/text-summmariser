@&#MAIN-TITLE@&#
Interference effects in quantum belief networks

@&#HIGHLIGHTS@&#
Quantum probability can deal with violations to the laws of classical probability.We propose a Quantum Bayesian Network with interference effects.Mathematical derivation of quantum interference terms from complex numbers.Analysis of the impact of quantum parameters in probabilistic inferences.Analysis of the impact of uncertainty in the proposed quantum Bayesian Network.

@&#KEYPHRASES@&#
Bayesian Networks,Quantum probability,Inference process,Interference,Sure thing principle,Cognitive psychology,

@&#ABSTRACT@&#
Probabilistic graphical models such as Bayesian Networks are one of the most powerful structures known by the Computer Science community for deriving probabilistic inferences. However, modern cognitive psychology has revealed that human decisions could not follow the rules of classical probability theory, because humans cannot process large amounts of data in order to make judgments. Consequently, the inferences performed are based on limited data coupled with several heuristics, leading to violations of the law of total probability. This means that probabilistic graphical models based on classical probability theory are too limited to fully simulate and explain various aspects of human decision making.Quantum probability theory was developed in order to accommodate the paradoxical findings that the classical theory could not explain. Recent findings in cognitive psychology revealed that quantum probability can fully describe human decisions in an elegant framework. Their findings suggest that, before taking a decision, human thoughts are seen as superposed waves that can interfere with each other, influencing the final decision.In this work, we propose a new Bayesian Network based on the psychological findings of cognitive scientists. In Computer Science, to the best of our knowledge, there is no quantum like probabilistic system proposed, despite their promising performances. We made experiments with two very well known Bayesian Networks from the literature. The results obtained revealed that the quantum like Bayesian Network can affect drastically the probabilistic inferences, specially when the levels of uncertainty of the network are very high (no pieces of evidence observed). When the levels of uncertainty are very low, then the proposed quantum like network collapses to its classical counterpart.

@&#INTRODUCTION@&#
The problem of violations of the axioms of probability go back to the early 60s. Ellsberg [32] published a work that influenced modern psychology by showing that humans violate the laws of probability theory when making decisions under risk. The principle that humans were constantly violating is defined by The Sure Thing Principle. It is a concept widely used in game theory and was originally introduced by Savage [64]. This principle is fundamental in Bayesian probability theory and states that if one prefers action A over B under state of the world X, and if one also prefers A over B under the complementary state of the world X, then one should always prefer action A over B even when the state of the world is unspecified.Cognitive psychologists Tversky and Kahneman also explored more situations where classical probability theory could not be accommodated in human decisions. In their pioneering work, Tversky and Kahneman [69] realized that the beliefs expressed by humans could not follow the rules of Boolean logic or classical probability theory, because humans cannot process large amounts of data in order to make estimations or judgments. Consequently, the inferences performed are based on limited data coupled with several heuristics, leading to a violation on one of the most important laws in Bayesian theory: the law of total probability.One of the key differences between classical and quantum theories is the way how information is processed. According to classical decision making, a person changes beliefs at each moment in time, but it can only be in one precise state with respect to some judgment. So, at each moment, a person is favoring a specific belief. The process of human inference deterministically either jumps between definite states or stays in a single definite state across time [19]. Most computer science, cognitive and decision systems are modeled according to this single path trajectory principle. Fig. 1illustrates this idea.In quantum information processing, on the other hand, information (and consequently beliefs) are modeled via wave functions and therefore they cannot be in definite states. Instead, they are in an indefinite quantum state called the superposition state. That is, all beliefs are occurring on the human mind at the same time. According to cognitive scientists, this effect is responsible for making people experience uncertainties, ambiguities or even confusion before making a decision. At each moment, one belief can be more favored than another, but all beliefs are available at the same time. In this sense, quantum theory enables the modeling of the cognitive system as it was a wave moving across time over a state space until a final decision is made. From this superposed state, uncertainty can produce different waves coming from opposite directions that can crash into each other, causing an interference distribution. This phenomena can never be obtained in a classical setting. Fig. 2exemplifies this. When the final decision is made, then there is no more uncertainty. The wave collapses into a definite state. Thus, quantum information processing deals with both definite and indefinite states [19].Tversky and Shafir [71] were one of the first researchers to test the veracity of Savage's principle under human cognition in a gambling game. In their experiment, participants were asked at each stage to make the decision of whether or not to play a gamble that has an equal chance of winning $200 or losing $100. Fig. 3illustrates the experiment. Three conditions were verified:1Participants were informed if they had won the first gamble;Participants were informed if they had lost the first gamble;Participants did not know the outcome of the first gamble.The two-stage gambling game was one of the first experiments used in order to determine if the sure thing principle would be verified even with people that did not know about the existence of this principle. The results obtained in Tversky and Shafir [71] experiment showed that this principle is constantly being violated and consequently humans do not perform inferences according to the laws of probability theory and Boolean logic.The overall results revealed that participants who knew that they won the first gamble, decided to play again. Participants who knew that they lost the first gamble, also decided to play again. Through Savage's sure thing principle, it was expected that the participants would choose to play again, even if they did not know the outcome of the first gamble. However, the results obtained revealed something different. If the participants did not know the outcome of the first gamble, then many of them decided not to play the second one.Several researchers replicated this experiment. The overall results are specified in Table 1.Why did the findings reported in Table 1 generate so much controversy in the scientific community? Because, the data observed is not in accordance with the classical law of total probability. In Tversky and Shafir's experiment [71], the probability of a participant playing the second gamble, given that the outcome of the first gamble is unknown, Pr(G|U), can be computed through the law of total probability:(1)Pr(G|U)=Pr(W|U)·Pr(G|W)+Pr(L|U)·Pr(G|L)In Eq. (1), Pr(W|U) corresponds to the probability of a player winning the first gamble, given that (s)he participated on the game in the first place. Pr(G|W) is the probability of playing the second gamble, given that it is known that the player won the first one. Pr(L|U) corresponds to the probability of losing the first gamble, given that the participant decided to play the game in the first place. And finally, Pr(G|L) is the probability of a participant playing the second gamble, given that it is known that (s)he lost the first one.Following the law of total probability in Eq. (1), the probability of playing the second gamble, given that the player did not know the outcome of the first one, should be between the following values [19]:(2)Pr(G|W)≥Pr(G|U)≥Pr(G|L)The findings reported by Tversky and Shafir [71], however, revealed a different relation. Eq. (3) demonstrates that this relation is violating one of the most fundamental laws of Bayesian probability theory:(3)Pr(G|W)=0.69≥Pr(G|L)=0.58≥Pr(G|U)=0.37Tversky and Shafir [71] explained these findings in the following way: when the participants knew that they won, then they had extra house money to play with and decided to play the second round. If the participants knew that they lost, then they chose to play again with the hope of recovering the lost money. But, when the participants did not know if they had won or lost the first gamble, then these thoughts, for some reason, did not emerge in their minds and consequently they decided not to play the second gamble. Other works in the literature also replicated this two-stage gambling experiment [65,50,51], also reporting similar results to Tversky and Shafir [71]. Their results are summarized in Table 1.There have been different works in the literature trying to explain and model this phenomena [19,61,23]. Although the models in the literature diverge, they all agree in one thing: one cannot use classical probability theory to model this phenomena, since the most important rules are being violated. This two stage gambling game experiment was one of the most important works that motivated the use of different theories outside of classical Bayesian theory and Boolean logic, more specifically the usage of quantum probability theory.Recent findings in the cognitive psychology literature revealed that humans are constantly violating the law of total probability when making decisions under risk [14,23,24]. These researchers also showed that quantum probability theory enables the development of decision models that are able to simulate human decisions. Given that most of the systems that are used nowadays are based on Bayesian probability theory, is it possible to achieve better inference mechanisms in these systems using quantum probability theory? For instance, many medical diagnosing systems are based in classical probabilistic graphical models such as Bayesian Networks. Can one achieve better performances in diagnosing patients using quantum probability?Generally speaking, a Bayesian Network is a probabilistic graphical model that represents a set of random variables and their conditional dependencies via a directed acyclic graph.There are two main works in the literature that have contributed to the development and understanding of Quantum Bayesian Networks. One belongs to Tucci [68] and the other to Leifer and Poulin [53].In the work of Tucci [68], it is argued that any classical Bayesian Network can be extended to a quantum one by replacing real probabilities with quantum complex amplitudes. This means that the factorization should be performed in the same way as in a classical Bayesian Network. One big problem with Tucci's work is concerned with the inexistence of any methods to set the phase parameters. The author states that, one could have infinite Quantum Bayesian Networks representing the same classical Bayesian Network depending on the values that one chooses to set the parameters. This requires that one knows a priori which parameters would lead to the desired solution for each node queried in the network (which we never know).In the work of Leifer and Poulin [53], the authors argue that, in order to develop a quantum Bayesian Network, it is required a quantum version of probability distributions, quantum marginal probabilities and quantum conditional probabilities. The proposed model fails to provide any advantage relatively to the classical models, because it cannot take into account interference effects between unobserved random variables. In the end, both models provide no advantages in modeling decision making problems that try to predict decisions that violate the laws of total probability.In this paper, the core of the proposed Bayesian Network is based on the psychological findings uncovered in the works of Busemeyer et al. [23,24], Busemeyer and Bruza [19], Pothos and Busemeyer [61] and on quantum information processing. These authors show that, before taking a decision, human thoughts are seen as superposed waves that can interfere with each other, influencing the final decision. In Bayesian Networks, nodes can either be query variables, evidences or simply unknown. Given that we do not observe the unknown nodes of a Bayesian Network, since we do not know for sure what values they can take, then what would happen to the inference process if these nodes are put in a representation of a quantum superposition and interfere with each other (Fig. 4)? Can a better inference process be achieved? These are the main research questions that this paper aims at answering. So far, to the best of our knowledge, there is no previous work in the Computer Science community that attempts to map these psychological findings into computer science decision making systems, such as Bayesian Networks.In order to validate our hypothesis, we performed experiments with well known classical Bayesian Networks from the literature. We first create a Quantum Bayesian Network that can accommodate the paradoxical findings in the two-stage gambling game. We then generalize our Quantum Bayesian Network in order to deal with larger and more complex datasets that are used in the literature: the well known Burglar/Alarm Bayesian Network from Russel and Norvig [63] and the Lung Cancer Bayesian Network from Pearl [59].Before describing the proposed model, we first need to introduce some quantum probability concepts for the understanding of this work. Sections 2 and 3 present the main differences between classical and quantum probability theory. Instead of just presenting a set of formulas, we show this difference by means of an illustrative example, just like proposed in [19]. In Section 4, we describe how beliefs can act like waves and interfere with each other. We show mathematically how this interference term can be derived by using well known rules of complex numbers. Section 5 addresses the main works of the literature that contributed for the development of the interference term. It also introduces a new interference formula that will be applied in the proposed quantum probabilistic graphical models. Section 6 presents a comparison between a classical Bayesian Network model against the proposed quantum interference Bayesian Network applied to the problem of two-stage gambles. Section 7 presents another comparison between the classical and quantum Bayesian Networks, but for a more complex network from the literature. In Section 8, it is made a discussion about the results obtained in the experiments performed in Section 7. Section 9 presents an additional experiment over another Bayesian Network in order to study the impact of the quantum interference parameters in different scenarios. Section 10 presents the most relevant works of the literature. Finally, Section 11 presents the main conclusions of this work.In this section, we describe the main differences between classical theory and quantum probability theory through examples. The example analyzed concerns jury duty. Suppose you are a juror and you must decide whether a defendant is guilty or innocent. The following sections describe how the classical and quantum theory evolve in the inference process. All these analyses are based on the book of Busemeyer and Bruza [19].In classical probability theory, events are contained in sample spaces. A sample space Ω corresponds to the set of all possible outcomes of an experiment or random trial [31]. For example, when judging whether a defendant is guilty or innocent, the sample space is given by Ω={Guilty, Innocent}. Fig. 5presents a diagram showing the sample space of a defendant being guilty or innocent.In quantum probability theory, events are contained in the so-called Hilbert spaces. A Hilbert space H can be viewed as a generalization and extension of the Euclidean space into spaces with any finite or infinite number or dimensions. It can be see as a vector space of complex numbers and offers the structure of an inner product to enable the measurement of angles and lengths [40]. The space is spanned by a set of orthonormal basis vectors H={Guilty, Innocent}. Together, these vectors form a basis for the space. Fig. 6presents a diagram showing the Hilbert space of a defendant being guilty or innocent [19]. Since a Hilbert space enables the usage of complex numbers, then, in order to represent the events Guilty and Innocent, one would need two dimensions for each event (one for the real part and another for the imaginary part). In quantum theory, one usually ignores the imaginary component in order to be able to visualize geometrically all vectors in a two-dimensional space.In classical probability theory, events can be defined by a set of outcomes to which a probability is assigned. They correspond to a subset of the sample space Ω from which they are contained in. Events can be mutually exclusive and they obey to set theory. This means that operations such as intersection or union of events are well defined. Since they respect set theory, the distributive axiom is also defined between sets. In our example, Guilty or Innocent can be seen as two mutually exclusive events.According to quantum probability theory, events correspond to a subspace spanned by a subset of the basis vectors contained in the Hilbert space. Events can be orthogonal, that is, they can be mutually exclusive. Operations such as intersection and union of events are well defined if the events are spanned by the same basis vector [19]. In quantum theory, all events contained in a Hilbert space are defined through a superposition state which is represented by a state vector S comprising the occurrence of all events. In our example, Guilty and Innocent correspond to column vectors representing the main axis of the circle in Fig. 7. They are defined as follows:Guilty=10Innocent=01In Fig. 7, the superposition state S can be defined as follows:(4)S=eiθG2Guilty+eiθI2InnocentIn Eq. (4), one might be wondering what theeiθ/2values mean. They are called probability amplitudes. They correspond to the amplitudes of a wave and are described by complex numbers. The eiθterm is defined as the phase of the amplitude. It can be seen as a shift of the wave. These amplitudes are related to classical probability by taking the squared magnitude of these amplitudes. This is achieved by multiplying the amplitude with its complex conjugate (represented by the symbol *).(5)Pr(Guilty)=eiθG22=eiθG2·eiθG2*=eiθG2·e−iθG2=ei(θG−θG)122=0.5In quantum theory, it is required that the sum of the squared magnitudes of each amplitude equals 1. This axiom is called the normalization axiom and corresponds to the classical theory constraint that the probability of all events in a sample space should sum to one.(6)eiθG22+eiθI22=1A system state is nothing more than a probability function Pr which maps events into probability numbers, i.e., positive real numbers between 0 and 1.In classical theory, the system state corresponds to exactly its definition. There is a function that is responsible to assign a probability value to the outcome of an event. If the event corresponds to the sample space, then the system state assigns a probability value of 1 to the event. If the event is empty, then it assigns a probability of 0. In our example, if nothing else is told to the juror, then the probability of the defendant being guilty is Pr(Guilty)=0.5.In quantum theory, the probability of a defendant being Guilty is given by the squared magnitude of the projection from the superposition state S to the subspace containing the observed event Guilty. Fig. 8shows an example. If nothing is told to the juror about the guiltiness of a defendant, then according to quantum theory, we start with a superposition state S.S=eiθG2Guilty+eiθI2InnocentWhen someone asks whether the defendant is guilty, then we project the superposition state S into the relevant subspace, in this case the Guilty subspace (PG), just like shown in Fig. 8. The probability is simply the squared magnitude of the projection, that is:Pr(Guilty)=|PG|2=eiθG22=0.5Which has exactly the same outcome as in the classical theory.State revision corresponds to the situation where after observing an event, we are interested in observing other events given that the previous one has occurred.In classical theory, this is addressed through the conditional probability formula Pr(B|A)=Pr(A∩B)/Pr(B). So, returning to our example, suppose that some evidence has been given to the juror proving that the defendant is actually guilty, then what is the probability of him being innocent? This is computed in the following way.Pr(Innocent|Guilty)=Pr(Innocent∩Guilty)Pr(Guilty)=0Since the events Guilty and Innocent are mutually exclusive, then their intersection is empty, leading to a zero probability value.In quantum theory, the state revision is given by first projecting the superposition state S into the subspace representing the observed event. Then, the projection is normalized such that the resulting vector is unit length. Again, if we want to determine the probability of a defendant being innocent, given he was found guilty, the calculations are performed as follows. We first start in the superposition state vector S.S=eiθG2Guilty+eiθI2InnocentThen, we observe that the defendant is guilty, so we project the state vector S into the Guilty subspace and normalize the resulting projection.SG=(eiθG/2)Guilty|eiθG/2|2SG=eiθGGuilty+0InnocentFrom the resulting state, we just extract the probability of being innocent by simply squaring the respective probability amplitude. Again, we obtain the same results as the classical theory (Fig. 9).Pr(Innocent|Guilty)=|0|2=0In order to describe direct dependencies between a set of variables, path diagrams are generally used. This section shows how to compute quantum probabilities in a Markov model using Feynman's path rules, just like presented in the work of Busemeyer et al. [23].Consider the diagram represented in Fig. 10.The computation of the probability of transiting from an initial state A to a final state C, transiting from an intermediate state B, can be achieved through a classical Markov model. The probability can be computed by making the product of the individual probabilities for each transition, from one state to another, through the usage of conditional probabilities. According to Fig. 10, the probability of transiting from state A, followed by state B and ending in state C, that is, Pr(A→B→C), is given by:(7)Pr(A→B→C)=Pr(A)·Pr(B|A)·Pr(C|B)In a quantum path diagram model, the computation of the probabilities for a single path trajectory is similar to the classical Markov model. The calculation can be performed using Feynman's first rule, which asserts that the probability of a single path trajectory consists in the product of the squared magnitudes of the amplitudes for each transition from one state to the next along the path. This means that the quantum probability value of a single path trajectory is the same as the classical Markov probability for the same path. In Eq. (8) and through the rest of this work, complex probability amplitudes will be represented by the symbol ψ.(8)Pr(A→B→C)=|ψA|2·|ψB|A|2·|ψC|B|2=Pr(A)·Pr(B|A)·Pr(C|B)An indistinguishable path consists in transiting from an initial state A to a final state D by transiting from multiple possible paths without knowing for certain which path was taken to reach the goal state. Fig. 11shows an example of multiple indistinguishable trajectories. In a classical Markov model, if one does not observe which path was taken to reach the final state D, then one simply computes this probability by summing the individual probabilities of each path. This is known as the single path principle and is in accordance with the law of total probability. So, following Fig. 11, in order to reach state D starting in state A, one can take the path A→B→D or the path A→C→D. The final probability is given by:(9)Pr(A→D)=Pr(A)·Pr(B|A)·Pr(D|B)+Pr(A)·Pr(C|A)·Pr(D|C)Quantum probability theory rejects the single path trajectory principle. If one does not observe which path was taken to reach the goal state, then one cannot assume that one of each possible paths was used. Instead, quantum probability argues that, when the path is unobserved, then the goal state can be reached through a superposition of path trajectories. This is known as Feynman's second rule, which states that the amplitude of transiting from an initial state A to a final state D, taking multiple indistinguishable paths, is given by the sum of all amplitudes for each path. This rule is in accordance with the law of total amplitude and the probability is computed by taking the squared magnitude of this sum. This probability is not equal to the classical Markov model.(10)Pr(A→D)=|ψA·ψB|A·ψD|B+ψA·ψC|A·ψD|C|2=|ψA·ψB|A·ψD|B|2+|ψA·ψC|A·ψD|C|2+2·|ψA·ψB|A·ψD|B|·|ψA·ψC|A·ψD|C|cos(θ)The term cos(θ) is the inner product between the vectors formed by |ψB|A·ψD|B| and |ψC|A·ψD|C|. It comes from Euler's rule: cos(θ)=(eiθ+e−iθ)/2 and corresponds to a quantum interference term that does not exist in classical probability theory. Section 4 details how this term is derived. Since the interference term can lead to estimations with values higher than 1, then it is necessary to normalize this value in order to obtain a probability value.When the path is observed, quantum probability theory collapses to the classical Markov model. This is know as Feynman's third rule and states that the probability amplitude of observed multiple path trajectories corresponds to the sum of the amplitudes of each individual path. The probabilities are then taken by making the squared magnitude of each individual path. Fig. 12illustrates this example:(11)Pr(A→D)=|ψA·ψB|A·ψD|B|2+|ψA·ψC|A·ψD|C|2=Pr(A)·Pr(B|A)·Pr(D|B)+Pr(A)·Pr(C|A)·Pr(D|C)Quantum theory enables the modeling of the decision system as a wave moving across time over a state space until a final decision is made. Under this perspective, interference can be regarded as a chain of waves in a superposition state, coming from different directions. When these waves crash, one can experience a destructive effect (one wave destroys the other) or a constructive effect (one wave merges with another). In either case, the final probabilities of each wave is affected. Psychological findings showed that this not only occurs in a microscopic scale (such as electrons), but also occurs at a macroscopic setting [44,61,24].In this section we show how to derive the interference term by just taking into account well known properties of complex numbers [48]. The interference term can be derived in two different ways: (1) from the general probability formula of the union of N mutually exclusive events and (2) through the total law of probability.For simplicity, we will start by deriving the interference term for 2 events and then we will generalize for N events. The classical probability formula of the union of two mutual exclusive events is given by:(12)Pr(A∪B)=Pr(A)+Pr(B)And the relation between a classical probability density function and a quantum probability amplitude is given by Born's rule, that is:(13)Pr(A)=|eiθAψA|2Again,|eiθAψA|2corresponds to the square magnitude of a complex amplitude. It is obtained by multiplying the probability amplitude with its complex conjugate. That is,|eiθAψA|2=eiθAψAe−iθAψA.Taking into account Eq. (13), one can write a superposition state between two mutual exclusive events A and B in the following way:(14)ψA+B=eiθAψA+eiθBψBThe relation of this superposed state with classical probability theory remains:(15)Pr(A∪B)=Pr(A)+Pr(B)∝|eiθAψA+eiθBψB|2=|ψA+B|2The quantum counterpart of the classical probability of the union of two mutually exclusive events, when we do not observe them, collapses to Feynman's second rule, that is:(16)Pr(A∪B)=|eiθAψA+eiθBψB|2=(eiθAψA+eiθBψB)·(eiθAψA+eiθBψB)*|eiθAψA+eiθBψB|2=eiθAψA·e−iθAψA+eiθAψA·e−iθBψB+eiθBψB·e−iθAψA+eiθBψB·e−iθBψB|eiθAψA+eiθBψB|2=|ψA|2+|ψB|2+eiθAψA·e−iθBψB+eiθBψB·e−iθAψAThe classical probability of the union of two mutual exclusive events is P(A∪B)=Pr(A)+Pr(B). In Eq. (16), the amplitude |ψA|2 corresponds to Pr(A) and |ψB|2 corresponds to Pr(B). So, what is the additional term, eiθAψA·e−iθBψB+eiθBψB·e−iθBψA, that we derived in Eq. (16)? This term does not exist in classical probability theory and is called the interference term. The interference term can be rewritten like in Eq. (17):(17)eiθAψA·e−iθBψB+eiθBψB·e−iθAψA=|ψA||ψB|ei(θA−θB)+|ψA||ψB|ei(θB−θA)Knowing that,cos(θ1−θ2)=ei(θ1−θ2)+ei(−θ1+θ2)2Then Eq. (17) can be rewritten as:(18)eiθAψA·e−iθBψB+eiθBψB·e−iθAψA=2|ψA||ψB|cos(θA−θB)So, the complete quantum probability formula for the union of 2 mutually exclusive events is given by:(19)ψAB=|eiθAψA+eiθBψB|2=|ψA|2+|ψB|2+2|ψA||ψB|cos(θA−θB)In the above formula, the angle θA−θBcorresponds to the phase of the inner product between |ψA| and |ψB|.Eq. (19) only computes the probability of the union of 2 mutually exclusive events. In classical probability theory, if we want to compute the probability of the union of N mutually exclusive events, then we use a generalization of Eq. (12), that is:(20)Pr(A1∪A2∪⋯∪AN)=∑i=1NPr(Ai)In quantum theory, we make an analogous calculation. In order to compute the probability of the union of N mutually exclusive events, then one needs to generalize Eq. (14). The outcome is given by the following formula:(21)Pr(A1∪A2∪⋯∪AN)=|ψA1+ψA2+⋯+ψAN|2=∑i=1NψAi2|eiθ1ψA1+eiθ2ψA2+⋯+eiθANψN|2=∑i=1N|ψAi|2+2∑i=1N−1∑j=i+1N|ψAi||ψAj|cos(θi−θj)This interference term can also be derived directly from the law of total probability. Suppose that events A1, A2, …, ANform a set of mutually disjoint events, such that their union is all in the sample space, Ω, for any other event B. Then, the classical law of total probability can be formulated like in Eq. (22).(22)Pr(B)=∑i=1NPr(Ai)Pr(B|Ai)where∑i=1NPr(Ai)=1The quantum interference law of total probability can be derived through Eq. (22) by applying Born's rule (Eq. (13)). That is:(23)Pr(B)=|∑x=1NeiθxψAxψB|Ax|2where∑x=1N|eiθxψAx|2=1For simplicity, we will expand Eq. (23) for N=3 and only later we will find the general formula for N events:(24)Pr(B)=|eiθ1ψA1ψB|A1+eiθ2ψA2ψB|A2+eiθ3ψA3ψB|A3|2(25)Pr(B)=(eiθ1ψA1ψB|A1+eiθ2ψA2ψB|A2+eiθ3ψA3ψB|A3)(eiθ1ψA1ψB|A1+eiθ2ψA2ψB|A2+eiθ3ψA3ψB|A3)*(26)Pr(B)=(eiθ1ψA1ψB|A1+eiθ2ψA2ψB|A2+eiθ3ψA3ψB|A3)(e−iθ1ψA1ψB|A1+e−iθ2ψA2ψB|A2+e−iθ3ψA3ψB|A3)(27)Pr(B)=eiθ1ψA1ψB|A1e−iθ1ψA1ψB|A1+eiθ1ψA1ψB|A1e−iθ2ψA2ψB|A2+eiθ1ψA1ψB|A1e−iθ3ψA3ψB|A3+eiθ2ψA2ψB|A2e−iθ1ψA1ψB|A1+eiθ2ψA2ψB|A2e−iθ2ψA2ψB|A2+eiθ2ψA2ψB|A2e−iθ3ψA3ψB|A3+eiθ3ψA3ψB|A3e−iθ1ψA1ψB|A1+eiθ3ψA3ψB|A3e−iθ2ψA2ψB|A2+eiθ3ψA3ψB|A3e−iθ3ψA3ψB|A3Simplifying Eq. (27), we obtain:(28)Pr(B)=|ψA1ψB|A1|2+|ψA2ψB|A2|2+|ψA3ψB|A3|2+InterferenceIn Eq. (28), one can see that it is composed by the classical law of total probability and by an interference term. This interference term comes from Eq. (27) and corresponds to:(29)Interference=eiθ1−iθ2ψA1ψB|A1ψA2ψB|A2+eiθ2ψA2ψB|A2e−iθ1ψA1ψB|A1+eiθ1−iθ3ψA1ψB|A1ψA3ψB|A3+eiθ3−iθ1ψA3ψB|A3ψA1ψB|A1+eiθ2−iθ3ψA2ψB|A2ψA3ψB|A3+eiθ3−iθ2ψA3ψB|A3ψA2ψB|A2Knowing thatcos(θ)=eiθ+e−iθ2⇒cos(θ1−θ2)=eiθ1−iθ2+eiθ2−iθ12Then, Eq. (28) becomes(30)Pr(B)=|ψA1ψB|A1|2+|ψA2ψB|A2|2+|ψA3ψB|A3|2+2ψA1ψB|A1ψA2ψB|A2cos(θ1−θ2)+2ψA1ψB|A1ψA3ψB|A3cos(θ1−θ3)+2ψA2ψB|A2ψA3ψB|A3cos(θ2−θ3)Generalizing Eq. (30) for N events, the final probabilistic interference formula, derived from the law of total probability, is given by:(31)Pr(B)=∑i=1N|ψAiψB|Ai|2+2∑i=1N−1∑j=i+1NψAiψB|AiψAjψB|Ajcos(θi−θj)Following Eq. (31), when cos(θi−θj) equals zero, then it is straightforward that quantum probability theory converges to its classical counterpart, because the interference term will be zero.For non-zero values, Eq. (31) will produce interference effects that can affect destructively the classical probability (when interference term in smaller than zero) or constructively (when it is bigger than zero). Additionally, Eq. (31) will lead to a large amount of θ parameters when the number of events increases. For N binary random variables, we will end up with 2Nparameters to tune.In the 20th century, the physicist Max Born proposed a problem related to quantum probabilities, which was later known as the The Inverse Born Problem. The problem consisted in constructing a probabilistic representation of data from different sources (physics, psychology, economy, etc.), by a complex probability amplitude, which could match Born's rule (already presented in Eq. (13)).The probabilistic interference formula for the law of total probability, derived in the previous section (Eq. (31)), can be seen as an answer to the Inverse Born Problem. The most important works in the literature that contributed for the derivation of this interference term, through the law of total probability, correspond to the works of Khrennikov [47,44,48,39,43,26]. These authors address the interference term as |λ(B|A)|. In the situations where |λ(B|A)|≤1, then one can apply the trigonometric formula derived in Eq. (31). However, there are some data where this condition is not verified. Therefore, when |λ(B|A)|≥1, the authors propose the usage of a hyperbolic interference term, to act like an upper boundary in order to constraint the probability value to a maximum value of 1. This would require the usage of hyperbolic Hilbert spaces instead of the complex ones.In this paper, we argue that there is no need to represent probabilities in a hyperbolic Hilbert space in order to avoid non-probability values. Since we will be dealing with probabilistic graphical models, we will always be required to normalize the probability amplitudes when performing probabilistic inferences. For this work, we follow the same probabilistic paradigm used in traditional Bayesian Networks. Thus, we constrain Eqs. (32) and (33) by a normalization factor α that will guarantee that the computed values will always be probabilities lesser or equal than one. This normalization factor corresponds to Feynman's conjecture [33] that an electron can follow any path. Thus, in order to compute the probability Pr(B) that a particle ends up at a point B, one must sum over all possible paths that the particle can go through. Since the interference term can lead to estimations with values higher than 1, then it is necessary to normalize in order to obtain a probability value.(32)Pr(B)=α∑i=1N|ψAiψB|Ai|2+2∑i=1N−1∑j=i+1NψAiψB|AiψAjψB|Ajcos(θi−θj)whereα=1Pr(B)+Pr(¬B)(33)Pr(A1+⋯+AN)=α∑i=1N|ψAi|2+2∑i=1N−1∑j=i+1N|ψAi||ψAj|cos(θi−θj)whereα=1Pr(A1+⋯+AN)+Pr(¬A1+⋯+¬AN)Through the triangular inequality, we can also prove that the proposed interference term is also always positive. This way, the axioms of probability theory that state that 0≤Pr(A)≤1 will always be satisfied. By applying the triangular inequality, one can easily demonstrate that Eq. (33) and, consequently, Eq. (32) have always to be positive.Through the triangular inequality, Eq. (33) can be related to:(34)|ψA1+ψA2+⋯+ψAN|2≤(|ψA1|+|ψA2|+⋯+|ψAN|)2(35)∑i=1N|ψAi|2+2∑i=1N−1∑j=i+1N|ψAi||ψAj|cos(θi−θj)≤∑i=1N|ψAi|2+2∑i=1N−1∑j=i+1N|ψAi||ψAj|(36)∑i=1N−1∑j=i+1N|ψAi||ψAj|cos(θi−θj)≤∑i=1N−1∑j=i+1N|ψAi||ψAj|(37)−∑i=1N−1∑j=i+1N|ψAi||ψAj|cos(θi−θj)+∑i=1N−1∑j=i+1N|ψAi||ψAj|≥0(38)∑i=1N−1∑j=i+1N|ψAi||ψAj|[1−cos(θi−θj)]≥0The maximum value that cos(θi−θj) can have is 1 and the minimum value is −1. So, the minimum value that the term 1−cos(θi−θj) can have is 0 (when the cosine achieves its maximum value). Given thatψA1,ψA2,…,ψANare always positive numbers, then it is straightforward that Eq. (32) has to be always positive.A classical Bayesian Network can be defined by a directed acyclic graph structure in which each node represents a different random variable from a specific domain and each edge represents a direct influence from the source node to the target node. The graph represents independence relationships between variables and each node is associated with a conditional probability table which specifies a distribution over the values of a node given each possible joint assignment of values of its parents. This idea of a node depending directly from its parent nodes is the core of Bayesian Networks. Once the values of the parents are known, no information relating directly or indirectly to its parents or other ancestors can influence the beliefs about it [49].Associated to Bayesian Networks there is always the concept of conditional independence. Two random variables X and Y are conditionally independent given a third random variable Z if and only if they are independent in their conditional probability distribution given Z. In other words, X and Y are conditionally independent given Z, (X=x⊥Y=y|Z), if and only if, given any value of Z, the probability distribution of X is the same for all values of Y and the probability distribution of Y is the same for all values of X.This means that an independence statement over random variables is a universal quantification over all possible values of random variables [49]. Therefore, a probability distribution Pr satisfies (X⊥Y|Z) if and only if:(39)Pr(X,Y|Z)=Pr(X|Z)Pr(Y|Z)In classical probability theory, a random variable X is defined by a function that associates a value to each outcome in the sample space Ω,X:Ω→ℝ.In the two-stage gambling game, the random variables correspond to the nodes and their respective conditional probability tables of the Bayesian Network in Fig. 13. That is, the variable U corresponds to a player willing or not to participate in the game. Variable G1 corresponds to the variable winning or losing the first gamble. Variable G2 corresponds to a player playing or not the second gamble.In classical probability theory, the full joint distribution over a set of n random variables χ={X1, X2, …, Xn} defined over the same sample space, Pr(X1, X2, …, Xn), is the distribution that assigns probabilities to events that are specified in terms of these random variable [49].Then, the full joint distribution of a Bayesian Network, where X is the list of variables, is given by [63]:(40)Pr(X1,…,Xn)=∏i=1nPr(Xi|Parents(Xi))Using Eq. (40), the full joint distribution of Fig. 13 corresponds to the calculations discriminated in Table 2.Given a query random variable X and let Y be the unobserved variables in the network, the marginal distribution of X is simply the probability distribution of X averaging over the information about Y. The marginal probability for discrete random variables can be defined by Eq. (41). The summation is over all possible y, i.e., all possible combinations of values of the unobserved variables y. The term α corresponds to a normalization factor for the distribution Pr(X) [63].(41)Pr(X=x)=α∑yPr(X=x,Y=y)=α∑yPr(X=x|Y=y)Pr(Y=y),whereα=1∑x∈XPr(X=x)After computing the full joint distribution, we need to sum out all the variables that are unknown, in this case, the variable corresponding to the outcome of the first gamble: G1. This is achieved by applying the marginal probability formula in Eq. (41).Pr(G2=Play|U=Play)=α∑g∈G1Pr(U=Play,G1=g,G2=Play)Pr(G2=Play|U=Play)=αPr(U=Play)∑g∈G1Pr(G1=g|U=Play)Pr(G2=Play|G1=g)Pr(G2=Play|U=Play)=αPr(U=Play)[Pr(G1=win|U=Play)Pr(G2=Play|G1=win)+Pr(G1=lose|U=Play)Pr(G2=Play|G1=lose)]Pr(G2=Play|U=Play)=α0.295Pr(G2=Not_Play|U=Play)=α0.205The parameter α corresponds to a normalization factor and, for this example, is given by:α=1Pr(G2=Play|U=Play)+Pr(G2=Not_Play|U=Play)=10.295+0.205=10.5So, the final normalized classical probabilities for the two-stage gambling game correspond to:Pr(G2=play|U=play)=0.59Pr(G2=Not_Play|U=Play)=0.41The probabilities computed are not in accordance with the probabilistic findings reported by Tversky and Shafir [71], because it was empirically observed that Pr(G2=play|U=play)=0.42. Therefore, a classical Bayesian Network can never be used to model such experiments, because of the relation already presented in Eq. (3).Following the work of Leifer and Poulin [53], a Quantum Bayesian Network can be defined by a pair (G,ρv), where G = (V,E) is a directed acyclic graph, and each vertexv∈Vis associated with a quantum system with a Hilbert spaceHvandρvis a quantum state onHV=Hv1⊗Hv2⊗…⊗Hvn. The stateρvsatisfies the same conditional independence constraints as in a classical Bayesian Network. Note that the definition of a classical Bayesian Network can be directly obtained by replacing the word quantum system by random variable.The symbol ⊗ is defined by tensor product and corresponds to a mathematical method that enables the construction of a Hilbert space from the combination of individual Hilbert spaces. Suppose that we have 2 different two-dimensional Hilbert spaces Hxand Hy, where Hx={x1, x2} and Hy={y1, y2}. Then, their tensor product would be:x1x2⊗y1y2=x1y1x1y2x2y1x2y2In quantum theory, random variables are associated to a set of N quantum systemsV={v1,v2,…,vN}, each associated with a Hilbert space with a specific dimension [53]. Consequently, all values contained in the conditional probability tables associated to the random variables are complex numbers.Each node on the Bayesian Network in Fig. 14can be seen as a subsystem belonging to a specific Hilbert space. For instance, the first node U can be represented in a Hilbert subspace Huin the following way:U=eiθU12Play+eiθU12Not_PlayWhere Play and Not_Play are column vectors corresponding to the basis of the subspace Hu:Play=10Not_Play=01Since our goal is to compare the Quantum Bayesian Network with its classical counterpart, we will convert the conditional probability tables in Fig. 13 into conditional amplitude tables. That is, we simply convert classical probabilities into complex amplitudes through the relation in Eq. (13).(42)Pr(A)=|eiθAψA|2→ψA=eiθAPr(A)The representation of a general state in a Bayesian Network can be described by a bipartite state. Suppose that H=HX⊗HY⊗HZis a Hilbert space defined by the composition of three Hilbert spaces HX, HYand HZ. Then, a quantum state SXYZis designated bipartite if it can be specified with respect to the random variables X, Y and Z. For Xi, Yjand Zkas basis in HX, HYand HZ, respectively, the bipartite state is given by Eq. (43).(43)SXYZ=∑i,j,kψXiψYjψZkXi⊗Yj⊗ZkIn Eq. (43), ψXiψYjψZkcorresponds to the amplitudes of states Xi, Yjand Zk, respectively. The states Xi, Yjand Zkcorrespond to column vectors representing basis vectors:X0=Y0=Z0=10,X1=Y1=Z1=01The general quantum state represented by the Bayesian Network in Fig. 14 is given by quantum bipartite states. Reformulating Eq. (43) for the problem of two step gambles, we obtain:(44)SU,G1,G2=∑ijkψUiψG1jψG2kUi⊗G1j⊗G2kFor simplicity, we will write ψUiψG1jψG2kas ψijk. So, expanding Eq. (44),(45)SU,G1,G2=ψ000U0G10G20ei(θU0+θG10+θG20)+ψ001U0G10G21ei(θU0+θG10+θG21)+ψ010U0G11G20ei(θU0+θG11+θG20)+ψ011U0G11G21ei(θU0+θG11+θG21)+ψ100U1G10G20ei(θU1+θG10+θG20)+ψ101U1G10G21ei(θU1+θG10+θG21)+ψ110U1G11G20ei(θU1+θG11+θG20)+ψ111U1G11G21ei(θU1+θG11+θG21)Note that UiG1jG2kare basis column vectors representing the axis of the Hilbert space,U0G10G20=10000000,U0G10G21=01000000,U0G11G20=00100000,…,U1G11G21=00000001.Following the Quantum Bayesian Network in Fig. 14, we compute the values of ψUiψG1jψG2kby multiplying the correspondent values in the conditional probability tables. For example, ψ000 corresponds to the product of the variables U=Play with G1=Win|U=Play with G2=Play|G1=Win. On the other hand, the entry ψ110 corresponds to the product of the variables U=Not_Play with G1=Lose|U=Not_Play with G2=Play|G1=Lose. Replacing Eq. (45) by the conditional probability values in Fig. 14, we obtain:SU,G1,G2=ei(θU0+θG10+θG20)1212175U0G10G20+ei(θU0+θG10+θG21)121285U0G10G21+ei(θU0+θG11+θG20)121212U0G11G20+ei(θU0+θG11+θG21)121212U0G11G21+ei(θU1+θG10+θG20)1212175U1G10G20+ei(θU1+θG10+θG21)121285U1G10G21+ei(θU1+θG11+θG20)121212U1G11G20+ei(θU1+θG11+θG21)121212U1G11G21SU,G1,G2=0.4123U0G10G20+0.2828U0G10G21+0.3536U0G11G20+0.3536U0G11G21+0.4123U1G10G20+0.282885U1G10G21+0.3536U1G11G20+0.3536U1G11G21Note that the sum of the squares of all probability amplitudes ψUiψG1jψG2ksum to 1,∑ijk|ψUiψG1jψG2k|2=1This bipartite state represents a quantum superposition over all possible states. We can think of this as various wave functions that are occurring at the same time.In quantum probability theory, a full joint distribution is given by a density matrix. This matrix provides the probability distribution of all states that a Bayesian Network can have. In our Quantum Bayesian Network model, the density matrix ρ corresponds to the multiplication of the bipartite state described in Eq. (43) with itself (the symbol † corresponds to the conjugate transpose):(46)ρ=SXYZSXYZ†The reason why one multiplies the same state with itself is to obtain the probability value out of the amplitude. Note that the bipartite state contains amplitudes instead of probability values. Knowing that the probability value is obtained by taking the squared magnitude of the amplitude, then, by multiplying a state with its conjugate transpose, one can obtain the full joint probability distribution.From the bipartite state, one can compute the density matrix by applying Eq. (46) to this calculation. The density matrix will be useful for later calculations and enables the calculation of the probability distribution of the bipartite state.(47)ρUG1G2=SUG1G2·SUG1G2†In this two step gambling game, Eq. (47) produces an 8×8 density matrix:(48)ρUG1G2=|ψ000|200…00|ψ001|20…000|ψ010|2…0⋮⋮⋮⋱⋮000…|ψ111|2=0.1700000000000.0800000000000.1250000000000.1250000000000.1700000000000.0800000000000.1250000000000.1250The concept of quantum marginalization is analogous to the one in classical probability theory. Given two quantum random variables X and Y, the general idea is to compute the average of the probability distribution of X over the information about Y. This is performed by using the partial trace operator, which basically consists in accessing certain positions of the density matrix ρ.(49)Xij=α∑y∈Yρ[iy,jy]In Eq. (49), the parameter α corresponds to the normalization factor, which is also present in the classical Bayesian Network inference.Considering the two-stage gambling game that we are analyzing, imagining that we want to determine the probability of a participant playing the second gamble, Pr(G200), given that we verified that the player participated in the game in the first place (u=0). That is, we will be summing out variable G1. Through Eq. (49), one would proceed in the following way:G2ij=G200=α∑g∈G1ρ[ugi,ugj]=α(ρ[000,000]+ρ[010,010])=α0.2950→0.59And in the same way for G211:G211=α∑g∈G1,u∈Uρ[ugi,ugj]=α(ρ[001,001]+ρ[011,011])=α0.2050→0.41Note that the indexes of the density matrix are encoded. The assignment 000 corresponds to index 0, the assignments 001 to index 1, …, and the assignment 111 corresponds to the index 8.The normalized results correspond to the same probabilities obtained in classical theory. Therefore, we need to incorporate the interference terms found in cognitive psychology into the quantum marginalization formula in order to obtain different results.A quantum interference effect will occur when performing the marginalization of a random variable. If we do not observe a random variable, then it can be in represented by superposition and perform interferences in other random variables, changing the final outcome.The quantum interference marginalization formula proposed in this work consists in merging the equation that presents the interference term (Eq. (21)) with the formula of quantum marginalization (Eq. (49)). This leads to Eq. (50):Xij=α∑y∈Yρ[iy,jy]2(50)Xij=α∑iN|ψi|2+2∑iN−1∑j=i+1N|ψi||ψj|cos(θi−θj),where|ψi|2=∑y∈Yρ[iy,jy]Following the experiment performed by Tversky and Shafir [71], we want to determine the probability of a participant playing the second gamble, given that (s)he does not know the outcome of the first gamble.Through Eq. (50), this can be computed in the following way:G2ij=α∑u∈U,g∈G1ρ[iug,jug]·2Given that we want to know the probability of a participant playing the second gamble, then we know for certain that (s)he accepted to play. Thus, we will fix the variable U, representing that a participant accepted to play the game in the first place, U=0:(51)G2ij=α∑g∈G1ρ[i0g,j0g]2Expanding Eq. (51),G200=α|ρ[000,000]+ρ[010,010]|2(52)G200=α|0.17+0.125|2=α(0.17+0.125+20.170.125cos(θ1−θ2))Computing the probability of a participant deciding to not play the second gamble, given that he does not know the outcome of the first play, we obtain:G211=α|ρ[001,001]+ρ[011,011]|2(53)G211=α|0.08+0.125|2=α(0.08+0.125+20.080.125cos(θ1−θ2))The normalization factor α is computed by summing the results when G2=Play and G2=Not_Play, that is, summing Eq. (52) with Eq. (53):α=10.5+20.170.125cos(θ1−θ2)+20.080.125cos(θ1−θ2)=10.5+0.4915cos(θ1−θ2)Then, the normalized results are given by:(54)G200=0.2950+20.170.125cos(θ1−θ2)0.5+0.4915cos(θ1−θ2)(55)G211=0.2050+20.080.125cos(θ1−θ2)0.5+0.4915cos(θ1−θ2)The aim of this quantum interference Bayesian Network is to simulate the averaged results reported in Table 1. More specifically, we are interested in simulating the value corresponding to the probability of the participant playing the second gamble, given that (s)he does not know the outcome of the first one. In Table 1, this value corresponds to 0.42 and cannot be obtained through classical probability theory, since the law of total probability is violated. In our model, we can tune the angle θ of the interference term in order to obtain such results. Calculations showed that in order to achieve a probability of 42%, cos(θ) must be equal to −0.998853, which corresponds to an angle of 177.3° or 3.09rad.(56)G200=0.2950+20.170.125cos(3.09)0.5+0.4915cos(3.09)=0.42(57)G211=0.2050+20.080.125cos(3.09)0.5+0.4915cos(3.09)=0.58This section described the application of the proposed quantum interference Bayesian Network to explain the puzzling findings in the two-stage gambling game, that could not be explained through classical probability theory. Eqs. (56) and (57) showed that the proposed approach is able to simulate the human decisions observed in the works of Tversky and Shafir [71], Shafir and Tversky [65], Kuhberger et al. [50] and Lambdin and Burdsal [51].The interference term that we show in this paper for the two-stage gambling game, was proposed by cognitive psychologists in order to explain their observations. That is, they manually tuned this parameter θ to fit their data. In this work, we look at this parameter from a different perspective: what happens to the quantum computed probabilities if we vary this angle θ? Can better inferences be achieved? In this section, we make use of the probabilistic interference term proposed in the cognitive psychology literature [61,24], and investigate the impact that the phase parameter θ can have, when computing quantum probabilities in the two step gambling game.In the previous section, the general probability formula of a player willing to play the second gamble, given that the outcome of the first gamble was unknown, was given by Eq. (54). In order to analyze the consequences of the angle θ in the final probability, we varied θ from 0 to 2π in steps of 0.0001rad. The results obtained are discriminated in Fig. 15.Fig. 15 reveals that quantum probabilities can achieve much higher values than the classical probability theory. The quantum probability can reach a maximum of 0.5915 where the classical probability can only have a fixed value of 0.59.Fig. 15 is also supporting the quantum information processing theory already mentioned in Section 1 of this work: information is modeled via wave functions and therefore, they cannot be in a definite state (only when a final decision is made, a definite state emerges). One can look at all values that this parameter θ as all possible probabilities (or outcomes) that a player has when deciding to whether or not to play the second gamble. Since in quantum theory we model the participant's beliefs by wave functions, then the superposed states can produce different waves coming from opposite directions that can crash into each other. When they crash, the waves can either unite or be destroyed. When they unite, it causes a constructive interference effect that will cause a bigger wave, leading to a maximum or minimum quantum probability value, depending on the phase of the wave (Fig. 16). When the waves crash and are destroyed, then a destructive interference effect occurs (Fig. 17).In conclusion, quantum probability enables the free choice of parameters in order to obtain a desired probability value. The two-stage gambling game is just a small example where the proposed model could be applied. In the next section, we turn to the task of burglary detection. We will analyze a more complex Bayesian Network that will determine the probability of a burglary occurring, given that the neighbors think that they heard an alarm.In this section, we compare classical inferences in Bayesian Networks with the proposed quantum model. The example that we will examine correspond to a modified version of the Burglar/Alarm Network, which is very used in the scope of Artificial Intelligence [59,63].The network proposed in the book of Russel and Norvig [63] is inspired in the following scenario. Imagine that a person installed a new burglar alarm in his house. The alarm has a big credibility in what concerns detecting burglaries. The person has two neighbors: Mary and John. They have promised to call the person, whenever they think they heard the alarm. John always calls the police, when he hears the alarm, however, he sometimes confuses the sound of the alarm with the ringtone of his phone, resulting in a misleading phone call to the police. Mary, on the other hand, cannot hear the alarm very often, because she likes to hear very loud music. Given the evidence of who has or has not called the police, we want to represent in a Bayesian Network the probability of a burglary occurring [63].Fig. 18represents a classical Bayesian Network to account for burglary detection. It's quantum counterpart corresponds to Fig. 19. In order to make a fair comparison between both networks, the quantum Bayesian Network was built in the same way as the classical one, but we replaced the real probability values by quantum complex amplitudes, just like proposed in Tucci [68] and Leifer and Poulin [53].We also performed a set of queries so we could compare the classical Bayesian Network with the proposed quantum interference Bayesian Network. Table 3presents the final probabilities computed over the classical Bayesian Network of Fig. 18 and Table 4presents the results for the Quantum Bayesian Network (Fig. 19).In the experiment of the two-stage gambling game, we saw that the parameter θ plays an important role in determining the final probability value of a variable. For the Burglar/Alarm Bayesian Network, in order to find the best parameter for each query, we varied θ between 0 and 2π, in steps of 0.1rad, and collected the θ that would maximize most variables in the network. Eq. (58) represents the interference formula that we used to maximize the probability with respect to parameter θ.(58)Pr(A)=argmaxθα∑i=1N|ψi|2+2∑i=1N−1∑j=i+1N|ψi||ψj|cos(θi−θj)In the next section, we will analyze the results specified in Tables 3 and 4 for each single query.Tables 3 and 4 present the results obtained for different queries performed over the classical Bayesian Network (Fig. 18) and the proposed quantum interference Bayesian Network (Fig. 19), respectively.Analyzing the first row of both tables, when we provide no piece of evidence to the network, the proposed model was able to increase, on average, the probabilities of the query variables about 270.625%. In the case where nothing is observed, the proposed network achieves its maximum level of uncertainty: all variables are interfering with each other causing both destructive and constructive interference effects on the final probabilities. It is interesting to notice that when the classical probabilities computed for each query variable are very low, then the Quantum Bayesian Network cannot greatly increase these probabilities. If the probabilities in a classical setting are very low, then the quantum Network is able to keep those probabilities low as well.Figs. 20–23illustrate all possible values that each query variable could have, by varying the θ parameters. These graphs were plotted in the following way: for each query, we found the set of all θ's that would lead to a maximum and a minimum probability value. We then compared the set of θ's and realized that in the majority of the cases there were components that shared the same parameters. Given this situation, we fixed 6 of the parameters which were common and varied the remaining parameters, leading to a three-dimensional graph.The moment we start to provide pieces of evidence to the network, the uncertainty starts to decrease. Analyzing the situation where we observe that the Alarm variable is true, then an interesting phenomena occurs: the probabilities of the proposed quantum Bayesian Network collapse to the same probability values as in its classical counterpart. If one observes the variable Alarm, then the variables Burglar, MaryCalls and JohnCalls become independent of each other. This means that there is no possible way that these variables can interfere with each other. Since the variables do not provoke any interferences among them, then the interference term will be zero and will collapse to the classical probability. This independence phenomena in Quantum Bayesian Networks has also been noticed in the work of Leifer and Poulin [53]. For a mathematical proof of why this phenomena also occurs in a quantum setting, please refer to their work. This means that, whenever we observe the variable Alarm, the interference term will always be null and consequently all probabilities will be exactly the same as in a classical Bayesian Network inference.In the case where we observe the Burglar variable, then, according to the scenario of the Bayesian Network, the Alarm variable should also increase. The variable JohnCalls is highly correlated with the variable Alarm. In its conditional probability table, there is a chance of 90% of the variable JohnCalls occurring when the variable Alarm is true. So in this situation, the quantum Bayesian Network is able to give more strength to that correlation. On average, the probabilities of all queries increased 22.8% when the variable Burglar is observed, when compared to the respective classical setting.When we observe that the variable JohnCalls is true, then it is expected that the probability of the Alarm variable increases as well, according to the scenario of the Bayesian Network. When we observe the variable MaryCalls the variable Alarm increases even more when compared to the situation where JohnCalls=t or its classical counterpart. This means that the correlation between the Alarm variable and the variable JohnCalls is not as strong as when we observe MaryCalls. According to the scenario, John always calls the police, leading to many misleading calls. Consequently, the Alarm variable cannot be increased too much. However, when MaryCalls is observed, then, according to the Bayesian scenario, it is almost certain that she heard the alarm and therefore, a strict correlation exists between these two variables. The quantum Bayesian Network was able to represent the correlations between variables in a more realistic and reliable way than its classical counterpart.Finally, when we start to provide two pieces of evidence to the network, then the uncertainty levels start to decrease. Consequently, the probabilities computed in a quantum setting start to converge to the ones computed in a classical Bayesian Network. This means that, there are two situations there the proposed Quantum Bayesian Network converges to its classical counterpart: (1) when the variables of the network become independent of each other and (2) when there are very low levels of uncertainty, because too many evidences were provided to the network.In this section, we perform a study on the impact of the θ parameters in the context of medical decision making. Consider the classical Bayesian Network of Fig. 24, which corresponds to a slightly modified version of the Bayesian Network proposed in the book of Pearl [59]. Fig. 25corresponds to its quantum counterpart.Again, in order to determine the maximum probability value, we varied the θ parameters between 0 and 2π in steps of 0.1rad. The method that we used to compute these parameters in described in Appendix A. We then performed the following queries: Pr(Smoke = true), Pr(Dyspnea = true), Pr(Cough = high) and Pr(Lung Cancer = positive). The results obtained are discriminated in Fig. 26.Analyzing Fig. 26, one can observe that again, the quantum probability values tend to overcome their classical counterparts. A special note goes to the quantum probabilities verified for the query Pr(Cancer = positive). The increase verified in this probability was much higher than any other value achieved by the other quantum probabilities. Since the Lung_Cancer random variable is located at the centered position of the Bayesian Network, then, when nothing is observed, its probability is influenced by the probabilities of all nodes in the network.In order to determine the impact of the θ parameters, Table 5shows the parameters that were used in the Burglar/Alarm and Lung Cancer Bayesian Networks.Figs. 27–30also show all the possible probability values that the variables from the Lung Cancer Bayesian Network can achieve, when nothing is known.When we are at a maximum level of uncertainty, classical probabilities tend to assume that events are equiprobable, that is, the probability of their outcome is always the same no matter their context. Quantum theory, on the other hand, provides a more relaxed framework. When nothing is known, then there are more degrees of freedom that enable the outcome of any event to be any possible value. This way, by analyzing the context of certain events (for example, a medical decision scenario, a gambling game, etc.), the quantum parameters θ can be modeled in such a way that they can simulate reality more accurately and more precisely than its classical counterpart.In this sense, in order to develop more accurate quantum Bayesian Networks, a study of the context of the problem is required in order to start the search for the optimum θ parameters. Techniques to search for these optimum quantum parameters is still an open research question and an unexplored field in the literature of quantum cognition and quantum decision models.

@&#CONCLUSIONS@&#
This work was motivated by the preliminary experiments of Tversky and Shafir [71] about violations of the classical probability theory on the sure thing principle. This principle states that if one chooses action A over B in a state of the world X, and if one also chooses action A over B under the complementary state of the world X, then one should always choose action A over B, even when the state of the world in unspecified. When humans need to make decisions under risk, several heuristics are used, since humans cannot process large amounts of data. These decisions coupled with heuristics lead to violations on the law of total probability.Recent work in cognitive psychology revealed that quantum probability theory provides another method of computing probabilities without falling into the restrictions that classical probability have in modeling cognitive systems of decision making. Quantum probability theory can also be seen as a generalization of classical probability theory, since it also includes the classical probabilities as a special case (when the interference term is zero).The main difference between quantum and classical probability lies in the fact that on quantum probability we are constantly updating some beliefs when making a decision, while in classical probability all beliefs are assumed to have a definite value before a decision is made, and this value is the outcome of the decision [3].The main research question for this work was how could these quantum probabilities affect probabilistic graphical models, such as Bayesian Networks, since many of nowadays decision making systems are based on such structures (medical diagnosis, spam filtering, image segmentation, etc.).In this work, we proposed a novel Bayesian Network for the Computer Science community based on quantum probabilities. Our method can accommodate puzzling observations that the classical probability failed to explain (for instance, the two-step gambling game). When the nodes of the proposed Bayesian Network are represented as a superposition state, then one can look at this state as many waves moving across in different directions. These waves can crash into each other causing waves to be bigger or to cancel each other. This is the interference phenomena that the proposed Bayesian Network offers and that has direct implications when making inferences. Therefore, the proposed network represents and simulates quantum aspects motivated by Feynman's path integrals.Experimental results revealed that the proposed quantum Bayesian Network enables many degrees of freedom in choosing the final outcome of the probabilities. If we had a real scenario, with real observations, one could use the present model to fit it to the observed data, by simply tuning the parameter θ. This parameter can open a door into machine learning approaches. Learning algorithms using the proposed method might produce better prediction models, since the quantum probability amplitudes are able to fully represent real word data. For future work, we intend to explore machine learning algorithms under quantum probabilistic graphical model formalisms.The overall results also suggested that when the classical probability of some variable is already high, then the quantum probability tends to increase it even more. When the classical probability is very low, then the proposed model tends to lower it.When there are many unobserved nodes in the network then the levels of uncertainty are very high. But, in the opposite scenario, when there are very few unobserved nodes, then the proposed quantum model tends to collapse into its classical counterpart, since the uncertainty levels are very low.The proposed Bayesian Network can integrate human thoughts by representing a person's beliefs in an N-dimensional unit length quantum state vector. In the same way, the proposed quantum structure is general enough to represent any other context in which there is a need to formalize uncertainty, including prediction problems in data fusion. In the context of Bayesian Networks, data fusion is introduced in the work of Pearl [58]. The author argues that, just like people, Bayesian Networks are structures that integrate data from multiple sources of evidence and enable the generation of a coherent interpretation of that data through a reasoning process. The fusion of all these multiple data sources can be done using Bayes theorem. When a data source is unknown, then the Bayes rule is extended in order to sum out all possible values of the probability distribution representing the unknown data source. The proposed Quantum Bayesian Network takes advantage of these uncertainties by representing them in a superposition state, which enables the fusion of the data sources through quantum interference effects. These effects produce changes in the final likelihoods of the outcomes and provide a promising way to perform predictions more accurately according to reality. So, the Quantum Bayesian Network that is proposed in this work is potentially relevant and applicable in any behavioral situation in which uncertainty is involved.The most naive approach of parameter search is the grid search method. In this approach, it is placed a grid over the parameter space and the data is evaluated at every grid intersection, returning the parameters, which lead to the maximum performance of an algorithm [55]. However, grid search has the problem of being unbounded, since an infinite set of parameters are available to be tested. In the scope of this work, this is not a problem, because we always have a boundary. The values of a cosine function can all be specified for angles between the range [0, 2π]. The grid search approach is a very naive method for finding parameters in the parameter space. In fact, there are several advanced parameter search algorithms in the literature, which do not have a heavy computational cost. The motivation behind the usage of the grid search approach in this work is that the computational time required by any of the other advanced methods is almost the same as using grid search. In addition, many of the advanced search parameter methods in the literature perform approximations, which can be avoided by the direct parameter search of the grid search approach [55].Algorithm 1Grid search to find θ parameters