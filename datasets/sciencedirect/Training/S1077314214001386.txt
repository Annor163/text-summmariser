@&#MAIN-TITLE@&#
Refinement of human silhouette segmentation in omni-directional indoor videos

@&#HIGHLIGHTS@&#
A methodology for refining the segmentation of human silhouette is proposed.Video is acquired indoors from a ceiling-based omni-directional camera.A calibrated model of the camera combined with geometry based reasoning are used.Significant improvement in reducing false positive human activity segmentation is achieved.The algorithm works in real time with input from any fisheye camera.

@&#KEYPHRASES@&#
Video segmentation,Human activity detection,Mathematical model of fisheye camera,Geometric reasoning,

@&#ABSTRACT@&#
In this paper, we present a methodology for refining the segmentation of human silhouettes in indoor videos acquired by fisheye cameras. This methodology is based on a fisheye camera model that employs a spherical optical element and central projection. The parameters of the camera model are determined only once (during calibration), using the correspondence of a number of user-defined landmarks, both in real world coordinates and on a captured video frame. Subsequently, each pixel of the video frame is inversely mapped to the direction of view in the real world and the relevant data are stored in look-up tables for fast utilization in real-time video processing. The proposed fisheye camera model enables the inference of possible real world positions and conditionally the height and width of a segmented cluster of pixels in the video frame. In this work we utilize the proposed calibrated camera model to achieve a simple geometric reasoning that corrects gaps and mistakes of the human figure segmentation, detects segmented human silhouettes inside and outside the room and rejects segmentation that corresponds to non-human activity. Unique labels are assigned to each refined silhouette, according to their estimated real world position and appearance and the trajectory of each silhouette in real world coordinates is estimated. Experimental results are presented for a number of video sequences, in which the number of false positive pixels (regarding human silhouette segmentation) is substantially reduced as a result of the application of the proposed geometry-based segmentation refinement.

@&#INTRODUCTION@&#
The field of human activity monitoring based on cameras has gained significant interest during the last years in the context of developing ambient assisted living environments. A number of approaches exist in the research literature, based either on 3D human models, or on local image descriptors, exploiting both spatial and temporal information. Detailed reviews of this field exist in [1–4]. The majority of the approaches to the problem of vision-based recognition of human motion and action, utilize descriptors from segmented human silhouettes. For instance in [3] 14 publications are listed that use silhouettes as abstraction level for vision based human motion capture. In the survey presented in [1], it is stated that segmented silhouettes are used for human motion detection using 3D human models of image descriptors, but segmentation artifacts limit the performance of the corresponding methods. In [2] the use of silhouettes for human action recognition using image descriptors, as well as space volumes, is surveyed through a number of listed works. These algorithms perform well with datasets that include very short videos of single humans performing a single task at each time. Examples of these datasets include the INRIA XMAS [5], the Weizmann [6], the KTH [7], the CMU MoBo database [8] and the Human EVA [9]. In these video segments, no other action is usually visible in the background, thus the segmentation of the human silhouettes is rather easy, defect free and unambiguous. Only few databases exist that contain challenging video sequences (changing illumination in background, existence of multiple persons close to each other or interacting), such as the HOHA database [10].It is obvious from the above discussion that the improvement of the human silhouettes segmentation will result in improving the quality of human motion/action recognition. The contribution of this work is a methodology that isolates the segmented human silhouettes from fisheye video sequences inside a designated area, from other irrelevant segmentation and eliminates artifacts and outliers. The video data used in this research are acquired indoors from a fixed fisheye camera, installed at the ceiling of the living environment. The proposed algorithm uses a novel fisheye camera model that enables reasoning based on real world geometry to correct and enhance the segmentation output of the human figures. The proposed algorithm does not make use of models of the required object (human silhouette), or local image features.The input of the proposed algorithm is the result of the initial video segmentation based on background modeling and subtraction. Several methodologies for background modeling exist in literature. For instance in [11] the background model is simply defined as the previous frame and global thresholding is employed to extract the foreground. This method is very simple to implement, but it is prone to a number of segmentation errors. Background can be modeled by median filtering [12] of a predefined number of last frames that are held in a buffer. This approach requires significant computational and memory resources and cannot be executed in real time. In order to alleviate the increased computational requirements, a class of recursive background modeling algorithms is proposed in literature. These algorithms use an incremental update of the background. Simple and efficient members of this class of algorithms are the approximation of the median filtering method [13] and the running Gaussian average method [14]. A comprehensive review of background modeling algorithms for foreground detection is presented in [15]. A popular methodology is the Mixture of Gaussians, initially described for video sequences by Stauffer and Grimson [16], according to which, the values of each pixel are modeled as a lineal combination of weighted Gaussian probability distributions. However, this method is also computationally expensive. In this work we have performed video segmentation using the illumination-sensitive background modeling approximation, as originally described in [17] and modified in [18], although any other video segmentation algorithm can be employed. This algorithm was selected due to its simplicity and its efficient handling of illumination changes.The video sequences used in this work are captured by a hemispheric camera, also known as omni-directional, or fisheye camera with 180° field of view (FoV). The use of this type of cameras is increasing in robotic and in video surveillance applications [19,20], due to the fact that they allow constant monitoring of all directions with a single camera. In [21–23] the calibration of fisheye camera is reported using high degree polynomials to emulate the strong deformation introduced by the fisheye lens, radial and/or tangential. In [24] the authors present a methodology for correcting the distortions induced by the fisheye lens. In [25,26] a well established calibration for omni-directional cameras is proposed, which utilizes a standard chess pattern imaged at arbitrary orientations, without requiring point input by the user. In this paper we present a very efficient camera model that extends our previously proposed fisheye model that used only 3 parameters [27] with exhaustive search calibration. Subsequently, we utilize the proposed inverse fisheye model to refine the segmentation of moving humans and eliminate non-human activity, such as window reflections, sudden changes of illumination, small objects, moving doors, etc., as well as human silhouette outside the designated room. The position of a human silhouette (or any other segmented foreground object) is estimated accurately under the assumption that its base (area of the surface touching the floor) is small, such as a standing, walking or sitting the human. Finally, the refined segmented silhouettes are uniquely labeled using spatiotemporal information concerning both real world position and RGB appearance and the trajectory of each silhouette in real world coordinates is also estimated.The rest of the paper is organized as follows. In Section 2, the overall architecture is presented, while the forward and inverse modeling of the fisheye camera is also described. The proposed methodology for the refinement of the human silhouette segmentation using reasoning based on the geometric relation between the binary connected components is presented in Section 2.4. Initial experimental results are presented in Section 3, whereas the proposed algorithm and the future work are discussed in the last concluding Section 4.The main characteristic of the fisheye camera is the ability to cover a field of view of 180°. The proposed methodology is based on a parametric model of image formation, so that any real-world point (x,y,z) can be associated with a frame pixel (i,j). Furthermore a pixel (i,j) in the video frame can be associated with the direction of view, defined by two angles: the azimuth θ and the elevation φ. The parameters of the fisheye camera model are determined only once (calibration), using manually inserted reference points. The resulting association between pixels and azimuth θ and the elevation φ is stored in look-up tables for quick utilization, as shown in the right side of the block diagram of Fig. 1. The part of the proposed algorithm that is executed in real time includes the segmentation of moving objects (black and white thumbnail image at the left of Fig. 1), its refinement based on the reasoning presented in Section 2.4 and the unique labeling of the silhouettes (in case of multiple silhouette segmentation) as described in Section 2.5. The refined human silhouette segmentation is encoded in color (red for rejected segmentation, other colors for human silhouette segmentation – see thumbnail image at the left part of Fig. 1). Fig. 1 illustrates the modules of the proposed methodology.The fisheye model M can be written in the general form(1)(j,i)=M(x,y,z)where (j,i) are pixel coordinates in the video frame and (x,y,z) are real world coordinates of the imaged point. In the following subsection, we will describe the inverse fish-eye camera model, i.e. given a pixel (j,i) in the video frame, the direction of view is obtained, defined in spherical coordinates by two angles: the azimuth θ and the elevation φ (Fig. 2):(2)(θ,φ)=M1(i,j).The definition of a model for the fisheye camera is based on the physics of image formation, as described in [28,29] and demonstrated in [30]. The model consists of:•A spherical element of arbitrary radius R0 with its center at K(0,0,zsph).The plane of the CMOS sensor defined as passing through (0,0,zplane) and by its unit length normal vector n.For any point P with real world coordinates (x,y,z), we determine the intersection Q of the line KP with the spherical optical element of the fisheye lens. The point P is imaged at the central projection (xim,yim) of Q on the image plane, using the O (0,0,0) as center of projection, assuming that the installation of the camera is such that the imaging plane (i.e. the image sensor) is horizontal and the axis of the spherical lens is not misaligned. Thus, it becomes obvious that all real world points that lie on the KP line are imaged at the same point (xim,yim) in the image plane. The KP line is uniquely defined by its azimuth and elevation angles, θ and φ, respectively. The concept of the fisheye geometric model is shown in Fig. 2.The fisheye camera has no moving parts. Therefore, the relation between zsphand zplanedefines the formation of the image. Let us set zplaneto an arbitrary value, less than R0 and define zsph=pzplane, where p is the primary parameter of the fisheye model. To account for possible lens misalignments with respect to the camera sensor that could introduce imaging deformations on the imaged frame [23], we introduce two extra model parameters: the X and Y position of the center of spherical lens K (xsph, ysph, zsph) with respect to the optical axis of the camera. Now the camera model parameters consist of n, p,xsph, and ysph. Fig. 2 shows the geometry of the fisheye camera model for xsph=0 and ysph=0.The position of any point of the line segment KP, thus Q as well, is given by(3)(Qx,Qy,Qz)=(λ(x-xsph),λ(y-ysph),λ(z-zsph)),where λ is a parameter in range [0,1]. If we insert (3) into the equation of the spherical optical element, we obtain:(4)(λ(x-xsph)-xsph)2+(λ(y-ysph)-ysph)2+(λ(z-zsph)-zsph)2-R02=0.The parameter λ that defines the position of Q may be obtained by solving Eq. (4) and accepting the solution in the allowed range [0,1]. In order to generalize the fish-eye model, we assume that the plane of the sensor is not the XY plane, but it is defined as passing through point (0,0,zplane) and being normal to vector n=(n1,n2,n3). The components of n are included as new parameters to the model of the fish-eye camera that was originally described in [27].Then, we calculate the central projection (xim,yim,zim) of Q on the image plane, given by:(5)(mQx,mQy,mQz),where the parameter m is given by:m=zplanen3Qxn1+Qyn2+Qzn3.Finally, the coordinates of the projection point (xim,yim,zim) can be obtained by geometrically transforming the central projection point, in homogeneous coordinates, so that the vector n normal to the sensor, becomes parallel to the Z axis:(6)(xim,yim,zim,1)T=A(mQx,mQy,mQz,1)T,where A is the matrix that transforms vector (n1,n2,n3) onto the Z axis:A=λ|n|-n1n2λ|n|-n1n2λ|n|00λn3-n2n30n1|n|n2|n|n3|n|00001,λ=n22+n32.when x→±∞ then Qx→R0±xsph(the same holds for the y coordinate as well). Thus, any point P with real world coordinate z>zsph, will be imaged on the image plane at position (xim,yim), which is bounded as following:(7)xim_min⩽xim⩽xim_maxandyim_min⩽yim⩽yim_maxwherexim_max=λ+(xsph+R0)xim_min=λ-(xsph-R0),λ±=n3zpl(xsph±R0)n1+(ysph±R0)n2+zsphn3The image pixel position (i,j) that corresponds to the projection on the image plane (xim,yim) is calculated by a simple linear transform:(8)j=ximRFoVf1+CoDx,f1=xim_max-xim_min2i=yimRFoVf2+CoDy,f2=yim_max-yim_min2,where (CoDx, CoDy) is the center of distortion pixel that corresponds to elevation φ=π/2 and RFoVis the radius of the circular field of view (FoV), as it is explained in the next paragraph.In the case of fisheye lens, [31] suggests that the CoD is located as the center of the circular field-of-view. In our case this is a very practical approach, since almost the whole field-of-view is imaged (full frame imaging). We therefore apply the Canny edge detector [32], using a standard deviation equal to 2 to detect the stronger edges in the image, which are the edges of the circular field of view. Then, a simple least squares optimization to obtain the CoD and the radius of the FoV is employed. The resulting CoD and radius of FoV are shown in Fig. 3. This process is applied only once after the installation of the camera.In order to the proposed fisheye camera model in our algorithm, we need to determine the values of the unknown parameters (p,xsph,ysph,n). This is done during calibration. Initially, we provide the position of Nplandmark points{(Ximk,Yimk)},k=1,2,…,Npon one video frame. Although a minimum required number of points is not defined, in this work we used Np=18 prominent landmarks that are distributed over the field of view of the camera. The real world coordinates of these landmark points{(xrealk,yrealk,zrealk)}were also measured, where k=1,2,…,Np, with respect to the reference system, (superscripts do not indicate powers). According to Eq. (1), the expected pixel position of the landmark points in the video frame, given the current model parameters are:(9)(ximk,yimk)=M(xrealk,yrealk,zrealk;p,xsph,ysph,n).The values of the model parameters are obtained by minimizing the error between the expected and the observed pixel coordinates of the landmark points:(10)(p,xsph,ysph,n)=argminp,xsph,ysph,n∑k=1Np(Ximk-ximk)2+(Yimk-yimk)2.Due to the large number of parameters and the complexity of the objective function, we employed a recently proposed variant of the Differential Evolution (DE) presented in [33]. DE has been designed as a stochastic parallel direct search method that typically requires few, easily chosen, control parameters. Experimental results have shown that DE has good convergence properties and outperforms other well-known evolutionary algorithms [34,35]. A population of individuals is randomly initialized in the optimization domain. Subsequently, the individuals are iteratively evolved in order to explore the search space and locate the optima of the objective function. Note that the number of the individuals in the population remains constant throughout the evolution.At each iteration, which is called generation, new vectors (offsprings) are produced by a combination of randomly chosen vectors. This operation is referred to as mutation. At the next step, the recombination operation mixes the offspring vectors with another predetermined vector – the target vector. This yields the so-called trial vector. The trial vector replaces the target vector if and only if it yields a reduction in the value of the objective function. This last operation can be referred to as selection. Performing the mutation, recombination and selection operations for all the population members constitutes a single iteration of the DE algorithm.We allow p to vary from 0.5 to 1.5, xsph,ysphto vary from in the range of [−R0/4, R0/4] and each coordinate of the normal vector n in the range [0,1]. Despite n being a unit vector (|n|=1), its components n1, n2 and n3 are treated as independent parameters by the optimizer and vector normalization is applied before calculating Eqs. (5)–(7). In this manner, we avoid the alternative of using only two vector components as independent parameters and calculating the third component using the unit-length, since this approach requires imposing penalties to the objective function in case of vector non-computability. The model parameters are obtained in just a few seconds using the Matlab programming environment in an average laptop computer. The resulting calibration of the fisheye model is shown in Fig. 4, where a virtual grid of points is laid on the floor and on the two walls of the imaged room where the camera is installed.The proposed fisheye calibration approach is compared to a well established toolbox for omnidirectional image calibration [25,26], which works without point input by the user, using a standard chess pattern, imaged at arbitrary orientations. Fig. 4 shows the resulting calibration using the proposed model and the method in [26]. The landmark points defined by the user are shown as circles and stars mark their expected position on the frame. A selected region of the frame is magnified in (b) and (d) for comparison. The mean displacement error for the proposed model is lower than the mean displacement error achieved by [26], considering the Nplandmark points (7.1 versus 9.2 pixels). It should be noted that this operation is only performed once after the initial installation of the fisheye camera and it does not need to be repeated in real time.The proposed calibration was further assessed by calculating the displacement error (pixels) for a new set of 18 points on the floor of the imaged room, as shown in Fig. 5, different than the Np(=18) points used for calibration in Eqs. (9) and (10). The resulting error using the proposed calibration and the calibration in [25,26] are given in Table 1. The maximum positional error is approx. 10 pixels, slightly lower than the maximum error of 11 pixels achieved by the calibration of [25,26].As with any kind of projective transformation, only the line of view can be recovered by the inverse transform, since any point on this line of view will be projected on the same image pixel. To use the model of the fisheye camera for refinement of video segmentation of human silhouettes, we need to utilize the elevation θ and azimuth φ of the line of view for each segmented pixel. In this subsection, we describe the inverse fish-eye camera model, i.e. given a pixel (j,i) in the video frame, the direction of view is obtained, defined by two angles: the azimuth θ and the elevation φ (Fig. 2), as described in (2). Using Eq. (5), the position of the pixel on the camera sensor is calculated:(11a)(xim,yim)=(f1,f2)RFOV((j,i)-(CoDx,CoDy)).The Z-coordinate of the image pixel on the sensor plane is given by:(11b)zim=zplane-ximn1+yimn2n3.The intersection Q of the spherical optical element with the line defined by O(0,0,0) and (xim,yim) is determined, as(12)(Qx,Qy,Qz)T=mA-1(xim,yim,zplane)T,where the parameter m is determined by requiring that Q lies on the spherical optical element:(13)m2(xim2+yim2+zplane2)-2(ximxsph+yimysph+zplanezsph)m+xsph2+ysph2+zsph2-R02=0.The required θ and φ are obtained by converting the Cartesian (Qx,Qy,Qz) to spherical coordinates:(14)φ=cos-1Qz-zsphR0,θ=sin-1Qy-ysph(Qx-xsph)2+(Qx-xsph)2.The above process is executed only once, after the calibration of the fisheye camera model and the resulting values for the θ, and φ parameters for each frame pixel are stored in two look-up tables, of size equal to a single video frame. The look-up tables for the azimuth θ and the elevation φ are shown in Fig. 6(a) and (b), respectively. As expected, the azimuth obtains values in [−π,π], whereas the elevation obtains values in [0,π], with the maximum value at the CoD pixel of the frame.In this subsection we discuss the height and width calculation of segmented objects and describe the employed metric of proximity of different segmented binary objects in the fish-eye frame. The detection of non-plausible segmentation and the proposed algorithm of geometric reasoning is also outlined.Let us suppose that an object in real world, shown in Fig. 7(a) as observed in a view from above, is segmented in a video frame. Using the azimuth look up table, its minimum, maximum and average azimuth angle can be easily retrieved. Similarly, its minimum and maximum elevation angle can be easily estimated, using the elevation look-up table, as shown in Fig. 7(b).Assuming that the segmented object is the silhouette of a person touching the floor with a surface of limited area (such as a standing, walking or sitting human), the position on the floor (xreal, yreal) can be calculated, as follows. Let φmaxand θavgbe the maximum elevation and average azimuth of the pixel of the binary object in the segmented frame that corresponds to the person (Fig. 7(a and b)). Since zmax is the z coordinate of the floor with respect to the system of reference, the intersection of the line originating from the camera, defined by (θavg, φmax) with the floor plane (z=zmax) can be easily obtained:(15)xreal=zmaxsinφmaxcosφmaxcosθavg,yreal=zmaxsinφmaxcosφmaxsinθavg.Let us assume further that the person is standing and its position is not directly below the fisheye camera, (equivalentlyφmin<π2). Then the height and width of the object/person are estimated, as following:(16)height=zmax-tan(φmin)xreal2+yreal2,width=2xreal2+yreal2tan|θmax-θmin|2.By requiring that φmin is sufficiently below π/2 (e.g.φmin<910π2) and assuming that the imaged object has no strong concavities, then|θmax-θmin|2<π2should hold, therefore the maximum width of the object can also be estimated using (17). Otherwise, the height and the width are given a label value of −1.Very often the human silhouette is not segmented as a single binary object, but as a collection of separate adjacent binary objects. In addition, two or more segmented silhouettes, each one consisting of a number of binary components, may be close to each other in the video frame. In most of the cases where a perspective camera is used, the neighborhood of a pixel is defined as a square/parallelogram of constant size, irrespectively of the position of the pixel in the image. In the case of fisheye video frames, this approach is not satisfactory. In this work, the proximity of two segmented binary objects is inferred by their elevation and azimuth angles, as following. Let θi,min, θi,max, φi,min, φi,max be the minimum and maximum azimuth and elevation for objects i=1,2. Also, let δθ and δφ be two thresholds for the azimuth and elevation, respectively. If the two binary objects in the segmented frame do not lie close to the CoD (equivalently they are not directly below the camera, or φi,min<π/2), then they are proximal in the space of the fisheye frame, if the following holds:(17a)((θ1,min<θ2,minANDθ1,max>θ2,max)OR|θ1,max-θ2,max|<δθOR|θ1,min-θ2,min|<δθ)AND((φ1,min<φ2,minANDφ1,max>φ2,max)OR|φ1,max-φ2,max|<δθOR|φ1,min-φ2,min|<δφ).If any of the segmented binary objects lies directly below the camera (φi,max=π/2) then the concept of δθ is not applicable and proximity in the fisheye frame is defined if the following holds(17b)(θ1,min<θ2,minANDθ1,max>θ2,max)AND((φ1,min<φ2,minANDφ1,max>φ2,max)OR|φ1,max-φ2,max|<δθOR|φ1,min-φ2,min|<δφ).Recent research has been reported in feature detection in fisheye images using the geodesic distance between pixels on a sphere to define pixel neighborhoods in omni-directional images [36,37]. However, the geodesic distance results in neighborhoods with size that does not reduce as the pixel moves towards the edge of the field of view of the fisheye image. In this work we used the following heuristic: the δφ threshold is predefined and kept constant (δφ=π/32). The definition of δθ is slightly more complicated however: δθ=δθ0sin(φi,max), with δθ0=π/32. The use of thresholds δθ and δφ provides a more accurate metric of proximity in the case of fisheye images, than a rectangular region of pixels, commonly used in images. Fig. 8shows the corresponding regions using the constant δφ=π/32 and δθ=δθ0sin(φi,max), with δθ0=π/32 for a number of points every 50 pixels, along the horizontal line passing through the CoD.Fig. 9(a) shows the two main binary objects from the segmentation of a human silhouette: the main body (object 1) and the head (object 2). Using the azimuth and elevation look-up tables, we locate the pixels with minimum and maximum angle values: for object 1 points C and D with values φ1,min, φ1,max, points G and D with θ1,min, θ1,max. The corresponding points for object 2 are: points A and B with values φ2,min, φ2,max, points E and F with θ2,min, θ2,max. The φ1,min, φ1,max, φ2,min, φ2,max and δφ are shown graphically in Fig. 9(b). The criterion of Eq. (18) is satisfied, therefore, objects 1 and 2 are in proximity in the space of the fisheye frame.Change of illumination from a window, displacement of small objects on walls or inside the room, reflections of moving objects, or automatic changes of camera exposure settings, often results in foreground segmentation and false human silhouette detection. The calculation of possible object height, width and its position coordinates in the real world (xrealand yreal), combined with the known dimensions of the room allows the identification of non-plausible segmentation (segmentation that does not correspond to human silhouette). If a binary object in the segmented frame is not in proximity to any other object according to Eq. (17), then its real world position (xreal, yreal, zmax) is calculated as if the object was touching the floor. If the estimated real position, or its height is outside the allowed range (i.e. outside the room), the segmentation is flagged as non-plausible. Object B is an artificial example of such a case in Fig. 10(a). If the estimated position (xreal, yreal, zmax) falls within the allowed range, then the estimated height and width according to (17) are used to detect non-plausible segmentation, by imposing lower and upper thresholds. By relaxing the height threshold, the proposed algorithm can correctly identify as human silhouette, persons at different poses. The results shown in this work have been produced with a low height threshold of 1m, which allows identification of bending or sitting humans, while rejecting small moving objects (handbags, stools, etc.). The high threshold was set to 2.2m. Low and high thresholds for the estimated width were set to 0.2m and 1.5m respectively (the high threshold should not exclude silhouettes of single humans with activity such as hand gestures).Object C in Fig. 10(a) is a graphic example of such a case. This method allows detection of segmented objects that are not silhouettes of standing humans. An example of the applicability of detecting non-plausible segmentation is shown in Fig. 10(b). In this specific frame three humans are inside the room, the room borders are also superimposed. The moving door also causes foreground segmentation, as well as the reflection of one of the humans. As it will be shown in the result section, the proposed segmentation refinement algorithm rejects the segmentation caused by the moving door and the moving human reflection and identifies the binary components that correspond to each one of the three persons, despite the fact that two of them appear close to each other. Additionally, since the dimensions of the room are known, the proposed algorithm will also detect if the humans move outside the room border (e.g. through one of the doors).The proposed methodology for geometric reasoning initially utilizes a standard algorithm for labeling the binary objects (connected components) in the current segmented frame. This algorithm produces the labeled image L. The term “connected component” and “binary object” is used interchangeably in this section. In order to avoid excessive numbers of binary objects due to segmentation noise, each pixel of the segmented image is set to 1 if it has at least 5 non-zero neighbors (including itself), otherwise it is set to 0. We denote by Lithe binary image containing the pixels of the segmented frame that belong to the ith binary connected component (i.e. Li(p)=1,p:L(p)=i). Therefore, we will also use Lifor denoting the ith binary object. We will use•The function OK(B) that returns TRUE if the binary object in B is plausible and FALSE otherwise, as described in Section 2.4.3.An array gp that holds a flag indicating that the ith binary object has been processed.The proximity function which, given 2 binary segmented objects from a fisheye video frame, returns TRUE if the objects are in proximity according to Section 2.4.1 and FALSE otherwise.Initially, the largest plausible, non-processed binary object is selected as the required object L0 and appropriately flagged as processed. Subsequently, the first non-processed binary object Liis selected that is proximal to the required object, according to (17). The height and the width of the union of L0 and Liis calculated using (17). If the combined height and width is acceptable (OK(L0∪Li)==1), then the required object L0 is being updated as the union of L0 and Li(L0=L0∪Li) and the minimum and maximum values of the azimuth and elevation of the required object (θ0,min, θ0,min, φ0,min, φ0,min) are updated according the look-up tables. The ith binary object in the segmented frame is flagged as processed. The process is repeated until no other segmented object may be appended to the current object. If more binary objects exist in the segmented images that have not been processed, the steps are repeated for the detection of a different person in the room. Notice that the plausible segmentation is checked for the binary object L0 (the first time that the object is selected as the largest, non-processed, plausible object) and it is also checked for each time another object is appended to L0.The binary objects that are appended to L0 are not checked independently for plausible segmentation. The overall algorithm for geometric reasoning is presented in pseudocode below in Algorithm 1, where we assume for simplicity that the 1st binary object is the largest one. The corresponding workflow diagram is also provided in Fig. 11.Algorithm 1: The geometric reasoning algorithm in pseudocode.i=0 // Set the binary object index i=0num=max(L) // Set the maximum number of binary objects in L,while i<numi=1+1L0=(L==i) // binary image of the ith connected component (object)i1=0 // secondary index of connected components to be amended to ith objectIf OK(Li) AND gp(i)=0gp(i)=1i1=1while (i1<num)L1=(L==i1)if gp(i1)=0 AND proximity(Li,L1)=1 AND OK(L0∪Li)==1L0=L0∪Ligp(i1)=1i1=0updateθ0,min, θ0,max, φ0,min, φ0,maxendi1=i1+1endendendThe proposed algorithm merges binary objects into a segmented silhouette, rejects binary objects that cannot be merged to any silhouette and it is capable of handling multiple silhouettes. The algorithm also calculates the trajectory of each silhouette. Typical segmentation of a human silhouette consisting of 12 binary objects from a single frame is shown in Fig. 12(a)–(c). The largest connected component is shown in (a), with the elevation φ values of each pixel encoded as color. In (b) an intermediate step of the geometry-based silhouette refinement algorithm is shown, where the binary object corresponding to the human head has been added. In (c) the final result is shown, with the right arm and foot also added to the segmented human. Fig. 13shows examples of the application of the algorithm in cases of multiple activities. The output of the algorithm is color-encoded for visualization purposes: red color indicates segmentation that has been rejected, while other colors indicate activity segmented as human silhouette (each color corresponds to a single silhouette). In Fig. 13(a) the human silhouette that consists of 4 binary objects has been indentified (yellow color), whereas the moving door is discriminated against the silhouette and rejected (red color). Fig. 13(b) shows a similar case with two human silhouettes discriminated against each other and against the moving door. In Fig. 13(c) the two human silhouettes have been correctly discriminated against each other (yellow and green color), despite the fact that they appear very close in the frame. In Fig. 13(d) a human silhouette is correctly identified against a smaller object (moving stool). In Fig. 12(f)–(h) and (l), multiple human silhouettes are correctly identified despite their proximity and the difficulty in segmenting the shirt in one of them, whereas in (k) incorrect merging of two silhouettes is shown. In figure (e) a segmented silhouette is rejected as being outside the room. Finally in Fig. 12(i) and (j) a sitting human silhouette is also correctly identified by merging a large number of binary segments.It has to be mentioned that if one silhouette is partially occluded by another, then the two silhouettes will be counted as one. A long as this occlusion does not last for long, the unique labeling of the silhouettes (described in the next subsection) will still assign the correct label, using position, temporal information and appearance.The proposed algorithm identifies the individual human silhouettes in each video frame and rejects irrelevant segmentation. The algorithm that is proposed in this subsection uses appearance information as well as spatio-temporal information to assign a unique label to each silhouette, valid for the whole video. In this way, the trajectory of each silhouette may be determined and clues about the behavior of each person may be drawn.The problem may be formulated as following: assuming that N binary silhouettes have been segmented in the current frame k and M silhouettes have been uniquely labeled by the proposed algorithm so far, we need to map the N current silhouettes to the existing M labels according to a distance metric and possibly introduce a new label if necessary.The following are maintained for each unique label: the previous positions in 3D space (determined by the proposed geometry-based algorithm) and its mean normalized values of each of the Red, Green, Blue channels. Each new segmented silhouette i of the current frame is compared with each of the existing labels j, using a distance metric rijdefined as the product of the Euclidean distance dijbetween their known real positions on the floor and their difference in appearance in terms of their mean normalized RGB values.The minimum Euclidean distance dijis calculated between i and the last K0 positions of label j:(18)dij=minm((xi,real-xj,realm)2+(yi,real-yj,realm)2),m=k-K0+1,…kSimilarly, the minimum distance in the normalized RGB color space dijis calculated between i and the last K0 positions of label j where(19)cij=min(max(|Ri-Rjm|,|Gi-Gjm|,|Bi-Bjm|)),m=k-K0+1,…,kThe second term is the infinity norm of the mean normalized RGB values of the pixels that belong to silhouette i and unique label j (at m0 frame). RGB normalization is achieved by(Ri,Gi,Bi)=(ri,gi,bi)ri+gi+bi∈[0,1], where ri,gi,biare the mean R, G, B values of the video frame pixels that belong to the binary segmented silhouette i. This normalization allows independence from the image intensity, a feature necessary in our problem, since the surveillanced space extents over 12m, thus significant illumination changes are expected.The combined distance metric rijis calculated and stored in matrix Rij={rij} for all available labels as following:rij=dijcij,ifdij<d0ANDcij<c0max_val,otherwiseIf cijor dijare greater than a threshold, rijis set to a large value (max_val) to indicate no match between the silhouette and the existing labels. In this work, d0 was set to 0.5m and c0=20/255. The silhouette labeling algorithm proceeds as following: the pair (i0,j0) is found that holds the minimum value of rij. If the minimum value is less than the max_val, then silhouette j0 is assigned the label i0. Otherwise, the next label (M+1) is used. In order to avoid assigning the same label to more than one silhouette of the current frame, the j0th column and label i0th line of rijmatrix is set to the max_val. The above algorithm may be described in pseudocode as following:Initialize the current number of unique labels, M=0For each frameN: number of segmented silhouettes in current frameFor each binary segmented silhouette in the frame, j=1,...,NObtain its position in 3D coordinates (xj,real,yj,real) from the proposed geometry based algorithmCalculate the mean normalized values for each of the R, G, B channelsFor all labels i=1,2,…,MCalculatedij,cijand matrixRij={rij}endendwhileRijcontains values < max_valfind the position (i0,j0) of the minimum value m ofRijIf m<max_val (matching label exists)label the current segmented silhouettei0withj0update the real position and mean RGB values for labeli0setRi0,j=max_val,j=1,2,…,NsetRi,j0=max_val,i=1,2,…,Melse (no matching label exists)update the number of unique labelsM=M+1label the current segmented silhouettei0using the next available labelj0=Mupdate the real position and mean RGB values for labelj0append one line to theRijtable, with values max_val:RM,j=max_val,j=1,2,…,Nset the (j0)th column ofRijequal to max_val:ri,j0=max_val,j=1,2,…,MendendAfter the completion of the algorithm, unique labels that only appeared for vary limited number of frames, due to incorrect segmentation/geometric reasoning may be easily rejected.The proposed labeling algorithm has been shown to work well when a small/moderate number of persons are present in the room and the occlusions are relatively short. Exemplar results from the execution of this algorithm are given in Fig. 14for an environment of 4 discreet silhouettes with occlusions. The red color denotes rejected segmentation (not identified as human silhouette), the rest of the colors denote unique segmented silhouettes. The unique labels are superimposed on each silhouette (color indicates acceptance/rejection, but not unique label). It can be seen that correct unique labeling is achieved, despite two occlusions. Details are given in the figure captions. Trajectory estimation of each silhouette is shown in Fig. 17(c) and (d).

@&#CONCLUSIONS@&#
In this paper we presented a methodology that identifies the segmentation of human silhouettes from fisheye video sequences, against other irrelevant segmentation, or versus segmentation of silhouettes outside a predefined area. The proposed algorithm is based on clues of real world geometry, derived from a calibrated model of a fisheye camera.Regarding the complexity of the proposed methodology the performed experiments have shown that the number of connected binary objects in the segmented frames, num, is of the order of 20. It can be verified that the worst case complexity of geometric reasoning for any binary frame is O(N2), where N is the number of binary objects in the segmented frame. The segmentation algorithm, as well as the geometric reasoning were implemented using Matlab and executed on an Intel(R) Core i5-2430 CPU @ 2.40GHz Laptop with 4GB Ram, under Windows 7 Home Premium. The mean execution time for the segmentation was approximately 50ms per frame of dimension 480×640. The mean execution time for the proposed geometric reasoning was measured approximately 40ms per frame. The silhouette labeling algorithm imposes negligible computational load compared to the other algorithmic steps. No special source code optimization or parallelization was used for these time measurements.Initial results show that the proposed algorithm significantly reduces the false positive pixels regarding human silhouette segmentation, by rejecting a number of instances of segmentation that cannot be easily rejected without any geometric information. Furthermore, the proposed algorithm assigns the remaining binary, segmented objects to the silhouette they belong. The algorithm can handle multiple silhouettes. Finally, the proposed approach employs an algorithm for identifying individual silhouettes using unique labels, thus allowing the estimation of the real world trajectory of each silhouette. This output may be used to enhance human motion and action recognition software that is based on silhouettes, often used in assistive environments. The silhouette labeling algorithm has been shown to work in video sequences with a moderate number of silhouettes with occlusions of short duration.Future work includes the enhancement of the labeling algorithm to improve the accuracy of the geometry-based segmentation refinement algorithm, by correcting sudden changes of the number of identified silhouettes by the recent history of number, position and trajectory of the silhouettes. Finally, the proposed methodology may be applicable to the traditional type of projective cameras, since they would require much simpler calibration. The geometry based segmentation refinement algorithm and the silhouette labeling can also be applied, after modifications to other (projective) type of cameras. Furthermore, if the camera is installed on the wall (with a horizontal central line of view) rather than on the ceiling, the geometry-based silhouette segmentation refinement would be simpler (in this manuscript, a standing human may have any orientation in the video frame, depending on its location).