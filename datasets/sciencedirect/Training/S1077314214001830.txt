@&#MAIN-TITLE@&#
Cross-calibration of time-of-flight and colour cameras

@&#HIGHLIGHTS@&#
Survey of depth-colour calibration methods.Novel calibration method based on projective alignment.Indepth experimental validation and error analysis.Alignment of time-of-flight and colour camera networks.

@&#KEYPHRASES@&#
Camera networks,Time-of-flight cameras,Depth cameras,Camera calibration,3D reconstruction,RGB-D data,

@&#ABSTRACT@&#
Time-of-flight cameras provide depth information, which is complementary to the photometric appearance of the scene in ordinary images. It is desirable to merge the depth and colour information, in order to obtain a coherent scene representation. However, the individual cameras will have different viewpoints, resolutions and fields of view, which means that they must be mutually calibrated. This paper presents a geometric framework for the resulting multi-view and multi-modal calibration problem. It is shown that three-dimensional projective transformations can be used to align depth and parallax-based representations of the scene, with or without Euclidean reconstruction. A new evaluation procedure is also developed; this allows the reprojection error to be decomposed into calibration and sensor-dependent components. The complete approach is demonstrated on a network of three time-of-flight and six colour cameras. The applications of such a system, to a range of automatic scene-interpretation problems, are discussed.

@&#INTRODUCTION@&#
The segmentation of multi-view video data, with respect to physically distinct objects of interest, is an essential task in automatic scene-interpretation. Visual segmentation can be based on colour, texture, parallax and motion information (e.g. [1,2]). The task remains very difficult, however, owing to the combined effects of non-rigid surfaces, variable lighting, and occlusion. It has become clear that depth cameras can make an important contribution to scene understanding, by enabling direct depth segmentation, based on the measured scene-structure (see CVIU special issue [3]). This approach is also highly effective for dynamic tasks, such as body tracking and action recognition [4]. Furthermore, if depth and colour information can be merged into a single representation, then a complete 3D representation is possible, in principle. This is clearly desirable, because colour and texture data are essential to many other aspects of scene-understanding, such as identification and tracking [5].There are two major obstacles to the construction of a complete scene representation, from a multi-modal camera network. Firstly, typical depth sensors are unable to capture RGB data [6]. This means that the depth and colour cameras will have different viewpoints, and so the raw data are inconsistent. Secondly, typical TOF and RGB cameras have limited fields of view, and so the depth and colour data are incomplete. This paper addresses both of these problems, by showing how to estimate the geometric relationships in a multi-view, multi-modal camera network. This task will be called cross-calibration.In order to constrain the problem, two practical constraints are imposed from the outset. Firstly, the system will be based on time-of-flight (TOF) cameras, in conjunction with ordinary RGB cameras. The TOF cameras are compact, can be properly synchronized, and are industrially specified, e.g., [6]. Secondly, a modular network of TOF+RGB units is required. This is so that individual units can be added or removed, in order to optimize the scene-coverage.

@&#CONCLUSIONS@&#
