@&#MAIN-TITLE@&#
Multi-bridge transfer learning

@&#HIGHLIGHTS@&#
MBTL constructs multiple latent spaces to exploit more common latent factors.MBTL reduces the discrepancies of the distributions in different latent spaces.To solve MBTL, we present an iterative algorithm with convergence guarantee.MBTL outperforms state-of-the-art learning methods on several datasets.

@&#KEYPHRASES@&#
Transfer learning,Non-negative matrix tri-factorization,Multi-bridge,Cross-domain classification,

@&#ABSTRACT@&#
Transfer learning, which aims to exploit the knowledge in the source domains to promote the learning tasks in the target domains, has attracted extensive research interests recently. The general idea of the previous approaches is to model the shared structure in one latent space as the bridge across domains by reducing the distribution divergences. However, there exist some latent factors in the other latent spaces, which can also be utilized to draw the corresponding distributions closer for establishing the bridges. In this paper, we propose a novel transfer learning method, referred to as Multi-Bridge Transfer Learning (MBTL), to learn the distributions in the different latent spaces together. Therefore, more latent factors shared can be utilized to transfer knowledge. Additionally, an iterative algorithm with convergence guarantee based on non-negative matrix tri-factorization techniques is proposed to solve the optimization problem. Comprehensive experiments demonstrate that MBTL can significantly outperform state-of-the-art learning methods on the topic and sentiment classification tasks.

@&#INTRODUCTION@&#
Traditional machine learning classification algorithms implicitly assume that the training and test data are drawn from the same distribution. However, this assumption seldom holds in reality. To tackle the challenge of different data distributions, many transfer learning methods have been proposed recently for real-world applications, such as text classification [13], computational biology [11] and image classification [12]. The key idea of transfer learning or domain adaptation is to exploit the labeled examples in the source domain to model a better classifier for predicting the classes of the test examples in the target domain where exist less or no labeled examples. Some of these previous approaches show that features on raw words are not reliable for text classification in cross-domain learning. For example, when the documents which belong to the category of “computer” are drawn from the domains “hardware” and “software”, the words in these documents indicating the concept of “computer technology” can be “keyboard”, “CPU”, “operating system”, “programmer”, and so on. However, the frequencies of these words may be different in different domains. In the domain of “software”, high-frequency words are “operating system”, “programmer”, etc., while the words like “keyboard” and “CPU” are the high-frequency ones in the domain of “hardware”. Since these original features can not be shared directly, only the high-level concept “computer technology”, which is extracted from these words, can be utilized to distinguish the category of “computer” across domains. Therefore, the latent high-level concepts, which are related to feature clusters extracted on the raw features, are more appropriate for the text classification across domains than learning from the original features [7]. CoCC [3] learns the identical concept. MTrick [4] exploits the association between the homogenous concept and the example classes as the bridge across domains. DTL [5] models the shared concepts including the identical and homogenous concepts to establish the bridge. In addition, Tri-TL [6] and HIDC [7] exploit the distinct concept to training classifier besides theshared concepts.These previous methods usually build one bridge across domains by constructing a transformed high-level feature space and reducing the corresponding distribution divergences. We represent such method as the single bridge transfer learning. The limitation of the single bridge approaches is two-fold. (1) A set of latent factors in one latent feature space is just a subset of all the latent factors. The assumption that all the shared factors only exist in one latent feature space cannot hold in reality and may ignore the latent factors existing in the other latent feature spaces, which may also help to model the shared structure across domains. (2) To transfer knowledge, these methods exploit the latent factors in one latent feature space to learn the corresponding distributions in this latent feature space. However, not all these latent factors can be utilized to draw the corresponding distributions in the latent feature space closer. Some of the latent factors which are also useful for knowledge transfer may represent the discrepancy between the distributions across domains. In the worst case, when the divergences of the distribution in one latent space are so large and there exist some latent factors, which cannot be used to draw the corresponding distributions in the latent feature space closer, utilized to learn the distributions, these single bridge methods will happen negative transfer.In this paper, we propose Multi-Bridge Transfer Learning (MBTL), a novel transfer learning method based on non-negative matrix tri-factorization (NMTF) techniques, which constructs multiple latent feature spaces, and learns the corresponding distributions in the different latent spaces simultaneously. The key idea of MBTL is as follows. Firstly, as shown in Fig. 1, by constructing multiple different latent feature spaces, more latent factors can be exploited to model the shared structure across domains. Secondly, since MBTL learns the distributions in the different latent spaces simultaneously to establish multiple bridges across domains as shown in Fig. 2, the latent factors in the different latent feature spaces can be used to reduce the distribution divergences in the different latent feature spaces respectively. Additionally, when Some of latent factors in one latent feature space represent the discrepancy between the distributions across domains in this latent space, they can also be utilized to reduce the distribution discrepancies in the other latent feature spaces.The main contributions of MBTL are summarized as follows:(1)Motivated by the observation that the latent factors usually exist in different latent feature spaces, we propose the MBTL method to utilize the latent factors to reduce the distribution divergences in different latent feature spaces simultaneously.To solve the proposed method MBTL, we present an iterative algorithm with convergence guarantee based on non-negative matrix tri-factorization techniques.We construct the systematic experiments to show the effectiveness of MBTL. In particular, all the compared topic-oriented methods happen negative transfer on data set 20-Newsgroups frequently and can not compete with the traditional machine learning method Logistic Regression (LR) on the sentiment tasks. Only MBTL seldom occurs negative transfer and obtains the best performance on all the tasks.The rest of this paper is organized as follows. Section 2 introduces the related work. We review preliminary knowledge in Section 3. In Section 4, we describe the model of MBTL. Section 5 provides the experimental results. Finally, Section 6 concludes this paper.

@&#CONCLUSIONS@&#
