@&#MAIN-TITLE@&#
Automatic scoring for answers to Arabic test questions

@&#HIGHLIGHTS@&#
The research deals with responses written in Arabic.The research presents a new benchmark Arabic dataset that contains 610 answers.The system gets a model response from an already built database for specific curriculum.Responses are translated into English to overcome the lack of NLP resources in Arabic.Different methods of scaling the similarity values to be in the same range as the manual scores are presented.

@&#KEYPHRASES@&#
Short answer scoring,Text similarity,Semantic similarity,Arabic corpus,

@&#ABSTRACT@&#
Most research in the automatic assessment of free text answers written by students address English language. This paper handles the assessment task in Arabic language. This research focuses on applying multiple similarity measures separately and in combination. Many aspects are introduced that depend on translation to overcome the lack of text processing resources in Arabic, such as extracting model answers automatically from an already built database and applying K-means clustering to scale the obtained similarity values. Additionally, this research presents the first benchmark Arabic data set that contains 610 students’ short answers together with their English translations.

@&#INTRODUCTION@&#
The rapidly growing educational community, both electronic and traditional, with an enormous number of tests has caused a need for automatic scoring systems. Automatic Scoring (AS) systems address evaluating a student's answer by comparing it to model answer(s). AS technology handles different types of students’ responses, such as writing, speaking and mathematics. Writing assessment comes in two forms: Automatic Essay Scoring (AES) and Short-Answer Scoring. Speaking assessment includes low and high entropy spoken responses, while mathematical assessments include textual, numeric or graphical responses. AS Systems are easily implemented for certain types of questions, such as Multiple Choice, True–False, Matching and Fill-in-the-Blank. Implementing an automatic scoring system for questions that require free text answers is more difficult because students’ answers require complicated text understanding and analysis. In this research, short-answer scoring is handled through an approach that addresses students’ answers holistically and depends on text similarity measures (Mohler and Mihalcea, 2009; Mohler et al., 2011). Three types of text similarity measures are handled: String similarity, Corpus-based similarity and Knowledge-based similarity. String similarity measures operate on string sequences and character composition. Corpus-based depends on information derived from large corpora. Knowledge-based uses semantic networks (Mihalcea et al., 2006; Budanitsky and Hirst, 2001; Gomaa and Fahmy, 2013).This research presents a system for short-answer scoring in the Arabic language. Arabic is a widespread language that is spoken by approximately 300 million people around the world. From a natural language point of view, the Arabic language is characterized by high ambiguity, rich morphology, complex morpho-syntactic agreement rules and a large number of irregular forms (Habash, 2010). Our system focuses mainly on measuring the similarity between the student and the model answers using a bag of words (BOW) model and disregarding complex Arabic computational linguistics tasks.The system translates students’ responses into English to overcome the lack of text processing resources in the Arabic language. Acknowledging that machine translation is sub-optimal, but it is still helpful for the scoring task as experiments will explain in the next sections. Different methods of scaling the similarity values to be in the same range as the manual scores are presented and tested. Multiple text similarity measures were combined using supervised and unsupervised methods; this combination affected the obtained results positively.Additionally, the system presents a module that searches for a model answer from an already built database that is aligned with the curriculum.This paper is organized as follows: Section 2 presents related work on automatic short-answer scoring systems. Section 3 introduces the three main categories of Similarity Algorithms used in this research. Section 4 presents the first Arabic data set to be used for benchmarking short-answer scoring systems. In Section 5, the proposed system is illustrated with a walk-through example. Section 6 shows the experiment results, and finally, Section 7 presents the conclusions of the research.

@&#CONCLUSIONS@&#
This paper examines an important research area, which is short-answer scoring for non-Latin languages, especially Arabic.Our research goes through five stages. The First stage was building a data set that supports the Arabic language due to the unavailability of Arabic data sets. While building the data set, three aspects were considered: the variety of question types, the assessment process and the scoring quality. The variety of short-answer question types provided by the specialist cover every part of the curriculum, which increases the system scalability and allows the examiner to suggest any question within the domain. The assessment process was performed by two specialists, to improve the scoring quality.The Second stage was scaling the similarity values that were obtained from any similarity algorithm, to make them in the same range of the manual scores, which was accomplished through 4 different methods; SimpleScale, IsotonicScale, Clust6 and Clust11. The IsotonicScale, Clust6 and Clust11 methods have positively affected the correlation and error rate results. Using the simple unsupervised K-means algorithm to scale the similarity value is one of the important contributions of our system.The third stage was applying the similarity algorithms to the original Arabic text; Three string-based similarity algorithms were tested: DL, LCS and N-gram, and the N-gram character-based was found to be the best. It achieved promising correlation but disappointing RMSE results. The two Corpus-Based Similarity algorithms, DISCO1 and DISCO2 were tested and both achieved promising r and RMSE results.The Fourth stage was translating from Arabic to English. Two reasons were behind using translated texts: First, comparing the behavior of String-Based and Corpus-Based methods for different languages. Second, testing the Knowledge-Based measures using the English WordNet to benefit from its wide word coverage. Human translation and two machine translations (Google and Bing) were used. String-Based Similarity measures have the same behavior for different languages, while Corpus-Based Similarity measures for English obtained a higher correlation and a lower RMSE than for Arabic due to the role of the corpus size and other features, which are shown in Table 1. The experiments proved that machine translation can be relied upon for building a fully automatic application.The Fifth and final stage was combining the measures from different categories, which raised the correlation results to be 0.83 and decreased the RMSE to be 0.75. These resulting values are very close to values that were scored manually by the two annotators, which were r=0.86 and RMSE=0.69. A combination task is performed using Weka through examining three supervised models: Simple Linear Regression, Linear Regression and SMOreg models. The three models were applied with two methods: one method collects all of the measures on the runs, and the second method is applied to the best runs. The experiments emphasized the idea of mixing measures not only from different categories but also from different languages as well.In conclusion, we presented a study whose results can enhance the learning of environment tasks, especially those that cover Arabic language. The first benchmark Arabic data set was introduced and contained 610 students’ short answers together with their English translations. The obtained results prove that the presented system performs well enough for deployment in a real scoring environment. The application is suitable for any language by providing a corpus in a certain language and a machine translation. The application ensures the benefits of mixing different similarity approaches and similarity value scaling methods. A final advantage of the presented system is its usability for the examiners and the course's authors because they are able to enter their conceived questions without restriction to the questions’ forms that were previously saved in the data set.The future work will cover three main points. The first point is comparing the usage and training of generic and domain-specific corpora in corpus-based similarity measures. The second point is enriching the Arabic WordNet data to be suitable for knowledge-based measures. The final point is providing the student with useful feedback that contains comments on the students’ answers and an explanation for the assigned automatic score.Supplementary material related to this article can be found, in the online version, at http://dx.doi.org/10.1016/j.csl.2013.10.005.