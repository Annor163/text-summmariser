@&#MAIN-TITLE@&#
Shape-based modeling of the fundamental frequency contour for emotion detection in speech

@&#HIGHLIGHTS@&#
We propose neutral reference models to detect emotions in the fundamental frequency.A novel scheme based on functional data analysis is presented.The proposed approach achieves higher accuracies than the state-of-the-art method.The scheme is also employed to the to detect the most emotionally salient segments.The approach is validated at the sub-sentence level by employing a natural database.

@&#KEYPHRASES@&#
Emotion detection,F0 contour modeling,Emotional speech analysis,Expressive speech,

@&#ABSTRACT@&#
This paper proposes the use of neutral reference models to detect local emotional prominence in the fundamental frequency. A novel approach based on functional data analysis (FDA) is presented, which aims to capture the intrinsic variability of F0 contours. The neutral models are represented by a basis of functions and the testing F0 contour is characterized by the projections onto that basis. For a given F0 contour, we estimate the functional principal component analysis (PCA) projections, which are used as features for emotion detection. The approach is evaluated with lexicon-dependent (i.e., one functional PCA basis per sentence) and lexicon-independent (i.e., a single functional PCA basis across sentences) models. The experimental results show that the proposed system can lead to accuracies as high as 75.8% in binary emotion classification, which is 6.2% higher than the accuracy achieved by a benchmark system trained with global F0 statistics. The approach can be implemented at sub-sentence level (e.g., 0.5s segments), facilitating the detection of localized emotional information conveyed within the sentence. The approach is validated with the SEMAINE database, which is a spontaneous corpus. The results indicate that the proposed scheme can be effectively employed in real applications to detect emotional speech.

@&#INTRODUCTION@&#
Emotional understanding is a crucial skill in human communication. It plays an important role not only in interpersonal interactions, but also in many cognitive activities such as rational decision making, perception and learning (Picard, 1997). For this reason, modeling and recognizing emotions is essential in the design and implementation of human-machine interfaces (HMIs) that are more in tune with the user's needs. Systems that are aware of the user's emotional state will facilitate several new scientific avenues that serve as truly innovative advancements in security and defense (e.g. threat detection), health informatics (e.g., depression, autism), and education (e.g., tutoring system) (Burleson and Picard, 2004; Langenecker et al., 2005). Given the important role of speech in the expression of emotions, an increasing number of publications have reported progress in automatic emotion recognition and detection using acoustic features. Complete reviews are given by Cowie et al. (2001), Zeng et al. (2009), Schuller et al. (2011a), Koolagudi and Rao (2012), El Ayadi et al. (2011).The dominant approach in emotion recognition from speech consists in estimating global statistics or functionals at sentence level from low level descriptors such as F0, energy and Mel-frequency cepstral coefficients (MFCCs) (Schuller et al., 2011a). Among prosodic based features, gross pitch statistics such as mean, maximum, minimum and range are considered as the most emotionally prominent parameters (Busso et al., 2009). One limitation of global statistics is the assumption that every frame in the sentence is equally important. Studies have shown that emotional information is not uniformly distributed in time (Lee et al., 2004; Busso and Narayanan, 2007). For example, the intonation in happy speech tends to increase at the end of the sentence (Wang et al., 2005). Since the statistics are computed at the global level, it is not possible to identify local salient segments or focal points within the sentence. Furthermore, features describing global statistics do not capture local variations (e.g., in F0 contours), which in turn could provide useful information for emotion detection. In this context, this paper proposes a novel shape-based approach to detect emotionally salient temporal segments in the speech using functional data analysis (FDA). The detection of localized emotional segments can shift current approaches in affective computing. Instead of recognizing the emotional content on pre-segmented sentences, the problem can be formulated as a detection paradigm, which is appealing from an application perspective (e.g., continuous assessments of unsegmented recordings). The emotion recognition system can be more robust by weighting each frame according to their emotional saliency. From a speech production viewpoint, the approach can shed light into the underlying interplay between lexical and affective human communication across various acoustic features (Busso and Narayanan, 2007).This study focuses on detecting emotionally salient temporal segments on the fundamental frequency. Patterson and Ladd (1172) argued that the range (i.e., the difference between the maximum and the minimum of F0 contour in a sentence or utterance) does not give information about the distribution of F0 and hence valuable emotional information is neglected. Also, according to Lieberman and Michaels (1962) low variations in F0 can be subjectively relevant in the identification of emotions. In the literature, there are some attempts to model the shape of the F0 contour. Paeschke and Sendlmeier (2000) analyzed the rising and falling movements of F0 within accents in affective speech. The study incorporated metrics related to accent peaks within a sentence. The authors found that those metrics present statistically significant differences between emotional classes. Also, Paeschke (2004) modeled the global trend of F0 in emotional speech as the gradient of linear regression. The author concluded that global trend can be useful to describe emotions such as boredom and sadness. Rotaru and Litman (2005) employed linear and quadratic regression coefficients and regression error as features to represent pitch curves. Yang and Campbell (2001) argued that concavity and convexity of the F0 contour reflect the underlying expressive state. The Tone and Break Indices system (ToBI) is a scheme for labeling prosody that has been widely used for transcribing intonation (Silverman et al., 1992). Liscombe et al. (2003) analyzed affective speech with acoustic features by using ToBI labels to identify the type of nuclear pitch accent, the contour type and the phrase boundaries. Despite the fact that ToBI provides an interesting approach to describe F0 contours, more precise labeling is required to generate prosodic transcripts. Taylor (2000) introduced the Tilt Intonation Model to represent intonation as a linear sequence of events (e.g. pitch accents or boundaries), which in turn are given by a set of parameters. However, an automatic event segmentation algorithm is required to employ this scheme and, hence, it cannot be easily applied to emotion recognition or detection tasks.Despite current efforts to address the problem of affective speech characterization by means of modeling F0 contour, this is still an open task. The contributions of the paper concern: (a) a novel framework to detect emotional modulation based on reference templates that models F0 contours of neutral speech; (b) an insightful and thorough analysis of neutral references as a method to detect emotion in speech; (c) the generation of reference F0 contour templates with functional data analysis (FDA); and, (d) a study of the shortest segmentation unit that can be used in emotion detection. Extensive experiments are presented to demonstrate the discriminative power of the FDA based approach to detect emotional speech. The results on the SEMAINE database reveal that the approach captures localized emotional information conveyed in short speech segments (0.5s). These properties of the proposed approach are interesting from the research and application points of view.The analysis and results presented in Sections 3 and 4 require recordings with controlled, lexicon-dependent conditions (e.g., recordings of sentences with the same lexical content conveying different emotional states). Therefore, the study considers, for these sections, two emotional databases recorded from actors (Table 1). Even though acted emotions differ from real-life emotional manifestation, they provide a good first approximation, especially when controlled conditions are required, as in this study. The first database is the EMA corpus collected at the University of Southern California (USC) (Lee et al., 2005).11The EMA database is available at http://sail.usc.edu/ema_web/Three speakers participated in the recordings (two of them with formal theatrical vocal training). They read ten sentences five times in happy, angry, sad and neutral conditions (one subject read 4 additional sentences producing 80 extra samples). The subjects were asked to record the sentences in random order to attenuate or eliminate reproductions with similar intonation. To reduce fatigue, the recording was split into small sessions separated by breaks. The EMA database was evaluated by four native speakers of American English in terms of the emotional classes happy, angry, sad, neutral and other. The average human recognition rate was 81.8% (Grimm et al., 2007). The second corpus is the Berlin Database of Emotional Speech (EMO-DB) (Burkhardt et al., 2005). This database is composed of ten speaker (five male and five female), who read ten German sentences one time expressing six different emotions (fear, disgust, happiness, boredom, sadness, anger), in addition to neutral state. This database is available to the community and has been widely used in related work on emotion recognition. Therefore, other researchers can easily replicate the results presented here.After relaxing the controlled lexicon-dependent conditions, the framework is validated using a spontaneous emotional corpus (Section 5). The study considers the SEMAINE database, which includes audiovisual recordings of natural human computer interactions (McKeown et al., 2010). The emotions are elicited using the Sensitive Artificial Listener (SAL) approach. We consider sessions recorded from ten subjects. The data contains subjective evaluations generated by human raters using Feeltrace (Cowie et al., 2000). This is a labeling tool employed to continuously track the perceived emotional state over time (as oppose to assigning one discrete label per sentence). The raters are asked to move the cursor as they watch/listen the stimulus using a graphical user interface (GUI). The GUI records the position of the pointer, which describes the emotional content in term of continuous attributes. Although the corpus has been annotated with various emotional attributes, we consider only the activation/arousal (calm versus active) and valence (negative versus positive) dimensions.The fundamental frequency is estimated according to the following steps: first, speech signals are divided into 400-sample (25ms) frames with 50% overlap. The fundamental frequency is estimated by using the autocorrelation based Praat pitch detector system (Boersma and Weenink, 1996). Then, F0 at each frame is represented in a semitone scale according to:(1)F0semitone(t)=12logF0(t)log2where F0(t) and F0semitone(t) are the fundamental frequency at frame t in Hertz and semitones, respectively. The proposed scheme in this paper aims to model the F0 contour to compare neutral and emotional speech. Consequently, the logarithm attempts to represent differences in F0 according to the human-like perception scale. After estimating F0semitone(t), unvoiced segments are interpolated with cubic spline to obtain smooth and continuous F0 contours. Finally, the resulting interpolated F0semitone(t) contour is normalized by subtracting the mean. Henceforth, the term “F0 contour” denotes the F0 curve in the semitone scale after interpolation and mean normalization.

@&#CONCLUSIONS@&#
This paper proposed a novel method to detect emotional modulation in F0 contours by using neutral reference models with functional PCA basis functions. The projections into this basis define the features that are used to train an emotion detection system. The approach was evaluated under different conditions. First, we built lexicon-dependent conditions (i.e., one basis per sentence), which achieved accuracies as high as 75.8% in binary emotion classification tasks. This performance is 6.2% higher than the ones achieved by a benchmark classifier trained with global F0 statistics. Then, we evaluated lexicon-independent functional PCA basis, built with F0 contours extracted from utterances with different lexical content. The results showed that the degradation in accuracy provided by lexicon-independent models was not significant when compared with the lexicon-dependent system (i.e. from 75.8% to 74.2%). The proposed system was then applied at the sub-sentence level to detect the most emotionally salient segments. The difference in accuracy between sentence and time-based segment classifiers was not significant across all the emotional categories. Finally, the approach was validated with a spontaneous database. The objective emotional prominence metric given by the functional PCA projections correlates with subjective evaluations. Furthermore, the system achieved 62.7% accuracy in binary classification which is 5% higher than benchmark classifier.Our future work includes the incorporation of other prosodic features such as energy contour and duration. Likewise, the proposed scheme can be extended to detect specific emotional categories (e.g., happiness versus anger). For example, we can build emotion dependent functional PCA basis with F0 contours extracted from sentences labeled with a target emotion. Alternatively, the emotion detection system can be used as a first step in a more sophisticated multi-class emotion recognition system, in which emotional speech samples are further assigned to finer emotional labels (e.g., happiness versus anger). The combination of FDA with the polynomial representation of curves can also be proposed as future research. Finally, the functional PCA based method presented in this paper can be even extended to other speech processing tasks such as prosody assessment in second language learning. Basically, the idea is to train a functional PCA basis by using prosodic patterns extracted from native speech. Then, the testing F0 contours generated by non-native speakers can be assessed by employing the projections onto the native speaker basis.