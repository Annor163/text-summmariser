@&#MAIN-TITLE@&#
Object motion analysis description in stereo video content

@&#HIGHLIGHTS@&#
We study object motion in stereo video content by providing a novel mathematical analysis.The relationship between the viewers angular eye velocity and object motion is studied.We elaborate on how disparity modifications affect the perceived position of the object in the theater space.Novel algorithms for the semantic description/characterization of object motion in stereo video content are provided.

@&#KEYPHRASES@&#
Motion analysis,Motion characterization,Stereo video,Semantic labeling,

@&#ABSTRACT@&#
The efficient search and retrieval of the increasing volume of stereo videos drives the need for the semantic description of its content. The analysis and description of the disparity (depth) data available on such videos, offers extra information, either for developing better video content search algorithms, or for improving the 3D viewing experience. Taking the above into account, the purpose of this paper is twofold. First, to provide a mathematical analysis of the relation of object motion between world and display space and on how disparity changes affect the 3D viewing experience. Second, to propose algorithms for semantically characterizing the motion of an object or object ensembles along any of the X, Y, Z axis. Experimental results of the proposed algorithms for semantic motion description in stereo video content are given.

@&#INTRODUCTION@&#
In recent years, the production of 3D movies and 3D video has been growing significantly. A large number of 3D movies have been released and some of them, e.g. Avatar [1] had great success. These box-office successes have boosted (a) the delivery of 3D productions, such as movies and documentaries, to home or to cinema theaters through 3D display technologies [2] and (b) the 3DTV broadcasting of various events, such as sports [3], [4], for a high quality 3D viewing experience. Furthermore, virtual reality systems for computer graphics, entertainment and education, which use stereo video technology, have been developed [5–7]. 3D video devices such as laptops, cameras, mobile phones, TV, projectors are now widely available for professional and non-professional users [1]. Because of the 3D movie success, several tools have been developed for the production and editing of 3D content [8,9].Since 3DTV content is now widely available, it must be semantically described toward fast 3D video content search and retrieval. Analysis of stereoscopic video has the advantage of deriving information that cannot be inferred from single-view video, such as 3D object position through depth/disparity information. Depth information can also be obtained from multiple synchronized video streams [10–12]. MPEG-4 offers a set of motion descriptors for the representation of motion of a trajectory [13]. 3D motion descriptors include the world coordinates and time information. In this paper, we propose the adoption such 3D descriptors for the extraction semantic labels such as “an object approaches the camera” or “two objects approach each other”. Such semantic description is only possible using 3D descriptors instead of 2D descriptors. In this paper, we concentrate on 3D object motion description in stereo video content. Various algorithms for semantic labeling of human, object or object ensemble motion are proposed. We utilize the depth information, which is implicitly available through disparity estimation between the left and right views, to examine various cases, where camera calibration information and/or viewing parameters may or may not be available, assuming that there are no camera motion and fixed intrinsic parameters. For example, we can characterize video segments, where an object approaches the camera or where two objects approach each other in the real world. It should be noted that the proposed algorithms can be applied in the case of a calibrated Kinect camera as well [14]. Indeed, a lot of works investigate the 3D reconstruction of object trajectories [15–17]. The novelty of the proposed algorithms is the object motion analysis providing semantic labels. Such semantic stereo video content description is very useful in various applications, varying from video surveillance and 3D video annotations archiving, indexing and retrieval to implementation of better audiovisual editing tools and intelligent content manipulation. Such characterization is not possible in classical single view video, without knowing depth information to get 3D position/motion clues [8]. Furthermore, such characterizations can be used for detecting various stereo quality effects [8]. For example, if an object having strong negative disparity has been labeled as moving along the x axis toward the left/right image border, then it is likely that a left/right stereo window violation may arise. The distance between foreground objects and the background influences the entire amount of depth information (depth budget) of the scene during display.Furthermore, we examine how the viewer perceives object motion during stereo display. Typically, stereo video is shot with a stereo camera to display objects residing and moving in the world space (Xw, Yw, Zw). The acquired stereo video depends on the stereo camera parameters, e.g., focal length and the baseline distance [8]. When displayed, the perceived object position and motion occurs in the display (theater) space (Xd, Yd, Zd). The perceived video content depends on the viewing parameters, e.g., the screen size and the viewing distance. The real and the perceived object motion may differ, depending on the camera and viewing parameters, as well as on stereo content manipulations [8]. Specifically, we assume that an object is moving with a known motion type (e.g., constant speed motion along the Zwaxis) and we determine what motion is perceived by the viewer. We examine various simple motion types, such as motion with constant velocity or constant acceleration along axes Xw, Ywor Zw. This analysis is very useful for avoiding cases where excessive motion particularly along the Zwaxis can cause viewing discomfort [18]. In addition, we elaborate on how disparity modifications affect the perceived position of the object in the theater space with respect to the viewer. This is very important in the stereo video post-production, when the scene depth is adapted for visually stressing important scenes or for ensuring visual comfort [8]. In this respect, the relationship between the viewer’s angular eye velocity and object motion in the world space is very important.The main novel contributions of this paper are:1.we study (Section 3) object motion in stereo video content by providing a novel mathematical analysis. The object position, velocity and acceleration are examined in various simple motion types. In addition, we study the relationship between the viewer’s angular eye velocity and object motion, in order to examine how the viewer perceives object motion during stereo display. In the same theoretical context, we elaborate on how disparity modifications affect the perceived position of the object in the theater space.We provide (Section 4) novel algorithms for the semantic description/characterization of object motion in stereo video content along the horizontal, vertical and depth axis, as well as characterizations of relative motion of pairs of objects (whether the objects approach each other or move away).These two contributions (theoretical, algorithmic) refer to different motion characteristics and thus are not related.The paper extends the work in [19] and [20] by including (a) the study of object motion in stereo video content providing a novel mathematical analysis and (b) the assessment of the robustness of the presented motion labeling methods in challenging scenes recorded outdoors in realistic conditions.The rest of the paper is organized as follows. In Section 2, the geometry of the stereo camera and of the display system is discussed. The transformations between the different coordinate systems of the world, stereo camera, screen and display (theater) space are given for two stereo camera setups, the parallel and converging ones. Section 3 contains the mathematical analysis for the relation between world and display system, the impact of screen disparity modifications on object position during display and the relation between object and viewer’s eye motion. In Section 4, algorithms for characterizing object and object ensemble motion are proposed. In Section 5, experimental results for motion characterization are presented. Finally, concluding remarks are given in Section 6.In stereo video, a 3D scene is captured by a stereo camera (a video camera pair), as shown in Fig. 1(a). A point of interestPw=[Xw,Yw,Zw]⊺in the 3D world space is projected on the left and right image plane positionspcl=[xcl,ycl]⊺andpcr=[xcr,ycr]⊺,respectively. For stereo video display, both images are projected (mapped) on the display screen plane locationspsl=[xsl,ysl]⊺andpsr=[xsr,ysr]⊺,respectively, as shown in Fig. 1(b). During display, the pointPd=[Xd,Yd,Zd]⊺which corresponds to Pwis perceived by the viewer in front of, on or behind the screen in the display (theater) space, as shown in Fig. 1(b), if the disparityd=xsr−xslis negative or positive, respectively.In this section, we describe in more detail the geometrical relations between the world and theater space coordinates for two types of stereo camera setups, the parallel [21], which is the most common case, and the converging one [22].The geometry of a stereo camera with parallel optical axes is shown in Fig. 2. The centers of projection and the projection planes of the left and right camera are denoted by the points Ol, OrandTl,Tr,respectively. The distances between the two camera centers and between the camera center of projection and the projection plane are the baseline distance Tcand the camera focal length f. The midpoint Ocof the baseline is the center of the world coordinate system (Xw, Yw, Zw). The world coordinate axis Xwcan be transformed into the left/right camera axesXwl,Xwrby a translation by ± Tc/2. A point of interestPw=[Xw,Yw,Zw]⊺in the world space is projected on the left and right image planes at the pointspcl=[xcl,ycl]⊺andpcr=[xcr,ycr]⊺respectively, while the pointsPwl=[Xwl,Ywl,Zwl]⊺andPwr=[Xwr,Ywr,Zwr]⊺refer to the same point Pwwith respect to the left and right camera coordinate systems, respectively. The projectionspclandpcrare related with the 3D pointsPwlandPwrusing perspective projection [21]:(1)xcl=fXwlZwl,ycl=fYwlZwl,xcr=fXwrZwr,ycr=fYwrZwr.Thus, the following equations give us the transform from the world space to the camera system coordinates:(2)xcl=fXw+Tc2Zw,ycl=fYwZw,xcr=fXw−Tc2Zw,ycr=fYwZw.It is well known that the Pwworld space coordinates can be recovered from thepcl,pcrprojections, as follows [21]:(3)Zw=−fTcdc,Xw=−Tc(xcl+xcr)2dc,Yw=−Tcycldc=−Tcycrdc,wheredc=xcr−xclis the stereo disparity. In the case of the parallel camera setup, we always have negative disparities:(4)dc=−fTcZw<0.The geometry of the display (theater) space is shown in Fig. 3. Teis the distance between the left/right eyes (typically, 60 mm) [23]. The distance from the viewer’s eye pupil centers (eland er, respectively) to the screen is denoted by Td. The origin Odof the display coordinate system (Xd, Yd, Zd) is placed at the midpoint between the eyes. The Xdaxis is parallel to the eye baseline. The Zd, Ydaxes are perpendicular to the screen and XdZdplanes, respectively. During stereo image display, the mapping of the projectionspclandpcrto the screen planepsl=[xsl,ysl]⊺andpsr=[xsr,ysr]⊺is achieved by scaling using a factorm=ws/wc,where wsis the width of the screen and wcthe width of the camera sensor,(5)xsl=mxcl,ysl=mycl,xsr=mxcr,ysr=mycr,that magnifies the image, according to the screen size, while the screen center coordinate (xs, ys) coincides with the shifted by Tcleft/right image plane coordinate(xcl,ycl),(xcr,ycr)centers, so that they coincide. Here, the distance ofxslandxsr,ds=xsr−xsl,is the screen disparity. The resulting perceived object position is in front of, on and behind the screen for negative, zero and positive screen disparity, respectively, as shown in Fig. 3a and b. The perceived location Pd(Xd, Yd, Zd) of the point Pwcan be found using triangle(pslPdpsr),(elPder) similarities [22]:(6)Zd=TdTeTe−ds,(7)Xd=Te(xsl+xsr)2(Te−ds),Yd=Te(ysl+ysr)2(Te−ds).Since in the parallel camera setup we always have negative disparities dcand thusTe−ds>Te,all objects appear in front of the screen Zd< Td. It can be easily proven that the coordinate transformation from the camera image plane to display space is given by:(8)Xd=mTe(xcl+xcr)2(Te−mdc),Yd=mTe(ycl+ycr)2(Te−mdc),Zd=TdTeTe−mdc.Finally, we can compute the overall coordinate transformation from world space to display space(9)Xd=mfTeXwmfTc+TeZw,Yd=mfTeYwmfTc+TeZw,Zd=TdTeZwmfTc+TeZw.The display geometry shown in Fig. 3 describes well stereo projection in theater, TV, computer and mobile phone screens, but not in virtual reality systems (head-mounted displays) [24].In this case, the optical axes of the left and right camera form an angle θ with the coordinate axis Zw, as shown in Fig. 4. The origin Ocof the world space coordinate system is placed at the midpoint between the left and right camera centers. The two camera axes converge on the point Ozat distance Tzalong the Zwaxis. A point of interestPw=[Xw,Yw,Zw]⊺in the world space, which is projected on the left and right image planes at the pointspcl=[xcl,ycl]⊺andpcr=[xcr,ycr]⊺,respectively, can be transformed into the left or right camera system by a translation by Tc/2 or−Tc/2,respectively, followed by a rotation by angle−θor θ about the Ywaxis, respectively:(10)[XwlYwlZwl]=[cosθ0−sinθ010sinθ0cosθ][Xw+Tc2YwZw],(11)[XwrYwrZwr]=[cosθ0sinθ010−sinθ0cosθ][Xw−Tc2YwZw].Using (1), the following equations transform the world space coordinates to the left/right camera coordinates:(12)xcl=f(Xw+Tc2)cosθ−Zwsinθ(Xw+Tc2)sinθ+Zwcosθ=ftan(arctan(Xw+Tc2Zw)−θ)(13)ycl=fYw(Xw+Tc2)sinθ+Zwcosθ,(14)xcr=f(Xw−Tc2)cosθ+Zwsinθ−(Xw−Tc2)sinθ+Zwcosθ=−ftan(arctan(−Xw+Tc2Zw)−θ),(15)ycr=fYw−(Xw−Tc2)sinθ+Zwcosθ.For very small angles θ(12)–(15) can be simplified using cos θ ≃ 1, sin θ ≃ θ rad. Whenθ=0,then equations (12)–(15) collapse to (8) and (9). As proven in Appendix A, the following equations can be used, in order to revert from the left/right camera coordinates into the world space coordinates:(16)Xw=Tcxcl+tanθ(f+xclxcrf+xcrtanθ)xcl−xcr+tanθ(2f+2xclxcrf−xcltanθ+xcrtanθ)−Tc2,(17)Yw=Tcyclfcos(arctan(xclf)+θ)cos(arctan(xclf))sin(arctan(xclf)+arctan(xcrf)+2θ),(18)Zw=Tcf−(xcl−xcr+xclxcrftanθ)tanθxcl−xcr+tanθ(2f+2xclxcrf−xcltanθ+xcrtanθ).Following the same methodology as in the parallel setup, the transformations from camera plane to the 3D display space are given by (5)–(7), respectively. For the case ofXw=0,it can easily be proven that, when Zw> Tz, the object appears behind the screen (Zd> Td), while for Zw< Tz, the object appears in front of the screen, as exemplified in Fig. 3a. This is the primary reason for using the converging camera setup in 3D cinematography. However, only small θs are used, because otherwise the so-called keystroke effect is very visible [8].Finally, the overall coordinate transformation from world space to display space is given [22] by the equations (19)–(21).(19)Xd=mfTe(tan(arctan(Xw+Tc2Zw)−θ)−tan(arctan(−Xw+Tc2Zw)−θ))2Te+2mf(tan(arctan(−Xw+Tc2Zw)−θ)+tan(arctan(Xw+Tc2Zw)−θ)),(20)Yd=mTe(fYw(Xw+Tc2)sinθ+Zwcosθ+fYw−(Xw−Tc2)sinθ+Zwcosθ)2Te+2mf(tan(arctan(−Xw+Tc2Zw)−θ)+tan(arctan(Xw+Tc2Zw)−θ)),(21)Zd=TdTeTe+mf(tan(arctan(−Xw+Tc2Zw)−θ)+tan(arctan(Xw+Tc2Zw)−θ)).Whenθ=0,(16)–(18) and (19)–(21) collapse to the parallel setup equations (3) and (9).In this section, the 3D object motion in stereo vision is mathematically treated. No such treatment exists in the literature, at least to the authors’ knowledge. In Section 3.1, we examine the true 3D object motion compared to the perceived 3D motion of the displayed object in the display space. In Section 3.2, we elaborate on how the change of screen projections affects stereo video content display. Finally, the effect of the perceived object motion on visual comfort is presented in Section 3.3.In this section, we analyze the perceived object motion during stereo video acquisition and display, assuming that the object motion trajectory in world space [Xw(t), Yw(t), Zw(t)]⊺ is known. We consider the parallel camera setup geometry. The perceived motion speed and acceleration can be derived by differentiating (9):(22)vZd(t)=TeTdTcfmZw′(t)(mfTc+TeZw(t))2,(23)aZd(t)=−TeTdTcfm(−2TeZw′(t)2+(Tcmf+TeZw(t))Zw′′(t))(mfTc+TeZw(t))3,(24)vXd(t)=mfTe((mfTc+TeZw(t))Xw′(t)−TeXw(t)Zw′(t))(mfTc+TeZw(t))2,(25)aXd(t)=mfTe((mfTc+TeZw(t))Xw′′(t)−TeXw(t)Zw′′(t))(mfTc+TeZw(t))2−2mfTe2Zw′(t)((mfTc+TeZw(t))Xw′(t)−TeXw(t)Zw′(t))(mfTc+TeZw(t))3.Similar equations can be derived for motion speed and acceleration along the Ydaxis. The following two cases are of special interest:(a)If the object is moving along the Zwworld axis with constant velocityZw(t)=Zw0+vZwt,its perceived motion along the Zdaxis has no constant velocity anymore:(26)Zd(t)=TeTd(Zw0+vZwt)mfTc+Te(Zw0+vZwt),(27)vZd(t)=TeTdTcfmvZw(mfTc+Te(Zw0+vZwt))2,(28)aZd(t)=−2TcTe2TdfmvZw2(mfTc+Te(Zw0+vZwt))3.If the object is moving along the Zwworld axis with constant accelerationZw(t)=Zw0+12aZwt2,the perceived motion along the Zdaxis is even more complicated:(29)Zd(t)=TeTd(aZwt2+2Zw0)2mfTc+Te(aZwt2+2Zw0),(30)vZd(t)=4TeTdmfTcaZwt(2mfTc+Te(aZwt2+2Zw0))2,(31)aZd(t)=−mfTeTdTc(12TeaZwt2−8mfTc−8TeZw0)aZw(2mfTc+Te(aZwt2+2Zw0))3.In both cases the perceived velocity and acceleration are not constant. Additionally, under certain conditions an accelerating object may be perceived as a decelerating one. If the object is moving along the Xwworld axis with constant velocityXw(t)=Xw0+vXwtand is stationary along the Zwworld axisZw(t)=Zw0,the perceived motion along axis the Xdaxis has constant velocity:(32)Xd(t)=mfTemfTc+TeZw0(Xw0+vXwt),(33)vXd(t)=mfTemfTc+TeZw0vXw,(34)aXd(t)=0.If the object is moving along the Xwworld axis with constant accelerationXw(t)=Xw0+12aXwt2and is stationary along the Zwworld axis,Zw(t)=Zw0,the same motion pattern applies to the perceived motion in the theater space:(35)Xd(t)=mfTemfTc+TeZw0(Xw0+12aXwt2),(36)vXd(t)=mfTemfTc+TeZw0aXwt,(37)aXd(t)=mfTemfTc+TeZw0aXw.In both cases the perceived velocity and acceleration are the actual world ones, scaled by a constant factor. If the object is moving along the Xwand Zwworld axes with constant velocitiesXw(t)=Xw0+vXwt,Zw(t)=Zw0+vZwt,the perceived motion pattern is very complicated.(38)Xd(t)=mfTemfTc+Te(Zw0+vZwt)(Xw0+vXwt),(39)vXd(t)=mfTe(mfTcvXw−TevZwXw0+TevXwZw0)(mfTc+Te(Zw0+vZwt))2,(40)aXd(t)=−2mfTe2vXw(mfTcvXw−TevZwXw0+TevXwZw0)(mfTc+Te(Zw0+vZwt))3,The case of motion along the Ywworld axis is similar to the one along the Xwaxis. For the case of constant velocities along both the Xwand Zwworld axes, it is apparent thatvXwvXd≠vZwvZd. Thus the perceived moving object trajectory is different than the respective linear trajectory in the world space. It is clearly seen that special care should be taken when trying to display 3D moving objects, especially when the motion along the Zwis quite irregular.Let us assume that the position of the projectionspsl=[xsl,ysl]⊺andpsr=[xsr,ysr]⊺of a point Pwon the screen can move with constant velocity. Assuming that there is no vertical disparity, we examine only x coordinates change at constant velocities uxl, uxr:(41)xsl(t)=xs0l+vxlt,(42)xsr(t)=xs0r+vxrt,wherexs0landxs0rare the initial object positions on the screen plane and vxland vxrindicate the corresponding velocities, having left and right direction respectively. Correspondingly, the screen disparity changes:(43)ds(t)=xs0r−xs0l+(vxr−vxl)t.Based on the equations (6) and (7), which compute the Xd, Ydand Zdcoordinates of Pdduring display with respect to screen coordinates, the following equations give the Pdposition and velocity:(44)Zd(t)=TdTeTe−ds(0)−(vxr−vxl)t,(45)dZd(t)dt=TdTe(vxr−vxl)(Te−ds(0)−(vxr−vxl)t)2,(46)Yd(t)=Te(ysl+ysr)2(Te−ds(0)−(vxr−vxl)t),(47)dYd(t)dt=Te(ysl+ysr)(vxr−vxl)2(Te−ds(0)−(vxr−vxl)t)2,(48)Xd(t)=Te(xs0r+vxrt+xs0l+vxlt)2(Te−ds(0)−(vxr−vxl)t),(49)dXd(t)dt=Te2(vxr+vxl)+2Te(vxrxs0l−vxlxs0r))2(Te−ds(0)−(vxr−vxl)t)2.As expected, according to the (45) the object appears moving away from the viewer, when vxr> vxl, and approaching the viewer, when vxr< vxl. In the case ofvxr=vxl,the value of Zddoes not change. Similarly, though the vertical disparity is zero, according to (47), the object appears moving downward/upward, when vxris bigger/smaller than vxl, respectively, while in case ofvxr=vxl,the value of Yddoes not change. Finally, according to (49), the cases where Xdincreases, decreases and does not change are illustrated in Fig. 5.Therefore, disparity manipulations (e.g., increase/decrease) during post-production can create significant changes in the perceived object position and motion in the display space. These effects should be better understood, in order to perform effective 3D movie post-production. It should be noted that viewing experience is also affected by motion cues and the display settings [25].When eyes view a point on the screen, they converge to the position dictated by its disparity, as shown in Fig. 3. The eye convergence anglesϕxl,ϕxrare given by the following equations:(50)ϕxl=arctan(xsl+Te2Td),(51)ϕxr=arctan(xsr−Te2Td).The angle ϕyformed between the eye axis and the horizontal plane is given by:(52)ϕy=arctan(yslTd)=arctan(ysrTd).If the camera parameters are unknown, the angular eye velocities can be derived by differentiating (50)–(52):(53)dϕxl(t)dt=4Tddxsl(t)dt4Td2+Te2+4Texsl(t)+4xsl(t)2,(54)dϕxr(t)dt=4Tddxsr(t)dt4Td2+Te2−4Texsr(t)+4xsr(t)2,(55)dϕy(t)dt=Tddys(t)dtTd2+ys(t)2.If the camera parameters are known and the position of a moving object in the world space is given byPw(t)=[Xw(t),Yw(t),Zw(t)]⊺,(2) and (5) can be used to derive, the angular eye positions over time:(56)ϕxl(t)=arctan(mfTc+2mfXw(t)+TeZw(t)2TdZw(t)),(57)ϕxr(t)=arctan(−mfTc+2mfXw(t)−TeZw(t)2TdZw(t)),(58)ϕy(t)=arctan(mfYw(t)TdZw(t)).The angular eye velocities can be derived by differentiating (56)–(58) as given by (59)–(61):(59)dϕxl(t)dt=2mfTd(2Zw(t)Xw′(t)−(Tc+2Xw(t))Zw′(t))m2f2Tc2+4m2f2Xw(t)2+2mfTeTcZw(t)+(4Td2+Te2)Zw(t)2+4mfXw(t)(mfTc+TeZw(t)),(60)dϕxr(t)dt=−2mfTd(2Zw(t)Xw′(t)+(Tc−2Xw(t))Zw′(t))m2f2Tc2+4m2f2Xw(t)2+2mfTeTcZw(t)+(4Td2+Te2)Zw(t)2−4mfXw(t)(mfTc+TeZw(t)),(61)dϕy(t)dt=mfTd(Zw(t)Yw′(t)−Yw(t)Zw′(t))m2f2Yw(t)2+Td2Zw(t)2.A few simple cases follow. If the object is moving along the Zwaxis and it is stationary with respect to the other axes,Zw(t)=Zw+vwzt,Xw(t)=0Yw(t)=0as given by (62)–(64):(62)dϕxl(t)dt=−2mfTdTcvzwm2f2Tc2+2mfTeTc(Zw+vzwt)+(4Td2+Te2)(Zw+vzwt)2,(63)dϕxr(t)dt=2mfTcTdvzwm2f2Tc2+2mfTeTc(Zwvzwt)+(4Td2+Te2)(Zw+vzwt)2,(64)dϕy(t)dt=0.If the object is moving along the Xwaxis and it is stationary with respect to the other axes,Zw(t)=Zw,Xw(t)=vxwt,Yw(t)=0,the following angular eye velocities result as given by (65)–(67):(65)dϕxl(t)dt=4mfTdvxwZwm2f2Tc2+4m2f2vxw2t2+2mfTeTcZw+(4Td2+Te2)Zw2+4mfvxwt(mfTc+TeZw),(66)dϕxr(t)dt=4mfTdvxwZwm2f2Tc2+4m2f2vxw2t2+2mfTeTcZw+(4Td2+Te2)Zw2−4mfvxwt(mfTc+TeZw),(67)dϕy(t)dt=0.If the object is moving along the Ywaxis and it is stationary with respect to the other two axes,Zw(t)=Zw,Xw(t)=0,Yw(t)=vywt,we have the following angular eye velocities:(68)dϕxl(t)dt=0,(69)dϕxr(t)dt=0,(70)dϕy(t)dt=mfTdvywZwm2f2vyw2t2+Td2Zw2.This analysis is important for determining the maximal object speed in the world coordinates or the maximal allowable disparity change, when capturing a fast moving object. If certain angular velocity limits (e.g., 20 deg/s for ϕx[26]) are violated viewer’s eyes cannot converge fast enough to follow it, therefore causing visual fatigue. In addition, there are also limits (e.g., 80 deg/s [27]) for the cases of smooth pursuit (65),(66) and (70) that must not be violated either.In this section, we will present a set of methods for characterizing 3D object motion in stereo video. In our approach, an object (e.g., an actor’s face in a movie or the ball in a football game), is represented by a region of interest (ROI), which can be used to refer to an important semantic description regarding object position and motion characterization. It must be noted that, in most cases, neither camera nor viewing parameters are known. In such cases, object motion characterization is based only on object ROI position and motion in the left and right image planes.Object ROI detection and tracking is overviewed in Section 4.1. In Sections 4.2 and 4.3, object motion description algorithms are presented, which describe the object motion direction in an object trajectory and the relative motion of two objects, respectively.We consider that an object is described by an ROI within a video frame or by an ROI sequence, over a number of consecutive frames. These ROIs may be generated by a combination of object detection (or manual initialization) and tracking [28]. Stereo tracking can be performed as well for improved tracking performance [29]. In its simplest form, a rectangular ROI (bounding box) can be represented by two pointsp1=[xleft,ytop]⊺andp2=[xright,ybottom]⊺,where the xleft, ytop, xright and ybottom are the left, right, top and bottom ROI bounds, respectively. Such ROIs can be found on both the left and right object views. In the case of stereo video, object disparity can be found inside the ROI by disparity estimation [21]. This procedure produces dense or sparse disparity maps [30]. Such maps can be used to obtain an ‘average’ object disparity, e.g., by averaging the disparity over the object ROI [19]. Alternatively, gross object disparity estimation can be a by-product of the stereo video tracking algorithm, based, e.g., on left/right view SIFT point matching within the left/right object ROIs [31]. In the proposed object motion characterization algorithms, an ROI is represented by its center coordinatesxcenter=(xleft+xright)/2,ycenter=(ytop+ybottom)/2along x and y axis, its width and height (if needed) and an overall (‘average’) disparity value.In order to better evaluate an overall object disparity value for the object ROI, we first use a pixel trimming process [32], in order to discard pixels that do not belong to the object, since the ROI may contain, apart from the object, background pixels. First, the mean disparityd¯using all pixels inside a central region within the ROI. A pixel within the ROI is retained only when its disparity value is in the range [d¯-a,d¯+a], where a is an appropriately chosen threshold. Then, the trimmed mean disparity valued¯αof the retained pixels is computed [19,32].In order to characterize object motion, when not knowing the camera and display parameters, we examine the motion separately on x and y axes in the image plane and in the depth space, using object disparities. Specifically, we use the x and y ROI center coordinates [xcenter(t), ycenter(t)]⊺ in both left/right channels and (3) or (7) for characterizing the horizontal and vertical object motion. We can also use the trimmed mean disparity valued¯αand (3) or (6) for labeling object motion along the depth axis over a number of consecutive video frames. In any case, the unknown parameters are ignored. An example of ad¯αsignal (time series), where t indicates the video frame number is shown in Fig. 6. In this particular case, in the theater space the object first stays at a constant depth Zdfrom the viewer, then it moves away and finally it moves closer the viewer. Whend¯α(t)=0,the object is exactly on screen (Zd=Td). To perform motion characterization, we use first a moving average filter of appropriate length, in order to smooth such a signal over time [33]. Then, the filtered signal can be approximated, using, e.g., a linear piece-wise approximation method [34]. The output of the above process is a sequence of linear segments, where the slope of each linear segment indicates the respective object motion type. The motion duration is defined by the respective linear segment duration. Depending on whether the slope has a negative, positive or close to zero value, respective movement labels can be assigned for each movement, as shown in Table 1. If too short linear segments are found and their slopes are small/moderate, the respective motion characterization can be discarded.If the stereo camera parameters are known, then the true 3D object position of the left/right ROI center in the world coordinates can be found, using (3) or (16)–(18) for the object ROI center for the parallel and converging stereo camera setups, respectively. In the uncalibrated case, there are cases where the true 3D object position can be also recovered [35]. The same can be done for the display space, if we know the display parameters m, Td, Te, using the ROI center coordinates. Therefore, the movement labels of Table 1 can be used for both world space and display space, following exactly the same procedure for characterizing object motion in the world and display spaces, by using the vector signals [Xw(t), Yw(t), Zw(t)]⊺ and [Xd(t), Yd(t), Zd(t)]⊺, respectively.In such cases, characterizations of the form ‘object moving away/approaching the camera or the viewer’ have an exact meaning. Values of Zd(t) outside the comfort zone [8] indicate stereo visual quality problems. Large slope of Zd(t) over time, i.e., its derivative exceeding an acceptable thresholdZd′(t)>ud,can also indicate stereo quality, e.g., eyes convergence problems.Two (or more) objects or persons may approach to (or distance from) each other. For such motion characterizations of object ensembles, we shall examine two different cases, depending on whether camera calibration or display parameters are known or not. If such parameters are not available, 3D world or display coordinates cannot computed. Thus, object ensemble motion can be labeled independently along the spatial (image) x, y axes and along the ‘depth’ axis (using the trimmed average disparity values), only for the parallel camera setup and display. For a number of consecutive video frames, the ROI center coordinates of the left and right video channels are combined intoXcenteri=xlcenteri+xrcenteri2(Te−d¯αi)andYcenteri=ycenteriTe−d¯αi(a typical value for Teis used) using (7) orXcenteri=xlcenteri+xrcenteri2d¯αiandYcenteri=ycenterid¯αiusing (3), for the display or parallel camera, respectively, in all cases the unknown parameters are ignored. The Euclidean distances betweenpi=[Xcenteri,Ycenteri]⊺andpj=[Xcenterj,Ycenterj]⊺and the respective disparity valuesd¯αiandd¯αjof two objects i, j are computed as follows:(71)Dxy=(Xcenteri−Xcenterj)2+(Ycenteri−Ycenterj)2,(72)Dd=(d¯αi−d¯αj)2.The resulting two signals are filtered and approximated by linear segments, as described in the previous subsection. Similarly, depending on whether the linear segment slope has a negative, positive or close to zero value, the corresponding motion label can be assigned, as shown in Table 2. Even in the absence of camera and display parameters, disparity information can help in inferring the relative motion of two objects: if both Dxyand Dddecrease, the objects come closer in the 3D space. However, in such a case no Euclidean distance (e.g., in meters) can be found.The same procedure can be extended to the case of more than two objects: we can characterize whether their geometrical positions converge or diverge. To do so, we can find the dispersion of their positions vs their center of gravity in the xy domain and in the ‘depth’ domain:(73)Dxy=∑i=1N[(Xcenteri−X¯center)2+(Ycenteri−Y¯center)2],(74)Dd=∑i=1N(d¯αi−d¯¯α)2.and then perform the above mentioned smoothing and linear piece-wise approximation.When camera calibration parameters are available, the world coordinates [Xw, Yw, Zw]⊺ of an object, which is described by the respective ROI center [xcenter, ycenter]⊺ and trimmed mean disparity valued¯α,can be computed by the equations using (3) and (16)–(18) for the parallel and converging camera setup, respectively. Consequently, the actual distance between two objects, which are represented by the two points P1 and P2, can be calculated by using the Euclidean distance∥P1−P2∥2in the 3D space. Then, the same approach using smoothing and linear piece-wise approximation can be used for characterizing the motion of two objects.The same procedure can be applied for characterizing their motion in the display space, if the display parameters are known.

@&#CONCLUSIONS@&#
In this paper, 3D object motion mapping from the world space to the image space and to the display (theater) space is first analyzed in a novel way. The effect of screen disparity changes on the viewing experience is presented. Then new algorithms are presented that characterize object motion in stereo video content along the horizontal, vertical and depth axis and assign labels depending on whether two objects approach each other or move away. On the other hand, a mathematical analysis is presented about the relation of object motion in world coordinates compared to their perceived motion in the display (theater) space. Finally, we examine whether and how the viewing experience is affected by disparity manipulations.