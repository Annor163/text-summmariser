@&#MAIN-TITLE@&#
A practical efficient human computer interface based on saccadic eye movements for people with disabilities

@&#HIGHLIGHTS@&#
A wearable EOG-based system was developed to enable people with disabilities to type.Saccadic eye movements could be detected efficiently by an adaptive algorithm.Average accuracy of the system was 84% with a typing speed of 4.5 character per min.Participants with disabilities could learn to perform necessary eye movements.

@&#KEYPHRASES@&#
People with disabilities,Electro-oculogram,Human computer interface,Saccadic eye movement,Wearable systems,

@&#ABSTRACT@&#
Human computer interfaces (HCI) provide new channels of communication for people with severe motor disabilities to state their needs, and control their environment. Some HCI systems are based on eye movements detected from the electrooculogram. In this study, a wearable HCI, which implements a novel adaptive algorithm for detection of saccadic eye movements in eight directions, was developed, considering the limitations that people with disabilities have. The adaptive algorithm eliminated the need for calibration of the system for different users and in different environments. A two-stage typing environment and a simple game for training people with disabilities to work with the system were also developed. Performance of the system was evaluated in experiments with the typing environment performed by six participants without disabilities. The average accuracy of the system in detecting eye movements and blinking was 82.9% at first tries with an average typing rate of 4.5cpm. However an experienced user could achieve 96% accuracy and 7.2cpm typing rate. Moreover, the functionality of the system for people with movement disabilities was evaluated by performing experiments with the game environment. Six people with tetraplegia and significant levels of speech impairment played with the computer game several times. The average success rate in performing the necessary eye movements was 61.5%, which increased significantly with practice up to 83% for one participant. The developed system is 2.6×4.5cm in size and weighs only 15g, assuring high level of comfort for the users.

@&#INTRODUCTION@&#
Diseases or injuries in the neuromuscular system may lead to levels of motor disabilities. People with brain injury, locked-in-syndrome, cerebral palsy, muscular dystrophy or amyotrophic lateral sclerosis may have very limited movement ability in their limbs, which can also be accompanied by speech impairments. These conditions leave them completely dependent on others, leading a very poor quality of life. Assistive technology has recently had a paramount impact on the life of people with disabilities, by helping them to overcome their limitations, and to benefit from social interactions, education, and even employment.In recent decades, human–computer interfaces (HCIs) have been developed to provide people with disabilities new channels of communication with others, as well as the means to control their environment. Different approaches have been used in these systems to detect the user׳s intention, including the use of electroencephalogram (EEG) [1–3] and electromyogram (EMG) signals [4,5] Most people with severe motor disabilities can still control their eye movements. Therefore, eye-tracking systems have also been used to detect the user’s selection among several visual options. Different techniques have been used to detect eye movements which includes the use of a search coil [6,7], infrared oculography [8,9], video-based eye tracking [10], dual Purkinje image technique [11] and electrooculography.Electrooculography (EOG) is the technique of recording eye dipole (negatively charged retina relative to the cornea) from the skin around the eyes. The signal is usually recorded horizontally and vertically [12]. It mainly reflects the gaze direction with a practically linear relationship for gaze angles of up to ±30° [13].In contrast to other aforementioned methods for eye tracking, systems that use EOG signal are easy to operate, more accurate and reliable [14]. The EOG signal has relatively high amplitude and a simple relationship with the eye movements, which makes its interpretation and connection with the user׳s intention simple in comparison to EEG and EMG signals. Detection of eye movements with EOG is non-invasive, inexpensive and fast which make it suitable for real-time applications. In addition, easy classification of different gestures that the eye can take, makes the use of eye movements a relatively rich channel of communication with a relatively high transfer rate [15] for people with severe motor disabilities in comparison to EEG or EMG based systems.The EOG signal has been used in several studies to develop human computer and human machine interfaces. It is used to handle wheelchairs [13,16–18] control robots [19–21], recognize reading activity [22], move cursor position on the computer screen [23], type in a virtual keyboard [24,25], and play games [26].There are certain challenges associated with interpreting the EOG signals, including its baseline drift which can hardly be filtered due to the low frequency nature of the EOG signal. In addition, the EOG signal is slightly different for each participant and its amplitude depends on the background light of the environment. Without using a practical automated algorithm, frequent recalibration is necessary during the EOG analysis.Different signal processing methods have been utilized to identify eye gestures from the EOG signal. Yamagishi et al. [27] have defined eight directional eye movements and Banik et al. [28] have considered different specific sequences of horizontal eye movements as commands and used threshold analysis of the signal amplitude to detect them. Shang-lin et al. [29] have applied a four-level thresholding on the signal amplitude in order to identify eye movements in eight directions as well as blinking. Ianez et al. [30] have used thresholds on the derivatives of horizontal and vertical EOG to detect predefined eye movements to left, right, up and down. They used these movements in addition to the blink to control a robot arm. Several statistical parameters of the EOG signal were also tested for off-line saccade sequence recognition [31]. Deng et al. [26] used fuzzy distinction rules for recognition of eye movements in four directions to control an EOG-based game. Usakli and Gurkan [25] have employed the nearest neighborhood algorithm to distinguish eye movements in four directions as well as blinking for typing in a virtual keyboard. Continuous wavelet transform with a radial basis function (RBF) classifier has also been utilized [32] to detect saccadic eye movements over long periods of time. Bulling at al. [33] have used a thresholding method on Continuous Wavelet Transform of the signal to detect saccades, fixations and blinks. They have devised 90 different features based on these characteristics, and used the minimum redundancy maximum relevance feature selection method to select a subset of these features. A support vector machine (SVM) has then been used to detect five specific activity classes. Wu et al. [34] have also used wavelet transform and a SVM classifier to identify three commands of looking up, single blink and double blinks.In recent years, wearable HCI systems have been developed to provide the users, comfort and convenience in long term use. The main features of these systems are the small size, light weight, and low power consumption. The trade-off between wearability and the performance is the main challenge in developing these systems. Bulling et al. [14] have proposed an EOG-based embedded eye tracker. This wearable device consists of a pair of goggles and a wearable EOG processing unit. The goggles include dry active electrodes, a light sensor, and an accelerometer, all attached to a frame. The Goggles are connected to the processing unit with a shielded 14-core cable. The device has been aimed for mobile HCI applications, activity recognition and context awareness. Kirbis and Kramberger [35] have designed a mobile device for electronic eye gesture recognition. It has been developed to control different computer applications and home appliances wirelessly based on the users’ eye gestures, and uses a Bluetooth or alternatively an IR module for transferring data to the computer. Barea et al. [36] have developed an EOG-based HCI system using a commercially available video eyewear, with dry electrodes installed on it. The eyewear is connected to the main desktop system via ZigBee, which controls a home automation system using a WiFi network. Recently, Wu et al [37] have proposed a new classification algorithm to detect eye movements in eight directions as well as blinking, showing its performance with a wireless EOG acquisition device.The previous studies reviewed above have examined the developed EOG-based HCI systems on people without disabilities. Our observations show that this can be a major drawback; since people with disabilities usually have restrictions even in moving their eyes or may have involuntary movements in their head and therefore a system that demonstrates a high performance when evaluated by people without disability, may be unpractical for people with disabilities. The ease of use is a very important factor in a practical HCI for people with disabilities, depending on the physical specifications of the system, the eye gestures protocol, and the user interface. In addition, the systems developed in some previous studies are very sensitive to several threshold values and demand special calibrations for each individual user, and repeated calibrations in long runs for proper functionality [24,26,27], which makes the use of the system difficult. Moreover, the algorithms proposed in some studies [14,33] are too heavy to be used for online processing in real applications.This paper describes the development and evaluation of a wearable EOG-based human computer interface, aimed to be practical and comfortable for people with disabilities. Eye gesture commands were defined based on the limitations observed in eye movements and blinking in people with disabilities. An adaptive algorithm was proposed to improve the system performance in detecting eye movements in eight directions as well as double blinks, and to eliminate manual calibration for each user, and repeated calibrations in long runs. An efficient typing environment is proposed that allows the user to select each desired character by only two eye movements and two double blinks. The system consists only of a miniaturized hardware, which is mounted on eyeglasses and is connected wirelessly to a laptop computer via Bluetooth. The system has been designed for low power consumption, small size and lightweight to improve the wearability. The performance of the system in detecting eye movements was evaluated by participants with severe motor disabilities using a game environment. The typing environment was also evaluated by participants without disabilities.A novel wearable EOG-based human computer interface has been proposed. The system consists of wearable hardware mounted on a glasses frame, recording and transmitting horizontal and vertical EOG directly to a laptop, and a computer program that receives and analyses the EOG data in real time for recognizing eye movements and interpreting them as commands for an efficient user interface for typing.The system was aimed as a wearable device with small size, lightweight and low power consumption, to guarantee the user׳s comfort in real applications. The block diagram of the hardware is shown inFig. 1. It consists of an ADS1294 integrated analog front-end for bio-potential recording (Texas Instruments), a PIC16lf88 micro-controller (Microchip) to program the front-end chip and relay the recorded signals to a BTM182 Bluetooth module (Rayson Technology) that transmits the data to a laptop.Ag/AgCl electrodes were used at positions shown in Fig. 1 to record two channels of horizontal and vertical EOG. The sampling rate was considered 250 sps, high enough for the low-frequency EOG signal. The data were then transmitted wirelessly with the baud rate of 19,200bps to the computer.Fig. 2 demonstrates a prototype of the developed device mounted on the temple of eyeglasses. The total weight of the hardware (excluding the weight of the eyeglasses) is 7g without battery and 15g with a 3.7V, 260mAh Li/Ion battery which is mounted on the back of the board. The electronic board is 4.5×2.5cm. The total power consumption of the hardware was measured 123mWh, and it works continuously for 6h without the need to recharge the battery. The radio works properly up to 10m from the computer.The software was implemented in MATLAB environment version 7.10 (Mathworks Inc.), to analyze the received EOG signals, recognize the nine defined eye gestures in real time and to provide an environment for efficient typing. In another version of the software, the identified eye gestures were used to run a simple game, aimed to evaluate the ability of participants with disabilities to work with the system and help them learn to work with that efficiently.The authors encountered problems in reading the serial port associated with the Bluetooth link directly in MATLAB. The port could not be opened most of the times or with delays up to several minutes. Therefore, a serial capture program, RealTerm (Version 2), was called from MATLAB, that received the data and passed it to the developed program.A robust real-time algorithm was developed to process the horizontal and vertical EOG signals and to recognize the user commands based on his/her eye gestures. The pre-processing step includes an electrode-off detection algorithm, and filtering for the elimination of the noise. The processing steps are aimed to detect saccadic eye movements and blinks, and to follow their sequences to identify the predefined eye movements which a user is supposed to perform to control the interface. The protocol was defined based on preliminary evaluation of the limitations that participants with disabilities had in moving their eyes and blinking. These limitations include their difficulty in moving their eyes which makes their eye movements very slow, and difficulty in closing their eyes firmly and instantly. In participants with cerebral palsy involuntary head and body movements usually affected the EOG signal. However, since the system was wearable and wireless, these effects were minimum and participants could still manage to work with the system.In the typing environment, the user is supposed to look straight forward at the center of the computer screen (The laptop was placed in front of his/her face). To move the cursor on the virtual keyboard, the user should move his/her eyes for more than 30° in a direction and return to the center from 0.4 to 1.5s later. This specific protocol and the 30° threshold were considered for limiting false command detections due to usual eye movements in exploring the screen. Because eye movements were slow in people with disability, a return of sight to the center of the screen was allowed for up to 3s in their experiments with the game. The eye movements are recognized in eight main and diagonal directions. Selecting a key on the keyboard was initially performed by closing the eyes for 1s. However, participants with disability usually could not firmly and timely close and open their eyes. Therefore the protocol was changed to a double blink in less than 0.8s. This also reliably prevented unwanted selections due to involuntary eye blinks. The processing steps are demonstrated as a flow chart inFig. 3, while the command recognition algorithm is shown inFig. 4.In long-term recordings, the electrode connections may become weak due to the head movements and sweating, which results in an unstable baseline and high amplitude noise on the signal. This leads to undesired cursor movements or selections at the typing environment. To avoid this, the algorithm continuously monitors the standard deviation of the recorded signals. If the standard deviation in a time window exceeds an expected range (Min=0.1nV and Max=100mV), the input signal is not considered valid, and no further processing steps are taken until it comes back to the range.A fifth order Butterworth low-pass filter with cut-off frequency of 20Hz was employed to remove almost all high frequency noises including those from the power supply and the EMG of the muscles close to the electrodes. In addition, to cancel the baseline drift of the signal, a second order Butterworth high-pass filter with cut-off frequency of 0.05Hz was implemented.Blinks appear mostly in the vertical EOG signal and have to be detected, first because two consecutive blinks are considered the ‘select’ command, and second because the algorithm has to distinguish them from saccadic eye movements that are considered the ‘move’ commands. Here, detection of the eye blinks was accomplished by a template matching technique. A 400ms (95 samples) template was created by averaging 50 sample recorded blinks from one participant after aligning them based on the time of their peaks. The template was used for all the experiments. The detected blinks are then replaced in the vertical signal by a linear interpolation between the first and the last point of the blink.When the user moves his/her eyes, based on the direction and the angle of the eye movement, rapid changes with different amplitudes appear in vertical and/or horizontal EOG signals. The derivative of both vertical and horizontal EOG signals is calculated to detect these saccadic eye movements. Any significant positive peak of horizontal and vertical derivative signals means right and up saccades, respectively, while the negative peaks indicate left and down saccades. The amplitude of these changes varies among individuals, especially among people with disabilities. It also depends on the environmental illumination. Therefore, the thresholds that are applied on the derivative signals to identify saccadic eye movements from noise were considered adaptive. The four thresholds (positive and negative threshold on horizontal and vertical signals) are determined based on concurrent values of peak and noise level as described in Eq. (1). In each step of the analysis, the peak or noise levels are updated based on Eqs. (2) or (3) if the derivative signal is identified as an eye movement peak or if it is not, respectively. In these equations, ‘a’ and ‘b’ are constant values between 0 and 1 (0.5 for ‘a’ and 0.825 for ‘b’ were used in this study).(1)Threshold=NoiseLevel+a.(PeakLevel−NoiseLevel)(2)PeakLevel=(1−b).PeakLevel+b.Signal(3)NoiseLevel=(1−b).NoiseLevel+b.SignalInitial system evaluations during the design process demonstrated that this adaptive algorithm significantly improves the systems performance for users with disability.Fig. 5 illustrates the process when the eyes moved to up-left side and returned to center and then two blinks occurred.The sequence of detected saccadic eye movements is then analyzed to recognize eight user commands. The commands were defined as rapid movement of the eyes to one of the main (up, down, right or left) or diagonal (up-right, up-left, down-right or down-left) directions for more than about 30°, and then returning the eye gaze to the front. Fig. 4 illustrates the flow chart of recognizing the commands based on the sequence of eye movements.In this study, two different user interfaces, a simple computer game and an efficient environment for typing, were developed. They operate based on the commands produced by the eye movements defined and extracted in the previous stages.A simple computer game that works based on the user׳s eye movements as defined in Section 2.2 was developed. This game is considered a tool to evaluate if people with movement disabilities can work with the proposed system by performing eye movements as required by the system. Moreover, it can be used by them to improve their performance in working with the system.In this game, a soccer player with a ball appears in a green field. The user is supposed to kick the ball into the goal area by moving his/her eyes in the appropriate direction. In the first stage of the game, the soccer player is on the right side and the goal area is on the left (Fig. 6(a)). Therefore, the user should move his/her eyes to the left side and return to the center. By this action, the ball crosses the goal line, and the user is encouraged by a clapping sound effect. In this stage of the game, other commands (eye movements in other directions) would have no effect. The stage will be completed if the user can successfully kick the ball five times. The next stage of the game starts by blinking twice. The game was considered in four stages, to practice eye movements towards the four main directions.The main user interface developed for this system for typing is based on eight identified eye movements plus double blinking. As shown in Fig. 6(b), in the developed keyboard, letters and numbers are divided into nine groups. The user can move a cursor on the keyboard by each eight predefined eye gestures, and select the desired group by a double blink. Then, the second page of the keyboard appears (Fig. 6(c) as an example) that contains all the letters and numbers in the selected group as well as the specific characters of DOT (.), DEL (for deleting the last typed character), SPACE, CLEAR ALL (for deleting all typed characters) and BACK (for going back to the main page of the keyboard). The selected letter is then typed on the white space at the bottom of the main page.The interface is designed in such a way that the user can type each desired letter or number by at most two eye movements, and two double-blinks, which allows for a uniform and appropriate speed of typing.To demonstrate the functionality of the proposed system, two series of experiments were performed, one by a group of able-bodied people that examined the typing environment, and the other by a group of people with disabilities that tried the game. It was not possible to test the typing environment by people with disabilities, since those that could be encouraged to participate in these experiments were either too young to use a typing environment or unfamiliar with the English alphabet. However, the signal processing part of the system is the same for typing and game environments and it is supposed that if participants with disabilities can perform well in the game environment, they also can work appropriately in the typing environment. All the participants took part in the experiments voluntarily, after getting permission from them or their parents. The procedure was described for them and an informed consent was obtained. The experiments were performed in the presence of the parents of participants with disabilities. Moreover, the experiments were ceased immediately if a participant hesitated to continue.To start working with the system, each participant was seated in front of a laptop equipped with the Bluetooth module. The participant put on the recording eyeglasses and the electrodes were mounted at the proposed places around his/her eyes (Fig. 1). The user was asked to look straight at the center of the screen (the laptop was placed in front of the face) for the first 30s during which the system automatically detects the baseline of the signals. The measured baselines were then compensated throughout the experiment. No more calibration procedure is necessary for the system. On instruction, the participant tried to work with the system by performing the eye movement protocols. When the user was able to perform each eye movement twice without any problem, the main experiment started. In both series of experiments, the performance of the system was quantitatively evaluated.Six volunteers without disabilities aged 16–50 (Mean±Std=32.2±12.5 years), three females and three males, were asked to participate in the experiment by typing five short sentences, “IT IS OK”, “BE FRESH”, “FIX MY PC”, “SAY YOUR MIND”, and “I HAVE A COLD”. The sentences which consisted of 51 characters were not specifically selected. Different eye movements are necessary to be performed to type the characters throughout these sentences. The participants had no previous experience of working with this system, except participant #6 that had previously performed experiments with the system for several times. The participants were supposed to use eight different eye movements and blinking according to the predefined protocols to type the characters of each sentence one by one. The experiment was repeated three times for participants 1–5, and 6 times (in 2 sections) for user 6. Repeating the experiments for participant #6 was to investigate how much the results may improve if a user works frequently with the system. The participants had 2 minutes to rest before repeating the experiment.During the experiments, a video of the face of the participant as well as the desktop screen was recorded, and then it was carefully examined to identify the actual eye movements performed by the user and evaluate the performance of the system in detecting these eye movements. Based on the results, true positive (TP), false negative (FN) and false positive (FP) counts were taken to calculate the precision TP/(TP+FP), sensitivity TP/(TP+FN), accuracy (TP+TN)/(TP+TN+FP+FN) and f1-score (2TP/(2TP+FN+FP)) of detecting eight eye movements and double-blink by the system as tried by the participants [38].The main goal of this study has been to develop an interface for typing for people with disabilities who are not able to speak. However, participants with disabilities in this study were not able to type in English which was not their native language, and working with the typing interface was not so easy for them due to the focus that is needed for typing, limitations they had in performing eye movements, and sometimes involuntary movements in their head. Moreover, some of the participants were too young and some suffered from depression. These factors made it very difficult to motivate them to participate in an experiment with the typing environment. Therefore the performance of the system for participants with disabilities was evaluated using simple and motivating interface of a game.Ten people with severe movement disabilities that could not speak clearly were invited to participate in the experiments. In the first session, their general condition was evaluated and they were trained to do specific eye movements necessary for working with the system. The criteria for selecting the participants to continue performing the main experiment were the ability to move their eyes and blink. Two of the participants suffered from both physical and mental disabilities, and it was concluded that their training would take too much time. One of the participants could not move his eyes vertically, and another one could not continue to participate in the experiments. Therefore, only six participants performed the main experiments. These participants were two males aged 6 and 26 and a female aged 7 with cerebral palsy. They had involuntary head movements, could hardly move their hands and legs, and could speak very unclearly, except the 26 years old participant that could not speak at all. The other participant was a female aged 8 with traumatic brain injury, who could hardly move her hands and legs and could hardly speak. The last two participants were females aged 49 and 59. The former suffered from ALS for 6 years, and the latter for 2 years, and she was also diagnosed with depression. None of them could have any movements in their hands and legs and could not speak.In the second session, the participants were encouraged to learn and work with the developed game. The main experiments were held in the third or later sessions. It was intended to perform the main experiment twice for each participant. However, the second experiment could not be performed for two of the participants due to the lack of enough motivation. The experiment was repeated for more several times for two of the participants in order to investigate the possible improvement in the performance by increased experience.A video of the face of the user and the desktop screen was recorded during the experiments. The video was then carefully evaluated to calculate the number of eye movements detected correctly by the system. The total number of eye movements performed by the user until performing a successful one and the ratio of these two values, as the rate of success for each participant, were extracted from the videos.

@&#CONCLUSIONS@&#
