@&#MAIN-TITLE@&#
A repartition method improving visual quality for PCA image coding

@&#HIGHLIGHTS@&#
This study formulated a clustering method embedded in a GA framework.Individuals of the same group are homogeneous, and vice versa.The homogeneity property is in favor of the PCA subspace projection mechanism.The repartition method effectively increases the image quality and visual effect.

@&#KEYPHRASES@&#
Principal component analysis,K-means clustering,Genetic algorithm,

@&#ABSTRACT@&#
Image coding using principal component analysis (PCA), a type of image compression technique, projects image blocks to a subspace that can preserve most of the original information. However, the blocks in the image exhibit various inhomogeneous properties, such as smooth region, texture, and edge, which give rise to difficulties in PCA image coding. This paper proposes a repartition clustering method to partition the data into groups, such that individuals of the same group are homogeneous, and vice versa. The PCA method is applied separately for each group. In the clustering method, the genetic algorithm acts as a framework consisting of three phases, including the proposed repartition clustering. Based on this mechanism, the proposed method can effectively increase image quality and provide an enhanced visual effect.

@&#INTRODUCTION@&#
Numerous data analysis techniques, such as regression and principal component analysis (PCA), possess time or space complexity and are thus impractical for large datasets [6,25]. Therefore, instead of applying such techniques directly to the entire dataset, researchers adopt cluster analysis and apply these techniques to each cluster, which consists of only a portion of the original data. Depending on the type of cluster analysis, the number of clusters, and the accuracy with which the clusters represent the data, the results can be comparable with those that would have been obtained by using all data. Cluster analysis techniques have recently been applied to microarray data, image analysis, and marketing science [13,26].Cluster analysis [11] is a core issue in data mining with innumerable applications spanning many fields. In order to mathematically identify clusters in a dataset, it is usually necessary to first define a measure of similarity or proximity which will establish a rule for assigning patterns to a particular cluster. The measure of similarity is usually data dependent. The clustering aims to optimize a cost function that is defined over all possible groupings. Moreover, the cost function depends on the manner by which the data are decomposed and has limited meaning on one separate item [20]. In this technique, the collected information is divided into various clusters to show the system behavior patterns effectively. In other words, patterns in the same group are similar in some sense and patterns in different groups are dissimilar in the same sense [4,5]. In terms of analysis of variance (ANOVA), the within-variance is low and between-variance is high. Here, “variance” means the sample variance among all possible linear combination of observations [8]. We will apply this property to the proposed method, in which PCA is employed as the data analysis technique for image coding. In this study, we adopt the K-means algorithm [10,24,30] proposed by Mac Queen (1967) to minimize the sum of the distance from each data to its cluster center. The K-means algorithm is a popular clustering method for its capability to group huge datasets efficiently.Data reduction techniques aim to efficiently represent data [14,15,28]. One example is the Karhunen–Loeve Transform (KLT), in which a higher dimensional input space is mapped to a lower dimensional feature space through linear transformation [19]. As an alternative approach to feature extraction in the n-dimensional space, PCA finds the m (m<n) basis components, such that the projection to the corresponding subspace possesses the largest variations [27]. In a similar fashion, PCA computes for the covariance matrix of input data with zero mean. After solving the eigenvalues of a covariance matrix, PCA extracts the eigenvectors corresponding to the maximum eigenvalues [7,16]. Dimension reduction is achieved by using the eigenvectors with the most significant eigenvalues, which form an orthogonal basis for a low dimension subspace. Every vector in the original space can be approximated by a corresponding to a vector in the subspace [9,22]. Dimensionality reduction is frequently used as a pre-processing step in data mining. Selecting a smaller number of features carries a significant role in applications involving hundreds or thousands of features. Besides relevant features, there might be derogatory features, indifferent features, and redundant (dependent) ones. Removal of these features not only makes the learning task easier, by reducing computational constraint but also often improves the performance of the classifier [4,5]. Such data reduction is applied to images to achieve image compression. In this work, we separately use PCA for each cluster, which consists of some specified block images, to reconstruct the original (or input) image [29].The genetic algorithm (GA) [17,18,21], originally developed by Holland over the course of the 1960s and 1970s, is a biological analogy. In the selective breeding of plants or animals, for example, offspring is produced as a combination of the parent chromosomes according to certain characteristics that are determined at the genetic level. When the fitness landscape (or cost surface) of the problem is unclear or riddled with a large number of local optima, the GA usually has good searching capability because the candidate solutions will not become stuck at the local optima [23]. The GA has been successfully applied to many fields of science and engineering [12]. In the proposed algorithm, we partition the dataset into numerous clusters, in which the numbers of principal components using PCA can vary. In this work, we use GA as a framework with three phases, namely, GA operation, repartition clustering, and clustering PCA for image coding. In repartition clustering, the clustering and the number of principal components for each cluster are determined progressively.Some GA-based clustering algorithms such as stochastic clustering algorithms based on GA, Simple GA (SGA), Hybrid Niching GA (HNGA), and multi-objective GA are mentioned in [1]. In the latter study, these methods are considered only able to find compact hyperspherical, equisized, and convex clusters like those detected by the K-means algorithm [2]. If clusters of different geometric shapes are present in the same dataset, the above methods will not be able to find all of them perfectly [3]. This paper provides a preliminary study in this direction. Here, we apply PCA to the whole dataset obtained from an image to achieve image compression. To improve the reconstructed image quality, we use K-means to partition the dataset, and then apply PCA to each cluster separately. In this method, different numbers of principal components are allowed, and GA is used to identify the optimal number of principal components for each cluster. Finally, we propose the repartition clustering method to improve the image quality and visual effect.The proposed method can improve the homogeneity in each cluster by increasing the within-group correlation corresponding to PCA image coding. Under the condition that the total numbers of variables to store are roughly the same, the proposed algorithm removes redundant variables in clusters with simple structures and increases the number of principal components to improve the reconstructed quality of certain clusters with complex structures. Experimental results show that the proposed method can effectively increase image quality and improve the visual effect.PCA is a variable reduction procedure that is useful when the data that are obtained on a number of variables (possibly a large number of variables) may have some redundancy. In this case, redundancy indicates that there could be features whose presence in the dataset does not affect the performance of a classifier at all. There could even be some correlated set of features and selection of just a few of them might be sufficient for the classifier. This redundancy facilitates the reduction of the observed variables into a smaller number of principal components (or artificial variables) that will account for most of the variance in the observed variables. The principal components may then be used as predictor or criterion variables in subsequent analyses.The first principal component of the observations is the linear combination of the original variables with the greatest sample variance among all possible linear combinations. The second principal component is defined as the linear combination of the original variables that accounts for a maximal proportion of the remaining variance subject to being uncorrelated to the first principal component. Subsequent components are defined similarly. The algebra of sample principal components is summarized briefly.The first principal component of the observations xi=(xi1, xi2, …, xin)T, yi1 for i=1, 2, …, l, is the linear combination(2.1)yi1=a11xi1+a12xi2+⋯+a1nxinthe sample variance of which is the greatest among all such linear combinations. The variance of yi1 can be increased without limit simply by increasing the coefficientsa1T=(a11,a12,…,a1n), such that a restriction must be applied to these coefficients. A sensible constraint is to require the sum of squares of the coefficients,a1Ta1, to take the value of 1, although other constraints are possible. The second principal componentyi2=a2Txiwithxi=(xi1,xi2,…,xin)Tis the linear combination with the greatest variance subject to the two conditionsa2Ta2=1anda2Ta1=0. The second condition ensures that y1 and y2 are uncorrelated. Similarly, the jth principal component is the linear combinationyij=ajTxithat has the greatest variance subject to the conditionsajTaj=1andajTak=0for k<j.To identify the coefficients that define the first principal component, we need to choose the elements of the vector a1 to maximize the variance of y1 subject to the constrainta1Ta1=1. To maximize a function of several variables subject to one or more constraints, the method of Lagrange multipliers is used. This case leads to a solution in which a1 is the eigenvector of the sample covariance matrix, V, corresponding to its largest eigenvalue. The other components are derived in similar fashion, with ajbeing the eigenvector of V associated with its jth largest eigenvalue. If the eigenvalues of S are λ1, λ2, …, λn, then the variance of the jth component is given by λjbecauseajTaj=1. The total variance of the n principal components will be equal the total variance of the original variables, such that(2.2)∑j=1nλj=s12+s22+⋯+sn2wheresj2is the sample variance of xij. We can write this formula more concisely as(2.3)∑j=1nλj=trace(V).Consequently, the jth principal component accounts for a proportion Pjof the total variation of the original data, where(2.4)Pj=λjtrace(V).The first m principal components, where m<n, account for a proportion(2.5)P(m)=∑j=1mλjtrace(V).Notably, the covariance matrix must be positive-definite; otherwise, some eigenvalues will be complex numbers. Moreover, when the variables are on very different scales, PCA is usually applied to the correlation matrix rather than the covariance matrix.PCA provides a simple yet efficient method image compression, which is a practical requirement for the storage, transmission, and feature extraction of digital images. PCA can be defined as the orthogonal projection of the given data onto a lower dimensional linear space, called the principal subspace, such that the variance of the projected data is maximized.For a given image, we collect all of the non-overlapping blocks of size p×p as the dataset. For convenience, we regard these blocks as one-dimensional vectors, where n=p×p, that is,S={xi}i=1l,xi∈Rn. For the dataset, we consider an m-dimensional projection subspace, where m<n. The optimal linear projection, yifor i=1, 2, …, l, is defined by the m eigenvectors, ajfor j=1, 2, …, m, computed from the covariance matrix of the dataset corresponding to the first m largest eigenvalues.Given the original data vector xi, we may use Eq. (2.1) to compute for the set of principal components as(2.6)yi=yi1yi2⋮yim=a1Ta2T⋮amTxi,m<n.The linear projection of Eq. (2.6) from Rnto Rm, that is, the mapping from the data space to the feature space, represents an encoder for the approximate representation of the data vector xi. The computation network is depicted in Fig. 1(a).We can construct a linear least square estimatexˆiof the data point xifor i=1, 2, …, l, which may be viewed as a data reconstruction procedure as follows:(2.7)xˆi=∑j=1myij⋅aj=a1a2⋯amyi1yi2⋮yim,m<nCorrespondingly, the linear projection of Eq. (2.7) from Rmto Rn, that is, the mapping from the feature space back to the data space, represents a decoder for the reconstruction of the original data vector xi, which is illustrated in Fig. 1(b). Notably, the largest eigenvalues λ1, λ2, …, λndo not enter the computations described in Eqs. (2.6) and (2.7); they merely determine the number of principal components used for encoding and decoding.The approximation error vector eiis equal to the difference between the original data vector xiand the reconstructed vectorxˆi, as shown by(2.8)ei=xi−xˆiSubstituting Eq. (2.7) into (2.8) yields(2.9)ei=xi−xˆi=∑j=1nyijaj−∑j=1myijaj=∑j=m+1nyijajPCA provides a simple and efficient method for image coding. For instance, an image block of size n=8×8 can be regarded as a 64-dimensional,vector and we assume that m=16. The PCA encoding process is the projection of a 64-dimensional vector on a 16-dimensional subspace. The resulting components are referred to as the compression code for the image block. In this model, the eigenvectors can be regarded as the codebook. Meanwhile, the linear combination of the 16 codewords with the compression code as coefficients is the reconstructed vector.In this section, we partition the dataset scanned from the original image in Section 2 into K clusters and apply PCA to each cluster separately. The block diagram of the clustering method is shown in Fig. 2. To obtain the optimal number of principal components, GA is introduced. After decoding each cluster, we can reconstruct the image by merging.Prototype-based clustering techniques facilitate one-level partition of data. The most prominent algorithm is K-means, which defines a cluster prototype in terms of a centroid, which is the mean of the group of points, and is typically applied to objects in a continuous n-dimensional space.In the clustering process, we first choose K points as the initial centroids ci, i=1, 2, …, K, where K is the number of clusters desired. Each member x in the dataset is then assigned to the cluster Cqif this member is the closest to the centroid cq. After partitioning, the new centroid of each cluster is updated according to its cluster members. We repeat the process until some stopping criterion is met. The steps of K-means clustering are illustrated in Table 1.To assign a point to the closet centroid, we need a proximity measure that quantifies the term “closest” for the specific data under consideration. Euclidean distance (L2-norm) is often used for data points.The clustering mechanism minimizes an objective function that depends on the proximities between points, that is, to minimize the squared distance of each point from its cluster centroid. We show how the centroid for the K-means algorithm can be mathematically derived when the proximity function is the Euclidean distance with the objective of minimizing the sum of squared errors (SSE). We define(3.1)E=∑i=1K∑x∈Ci(ci−x)2where Ciis the ith cluster, and ciis the centroid of the ith cluster. We can solve for the kth centroid ck, which minimizes Eq. (3.1) by differentiating the SSE and setting it to zero, with the solving as follows:∂∂ckE=∂∂ck∑i=1K∑x∈Ci(ci−x)2=∑i=1K∑x∈Ci∂∂ck(ci−x)2=∑x∈Ck2⋅(ck−x)=0Thus,∑x∈Ck2⋅(ck−x)⇒ck=1lk∑x∈Ckxkwhere lkis the number of points in the kth cluster. Therefore, the best centroid for minimizing the SSE of a cluster is the centroid of the points in the cluster.For some combinations of proximity functions and types of centroids, K-means can converge to a solution, that is, K-means reaches a state in which no points shift from one cluster to another, such that the centroids remain the same. We may set the stopping criterion as when convergence is reached. Other stopping criteria, such as partial convergence or the maximum number of iterations, can also be specified.The GA is a biologically motivated optimization technique mimicking natural selection and natural genetic operations. GA is a general population-based search method in which each individual is referred to as a chromosome.We assume that we have a discrete search space W and a function(3.2)f:W→R.For maximization problem, the general purpose is to identify w*, which maximizes the fitness function or objective function f(w), that is,(3.3)w*=argmaxw∈Wf(w).In this work, w is the vector of decision variables. Such problem is commonly called a discrete or combinatorial optimization problem. The GA starts with a population of possible candidate solutions to the problem at hand. Initial population is randomly generated in general.Selection is a process of choosing parents and putting them into the mating pool for reproduction. A prescribed fitness function is defined for the chromosomes, and highly fit chromosomes are selected for reproduction. Commonly used selection schemes include fitness-proportionate selection, stochastic sampling, random selection, Boltzmann selection, tournament selection, etc.Crossover is a reproduction operator that forms a new chromosome from two parent chromosomes by combing part of the information from each, controlled by a parameter called crossover probability, with the hope to generate better offspring. Commonly used crossover schemes include single-point crossover, two-point crossover, uniform crossover, and parameterized uniform crossover.Mutation is a reproduction operator that randomly alters the values of genes in a chromosome, controlled by a parameter called mutation probability, with the hope to escape from the local optima of the fitness landscape. Commonly used mutation schemes include point mutation, uniform mutation, and parameterized uniform mutation.After the genetic operations of crossover and mutation, a new generation of candidate solutions is formed. The algorithm may be terminated when an acceptable solution has been obtained, or when the pre-specified maximum number of iterations has been reached, or there is no significant improvement of fitness values over prescribed runs. The process of GA is illustrated in Fig. 3.For a given datasetS={xi}i=1l,xi∈Rn, the proposed approach identifies the partition of the l individuals into K groups, which minimizes the within-group mean square error (MSE) under some pre-specified total number of variables to record for the original image. In this work, the within-group MSE is defined as(3.4)1n⋅l∑i=1l||xi−xˆi||2wherexˆiis the prediction of xi. This MSE is the squared distance between the original data point xiand its predictionxˆi.The desired number of principal components for each cluster may be obtained through the GA prescribed in Section 3. Based on the predefined number of variables to record, the chromosomes are K−1 dimensional vectors, in which every gene (entry) denotes the number of principal components for each cluster. The cost function is then defined as the MSE of Eq. (3.4) for chromosomes. The GA process is repeated until some criterion is met, and an optimal solution,M=m1,m2,…,mK−1T∈ZK−1, would be found. The number of principal components of the Kth cluster, mK, should be calculated for the given storages.Finally, we use these components to encode and decode the block images for each cluster and merge all predicted block images to reconstruct the input image.The proposed method imposes a repartition mechanism to the PCA clustering method. For a given datasetS={xi}i=1l,xi∈Rn, the approach is to partition S into K groups by minimizing the within-group MSE in Eq. (3.4) under some pre-specified number of variables to record.Our goal is to approximate the data point using a representation involving a restricted number m, the number of principal components, with m<n of variables corresponding to a projection onto a lower dimensional subspace. The m-dimensional linear subspace can be assumed as the first m orthonormal basis vectors, u1, u2, …, un. We thus approximate each data pointxi∈Rn,i=1,2,…,lkin the kth cluster by(4.1)xˆi=∑j=1mαijuj+∑j=m+1nβjujwhere lkis the number of points in the kth cluster for k=1, 2, …, K, such thatl1+l2+…+lK=l. The coefficients αijdepend on the particular data point, whereas the coefficients βjare the same for all data points. We are free to choose the basis ujand the coefficients αijand βjto minimize the distortion introduced by the reduction in dimensionality. For the distortion measure, we will use the squared distance between the original data point xiand its predictionxˆi, averaged over the dataset. That is, our goal is to minimize(4.2)Jk=1lk∑i=1lk||xi−xˆi||2when the total number of variables to record are given.First, we substitutexˆiin Eq. (4.1) into Eq. (4.2). We set the derivative with respect to αijand βjto zero and obtainαij=xiTujfor j=1, 2, …, m andβj=x¯Tujfor j=m+1, …, n. By substituting αijand βjback to Eq. (4.2), we obtain(4.3)Jk=1lk∑i=1lk∑j=m+1n(xiTuj−x¯Tuj)2=∑j=m+1nujTSkuj.where Skis the covariance matrix of the data in the kth cluster. Using the Lagrange method, we obtain the general solution that minimizes Jkfor arbitrary n with m<n. In this case, we need to choose ujas the eigenvector of the covariance matrix given by(4.4)Skuj=λkjujwhere λkjis the corresponding eigenvalue. The corresponding value of the distortion measure is then given by(4.5)Jk=∑j=m+1nλkj.According to the above derivation, the minimum value of Jkcan be obtained when we select the eigenvectors corresponding to the n−m smallest eigenvalues. It is equivalent to selecting the eigenvectors defining the principal subspace corresponding to the m largest eigenvalues. Such minimization process matches the essence of PCA. We therefore employ the PCA process introduced in Section 2 for each cluster.In the clustering analysis, the partition of l individuals into K groups is an NP-hard problem. To reduce the difficulty in examining every possible partition, we use a repartition method to partition the original data progressively. Three phases are embedded in a GA evolution framework.Phase 1: GA operationsPhase 2: repartition clusteringPhase 3: clustering PCA image codingBased on the output of Phase 3, we can calculate the reconstructed image blocksxˆifrom the original xi. According to the cost function defined in Eq. (3.4), the GA process is repeated until a stopping criterion is met. The block diagram is illustrated in Fig. 4. LetS={xi}i=1l,xi∈Rnbe the dataset of l points of dimension n. We assume that the dataset S should be partitioned into K clusters, denoted as C=(C1, C2, …, CK).In Phase 1, we will calculate the error thresholdsE=(ε1,ε2,…,εK−1)∈RK−1for the partition. The determination of E is truly heuristic. In this work, we use GA to obtain an optimal solution. The chromosomes, that is, the candidate solutions, are of the form[ε1ε2…εK−1]T, in which each gene ɛjis encoded as a real number. We first need to specify the population size, as well as the GA parameters, and initialize the chromosomes. The GA operations essentially include selection, crossover, and mutation. The output of this phase is the best chromosome, which denotes the error thresholds for each clusterE=(ε1,ε2,…,εK−1)∈RK−1. We only require K−1 error thresholds because the remaining data will be assigned to cluster CK.Repartition clustering is performed in Phase 2 according to the error threshold E and a pre-specified minimal size T. In the first stage, we perform PCA on the whole dataset. We choose the first principal eigenvector to compute the coding error for every member. The pointx∈Sis assigned to the first cluster C1 if(4.6)||x−xˆ||22n≤ε1wherexˆis the decoded result of x. We then verify whether the size of C1, #(C1), is greater than the minimal size T where the notation # represents the cardinal number, i.e., the number of members of the set. If not, we need to add one more subsequent principal component. After calculating the coding errors and reassigning the members, the size of C1 will be increased. The procedure will be repeated until the minimal size is reached. The resulting number of principal components m1 used should be recorded for later reference.When the cluster C1 is obtained in the first stage, we will continue to perform the same procedure on the remaining data to obtain C2. We assume that Cj−1 has been obtained in the j−1 stage, j≤K−1. We will now work on the remaining membersx∉C1∪C2∪…∪Cj−1. The point x is assigned to the jth cluster Cjif(4.7)||x−xˆ||22n≤εj.Moreover, the number of principal components mjshould be calculated and recorded.Finally, after K−1 stages are completed, and C1, C2, …, CK−1 are obtained, the remaining data will be assigned to the cluster CK. In this stage, the number of principal components will not be determined in the same way as before. The number is assigned directly according to the constraint of the compression ratio, that is, the total number of variables to be recorded. The outputs of Phase 2 are the partition C=(C1, C2, …, CK) and the numbers of principal components of the clusters M=(m1, m2, …, mK).In Phase 3, we perform PCA image coding for each cluster Cj, j=1, 2, …, K separately, as presented in Section 3. The corresponding number of principal components for each cluster is specified in M=(m1, m2, …, mK). Each image block xi,i∈{1,2,…,l}is encoded and decoded, as denoted byxˆi, using the corresponding principal components in the cluster. The final MSE, that is, the GA cost value, of the whole image is then calculated from Eq. (3.4), which is the key to verifying the stopping criterion.The main steps of the proposed method are listed as follows:1.Initialize the original dataset S(0)=S, j=1.Perform GA crossover and mutation. The output is the error thresholdsE=(ε1,ε2,…,εK−1)where K=4 is the desired number of clusters.Compute all of the eigenvalues and eigenvectorsΩ(j−1)={(λi,ui):i=1,…,64}of the covariance matrix corresponding the dataset S(j−1).Set the number of principal eigenvectors t=0.(i)t←t+1Compute the coding error||x−xˆ||22for every member in S(j−1) using t principal eigenvectors.For all of the pointsx∈S(j−1), assign x to the jth cluster Cjif it satisfies Eq. (4.7),(||x−xˆ||22/n)≤εj.If the size of Cjis less than the pre-specified minimal size T, go to (i).Update the number of principal components mj=tUpdate the datasetS(j)=S−(C1∪C2∪…∪Cj).Set j←j+1. If j<K, go to step 3.SetCK=S(K−1)and compute mKaccording to the total number of recorded variables in Eq. (5.1) and output C=(C1, C2, …, CK) and M=(m1, m2, …, mK).Perform PCA separately for each cluster and calculate the MSE from Eq. (3.4).If GA convergence is not reached, go to step 2.The GA framework does not, of course, guarantee the identification of the global optimal solution. The solution depends on the choice of initial error thresholds. Therefore, several trials are necessary to identify a good solution.

@&#CONCLUSIONS@&#
This study formulated a clustering method embedded in a GA framework to improve the performance of clustering PCA image coding. For cluster analysis, we proposed a repartition clustering algorithm that partitions the image blocks into groups, such that individuals of the same group are homogeneous, and vice versa. Furthermore, the homogeneity property in a group is in favor of the PCA subspace projection mechanism in terms of preserving most of the information. Thus, the proposed method can effectively increase the image quality and improve the visual effect.