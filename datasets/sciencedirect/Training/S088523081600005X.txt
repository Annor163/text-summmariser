@&#MAIN-TITLE@&#
Integrated concept blending with vector space models

@&#HIGHLIGHTS@&#
A method for merging an arbitrary number of nouns, mixing their meanings.Successful model using vector representation, scantily explored for concept retrieval.Experiments with 3 semantic space models: WN, a thesaurus, and a topic based model.Good performance with an automatically obtained resource comparable to a manual one.Evaluation by qualified reviewers and comparison with a traditional dictionary.

@&#KEYPHRASES@&#
Computational linguistics,Natural language processing,Lexicography,Vector space models,Reverse lookup dictionaries,Concept-blending,

@&#ABSTRACT@&#
Traditional concept retrieval is based on usual word definition dictionaries with simple performance: they just map words to their definitions. This approach is mostly helpful for readers and language students, but writers sometimes need to find a word that encompasses a set of ideas that they have in mind. For this task, inverse dictionaries are ready to help; however, in some cases a sought word does not correspond to a single definition but to a composite meaning of several concepts. A language producer then tends to require a concept search that starts with a group of words or a series of related terms, looking for a target word. This paper aims to assist on this task by presenting a new approach for concept blending through the development of a search-by-concept method based on vector space representation using semantic analysis and statistical natural language processing techniques. Words are represented as numeric vectors based on different semantic similarity measures and probabilistic measures; the semantic properties of a word are captured in the vector elements determined by a given linguistic context. Three different sources are used as context for word vector construction: WordNet, a distributional thesaurus, and the Latent Dirichlet Allocation algorithm; each source is used for building a different semantic vector space.The concept-blender input is then conformed by a set of n-nouns. All input members are read and substituted by their corresponding vectors. Then, a semantic space analysis including a filtering and ranking process is carried out to deploy a list of target words. A test set of 50 concepts was created in order to evaluate the system's performance. A group of 30 evaluators found our integrated concept blending model to provide better results for finding an adequate word for the provided set of concepts.

@&#INTRODUCTION@&#
Generally, traditional dictionaries are designed with readers in mind. In this scenario, a query is based on looking up single words in order to get their corresponding meanings; this is not always helpful when viewed from language producer's perspective. For them –speakers, writers, etc.– necessities are different: they may have an idea composed by several meanings or concepts, and their goal is to find the best word that is able to represent their thoughts. Asmore dictionaries in electronic format are available, searching by concept is readily at hand; but, as we will detail in Section 2, current implementations have limitations due to the presence of syntactic problems, query expansion issues, and gloss reliance. The implementation of vector-space word representations enables circumventing most of these well-known problems through their capacity to capture syntactic and semantic regularities in language (Mikolov et al., 2013). Also, working with continuous space models permits a distributed representation with good levels of generalization; moreover, semantic vector spaces have the characteristic that similar words tend to have similar vectors.This paper presents a new approach for concept retrieval, based on vector space representation constructed with semantic similarity measures and statistical NLP techniques. A sought concept is expressed as an input of n nouns. We propose representing each noun as a numeric vector in order to allocate them in a so-called semantic space of m dimensions; these dimensions correspond to a predefined set of concepts, words or topics. Then, the value of each element in the noun's vector is given by its relation (similarity measure, or probabilistic distribution with regard to a topic) with those reference concepts, words or topics. Given the input set of points represented in the m-dimensional vector space, we find an equidistant new point, from which a sample of the nearest neighbors is taken. The words included in this sample should semantically mix the characteristics described by the original entries, representing the target words from the reverse lookup process of our concept blending method.The semantic space is created from three different sources in order to evaluate different approaches: a supervised approach assisted by WordNet, and two unsupervised approaches using (1) a distributional thesaurus, and (2) the Latent Dirichlet Allocation (LDA) algorithm.Once the concept blending method was ready, a test set was created to attest the performance of our proposal, and an evaluation procedure determined which one of the proposed models used for the semantic space creation was closer to human associative reasoning. Results were compared with an existing search-by-concept dictionary (OneLook), showing that our results were preferred, in the majority of cases, by master class evaluators. Additionally, we found a greater semantic association for the words provided by our method.In Section 2 we present similar works in the state of the art; in Section 3 we present our method. In Section 4 our experiments and results can be found, and finally, our conclusions are drawn.This work uses the representation of words as a vector. This is not a new idea, perhaps the earliest works on this were those of Hinton et al. (1986) and Hodgson (1991). In Section 2.1 we give a brief survey on this subject, while in Section 2.2 we present works related to concept retrieval.Distributional analysis is based in structuralist linguistics (Harris, 1951), corpus linguistics (Firth, 1968), psychology (Miller and Charles, 1991), and it is based on the idea that not only the semantic properties of a lexical item are fully reflected in appropriate aspects of the relations it contracts with actual and potential contexts, but “there are good reasons for a principled limitation to linguistic contexts” Cruse (1986). Distributional hypothesis suggests that we can induce aspects of the meaning of words from their contexts; it is a “theory of meaning” that can be easily operationalized into a procedure to extract “meaning” from text corpora on a large scale.Models that represent the meaning of words as vectors keeping track of the words’ distributional history focus on the notion of semantic similarity, measured with geometrical methods in the space inhabited by the distributional vectors. Examples of these models are LSA (Landauer and Dumais, 1997), HAL (Lund and Burgess, 1996), the work of Sahlgren (2006), Padó and Lapata (2007), and Baroni and Lenci (2010). We follow the principle of distributional semantics as models of word meaning following Landauer and Dumais (1997), and Turney et al. (2010).Distributional semantics can model human similarity judgments, lexical priming (hospital primes doctor) synonymy (zenith-pinnacle), analogy (mason is to stone like carpenter is to wood), relation classification (exam-anxiety:CAUSE-EFFECT), etc. One of the earliest works is Hodgson's (1991). He found similar amounts of priming for different semantic relations between primes and targets (approx. 23 pairs per relation), for example: synonyms: to dread/to fear, antonyms: short/tall, coordinates: train/truck, super- and subordinate pairs: container/bottle, free association pairs: dove/peace, phrasal associates: vacant/building.Distributional semantics in complex NLP systems include information retrieval within the broad topic of “semantic search”. Elsewhere, general-purpose distributional semantics models are not too common or too effective; lack of reliable, well-known out-of-the-box resources comparable to WordNet “Similarity” is too vague a notion for well-defined semantic needs. However, there are some successful attempts to use general-purpose distributional semantic information at least as supplementary resource in various domains, e.g. question answering (Tomás and Vicedo, 2007), bridging coreference resolution (Poesio et al., 2004; Versley, 2007), language modeling for speech recognition (Bellegarda, 1998), and textual entailment (Kotlerman et al., 2010).Distributional semantics in the humanities, social sciences and cultural studies are a great potential, only partially explored, e.g., Sagi et al. (2009) use distributional semantics to study semantic broadening (dog from specific breed to “generic canine”) and narrowing (deer from “animal” to “deer”) in the history of English phonastemes (glance and gleam, growl and howl) and the parallel evolution of British and American literature over two centuries.To our knowledge, the use of distributional semantics as a means to obtain a blended concept representation to aid language producers has not been particularly explored.We could not find any works directly comparable with our concept-retrieval method because traditional tools return a concept based on a description, while our method merges meanings of several words in order to find a single one that represents their blending. Nevertheless, it is relevant to mention works related to the task of returning words given a search for a concept. In general, we find such tools under the name of reverse-lookup dictionaries.As far as we know, there are only few printed reverse dictionaries for the English language. The reason for this is probably the complexity of their elaboration, especially the one of choosing the proper way to structure their information. The Bernstein's Reverse Dictionary (Bernstein and Wagner, 1975) was the first of its kind; in this book, the definitions of 13,390 words were reduced to their briefest form and then ordered alphabetically. In order to cover all routes to find a target word, some words have multiple references as the author re-orders the word sequence of the definitions in every possible form. However, the briefness in the definitions could be seen as a limitation, because this forced reduction may lead to information loss; albeit this is necessary because longer definitions might complicate the compiling work task.Regarding electronic dictionaries, other complications arise; information's different orderings, organization of concept hierarchies, or varying entry structures attempts have been made in the implementation of the reverse lookup method seeking for the best performance. The first attempts on electronic reverse dictionary creation were based on Boolean operators, i.e., exact match systems. Such works receive a definition or a list of words to begin the reverse search; target words containing the exact form of the input were displayed as output; however, this scenario had few possibilities of occurrence, showing its limited performance. The Merriam Webster's Collegiate Dictionary in its first electronic versions included a reverse search option based on this procedure.Another approach for reverse search was done in 1995 with the United States patent of Crawford et al. titled “Reverse electronic dictionary using synonyms to expand search capabilities” (Crawford et al., 1997). In this work, synonyms were used to expand search capabilities.Reverse lookup dictionaries for languages other than English have been also constructed. For example, Bilac et al. (2004) for Japanese compares the input phrase and the definitions from a concept dictionary. Before doing the lookup process, they parse all dictionary definitions with a morphological analyzer in order to generate frequency files which reflect the term frequencies in each definition. The output consists of those words whose definitions have the highest similarity with the user input. There is also an attempt to expand dictionary definitions of a concept by adding definitions of its hypernyms. This is possible due to the characteristics of the concept dictionary used. And, following their basic principle, a direct checking for a match within the user input and the concept definition is done as another output option. All of these measures were used as part of the reverse search process but each one evaluated separately.A different reverse lookup method was created in Dutoit and Nugues (2002). In this proposal, a lexical database of French words called ‘The Integral Dictionary’ (TID) acts as the main source for the reverse search operation. TID is a semantic network associated to a lexicon with a size comparable to WordNet (Miller, 1995), one of the most important lexical databases for English. The Integral Dictionary organizes words into a diversity of concepts, classified into categories being only used by this reverse search algorithm: classes and themes. Classes form a hierarchy and are annotated with their part-of-speech, and themes are concepts that can predicate the classes. As a graph of concepts, ontological concepts are the basic components of TID and each concept is annotated by a gloss of few words describing its content. TID also includes the implementation of different semantic lexical functions which allows the generation of word senses from another word sense given as an input.Another proposal for reverse search tries to emulate the behavior of human mind (Zock and Bilac, 2004) assuming that knowing a word does not imply that a person is able to access it in time, regardless of having it stored in memory. People use various methods to start a search process in their mind; it could be words, concepts, partial descriptions, related terms, etc. Based on the notion of association that considers that every idea, concept or word is connected, people should have a highly connected conceptual-lexical network in their minds. As a result, any word or concept has the potential to evoke each other. A few years later, an improved system was detailed in Zock and Schwab (2008). Again, the main concern was finding a correct manner to index the dictionary in order to gain a quick and intuitive access to words. This system allows lexical access based on underspecified input through the creation of a corpus-based association matrix, which is in turn composed of target words and access keys. In detail, the association matrix consists of a lexical matrix with one axis containing all the words of the language representing the target wordstw, and the other axis containing the access wordsawrepresenting the words or concepts capable and likely to evoke the target words.The most recent reverse dictionary system we have knowledge of can be found in Shaw et al. (2013). It is called the Wordster Reverse Dictionary and was built with two considerations in mind: (a) the user input is unlikely to exactly match the definition of a dictionary word; and (b) the response time of an input query needs to be minimum in order to create an application capable of supporting on-line interaction. For them, the main challenge consists in solving a concept similarity problem in a model with concepts as single words.We find in these aforementioned works that one of the common aspects is the usage of an electronic dictionary to build their databases upon, and the capacity of query expansion including different conceptually related terms (synonyms, antonyms, hypernyms and hyponyms) (Crawford et al., 1997; Bilac et al., 2004; Dutoit and Nugues, 2002; Zock and Schwab, 2008; Shaw et al., 2013). However, the reverse search done by Crawford et al. (1997), Bilac et al. (2004), Zock and Schwab (2008) and Shaw et al. (2013) at some point of their procedures perform a comparison between user's input phrase and dictionary's definition target words, looking for an exact matching, while Dutoit and Nugues (2002) based their reverse search on the highest similarity values measuring graph distances.A detailed description of the methodological features contained on the systems mentioned above are shown in Table 1, adding a column for the concept blending method (CBM) proposed in this work in order to show an initial review of some of its characteristics.As shown in Table 1, great progress has been accomplished over the last years by following common methodologies (e.g., query expansion, use of glosses for concept retrieval), but other methods, such as vector representation havebeen scantily explored. In addition, our goal is not just to effect a reverse lookup, but blending several words into a single concept. We present details of our proposal in the next section.Here we present a description of our proposed method. Roughly it consists in two stages: first, the creation of the semantic space with three different sources and then, the second stage consisting in the concept blending that is carried out in the previously created semantic space. We explain stage 1 in Section 3.1, and stage 2 in Section 3.2.Semantic spaces are very important for successful concept blending in the proposed method. The vector-space word representation presents several advantages, such as the ability of capturing certain generalization for the input words due to their distributed representation; its capacity to capture semantic regularities in language, and the proximity in space given to similar words: Once words are represented in a vector space, it is possible to compare two words by measuring their proximity in this space. If this is fulfilled, then, in order to blend the meanings of several words, we would need only to find an equidistant point to each involved word in a vector space.A semantic space is a way of representing words as vectors in an Euclidean space with axes determined by a given linguistic context. Three different sources were proposed for semantic space construction, having different linguistic contexts for word vectors. In this section we describe how vectors are constructed for words, based on the following sources:•WordNet – a large lexical database of English.Distributional thesaurus – a thesaurus generated automatically from a corpus by finding words occurring in similar contexts to each other.Latent Dirichlet Allocation – a topic modeling algorithm.With these three sources, the dimensionality of each semantic space can be fixed. In the following sections we will use semantic spaces of 25 dimensions.Under the semantic space based on WordNet lies a semantic analysis of words using semantic similarity and relatedness measures to represent their vectors. When using this kind of measures on an ontology-structured resource such as WordNet, it is possible to calculate the semantic similarity or semantic relatedness only between words that belong to the same part of speech. Due to this part of speech restriction, only nouns are considered as word members of the semantic space; this is supported by the fact that concepts are expressed mostly as noun phrases (Sowa, 1984).WordNet 3.0 includes 82,115 synsets where 117,798 nouns are distributed. WordNet describes a specific sense of a certain word as word#pos#sense where pos refers to the part of speech of the word and its sense is represented by a number.Based on WordNet's hierarchical principle, its 25 top concepts were defined as semantic primes to represent the linguistic context that determines the dimensionality of the space. The top concepts with their specific senses are listed below. This is also the order given to the top concepts during vector representation of words mentioned further on.activity#n#1animal#n#1artifact#n#1attribute#n#2body#n#1cognition#n#1communication#n#2event#n#1feeling#n#1food#n#1group#n#1location#n#1motive#n#1natural_object#n#1natural_phenomenon#n#1human_being#n#1plant#n#2possession#n#2process#n#6quantity#n#1relation#n#1shape#n#2state#n#1substance#n#1time#n#5It is important to note that in this case the input to the algorithm should be given as senses, instead of words. In this case the user is presented with a list of senses for each word so that senses are manually disambiguated. For every WordNet noun, its vector was created by calculating the semantic similarity or relatedness value between the respective sense and each top concept. Three measures were considered: JCN, Lin and Lesk Patwardhan et al. (2003). The first two are similarity measures while the latter one is a relatedness measure. After reading and creating the vectors for every noun, the process ends. Details of this algorithm are given as Algorithm 1.The process was repeated for each of the different measures mentioned above, resulting in a semantic space with JCN measured vectors, another with Lin measured vectors, and the last one with Lesk measured vectors. With the semantic spaces created, we normalized by column making the greatest value found equal to 1 and calculating the linear correspondence to the rest of values. For the Lin measure normalization was not necessary because the maximum value using the Lin semantic similarity measure is already 1.Finally, word vectors having the following form were obtained (for the sake of simplicity, only the first 10 out of 25 values are shown):Having vectors in this form, the WordNet semantic space construction was completed and word vectors were ready to conduct the concept blending.We used the publicly available distributional thesaurus published by Lin (1998). This resource lists the pairwise similarity between 5469 nouns and the top-200 most similar words for each one.The semantic space construction based on this resource is different in comparison with that of WordNet: Two variants for vector representation of words were proposed, one that imitates the fixed nature of the top-concepts used for WordNet semantic space, and other that uses a dynamic distribution of topics. Regardless of the vector representation variant, the initial step in the semantic space construction is the vocabulary extraction from the pairwise similarity values database.Regarding the first variant of vector representation, each word of the vocabulary was represented as a vector of 25 dimensions determined by words corresponding to the 25 top concepts in WordNet's graph (Miller, 1990). These topics were selected due to their character of semantic primes representing the most generic concepts and unique beginners of different hierarchies, aiming for an equilibrated distribution of vector's dimensions. However, in this case, the specific sense of each topic could not be considered due to the distributional thesaurus nature.Each vector's value consists in the semantic similarity measured between the word being analyzed and each top concept. If there is no similarity value defined in the pairwise similarity database, the dimension value is zero. A word vector containing only zeros in all its dimensions was discarded from the semantic space. The remaining word vectors constituted the Thesaurus Semantic Space I (TSSI). The algorithm describing this first variant of vector representation is shown in Algorithm 2.During the TSSI construction, a vocabulary loss (approximately 20% of the words was not found) was noticed due to the number of words with vectors full of zeros. The discarded words could have been good answers during testing, but this assumption depended totally on the input concept. In order to increase the number of words related to our input concept, we would need to generate topics dynamically. This is the second variant for the vector representation of words.For every input concept in our method, a set of topics was generated with the most related terms of each noun included in the input concept. The set of topics represents the linguistic context determining the dimensionality of the space and received the name of dynamic topics. The dimensionality of the vectors depends on the number of nouns forming the input concept. For comparison purposes, we established that the dimensionality should be 25. So, for each input concept represented by n nouns, the highest number k satisfying the expression k * n ≤ 25 was calculated. Then, k became the number of the most related terms selected for each noun member of the input concept, getting at the end a set of topics related at least with one of the members of our input and discarding from the semantic space only those words with no relation at all, yielding a negligible loss of vocabulary that does not affect the quality of the concept blending results. In order to keep all spaces with the same dimensionality, the resulting space was completed with topics from TSSI. The algorithm corresponding to the dynamic topic generation is shown in Algorithm 3.Once the dynamic topics were obtained, the TSSI construction algorithm was applied. The only difference is that top concepts were replaced by dynamic topics. Finally, word vectors were saved in the thesaurus semantic space II (TSSII).For example, suppose the following input concept: “ball field stick sport”, n = 4 ; k = 6. For each noun member of the input, their six most similar terms are extracted from the Thesaurus DB being the following:The union of all similar terms would represent the linguistic context determining the dimensionality of the semantic space subsequently constructed. In the case that the extracted terms are shared between input nouns, only one is taken into account; then, as described above, the resulting space was completed with topics from TSSI.A normalization procedure was not necessary due to the properties of the Lin's semantic similarity measure. With the semantic space construction completed, word vectors were ready to continue with the concept blending.In order to generate word vectors using LDA, it was necessary to obtain an LDA model from a corpus. The corpus selected for this task was the Wikipedia corpus snapshot as of November 2013, which includes 4,105,489 articles in English and a vocabulary of 7,423,153 words. Wikipedia is a multilingual, web-based, free-content encyclopedia that covers a wide variety of topics making it an extraordinarily large corpus with broad scope.During corpus processing we noticed that a large number of terms appeared just a few times in all articles (for example, “AAABBNNNNN” appearing once). Such terms would not give relevant information during word vector representation; so, words appearing less than 5 times in the corpus were removed. After removing stopwords and words appearing less than 5 times, the vocabulary of the corpus was reduced to 1,680,882 words.The Daichi Mochihashi LDA package Mochihashi (2004) was used to process the Wikipedia corpus with each document being a Wikipedia article. With the Wikipedia corpus ready, the remaining parameter was the number of latent topics T to be assumed in the data. An assumption of 25 topics was made following traditional selection choices Blei et al. (2003).After implementing LDA, two outputs were generated:•α – T-dimensional row vector representing the parameter of prior Dirichlet distribution over the latent topics.β – [V,T]-dimensional matrix representing the set of words for each latent topic where V is the size of the vocabulary.Analyzing LDA's output, each cell in the β matrix indicates the probability of a specific wordvifor each one of the topics automatically generated by LDA, wherevi∈V. Thus, the rows from β matrix could be seen as word vectors of 25 elements as in a traditional word context matrix. Those word vectors constituted the LDA semantic space in which the linguistic context determining the axes of the space was represented by the automatically generated unlabeled topics, since LDA is an unsupervised method.This section explains the procedure used to obtain a list of target words given an input concept. The method's input consists of a concept formed by n nouns; then the method looks for their respective vectors in the semantic space previously selected by the user and calculates the average vector resulting from the input words, giving as a result a new vector that should be located in the semantic space representing a word combining the semantics of the nouns of the input concept. However, getting an average vector located exactly over an existing word in the semantic space is highly unlikely; so, a sample of the N-nearest neighbors is taken (N may vary depending on the Euclidean distance parameter.)According to the semantic space source, different parameters were considered for the selection of the N-nearest neighbors. The parameters were empirically found as part of our experiments, to improve the output quality by taking advantage of the semantic space's properties.The following considerations were made during this stage:•For the semantic space based on WordNet we used the Euclidean distance between vectors. Then, the product of the pairwise semantic similarity values between each noun member of the input concept and the candidate words is calculated.Candidate words with a defined pairwise similarity in the distributional thesaurus DB for all nouns in the input concept have priority over other candidates.For the semantic space based on LDA, we used the Euclidean distance between the average vector and the words existing in the semantic space.

@&#CONCLUSIONS@&#
The concept blending method described in this work demonstrated a good performance after being tested with input concepts covering a wide range of subjects. Our proposal consisted of using semantic spaces for this purpose. The semantic properties of a word were captured in the vector elements determined by a given linguistic context. We experimented with three different sources for the semantic space construction: WordNet, a distributional thesaurus, and LDA. Our method showed, with two semantic spaces – WordNet and the distributional thesaurus – a better performance compared with existing implementations of reverse dictionaries used as concept blenders.Evaluator's agreement showed that evaluators did not find an straightforward task, as some of them could grade the same output as weak semantic association, while others can grade it as medium. However, the kappa agreement value of evaluators is not unusual for tasks involving semantic similarity ratings (cfPakhomov et al., 2011; Cramer, 2008). Evaluators were not previously trained on what to consider a weak or strong semantic association. Although a master qualification was required, a further refined qualification, such as linguistics knowledge was not specified.The analysis of the evaluation data revealed that semantic similarity measures performed well when used as source for word vector creation. Word vectors from WordNet semantic space were created using the JCN semantic similarity measure, while word vectors from the distributional thesaurus semantic space were created using Lin's semantic similarity measure. Also, distributed representations based on WordNet top concepts achieved good levels of generalization, capturing word's semantic features properly.On the other hand, the LDA semantic space presented poor results, mainly because of the automatic generation of topics, having low-informative topics generating zeros inside word vectors in most of the cases. Many topics related with countries, states and religion were detected. The evaluation data revealed that this linguistic context was not useful enough for representing the semantics of a word. Despite having considered a broad and relatively big corpus for creating the LDA semantic space, the WordNet vectors were denser than the vectors obtained with this model. Further experiments with the LDA semantic space should be made before generalizing that this model is not useful for this task.Another aspect to highlight was the good performance of the semantic space analysis proposed for concept blending, which in most cases seemed to have merged properly the characteristics of the words forming the input nouns. So, the main conclusion of this work is that vector space word representation gives promising results for concept blending.