@&#MAIN-TITLE@&#
Crisscross optimization algorithm and its application

@&#HIGHLIGHTS@&#
This paper introduces a new optimization algorithm called crisscross optimization algorithm (CSO).The horizontal crossover searches the offspring within a half population of hypercubes.The vertical crossover is able to accelerate the stagnant dimensions of population to jump out of local optima.The integration of the dual search mechanisms gifts the CSO algorithm with powerful global search ability.

@&#KEYPHRASES@&#
Crisscross optimization algorithm,Economic dispatch,Optimization problem,Horizontal crossover search,Vertical crossover search,

@&#ABSTRACT@&#
How to improve the global search ability without significantly impairing the convergence speed is still a big challenge for most of the meta-heuristic optimization algorithms. In this paper, a concept for the optimization of continuous nonlinear functions applying crisscross optimization algorithm is introduced. The crisscross optimization algorithm is a new search algorithm inspired by Confucian doctrine of gold mean and the crossover operation in genetic algorithm, which has distinct advantages in solution accuracy as well as convergence rate compared to other complex optimization algorithms. The procedures and related concepts of the proposed algorithm are presented. On this basis, we discuss the behavior of the main search operators such as horizontal crossover and vertical crossover. It is just because of the perfect combination of both, leading to a magical effect on improving the convergence speed and solution accuracy when addressing complex optimization problems. Twelve benchmark functions, including unimodal, multimodal, shifted and rotated functions, are used to test the feasibility and efficiency of the proposed algorithm. The experimental results show that the crisscross optimization algorithm has an excellent performance on most of the test functions, compared to other heuristic algorithms. At the end, the crisscross optimization algorithm is successfully applied to the optimization of a large-scale economic dispatch problem in electric power system. It is concluded that the crisscross optimization algorithm is not only robust in solving continuous nonlinear functions, but also suitable for addressing the complex real-world engineering optimization problems.

@&#INTRODUCTION@&#
In recent decades, many population-based stochastic optimization algorithms have been designed to address different optimization problems. These commonly used algorithms include particle swarm optimization (PSO) [16], ant colony optimization (ACO) [8], genetic algorithm (GA) [10], artificial bee colony (ABC ) [15], gravitational search algorithm (GSA) [32], intelligent water drops (IWD) [43], river formation dynamics (RFA) [33], charged system search (CSS) [18], harmony search algorithm (HSA) [9], invasive weed optimization (IWO) [23], bacterial foraging optimization algorithm (BFO) [28], group search optimizer (GSO) [12], and fruit fly optimization (FOA) [27]. Also, plenty of their variations and combinations are proposed to improve the performance in terms of convergence rate and solution accuracy [1,30,37,39,52,57,59]. These swarm intelligence based algorithms are commonly inspired by observing natural swarming behavior or physical phenomenon. They have exhibited good performance in solving many engineering real-world problems, such as filter optimization[14,40,42], PID parameter optimization [24,34,44], topology structure optimization [20,22], neural network training [11,48,49,56,58], mechanical design [6,17,45], cell formation [19,25], image compression[47], and some other optimization applications [45,46,50,51,55].Despite many efforts invested so far, the aforementioned algorithms still face some challenges and disadvantages in their utilization. For example, PSO exhibits slow convergence speed and it is liable to suffer from the premature convergence problem when addressing some multimodal problems. GSA and CSS are often time-consuming. Some algorithms like ABC and BFO need to tune several control parameters to maintain the balance between local search and global search. Others manage to achieve the preservation of population diversity at the cost of slow convergence or complicated algorithmic structures. So far, none of the heuristic search algorithms are capable of offering adequately high performance to solve all optimization problems in comparison with other alternatives [54]. Consequently, it remains a challenge to develop a population-based heuristic search algorithm that is able to manage to prevent premature convergence and meanwhile keep the fast-converging feature.Inspired by Confucian doctrine of gold mean and the crossover operation in genetic algorithm, a novel optimization algorithm called crisscross optimization algorithm (CSO) is introduced. In history, no philosopher has ever affected more individual lives than Confucius in China. One of his important thoughts holds that moderation in all things is the best of rules. Based on this middle-of-the-road concept, we realize the proposed crisscross search strategy by adopting a duo search mechanism including horizontal crossover and vertical crossover, which reproduce a population of moderation solutions at each generation by performing different crossover operations in opposite direction. In the new generation, only those moderation solutions that outperform their counterparts in the parent population can survive. All others are eliminated in the competition. Such competitive mechanism makes the crisscross search always maintain a population in the best position historically in order to accelerate the convergence speed. The innovation of this study consists in three aspects: Firstly, the horizontal crossover in CSO divides the multidimensional problem-solving space into half-population of hypercubes. Each pair of the parent individuals reproduces the offspring in the space of their own hypercube to a greater extent. To reduce the blind spot that cannot be reached, the horizontal crossover also searches the periphery of each hypercube with a smaller probability. This gifts CSO with powerful global search ability. Secondly, the motivation of introducing vertical crossover is to facilitate some stagnant dimensions of the population to escape from the dimension premature convergence. In CSO, vertical crossover is inspired by our observations that the premature convergence of most of the swarm intelligence search algorithms is caused by a few stagnant dimensions of the population. Finally, the combination of horizontal crossover and vertical crossover brings a magical effect on improving the convergence speed and solution accuracy. Once certain stagnant dimension of an individual jumps out of the local minima in the vertical crossover operation, it will spread rapidly through the whole swarm by horizontal crossover. That, in turn, facilitates the other stagnant dimensions to jump out the local minima as quickly as possible in the vertical crossover operation. It is just this crisscross search behavior resembling the chain reaction that enables CSO the competitive advantage over other heuristic algorithms in terms of global search ability and convergence speed.The performance of the CSO algorithm is verified by twelve well-known benchmark functions. At the end, the CSO algorithm is applied in a complex constrained unit commitment optimization problem with 40 thermal generating units in power system. The satisfactory results demonstrate that the proposed new algorithm is not only appropriate for the optimization of continuous nonlinear functions, but also suitable for addressing complex optimization problems in engineering and science.The rest of this paper is organized as follows: Some related concepts and procedures of the crisscross optimization algorithm are introduced in Section 2. Section 3 discusses the search behavior of horizontal crossover and vertical crossover in CSO. Section 4 uses twelve benchmark test functions to verify the performance of the proposed algorithm. In Section 5, the performance of CSO is measured by addressing a complex economic dispatch problem in 40-unit power system. The conclusion and future work are included in Section 6.The crisscross optimization algorithm (CSO) is a new population-based stochastic search algorithm consisting of horizontal crossover as well as vertical crossover, which execute in sequence within iteration. By incorporating a simple competitive mechanism, the horizontal crossover and the vertical crossover are merged perfectly. Similar to other swarm intelligence like PSO, the CSO evolutionary process is based on the population. In CSO, the population is updated twice per iteration by horizontal crossover and vertical crossover, respectively. The updated populations are then reselected twice by the competitive operation. To clarify the procedure of the crisscross optimization algorithm, several related concepts are explained as follows.For the sake of convenience, the population in the CSO algorithm is represented by matrix X. Where, each row is a potential solution represented by X(i) called individual. X(i,j) is the ith individual with jth dimension. The number of rows and columns in the matrix are the size of population M and the dimensions D in problem-solving space, respectively. The solutions generated by the horizontal crossover and the vertical crossover are called moderation solutions represented by MShcand MSvc. The solutions updated by the competitive operator are called dominant solutions represented by DShcand DSvc. The population of moderation solutions generated by horizontal crossover competes with the population of dominant solutions obtained by the competitive operation after vertical crossover. Similarly, the population of moderation solutions generated by vertical crossover competes with the population of dominant solutions obtained by the competitive operation after horizontal crossover.The procedure of crossover optimization algorithm is summarized as follows:step 1. Initialize the population.step 2. Perform horizontal crossover with the competitive operator.step 3. Perform vertical crossover with the competitive operator.step 4. Terminal Condition: If the number of iterations is larger than the predefined maximum number, the process terminates. Otherwise, go to Step 2 for a new round of iteration.The detailed explanation for steps 2 and 3 is given in the following sections.The horizontal crossover is an arithmetic crossover operated on all the dimensions between two different individuals. Suppose the ith parent individual X(i) and the jth parent individual X(j) are used to carry out the horizontal crossover operation at the dth dimension, their offspring can be reproduced through the following equation:(1)MShc(i,d)=r1·X(i,d)+(1-r1)·X(j,d)+c1·(X(i,d)-X(j,d))(2)MShc(j,d)=r2·X(j,d)+(1-r2)·X(i,d)+c2·(X(j,d)-X(i,d))where r1 and r2 are uniformly distributed random values between 0 and 1, c1 and c2 are uniformly distributed random values between −1 and 1. MShc(i,d) and MShc(j,d) are the moderation solutions that are the offspring of X(i,d) and X(j,d), respectively.According to Eqs. (1) and (2), in a multidimensional solution space, the horizontal crossover searches for the new solutions (i.e., MShc(i)) in a hypercube space that takes the two paired parent individuals (i.e. X(i) and X(j),) as its diagonal vertices with a larger probability. Meanwhile, the horizontal crossover might sample the new positions on the periphery of the hypercube with a smaller probability in order to reduce the blind spot that cannot be searched by the parent individuals. This cross-border search mechanism of the horizontal crossover distinguishes itself from the genetic algorithm. Further explanation can be referred to the search behavior of horizontal crossover in the next section.The procedure of the horizontal crossover search is given in Fig. 1. Within an iteration, the horizontal crossover search takes a population of dominant solutions (DSvc) achieved by the vertical crossover as its parent population except for the first iteration. To conduct a horizontal crossover search, it is necessary to make a pair for the individuals in matrix DSvc. This process is implemented by line 3, which performs a random permutation of the integers from 1 to M. The chosen individuals such as X(no1) and X(no2) generate their offspring (i.e. the moderation solutions: MShc(no1) and MShc(no2)) according to Eqs. (1) and (2). It is worth noting that the horizontal crossover probability P1 is generally set to 1 at most situations in order to find better solutions as many as possible. Another important parameter is the expansion coefficient c1 or c2 that has a great influence on the search scope of an individual. In this article, c1 or c2 is a uniformly distributed random value between −1 and 1. When the moderation solutions represented by MShcare generated, the next step is to perform the competitive operation between MShcand its parent population X (i.e., DSvc). Only the competition winners can survive and be stored into the matrix DShc.The vertical crossover is an arithmetic crossover operated on all the individuals between two different dimensions. Suppose the d1th and d2th dimensions of the individual i are used to carry out the vertical crossover operation, their offspring MSvc(i) can be reproduced by Eq. (3).(3)MSvc(i,d1)=r·X(i,d1)+(1-r)·X(i,d2)i∈N(1,M),d1,d2∈N(1,D)where r is a uniformly distributed random value between 0 and 1. MSvc(i,d1) is the offspring of X(i,d1) and X(i,d2) (i.e., DShc(i,d1) and DShc(i,d2)).The procedure of the vertical crossover search is given in Fig. 2. During all of the iterative process, the parent population of the vertical crossover search is the population of dominant solutions (DShc) from the horizontal crossover. Besides, there are several aspects for the vertical crossover that are apparently different from the horizontal crossover. Firstly, since each dimension of the individual solution may have different upper and lower bounds, it is necessary to undertake the normalization operation according to the upper and lower bound of each dimension to make sure that the offspring reproduced by the vertical crossover can locate within the boundary of each dimension. Secondly, the vertical crossover occurs between different dimensions of the same individual. It seems inconceivable, but it is really an efficient way to prevent the dimensions of the swarm from trapping into the local minima. Finally, each vertical crossover operation only generates a single offspring in order to provide an opportunity for the stagnant dimension to jump out of the local optima and not destroy another dimension that is probably global optimal. Consequently, the vertical crossover probability P2 is less than the horizontal crossover probability P1 (is set to 1 at most cases) based on the fact that only a few dimensions of the swarm are possibly trapped into the local minima at most situations. Numerous experiments show that it is appropriate to set P2 from 0.2 to 08. The competitive operation of the vertical crossover between MSvcand its parent population X (i.e. DShc) is similar to that of the horizontal crossover.The competitive operator provides an opportunity for the competition between the offspring population and its parent population. For example, as far as the horizontal crossover is concerned, only if its offspring individual (i.e. the moderation solutions MShc(i) outperforms its parent individual X(i) (i.e. the dominant solution DSvc(i) after the vertical crossover), can it survive and be saved in DSvc(i). Otherwise, the parent individual survives. The vertical crossover has a similar operation with regard to the competitive operator. It is the simplicity of this competitive mechanism that makes the population move rapidly to the search region with better fitness and quicken the converge rate to the global optima. The procedure of competitive operation is given in Fig. 3.Since the CSO algorithm merges both of horizontal crossover and vertical crossover, it is natural that the CSO algorithm inherits their respective advantages. From the perspective of information recognition, the vertical crossover searches the new positions based on self-recognition of the individuals, while the horizontal crossover searches the new moderation solutions based on the social recognition among individuals. The integration of these two cognitive capabilities into the CSO algorithm makes it greatly outperform either the horizontal crossover or the vertical crossover.We take one dimensional space as an exemplar to analyze the behavior of horizontal crossover. The multidimensional case can be derived similarly.The horizontal crossover searches for new positions according to Eqs. (1) and (2). We split Eq. (1) into two parts and simplify them in the form of Eqs. (4)–(6).(4)Z=X1+X2(5)X1=r1·x1+(1-r1)·x2(6)X2=c·(x2-x1)For the sake of convenience, assume x1=0 and x2=1. In this case, the probability density of X1 and X2 can be expressed by Eqs. (7) and (8):(7)fX1=1X1∈(0,1)(8)fX2=12X2∈(-1,1)Since X1 and X2 are two independent variables, the probability density of Z(9)fZ(z)=∫-∞∞fX1(x1)fX2(z-x1)dx1According to0<X1<1-1<Z-X1<1⇒0<X1<1Z-1<X1<Z+1Z Can be divided into three ranges: (−1,0), (0,1), and (1,2).When z∈(−1,0)(10)fZ(z)=∫0z+11×12dx=12x|0z+1=12(z+1)When z∈(0,1)(11)fZ(z)=∫011×12dx=12|01=12When z∈(1,2)(12)fZ(z)=∫z-111×12dx=12x|z-11=12(2-z)According to Eqs. (10)–(12), the probability density of Z is shown in Fig. 4For 2-D solution space, the probability density can be derived similarly, and it is shown in Fig. 5.It can be seen that, for 1-dimension space, the horizontal crossover samples the new positions within the range of [x1,x2] with a larger probability, but outside the range with a decreasing probability. Since the zone of the central square that takes the coordinate (x1,y1) and (x2,y2) as its diagonal vertices holds the highest value of probability density in Fig. 5, it becomes the focus of searching the new moderation solutions. The periphery of the square is sampled with a decreasing probability. For 3-dimension or high dimension space of horizontal crossover, the situation is similar. The sampling area is mainly centralized on a cube or hypercube nearby.Based on the analysis above, it is concluded that the horizontal crossover search divides the multidimensional problem-solving space into half-population of hypercubes that take the paired parent individuals (e.g. X(i) and X(j)) as their diagonal vertices assuming that the horizontal crossover probability is set to 1. Apparently, each pair of the parent individuals reproduces the offspring in the space of their own hypercube to a greater extent. To reduce the blind spot that cannot be reached, the horizontal crossover also searches the periphery of each hypercube with a smaller probability. Such search mechanism not only makes sure that the horizontal crossover focuses on sampling the better positions in different search spaces (i.e. hypercubes), but also enhances the global search ability in the solution space (see Table 2 and Figs. 7–9).As a unique feature of the CSO algorithm, the vertical crossover plays an important role in the search process. It is not only a highly efficient search method for complex functions with rotation (see f10–f12 in Table 1and Fig. 9), but also an efficient way to prevent the dimensions of the population from trapping into the local minima. As a matter of fact, an interesting finding is that the main reason that most of the swarm intelligence search algorithms (including the horizontal crossover search) converge to the local optima is because that a few dimensions of the population are possibly struck stagnant. With this in mind, the vertical crossover is implemented by exchanging the useful information between different dimensions of the same individual. Such operation makes possible for the stagnant dimensions to escape from the local minima by exchanging good information (moderation solutions achieved) with other “normal” dimensions. Once certain stagnant dimension of an individual jumps out of the local minima, it spreads rapidly through the whole swarm by the horizontal crossover operation. That, in turn, facilitates the other stagnant dimensions to jump out the local minima as quickly as possible by the vertical crossover operation. It is just because of the crisscross operation on both horizontal and vertical directions that makes the CSO have unique global search ability on addressing multimodal problems with many local minima. It is worthwhile to note that the operations on normalization and reverse normalization are needed to perform vertical crossover, considering different limit bounds for each dimension. Unlike other mutation operators in literature, the vertical crossover search for new positions by updating several whole dimensions of the parent population all at once. This search behavior is especially useful in the CSO algorithm when optimizing the complex functions such as the multimodal function f4 (Rastrigin) and the rotated function f10–f11 (i.e., Rotated High Conditioned Elliptic and Rotated Rastrigin’s) (see Table 1). When addressing such function optimization problems, the horizontal crossover search is often stuck into the local minima, but the vertical crossover is able to facilitate these stagnant dimensions to jump out of the local minimum (see Table 2, Figs. 8 and 10).The parameters’ setting is a ubiquitous problem for most stochastic optimization algorithms. In CSO, although there are two crossover operators (i.e., horizontal crossover and vertical crossover), only the vertical crossover probability needs to be set. Because CSO always manages to maintain a population of “pbest” solutions (dominant solutions) by using the competitive operator, it is natural for the horizontal crossover operator to reproduce as many potentially competitive offspring (moderation solutions) as possible. Accordingly, the horizontal crossover probability P1 is recommended to be set to 1 for all optimization problems.As for the vertical crossover probability P2, we investigated the effect of P2 on CSO by using several benchmark functions listed in Table 1, and the results are shown in Fig. 6. In the figure, the ordinate is the average error in 30 independent runs with P2 in the range [0,1]. The results show that, for the unimodal (f1–f3) or the multimodal problems (f4–f6) without coordinate shift or rotation, CSO can always converge to the global minimum point (zero) without any inaccuracy for whatever the value of P2 is set in the range [0,1]. Therefore, for the relatively simple unimodal or multimodal optimization problems, the vertical crossover probability P2 is recommended to be set to 0 in order to save half of the function evaluations (FEs). For more complex optimization problems such as functions with coordinate shift or rotation, if the vertical crossover operation is removed from CSO (i.e., P2=0), we observed that about 10–40% of dimensions in the population might simultaneously suffer from stagnancy while the rest of dimensions seems to be normal. In particular, for the shifted functions f7, f9 and the rotated functions f10, f11, we found that, in all 30 trials, the number of stagnant dimensions accounts for about 10–30% and 20–40%, respectively. In order to facilitate these stagnant dimensions to escape from the premature convergence, the vertical crossover operator in CSO provides a novel mechanism for these stagnant dimensions to mutate. As described in Section 2.2, one vertical crossover operation only reproduces a single offspring, For example, the value of P2=0.8 indicates that 80% of the dimensions participate in the vertical crossover operation, but the actual number of dimensions to be mutated occupies only 40%. Many empirical tests show that a value of P2 in the range [0.2,0.8] offers better performance when optimizing such complex problems as functions with shift or rotation. In particular, we suggest that P2 is to be set in the range [0.2,0.6]. For the rotated functions, P2 is set in the range [0.6,0.8].To exhibit CSO’s behavior and performance, two groups of experiments on 12 benchmark functions shown in Table 1[36] are conducted. The first group of experimental tests aims to exhibit the role of horizontal crossover and vertical crossover in CSO. The second one is used to validate the performance of CSO by comparing with other popular heuristic algorithm including particle swarm optimization (PSO) [16,38], quantum-behaved particle swarm optimization (QPSO) [41], and electromagnetism-like mechanism algorithm (ELM) [3].The test functions (f1–f12) adopted in this paper can be classified into four groups. f1–f3 are unimodal functions. f4–f6 are multimodal functions with many local optima, which can be used to test the global search ability of the CSO algorithm. f7–f9 and f10–f11 are shifted functions and rotated functions, respectively. They are more complicated functions and can be used to validate the comprehensive performance of the CSO algorithm.In Table 1, the parameter ξ stands for the acceptable error that is defined as the difference between the global optimum and the best value achieved by any algorithm in a trial. The trial is considered successful if and only if the error satisfies error ⩽ξ. The column “domain” defines the lower and upper bounds of the definition domain in all dimensions.In the experiments, the parameters of all the algorithms are set as follows: In CSO, The horizontal crossover probability is set to P1=1 and the vertical crossover probability is set to P2=0.8. In PSO, the inertia weight is set to ω=0.4, the acceleration coefficients are set to c1=c2=2.0 according to the classic configurations for the original PSO [36]. In QPSO, the creative coefficient beta decreases from 1.0 to 0.5 linearly in all 30 runs according to the suggestion in [31]. In all experimental tests, the number of dimensions of all the test functions is set to D=30. The population size is set to M=40. The maximum number of iterations is set to MaxIter=2000 for f1–f6, and MaxIter=5000 for f7–f12. To reduce statistical errors, each test is repeated 30 times independently. All algorithms are programmed in Matlab2010a, and the simulations are executed on a PC with a Core (TM) 2 Duo CPU P8400 running at 2.26GHz with memory capacity of 1.94GB under Windows XP Professional Operating System.

@&#CONCLUSIONS@&#
In this study, a novel optimization algorithm called crisscross optimization algorithm is introduced. CSO fuses the horizontal crossover search as well as the vertical crossover search to find the global optima in the problem-solving space. In CSO, the horizontal crossover focus on sampling the better positions in different search spaces (i.e. hypercubes and their respective peripheries), while the vertical crossover is designed to facilitate the stagnant dimensions of population to jump out of local optima. The integration of both crossover operators through the competitive operator gifts the CSO algorithm with obviously competitive advantage over other heuristic algorithms in terms of global search ability and convergence speed. Although CSO needs to evaluate the objective functions as many again as other heuristic algorithms like PSO per iteration, it exhibits a faster converging behavior. In addition, in CSO, there is only one parameter (i.e., the vertical crossover probability) that needs to be tuned. Many empirical tests show that a value of P2 in the range [0.2,0.8] offers better performance when optimizing complex problems such as functions with shift or rotation.The successful application of the CSO algorithm to addressing the benchmark functions optimization and the ED problems indicates that the CSO has great potential in solving complex optimization problems in engineering and science. Further research remains to be conducted on some concepts underlying the CSO algorithm. At present, we are preparing to apply the CSO algorithm to the optimization of more complex engineering problems with high dimensions for further evaluation of its performance.