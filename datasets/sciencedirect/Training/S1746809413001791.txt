@&#MAIN-TITLE@&#
The harmonic and noise information of the glottal pulses in speech

@&#HIGHLIGHTS@&#
Tool development for glottal excitation analysis in a speech evaluation context.Assessment of pathologic or dysphonic voices related to the glottis.Applications in the natural speech synthesis and pathological speech recognition.Harmonic and noise components can be applied in biological feedback signals.Improvement of an existent algorithm that estimates the glottal excitation.

@&#KEYPHRASES@&#
Voice quality,Voice diagnosis,Glottal inverse filtering,Glottal excitation,Harmonic and noise components,

@&#ABSTRACT@&#
This paper presents an algorithm, in the context of speech analysis and pathologic/dysphonic voices evaluation, which splits the signal of the glottal excitation into harmonic and noise components. The algorithm uses a harmonic and noise splitter and a glottal inverse filtering. The combination of these two functionalities leads to an improved estimation of the glottal excitation and its components. The results demonstrate this improvement of estimates of the glottal excitation in comparison to a known inverse filtering method (IAIF). These results comprise performance tests with synthetic voices and application to natural voices that show the waveforms of harmonic and noise components of the glottal excitation. This enhances the glottal information retrieval such as waveform patterns with physiological meaning.

@&#INTRODUCTION@&#
In producing speech sounds, humans are able to regulate the tension of laryngeal muscles in combination with the respiratory effort. These physiological settings change the time-varying flow of air through the vibrating vocal folds, i.e. the glottal volume velocity waveform. Since this signal serves as the source of (voiced) speech, it has an essential role in the production of several acoustical phenomena and cues that are used in everyday speech communication such as the regulation of vocal intensity [1–3], voice quality [4–7], and the production of different vocal emotions [8–10]. In addition, glottal pulse forms reveal physiological conditions and dynamics of the vocals folds, which might help detecting voice pathologies related to vocal fold changes [11–13]. Therefore, accurate analysis and parameterization of the glottal pulseform is beneficial in several areas of speech science including both healthy and disordered voices.It is well known that even in the case of sustained vowels produced by healthy subjects, the vibration of the vocal folds is never completely periodic. Consequently, the glottal source is typically regarded to comprise two major components; the harmonic (periodic) and the noise (aperiodic) component. Several previous studies indicate that for certain voice types, such as breathy and hoarse voices, the amount of noise is increased in the glottal flow [14,15]. The perceptual effects of the aperiodic components of the glottal flow have been studied, for example, in relation to the hoarseness [16] and the breathiness [17] of the voice. The perceptual importance of aperiodic components of the voice source is also recognized in speech synthesis where increasing efforts are currently devoted toward a better understanding of aperiodicities in the voice source [18,19]. Moreover, the aperiodic behavior of the vocal apparatus has been studied by voice pathologists who have used perceptual parameters such as hoarseness and roughness in their voice diagnosis. The importance of these perceptual parameters is reflected on the RASAT and GRBAS scales of voice quality [20] and it has been shown that hoarseness and roughness are connected to the presence of noise and acoustic aperiodicities in speech [21]. In particular, it has been found that some physiological conditions of the vocal folds mucosa are connected to specific perceptual parameters. For instance, rigidity of the mucosa is related to rough voices while the corrugation is related to hoarse voices [20].The separation of a voice signal into the harmonic and noise components, a concept named harmonic-noise splitting, has been widely studied in speech science during the past three decades. In most of the methods described in the literature, the (time-domain) signal to be processed is represented by the speech pressure waveform captured by a microphone but the processing can be also performed for the glottal flow waveform. Yegnanarayana et al. developed an algorithm based on a frequency domain approach [22]. In their method, harmonic regions of the speech pressure signal are defined by the harmonic peaks and the noise regions correspond to the inter-harmonic valleys and regions where harmonics are absent. A similar method was suggested by Jackson and Shadle [23] who used a comb filter structure to separate the harmonic and noise regions of the speech spectrum. In this method, the fundamental frequency of speech needs to be estimated prior to comb filtering in order to find the harmonic positions. Stylianou proposed a harmonic-noise splitting algorithm based on the assumption that there is a cut-off frequency that separates the speech spectrum into a low-frequency band and high-frequency band [19]. In his method, it is assumed that the low-frequency part contains mainly harmonic component information and the high-frequency contains noise information.In this paper, two techniques are combined to yield an algorithm that estimates the harmonic and noise components of the glottal pulse. These techniques take advantage of the harmonic-noise splitting which decomposes the signal into a harmonic and noise component, and the inverse filtering which removes the contribution of the vocal tract. The application of the harmonic-noise splitting technique to the signal followed by inverse filtering gives rise to better glottal pulse estimations. This new algorithm was tested with synthetic voices in order to assess the accuracy of the method, and was also tested with natural voices in order to characterize the algorithm behavior against an acoustic diversity.The main goal of the study is to develop an algorithm that splits the waveform of the estimated glottal airflow velocity into a harmonic and a noise component. The block diagram of the method is shown in Fig. 1.The algorithm consists of the following main phases. First (block 1), the speech pressure signal is divided into a harmonic and a noise component using a method that is described in detail in the following section. It is worth emphasizing that this harmonic noise splitting takes place prior to the estimation of the glottal airflow, a choice which is motivated by the fact that the estimation of the glottal airflow with inverse filtering deteriorates if the signal involves a significant amount of noise. Secondly (block 2), the obtained harmonic component of the speech signal, denoted by h(n) in Fig. 1, is used as an input to the glottal inverse filtering which yields an estimate of the vocal tract inverse filter (an FIR filter), denoted by V(z) in Fig. 1. Inverse filtering based on all-pole modeling is computed with a previously developed automatic algorithm, Iterative Adaptive Inverse Filtering (IAIF). For the detailed description of the IAIF method, the reader is referred to Alku [24] and Alku et al. [25]. Thirdly, this FIR filter is used in order to cancel the effects of the vocal tract from three signals: both from the harmonic and noise components obtained from the harmonic-noise splitter, and from the original speech pressure waveform. By further canceling the lip radiation effect using an integrator whose transfer function is simply given by L(z)=1/(1−0.99z−1), three glottal signals are finally obtained: the glottal pulse harmonic component, the glottal pulse noise component, and the glottal pulse, which are denoted in Fig. 1 by gh(n), gr(n), and g(n), respectively. The underlying model of the complete method and the principles of the harmonic-noise splitter and inverse filtering methods will be described in the following sections. Both the harmonic-noise splitting and inverse filtering are linear operations. Eqs. (1)–(4) express the resulting signals in Fig. 1.(1)s(n)=h(n)+r(n)(2)g(n)=v(n)∗ℓ(n)∗[h(n)+r(n)](3)g(n)=v(n)∗ℓ(n)∗h(n)+v(n)∗ℓ(n)∗r(n)(4)g(n)=gh(n)+gr(n)The parameters v(n) and ℓ(n) denote the impulse response of the inverse model of the vocal tract and lip radiation effect, respectively. Eq. (1) represents the harmonic-noise model, which serves as the basis for the harmonic-noise splitter. Inverse filtering is represented by Eq. (2). Finally, Eqs. (3) and (4) show that the glottal excitation consists of harmonic and noise components.The main advantage of the procedure depicted in Fig. 1 is the fact that it is very simple to be implemented once the harmonic and noise components of speech are split. However, it does not take into account non-linear phenomena in voice production.The harmonic-noise splitter used in our study is based on a model of the harmonic structure of speech, which is parameterized in frequency, magnitude and phase. The block diagram of the harmonic-noise splitter is depicted in Fig. 2.In the first stage (block no 1), the time domain input signal is transformed into the frequency domain using an Odd-Discrete Fourier Transform (ODFT) [27]. ODFT is obtained by shifting the frequency index of the Discrete Fourier Transform (DFT) by half a bin [27]:(5)Xo(k)=∑n=0N−1x(n)e−j(2π/N)(k+1/2)n,k=0,1,…,N−1where the time-domain input signal is denoted by x(n) and the frame length is N. If x(n) is real, this frequency shift makes the DFT samples above π a perfect mirror (in the complex conjugate sense) of the DFT samples below π which leads to more simple reconstruction equations. A peak picking algorithm is used to estimate the harmonics of the ODFT amplitude spectrum. This peak-peaking algorithm was based on sliding window method over the spectral magnitude where the harmonics peaks are represented. The window has an odd number of points whose middle point is tested and if the middle point is largest value in the window, then a peak is identified. Note that the sliding window moves one point at a time along the spectral magnitude in order to test all points.Next, the frequency, magnitude and phase of each harmonic are extracted (block 2) [27]. These parameters are then used to synthesize the spectrum of the harmonic structure of the input signal s(n) (block 3). The spectrum of each individual sinusoid is synthesized using the parameters extracted from that harmonic. The harmonic synthesis equations are presented by Sousa [26].Subsequently, the synthesized harmonic structure is subtracted from the signal s(n) and the result is regarded as the noise component. Finally, the spectra of both components are inverse transformed in order to get time-domain representations for the harmonic and noise components (blocks 4 and 5).An example of the harmonic-noise splitting algorithm is shown in the frequency domain in Fig. 3. The spectra are shifted vertically for visual clarity. The example demonstrates how the proposed method is able to estimate the harmonic structure of speech even in the inter-harmonic regions where the noise components prevail.The main disadvantage is that some harmonics may be missing in the spectrum of the harmonic component due to errors caused by the peak picking algorithm. These occur typically for voices whose spectra show a significant amount of high-frequency noise or weak upper harmonics.Experiments were conducted in order to assess the accuracy of the proposed algorithm and to compare its performance with a previously known technique, the IAIF method. In order to achieve this, two major experiments were designed. The first one was based on synthetic voices and will be explained in the following section. In the second experiment, a database of natural voices was used in order to illustrate how the proposed algorithm performs with real speech. All the analyses were implemented in the Matlab environment. The IAIF algorithm was included by utilizing the TKK Aparat tool [28].In order to generate synthetic speech material, a specific synthesizer was designed. The synthesizer is based on the source-filter model, but it also involves a harmonic-noise model. A block diagram of the synthesizer is shown in Fig. 4.The synthesizer uses the Liljencrants–Fant (LF) model as the parametric representation to mimic the differentiated glottal flow. The LF model is among to most prevalent synthetic voice source models and it enables modeling a large variety of glottal pulse waveforms [29]. In order to create a glottal excitation with a desired F0, a single glottal pulse is first generated (block 1) and then concatenated to obtain a waveform with a train of glottal pulses. The resulting harmonic waveform is denoted byg′h(n)in Fig. 4. In order to produce a synthetic voice with the desired HNR, signalg′h(n)is multiplied by factor α (block 2). The value of α is determined from(6)HNRdesired=∑n=1N[α⋅g′h(n)∗vr(n)]2∑n=1N[gr(n)∗vr(n)]2=α2⋅HNRcurrentwhere vr(n) is the impulse response of the filter that represents the vocal tract and lip radiation effect, N is the number of samples, HNRdesiredis the desired HNR value and HNRcurrentis the value with no adjustment (α=1). To generate the noise component of the glottal excitation, a white Gaussian noise sequence is first produced (block 3). In order to modulate the noise component, the generated noise sequence is multiplied in the time domain by the LF modeled glottal pulse. A similar approach was utilized in a study by Yegnanarayana et al. [22], who used as a multiplier a rectangular pulse centered at the instant of glottal closure instead of the LF waveform. The resulting modulated noise sequence is denoted by gr(n) in Fig. 4. In the final stage, gr(n) andg′h(n)are summed up, resulting in a glottal excitation waveform denoted by g(n). The two components are filtered (blocks 4 and 5) with the vocal tract and lip radiation filters in order to produce three outputs: the voice signal, and its harmonic and noise counterparts which are denoted, respectively, in Fig. 4 by s(n), sh(n), and sr(n).The synthesizer was used to generate a set of test vowels. The fundamental frequency F0 was varied from 100Hz up to 400Hz with an increment of 10Hz, in order to mimic both male and female speech. For each pitch, several vowel instances were generated by varying HNR from 9dB up to 21dB with an increment of 1dB. The HNR is acquired as:(7)HNR=10×log10EhErEhand Erdenote, respectively, the energy of the harmonic component, sh(n), and the noise component, sr(n), of synthetic speech. The values of the LF model were selected according to Gobl [9] in order to involve three different phonation types (breathy, normal and pressed). The vocal tract filter was adjusted to synthesize the vowel [a] (F1=664Hz, F2=1027Hz, F3=2612Hz). All the data were generated using the sampling frequency of 22.05kHz.To assess the performance of both IAIF and the proposed algorithm, the estimated waveforms were compared objectively with the LF waveform using features extracted from the flow waveforms and their derivatives. By referring to Fig. 4, this implies that glottal excitations computed from signal s(n) were compared with the ideal excitation represented by signalg′h(n). The selected voice source parameterization methods were the Normalized Amplitude Quotient (NAQ) and the difference (in dB) between the amplitudes of the first and second harmonic (DH12). The NAQ parameter is a time-based parameter that is extracted for each glottal pulse and it measures the pressedness of phonation from the ratio of the peak-to-peak flow and the negative peak amplitude of the flow derivative [30]. The DH12 parameter is a frequency domain quantity and it measures the decay of the voice source spectrum [31,32]. Both parameters are independent of time and amplitude shifts. The relative error, defined in percentage according to Eq. (8), was used for NAQ since this parameter is a time-domain quantity that is typically measured on the linear scale.(8)NAQRel=NAQo−NAQmNAQo×100In Eq. (8), NAQo is the original NAQ value computed from the LF pulse used in the sound synthesis. NAQm parameter is the measured NAQ value computed as a mean of NAQ values extracted from individual glottal pulses of the estimated glottal excitation.The absolute error, defined in dB according to Eq. (8), was used for DH12 because this parameter is typically expressed in the dB scale.(9)DH12Abs=DH12o−DH12mIn Eq. (9), DH12o is the value of DH12 obtained from the original glottal source modeled by the LF waveform. DH12m is the value of DH12 measured from the estimated glottal excitation waveform.The proposed algorithm was also tested with real speech. The database used included 39 sustained waveforms of the vowel [a] uttered by 13 subjects (7 males, 6 females) using breathy, normal and pressed phonation. The data were sampled with 22.050kHz and a resolution of 16 bits. From these signals, the most stable segments with duration of 200ms were selected for the voice source analysis.

@&#CONCLUSIONS@&#
In this article, a method to estimate the glottal excitation based on a known automatic inverse filtering method, IAIF, and a harmonic-noise splitter was proposed. The new method was compared with IAIF in the estimation of the glottal excitation using experiments with both synthetic and natural vowels.Results obtained with synthetic voices show that the proposed method improves the estimation of the glottal waveform. The harmonic component given by the new algorithm is a more accurate estimate of the glottal source because the method is able to suppress the influence of noise which is always present in natural speech, particularly in pathological voices. For voices with low noise levels, this method may not be necessary and the performance of IAIF and the proposed algorithm are similar. The behavior of both algorithms was tested as a function of the noise level and fundamental frequency. It was found that both IAIF and the proposed algorithm show good accuracy for voices with low fundamental frequency and high HNR. Drawbacks of the proposed method are due to the harmonic-noise splitter, which may pass noise to the harmonic component and itself is also sensitive to the noise level.The proposed method also enables joint estimation of the harmonic and noise components of the glottal waveform. These components may be used in the evaluation of pathological voices since the separation enables characterizing the vocal folds dynamics as a function of noise produced in the speech production process. In addition, the noise component estimated by the proposed method can be used in speech technology in order to improve the naturalness of synthetic speech.