@&#MAIN-TITLE@&#
Extended semi-supervised fuzzy learning method for nonlinear outliers via pattern discovery

@&#HIGHLIGHTS@&#
A parameterized fuzzy LDA is proposed as semi-supervised learning precursor.Apply Hopfield Neural Network to dynamic parameter estimation.Separate outlier instances from the whole feature space.Obtain the initial fuzzy classification of regular feature space.Semi-supervised fuzzy clustering algorithm is presented on the basis of class discriminatory measure for the nonlinear outliers.

@&#KEYPHRASES@&#
Fuzzy discriminant analysis,Parameter estimation,Outlier instances,Semi-supervised clustering,Image recognition,

@&#ABSTRACT@&#
This article presents an extended Parameterized Fuzzy Semi-supervised learning (PFSL) method, in which the key innovation is the capability of separating a sample set into two independent subsets: outlier sample subset and regular sample subset. In our proposed PFSL, we first develop an improved parameterized Fuzzy Linear Discriminant Analysis (F-LDA) algorithm to classify regular samples, in which the distribution information of each sample in terms of fuzzy membership degree is incorporated with the redefined within-class and between-class scatter matrices. To achieve good parameter estimation for this improved F-LDA, we advocate the use of Hopfield Neural Networks (HNN) due to its efficiency. Second, a new semi-supervised Fuzzy C-Means (S-FCM) algorithm is designed using pre-computed cluster number and cluster centers in the supervised pattern discovery stage. It is applied to classify the remaining outlier samples and generate the final classification result. Third, since Kernel Fisher Discriminant (KFD) is an efficient way to extract nonlinear discriminant features, a kernel version of the proposed PFSL (K-PFSL) is discussed. Extensive experiments on the ORL, NUST603, FERET and Yale face datasets show the effectiveness and the superiority of the proposed algorithm.

@&#INTRODUCTION@&#
Semi-supervised learning (SSL) aims at using the combination of labeled and unlabeled samples to construct a classifier. As labeled data is insufficient in many scenarios and unlabeled samples are abundant, SSL is very helpful for better exploiting the information of all these samples. From the previous studies [1,2], we conclude that the information obtained from the distribution of unlabeled data improves the potential of learning from the labeled samples of classifiers. Despite the great success of SSL methods in many applications, some issues are not properly solved. (1) The selections of the number of clusters and the initial center of each cluster are still non-trivial tasks. Many unsupervised clustering algorithms also require high computational load because they involve an extensive search process such as forward selection or backward elimination [3,4]. Therefore, these issues mentioned above are major limitations of the unsupervised learning algorithms. (2) Though SSL methods are specifically trained to identify a set of patterns, they often fail in the cases where some samples called outliers with uncommon behavior are difficult to be accurately classified. The detection of outlier samples is an unavoidable task because traditional SSL methods are dependent on the availability of the training set consisting of regular samples, which might misclassify a normal observation that falls outside the trained boundary. Therefore, the major drawback of traditional SSL methods is that the training set must represent all possible classes [5].The importance of outlier samples analysis in pattern recognition is that the useful anomalous feature information hidden in large data sets could be discovered by evaluating the correlation between each pattern to identify unusual samples. A good image feature representation method could significantly reduce the computational complexity of a model. The methods that utilize the different results of output variables to identify the best subset of given features in a dataset can be divided into supervised and unsupervised methods [6–10]. Despite extensive studies have used supervised or unsupervised models to exploit effective feature representations, few attempts have been made to identify important outlier instances by means of the SSL methods [11–13].In order to classify high-dimensional images, it is necessary to extract features that enable different feature regions to be well separated. In fact, image recognition can be either supervised or unsupervised, depending on whether the prior knowledge or the label of a training sample is available or not. In many learning problems, the difficulty most often encountered is prior knowledge analysis about pattern variations [14,15]. A supervised learning method identifies and separates regions that match feature properties previously learned from training samples. On the contrary, unsupervised feature segmentation has to distinguish the feature classes as well as to separate them into different regions. Although some attempts have been made to incorporate a supervised classifier into an unsupervised strategy [16,17], those previous studies only classify a small number of features that correspond to patterns with low-confidence obtained using an unsupervised algorithm. In fact, if supervised classifier fails in the remaining features, which is mainly due to its unsupervised attributes.In this study, we propose an extended parameterized fuzzy semi-supervised learning method that separates data into two independent regions, including outlier instances or regular samples, contained in a feature space. More specifically, firstly, an improved supervised F-LDA algorithm for regular samples is proposed. It achieves the distribution information of each sample that is represented with fuzzy membership degree, and then the membership grade is incorporated into the redefinition of scatter matrices. As a result, the initial fuzzy classification of whole regular feature space is obtained. Moreover, the need for such a novel F-LDA model construction is reduced to parameter estimation when the structure of learning model is given beforehand, that is, the parameter estimation method must recursively process the measured data as they become available. Secondly, a new semi-supervised fuzzy C-means (S-FCM) algorithm is presented on the basis of precise number of clusters and initial pattern centers that obtained in the pattern discovery stage, which are then applied to perform the outlier instances classification and to yield the final classification result. Thirdly, since the Kernel Fisher Discriminant (KFD) algorithm is an effective way to extract nonlinear discriminative information of the feature space by using the kernel trick [18–21], a kernel version of our method is presented subsequently, which has the potential to outperform the traditional learning algorithms, especially in the case of nonlinear small sample size. We compared the proposed method PFSL and the kernel version of PFSL (K-PFSL) with various face recognition methods including Fisherface [22], direct LDA (D-LDA) [23], complete PCA plus LDA (C-LDA) [24], random discriminant analysis (R-DA) [25], fuzzy linear discriminant analysis (F-LDA) [26], fuzzy local discriminant embedding (F-LDE) [27], fuzzy maximum margin criterion (F-MMC) [28], reformative fuzzy linear discriminant analysis (RF-LDA) [29], bias corrected FCM (BC-FCM) [30], K-means-based support vector machine (K-Means+SVM) [31], Mean shift-based support vector machine (Mean Shift+SVM) [32], Gaussian mixtures-based support vector machine (Gaussian Mixtures+SVM) [33] and Density-based spatial clustering of applications with noise based K-nearest neighbor classification (DBSCAN+KNN) [34], on the ORL [35], NUST603 [36], FERET [37] and Yale [38] face image databases.The motivations of this article are summarized as follows: (1) The proposed method validates the effectiveness of a supervised pattern discovery model associated with a semi-supervised manner. A semi-supervised fuzzy clustering algorithm is developed on the basis of prior information that obtained in supervised learning stage, which is then applied to perform outliers classification. (2) The objective of the proposed supervised learning step is to determine a set of suitable patterns, but this step could not accurately distinguish the samples from abnormal training regions such as outlier zones. That is the reason why we propose a new semi-supervised method of clustering to address this issue. (3) We can find that the proposed method is stable for the cases with only a small number of training samples, especially for the non-linearly separable problems, which again validates the advantage of the proposed algorithm in alleviating the nonlinear small sample size problem.This article is organized in the following manner. Section 2 presents a fuzzy supervised learning method with parameter estimation. Extended semi-supervised fuzzy clustering for outlier samples is proposed in Section 3, meanwhile, a kernel version of our method is presented in this section. Section 4 reports comprehensive classification results on several commonly used face databases, including NUST603, ORL, Yale and FERET. Finally, concluding comments are included in Section 5.In our previous work [29], we extended F-LDA [26] by including complete fuzziness in the calculations of the between-class and within-class scatter matrices. By this means, a relaxed normalized condition is presented to achieve the distribution information of each sample in terms of fuzzy membership degree.Nevertheless, how can we dynamically assign a particular value of the offset of membership grade? There is not explained in our previous work [29]. Moreover, the need for such a novel F-LDA model construction is reduced to parameter estimation when the structure is given beforehand. The parameter estimates should then be based on observations up to the current time, and therefore the parameter estimation methods must recursively process the measured data as they become available. Traditional methods include recursive least-squares and the Kalman filter for parameter estimation [39]. Newer alternative methods include HNN for parameter estimation [40–42]. Therefore, in this Section, the HNN has been further considered in the context of the problem of the proposed fuzzy LDA parameter estimation.The objective of the proposed initial step is to find out a set of suitable patterns, but this step does not accurately define all sample regions. This process is carried out by using all of training samples and separates training set that matches different sample properties (including outlier instances and regular ones). Henceforth, a pattern discovery stage that relies on an improved supervised fuzzy LDA approach is proposed for determining the original patterns of training samples, which is based on the outcome of identification of outlier instances and their counterparts.The conventional LDA methods [43–47] aim at maximizing the ratio of between-class scatter matrix to within-class scatter matrix. Given a set of nisamples belonging to class ci, we can define the mean of each class as:(1)mi=1ni∑xk∈cixkwhere i=1, 2, ..., C, C is the number of classes. The within-class scatter matrix is then defined as:(2)Sw=1N∑i=1C∑xk∈ci(xk−mi)(xk−mi)Twhere N is the total number of image samplesN=∑i=1Cni. The between-class scatter matrix is defined as:(3)Sb=1C∑i=1C(mi−m¯)(mi−m¯)Twherem¯is the grand mean, i.e. the mean of the means miof all classes. Thus, the classical Fisher's criterion function is generally defined as(4)J(φopt)=argmaxφφTSbφφTSwφwhere the solved φopt=[φ1, ..., φL] is the set of discriminant vectors of SbandSw, L is the number of selected optimal discriminant vectors.According to the basic theory of LDA, Kwak [26] proposed to incorporate a gradual level of assignment to class being regarded as a membership grade with anticipation that such discrimination helps improve classification results.Suppose we have C known classes ci, i=1, 2, ..., C, and a set of samples X=(x1, x2, ⋯xN),xN∈ℝm. We assign each sample in X to one of the known classes ci, i.e. xj∈ci, j=1, 2, ..., N, i=1, 2, ..., C. With the fuzzy K-nearest neighbor (FKNN) [48] algorithm, the fuzzy membership grades can be obtained through a sequence of steps as follows. First, the Euclidean distance matrix is computed. Second, the diagonal elements of this matrix are set as infinity. Third, we treat each of columns separately and sort the distance matrix in the ascending order. Finally, the membership grade matrix μijto class “i” for the jth pattern is generated by:(5)μij=0.51+0.49(nij/k)i,jbelong to the same class0.49(nij/k)otherwisewhere nijstands for the number of the neighbors of the jth sample that belong to the ith class [26].The results of formula (5) are used in the computations of the statistical properties of patterns such as mean and scatter covariance matrices. Taking into account the fuzzy membership grades (i.e., the membership grade to class “i” for jth pattern), the mean of each class fmiis:(6)fmi=∑j=1Nuijxj∑j=1Nuijwhere μijstands for the membership grade to class “i” for the jth pattern. Therefore, the class center matrix FM and the fuzzy membership matrix U can be achieved with the result of FKNN.(7)U=[μij],i=1,2,⋅⋅⋅,C;j=1,2,⋅⋅⋅,N(8)FM=[fmi],i=1,2,⋅⋅⋅,CThe within-class fuzzy scatter matrixFSwand between-class fuzzy scatter matrix FSbincorporate the membership values in their calculations:(9)FSw=∑i=1C(∑xj∈ci(xj−fmi)(xj−fmi)T)(10)FSb=∑i=1C(fmi−m¯)(fmi−m¯)Twherem¯stands for the mean of all the training samples. The optimal fuzzy projection ΦF-LDA and the feature vector transformed by F-LDA method follows the expressions(11)ΦF-LDA=argmaxφφTFSbφφTFSwφ=[φ1,φ2,...,φL]where{φii=1,2,...,L}is the set of generalized eigenvectors (discriminant vectors) of FSbandFSwcorresponding to the C−1 largest generalized eigenvalues, that is(12)FSbφi=λiFSwφi,i=1,2,...,LThe first key step of the proposed RF-LDA method is to address the issue coming under the influence of outliers in the patterns. The membership matrix denoted by U=[μij] satisfies:(13)∑i=1C∑j=1Nμij=1where i=1, 2, ···, C and j=1, 2, ···, N. Considering the fact that outliers may have adverse influence on F-LDA, thus a relaxed normalized condition in fuzzy membership grade is proposed as follows:(14)∑i=1C∑j=1Nμi(xj)=NWith Eq. (14), μijis redefined in our algorithm as follows:(15)μij=γ+(1−γ)⋅(nij/k)i,jbelong to the same class(1−γ)⋅(nij/k)otherwise(16)γ=N−C2ϑ⋅Nwhere ϑ is the parameter that controls the values of μijand satisfies the constraints ϑ∈(0, 1), γ∈(0, 1).In fact, the original membership grade μijshown in Eq. (5) is assigned by calculating the contribution of the k-nearest neighbors. Thus, the membership selected as an offset of 0.51 reflects an initial manual distribution characteristic of training samples located in different classes. Unfortunately, in face recognition, the underlying data distributions for different classes (especially in outlier regions) cannot be acquired in advance. Therefore, we conclude that the value of γ in assigning the new membership grades in our redefined formulas (15) and (16) would have corresponding effect on recognition accuracy. Now, the following study aims at dynamically estimating this parameter of RF-LDA by means of an efficient artificial neural networks HNN.The value of γ is the offset of the membership grade assigned to a training sample in its class. The constraint for γ shown in (16) is to satisfy the condition stated in (15). Therefore, we design an expression (16) to estimate the value of ϑ by using HNN in order to obtain the optimal solution. In this set-up, the estimated value of ϑ is adaptive in the search space bounded between 0 and 1, i.e. it satisfies the constraints ϑ∈(0, 1), γ∈(0, 1).The second key step of this method is how to incorporate the contribution of each training sample into the new scatter matrices. Specifically, the membership grade of each sample (contribution to each class) should be considered thus the corresponding fuzzy within-class scatter matrix is remodified as:(17)RFSw=∑i=1C(∑xj∈ciuijp(xj−fmi)(xj−fmi)T)and the fuzzy between-class scatter matrix is remodified as:(18)RFSb=∑i=1C1−∑xj∈ciuijp∑j=1Nuijp(fmi−m¯)(fmi−m¯)Twhere p is the parameter which controls the influence of fuzzy membership grade in the redefined within-class and between-class scatter matrices,m¯stands for the mean of all image vectors. Therefore, the proposed RF-LDA shows high stability compared with others due to its power of regulating fuzzy membership grades.Comparatively, the traditional F-LDA [26] only takes into account the mean value of each class to regain the statistical properties of scatter matrices. As an alternative, we design a solution that makes full use of the distribution information of each sample to the redefined scatter matrices. Since the normalized membership constraint in (13) has been relaxed to N in (14), the samples associated with the membership matrix U are insensitive to the center of each class. In fact, with the original normalized constraint in (13), the relatively large membership grades of different classes in terms of outliers would be achieved. For instance, in a two-class classification for outliers, the two membership grades between the outliers and each class are close to 0.5 at the same time. Furthermore, the experiments conducted on different face databases in Section 4 show the superiority of our method.Subsequently, the third key step of our method is the use of an efficient artificial neural networks HNN to estimate the parameter ϑ of a specific RF-LDA model. As shown in [49], parameter estimation may be inaccurate and heavily influenced by outliers, especially for small sample size. In addition, many traditional techniques including recursive least-squares and Kalman filter for parameter estimation [39] require numerical solutions to complex equations. To avoid these issues, an alternative HNN-based approach was adopted for parameter estimation in RF-LDA, which is called HRF-LDA. The desirable characteristics of HNN are concluded as follows: (1) nonlinearity in representing the complex samples well; (2) robustness in the presence of noisy and inaccurate measurements; (3) good generalization and adaptation; (4) estimation is formulated as a function approximation problem, to which HNN is particularly well suited.By this means, as a control parameter, ϑ is dynamically assigned with particular value of offset to calculate the grade of membership. The corresponding work is related to [40–42]. Here, we use a neural network with N neurons,N=∑i=1Cni, to estimate the parameter ϑ. Therefore, this work attempts to improve the control parameter of the membership function in RF-LDA by setting up the problem for optimizing the parameter ϑ in formula (16). In fact, the value of ϑ found by HNN depends on different search ranges in various feature spaces of data sources. The optimal values of ϑ that result in different experiments indicate that the error surface has multi-modal minima.The proposed HRF-LDA is inspired by a reformative fuzzy LDA, which ranks each feature according to the relevancy between outlier patterns and their counterparts by calculating the membership grades. Moreover, HRF-LDA is developed to combine different genres of feature analysis to a consensus decision. Generally, HRF-LDA involves two aspects: (1) sample distribution evaluation, which determines the contribution of each feature on behalf of classification results; (2) parameter evaluation, which dynamically retains the most significant control parameter concerning the feature similarity. Here, the detailed HRF-LDA supervised learning method is described as follows:Step 1:Principal Component Analysis (PCA) is implemented to transform the original feature spaceℝmto a lower dimensional spaceℝg.Step 2:With the relaxed constraint, the class center matrix FM and membership grade matrix U can be obtained by FKNN in the PCA transformed spaceℝg.Step 3:Dynamically assign particular value of offset in the calculation of the membership grade by considering the formulation of an HNN model.Step 4:In formulas (15) and (16), ϑ is a parameter controlling the value of μijthat stands for the jth sample that belong to the ith class. While a smaller threshold φ is fixed and all membership grades to class ‘i’ for the jth sample satisfy the condition μij<φ, the sample is labeled as a outlier, otherwise, the sample is labeled as regular sample. Thereby, the whole training set that matches different sample properties (either outlier or regular) is separated in the training stage for initial pattern discovery.Step 5:For regular training samples, we calculate the fuzzy within-class scatter matrixRFSw, fuzzy between-class scatter matrix RFSband fuzzy total scatter matrix RFStin transformed spaceℝg, and solve the optimal discriminant eigenvectors φopt=[φ1, ..., φL] using RFSband RFStinstead of Sband Stunder the conventional generalized Fisher criterion. Noticeably, L represents the number of the optimal discriminant eigenvectors, which is C−1 or less than the rank of RFSb.In this stage, a new semi-supervised fuzzy clustering algorithm is presented in conjunction with precise number of clusters and initial pattern centers obtained previously in the stage of fuzzy supervised learning, and then applied to perform outlier samples classification, yielding the final recognition result. In particular, considering the adverse influence from outlier instances, we firstly remove the outlier samples from the whole training set by means of the previous supervised stage. Simultaneously, the number of clusters and accurate initial pattern centers are also obtained.The FCM clustering algorithm was first proposed by Dunn [50] and extended as a general clustering algorithm by Bezdek [51]. The main purpose of FCM algorithm is to divide the vector space of the sample points into a number of sub-spaces using distance measurement [52,53]. However, FCM algorithm often fails in dealing with local spatial property of images that leads to strong noise sensitivity.To introduce spatial information into objective function, many different algorithms were presented to improve the robustness of FCM. Pham et al. [54] modified the FCM objective function by introducing a spatial penalty, enabling the iterative algorithm to estimate spatially smooth membership functions. Meanwhile, Ahmed et al. [30] introduced a neighborhood averaging additive term into the objective function of FCM, named bias corrected FCM (BC-FCM). In BC-FCM, the objective function is modified as:(19)JBC-FCM=∑i=1C∑k=1Nuik2yk−bk−vi2+αNR∑i=1C∑k=1Nuik2(∑yr∈Nkyr−br−vi2)where ykis the observed log-transformed intensities at the kth pixel,viis the log-transformed prototype of the kth cluster and bkis the bias field at the kth pixel. Nkstands for the set of neighbors that exist in a window around ykand NRis the cardinality of Nk. The effect of the neighborhood term is controlled by α. The updating function of membership, centroids and bias field can be written as:(20)uik=∑j=1CDik+(α/NR)γiDjk+(α/NR)γj−1(21)vi=∑k=1N(yk−bk)+αNR∑yr∈Nk(yr−br)(1+α)∑k=1Nuik2(22)bk=yk−∑i=1Cuik2vi∑i=1Cuik2whereDik=yk−bk−vi2andγi=∑yr∈Nkyk−br−vi2.According to [55], clustering is the task of identifying natural groups in data. Identifying the most important variables and detecting quasi-homogenous groups of data are problems of interest in this context. Solving such problems is a difficult task, mainly due to the unsupervised nature of the underlying learning process.Existing studies in the literature propose and experiment with various clustering criteria [56]. The main concern is the bias on the number of clusters introduced by these criteria. Since this bias proved to be hard to eliminate, multi-objective algorithms were proposed [57], which evaluate the quality of a partition against several criteria. In order to design a robust optimizer for a special sample instances clustering problem, an approach for searching the precise number of clusters and initial pattern centers utilizing the previous stage of fuzzy supervised learning is needed.As a consequence of above, a new semi-supervised FCM (S-FCM) algorithm for outliers originates in the analogy with the proposed HRF-LDA theorem. The proposed S-FCM algorithm is described as follows:Step 1:By means of the HRF-LDA algorithm based on the class discriminatory measurement in Section 2.3, the whole sample set that matches different sample properties is separated into outlier and regular subsets in the supervised learning stage.Step 2:Project regular samples into the subspace spanned by optimal discriminant vectors and classify them. Hence, the number of classes and initial cluster centersV={v1,v2,…,vC}is obtained for the next step of outlier instances clustering.Step 3:Determine the termination of clustering iterations: if either the distanceVnew−Vold2of objective function is less than a threshold with a small value ɛ(ɛ>0) or the number of iterations exceeds a prescribed maximum number of iterations, the iteration is stopped, where2is the Euclidean distance, V is a vector of cluster centers.Step 4:Apply the fuzzy semi-supervised clustering criterion to outlier instances classification, and utilize the fusion of the classification results between the outlier and regular samples to obtain the final classification result.In addition, we extend the parameterized fuzzy semi-supervised learning method (PFSL) for nonlinear classification problems. The PFSL is a linear technique for feature extraction, thus it is not sufficient in dealing with features that have nonlinear relationship. To overcome this problem, we expand the approach of nonlinear semi-supervised learning to kernel PFSL (K-PFSL). In K-PFSL, we first map the input data into a potentially much higher dimensional feature space by virtue of nonlinear kernel trick, and in such a way, the problem of feature extraction in the nonlinear space is overcome. The standard KFD algorithm can be divided into two steps [24]:Step 1:Kernel Principal Component Analysis (KPCA) transformation from feature space H into Euclidean spaceℝg':(23)y=γ1λ1,⋯,γg'λg'T(Φ(x1),⋯,Φ(xg'))TΦ(x)=γ1λ1,⋯,γg'λg'T[k(x1,x),⋯,k(xg',x)]Step 2:LDA transformation in the KPCA transformed spaceℝg':(24)φ=GTywhere the solved G=(β1, ⋯, βd) is the set of discriminant vectors of LDA.Based on the above descriptions, we develop a novel nonlinear feature extraction method, i.e. K-PFSL, the main idea of K-PFSL is to implement PFSL on the KPCA transformed space, and the conclusion can be drawn that the essence of the proposed method is KPCA plus PFSL. The proposed K- PFSL can be summarized as follows:Step 1:Apply KPCA to the input data, using Eqs. (23) and (24) to transform all samples from spaceℝmto spaceℝg', where g'=N−1, and N is the number of training samples.Step 2:Project regular samples into the kernel transformed subspace spanned by optimal discriminant vectors, and perform classification on the regular samples.Step 3:By means of the proposed PFSL algorithm, we perform classification on the outlier instances in the kernel transformed subspaces, and obtain the final classification results.In this section, the computational complexity of Fisherface [22], D-LDA [23], C-LDA [24], R-DA [25], F-LDE [27], F-MMC [28] and the proposed PFSL are analyzed in Table 1. Here m×n is the resolution of the training image, L and N are the numbers of the projection vectors and the training samples, respectively.Concretely, the computational complexity of Fisherface depends on both the size of matrices in eigen equations and the number of the projection vectors that are required to be computed [58], while the computational complexity of D-LDA is dependent on the number of training samples. Moreover, the computation scales of C-LDA, R-DA and the proposed PFSL depend on the number of reduced subspaces, where S is the number of outliers, K is the number of clusters and T is the number of iterations. As analyzed above, although PFSL can be more effective than other ones for classification, it needs more CPU time for the whole process because the computation of fuzzy supervised classification with semi-supervised clustering is very expensive.This section reports a set of experiments for the tasks of face image classification. We compared the proposed method with various face recognition approaches including Fisherface [22], D-LDA [23], C-LDA [24], R-DA [25], F-LDA [26], F-LDE [27], F-MMC [28] and RF-LDA [29]. More specifically, among these approaches, a popular technique called PCA plus LDA that belongs to the two-stage LDA is commonly used. In Fisherface [22], the PCA is first used for dimensionality reduction before the application of LDA. The D-LDA method [23] takes the range space of the between-class scatter matrix as the intermediate subspace. The C-LDA framework [24] searches the discriminant vectors both in the range space and in the null space of within-scatter matrix. On the basis of random subspace, the R-DA algorithm [25] combines the advantages of Fisherface and D-LDA together. The F-LDA [26] is presented to incorporate a gradual level of assignment to class which is regarded as a membership. In F-LDE [27], a membership degree is also incorporated into the definition of the Laplacian scatter matrix to obtain the fuzzy Laplacian scatter matrix. The F-MMC [28] utilizes generalized singular value decomposition to the maximum margin criterion (MMC) criterion, and the fuzzy scatter matrices are simultaneously redefined by means of a membership. RF-LDA is our previous work [29] that extends F-LDA and includes complete fuzziness in the calculation of between-class and within-class scatter matrices. Meanwhile, RF-LDA is a part of the foundation of this article. These experiments mentioned above are performed on four publicly available databases, the ORL [35], NUST603 [36], FERET [37] and Yale [38] face databases.As discussed in [29], it is well known that the KNN rule is widely used for classification, which stores a set of prototypes that represent the knowledge of the problem. To classify a new instance x, the K-nearest neighbors of x are obtained, and x is classified into the class most frequent in these K neighbors. KNN method is widely used because of its good generalization and easy implementation compared with many other complex methods.Moreover, we applied cross-validation results of the proposed method on the ORL database, for five different values of K, (K=1, 3, 5, 7, 9) using Euclidean distance. The whole dataset per individual was randomly split into 10 subsets. We repeated this 10 times. In each round, 5 subsets per individual on ORL were chosen for training and the remaining ones were used for test. As shown in Table 2, we find that the recognition rate obtained for K=3 is highest. Therefore, the parameter of KNN classifier is fixed as K=3 in the following experiments. In addition, some observations and discussions of experimental results are included in Section 4.6.The ORL database contains 40 distinct persons with 10 images per subject. The images are taken at different time instances, with slightly varying lighting conditions, facial expressions, and facial details. All the faces in ORL are in the up-right, frontal position, with tolerance for some side movement. Each image in ORL is rescaled to 23×28. For example, the images in Fig. 1show the first two classes from the ORL database.In this experiment, we randomly select θ(θ=4, 5, 6) subsets per individual for training and the remaining ones for test. Thus, a training set of 40×θ images and a test set with 40×(10−θ) images are created. We repeated this 10 times and report the mean and standard deviations.As described in Section 2.3, how to adaptively tune the parameter ϑ is a crucial issue for the RF-LDA approach. More specifically, ϑ is a parameter which controls the influence of fuzzy membership grade in the redefined each scatter matrix, which satisfies the constraint ϑ∈(0, 1). Thus, with variation of ϑ, the discriminatory capability of RF-LDA is gradually changed due to the adjustment of the membership grade to class “i” for jth pattern. Now, we approach the problem of control parameter estimation in RF-LDA by considering the formulation of an HNN. By this means, as a control parameter in formula (16), ϑ is dynamically assigned a particular value of offset in the calculation of the membership grade. After executing the evaluation algorithm HRF-LDA on the ORL database, we have found the optimal value for the parameter ϑ=0.3. In the following, Fig. 2indicates the convergence time of the HNN estimator as a function of the adjustable parameter m.First, we applied the presented separation algorithm S-FCM on ORL. The whole dataset per individual was randomly split into 10 subsets. In each round, 4 subsets per individual were chosen for training and the remaining ones were used for test. Thus, a training set of 160 images and a test set with 240 images are created in each round. We repeated this 10 times, and the mean of clustering accuracy of S-FCM is provided. A detailed view containing two clustering approaches, BC-FCM and S-FCM, is shown in Table 3. By means of the comparisons between these two algorithms, the proposed S-FCM performs much better. For the test image samples with 5% outliers, the conventional unsupervised BC-FCM cannot get an accurate separation. In contrast, the S-FCM algorithm obtains a higher accuracy in these special outlier areas.Second, Table 4indicates that the average recognition rates of D-LDA, C-LDA, R-DA, F-LDA, RF-LDA and PFSL vary with the number of training samples per individual on ORL. The values in parentheses of the table denote the dimensionality of the reduced subspace of each method. As shown in Table 4, it is therefore reasonable to believe that the proposed method is the most effective one no matter how many training samples per individual are used. As can be seen from the previous experiments, to evaluate the efficiency of the algorithm and remove the influence caused by the choice of the training set and test set, we repeated the experiment 10 times, and the standard deviations for both methods are provided.As the axes numbers of classes varying from 2 to 40, the average recognition rates of Fisherface, F-LDA, RF-LDA, F-LDE, F-MMC and the proposed method are shown in Fig. 3. Similarly, in each round, 5 subsets per individual on ORL were chosen for training and the remaining ones were used for test. Thus, a training set of 200 images and a test set with 200 images are created. Fig. 3 demonstrates that the performance of the proposed method is better than that of the others.Meanwhile, as the axes numbers of subset of subjects varying from 1 to 9, the average recognition rates of classical semi-supervised algorithms including K-Means+SVM [31], Mean Shift+SVM [32], Gaussian Mixtures+SVM [33], DBSCAN+KNN [34] and the proposed method are shown in Fig. 4. Fig. 4 shows that the performance of the proposed method is better than that of the traditional semi-supervised clustering algorithms.The NUST603 contains a set of faces taken at the 603 Research Laboratory in School of Computer Science and Technology, Nanjing University of Science and Technology, P.R. China. The database contains 960 face images of 96 individuals, including frontal views of faces with different facial expressions, lighting conditions and facial details. In our experiment, each image is rescaled to 32×32. Some examples from the NUST603 database are shown in Fig. 5.Similarly, we adopt the same dynamical parameter evaluation method presented in Section 2.3, the control parameter ϑ is dynamically assigned using a particular value of the membership grade as ϑ=0.1 by executing the HRF-LDA algorithm on the NUST603 database.In this experiment, the whole dataset per individual was randomly split into 10 subsets. In each round, θ(θ=3, 4, 5) subsets per individual on NUST603 were chosen for training and the remaining ones were used for test. Thus, a training set of 96×θ images and a test set with 96×(10−θ) images are created in each round. We repeated this 10 times and the mean and standard deviation were reported. Here, the values in parentheses of the table denote the dimensionality of the reduced subspace of each method. Table 5indicates that the recognition rates of D-LDA, C-LDA, R-DA, F-LDA, RF-LDA and PFSL vary with the number of training samples per individual. Table 5 shows that the proposed method is no worse than other ones regardless of the number of training samples per individual.The FERET face image database is a standard benchmark for state-of-the-art face recognition techniques. The algorithm was evaluated on a subset of FERET database, which includes 1400 images of 200 individuals with seven different images of each individual. In our experiment, all the images are rescaled to a resolution of 40×40. Some examples of the FERET database are shown in Fig. 6. Similarly, the control parameter ϑ is dynamically assigned as ϑ=0.3.In this experiment, the whole dataset per individual was randomly split into 7 subsets. In each round, 3 subsets per individual on FERET were chosen for training and the remaining ones were used for test. Thus, a training set of 600 images and a test set with 800 images are created in each round. We repeated this 10 times and reported the mean and standard deviation. Table 6presents the average results of D-LDA, C-LDA, RDA, F-LDA, RF-LDA and PFSL. For all methods, the average CPU time consumed for training and testing and the corresponding dimensionality of the reduced subspaces of each method are also given in Table 6. Again, the experimental results indicate that the proposed method is the most effective one for the facial feature extraction. However, it is worth stressing that the proposed method needs more CPU time for whole process (supervised and semi-supervised learning stage) because it costs more computation by using fuzzy semi-supervised clustering for outlier instances.The Yale face image database contains 165 grayscale images of 15 individuals. There are 11 images per subject, one per different facial expression or configuration. We manually cropped the facial portion of each face image and rescaled it to 56×46 pixels. Some examples from the Yale database are shown in Fig. 7.In this experiment, the whole dataset per individual was randomly split into 11 subsets. In each round, 5 subsets per individual on Yale were chosen for training and the remaining ones were used for test. Thus, a training set of 75 images and a test set with 90 images are created in each round. We adapt the same dynamical parameter evaluation method presented in Section 2.3, and the control parameter is dynamically assigned as ϑ=0.2 on the Yale database. Table 7presents the results of Fisherface, D-LDA, C-LDA, RDA, F-LDA, RF-LDA, PFSL and K-PFSL. Here, the corresponding dimensionality of the reduced subspaces of each method is also given in a row of Table 7. For the kernel method, we employed the polynomial kernel function k(x, y)=(x·y+1)d, where d is empirically set to 0.5.To evaluate the effect of the different kernel functions, we take into consideration two kernels [59], polynomial kernel function and Gaussian kernel functionk(x,y)=exp(−x−y2/δ). The parameter d of the polynomial kernel function is set to 0.1, 0.5, 1, and 2. The parameter δ of the Gaussian kernel function is empirically set as δ=0.3×n[24], where n is the image dimension. In this experiment, we randomly chose three images per individual for training and the remaining samples for test, and the nearest neighbor classifier using Euclidean distance is employed for the supervised classification. We repeated this 10 times, and the experimental results are shown in Table 8and reported by the mean and standard deviation.Moreover, as the axes numbers of classes varying from 2 to 15, the average recognition rates of F-LDA, RF-LDA, F-LDE, F-MMC and the proposed method are shown in Fig. 8. Similarly, in each round of the experiment, 5 subsets per individual on Yale were chosen for training and the remaining ones were used for test. We repeated the experiment 10 times. In the experiment, the KNN classifier with Euclidean distance is employed for the supervised classification. Again, the performance of the proposed method is much better than those the traditional ones.From the experimental results listed in the tables and figures discussed above, we have the observations and discussions as follows.(1) In this work, the image objects were labeled as different types (outlier samples and regular samples). It is worth noting that the supervised learning step was proposed to iteratively learn multiple flexible discriminative subspaces for dimensionality reduction, which ranks each sample according to the relevancy between the outliers and regular ones. The supervised pattern discovery stage focuses on the distribution evaluation of samples, which determines the contribution of each feature vector on behalf of classification results. The semi-supervised clustering stage aims to address the issue that the first step of our method could not accurately classify the abnormal sample regions such as outliers zones. The performance results shown in Figs. 3, 4 and 8 and Tables 3–7 demonstrate the effectiveness of the proposed method.(2) We can reach a conclusion from Tables 7 and 8 as follows. First, the kernel methods outperform the linear ones. Second, the selected kernel function and its corresponding parameter value could influence the recognition rates. Third, the reason why the presented kernel algorithm generates a better performance could be attributed to the fact that the K-PFSL can efficiently control the nonlinear ambiguity of facial images degraded by some factors such as occlusion and illumination components.(3) Fig. 3 shows the average recognition rate of each method and the range of the number of classes (from 2 to 40). We evaluated the influence of the number of classes (or training samples) on classification of each algorithm by this experiment. As shown in Fig. 3, the proposed method achieves more effective and stable performance, especially in the case when the number of classes is relatively small, while the recognition for other methods improves with an increase of classes. This implies that sufficient training samples cannot be efficiently obtained in many face recognition applications. As a result, the corresponding phenomenon of underfitting might be raised. The proposed method validates the effectiveness of a supervised pattern discovery model associated with a semi-supervised manner. When the supervised learning step of the proposed method could not accurately classify the samples from the abnormal sample set (i.e. outliers), the semi-supervised fuzzy clustering step of our method can be carried out on the basis of the prior knowledge that obtained in the supervised learning stage, therefore, the issue of outlier classification under the condition of small sample size could be well addressed. That is the reason why the proposed method obtains an accuracy of 100% (from 2 to 4 class) while the recognition for other methods improve with an increase of classes.

@&#CONCLUSIONS@&#
