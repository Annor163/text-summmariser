@&#MAIN-TITLE@&#
Knowledge reduction for decision tables with attribute value taxonomies

@&#HIGHLIGHTS@&#
We present an attribute-generalization reduct for decision tables with AVTs.We analyze relationships between the attribute reduct and the generalization reduct.We develop a heuristic algorithm AGR-SCE to find the generalization reduct.The generalization reduct can objectively control the generalization process.The generalization reduct can avoid over-generalization or under-generalization.

@&#KEYPHRASES@&#
Knowledge reduction,Attribute value taxonomy,Attribute generalization,Classification,Rough set theory,

@&#ABSTRACT@&#
Attribute reduction and attribute generalization are two basic methods for simple representations of knowledge. Attribute reduction can only reduce the number of attributes and is thus unsuitable for attributes with hierarchical domains. Attribute generalization can transform raw attribute domains into a coarser granularity by exploiting attribute value taxonomies (AVTs). As the control of how high an attribute should be generalized is typically quite subjective, it can easily result in over-generalization or under-generalization. This paper investigates knowledge reduction for decision tables with AVTs, which can objectively control the generalization process, and construct a reduced data set with fewer attributes and smaller attribute domains. Specifically, we make use of Shannon’s conditional entropy for measuring classification capability for generalization and propose a novel concept for knowledge reduction, designated attribute-generalization reduct, which can objectively generalize attributes to maximize high levels while keep the same classification capability as the raw data. We analyze major relationships between attribute reduct and attribute-generalization reduct and prove that finding a minimal attribute-generalization reduct is an NP-hard problem and develop a heuristic algorithm for attribute-generalization reduction, namely, AGR-SCE. Empirical studies demonstrate that our algorithm accomplishes better classification performance and assists in computing smaller rule sets with better generalized knowledge compared with the attribute reduction method.

@&#INTRODUCTION@&#
Real-world applications in data mining and machine learning require that their tools can reduce dimensionality of large databases and build classifiers with improved classification performance [1,25]. In practice, data are often represented in the form of decision tables with not only a huge number of attributes but also large cardinalities of attribute domains. Utilizing most classification methods, such as rule-based classifiers, on such data is rather difficult and often infeasible [19,38]. It is therefore desirable to develop data preprocessing techniques to reduce both the number of attributes and the cardinalities of attribute domains and generate more accurate and compact classifiers from reduced data [19,36].One of the key issues of preprocessing techniques is knowledge reduction. Different methods have been proposed for effective and efficient reduction of knowledge [1,4,5,24]. Of all the paradigms, rough set theory (RST), proposed by Pawlak [21], is a relatively new soft computing tool for analyzing various types of data and makes significant contributions to this field [7,8,22]. Attribute reduction in RST offers a systematic theoretic framework for finding particular subsets of condition attributes that provide the same descriptive or classification ability as the entire set of attributes. However, this reduction method can only reduce the number of attributes and is thus unsuitable for attribute domain reduction.One method in data mining is to provide domain reduction of attributes in a database by generalization [3,9]. This approach is known as attribute-oriented induction (AOI). Attribute generalization is achieved by using an attribute value taxonomy (AVT, also known as concept hierarchy). Such an AVT reflects necessary background knowledge, which controls the generalization process, and can range from the single, most generalized root concept to the most specific concepts corresponding to the specific values of attributes in the database [3,9,10,36].In the AOI method, the generalization process is performed by either attribute removal or attribute generalization and guided by two thresholds: the attribute threshold and the relation threshold [3,10,36]. The attribute threshold specifies the maximum number of distinct values of any attribute that may exist after generalization, and the relation threshold provides an upper bound to the number of generalized objects that remain after the generalization process. However, the thresholds are often provided by domain experts or knowledge engineers, and the control of how high an attribute should be generalized is typically quite subjective, which may lead to over-generalization or under-generalization [3,9,10,26]. Therefore, it is interesting to objectively calculate the corresponding abstraction level that each attribute value should be generalized to.In an AVT, the root is the most abstract value “ANY” of an attribute. Leaf nodes are attribute values appearing in the given raw table, and internal nodes represent generalized attribute values of their child nodes. A generalization replaces some values with a parent value in the AVT. With respect to classification tasks, there are two generalization schemes for domain consistency: full-domain generalization (FDG) [2,6,11,12] and full-subtree generalization (FSG) [34–37]. In FDG, all values in an attribute are generalized to the same level of the AVT. In the FSG, at a non-leaf node, either all child values or none are generalized, and a generalized attribute has values that form a “cut” through its AVT. AVT may be defined for either a discrete or continuous valued attribute. A leaf concept for a continuous attribute is expressed as a range of values.Example 1Consider a flat decision table with condition attribute set C={Job, Age, Sex} and decision attribute set D={Salary} in Fig. 1a. The table has 12 records in total. Each row means one or more records with the Salary column including the class frequency of the records represented. “L” means their Salary is Low, and “H” means their Salary is High. Fig. 1b shows the AVTs of C, Fig. 1c and d show the FDG and FSG for the AVT of attribute Job, respectively. According to FDG, if “Engineer” and “Lawyer” are generalized to “Professional”, then it also requires generalizing “Dancer” and “Writer” to “Artist”. While according to FSG, if “Engineer” is generalized to “Professional”, this scheme also requires the other child node, “Lawyer” to be generalized to “Professional”, but “Dancer” and “Writer”, which are child nodes of Artist, can remain ungeneralized.Recently, RST has been used to mine generalized decision rules from decision tables and hierarchical attributes [6,11,12]. However, they still lack the adaptability in solving attribute generalization. In particular, their work adopted the FDG on the AVT and caused the largest distortion of the data for the same granularity level requirement on all paths of a taxonomy tree. AVT with the FSG is more general and provides a way to organize data at different levels of granularity, which have been demonstrated to be useful in generating accurate, compact, and comprehensible classifiers [34–37].According to the above observations, we focus on the issues involved with FSG when we are faced with decision tables and attribute value taxonomies (AVTs) for condition attributes and generalizing condition attribute values to maximize concept levels while retaining the classification ability at primitive level. Wang [29,30] used Shannon’s conditional entropy to measure the classification ability of condition attributes with respect to decision attributes and constructed a heuristic algorithm for attribute reduction. This reduction method keeps the conditional entropy of target decision unchanged. Therefore, we use it to measure classification capability of different generalization levels of condition attributes. Paralleling attribute reduct for a flat data table, we present a novel concept for knowledge reduction in decision tables and attributes associated with AVTs, namely attribute-generalization reduct.Attribute-generalization reduction can objectively calculate the corresponding abstraction level that each attribute value should be generalized to and induce the most abstract generalized decision table with the same classification ability on the raw decision table, and no other generalized decision table exists that is more abstract than it.We discuss the relationship between attribute-generalization reduct and attribute reduct. Generally speaking, the reduced data generated by attribute reduction methods are composed of primitive values, while the reduced data generated by attribute-generalization reduction methods are composed of primitive values, as well as generalized values from the AVTs. Therefore, we can think of attribute reduct as a special case of attribute-generalization reduct where for each attribute, a two-level hierarchy with the root node of “ANY” and leaf nodes corresponding to all attribute values in the raw decision table.We prove that the problem of attribute-generalization reduct generation is NP-hard and develop a heuristic algorithm for attribute-generalization reduction. To evaluate our algorithm, we conducted experiments with datasets from the UCI machine learning repository. Our algorithm can reduce both the number of attributes and the cardinalities of attribute domains and assist in obtaining smaller generalized decision tables and smaller decision rule sets with better predication ability compared with the attribute reduction algorithms.The paper is organized as follows. Section 2 outlines some preliminary knowledge. Section 3 presents the concept of generalized decision table generated by attribute generalization and discusses its properties related to RST. In Section 4, a novel concept for knowledge reduction called attribute-generalization reduct is presented. The relationship between attribute-generalization reduct and attribute reduct is analyzed. It is proved that finding minimal attribute-generalization reduct or all attribute-generalization reducts are both NP-hard problems. An attribute generalization reduction algorithm based on Shannon’s conditional entropy named AGR-SCE is proposed, and an example is employed to illustrate the AGR-SCE method. Experimental analysis is conducted in Section 5. Finally, Section 6 reviews related work, while Section 7 draws conclusions and discusses future work.In this section, we will introduce several basic notions of rough set theory and full-subtree generalization model.Rough set theory deals with data represented by a table, where columns of the table are labeled by attributes, rows by objects of interest, and entries in the table are attribute values. This paper is only concerned with decision tables having only one decision attribute.Definition 1Decision tableA decision table is a quadruple S=〈U, A, V, f〉, where U={x1, x2, …, xn} is a non-empty finite set of objects; A=C∪D is a non-empty finite set of attributes, where C represents condition attribute set, D={d} is decision attribute set, C∩D=Ø; V=∪a∈AVa, where Vadenotes the value set of attribute a, called the domain of a; f: U×A→V is an information function such as for any a∈A and x∈U, f(x, a)∈Va.Given a decision table S=〈U, C∪D, V, f〉, for any B⊆(C∪D), B determines an indiscernibility relation IND(B) on U as follows:IND(B)={(x,y)|∀a∈B,f(x,a)=f(y,a)}Obviously, IND(B) is an equivalence relation and partitions the set U into disjoint subsets. The family of all equivalence classes of IND(B) is denoted by U/IND(B), or simply U/B. An equivalence class of IND(B) containing x is denoted by [x]B.Given a decision table S=〈U, C∪D, V, f〉, B⊆(C∪D) and U/B={X1, X2, …, Xm}, the information entropy of B is defined byH(B)=-∑i=1m|Xi||U|log2|Xi||U|The information entropy achieves the maximum value log2∣U∣ for the finest partition consisting of singleton subsets of U, and it achieves the minimum value 0 for the coarsest partition {U}.Given a decision table S=〈U, C∪D, V, f〉 and B⊆C. Let U/B={X1, X2, …, Xn}, U/D={Y1, Y2, …, Ym}. The conditional entropy of B on D can be defined as:H(D|B)=-∑i=1n|Xi||U|∑j=1m|Xi∩Yj||Xi|log2|Xi∩Yj||Xi|Wang [29] used the conditional entropy H(D∣C) to measure the classification ability of all condition attributes with respect to the decision attribute, i.e. the classification quality of a decision table can be represented by H(D∣C).Given a decision table S=〈U, C∪D, V, f〉, for any B⊆C, we have H(D∣B)=H(B∪D)−H(B). Based on conditional entropy, an attribute reduct can be defined as follows.Given a decision table S=〈U, C∪D, V, f〉. B⊆C is an attribute reduct of C relative to D, if and only if(i)H(D∣B)=H(D∣C), and∀ B′⊆B, H(D∣B′)·H(D∣C).Wang [30] used the conditional entropy to construct heuristic algorithm for attribute reduction which keeps the conditional entropy of target decision unchanged.Definition 6Attribute value taxonomy 34–36The attribute value taxonomy AVT(a) for attribute a is a tree-structured concept hierarchy in the form of a partially ordered set (Va, ≺), where Vais a finite set that enumerates all attribute values in a and ≺ is the partial order that specifies a relationship among attribute values in Va. Collectively, AVTs(C)={AVT(a1), AVT(a2), …, AVT(am)} represents the ordered set of attribute value taxonomies (AVTs) associated with C={a1, a2, …, am}.In AVT(a), leaf nodes correspond to the actual attribute values of attribute a appearing in the raw data. Internal nodes represent generalized attribute values of their child nodes, and each arc of the tree corresponds to a relationship over attribute values. Let Leaf(a) represent the set of leaf nodes, Root(a) stand for the root node, Node(a) represent the set of all nodes, and the internal nodes of AVT(a) (i.e. Node(a)-Leaf(a)) correspond to generalization values of attribute a. Child(v, a) is the set of all children of a node corresponding to value v in the AVT(a), and Depth(a) is the maximum length of a path from the root to a leaf in the AVT(a).For example, Fig. 1b shows three attributes with corresponding AVTs(C)={AVT(Job), AVT(Age), AVT(Sex)}. With regard to AVT(Sex) associated with the Sex status, Root(Sex)={ANY_Sex}, Leaf(Sex)={Male, Female}, Node(Sex)={ANY_Sex, Male, Female}, the internal nodes of AVT(Sex) correspond to {ANY_Sex}, and Depth(Sex)=1.Intuitively, with the full-subtree generalization model [36], a valid generalization is represented by a “cut” through the AVTs. To distinguish different types of cuts, we call the “cut” through one attribute AVT a single-dimensional cut and call the “cut” through the attribute set AVTs a multi-dimensional cut. A single-dimensional cut of a tree is a subset of values in the tree that contains exactly one value on each root-to-leaf path. A single-dimensional cut γ for AVT(a) can be defined as follows.A single-dimensional cut γ is a subset of elements in Node(a) satisfying the following properties:(1)∀ p∈Leaf(a), either p∈γ or p is a descendant of an element q∈γ, and∀ p, q∈γ, p is neither a descendant nor an ancestor of q.Now let Ωabe the set of all valid single-dimensional cuts in AVT(a) of attribute a, and ΩB=Ω1×Ω2×⋯×Ωmrepresent the Cartesian product of the cuts through the individual AVTs(B) of B={a1, a2, …, am}. Hence, ΓB={γ1, γ2, …, γm} defines a multi-dimensional cut through AVTs(B), where each γi∈Ωiand ΓB∈ΩB.For a given multi-dimensional cut Γ, if each cut γ∈Γ corresponds to the primitive values of the respective attribute a, i.e., γ=Leaf(a), then the cut Γ is denoted by Γleaf. If each cut γ∈Γ corresponds to the most generalization values of the respective attribute, i.e., γ=Root(a) and the cut Γ is denoted by Γroot.Fig. 2a illustrates all valid single-dimensional cuts through each AVT of AVTs(C) where C={Job, Age, Sex} in Fig. 1b. Fig. 2b illustrates a multi-dimensional cut Γ through AVTs(C). In Fig. 2b, we have Γ={A1, B1, C0} where A1={Professional, Artist}, B1={[45,60),[28,45)}, and C0={ANY_Sex}.In this section, AVTs are introduced into a flat decision table, and some notions and properties of the rough sets for decision tables with AVTs are revealed.Most previous studies on rough sets focused on flat data (also known as single-level data). Flat data are usually provided in real-world applications, from which the inherent hierarchical characteristics of data cannot be reflected effectively. The AVT for each attribute can easily reflect the inherent hierarchical characteristics of data and provide a more compact representation for the data.Moreover, we can obtain a compressed, high-level, generalized data by the hierarchical structure of AVTs. So, it is useful to generalize the given flat data to a hierarchical structure, that is, to transform a flat decision table to a generalized decision table under attribute generalization. Given a multi-dimensional cut Γ, we can produce a generalized decision table SΓfrom the raw decision table S, which is uniquely generalized from all attribute values in S to their taxonomical ancestors in Γ.Given a flat decision table and AVT associated with each attribute, we can obtain a generalized decision table with different levels of abstraction. The generalized decision table can be defined as follows.Definition 8Generalized decision tableGiven a decision table S=〈U, C∪D, V, f〉 and AVTs(C), a cut Γ={γ1, γ2, …, γm} through AVTs(C) determines a generalized decision table SΓon S with respect to AVTs(C):SΓ=〈U,C∪D,VΓ,fΓ〉,where VΓ=∪a∈C∪DVa, Vais unchanged for any a∈D, Va=γafor any a∈C; fΓ: U×C∪D→VΓis an information function such that for any a∈C∪D and x∈U, fΓ(x, a)∈Va.Obviously, a generalized decision table (multi-level decision table [34]) is an attribute-value system in which each object under each attribute is composed of primitive values as well as generalized values at different levels in the AVTs, and the flat decision table S can be determined by the cut Γleaf∈ΩC, denoted bySΓleaf.On a generalized decision table SΓdetermined by the cut Γ∈ΩC, the C, U/C, U/(C∪D), and H(D∣C) are denoted by CΓ, U/CΓ, U/(CΓ∪D), and H(D∣CΓ), respectively. For any attribute a∈C and its cut γ∈Ωa, the a, U/{a}, U/({a}∪D), and H(D∣{a}) on the generalized decision table SΓare denoted by aγ, U/{a}γ, U/({aγ}∪D), and H(D∣{a}γ), respectively.To analyze the properties of rough set theory for generalized decision table with different levels of abstraction, we will first give some definitions as follows.For a single-dimensional cut γ of attribute a in AVT(a), v∈γ, a refinement of γ is defined as: γ′=γ∪Child(v, a)−{v}, which is denoted by γ⩾γ′ (or γ′⩽γ). We say thataγ′is a refinement of the attribute aγ, denoted byaγ⩾aγ′(oraγ′⩽aγ). Correspondingly, for a multi-dimensional cut Γ, γ∈Γ, γ⩾γ′, a refinement of Γ∗ is defined as: Γ∗=Γ∪{γ′}−{γ}, which is denoted by Γ⩾Γ∗ (or Γ∗⩽Γ). We say that CΓ∗ is a refinement of the attribute set CΓ, denoted byU/CΓ⩾U/CΓ∗(orU/CΓ∗⩽U/CΓ). Conversely, γ is a generalization of the cut γ′, aγis a generalization of the attributeaγ′,Γis a generalization of the cut Γ∗, and CΓis a generalization of the attribute set CΓ∗, respectively.Fig. 3illustrates a cut refinement process based on the AVTs(C) shown in Fig. 1b. The single-dimensional cut γ={Any_Sex} in attribute Sex has been refined to γ′={Male, Female} by replacing Any_Sex with its two children, i.e., Male and Female. Therefore, the multi-dimensional cut Γ2={{Professional, Artist}, {[28, 45), [45, 60)}, {Male, Female}} is a refinement of the multi-dimensional cut Γ1={{Professional, Artist}, {[28, 45), [45, 60)}, {ANY_Sex}.Given a decision table S=〈U, C∪D, V, f〉, AVTs(C), and two cuts Γ1, Γ2∈ΩC, if Γ1⩾Γ2, then generalized decision tableSΓ1is coarser thanSΓ2, orSΓ2is finer thanSΓ1, denoted bySΓ1⩾SΓ2(orSΓ2⩽SΓ1).Obviously, the coarser/finer relation of generalized decision tables with different levels of abstraction is determined only by the granularity of its attribute values. The decision tableSΓrootthat corresponds to the cut Γrootis the coarsest decision table, and the decision tableSΓleafthat corresponds to the cut Γleaftis the finest decision table.Given a decision table S=〈U, C∪D, V, f〉 and AVTs(C). For any γ1, γ2∈Ωa, if for eachX∈U/{a}γ1, there exists Y∈U/{a}γ2 such that X⊆Y, we say thatU/{a}γ1is finer thanU/{a}γ2, orU/{a}γ2is coarser thanU/{a}γ1, denoted byU/{a}γ1⩽U/{a}γ2(orU/{a}γ2⩾U/{a}γ1). Correspondingly, for any Γ1, Γ2∈ΩC, if for eachX∈U/CΓ1, there existsY∈U/CΓ2such that X⊆Y, we say thatU/CΓ1is finer thanU/CΓ2, orU/CΓ2is coarser thanU/CΓ1, denoted byU/CΓ1⩽U/CΓ2(orU/CΓ2⩾U/CΓ1).Given a decision table S=〈U, C∪D, V, f〉, if there exists two partitions P and Q on U, U/P={X1, X2, …, Xn}, and U/Q={X1, X2, …, Xi−1, Xi+1, …, Xj−1, Xj+1, …, Xn, Xi∪Xj}, that is, U/Q is generated through combining the equivalence classes Xiand Xjof the partition U/P to Xi∪Xj, then we have H(D∣P)⩽H(D∣Q). FromDefinition 11 and Lemma 2, we can obtain the following corollary.Given a decision table S=〈U, C∪D, V, f〉 and AVTs(C). For any Γ1, Γ2∈ΩC, ifU/CΓ1⩽U/CΓ2, then we haveH(D|CΓ1)⩽H(D|CΓ2).Given a decision table S=〈U, C∪D, V, f〉 and AVTs(C), for any Γ1, Γ2∈ΩC, if Γ1⩾Γ2, we have the following properties:(1)U/CΓ1⩾U/CΓ2, andH(D|CΓ1)⩾H(D|CΓ2).Let Γ1={γ1, γ2, …, γm}. According to Definition 11, if Γ1⩾Γ2, there exist a local cut γa∈Γ1 and v∈γasuch thatΓ2=Γ1∪γa′-{γa}, whereγa′=γa∪Child(v, a)−{v}. In Γ1 and Γ2, for each attribute q∈C−{a}, the values of attribute q at the same level of abstraction, then we haveU/(C-{a})Γ1′=U/(C-{a)Γ2′,whereΓ1′=Γ1-{γa}, andΓ2′=Γ2-γa′.Let U/{a}γa={X1, X2, …, Xn} and Child(v, a)={v1, v2, …, vk}, then there existsXj=Xj1∪Xj2∪…∪Xjk,where Xj=[v]{a}∈U/{a}γa, Xjp=[vp]{a}, 1⩽p⩽k.Therefore, we haveU/{a}γa′={X1,X2,…,Xj-1,Xj1,Xj2,…,Xjk,Xj+1,…,Xn}.(1)LetU/(C-{a})Γ1′=U/(C-{a})Γ2′={Z1,Z2,…,Zt}. Then we can obtain thatU/CΓ1={Xi∩Zj|Xi∈U/{a}γa,Zj∈U/(C-{a})Γ1′}andU/CΓ2={Xi∩Zj|Xi∈U/{a}γa′,Zj∈U/(C-{a})Γ2′}.According to Definition 11, we can obtainU/CΓ1⩾U/CΓ2.From (1) and Corollary 1, we can obtainH(D|CΓ1)⩾H(D|CΓ2). This completes the proof.Theorem 1 indicates that a finer multi-dimensional cut can induce a finer partition and lower conditional entropy (i.e., a higher of classification ability).From Theorem 1, we can obtain the following corollary. □Given a decision table S=〈U, C∪D, V, f〉 and AVTs(C), if Ω={Γroot, Γ1, …, Γn, Γleaf}⊆ΩCare a family of multi-dimensional cut sets with Γroot⩾Γ1⩾⋯⩾Γn⩾Γleaf, then we have the following properties:(1)U/CΓroot⩾U/CΓ1⩾⋯⩾U/CΓn⩾U/CΓleaf, andH(D|CΓroot)⩾H(D|CΓ1)⩾⋯⩾H(D|CΓn)⩾H(D|CΓleaf).Corollary 2 indicates a partition induced by the equivalence relation with attribute values at different levels of abstraction and provides a nested sequence of granulation worlds from a coarser to a finer which can be determined by a nested sequence of cuts with granulations from coarse to fine in the set of multi-dimensional cuts.According toCorollary 2, the raw decision table has the smallest conditional entropyH(D|CΓleaf)among all the generalized decision tables. Corollary 2 also reveals that the nested sequence of conditional entropy is a monotonic decreasing nested sequence with the generalized partition becoming finer. That is, the finer the partition in a generalized decision table is, the smaller its conditional entropy and the larger its classification capability.In this section, we present the novel concept for knowledge reduction called attribute-generalization reduct for decision tables and the condition attributes associated with AVTs, discuss the relation of attribute-generalization reduct and attribute reduct, and then propose a heuristic algorithm for attribute-generalization reduction.Given a decision table and AVTs for condition attributes, when some of its condition attribute values are generalized to a higher level of abstraction, correspondingly, its domain will be changed, its information function will be changed, and thus, the decision table will be changed [33]. Therefore, attribute values with different levels of abstraction will determine different generalized tables, or we can say that a generalized table is determined uniquely by the levels of abstraction of attribute values [34]. This paper focuses on finding a most abstract generalized decision table with the same classification ability on the raw decision table, and no other generalized decision table exists that is more abstract than it.Based on conditional entropy, an attribute-generalization reduct can be defined as follows.Definition 12Attribute-generalization reductGiven a decision table S=〈U, C∪D, V, f〉 and AVTs(C), for B⊆C and a multi-dimensional cut Γ∈ΩB, BΓis an attribute-generalization reduct of C relative to D, if and only if(i)H(D∣BΓ)=H(D∣C),∀ Γ∗∈ΩBandΓ∗⩾Γ,H(D|BΓ∗)≠H(D|C), and∀ a∈B, Root(a)∉Γ.The first condition (i) indicates the joint sufficiency of the multi-dimensional cut Γ, i.e., BΓis sufficient to preserve the conditional entropy of target decision unchanged.The condition (ii) and (iii) indicates that each single-dimensional cut γ in Γ is individually necessary, i.e., any generalization of BΓis not sufficient to preserve the conditional entropy of target decision unchanged.According to Definition 12, the corresponding generalization level that each attribute value should be generalized is objectively controlled by an attribute-generalization reduct. According to the reduct BΓ, we can obtain the generalized decision table SΓ, which keeps the same classification ability on the raw table S, and when some of its attribute values are generalized to a higher level of abstraction, correspondingly, its classification ability will be decreased.Given a decision table S=〈U, C∪D, V, f〉 and AVTs(C), let B⊆C be an attribute reduct, Γ={γa∣a∈B, γa=Leaf(a)}, then we have H(D∣BΓ)=H(D∣C).According to Definitions 5 and 8, we can easily obtain the theorem. □Given a decision table S=〈U, C∪D, V, f〉 and AVTs(C), let B⊆C, Γ={γa∣a∈B, γa=Leaf(a)}, if BΓis an attribute-generalization reduct, then B is an attribute reduct.According to Definitions 5 and 12, we can easily obtain the theorem. □Given a decision table S=〈U, C∪D, V, f〉 and AVTs(C) that Depth(a)=1 for a∈C, for B⊆C, Depth(a)=1 for a∈B and Γ={γa∣a∈B, γa=Leaf(a)}, BΓis an attribute-generalization reduct if and only if B is an attribute reduct.According to Definitions 5 and 12, we can easily obtain the theorem.From Theorem 4, we can think of attribute reduct as a special case of attribute-generalization reduct where for each attribute, a two-level hierarchy with the root node of “ANY” character and leaf nodes correspond to all attribute values in the raw decision table.From Theorems 2 and 4, we can obtain the following corollary. □Given a decision table S=〈U, C∪D, V, f〉 and AVTs(C), let B⊆C be an attribute reduct. Then, there is at least one attribute-generalization reduct BΓwhere Γ={γa∣a∈B, γa=Node(a)}.In rough set theory, an attribute reduct with minimal cardinality among all the reducts is called a minimal attribute reduct. In this paper, an attribute-generalization reduct with minimal cardinality sum of attribute domains among all the reducts is called a minimal attribute-generalization reduct. The goal of attribute-generalization reduction is to find a minimal reduct.It is well-known that finding a minimal attribute reduct or all attribute reducts are both NP-hard problems [16,23,28]. According toCorollary 3, we can see that the problem of attribute-generalization reduct generation is also NP-hard.In terms of Corollary 3, we can make the following conclusions.Theorem 5Given a decision table S=〈U, C∪D, V, f〉 and AVTs(C), the problem of finding all attribute-generalization reducts is NP-hard.Given a decision table S=〈U, C∪D, V, f〉 and AVTs(C), the problem of finding minimal attribute-generalization reducts is NP-hard.Considering attribute a with its AVT(a), the number of all valid cuts can be recursively computed by#cuts(Root(a))=1+Πr∈Child(Root(a),a)#cuts(r),where Child(Root(a),a) is the set of all child nodes of Root(a).Obviously, the number of cuts grows exponentially with the number of internal nodes on AVT(a).Example 4All the valid single-dimensional cuts for each of the three attributes in Fig. 2a can be represented as a lattice of cuts, as in Fig. 4a. The multi-dimensional cuts for the three attributes can be represented as a lattice of cuts, as in Fig. 4b, and the corresponding lattice of generalized decision tables is shown in Fig. 4c.In Fig. 4b, the arrows illustrate the possible cut generalization (or refinement) paths that can be taken through the lattice. Each node represents a multi-dimensional cut. The top node 〈A0, B0, C0〉 represents the most generalization cut, i.e., Γroot, and the bottom node 〈A4, B2, C1〉 represents the most refinement cut, i.e., Γleaf. A series of connected paths from the bottom nodes to the top node is a cut generalization strategy. Conversely, a series of connected paths from the top node to the bottom nodes is a cut refinement strategy.Given a flat table and AVTs for condition attributes, a generalized decision table is induced with different condition attributes at different levels of abstraction. In Fig. 4c, each node represents a generalized decision table.From Corollary 2 and Definition 12, we can obtain two properties about the lattice of generalized decision tables. First, if a node N is found to be an attribute-generalization reduced table, then all nodes below N on the same generalization strategies that pass through N are also have the same classification ability of all condition attributes with respect to the decision attribute on the raw table. Second, if a node N is found not to be an attribute-generalization reduced table, then all nodes above N on the same generalization strategies that pass through N are also not attribute-generalization reduced tables.Our goal is to find an optimal node in Fig. 4c where the generalized decision table has the same classification ability in the raw decision table, while no other node exists that is more abstract than it. One of these nodes in Fig. 4b or corresponding node in Fig. 4c is the optimal solution and the objective of a heuristic reduction algorithm for attribute-generalization reduction is to find it efficiently.The general method of achieving this goal is using a heuristic search, which contains two important aspects: evaluation of a candidate node and search strategy through the lattice of multi-dimensional cuts. In this paper, we make use of conditional entropy for measuring classification ability for generalized decision tables, whereas the search strategy and attribute-generalization reduction method will be discussed in the Section 4.2.There are two different ways for hill-climbing search of the attribute-generalization reduct: top-down and bottom-up. Formally, a top-down search algorithm for attribute-generalization reduction can be formulated as follows.According to Corollary 2, the attribute-generalization reduct CΓcan be refined by starting from the most abstract level that is based on the most abstract value of each condition attribute (i.e. Γroot) and progressively refining the current cut using a criterion that is designed for classification accuracy.Our method iteratively refines an abstract value selected from the current cut and keeps the same classification ability with the raw table. According to Corollary 2, each refinement decreases the conditional entropy of C with reference to D (i.e., increases the classification ability). Which attribute value in the current cut should be selected is very difficult to decide for non-experts and even for experts. The key is selecting the best refinement at each step. At any iteration, the top-down refinement greedily selects the refinement on attribute value v that has the highest refinement quantitatively.To measure the benefits of a refinement quantitatively, a utility refinement significance measure is defined as follows.Definition 13Significance measureGiven a decision table S=〈U, C∪D, V, f〉 and AVTs(C). Consider a cut Γ∈ΩC, a∈C, and γa∈Γ. If v∈γaand v∉Leaf(a), then the significance of the refinement (i.e. v→Child(v, a)) is defined asSig(v,γa,Γ)=H(|D|CΓ)-H(|D|CΓ∗),whereΓ∗=Γ∪γa′-{γa},γa′={γa}∪Child(v,a)-{v},Γ∗is a refinement of a cut Γ by replacing v with its Child(v, a), H(D∣CΓ) is the conditional entropy of C on D before the refinement, andH(D|CΓ∗)is the conditional entropy of C on D after the refinement.Sig (v, γa, Γ) reflects the increment of the conditional entropy of C with respect to D, which means the conditional entropy decreases if we refine attribute value v in γa, and the increment of the conditional entropy is the significance of attribute value refinement.Consider the decision table and AVTs(C) in Fig. 1, where C={Job, Age, Sex} and D={Salary} illustrated. Two multi-dimensional cuts on AVTs(C) are as follows:Γ1={{ANY_Job},{ANY_Age},{ANY_Sex}},Γ2={{ANY_Job},{ANY_Age},{Male,Female}}.The refinement step from Γ1 to Γ2 refines ANY_Sex into Male and Female.Let γ3={ANY_Sex}.The calculation of the Sig of the refinement {ANY_Sex}→{Male, Female} is as follows.U/D={{x1,x2,x6,x8,x10,x12},{x3,x4,x5,x7,x9,x11}},U/CΓ1={U},U/CΓ2={{x1,x4,x5,x6,x9,x10,x11},{x2,x3,x7,x8,x12}}.Therefore,H(D|CΓ1)=1, andH(D|CΓ2)=0.9793.Computing the Sig of {ANY_Sex}→{Male, Female}, we obtainSig(ANY_Sex,γ3,Γ1)=H(D|CΓ1)-H(D|CΓ2)=0.0207.According to Theorem 1, a refinement v→Child(v, a) on attribute a replaces value v with one of the values in Child(v, a), where Child(v, a) contains all the children values of v in the AVT(a). After the refinement, the equivalence classes containing value v are partitioned into subgroups according to Child(v, a). Thus, the refinement may decrease the conditional entropy of C with respect to D, and improve its classification ability. For the current cut Γ, we select a given attribute value v, and if Γ∗ is obtained by replacing v with its children in Γ, theH(D|CΓ∗)decreases faster and the maxsize (H(D∣CΓ)−H(D∣CΓ∗)) is larger than by replacing any other attribute value, which can increase faster the classification ability.According Theorems 2 and 3, computing an attribute-generalization reduct can start from an attribute reduct and involve techniques such as top-down search strategies to perform attribute value refinement to the corresponding concept levels.From the above observations, we propose a novel algorithm named Attribute Generalization Reduction based on Shannon’s Conditional Entropy (AGR-SCE). The pseudo-code is shown in Table 1. In the pseudo-code, initially, the global cut Γ contains only the top most value for each condition attribute in an attribute reduct B. The valid refinements form the set of candidates to be performed next. At each iteration, first, for each valid v (i.e, not a leaf node) in γi(γi∈Γ), we compute its significance measure Sig(v, γi, Γ) (Lines 6–10). Secondly, we find the candidate of the highest Sig (va, γa, Γ), denoted va, and refine Γ by replacing vawith its Children (va, a) (Lines 11–12). Finally, we update SΓwith Γ, and compute H(D∣BΓ) (Line 13). The algorithm terminates when H(D∣BΓ)=H(D∣B), in which case it returns the attribute-generalization reduced table SΓtogether with the attribute-generalization reduct BΓ.An example is used to demonstrate the AGR-SCE algorithm below.Example 6Consider the decision table in Fig. 1a and the AVTs(C) illustrated as Fig. 1b. Fig. 5provides a step-by-step refinement procedure illustration to using AGR-SCE for attribute generalization reduction.To begin with, we have C={Job, Age, Sex}, D={Salary}, and U/D={{x1, x2, x6, x8, x10, x12}, {x3, x4, x5, x7, x9, x11}}.According to the representative attribute reduction algorithm (AR-SCE) proposed in [32], we obtain an attribute reduct B={Job, Age} and H(D∣B)=0.The top most generalized decision table contains one row on B with Γ={{ANY_Job}, {ANY_Age}}, and the class frequency is 6H, 6L.Let γ1={ANY_Job}, γ2=ANY_Age}, and Γ={γ1, (γ2}.From here, we can obtain U/BΓ={U} and H(D∣BΓ)=1.Note that H(D∣BΓ)≠H(D∣B), and then we calculate the Sig of the two candidate refinements:{ANY_Job}→{Professional, Artist},{ANY_Age}→{[28, 45), [45, 60)}.From here, we can obtain that.Sig (ANY_Job, γ1, Γ)=0, Sig (ANY_Age, γ2, Γ)=0.3113.According to the Sig criterion, ANY_Age will be first refined because it has the highest Sig. The result is shown in Fig. 5. Then, we can obtain that.γ2={[28, 45), [45, 60)},Γ={{ANY_Job}, γ2, {ANY_Sex}}, and H(D∣BΓ)=0.6887.Note that H(D∣BΓ)≠H(D∣B), and so we need to further calculate the Sig of the two candidate refinements:{ANY_Job}→{Professional, Artist},{[28, 45)}→{[28, 36), [36, 45)}.From here, we can obtain that.Sig (ANY_Job, γ1, Γ)=0.1887, Sig ([28, 45), γ2, Γ)=0.0137.According to the Sig criterion, ANY_Job will be further refined because it has the highest Sig. The result is shown in Fig. 5. Then, we obtain that.γ1={Professional, Artist},Γ={γ1, {[28, 45), [45, 60)}}, and H(D∣BΓ)=0.5.Note that H(D∣BΓ)≠H(D∣B), and so we need to further calculate the Sig of the three candidate refinements:{Professional}→{Engineer, Lawyer},{Artist}→{Dancer, Writer}, and{[28, 45)}→{[28, 36), [36, 45)}.From here, we can obtain that.Sig (Professional, γ1, Γ)=0.5,Sig (Artist, γ1, Γ)=0, and Sig ([28, 45), γ2, Γ)=0.4308.According to the Sig criterion, Professional will be further refined because it has the highest Sig. The result is shown in Fig. 5. Then, we obtain that.γ1={Engineer, Lawyer, Artist},Γ={γ1, {[28, 45), [45, 60)}}, and H(D∣BΓ)=0.Note that H(D∣BΓ)≠H(D∣B). Hence, BΓis an attribute-generalization reduct with Γ={{Engineer, Lawyer, Artist}, {[28, 45), [45, 60)}}.According to the attribute-generalization reduct BΓ, we obtain the attribute-generalization reduced table SΓin Fig. 6a. From the reduced table SΓ, we can obtain four generalized decision rules as follows.(1)If Job=“Artist” and Age=“[45, 60)”, then Salary=“Low”.If Job=“Artist” and Age=“[28, 45)”, then Salary=“High”.If Job=“Engineer” and Age=“[28, 45)”, then Salary=“Low”.If Job=“Lawyer” and Age=“[28, 45)”, then Salary=“High”.According to the attribute reduct B, we obtain the attribute reduced table S′ in Fig. 6b. From the reduced table S′, we can obtain nine decision rules at the original level.This example demonstrates that our method can obtain smaller rule sets with better generalized knowledge compared with the attribute reduction method.In this section, we empirically evaluate the proposed knowledge reduction method by comparing AGR-SCE with other knowledge reduction methods. Attribute reduction algorithms can remove the redundant and irrelevant attributes from the raw data while keeping or improving classification performances [8,16,23,25,34]. Therefore, we only compare the classification performances of various classifies on the attribute reduced data and attribute-generalization reduced data.We select three datasets from the UCI Repository of Machine Learning Databases, build attribute value taxonomies by combining each given data set with some relevant domain knowledge, and conduct the representative attribute reduction algorithm (AR-SCE) [30] on the datasets (Mushroom is preprocessed by removing the objects with missing values, whereas German Credity data are preprocessed by discretization with entropy). The attribute value taxonomy for each attribute in the reduced data is described in Tables 2–4.In the experiments, 11 classification model constructing methods are used from the Weka software: two Naive Bayes classifiers (NaiveBayes NB and BayesNet BN), one function model (RBFNetwork RBFN), one boosting algorithm (AdaBoostM1 AB), one bagging model (Bagging BA), one k-nearest neighbor algorithm (IBK), three types of decision trees (RandomForest RF, J48, and NBTree NBT), and two rule-based classifiers (PART and Prism). All classifiers are trained using the Weka default parameters. The classification accuracy and the numbers of rules are obtained by 10-fold cross-validation.The classification accuracy determines the ratio of successfully classified patterns by each classification model [2,26]. Fig. 7shows the comparison of average classification accuracies produced by the selected classifies for various reduced data.The figure reveals that the accuracy of models built on attribute-generalization reduced data by AGR-SCE is higher than the accuracy of models built on attribute reduced data by AR-SCE in most cases.The number of rules determines the complexity of the model [8,13,38]. Fig. 8shows the comparison of numbers of rules derived by four classifies for various reduced data. From Figs. 7 and 8, we can see although the number of rules of models built on attribute-generalization reduced data by AGR-SCE is less than the number of rules models built on the attribute reduced data by AR-SCE, the models built on attribute-generalization reduced data achieve higher classification accuracies.With the above experimental analysis, we can obtain the following conclusions: AGR-SCE can objectively control the generalization level that an attribute should be generalized to, accomplish better classification performance, and assist in computing smaller rule sets with better generalized knowledge compared with the attribute reduction method.

@&#CONCLUSIONS@&#
In this paper, we focused on investigating knowledge reduction for decision tables with AVTs to reduce both the number of attributes and the cardinalities of attribute domains and extended the attribute generalization method. Combining Shannon’s conditional entropy and attribute generalization, we proposed a novel concept for knowledge reduction, designated attribute-generalization reduct. We proved that finding a minimal attribute-generalization reduct is an NP-hard problem, and developed a heuristic algorithm for attribute-generalization reduction, which can produce a reduced table with fewer attributes and smaller attribute domains. Generalization levels are optimized based on the Shannon’s conditional entropy for keeping classification capability. Experimental results achieved on UCI datasets demonstrated the effectiveness of the proposed approach in improving the accuracy of different classifiers compared with the attribute reduction method. In particular, AVT integration was demonstrated to reduce the number of rule while improving the accuracy of the classifiers and datasets. Future studies will address the following: (i) the effect of attribute-generalization reduct using different classification ability measures in rough set theory, (ii) the development of more efficient attribute-generalization reduction algorithms (e.g., different generalization paths and pruning strategies) which integrate AVTs in classifier training, and (iii) considerations regarding not only classification ability but also coverage and strength.