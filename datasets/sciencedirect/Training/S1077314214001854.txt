@&#MAIN-TITLE@&#
New Gradient-Spatial-Structural Features for video script identification

@&#HIGHLIGHTS@&#
Dominant pixel selection by exploring gradient information with histogram.Proposing Gradient-Spatial-Features for text candidate selection.Proposing Gradient-Structural-Features in new way for text candidate selection.Determining weights based on templates for correct classification.Integrating Gradient-Spatial-Structural-Features in novel way for classification.

@&#KEYPHRASES@&#
Video text blocks,Gradient blocks,Dominant video text pixels,Gradient-Spatial-Structural-Features,Video script identification,

@&#ABSTRACT@&#
Multi-script identification helps in automatically selecting an appropriate OCR when video has several scripts; however, script identification in video frames is challenging because low resolution and complex background of video often cause disconnections or the loss of text information. This paper presents a novel idea that integrates the Gradient-Spatial-Features (GSpF) and the Gradient-Structural-Features (GStF) at block level based on an error factor and the weights of the features to identify six video scripts, namely, Arabic, Chinese, English, Japanese, Korean and Tamil. Horizontal and vertical gradient values are first computed for each text block to increase the contrast of text pixels. Then the method divides the horizontal and the vertical gradient blocks into two equal parts at the centroid in the horizontal direction. Histogram operation on each part is performed to select dominant text pixels from respective subparts of the horizontal and the vertical gradient blocks, which results in text components. After extracting GSpF and GStF from the text components, we finally propose to integrate the spatial and the structural features based on end points, intersection points, junction points and straightness of the skeleton of text components in a novel way to identify the scripts. The method is evaluated on 970 video frames of six scripts which involves font, font size or contrast variations, and is compared with an existing method in terms of classification rate. Experimental results show that the proposed method achieves 83.0% average classification rate for video script identification. The method is also evaluated by testing on noisy images and scanned low resolution documents, illustrating the robustness and the extensibility of the proposed Gradient-Spatial-Structural Features.

@&#INTRODUCTION@&#
Due to the advancement of IT technology and the increase in the usage of video that is delivered via TV broadcasting, Internet and wireless networks, the size of video database is increasing drastically nowadays [1–4]. In order to enable users to locate their interested content in such enormous quantity of video data quickly, there must be powerful indexing, retrieval and summarization techniques. In this regard, the current methods proposed in the field of content based image retrieval are not much effective for a large video database especially for retrieving and labeling video events, which requires high level semantics but not only low level features [5]. Since content based image retrieval methods are not effective in bridging the gap between high level semantics and low level features, text detection, script identification and recognition have come into play with the objective of filling the gap with the help of Optical Character Recognizer (OCR). In this way, text detection and recognition helps in generating meaning that is close to the content of video. Thus, text information can be used effectively for video labeling or events retrieval [1–5].Text detection and recognition is a familiar problem for the document analysis community where researchers detect and extract text information from scanned document images with a high resolution. Since a document image contains plane or homogenous background with a high resolution, text detection based on connected component analysis is a successful technique in this field [6,7]. However, the same method cannot be used for detecting and extracting text from natural scene images because of complex background, font, font size variations and color bleeding [4,7].To solve this problem, several methods [8–13] have been proposed in the literature based on bottom up analysis or region segmentation. However, these methods still enjoy the high resolution of text which gives high clarity and visibility compared to background. These methods expect directly or indirectly the complete shape of the character or consider the character as one component. Hence the methods depend much on connected component analysis. Thus, these methods may not be used directly on video for text detection and recognition because video suffers from both low resolution and complex background, which often cause disconnections, the loss of information, distortion, etc. It is evident that these methods [14,15] have so far achieved the character recognition rates only between 60% and 72%. This poor accuracy is due to the complex background, for example, usually sports video contains scene texts embedded in complex background with greenery, buildings, advertisements, etc. In addition, video usually contains two types of texts: graphics text which is superimposed and scene text which is a part of the image. Since graphics text is the edited one, it is easy to process and we can expect good clarity and visibility, whereas scene text is unpredictable due to the variations in its characteristics. Further, graphic text is useful for news video analysis while scene text is useful for navigation applications, such as assisting blind people, vehicle tracking, assisting tourists, sports events extraction, and exciting events extraction [1–4]. The presence of both such texts in video makes the problem more complex and challenging compared to text detection and recognition from either natural scene images or scanned images. In the same way, experimental results shows that when we apply conventional character recognition methods directly on video, the character recognition rate is achieved typically from 0% to 45% [16,17], which is much lower than the recognition rate of scene text in natural scene images. The poor accuracy is because of the presence of both graphics and scene texts, low resolution and complex background. Note that although we can see lots of cameras with a high resolution for capturing images and video, low resolution cameras are still required in many real life applications such as mobile services and surveillance systems because of the shortage of memory, battery power, computation power or even operating cost that limits to capture relatively low resolution [18,19]. Therefore, text extraction and recognition from video is still challenging and thus has become an emerging area for researchers.To address the problems of video, several methods have been proposed in the past years. These methods can be classified widely as follows: connected component-based [6,20], texture-based [21–24], and gradient or edge based methods [25–28]. Although these methods achieve a good accuracy for text detection or text block detection irrespective of fonts, font sizes, types of text and orientations, they do not have the ability to differentiate multi-script frames in video because the goal of these methods was to detect text block or text region regardless of scripts.Similarly, we can see scene text recognition from natural scene images [29–31] and from video [16,17,32,33], which takes the output of text detection, such as text region, blocks, lines, words and characters as the input for recognition. Generally, these methods take care of segmentation and binarization of text areas/blocks with the help of enhancement criteria to increase the contrast of text lines before feeding them to OCR [34]. Most of the methods either use publicly available OCR for binarization or their own classifier to recognize text in video. However, these methods are limited to a single script frame in video/image but not video containing multi-scripts because the extracted features and OCR are usually designed for a specific language. In addition, there is no universal OCR for recognizing different scripts in video since it is a hard task. Therefore, without script identification that helps automatically choosing appropriate OCR, there will be a big gap between text detection and recognition. Thus script identification is essential when the environment is of multi-script and multi-lingual like in Singapore, Malaysia and India, where we can see more than two official languages.Script identification for a given document image with plain background at block level, text line level and word level in the field of document analysis and recognition has attained high recognition rates [35–38]. In contrast to document images, script identification in video frame is a new topic and is challenging as it suffers from low resolution, complex background and font variation as mentioned above. Besides, the presence of multi-scripts in a video frame makes script identification and recognition more complex and challenging for researchers. As noted from the methods on script identification in document images [39,40], Japanese, Korean, Chinese and Tamil scripts worsen the performances of the methods in terms of classification rate because the scripts share common features. Therefore, there is a great demand for developing a new method for automatically identifying scripts in video frames irrespective of text orientation, font variation, font size variation and contrast. In this work, we consider six scripts, namely, Arabic, Chinese, English, Japanese, Korean and Tamil since these languages represent a wide range of users in the world and are very popular.

@&#CONCLUSIONS@&#
