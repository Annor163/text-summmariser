@&#MAIN-TITLE@&#
Exploiting projective geometry for view-invariant monocular human motion analysis in man-made environments

@&#HIGHLIGHTS@&#
We model body poses and silhouettes using a reduced set of training views.We exploit projective geometry and assume that people move on a known ground plane.We compensate for changes in silhouette due to a novel viewpoint using a homography.The homography can be used in a bottom-up or a top-down manner.The homographic alignment outperforms the commonly used similarity alignment.

@&#KEYPHRASES@&#
Human motion analysis,Projective geometry,View-invariance,Video-surveillance,

@&#ABSTRACT@&#
Example-based approaches have been very successful for human motion analysis but their accuracy strongly depends on the similarity of the viewpoint in testing and training images. In practice, roof-top cameras are widely used for video surveillance and are usually placed at a significant angle from the floor, which is different from typical training viewpoints. We present a methodology for view-invariant monocular human motion analysis in man-made environments in which we exploit some properties of projective geometry and the presence of numerous easy-to-detect straight lines. We also assume that observed people move on a known ground plane. First, we model body poses and silhouettes using a reduced set of training views. Then, during the online stage, the homography that relates the selected training plane to the input image points is calculated using the dominant 3D directions of the scene, the location on the ground plane and the camera view in both training and testing images. This homographic transformation is used to compensate for the changes in silhouette due to the novel viewpoint. In our experiments, we show that it can be employed in a bottom-up manner to align the input image to the training plane and process it with the corresponding view-based silhouette model, or top-down to project a candidate silhouette and match it in the image. We present qualitative and quantitative results on the CAVIAR dataset using both bottom-up and top-down types of framework and demonstrate the significant improvements of the proposed homographic alignment over a commonly used similarity transform.

@&#INTRODUCTION@&#
In recent years, the number of cameras deployed for surveillance and safety in urban environments has increased considerably in part due to their falling cost. The potential benefit of an automatic video understanding system in surveillance applications has stimulated much research in computer vision, especially in the areas related to human motion analysis. The hope is that an automatic video understanding system would enable a single operator to monitor many cameras over wide areas more reliably.Example-based approaches have been very successful in the different stages of human motion analysis: detection, pose estimation and tracking. Some consist of comparing the observed image with a data base of stored samples as in [1–3]. In some other cases, the training examples are used to learn a mapping between image feature space and 3D pose space [4–7]. Such mappings can be used in a bottom-up discriminative way [8] to directly infer a pose from an appearance descriptor or in a top-down generative manner [7] through a framework (e.g., a particle filter) where pose hypotheses are made and their appearances aligned with the image to evaluate the corresponding observation likelihood or cost function. The exemplars can also be used to train binary human detectors [9–12], multi-class pose classifiers [13–15] or part-based detectors [16–20] that are later employed to scan images. The main disadvantage of all these example-based techniques is their direct dependence on the point of view: the accuracy of the result strongly depends on the similarity of the camera viewpoint between testing and training images.Ideally, to deal with viewpoint dependency, one could generate training data from infinitely many camera viewpoints, ensuring that any possible camera viewpoint could be handled. Unfortunately, this set-up is physically impossible and makes the use of real data infeasible. It could, however, be simulated by using synthetic data, but using a large number of views would drastically increase the size of the training data. This would make the analysis much more complicated; furthermore, the problem is exacerbated when considering more actions.In practice, roof-top cameras are widely used for video surveillance applications and are usually placed at a significant angle from the floor (see Fig. 1a), which is different from typical training viewpoints as shown in the examples in Fig. 1c. Perspective effects can deform the human appearance (e.g., silhouette features) in ways that prevent traditional techniques from being applied correctly. Freeing algorithms from the viewpoint dependency and solving the problem of perspective deformations is an urgent requirement for further practical applications in video-surveillance.The goal of this work is to track and estimate the pose of multiple people independently of the point of view from which the scene is observed (see Fig. 1b), even in cases of high tilt angles and perspective distortion. The idea is to model body pose manifold and image features (e.g., shape) using as few training views as possible. The challenge is then to make use of these models successfully on any possible sequence taken from a single fixed camera with an arbitrary viewing angle. A solution is proposed to the paradigm of “View-insensitive process using view-based tools” for video-surveillance applications in man-made environments: supposing that the observed person walks on a planar ground in a calibrated environment, we propose to compute the homography relating the image points to the training plane of the selected viewpoint. This homographic transformation can potentially be used in a bottom-up manner to align the input image to the training plane and process it with the corresponding view-based model, or top-down to project a candidate silhouette and match it in the image. In the presented work, we focus on specific motion sequences (walking), although our algorithm can be generalized for any action.

@&#CONCLUSIONS@&#
In this paper, we have presented a method for view invariant monocular human motion analysis in man-made environments. We have assumed that the camera is calibrated w.r.t. the scene and that observed people move on a known ground plane, which are realistic assumptions in surveillance scenarios. Then, we have proposed to discretize the camera viewpoint into a series of training viewpoints and align input and training images. We have demonstrated that exploiting projective geometry alleviates the problems caused by roof-top and overhead cameras with high tilt angles, and have shown that using 8 training views was enough to produce acceptable results when using the proposed projective alignment in a silhouette-based motion analysis framework.We have analyzed the results obtained when this homographic transformation is included within two different frameworks: (1) a bottom-up image analysis system where the homography is used to align an input image to a selected training plane for a view-based processing, and (2) a top-down tracking framework where the inverse homography is employed to transform and project candidate silhouettes in the image.We have conducted a series of experiments to quantitatively and qualitatively evaluate this projective alignment for a variety of sequences with perspective distortion, some with multiple interacting subjects and occlusions. In our experimental evaluation, we have demonstrated the significant improvements of the proposed projective alignment over a commonly used similarity alignment and have provided numerical pose tracking results which demonstrate that the incorporation of this perspective correction in the top-down pose tracking framework results in a higher tracking rate and allows for a better estimation of body poses under wide viewpoint variations.We have also analyzed the limitations of the proposed method in the bottom-up framework by evaluating its sensitivity to noisy measurements. We have observed that the result depends on the accuracy achieved when estimating both location on the ground floor and the orientation of the subject with respect to the camera. This problem does not appear when following a stochastic approach for estimating the optimum projective transformation by sampling multiple possible values for the camera viewpoint at multiple locations. Even if our results show that the top-down framework outperforms the bottom-up one, we believe that bottom-up techniques could benefit from a more sophisticated tracker and might outperform top-down approaches as different types of image features could then be employed.Even if all the presented experiments are specific to the walking activity (due to the higher availability of training and evaluation datasets), our method is general enough to extend to other activities. The limited number of required training views makes our work easily extendable to more activities and makes more feasible the development of future action recognition software in real surveillance applications.