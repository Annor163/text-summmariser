@&#MAIN-TITLE@&#
Multiple-path selection for new highway alignments using discrete algorithms

@&#HIGHLIGHTS@&#
Multiple dissimilar paths are computed within time and quality constraints.56 percent speed improvement without quality reduction.Bidirectional search using A* and simple height restriction outperforms other approaches.

@&#KEYPHRASES@&#
Horizontal alignment optimization,Shortest path,A* algorithm,k-shortest path,OR in road design (natural resources),

@&#ABSTRACT@&#
This paper addresses the problem of finding multiple near-optimal, spatially-dissimilar paths that can be considered as alternatives in the decision making process, for finding optimal corridors in which to construct a new road. We further consider combinations of techniques for reducing the costs associated with the computation and increasing the accuracy of the cost formulation. Numerical results for five algorithms to solve the dissimilar multipath problem show that a “bidirectional approach” yields the fastest running times and the most robust algorithm. Further modifications of the algorithms to reduce the running time were tested and it is shown that running time can be reduced by an average of 56 percent without compromising the quality of the results.

@&#INTRODUCTION@&#
In preliminary road design, selecting the best path for a new road is traditionally a long and political process. A variety of factors can require that road engineers design multiple alternative paths to be considered. Previous research has developed methods of modeling the road’s costs as well as computing the optimal alignment (Trietsch, 1987b; Turner, 1978). While this software provides useful insight for road engineers, it does not satisfy the need to find multiple road path options for review. There is a need for an algorithm that can efficiently compute multiple near-optimal, but distinctly different, paths that can serve as alternatives to the cheapest path.This paper is concerned with the first step of the road design process: corridor selection. An initial path can then be refined, optimizing the horizontal alignment within the corridor, using techniques from Mondal, Lucet, and Hare (2014), Hare, Koch, and Lucet (2011), Hare, Hossain, Lucet, and Rahman (2014a), Hare, Lucet, and Rahman (2014b), or citations therein.To locate the initial corridor we model the terrain as a three dimensional grid of points to construct a spatial graph. Adapted forms of Dijkstra’s Algorithm are then used to select the set of spatially-dissimilar corridors, while minimizing an approximation of the earthwork and pavement costs. Additional techniques to reduce the running time have also been developed. These include an adapted form of the A* algorithm and two methods of imposing height restrictions, to avoid spending time searching for unrealistic roads that are far from the ground.Previous work on road alignment selection has developed a number of discrete models. The simplest and most common is a regular grid (Turner, 1978), where the center of each grid cell is given a vertex and then edges are defined by the vertices of the eight adjacent grid cells. This model restricts possible road trajectories to having eight directions. Another method, presented by Trietsch (1987b), Trietsch (1987a), used a honey-comb grid, which allows for angles in multiples of 30  degree. In many previous discrete models the space between each adjacent vertex was as small as 200 meters and up to 2 kilometers (Trietsch, 1987a; 1987b; Turner, 1978), which does not allow for detailed and accurate assessment of costs in a given region of a cell. Many models also formulated the construction costs independently of the road’s direction (Turner, 1978).On the horizontal alignment problem, Huber and Church (1985) took an in-depth look at minimizing the errors in the cost evaluation associated with path planning problems, such as road design, by increasing the number of possible directions of movement. Later, Lee, Tsou, and Liu (2009) applied a neighbourhood search technique to approximate horizontal alignments with a piecewise-linear curve. Once they selected a horizontal alignment they refined it to be a smooth path meeting road standards such as curvature restraints. Easa and Mehmood (2008) used collision frequency data for various road types to improve the safety of horizontal alignments. Kang, Schonfeld, and Jong (2007) improved an existing Horizontal Alignment Optimization (HAO) model by restricting the search space with feasible gates.Kang (2008) used genetic algorithms to choose new highway segments that intersect with an existing highway network. A bi-level approach was developed to first select candidate paths with intermediate solutions to the genetic algorithm and then evaluate them for traffic flow optimization (Kang, Jha, & Schonfeld, 2012; Kang, Yang, Schonfeld, & Jha, 2010).Jha (2003) approached the road design problem with genetic algorithms. Further work was done to integrate GIS to include the costs of land boundaries, environmental impact, topography, travel time, and noise and air pollution of highways near cities (Jha & Kim, 2006; Jha & Schonfeld, 2004). Jha, Schonfeld, Jong, and Kim (2006) reviewed cost formulation as well as common road design optimization algorithms.Continuing this work (Yang, Kang, Schonfeld, & Jha, 2014) adapted the genetic algorithms to find alternate routes. Bosurgi et al. considered environmental constraints using particle swarm optimization (Bosurgi, Pellegrino, & Sollazzo, 2013). They also added new types of curves and provided a genetic algorithm approach to optimize the parameters (Bosurgi, Pellegrino, & Sollazzo, 2014). Shafahi and Bagherian proposed a customized particle swarm optimization algorithm (Shafahi & Bagherian, 2013) while another 3D highway alignment model solved by evolutionary algorithms was considered by Jong and Schonfeld (2003). It is also worth mentioning Jong’s Ph.D. thesis (Jong, 1998).User interface designs have been developed and proposed for the road design problem. Church, Loban, and Lombard (1992) designed one using a multi-objective model to allow finding both optimal and alternative paths.Many of the previous road design optimization methods are focused on finding a single path or corridor. In practice, road design is a political process in which it is impossible to determine and evaluate all of the environmental and political cost factors ahead of time. Rather than using a multi-objective model with what will surely be an incomplete set of costs and constraints, this paper focuses on finding k spatially dissimilar paths of nearly equal cost that can be reviewed by a board of engineers, politicians, and/or environmentalists for additional costs and impacts.Extensive research has been done on the problem of finding the k-shortest-paths in a network (Brander & Sinclair, 1995; Eppstein, 1998; Hoffman & Pavley, 1959; Lawler, 1972; Yen, 1971). In general there are two different approaches: deletion algorithms (Brander & Sinclair, 1995; Lawler, 1972; Yen, 1971) and deviation algorithms (Eppstein, 1998; Hoffman & Pavley, 1959).Deletion algorithms propose using a conventional path-finding algorithm, such as Dijkstra’s, to find the optimal path. The edges from the optimal path are successively deleted from the graph and the path-finding algorithm is re-run to generate a set of secondary paths. The cheapest among them is selected and the process iterates.Deviation algorithms use the information generated by a shortest-path tree to the destination to exploit the frequent locality of the k-shortest-paths (Eppstein, 1998; Hoffman & Pavley, 1959). They begin with the cheapest path and then search for the deviation that offers the smallest increase in cost. While this property of the k-shortest-paths is what allows them to achieve the most competitive time complexity, it also offers insight as to why it, and indeed any of the conventional k-shortest-path algorithms, are not well-suited for adaptation to the spatially-dissimilar multipath problem.Both families of algorithms perform poorly when applied to the multipath road design problem, which corresponds to a k-shortest-path problem applied to a dense spatial graph11We call a dense spatial graph, a spatial graph with vertices having degree 26 except on the boundary.with the additional complication of finding spatially dissimilar paths.As an academic example consider the behavior of a k-shortest-path algorithm on a uniform cost grid (something that loosely approximates a very flat prairie). The globally optimal path would be the straight line p1 seen in Fig. 1. Paths p2, p3, and p4 would all be equally priced and so any of them may be the second path found depending on how the algorithm breaks ties. Of the three options presented p4 would rank as the most desirable since it is the most different from p1. However, p4 is still nearly identical to p1 when using a refined grid and so none of these paths should be considered acceptable as candidate alternate paths to p1.The simplest way to adapt these algorithms to the multipath road-design problem is to iteratively generate paths until we have found k paths that satisfy our spatial dissimilarity criterion. Since early iterations of this approach would be spatially similar paths, this method would have to be iterated quite extensively before good alternative routes are found. As such, this approach performs poorly in practice.The dissimilar path problem is one often proposed in the context of transporting hazardous materials across highway networks (Dell’Olmo, Gentili, & Scozzari, 2005; Johnson, Joy, Clarke, & Jacobi, 1992; Kang, Batta, & Kwon, 2014). The goal of the dissimilar multipath problem is to find a set of spatially dissimilar paths between a source and a destination. A variety of dissimilarity indices have been used to approach this problem. For two given paths many of them define a relationship between the shared length of the paths and the non-shared length of the paths (Akgün, Erkut, & Batta, 2000; Kang et al., 2014; Kuby, Zhongyi, & Xiaodong, 1997). This method is well-suited to the hazardous material (HazMat) objective function which is based on the value of risk rather than the transportation costs. It yields local minima that are spatially diverse. Dissimilar path algorithms are better suited to finding alternative paths for road design than standard k-shortest-path algorithms. However, in the context of road design this method will find paths such as p4 seen in Fig. 1. These paths will share a minimal amount of length, but will be spatially very similar, due to the dense nature of the road design grid. Indeed, minimizing the shared length of paths is best suited to existing highway networks where alternate paths consist of different highway options.Dell’Olmo et al. (2005), also in the context of HazMat transportation, introduce an interesting modification to previous dissimilarity indices by defining a buffer of area around each of the paths and calculating an index determined by the area of overlap. This method is particularly useful in HazMat transportation where the buffer area can be computed as the expected area that would be affected by an accident.Another metric used by Marti, Campos, Resende, and Duarte (2015), Marti, Velarde, and Duarte (2009) is a possible option that could have provided meaningful candidate alternative paths for road design. The distance metric used by Marti computes the average of the shortest distance from each vertex on path p1 to p2 and vice versa. The average distances of each path are normalized by the lengths of the respective paths and then the average of the two paths is taken as the dissimilarity index.While both of the previous two methods have the potential to provide meaningful candidate alternative paths for road design, we opted to use another dissimilarity criterion, similar to one proposed by Lombard and Church (1993). Their metric looks at the area difference between two paths using a method analogous to computing the absolute difference of the area under two curves. This method was chosen out of the three since it has equal potential to providing significantly different paths, while requiring the least computational expense.Most of the dissimilar path algorithms in the literature use a dissimilarity criterion related to the shared length of the paths and many only compare the alternate paths to the single cheapest path rather than to each other. This paper uses the area difference between two paths as a metric to ensure that the paths are spatially different. This type of metric is required for the dense graph used for the road design model, since it ensures that small deviations as seen in Fig. 1 are not accepted for alternate paths. This paper also measures the area between each path to ensure that no two alternate paths are spatially similar.Several types of algorithms have been used to generate paths for the various dissimilarity criteria. A standard method among them is the Iterative Penalty Method (Akgün et al., 2000; Johnson et al., 1992; Rouphail, Ranjithan, El Dessouki, & Smight, 1996). Turner (1978) applied a similar method to the multipath road design problem by increasing the weights of the edges of the original path. We adapt this idea for the road design problem with spatially dense graphs in Section 4.2.Marti et al. (2015), Marti et al. (2009) approach the problem using a Greedy Randomized Adaptive Search Procedure (GRASP) with Path Relinking. Jha (2003) used Genetic Algorithms on the problem of road design and returned intermediate solutions as candidate alternate paths. Later, Kang et al. (2012), Kang et al. (2010) used Genetic Algorithms to generate path alternatives designing new highways that intersect with an existing highway network. Yang et al. (2014) build on this work using a multi-objective model and modifying the Genetic Algorithm. Genetic Algorithms were also used by Zhang and Armstrong (2005) for the multi-objective corridor problem. We did not study genetic algorithms, or GRASP, in this paper in favor of deterministic algorithms. Future research should explore comparisons between the approaches herein and nondeterministic methods, as well as explore the possibilities of hybrid approaches.A different method on multi-objective corridors was used by Dell’Olmo et al. (2005) to find the pareto-optimal set of non-dominated paths. Despite the different objectives we suggest an algorithm of a similar nature in Section 4.3.A common strategy is to generate a large set of candidate paths and then reduce this set based on the chosen dissimilarity criteria. Kuby et al. (1997) used a minimax method to select the set of paths which maximized the minimum dispersity between the paths, a problem which is known to be NP-hard (Duarte & Marti, 2007). On HazMat transportation (Kang et al., 2014) generate their initial path set using regular k-shortest-paths algorithm. This is appropriate for their problem since their objective function is based primarily on risk rather than travel costs. This objective function, unlike one based on construction costs, will often produce kth best paths that are spatially dissimilar.The Gateway Shortest-Paths method was introduced by Church et al. (1992), Lombard and Church (1993) as a method of generating a set of spatially dissimilar candidate paths. We use a similar path generation method in Section 4.4. A very recent method was developed by Scaparra, Church, and Medrano (2014) using a multi-gateway shortest-path method. This algorithm shows promise in generating a large number of candidate paths, but requires additional time, requiring a shortest-path algorithm to be run for every vertex in the graph. Given the density of vertices in the road design network this approach would take too long to run and would generate more paths than necessary to select reasonable alternatives.We introduce our road design model in Section 2 and discuss two height restrictions to decrease running time in Sections 2.4 and 2.5. Section 3 contains the dissimilarity constraint required for our corridors. Five dissimilar multipath algorithms are presented and their theoretical performance is discussed in Section 4. Numerical results were collected and are summarized in Section 5. Section 6 contains some concluding remarks and ideas for further improvements in future work.In order to achieve accurate cost assessments and realistic paths we are using a very dense grid with approximately 10 meters between each vertex. However, with the increased detail of the model a new challenge arises: an alignment with a 90  degree turn (or more) between two vertices will have to take place in a very short space. This abrupt turn will generally violate road safety curvature constraints, making these alignments infeasible. We begin by introducing a constraint that the sharpest turn any road can take is a 45  degree angle. While this constraint may prove sufficient for some applications, e.g. logging roads, two consecutive 45  degree turns would still yield an unsafe highway alignment. However, this method is still sufficient for producing an initial corridor in which further curvature constraints can then be applied during the fine-tuning of the horizontal alignment optimization process.Simply storing the current direction and applying this restriction results in a constrained shortest-path problem. In order to use classic shortest path algorithms, such as Dijkstra’s Algorithm, we create a new 2D-ω’ model based on an augmented graph whose shortest-path solution corresponds to a solution to the constrained shortest-path problem. Trietsch (1987b) used this idea with hexagonal grids to apply curvature constraints to preliminary road designs.The augmented graph has eight vertices for each (x, y) point, each one corresponding to an incoming edge orientation, see Fig. 2. A simple way of abstracting this information is to consider the direction of travel as another axis, let this be the ωh-axis, for horizontal orientation. Each direction can then be assigned a coordinate, 0–7.Now that we have a new coordinate, we need to modify our original coordinate system. Let v(x, y, ωh) be a vertex with Euclidean coordinates (x, y) and orientation ωh. A vertex with orientation ωh, will then have an outgoing edge to vertices with orientation h′, whereh′∈{ωh−1,ωh,ωh+1,(mod8).Theorem 2.1Finding the shortest-path with the 2D-ω model can be solved in log-linear time.In the original model, every vertex has at most 8 edges, one to each adjacent vertex (with vertices on the boundary having slightly less edges). So if our map contains V vertices, we have at mostE≤8V/2=4Vedges.In our new model, we have 8 different possible orientations per vertex. Let V′ be the new number of vertices, which is nowV′=8V.However, for this model, since we are restricting the possible directions of travel to 45  degree angles, we only have 3 possible outgoing and symmetrically 3 incoming edges at each of our augmented vertices, i.e., we have at mostE′≤6V′/2=24V=6Eedges.Thus, the number of vertices and edges is increased by a constant factor, which means that Dijkstra’s Algorithm will still run in log-linear time.□An added benefit of this model is that without any further modification, we can now easily improve the accuracy of our cost assessments. Since we are now storing the direction of travel, we can assign different costs to each edge, based not only on their location, but also their orientation. This increases the accuracy of the cost function, as road alignments traveling up a hill vary significantly in cost to those traveling along a hill.In order to extend our 2D-ω model to three dimensions, we add another axis, call it ωv, for vertical orientation. Unlike ωh, ωvonly needs 3 orientations: –1, moving down; 0 maintaining elevation; and 1, moving up. Note v(x, y, z, ωh, ωv) the vertex with Euclidean coordinates (x, y, z) and orientation (ωh, ωv). A vertex with orientation (ωh, ωv) is then a vertex with an incoming edge with orientation (ωh, ωv) and an outgoing edge to vertices with orientation(h′,ωv′),whereh′∈{ωh−1,ωh,ωh+1,(mod8),ωv′∈{min(ωv+1,1),ωv,max(ωv−1,−1).Theorem 2.2Finding the shortest-path with the 3D-ω model can be solved in log-linear time.Similar to our 2D-ω model, the number of vertices and edges increases by a constant factor. The basic 3D model has at most 24 edges per vertex (with boundary vertices having slightly less). If our map contains V vertices, then we have at mostE≤24V/2=12Vedges.In the new model we have 24 different possible orientations per vertex, which increases our number of vertices toV′=24V.Since we are restricting the possible directions of travel to 45  degree angles, we have up to 9 outgoing and 9 incoming edges at each of our augmented vertices (vertices withωv=0have 9 and vertices withωv=−1or1have 6). Which means we have at mostE′≤18V′/2=9(24V)≤18Eedges.The number of vertices and edges increases by a constant factor, which means that Dijkstra’s Algorithm will still run in log-linear time.□In practice Dijkstra’s Algorithm is able to find solutions, but requires significant computational time due to the size of the grid used. We discuss three methods of adapting Dijkstra’s Algorithm specifically for the road design problem in order to reduce the overall running time required. These modifications can also be used in combination to speed up all of the dissimilar multipath algorithms in Section 4 which use Dijkstra’s Algorithm as their underlying shortest-path algorithm.The A* algorithm (Delling, Sanders, Schultes, & Wagner, 2009) is one that is commonly used as it maintains the same worst-case complexity as Dijkstra’s Algorithm, but in practice it has a faster expected running time. It uses a heuristic for a lower bound of the distance remaining to the destination and modifies the arc weights accordingly. For our application we are using the cost of paving a straight road to the destination. When used in the Bidirectional Selection Algorithm presented in Section 4.4 we modify the heuristic using the formula given by Ikeda, Hsu, Imai, Nishimura, Shimoura, et al. (1994), which provides the best known running time without losing global optimality.In this model costs are assigned to each edge in the graph. Depending on the information available, a large variety of costs could be incorporated into the edge cost such as paving, earthwork, land acquisition, user costs, expected accident rates, etc. Some costs, such as expected accident rates or bridge construction, may be more complicated to quantify as the cost cannot be determined without considering the surrounding edges used in the road.For the numerical results in Section 5.2, we used only earthwork and paving costs. This approach was adopted to make edge costs easy to compute. The paving costs are simply proportional to the length of the edge, while the earthwork costs are based on the volume of earth excavation or embankment. Earth excavation and embankment was calculated using the height of the edge relative to the elevation profile of the ground. Namely, the ground is a piecewise linear function while the edge is linear so for each linear piece of the ground, we compute the area difference with the edge and multiply by the width of the road to obtain the volume. While changing cost structures may alter the final solutions determined, we believe that algorithmic performance would only be minimally effected.We note that it is more expensive to build roads that are far above or below the ground than to build roads along the ground. This trend is not exploited by Dijkstra’s Algorithm which wastes time searching unrealistic paths. We introduce a simple novel constraint that restricts how far from the ground the algorithm can search. We note the following parameters:•Hmax maximum displacement allowed from the ground,R radius checked around a given point.In the event the ground changes elevation abruptly, we may need to build a slope to allow our road to change from one elevation to the other. As a result, for these parts of the map we will need to allow the algorithm to search farther away from the ground, which is why we include our radius R. Let Z(x, y) be the elevation of the ground at (x, y). Then for any position(x¯,y¯),a maximum distance above the ground,δh+(x¯,y¯),is computed byδh+(x¯,y¯)=max(max{Z(xi,yj)},Z(x¯,y¯)+Hmax)−Z(x¯,y¯),whereZ(xi,yi)={(xi,yj):x¯−R≤xi≤x¯+Ry¯−R≤yj≤y¯+R}.Symmetrically, we have the maximum distance below the groundδh−(x,y)=min(min{Z(xi,yj)},Z(x¯,y¯)−Hmax)−Z(x¯,y¯).If R is set to 0, then only vertices within Hmax of the ground will be searched, which imposes the height restriction. The value of R increases the radius searched at each point when checking to see if the ground elevation abruptly changes. If the ground within the radius extends beyond Hmax, then the height restriction is relaxed to allow these points to be connected.As Hmax and R decrease, significant reductions in computation time can be observed. However, one must be careful to not over-constrain the problem, which could remove desirable roads from being considered.The Expanding Height Restriction method was designed to initially only consider a small subset of vertices that are near the ground. A set of rules were devised to expand this subset when necessary to ensure that the problem was not over-constrained, removing promising candidate paths.In particular there are two cases where building a road that is not near the initial ground elevation may have been cheaper. The first is when a cliff steeper than the maximum permissible road grade needs to be climbed, which may require a “ramp”-like structure to build the road. The second corresponds to when it is cheaper to build a straight road through an obstacle rather than to incur the added pavement costs of building a longer road to avoid the obstacle.In numerical experiments (see Section 5), these rules had no effect on solution quality, but significantly slowed solution time.We use an area constraint between each of the k alternate paths selected by the algorithms to ensure that the alternate paths are not small deviations of the globally optimal path.Definition 3.1We denote by δA(p, q) the percentage area difference of paths p and q. The percentage area difference between two paths is found by first projecting the paths onto an x-y plane, computing the area between the two paths, and then dividing by the area of a rectangle whose width is equal to the width of the map and whose length is equal to the straight-line distance between the endpoints of the paths. See Fig. 3.We denote by δAmin the minimum percentage area difference required between each of the alternate paths selected.Lemma 3.2The worst-case time complexity to compute the area between two paths is O(L), where L is the number of vertices in the longest path found. Note that L ≪ n, where n is the number of vertices.The area computed between two paths is computed analogously to the integral of the absolute difference of the area under each path. This requires passing the length of the paths three times: once each to compute the area under each vertex and once more to compute the absolute difference. Since each pass requires O(L) operations, the total time complexity is O(L).□This dissimilarity constraint was selected both for its relatively low computation time and its simplicity in implementation. While there are many other possible choices for dissimilarity metrics and this may not be the best one, this was believed to be an adequate metric to enforce spatial path dissimilarity. The constraint is related to the width of the map to help avoid over-constraining the problem. Should the dissimilarity constraint be set too high, most problems will become infeasible. By incorporating the width of the corridor into the constraint, future work can be done to determine a range of values which will typically yield feasible solutions with maximal dissimilarity.This Section presents the dissimilarity criterion used by the five dissimilar multipath algorithms presented in the following subsections. Pseudo-code algorithms are given and their theoretical performance is discussed.The Sensitive Elimination Method is an original algorithm similar in concept to deletion algorithms for finding the k-shortest paths (Brander & Sinclair, 1995; Lawler, 1972; Yen, 1971). Instead of removing each of the edges in the optimal path, this method seeks to identify a promising edge to cut. The Sensitive Elimination Method begins by computing the optimal path using Dijkstra’s Algorithm. Next, some “sensitivity analysis” is performed for each edge in the optimal path, to determine which edge is the most sensitive. Then that part of the map is removed from the set of feasible edges and the shortest-path algorithm is again run on this reduced map. We use the following notation:•w ≥ 1 – sensitivity width,k > 1 – target number of dissimilar paths,δAmin – minimum area difference required from every other path as a percent,δCmax – maximum price difference from the optimal path, as a percent.The sensitivity of a given incoming edge is computed by finding the difference in price of building an edge with the same vertical position and orientation, (z, ωh, ωv), but shifted to the left and right byd=1,2,...w. This then produces an array of cost differences to the right and an array of cost differences to the left. Then, the sum of the absolute value of each array is multiplied to give a single numerical representation of the sensitivity of that edge.The cost model in this paper uses earthwork costs and paving costs. Since the segments to the left and right are the same length, the paving costs will be the same. The difference in costs are based on the earthwork costs, which in turn is based on the displacement of the road segment from the ground. For simplicity, instead of computing the difference in edge costs, we use a simple surrogate and compute the difference in elevation at the vertex to which the edge points.Example 4.1Consider computing the sensitivity withw=2of the edge going into vertex A. Let the black line in Fig. 4 represent the elevation profile to the left and right of vertex A. The elevation differences to the left and right are[−0.8,−0.5]and[0.1,−0.2]respectively. Taking the absolute value and summing the elements of the arrays gives 1.3 and 0.3 respectively. Finally, we multiply these scores to get a raw index of 0.39, which can be compared with other sensitivity scores. We note that in this example the left side is considered more sensitive than the right, but the final sensitivity index is closer to that of the right. We chose to multiply the sensitivity values of the left and the right so that edges that are sensitive on both sides are selected first.For a given optimal path, let A be the vertex pointed to by the edge most sensitive to local perturbations. Instead of only removing A, we also eliminate all of the vertices with (x, y) coordinates equal to those of the edges to the left and right of A that were used to compute the sensitivity of A. By doing this, we have then created a wall in our map centered around the most sensitive vertices of our original path, through which any subsequent alternative paths cannot pass. See Fig. 5.Algorithmwhile we have not yet found k pathsCompute the optimal path.if no path was foundReplace the edges that were cut from the previousiteration.Select the next most sensitive, untried edge of theprevious path found.Remove it and those to the left and right.else if the new path is too expensiveStop. subsequent paths will also be too expensive.else if the new path is too similar to one of the old pathsReplace the edges that were cut from the previous iteration.Select the next most sensitive, untried edge of the previous path found.Remove it and those to the left and right.elseSave the newly found path.Compute the sensitivity of each edge.Select the most sensitive edge.Remove it and those to the left and rightendendOne of the problems with this method, is that it may require computing many paths that do not get used, a fact that is reflected in the worst-case time complexity.Proposition 4.2The worst-case time complexity for the Sensitive Elimination Algorithm isO(kLnlog(n)+k2L2),where n is the number of vertices, k is the number of paths to be found and L is the number of vertices of the longest path considered, with L ≪ n.The algorithm first finds the optimal path containing up to L vertices. It is possible for all but one cut to result in either an infeasible map, or an alternate path that is too similar to another path. This can then result in Dijkstra’s Algorithm being run up to L times to find a single feasible alternative path. Since we need to findk−1alternate paths, we may need to run Dijkstra’s AlgorithmO(L(k−1))times, assuming that only the Lth iteration results in an acceptable path each time.The predicted best cut is selected from each of the L choices, which requires L operations to select the maximum value. Dijkstra’s Algorithm needs to be run for each attempted cut. Dijkstra’s Algorithm requires O(nlog (n)), giving usO(nlog(n)+L)to generate each possible alternate path. Each time a new path is found it needs to be compared with the O(k) other paths, requiring O(kL) operations (see Lemma 3.2). Combining these gives usO(nlog(n)+(k+1)L)operations each time a new path is considered.Since we may need to repeat Dijkstra’s Algorithm at mostL(k−1)times, this yields a total time complexity ofO(L(k−1)(nlog(n)+(k+1)L)),or more simplyO(kLnlog(n)+k2L2).□A common method for finding alternative paths is to iteratively apply cumulative penalties along the shortest-paths (Akgün et al., 2000; Johnson et al., 1992; Rouphail et al., 1996). First, a shortest-path algorithm is run to find the optimal solution. Weights along the shortest-path are then increased by a penalty and the shortest-path algorithm is run again to find a second alternative path. This process is repeated iteratively to find the desired number of paths.Due to the dense nature of our graphs if we were to apply this method directly we would find that the secondary path found would often simply be the original path shifted 10 meters to the left or right to avoid the penalty, which does not provide us with sufficiently distinct alternate paths. We adapt existing methods by not only increasing the weight of the optimal path, but by also increasing the weight of a corridor-like buffer around the optimal path to discourage the alternative paths from being chosen too close to the original paths. This is achieved by adding a weight to the optimal path and decreasing that weight linearly on both sides of the path till it reaches zero. The effect of the buffer is to consider that any minor variation of the optimal path can be built by the engineer on the ground since our goal is to identify the corridor, not compute a precise path. Hence, the buffer forces any new solution to be clearly distinct.Iterative Penalty methods inherently have many variable options that will determine the performance of the algorithm. One could use either an additive or a multiplicative penalty. For simplicity we have opted to penalize all of the edges near the original paths with an additive penalty. We used a decaying penalty – that is, one with less weight farther away from the original paths. For simplicity we used a triangular decay shape with a fixed ratio of length to height and introduce the following notations:•wp– the initial penalty width,wmax – the algorithm terminates when wpexceeds this value,k > 1 – number of paths,δAmin – minimum area difference required from every other path as a percent,δCmax – maximum price difference from the optimal path, as a percent.Each iteration of the algorithm involves running a shortest-path algorithm and applying the corridor weights. The new path found will not always satisfy the price and area constraints. If this is the case we will either decrease or increase the penalty width, wp. We maintain a bracket beginning with a lower bound of zero, initially without an upper bound. When wpis increased we will initially double its value until it needs to be decreased, providing an upper bracket. Each time wpis changed the distance between it and the appropriate bracket is halved and the new bracket values are updated accordingly.Algorithmwhile we have not yet found k pathscompute the optimal path.if the new path is too expensivetry the middle of the wpbracketif we have already tried the new wpstopendelse if the new path is too similarif we have an upper bound for the wpbrackettry the middle of the wpbracketelsedouble the wpendif we have already tried the new wp, or wp> wmaxstopendelseadd the new path to the set of k pathsclear data about which values of wphave been usedendsave that the current value of wphas been usedendThe Iterative Penalty Adaptation method has a worst-case running time ofO(k2nlog(n)log(wmax)+k3Llog(wmax)).Each shortest-path algorithm iterations takes O(nlog (n)) operations. Each time a path is found it must be compared to the O(k)) other paths, requiring O(kL) operations (see Lemma 3.2). This gives a time complexity ofO(knlog(n)+k2L)for each iteration.However, since each iteration may not result in a feasible path the algorithm may have to be run again. Initially the penalty width, wp, does not have an upper bracket. As long as the path found is always too similar to the original paths it will continue to double wp. Since wpis doubled it will run at most O(log (wmax)) times before reaching wmax and terminating, or decreasing and setting an upper bound. The upper bound will be bounded by wmax and since each time the size of the bracket is halved, the algorithm will find the next path, or run out of new wpvalues, in at most O(log (wmax)) iterations.Each time a new path is found the bracket values are cleared and the data about which wpvalues have been tried before are reset. This means that the algorithm can spend up to O(log (wmax)) iterations again to find each of the k paths, requiring a total of O(log (wmax)) iterations.Since each iteration requiresO(knlog(n)+k2L),this yields a total time complexity ofO(k2nlog(n)log(wmax)+k3Llog(wmax)).□The k-shortest-paths Adaptation (Wikipedia, 2014) (KSPA) algorithm was designed to take advantage of work that has already been done by computing all k paths simultaneously, storing common information for each path only once. It is based on an adapted form of a k-shortest-paths algorithm that can be derived from a generalization of Dijkstra’s Algorithm.When a suboptimal path is found to a vertex, instead of discarding it, we store both the information of the optimal path and the suboptimal path, until we have at most κ paths to each vertex, one optimal, andκ−1suboptimal paths. In order to maintain the desired properties of our alternate paths, we enforce a restriction that the area between each suboptimal path to the same vertex is at least δAmin.Once we have found our optimal path, we can then set the termination condition to be when we have foundκ=kpaths to the destination, or when the cost of new paths is more than δCmax the price of the optimal path. We note the following parameters:•κ=k– number of paths found to each vertex,δCmax – maximum price difference from the optimal path as a percent,δAmin – minimum area difference required between every path as a percent.Let•activeheapbe a heap containing vertices that are adjacent to the known shortest-path tree from the source,vertexsbe the source vertex,vertexdbe the destination vertex,pathss−dbe a collection of found paths from the source to the destination,dimybe the maximum y value (width of the corridor).Also let each vertex have κ paths to it, each associated with its own cost. (Rather than each vertex storing only the single best path and cost.)Algorithmpush vsonto the activeheapwith a cost of 0while we have not yet reached the destination:current = cheapest vertex in activeheapfor each neighbour of currentif the new path costs within δCmax of the cheapest pathto neighbourif the new path is more than δAmin from other paths toneighbourif we have not yet found κ paths to neighbourpush neighbour onto activeheapwith its updatedcostelse if this path is cheaper than the most expensivepath to neighbourreplace the most expensive path and push this oneonto activeheapendelse if this path is similar to exactly one of the otherpathsif the new path is the cheaper of the tworeplace the old path and push the new one ontoactiveheapendendendendendOne limitation of the k-shortest-paths Adaptation algorithm is the behavior that is best illustrated on near-uniform cost grids as may be found in some prairies. As an academic example, consider the five paths p1, … p5 on a uniform cost grid, as seen in Fig. 6. Let B be the parent of A on this path. The path p3 is the cheapest path. Paths p2 and p4 are too similar to p3 and so will be rejected. Paths p1 and p5 are dissimilar from p3 and so would be acceptable alternate paths, however they are too similar to p2 and p4 respectively and so will also be rejected. When each path is very similar to the paths on either side of it, the algorithm is only ever to find one path to each vertex and hence will only find one possible road alignment.Proposition 4.4The k-Shortest-Paths Adaptation Algorithm achieves worst-case bounds of O(κ2Lnlog (n)), where n is the number of vertices, κ is the number of paths found to each vertex, and L is the number of vertices of the longest path considered, with L ≪ n.Since we are now finding at most κ paths to every vertex, instead of each vertex being used in at most 1 path, we can now have each vertex used in at most κ paths. This is then similar to having κn vertices, which means that we will have O(κnlog (κn)) iterations of the algorithm.In each iteration of the algorithm we need to compare the new path we have found with up to κ other paths. From Lemma 3.2 we see that each iteration will then have an additional O(κL) operations. This gives us a total worst-case time complexity of O(κ2Lnlog (κn)), or more simply O(κ2Lnlog (n)).□The Bidirectional Selection Method (BDS) is a simple extension of Bidirectional Dijkstra’s Algorithm, similar to the approach used by Lombard and Church (1993). Instead of using the regular termination condition for the Bidirectional Dijkstra’s Algorithm, we simply continue to grow both ends until we have found k dissimilar paths. Alternatively, we terminate the algorithm if the cost of the new paths found exceeds the maximum cost restriction, since we will not find any cheaper paths beyond those.The current version of our algorithm uses a method of path selection that is relatively simple. Each time a new path is found it is compared to the set of accepted paths and is added to the set, replaces one of the paths in the set, or is rejected. A computational shortcut is used by not considering a path again after it has been rejected. While this shortcut may result in a desirable solution being missed, the additional computation time required to process all possible road alignment groupings would be infeasible.A similar method was proposed to find dissimilar paths in road networks (Abraham, Delling, Goldberg, & Werneck, 2013; Lombard & Church, 1993). As they discussed, the s–d paths produced by this algorithm have the specific property that given an arbitrary vertex A an s–d path is formed by concatenating the cheapest path from vsto A and the cheapest path from A to vd. The parameters are•k > 1 – number of paths,δCmax – maximum price difference from the optimal path as a percent,δAmin – minimum area difference required from every other path as a percent.Let•vsbe the source vertex,vdbe the destination vertex.push vsand vdonto their respective heaps with a cost of 0while there are still entries in the source or destination heapsgrow from whichever side has a smaller heapif the newly added vertex forms a new pathif this new path is more than δAmin from the other pathsselectedif we have not yet selected k pathssave the newly found path in our set of k pathselse if this path is cheaper than the most expensive pathselectedreplace the most expensive pathendelse if this path is similar to exactly one of the other pathsif the new path is the cheapest of the tworeplace the old path with the newly foundpath.endendendif we have not yet found k paths and new paths are less thanδCmax of the cheapest pathpush the neighbours of the newly added vertex onto theheap, as neededendendThe Bidirectional Selection method computes at most n possible alternate paths with worst-case time complexity O(nlog (n)).For a given vertex, we are computing the cheapest path to the source and the cheapest path to the destination. That is, we compute the cheapest path from the source to the destination, that passes through each vertex. This is done by running Dijkstra’s Algorithm twice, once from the source and once from the destination, each taking O(nlog (n)). By concatenating these results, we then generate one path for each vertex, taking an additional O(n). This gives a combined worst-case time complexity ofO(nlog(n)+n),or O(nlog (n)).□The Bidirectional Selection Algorithm has worst-case boundsO(nlog(n)+kLn),where n is the number of vertices, k is the number of paths, and L is the number of vertices of the longest path considered, with L ≪ n.From Lemma 4.5 our algorithm finds at most one different path per vertex, we are then selecting k paths from a set of at most n paths. Each new path is compared to our set of up to k accepted paths, which, from Lemma 3.2, requires O(kL) operations for each new paths. With n new paths this gives us a total worst-case time complexity ofO(nlog(n)+kLn).□This algorithm combines the two strategies by running the KSPA algorithm from both directions, as is done in the BDS method. We do make note that the number of paths found to a particular vertex from one direction, as used in the k-shortest-paths Adaptation algorithm, need not be the same as the total number of paths found. The value of κ can be thought of as the number of shortest-path trees made from both the source and the destination, while q controls the number of paths that are selected and returned from the paths generated by the shortest-path trees. A value ofκ=1reduces the algorithm to the Bidirectional Selection method, but a large value of κ will increase the running time of the algorithm.We omit the pseudo-code algorithm for this section. The modifications provided in Section 4.3 do not affect the new termination and path selection process described in the pseudo-code of 4.5.Proposition 4.7The BDS-KSPA hybrid method hasO(κ2Lnlog(κn)+nκqL)as a worst case time complexity, where L is the number of vertices in the longest path considered, with L ≪ n.Similar to the KSPA algorithm each shortest-path tree generation requires O(κ2Lnlog (n)) operations as seen in Proposition 4.4.Since we are now finding at most κ paths to each vertex we are generating a set of O(κn) paths. Since this algorithm uses the same path selection process as the BDS method each path will require O(qL) operations (see Proposition 4.6). This gives a total of O(nκqL) operations for the path selection process.Combining these results gives us a worst-case time complexity ofO(κ2Lnlog(n)+nκqL)for the BDS-KSPA hybrid method.□Numerical tests were run to compare algorithm quality in terms of time required and ability to find a valid solution. We look at which algorithm is most consistently able to find spatially dissimilar paths that are near the cost of the globally optimal path. The running time is also compared to select the fastest algorithm with the best results. The numerical tests are also used to measure the improvement in the running time gained with the height restrictions.A parameter required for each test is δAmin, the difference threshold to state two given paths are ‘distinctly different’. This value should be selected based upon the number of paths desired to be found, k. We have chosenk=3and we are usingδAmin=12percent. Another parameter required for both is δCmax, which is the maximum cost difference allowed in secondary paths, as a percent of the optimal path. We have selectedδCmax=10percent.In our numerical results an algorithm is considered to have found a solution if it meets three criteria. First, we require three paths with one of them being the optimal path; second, the maximum path cost must be within the specified range of the cheapest path; and third, the minimum percentage area requirement must be satisfied.The algorithms perform differently based on the dimensions of the map (in terms of number of vertices). For both of our test sets we are using approximately 10 meters between each horizontally-adjacent vertex and 1 meter between each vertically-adjacent vertex. The first test set has three different map lengths of 40, 80, and 160 vertices, while the second test set has map lengths of 320 and 640 vertices. For each road length we have 7 maps with a length to width ratio of 2:1 and 3 maps with ratios 8:1, giving us a total of 30 maps in test set 1 and 20 maps in test set 2. The initial terrain data was obtained from the USGS National Map Viewer (USGS, 2015). The individual maps were found by sampling mountainous and prairie regions and selecting a variety of different vertical dimensions to ensure a good spread of map types. We provide a detailed breakdown of the terrain features present in each map in Appendix A. The exact data used can be obtained by contacting the authors.All of the tests forthe first test set were run on the Orcinus cluster, a 9600 core available through (Westgrid, 2014). Each test was run on a single core of an Intel Xeon X5650 six-core processor, running at 2.66 gigahertz. The different map sizes were allotted different amounts of memory, up to the 24 gigabyte of RAM available per node. The second test set was run on Grex, a 3792 core cluster available through Westgrid. Again, each test was run on a single core of an Intel Xeon X5650 six-core processor, running at 2.66 gigahertz. Each node had up to 96 gigabyte of RAM available. All of the code was written in MATLAB R2013b. Note that the algorithms are serial; the only reason we performed the tests on a cluster was to speedup running the algorithms on the full test set.We have selected five algorithms for testing in this paper, and three modifications that can be made to each to improve the performance. We include results for each of the basic algorithms without modification. We then selected the two best algorithms, and present the improvements gained by the three modifications. First they are combined with the heuristic described in Section 2.2, adapted from the A* algorithm. Next they are combined with both the adapted form of the A* algorithm, and the Simple Height Restriction from Section 2.4. Further tests were done with both the heuristic from the A* algorithm, as well as the Expanding Height Restriction from Section 2.5.When the algorithms are using the Simple Height Restriction we have set the following parameters:R=3andHmax=1. The Expanding Height algorithms will be using a value ofHi=0.5. We used these parameter values as preliminary experimentation indicated they would provide good results.The Sensitive Elimination method’s performance is related to the choice of w, which determines how much of the map is removed at each iteration and δAmin. If w is too small, then it is unlikely that the alternative paths found will be at least δAmin different. As a result, we have chosen to use a value that can be calculated byw=round(δAmin·wm−0.5),where wmis the width of the map.Preliminary testing done for the Iterative Penalty Adaptation method achieved optimal performance with an initial penalty width value ofwp=10percent, which is the value we used for our tests.For the BDS-KSP hybrid method we used a value ofκ=2,so that at most two paths would be found to each vertex and a value ofq=3so that a total of 3 paths would be found.

@&#CONCLUSIONS@&#
