@&#MAIN-TITLE@&#
Congestive heart failure detection using random forest classifier

@&#HIGHLIGHTS@&#
Heartbeat classification is substantial for diagnosing heart failure.Machine learning methods classify normal and congestive heart failure (CHF).The random forest method gives 100% classification accuracy in detecting CHF.

@&#KEYPHRASES@&#
Electrocardiogram (ECG),Congestive heart failure (CHF),Autoregressive (AR) modeling,Machine learning,Random forest,

@&#ABSTRACT@&#
Background and objectivesAutomatic electrocardiogram (ECG) heartbeat classification is substantial for diagnosing heart failure. The aim of this paper is to evaluate the effect of machine learning methods in creating the model which classifies normal and congestive heart failure (CHF) on the long-term ECG time series.MethodsThe study was performed in two phases: feature extraction and classification phase. In feature extraction phase, autoregressive (AR) Burg method is applied for extracting features. In classification phase, five different classifiers are examined namely, C4.5 decision tree, k-nearest neighbor, support vector machine, artificial neural networks and random forest classifier. The ECG signals were acquired from BIDMC Congestive Heart Failure and PTB Diagnostic ECG databases and classified by applying various experiments.ResultsThe experimental results are evaluated in several statistical measures (sensitivity, specificity, accuracy, F-measure and ROC curve) and showed that the random forest method gives 100% classification accuracy.ConclusionsImpressive performance of random forest method proves that it plays significant role in detecting congestive heart failure (CHF) and can be valuable in expressing knowledge useful in medicine.

@&#INTRODUCTION@&#
Human heart is the most important and hardest working muscle in human body, that together with blood vessels compose cardiovascular system. It pumps the blood into the every cell of the human body. Furthermore, heart muscle is the engine of the human body [1]. Heart failure is the common syndrome that progresses slowly but causes cardiac dysfunction resulted from the inability of the heart to pump the blood to all the cells of the human body efficiently. The heart weakens by heart attacks, long-term high blood pressure or an anomaly of one of the heart valves. Yet, heart failure is generally not recognized until it comes to the more progressive phase, entitled congestive heart failure, which causes fluid to flow to lungs, feet and abdominal cavity. Congestive heart failure is a condition that can be caused by heart diseases, such as coronary artery disease, damage of the heart after the heart attack, high blood pressure, valvular heart disease, diabetes and even the alcoholism [2]. According to European Heart Network and European Society of Cardiology [3], over 4 million people die from cardiovascular diseases in Europe and 1.9 million in European Union (EU) which is 47% deaths in Europe and 40% in EU.Considering that there is no definite diagnosis of heart failure, medical diagnosis such as history or physical examinations, electrocardiography (ECG), chest radiography or echocardiography is crucial for detecting the congestive heart failure. The electrocardiogram (ECG) is noninvasive tool that records electrical activity of the heart and shows irregularities of the heartbeats. It safely examines and records the electrical impulses of the heart and show possible damages of the heart or irregularities of the heartbeats [4]. Thus, ECG is an important tool for determining the function and the health of the cardiovascular system. Moreover, it is significant to define accurate and timely diagnosis of physicians to avoid more damage and to determine proper methods and approaches [5]. Still, the problem occurs when there is insufficient number of physicians to meet the needs of patients. Therefore, it is necessary to develop an effective and automated diagnostic systems based on ECG recordings, combined with application of machine learning techniques for classification of heart diseases. These diagnostic systems will aid medical experts in detecting the irregularities in the cardiovascular system. The diagnostic system will firstly process the ECG signals taken from different subjects and hence decomposed into few features performing feature extraction. Extracted ECG signals are used to detect different types of heart failure by using various machine learning techniques [6].According to the number of researches done, the field of heartbeats classification using different techniques is very popular. Beth Israel Deaconess Medical Center (BIDMC) congestive heart failure database were used in different studies. Baim et al. [7] were used this database to show the effect of treatment with oral milrinone. The larger group of 100 patients, with severe congestive heart failure belonging to NYHA3 and NYHA4 groups was treated with oral milrinone with an average initial dose of 27±8mg/day. Causes of congestive heart failure were different (e.g. ischemic heart disease, dilated cardiomyopathy, valve replacement, etc.). Thuraisingham used [8] BIDMC congestive heart failure database in detecting CHF from normal heartbeats, using k-nearest neighbor algorithm and features from the second-order difference plot (SODP) obtained from Holter monitor cardiac RR intervals. Six features are obtained from second-order difference plot and k-nearest neighbor algorithm is applied with the value of k=1. Authors obtained a success rate of 100% in separating CHF and normal heartbeats. The same database was used by Kuntamalla and Reddy [9] and they applied sequential trend analysis to differentiate CHF patients from patients with normal heart beats. The accuracy of the proposed method is 96.68%. Furthermore, Kuntamalla and Reddy [10] applied multiscale entropy (MSE) to HRV signals, to differentiate healthy young and elderly subjects from CHF patients. They applied Reduced Data Dualscale Entropy Analysis method to reduce the data size for clear differentiation of subjects. They achieved 100% accuracy. Hossen and Al-Ghunaimi [11] used technique based on recognition of power spectral densities pattern of decomposed sub-bands of R–R interval to identify patients with congestive heart failure. They used 12 subjects from BIDMC congestive heart failure database and reached accuracy of 90%. The same authors, Hossen and Al-Ghunaimi [12] used different wavelet decomposition filters with soft decision algorithm to estimate power spectral density of RR interval data for screening patients with congestive heart failure. The accuracy value of test data with almost all wavelet filters used was 88.6%. Yu and Lee [13] focused on selecting the best feature selector to get high accuracy in recognizing the CHF. The authors proposed conditional mutual information feature selector (CMIFS) with support vector machine (SVM) classifier. The combination achieved 97.59% accuracy using only 15 features. Işler and Kuntalp [14] combined classical HRV and wavelet entropy measures to distinguish healthy patients from patients with congestive heart failure. They applied genetic algorithm to select the best ones among all possible combinations of measures. Furthermore, they used k-nearest neighbor classifier with different values of k. Finally, they got accuracy value of 96.39% with value of k=5 and 7, using 8–10 features. Similar study was done by Asyali [15] and author used linear discriminant analysis to examine the discrimination power of 9 long-term HRV measures and validated the result by applying Bayesian classifier. Sensitivity and specificity value of the stated classifier are 81.8% and 98.1%, respectively. Pecchia et al. [16] investigated discrimination power of heart rate variability (HRV) in distinguishing normal subject and subjects with congestive heart failure (CHF). They performed time and frequency analysis in order to measure HRV features. The result is evaluated by applying classification and regression tree (CART) classifier. Furthermore, authors introduced two non-standard features: average of normal intervals and low/high frequencies for recording over the 24h and reached the success rate of 89.7% for sensitivity and 100% for specificity.Feature extraction methods are important in ECG signal classification. In this paper, features are extracted by use of autoregressive Burg method. Consequently, specified signal is categorized as normal or with congestive heart failure by using machine learning techniques. In previous works [8,9,11–15,17], various machine learning techniques were applied. However, k-NN method is not capable of dealing with high dimensional data without dimension reduction, where SVM is not strong enough to handle large number of trivial data without data selection [18]. Decision tree method might not perform well with more complex interactions and overfitting might lead to the instability of the model [19]. Method efficient to achieve excellent performance, but not applied in previously mentioned studies [8–15] is random forest (RF). In this paper, RF method is used to investigate the performance of classifier for ECG signals classification in diagnosing the congestive heart failure. Moreover, performance of different machine learning techniques such as C4.5 decision tree, ANN, k-NN and SVM in ECG signal classification is compared (see Fig. 1.).The rest of the paper is organized as follows: Section 2 gives explanation about the data used and present methods applied in each step of the ECG signal classification process. Experimental results are presented and discussed in Section 3. Finally, conclusions derived from the results are summarized in Section 4.To evaluate the performance of the proposed methods, ECG signals are taken from the group of PhysioNet databases. The paper includes ECG signals from 15 subjects from Beth Israel Deaconess Medical Center (BIDMC) Congestive Heart Failure (CHF) database. The BIDMC CHF database contains recordings of 11 male patients, aged 22–71 and 4 female patients, aged 54–63 with severe congestive heart failure, belonging to New York Heart Association (NYHA) class 3 and 4. This group of subjects is part of larger study group, treated with the oral inotropic agent, milrinone. Duration of each recording, containing two ECG signals is about 20h, sampled at 250 samples per second in a range of ±10mV, with 12-bit resolution. Each ECG recording contains around 17 million sample intervals. The larger study is based on 100 patients with severe congestive heart failure treated with digitalis glycosides, diuretics and one or more oral vasodilators [20].Second data set include ECG signals from 3 patients suffering from congestive heart failure, obtained from PTB Diagnostic ECG database. First patient is 74-years old female, belonging to NYHA 2, second patient is 70-years old male, belonging to NYHA 3 and third patient is 61-year old female, belonging to NYHA 4. These ECG signals are part of larger data group, collected from healthy volunteers and patients with various heart diseases, at the Department of Cardiology of University Clinic Benjamin Franklin, in Germany. Each ECG record includes 15 simultaneously measured signals: the conventional 12 leads with 3 Frank lead ECGs. Signal form lead I is taken for purpose of this study. Original signals are digitized at 1000 samples per second, with 16 bit resolution over a range of ±16.384mV [20].Normal heartbeats are taken from 13 subjects from MIT–BIH Arrhythmia database which totally contains 48 ECG recordings lasting for 30min, obtained from 47 subjects by the BIH Arrhythmia Laboratory, between 1975 and 1979. These recordings are randomly chosen from a larger set of recordings at Boston's Beth Israel Hospital, collected from a mixed population of inpatients and outpatients. These recordings are taken using recording bandwidth of 0.1–100Hz. Each of these recordings is sampled at 360 samples per second over the 10mV range, with 11-bit resolution. This database is freely available on http://physionet.org/physiobank/database/mitdb/[20].The ECG signals from both databases are preprocessed and features are extracted using autoregressive (AR) Burg algorithm. The autoregressive model is well-known feature extraction method, especially for biological signals. The important step in AR modeling is selection of the optimal order p. A process of model order p in autoregressive model is given by following formula [21]:(1)x[n]=−∑k=1pakx[n−k]+e[n]where x[n] represents data of the signal at point n, p represents model of order, akis an autoregressive coefficient and e[n] represents noise error [21]. The Burg method is used for estimating a real valued autoregressive coefficient akrecursively using akof previous order p−1. It is the form of order-recursive least squares method used to estimate parameters by minimizing the forward and backward errors of the linear system. The Burg method is precise because it uses many data points at the time, minimizing the backward and forward error [21]. However, Burg algorithm involves prediction error powers defined by formula [22]:(2)δk2=δk−12(1−|ak|2)whereδk2is prediction error power which decreases when model of order p is increasing, producing the stationary model.There is no straightforward way to determine the correct model order. As we increase the order of the model, the root mean square (RMS) error usually decreases quickly up to some order and then more slowly. An order just after the point at which the RMS error flattens out is usually an appropriate order. Much research has been done to discover good model order criteria. One of the criterion most often used is called the final prediction error (FPE). It is related to the variance of the modeling error.(3)FPE=sp2N+p+1N−p−1,wheresp2is the variance of the modeling error and N is the number of points in the signal. Model of order used in this study is 32. The length of each feature vector extracted using AR Burg is 33.Each signal is divided into subsections of signals, consisting of 1000 data points (window length of 1000). Each subsection was used to extract features using AR Burg. Figs. 2 and 3shows input to feature extraction and output data after AR Burg has been applied, respectively.In decision tree method, instances are classified by arranging them down the tree, from root to leaf node. Each internal node is a test for some attribute of the tree. The root node is the starting point of the classification process and is without incoming edges. The other nodes have exactly one incoming edge and are called leaves. The classification starts at the root node with testing the attribute specified by this node and continue down the tree branch according to the value of the attribute in the example. When a leaf node is reached, the instance is classified according to the class of the leaf [19,23].C4.5 algorithm is based on ID3 algorithm, a very simple decision tree algorithm, presented by Quinlan [24]. This algorithm passes through decision tree, visits each node and select optimal split. It is achieved by using the gain ratio, represented by following formula [19]:(4)GainRatio(S,A)=InformationGain(S,A)Entropy(S,A)where entropy is the term which describes how equally the attribute splits the data [19,23,24] and is calculated by formula:(5)Entropy(y,S)=∑cj∈dom(y)−σy=cjSS⋅log2σy=cjSSInformation gain is the impurity-based criterion which uses an entropy measure as the impurity measure, for some training set S with respect to the attribute A and is presented by formula [19]:(6)InformationGain(ai,S)=Entropy(y,S)−∑vi,j∈dom(ai)|σai=vi,jS||S|⋅Entropy(y,σai=vi,jS)The optimal accuracy is reached when the size of tree was 15 and number of leaves 8.Fix and Hodges [25] proposed a classification method which is easy to implement and give high accuracy. k-NN is non-parametric method for pattern classification based on finding the “nearest” training set T. The first step is to reduce the dimension of feature space by using distance function (commonly used Euclidian distance) between a test set and specified training set [26]:(7)d(xi,xj)=∑r=1n(ar(xi)−ar(xj))2where aris the value of rth attribute of instance x.A mathematical expression of k-NN for Y is expressed below:(8)Y=1k∑xi∈Nk(x)yiwhere Y is a local mean vector and Nk(x) is the neighborhood of x defined by the k closest points xiin the training set [27]. In k-nearest neighbor, the effective number of parameters is N/k, which is greater than p that represents parameters in least-square fits, and is getting lower value when increasing the value of k. So, there would be N/k neighborhoods and one parameter would fit in each of them, if the neighborhood were not overlapping [28]. Different values of k were applied to the dataset, such as k=1, 3, 5, 7 and 10, but accuracy value reached the highest number when using 3-nearest neighbor for classification.Concept of artificial neural network was presented by McCulloch and Pitts in 1943 [29], where artificial neuron is based on biological neuron. Neural network consist of huge number of processing units called neurons, nodes, processing units or cells. Each is connected to another by related weight. Weight is a term which represents synapses in biological neural network. Neural network consists of input nodes which receives data from outside and form input layer, output nodes which send data out of the network and form output layer and hidden node whose signals remain in the network and form hidden layer. In this study, 2800 instances are classified into two classes of output (normal and congestive heart failure). Network is created with one input layer, consisting of 33 inputs and 10 nodes in the hidden layer, as these parameters give the optimal classification accuracy. Every artificial neuron has internal state, called activation function of the inputs received. Many activation functions tested but sigmoid function gave the best accuracy. A sigmoid (S shaped curves) function presented by formula below [30]:(9)f(x)=11+e−xand gives only two outputs. Network architecture is the way nodes are organized in layers and interconnected. In this study, feed-forward backpropagation architecture is applied to the dataset [30–32].Backpropagation neural network architecture is proposed by Rumelhart et al. [32]. It is the classification algorithm that uses gradient descent method to adjust the connection weights in network model. In backpropagation algorithm, output of each neuron will be found by aggregating the neurons of previous level and multiplied by determined weight [33].Support vector machines (SVM) concept is first presented by Boser et al. [34]. They presented a training algorithm that will maximize the margin between training patterns and decision boundaries [28]. SVM is a hyperplane that splits positive and negative sets of examples, with maximal margin. SVMs belong to the category of kernel methods, used in high dimensional feature space for computing a dot product. Kernel methods solve the problems of quadratic increase in memory when storing the features and time required calculating the classifier's discriminant function by avoiding mapping the data into the high-dimensional feature space [35]. When considering the dataset mentioned above, consisting of pairs (x1, y1), (x2, y2), …, (xN, yN), where xi∈ℜPand yi∈{−1, 1}, hyperplane is defined by [28]:(10){x:f(x)=xTβ+β0=0}where β is a unit vector: ||β||=1.A classification rule created by f(x) is(11)G(x)=sign[xTβ+β0]Now, when classes are separable, a function f(x) with yif(xi)>0 for all i values and hyperplane that produces the biggest margin between the training points for classes −1 and 1 can be determined. The optimization problem for this concept is presented by following formula [28]:(12)maxβ,β0,||β||=1Msubject toyi(xiTβ+β0)≥M,i=1,…,N,So, margin is created and it is M unit away from hyperplane from both sides [28]. In this research, we used Puk kernel for SVM classifier. Linear, nonlinear and RBF kernels were also tested, but computational time increased in nonlinear RBF kernels, besides accuracies were significantly reduced. Hence, we used Puk kernel for SVM classifier which gives best accuracy among other kernels with ω=1 and σ=1. These values give optimal classification accuracy with reasonable computational time. However, based on literature [36], we tested different values of ω and σ such as 0.5 and 2, 5 and 0.5, respectively. Classification accuracy decreased, whereas computational time increased.Random forest is a classification method which combines multiple tree predictors in that way that each tree depends on a value of randomly chosen vector distributed among all trees in forest in the same way. So, in the random forest algorithm, a random vector θkis produced, independent from the previous random vectors and distributed to all trees, and each tree is grown using training set and random vector θk, which results in collection of tree-structured classifiers {h(x, θk), k=1, …} at input vector x. In random forest algorithm, generalization error is given by [37]:(13)PE*=PX,Y(mg(X,Y)<0)where subscripts X and Y are random vectors that indicate the probability is over the X, Y space and mg is the margin function which measures the extent to which the average number of votes at random vectors for the right output exceeds the average vote for any other output. Margin function is defined as(14)mg(X,Y)=avkI(hk(X)=Y)−maxj≠YavkI(hk(X)=j)where I(·) is the indicator function [37].Two parameters that measure accuracy of individual classifier and dependence between classifiers are strength and correlation, respectively. Random forest with random features is formed by choosing a small group of input variables on each node, randomly [37]. In this research, random forest consisted of 20 trees, each constructed while considering 6 random features. Optimal values for trees and random features are obtained by inserting different values of trees and random features, using the classification accuracy as fitting function. Therefore, 20 as the number of trees gave the optimal accuracy results.

@&#CONCLUSIONS@&#
