@&#MAIN-TITLE@&#
Limestone: High-throughput candidate phenotype generation via tensor factorization

@&#HIGHLIGHTS@&#
A nonnegative tensor factorization model that achieves high-throughput phenotyping.The method simultaneously generates multiple phenotypes with no human intervention.Our algorithm generally generates concise and clinically meaningful phenotypes.The model improves heart failure prediction performance on a cohort of 31K patients.

@&#KEYPHRASES@&#
Dimensionality reduction,Nonnegative tensor factorization,EHR phenotyping,

@&#ABSTRACT@&#
The rapidly increasing availability of electronic health records (EHRs) from multiple heterogeneous sources has spearheaded the adoption of data-driven approaches for improved clinical research, decision making, prognosis, and patient management. Unfortunately, EHR data do not always directly and reliably map to medical concepts that clinical researchers need or use. Some recent studies have focused on EHR-derived phenotyping, which aims at mapping the EHR data to specific medical concepts; however, most of these approaches require labor intensive supervision from experienced clinical professionals. Furthermore, existing approaches are often disease-centric and specialized to the idiosyncrasies of the information technology and/or business practices of a single healthcare organization.In this paper, we propose Limestone, a nonnegative tensor factorization method to derive phenotype candidates with virtually no human supervision. Limestone represents the data source interactions naturally using tensors (a generalization of matrices). In particular, we investigate the interaction of diagnoses and medications among patients. The resulting tensor factors are reported as phenotype candidates that automatically reveal patient clusters on specific diagnoses and medications. Using the proposed method, multiple phenotypes can be identified simultaneously from data.We demonstrate the capability of Limestone on a cohort of 31,815 patient records from the Geisinger Health System. The dataset spans 7years of longitudinal patient records and was initially constructed for a heart failure onset prediction study. Our experiments demonstrate the robustness, stability, and the conciseness of Limestone-derived phenotypes. Our results show that using only 40 phenotypes, we can outperform the original 640 features (169 diagnosis categories and 471 medication types) to achieve an area under the receiver operator characteristic curve (AUC) of 0.720 (95% CI 0.715 to 0.725). Moreover, in consultation with a medical expert, we confirmed 82% of the top 50 candidates automatically extracted by Limestone are clinically meaningful.

@&#INTRODUCTION@&#
The rapidly increasing availability of electronic health records (EHRs) from multiple heterogeneous sources has spearheaded the adoption of data-driven approaches for improved clinical decision making [1–4], prognosis [5–8], and patient management [9–12]. While knowledge discovery in EHRs show great promise towards providing better quality of care at lower costs [13–15], the vast information captured pose difficulties for both medical practitioners and data analysts [16]. EHR data offers many formidable challenges that has limited their utility for clinical research thus far. These include diverse patient populations from providers who may be using different, potentially incompatible EHR systems; heterogeneous information covering a variety of inter-related aspects of patients such as diagnoses, medication orders, and laboratory test findings [17,18]; sparsely sampled medical event sequences with different time scales across patients [19–21]; and noisy, incomplete, and inaccurate representation of patients [22,23]. Clinical research requires precise and concise medical concepts about patients. The process of mapping raw EHR data into meaningful medical concepts, or the task of learning the medically relevant characteristics of the data [24,25] is referred to as EHR-based phenotyping. The phenotyping process can not only be used to identify specific clinical characteristics important in the process of research subject selection [26,27], but also improve the discovery process such as optimizing interventions and predicting response to therapy [24]. While the term EHR-based phenotyping has various meanings [28], this paper focuses primarily on the process of extracting medical concepts, or phenotypes.Phenotypes encompass the entire spectrum of EHR data, using both structured information (e.g. billing codes, laboratory reports, and medication orders) and unstructured documents (e.g. clinical notes, pathology and radiology reports) [27,29]. Significant progress has been made in the generation and sharing of phenotypes [29–33]. Examples of such large-scale phenotyping efforts are typified by the Electronic Medical Records and Genomics (eMERGE) Network [34] and the Observational Medical Outcomes Partnership (OMOP) [35]. Furthermore, the eMERGE process supports portability via a process that iteratively tests and refines the phenotype at different institutions [29].The development of EHR-derived phenotypes currently relies primarily on rule-based, heuristic and iterative based approaches, which take significant time and expert knowledge to develop [24,36,37]. Often, the phenotyping process requires a team effort from clinicians, domain experts, and IT experts [24,37,38]. However, phenotypes are often disease-centric and the development of a phenotype for a single disease can take months [39]. Furthermore, phenoytyping requires significant interaction between the domain experts and informaticians [37] and each team member may bring his/her own biases, ignoring potentially useful information [24]. Thus, high-throughput phenotyping, or efficient and automated phenotype extractions to reduce manual development, has gained recent attention [24,36,37,26]. Data mining and machine learning tools have been utilized to automate the phenotype generation process [24,36,37,26]. Yet, the current state of the art high-throughput phenotyping cannot generate large amounts of candidate phenotypes that simultaneously achieve good performance without human annotated samples [37]. Thus, the limitations of existing phenotyping efforts can be summarized as follows:•A requirement for human annotation of case and control samples, taking substantial time, effort, and expert knowledge to develop.A lack of formalized methodology for deriving novel phenotypes such as disease subtypes.A failure to incorporate an automated process to support portability across institutions.To create a high-throughput phenotyping environment, the phenotyping process needs to shift towards a more data-driven, high-throughput approach, where multiple candidate phenotypes are generated while minimizing human intervention [24]. Our paper directly addresses all but the last limitation by focusing on dimensionality reduction to automate the generation of phenotypes.One possible approach to automatically discover phenotypes from EHR data is to use dimensionality reduction techniques [24], which represent the original data using lower dimensional latent space. Phenotyping takes high-dimensional EHR data and maps it to medical concepts, where an “ideal” phenotype (i) is concise and easily understood by a medical professional, (ii) represents complex interactions between several sources (e.g. diagnosis and medication), and (iii) maps to domain knowledge. Each phenotype can be viewed as the definition of a particular latent space along the multiple sources. Matrix factorization is a common dimensionality reduction approach in high-dimensional settings, but it may not concisely capture structured source interactions, such as multiple medications prescribed to treat a single disease. Thus, a more natural transformation is tensor factorization which utilizes the multiway structure to produce concise and more interpretable results.This paper presents Limestone, a nonnegative tensor factorization method to generate phenotype candidates without expert supervision. Our algorithm is named after a sedimentary rock obtained via geology mining, the extraction of valuable resources from earth. Limestone (rock) has a wide diversity of uses and is an excellent building stone. We view our nonnegative tensor factorization model as a building block for high-throughput phenotyping from EHR data. Our proposed model:1.Achieves high-throughput phenotyping by deriving multiple candidate phenotypes simultaneously from EHR data without any user supervision or domain expertise.Captures data source interaction, such as the diagnosis and medication interaction from the same medical visit.Generates concise and clinically meaningful phenotypes.Produces stable phenotype definitions across multiple factorizations and small perturbations of the data.We apply Limestone on real EHR data from Geisinger Health System. The case-control dataset contains 31,815 patients. We use our method to automatically derive multiple candidate phenotypes from the dataset and analyze the factors for stability, conciseness, predictive power, and clinical relevance. We also show that only 40 candidate phenotypes are needed as features to obtain better predictive accuracy of patients at risk of heart failure than the original set of medical features (640), achieving an area under the receiver operator characteristic curve (AUC) of 0.720 with a 95% confidence interval of (0.715, 0.725). Furthermore, 82% percent of the first 50 Limestone-derived phenotypes from the control population are confirmed by a medical expert to be clinically meaningful.The remainder of the paper is structured as follows. Section 2 presents existing work on matrix factorization and summarizes relevant existing tensor factorization approaches. Next, we detail Limestone in Section 3. Section 4 demonstrates and evaluates our proposed method on real EHR data. This is followed by a discussion of the limitations and proposed future work in Section 5. Finally, we summarize our work in the Section 6.Notation details.Table 1provides a key to the symbols used in this paper. We adopt the notation from [40] to maintain consistency with the referenced tensor decomposition papers.Structured EHR data can be represented using a feature matrix. The simplest representation for the data is a source independent feature matrix, where each row denotes a patient and each column represents a feature from a single source. Fig. 1a shows two matrices from a diagnosis source and a medication source. For context, in the Geisinger dataset, the diagnosis feature matrix contains 169 columns, where each column represents a single diagnosis such as asthma. However, the source independent feature matrix ignores potential interactions between the various sources, such as medications prescribed to treat a specific diagnosis. To incorporate “same visit” interactions,1Note that we do not explicitly define “same visit”, as what constitutes a same visit (e.g. a doctor visit, a hospital stay, etc) depends on the particular application.1a matrix whose column contains the combinations between the sources can be used. Fig. 1b illustrates a source interaction matrix for all diagnosis-medication combinations. This matrix introduces two problems: (1) the data is sparse because patients generally only experience a fraction of the diagnosis-medication combinations and (2) the data is high-dimensional (e.g.149×471possible combinations in the Geisinger case). Thus, dimensionality reduction can assist both interpretability and scalability of the data.Matrix factorization (MF) is a common dimensionality reduction approach, which represents the original data using a lower dimensional latent space. Standard MF approaches, which focus primarily on numeric data, find two lower dimensional matrices such that when multiplied together approximately produce the original matrix. The mathematical formulation is as follows, given anN×MmatrixX, find matricesWandHof sizeN×RandR×Msuch that:(1)X≈WH.Fig. 1b illustrates the use of matrix factorization to derive phenotypes using the source interaction matrix. Although many matrix decomposition techniques exist [41], principal component analysis (PCA) and nonnegative matrix factorization (NMF) are two common algorithms used to reduce the feature dimension.PCA calculates a set of basis vectors, or principal components, that minimizes the loss of information (i.e., the optimal approximation of the data in terms of least squared error). Generally, the number of principal components (R) is much smaller than the number of dimensions, which enables an encoding of the data as linear combinations of the basis vectors. Thus, PCA transforms the original, high-dimensional data to a lower-dimensional space defined by the principal components. One pitfall with PCA is the loss of “interpretability” which stems from several issues: (i) the principal components can have negative elements and (ii) the observed data can be approximated using both positive and negative combinations of the principal components. These are problematic because, in certain domains, negative elements and/or negative combinations are not easily interpretable [42,43]. For example, imagine the EHR feature matrix where each element represents the number of times a diagnosis or medication is recorded. Performing PCA on such a matrix results in a set of phenotypes, where each principal component defines the phenotype. A positive value in the principal component indicates the presence of a feature (diagnosis/medication) and a zero value denotes the absence. However, a negative entry does not readily map to some understanding about the feature’s relationship to the phenotype.The desire to prevent negative components motivated NMF [43]. Given a nonnegative matrixX, the NMF finds two nonnegative matricesWandHthat approximateX. Furthermore, the nonnegative constraint often leads to a sparse representation [43]. The enhanced semantic interpretability of NMF has led to its use across various fields such as mathematics, data mining, computer vision, and chemometrics [44]. Applications of NMF to biomedical data include discriminative feature selection from time–frequency representation of EEG data [45], feature extraction from brain CT images [46], and microarray gene data reduction for visualization and clustering purposes [47].A tensor, or multiway array, is a generalization of a matrix (and a vector and a scalar) to higher dimensions. A mode of a tensor refers to a dimension, or way, of the tensor. The number of modes in a tensor is also known as the order of the tensor. Tensor representations are powerful because they can capture relationships for high-dimensional data. An overview of tensors can be found in [48–50].A rank-one tensor can be written as the outer product of N vectors, where the outer product is defined as follows:Definition 1The outer product of N vectors,a(1)∘a(2)∘⋯∘a(N), produces aNth order tensorXwhere each elementxi→=xi1,i2,…,iN=ai1(1)ai2(2)⋯aiN(N).Tensor factorization or decomposition is a natural extension of matrix factorization and utilizes information from the multiway structure that is lost when modes are collapsed to use matrix factorization algorithms [48,49,51,52]. One of the common tensor decompositions, CANDECOMP/PARAFAC (CP) [53,54], can be considered a higher-order generalization of singular value decomposition [48]. The CP model approximates the original tensorXas a sum of R rank-one tensors:X≈∑r=1Rλrar(1)∘⋯∘ar(N)=〚λ;A(1);…;A(N)〛.Note that〚λ;A(1);…;A(N)〛is shorthand notation to describe the CP decomposition, whereλis a vector of the weightsλrandar(n)is therth column ofA(n). Fig. 2conceptually illustrates the process of generating phenotypes via a CP decomposition. The details of our algorithm to generate concise phenotypes are presented in Section 3.2.While several other tensor decomposition methods exist (Kolda and Bader provide a survey of existing models and example applications in their paper [48]), we focus on the CP decomposition for two primary reasons: (i) it is a well-known and commonly applied tensor factorization model [55], and (ii) the resulting structure (R rank-one tensors) is well-suited for capturing medical concepts in a concise and interpretable manner. The CP decomposition has been used to complete missing data in medical questionnaires [56], localize and extract artifacts from EEG data to analyze epileptic seizures [57,58], and as an exploratory decomposition tool for wavelet-transformed multi-channel EEG data [59].Nonnegative tensor factorization (NTF) models have been proposed for CP decompositions. Analogous to NMF, NTF requires the elements of the factor matrices and the weights to be nonnegative. Some examples of NTF models in the medical and bioinformatics domain include the extraction of features from EEG data [60,61] and gene-sample-time microarray data [62]. Cichocki et al. provides a broad survey of practical and useful NMF and NTF algorithms [42].The standard CP model is well-suited for continuous data, where the random variation follows a Gaussian distribution. However count data, which is nonnegative and discrete, is better described using a Poisson distribution [63]. The nonnegative CP alternating Poisson regression (CP-APR) model has been developed to fit count data [40]. We provide the CP-APR optimization problem formulation from [40] for convenience:(2)minf(M)≡∑i→mi→-xi→logmi→︸Kullback-Leibler(KL)divergencesubject toM=〚λ;A(1);…;A(N)〛∈Ω←sample space ofMΩ=Ωλ×Ω1×⋯×ΩNΩλ=[0,+∞)R←weights are nonnegativeΩn={A∈[0,1]In×R|||ar||1=1∀r}︸stochastic constraints on columns,wherei→represents the tensor element index(i1,i2,…,iN),Xis the observed tensor, andMis the CP tensor factorization that approximatesX.The CP-APR algorithm solves the optimization problem via an alternating minimization approach, where each subproblem computes the solution for an individual mode while fixing all the other modes. CP-APR specifies the mode-n matricization asXasX(n)=B(n)Π(n)[40], whereLet:B(n)=A(n)λΠ(n)=(A(1)⊙…⊙A(n-1)⊙A(n+1)⊙…⊙A(N))⊺.B(n)represents the weighednth mode factor matrix andΠ(n)denotes the fixed part.2The definition of the Khatri-Rao product is provided in the supplemental material.2The CP-APR optimization subproblem (repeated from [40]) for thenth factor matrix is:(3)B(n)=argminB⩾01⊺[BΠ(n)-X(n)∗log(BΠ(n))]1.In Eq. (3),1corresponds to a vector of ones and captures the summation of the tensor elements shown in Eq. (2). The details of the subproblem solver and the overall CP-APR algorithm can be found in the paper [40].NTF generally results in sparse representations. However, additional sparsity may be desired, for example, to improve factor interpretability. Various techniques have been used to induce sparsity, such as extending an NMF sparseness measure [64], enforcingL1penalties on the factor matrices and/or the core matrix for Tucker models [65–67], or regularizing the factors with bothℓ1andℓ2norms [52].Limestone is a tensor factorization model to achieve high-throughput phenotyping from EHR data. Our model extends the CP-APR work to (i) produce concise phenotype definitions for better interpretability and (ii) calculate a new patient’s phenotype membership given the learned phenotypes. Fig. 3illustrates the conceptual diagram for the Limestone process. This section details the tensor construction from raw EHR data, formally defines the candidate phenotypes obtained via tensor factorization, and the process to obtain the phenotype membership matrix for new patients.The first step in Limestone is to construct a count tensor from the raw EHR data. In this paper, we focus on diagnoses and medications due to their prominence in existing phenotype definitions [29,68]. However, our tensor construction can be generalized to other EHR data. We use medication orders from the raw EHR data that details the interaction between diagnoses and medications. Each medication order contains the prescribed medication, the diagnosis (such as an ICD-9 billing code) associated with the prescription, and the date of the prescription.Each patient is anchored using an index date (e.g. heart failure diagnosis date). The observation window is defined as a fixed time window of 2years prior to the index date, as illustrated in Fig. 4. Only data occurring during the observation window is used for the raw EHR construction. The tensor is constructed using the count of the co-occurrences between medications and diagnoses. For Fig. 4, the patient has the following counts in the 2-year observation window encompassing 3 visits:•2 counts of loop diuretics to treat coronary atherosclerosis;1 count of cardio-selective beta blockers to treat coronary atherosclerosis;2 counts of sulfonylureas to treat diabetes;1 count of nitrates to treat coronary atherosclerosis; and1 count of ACE inhibitors to treat hypertension.Note that the medication orders of sulfonylureas to treat diabetes at timet0and loop diuretics to treat congestive heart failure at timet4are outside the window and omitted from the tensor construction.The result is a third-order tensor with a patient mode, diagnosis mode, and medication mode. Each tensor element denotes the number of times medication m is prescribed to treat diagnosis d for patient p. Slicing the tensor along the three different modes yields the following views:1.Patient mode: a matrix of the patient’s diagnoses and associated medication treatment.Diagnosis mode: a matrix of the prescribed medications to treat the disease for all patients.Medication mode: a matrix of all the patients and the diseases treated with this medication.The count tensor is a more natural representation of the interactions between diagnoses and medications as it succinctly captures hierarchical information such as the set of medications that are used to treat a disease. In addition, the Limestone implementation only stores the non-zero elements of the tensor for efficient memory storage.Limestone extends the CP-APR model to derive phenotype candidates without supervision. The third-order count tensor is approximated using the CP decompositionM=〚λ;A(1),A(2),A(3)〛, shown in Fig. 2. The factor matrix for thenth mode,A(n), defines the elements from the mode that comprise the candidate phenotypes. Thus thejth candidate phenotype is defined using thejth column from the three factor matrices. Note that the stochasticity constraint (i.e., the last line in Eq. 2) on the factor matrix yields a conditional probability of the element’s membership to the phenotype. Given thejth phenotype,aij(k)represents the probability of seeing theith element in thekth mode. Thus, the sum of the entries for a mode element (∑jaij(k)) across all the phenotypes may not equal 1. Furthermore, λ allows us to automatically rank the candidate phenotypes in order of significance, or the candidate phenotype’s ability to capture the tensor data. Fig. 2 illustrates the tensor factorization of a patient by diagnosis by medication tensor into R phenotypes.We provide an illustrative example of a candidate phenotype resulting from Limestone in Fig. 5. The percentage of patients with the phenotype is calculated using the percentage of non-zero elements in thekth column of the patient factor matrix. The phenotype is defined as patients diagnosed with hypertension and taking three medications: (1) beta blockers cardio-selective, (2) thiazides and thiazide-like diuretics, and (3) HMG CoA reductase inhibitors. Limestone produced a single non-zero element along the diagnosis factor and three non-zero components along the medication factor.Our proposed model incorporates a sparsity constraint to minimize the presence of “minuscule and unnecessary” factor components. We extend the original CP-APR model by employing a hard-thresholding operator [69] to further reduce the phenotype factors by removing small factor components. Thus, Limestone minimizes KL divergence with a hard thresholding constraint, replacing Eq. (2) with the following objective:(4)min∑i[mi-xilogmi]︸CP-APR objective+γ∑j,n,r1ajr(n)>0︸hard-thresholding operator,whereajr(n)denotes thejth component of the factor vectorar(n). Individual componentsajr(n)that are below the threshold2γare set to zero. Thus, the candidate phenotypes are concise, which should offer better interpretability.Limestone also computes a new patient’s phenotype membership vector by projecting their observed features onto the space of existing candidate phenotypes. The phenotype membership vector(a)ˆ1is defined as the convex combination of the candidate phenotypes, where therth element of the vector,aˆr(1), is the probability the patient belongs torth phenotype. For example, a new patient’s vector may indicate probabilities of 0.6, 0.3, and 0.1 for the phenotypes of diabetes type 2, severe hypertension, and asthma, respectively. Note that the phenotype membership vector is not equivalent to the patient factor matrix, as therth column of the patient factor matrixA(1)represents a probabilistic interpretation over the entire patient population for a single phenotype.Our method uses the diagnosis factor matrixA(2)and the medication factor matrixA(3)from the existing candidate phenotypes to calculate the phenotype membership vector. Thus, given a new patient’s data,X^, we wish to findλˆandaˆ(1)that best approximates the new patient’s tensor:X^≈∑rλˆraˆr(1)︸membership∘ar(2)∘ar(3)︷phenotype definitions.t∑raˆr(1)=1.The projection onto the candidate phenotypes is illustrated in Fig. 6. Therefore, the optimization for calculating the phenotype membership vector isbˆ(1)=argminb⩾01⊺[bΠ(1)-X^(1)∗log(bΠ(1))]1s.t∑raˆr(1)=1,wherebˆ(1)=(a)ˆ1Λˆ. The objective function of this problem is equivalent to the optimization subproblem for mode 1. Therefore, we can utilize the same iterative MM approach to solve for the optimalbˆ(1). The new patient’s phenotype membership vectoraˆ(1)is the entries ofbˆ(1)normalized by the weightsλˆ.Software implementation. We developed a Python package based on the Matlab Tensor Toolbox3http://www.sandia.gov/tgkolda/TensorToolbox/index-2.5.html.3and Pytensor,4https://code.google.com/p/pytensor/.4a partial Python implementation of the Matlab Tensor Toolbox. Our software package implements the CP-APR algorithm described in [40] and provides the functions to post-process the tensor decomposition to obtain concise phenotypes and project new patients onto learned phenotypes.Our case study focuses on heart failure (HF), a leading cause of healthcare use with a projected medical cost in 2015 of $32.5 billion [70]. Heart failure (HF) affects roughly 5.7 million people in the US and is mentioned as the contributing cause for 1 out of every 9 deaths [71]. Nearly a quarter of the patients hospitalized with heart failure are readmitted within 30days [72]. Thus far, heart failure research has focused on epidemiology results, lifetime risk assessments from the Framingham study [73,74], predictions of hospital readmissions [75] or survival [76], and data-driven feature selection to complement known risk factors [77]. We demonstrate Limestone on a dataset primarily used for heart failure onset prediction studies and illustrate the potential of tensor factorization to derive candidate phenotypes without the supervision of domain experts. For this section, we will refer to candidate phenotypes (discovered clusters) as phenotypes for simplicity.Evaluation Metric Details. Our case study focuses on algorithmic evaluation and qualitative analysis of Limestone-derived phenotypes. We will evaluate the results in terms of similarity, conciseness, predictive power, and clinically meaningfulness. The metrics we will use are the following:1.Similarity(ar,br)=ar⊺br∥ar∥∥br∥.Conciseness=number of non-zero elements per mode.Predictive power=area under receiver operator characteristic curve (AUC) on a classification task.Clinical meaningfulness=domain expert’s opinion of whether or not a Limestone-derived phenotype mapped to a medical concept.The similarity calculation is the cosine similarity between two vectors, a component of the factor match score (FMS). FMS is [40,78,79] commonly used to compare two tensor factorization results, quantifying the closeness via a single number between[0,1]. However, FMS is an aggregate measure and can mask the mode-specific similarity results. Therefore, we compare the cosine along each mode, where the ideal value with two equivalent vectors is 1. Phenotypes from the two tensor factorization results are paired using an existing greedy FMS algorithm [40].The data for this study is based on real EHR data from the Geisinger Health system, which contains over 7years of longitudinal patient records. The dataset has a diverse set of clinical information that includes diagnoses comprised of ICD-9 billing codes and medication records with generic drug names, pharmacy class and subclass information. For this study, we analyze the following sets of patients:1.4626 case patients, where each patient has at least 2 outpatient HF diagnoses or 1 outpatient HF diagnoses with 2 or more HF medications.27,189 group-matched control patients, where each case is matched with 10 controls with the same gender, age, and clinic information of the case patients.5Note that the same control patient may be matched by multiple cases. Thus, we post-process the controls to make sure each control patient is only matched with one case. The 19,071 duplicate controls are removed from the dataset.5The control patients did not meet the HF diagnosis criteria described above.In the study, the heart failure index date for control patients is the date of the matched case patient (e.g. if the case patient was diagnosed on January 4, 2014, then the matched control patient would use January 4, 2014 as the index date). Further details of the cohort construction can be found in [23].The Geisinger dataset recorded the interaction between diagnoses and medications in the medication orders table. Each medication order contains the prescribed medication, the diagnosis (ICD-9 billing code) associated with the prescription, and the date of the prescription. Any medication that was used to treat several diagnoses has multiple entries corresponding to each diagnosis code. The raw diagnosis code and medication captures information at a fairly fine-grained level, which is not ideal for analysis because similar diagnoses and medications are considered independently. To avoid this problem, we consolidated the individual diagnosis codes and medications to higher level concepts using existing medical hierarchies. Specifically, diagnosis codes are aggregated using the Centers for Medicare and Medicaid (CMS) Hierarchical Condition Categories (HCC) and medications defined as pharmacy subclass (e.g. ACE inhibitors, calcium channel blockers, etc.).6Note that other hierarchies, such as the PheWAS code groups [80] could have been employed.6This resulted in 169 distinct HCC categories and 471 pharmacy subclasses. Therefore, the constructed tensor size for the control patients population is27,189patients by 169 diseases by 471 medications, where<1%of the tensor are non-zero.The first series of experiments focuses on evaluating the convergence, stability, computation time, and sparsity of Limestone. The following questions will be answered:1.How many alternating minimization iterations are necessary to converge to a stable solution?Are the generated phenotypes stable towards perturbation and different initializations?How concise are the generated phenotypes?Given a fixed number of phenotypes (R), we examine the KL divergence (or the objective function values) as a function of the number of alternating minimization iterations across 10 randomly initialized factorizations of the case patients’ tensor. The KL divergence is defined as∑i→mi→-xi→logmi→. Fig. 7a shows the mean and confidence interval of the objective function values as the number of iterations are increased. The first 30 iterations result in a significant decrease in the negative log-likelihood. Above 80 iterations, there are only slight changes in the objective function values with the values flattening around 120 iterations. The results suggest that less than 80 iterations are needed for convergence in this dataset.Our algorithm uses random matrices for the initial factor matricesA(n)which can have an impact on the solution of the tensor factorization. Thus, we study the effect of 10 random initializations of Limestone, factorizing the case patients tensor with a fixed number of phenotypes and varying number of maximum iterations. Fig. 7b illustrates the similarity score for each mode. The results show that the similarity scores are high across all three modes beyond 70 iterations. In particular, the diagnosis and medication modes have scores above 0.70. Note that the score of two random factors will tend towards 0. As such, in this case study, we can conclude that phenotype definitions are generally similar regardless of the initial factor matrices.We also study the effect of noise, or perturbation, on the tensor factorization results. Two experiments were performed:1.Additive noise: Poisson noise (∊∼Poisson(2)) is added to randomly selected non-zero elements of original tensor, increasing the overall mean of the tensor.Additive and subtractive noise: Random addition or subtraction of Poisson noise (∊∼Poisson(2)) to randomly selected non-zero elements of the original tensor. If subtraction results in a negative value, the value is set to zero and a random zero element of the original tensor is selected for added noise to maintain the overall mean and sparsity pattern of the original tensor.The resulting “noised” tensor is then factorized and compared to the original factorization using the similarity score.Figs. 8a and b illustrates the average similarity scores for 10 random noisy tensors as a function of the percentage of noised elements. The results show a decay in the similarity score as the percentage of perturbed elements increases, where the effect is more prominent in the additive and subtract noise results. However, even when half of the non-zero elements are perturbed for both experiments, the diagnosis and medication mode similarities remain above 0.75, an impressive number given the high dimensionality of our dataset. This observation suggests that phenotype definitions are stable with regards to perturbation.Limestone uses a hard thresholding operator which enables a tunable parameter to adjust the sparsity of the phenotypes. Fig. 9shows a graph of the individual mode component values for the diagnosis and medication modes for a case patients tensor factorization. A majority of the nonzero elements in the diagnosis and medication factor matrices are below 0.05 (the two points furthest left in the plot). However, a reasonable number of the components along the diagnosis factor have values above 0.75, while the medication factors tend to have several medications (centered closer to 0.20). Thus, individual components less than a threshold of 0.05 contribute minimally to the phenotype definition in comparison with the other non-zero elements and can be triaged to produce concise phenotypes.Fig. 10shows the number of non-zero entries for the diagnosis and medication factors using the suggested threshold from above. Twelve of the phenotype were defined using a single diagnosis. A majority of the phenotype definitions contained less than five medications. Thus, in this case study, Limestone produces concise phenotypes at the threshold of 0.05, where all phenotypes contain less than eight non-zero elements per factor.

@&#CONCLUSIONS@&#
