@&#MAIN-TITLE@&#
Classification of social laughter in natural conversational speech

@&#HIGHLIGHTS@&#
We observed several types of laughs in a natural speech corpus, and two predominant types of laughter (social vs. sincere) were categorized from a data manual examination of the data.Global prosodic and laughter-specific acoustic features were extracted for the two types of laughter. These parameters were analysed by Principal Component Analysis and Classification Trees to reduce the number of parameters.A Support Vector Machine was trained and tested using seven important features, and total classification accuracy was confirmed to be at least over 84 with unseen test material.

@&#KEYPHRASES@&#
Laughter,Prosody,Paralinguistic information,Non-verbal behaviour,Classification,Support Vector Machines,

@&#ABSTRACT@&#
We report progress towards developing a sensor module that categorizes types of laughter for application in dialogue systems or social-skills training situations. The module will also function as a component to measure discourse engagement in natural conversational speech. This paper presents the results of an analysis into the sounds of human laughter in a very large corpus of naturally occurring conversational speech and our classification of the laughter types according to social function. Various types of laughter were categorized into either polite or genuinely mirthful categories and the analysis of these laughs forms the core of this report. Statistical analysis of the acoustic features of each laugh was performed and a Principal Component Analysis and Classification Tree analysis were performed to determine the main contributing factors in each case. A statistical model was then trained using a Support Vector Machine to predict the most likely category for each laugh in both speaker-specific and speaker-independent manner. Better than 70% accuracy was obtained in automatic classification tests.

@&#INTRODUCTION@&#
In human–human interaction, communication involves both verbal and nonverbal information, and the latter serves especially to express discourse engagement. One of the most common nonverbal vocalizations in social conversation is laughter (Petridis, 2011) which is also reported as the most frequently annotated acoustic nonverbal behavior in meeting corpora (Laskowski and Burger, 2007) where 8.6% of the time a person vocalizes in a meeting is spent on laughing and 0.8% is spent on laughing while talking. Laughter is a universal and prominent feature of human communication (Jung, 2003), and expressed by both vocal and facial expressions. It is a powerful affective and social signal (Vinciarellia et al., 2009). There is no culture where laughter is not found. However, current dialogue systems and computer-based social skills training (a training method for people with autism or Asperger syndrome to learn social function (Ozonoff and Miller, 1995)) do not take into account laughter (Golan and Baron-Cohen, 2006).In a seminal study of the segmentation of laughs, Trouvain and Schroder (2004) suggest that we consider laughter as articulated speech, where at the low level there are sound segments that are either vowels or consonants. At the next higher level, there are syllables consisting of sound segments. The next higher level deals with larger units such as phrases which are made up of several syllables. Owren and Understanding (2007) recommend the term ‘bout’ for the longer sequence, and ‘call’ for the individual syllables; we will adopt that terminology in this study.Some earlier work on the automatic segmentation of laughter has been reported in the literature. Truong et al. (2007) reported automatic laughter segmentation in meetings. They performed laughter vs. speech discrimination experiments comparing traditional spectral features and acoustic phonetic features, and concluded that the performance of laughter segmentation can be improved by incorporating phonetic knowledge into the models. Scherer et al. (2012) reported that the total accuracy of detecting laughter from natural discourse in human–computer interaction reached over 90% in online and offline detection experiments with speech and visual information. Kennedy and Ellis (2004) focused on joint laughter in meetings, which means participants (more than just one) laugh simultaneously (Glenn, 1991; Jefferson, 1979; Kangasharju and Nikkot, 2009), and they obtained detection results with a correct accept rate of 87% and a false alarm rate of 13% by using Support Vector Machines.Types of laughter vary in natural conversational speech, and some classifications have been reported in the literature regarding different categories of laughter. Most types of laughter were discussed in Shimizu et al. (1994), and the major work is the discrimination of laughter into two types, voiced and unvoiced, based on acoustics (Bachorowski and Owren, 2001; Hudenko et al., 2009). Laurence and Laurence (2007) deal with a study of laughs in spontaneous speech and explore the positive and negative valence of laughter towards their global aim of detecting emotional behavior in speech. The conclusion of their acoustic analysis is that unvoiced laughs are more often perceived as negative and voiced segments as positive. Previous work in the literature has also discussed whether laughter patterns can be defined through stereotypes (Bachorowski et al., 2001; Trouvain and Schroder, 2004; Sundaramb and Narayananc, 2007). However, laughter is not simply positive or negative, or even defined by stereotypes; it is quite usual for people to infer different degrees of emotion and engagement based on its perceptions, and it is common for people to make use of social laughter in sophisticated social interaction. In this study we tested perceptual types of laughter to determine the main characteristics of laughter in social interaction by reference to the above previous studies.Automatic classification of four phonetic types of laughter in a natural-speech conversation corpus was conducted by Campbell et al. (2005), based on perceptual impressions of laughter, in which a laughter episode is considered as a sequence of speech-like phonetic segments (after Bachorowski et al., 2001). The work described 4 different laughter types: voiced, chuckle, breathy and nasal, and modeled each laugh as composed of different combinations of these segments using Hidden Markov Models (HMMs) statistical classification. The study reported an automatic discrimination using 3–15 states with mfcc-based HMMs for 4 functions of laughter (hearty, amused, satirical, and polite). In categorizing emotional classification the work achieved 76% accuracy. However because of the hidden nature of the statistical modeling the report did not provide explicit details about which specific acoustic features contributed to the various categorizations of the laughter.We report progress towards developing a sensor module that categorizes types of laughter for application in dialogue systems or social skills training situations. In the present study we only make use of the audio information but recognize that facial expression also carries an important channel of communicative information (Carroll and Russel, 1996; De Gelder and Vroomen, 2000). This paper reports a study of laughs in a corpus of human–human dialogues recorded from Japanese telephone conversational speech (The Expressive Speech, 2013). We employed a corpus of natural spontaneous speech where laughter occurred naturally as a consequence of the dialogue interaction. We specifically avoid the use of contrived laughter or even specifically elicited laughs since they may not be representative of natural spontaneous interaction.In the following sections we first provide details of the corpus, then introduce two Experiments. Experiment 1: a perceptual test by Japanese students to determine the number and types of easily discriminated laughter, and Experiment 2: describing the acoustic feature extraction, presenting the results of an analysis of the main acoustic features and finally reporting a classification of type of laughter using statistical methods.We used two types of Japanese corpora. First, the Expressive Speech Processing (ESP) corpus (The Expressive Speech, 2013) was used for this study. The speech data were recorded over a period of several months, with paid volunteers coming to an office building in a large city in Western Japan once a week to talk with specific partners in a separate part of the same building over an office telephone. While talking, they each wore a head-mounted Sennheiser HMD-410 close-talking dynamic microphone and recorded their speech directly to DAT (digital audio tape) at a sampling rate of 48kHz. They did not see their partners or socialize with them outside of the recording sessions. Partner combinations were controlled for sex, age, and familiarity, and all recordings were transcribed and time-aligned for subsequent analysis. Recordings continued for a maximum of eleven sessions between each pair which were numbered consecutively as session 01 to session 11. The additional eleventh session was only used in the case of absence of one of the volunteers from one of the regular sessions but provided useful additional material. Each conversation lasted for a period of 30min. In all, ten people took part as speakers in the recordings, five male and five female. Six were Japanese, two Chinese, and two native speakers of American English. All were resident and working in Japan at the time. The speech data were transferred to a computer and segmented into separate files, each containing a single utterance for manual transcription by professional transcribers. Laughs were marked with a special diacritic, and laughing speech was also bracketed to show which sections of ordinary speech were spoken with a laughing voice. Laughs were transcribed using the Japanese Katakana phonetic orthography, wherever possible, alongside the use of the identifying symbol. The present analysis focuses on speakers JMA (age 20s) JMB (age 20s), EMA (age 20s), EFA (age 20s), CMA (age 30s), and CFA (age 20s) to confirm that the same types of laughter are common across different native language groups. The other speakers are all female and similar to the speaker FAN in terms of age, sex, and native language, and thus we selected one female speaker as representative for the present analysis. JMC is omitted because his speech data is insufficient. The initial letters J, C and E indicate native speaker of Japanese, Chinese, and English respectively, M or F indicates the gender of speaker, and A or B indicates the session group of speakers as used for a different experiment.Second, data from speaker FAN (age 30s) was also used in this report. The FAN subset of the ESP corpus was recorded over a period of five years with everyday conversational speech collected from a single female volunteer wearing high-quality head-mounted microphones, recording her speech to a small Mini-Disc recorder as she went about her daily life. This part of the corpus features a lot of speech in various situations and much simple, repetitive and unstructured talk that illustrates how we spontaneously speak in everyday situations. Speaker FAN was a young female Japanese who personally provided more than 600h of usable speech material. Because we were not able to enter into contractual agreements with her various interlocutors, only the voice of FAN herself has been transcribed or analysed. While this material is less useful for the analysis of conversational interaction, it provides valuable insights into the range of voice qualities and speaking styles used by one person throughout her daily life.The study reported here includes two perceptual experiments. The first tested for perceptual types of laughter using Japanese students as subjects listening to the natural conversational speech recordings. We used these results to confirm the classification into the most easily perceived classes of laughter in the corpus. The second tested the degree to which opinions were shared between respondents in the initial classification. For both experiments we predicted the following:1.In social communication, people do not use hearty laughter with high frequency, rather they typically express polite social laughter (Experiment 1);There are some important acoustic features that can be used to distinctively classify the types of laughter; these include laughter specific parameters such as the number of the calls; andAutomatic classification of laughter is possible at rates greater than chance in both closed and open tests (Experiment 2).This experiment concerned the annotation of types of laughter found in the ESP corpus and we chose conversations between JMA and JMB, and JMA and EFA as illustrative.

@&#CONCLUSIONS@&#
