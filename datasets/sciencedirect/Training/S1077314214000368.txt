@&#MAIN-TITLE@&#
Exploiting relationship between attributes for improved face verification

@&#HIGHLIGHTS@&#
A novel method to model the relationship between attributes.An effective method to exploit the learned relationship for model training.A single framework adapted to both discrete and continuous attributes.Promising results for face verification and object recognition.

@&#KEYPHRASES@&#
Attribute relationship graph,Attribute-graph regularized SVM,Face verification,

@&#ABSTRACT@&#
Recent work has shown the advantages of using high level representation such as attribute-based descriptors over low-level feature sets in face verification. However, in most work each attribute is coded with extremely short information length (e.g., “is Male”, “has Beard”) and all the attributes belonging to the same object are assumed to be independent of each other when using them for prediction. To address the above two problems, we propose a discriminative distributed-representation for attribute description; on the basis of this description, we present a novel method to model the relationship between attributes and exploit such relationship to improve the performance of face verification, in the meantime taking uncertainty in attribute responses into account. Specifically, inspired by the vector representation of words in the literature of text categorization, we first represent the meaning of each attribute as a high-dimensional vector in the subject space, then construct an attribute-relationship graph based on the distribution of attributes in that space. With this graph, we are able to explicitly constrain the searching space of parameter values of a discriminative classifier to avoid over-fitting. The effectiveness of the proposed method is verified on two challenging face databases (i.e., LFW and PubFig) and the a-Pascal object dataset. Furthermore, we extend the proposed method to the case with continuous attributes with promising results.

@&#INTRODUCTION@&#
Recently, there has been growing interest in using middle-to-high level feature descriptors for face representation. One typical example is the attribute descriptors [1–9]. N.Kumar et al. [3,4] have recently shown that using the outputs of a series of component classifiers with each tailored to some particular aspects of the human face images, called visual attributes, they are able to achieve close to state-of-the-art performance of face verification on the challenging Labeled Faces in the Wild (LFW) [10]. This result is interesting in several aspects. Firstly, the number of features used in their work is very small (i.e., only 73 attributes), which means that it provides a very economical but powerful way to describe faces. This is in sharp contrast with the commonly used low-level features in image description, such as pixel values, gradient directions, scale-invariant feature transform (SIFT) [11], where usually thousands of features are needed. Secondly, the attribute descriptor is user-friendly in that its meaning is understandable to human beings (everyone knows what “white male” means) while the meaning of most previously mentioned low-level features is less intuitive to us. Last but not least, such a descriptor is generalizable and sharable, which makes it particularly suitable for such problems as zero-shot learning [12,13] or between-class transfer learning [2,14].However, in most work each attribute is coded with extremely short information length (e.g., using binary code such as “is Male”, “has Beard”) and all the attributes belonging to the same object are assumed to be independent of each other when using them for prediction. The one-bit information length of attribute coding makes the representation less stable, and could bring trouble to many interesting subsequent processing tasks, such as modeling the similarity between attributes. Actually, research in the field of cognitive discovery has shown the usefulness of the relationship between feature sets. For example, Bhatt and Rovee-Collier [15] experimentally showed that infants as young as three months of age gain the capability to encode the relations among object features, and use such a feature configuration for general object recognition. However, traditionally one of the major challenges in modeling the feature configurations lies in the huge number of low-level features (e.g., the dimension of a100×100face image is as high as10,000using the gray-value features). In addition, it is very difficult for a human being to understand what exactly such a big feature configuration mean. Fortunately, both aforementioned problems can be addressed by the attribute descriptors due to its high level and compactness in object description. Indeed, despite the partial success of using attribute descriptors by treating them statistically independent of each other [1,3,4,16] or conditionally independent given the class label [2], recent work has shown that it is beneficial to exploit the relationship between attributes under various contexts [5,17–19]. Some of them will be discussed in the next section.In this work, we propose a discriminative distributed-representation for attribute description; on the basis of this description, we investigate how to model the similarity relationship between attributes and how such relationship could be exploited to improve the performance of face verification. The idea of distributed representation was first introduced by Hinton [20], and successfully applied in statistical language modeling [21]. In this work, we develop a new distributed representation for each individual attribute by taking the information of subject identification into account. The method is inspired by the vector representation of words in the literature of text categorization, and the meaning of each attribute is embedded into a high-dimensional vector in the subject space (cf.Fig. 1). Such a representation allows us to model the similarity between attributes in a much stable and reliable way. In particular, we construct an attribute-relationship graph based on the distribution of attributes in the subject space, which effectively encodes the pairwise closeness relationship between any two attributes. For example, a “male” attribute is highly related to such attributes as “wearing necktie”, “bushy eyebrows”, “beard”, and so on (cf.Fig. 9). To exploit such information for prediction, we integrate the attribute-relationship graph into a linear classifier to constrain the searching space of its parameters, based on the assumption that similar attributes should have similar weights. This is helpful to avoid over-fitting and improve the generalization capability of the learned classifier. The uncertainty in attributes responses is also taken into account in the final model.This journal paper builds on the earlier conference work [22]. In this extended version, we extend above ideas and merge them into a single framework, which works for both discrete and continuous attributes. The effectiveness of the proposed methodology is empirically verified with encouraging results on two large-scale face databases, one object classification dataset and several UCI data sets. In what follows, we first review the related work in Section 2 and then present the proposed method in Section 3. Extensive experimental results are given in Section 4. Finally, we conclude this work in Section 5.

@&#CONCLUSIONS@&#
In this paper, we give a novel method to model the relationship between attributes, based on a discriminative distributed representation for attributes. This effectively allows our classifier to explore the hidden correlation between attributes in a general context of subjects. We show in this paper on the challenging face verification and object recognition databases that the mined attribute graph does reflect some real aspects of the semantic relationship among attributes existed in the real world; furthermore, such relationship is helpful to improve the accuracy and robustness of the face verification/object recognition system.We also extend this method to more general scenarios where the values of attributes are continuous, and this leads to a new attribute-graph regularized SVM algorithm, which essentially opens a door to incorporate additional structural (or spatial) information at the level of attributes into the classifier. The effectiveness and feasibility of the proposed methodology are empirically verified in several application domains, showing that it can improve overall classification results, even when the available data are limited.Although we do not focus on the problem of attribute extraction in this paper, our experiments on a-Pascal object dataset indicate that the accuracy of extracted attributes could have a great impact on the final performance of an attribute-based recognition system (cf.Fig. 12). For accurate attribute extraction, we have to decide where to look at them. For example, Kumar et al. [4] performed a greedy forward selection for each attribute to find out the most discriminative local facial regions among the predefined nine regions. Ferrari and Zisserman [19] proposed to tackle the uncertain location of features by optimizing the likelihood ratio. Chen et al. [9] exploited pose information for attribute extraction, and they also considered the sensitivity of different attributes to different feature types. Alternatively, attributes can be treated as a kind of latent variables, and the task of attribute prediction simply boils down to assign values to latent variables [5,25]. These latter methods actually get around the problem encountered in the former ones, but the extracted attributes are less interpretable. Further study on this will be the focus of our future work.