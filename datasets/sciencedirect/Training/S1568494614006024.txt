@&#MAIN-TITLE@&#
Dynamic clustering with improved binary artificial bee colony algorithm

@&#HIGHLIGHTS@&#
We proposed an improved binary artificial bee colony algorithm (IDisABC).We examined the proposed algorithm on dynamic clustering.Data and image clustering benchmark problems are chosen for experiments.The obtained results are compared with K-means, FCM, GA, DisABC, DCPSO.

@&#KEYPHRASES@&#
Cluster analysis,Automatic clustering,Discrete optimization,Binary artificial bee colony algorithm,

@&#ABSTRACT@&#
One of the most well-known binary (discrete) versions of the artificial bee colony algorithm is the similarity measure based discrete artificial bee colony, which was first proposed to deal with the uncapacited facility location (UFLP) problem. The discrete artificial bee colony simply depends on measuring the similarity between the binary vectors through Jaccard coefficient. Although it is accepted as one of the simple, novel and efficient binary variant of the artificial bee colony, the applied mechanism for generating new solutions concerning to the information of similarity between the solutions only consider one similarity case i.e. it does not handle all similarity cases. To cover this issue, new solution generation mechanism of the discrete artificial bee colony is enhanced using all similarity cases through the genetically inspired components. Furthermore, the superiority of the proposed algorithm is demonstrated by comparing it with the basic discrete artificial bee colony, binary particle swarm optimization, genetic algorithm in dynamic (automatic) clustering, in which the number of clusters is determined automatically i.e. it does not need to be specified in contrast to the classical techniques. Not only evolutionary computation based algorithms, but also classical approaches such as fuzzy C-means and K-means are employed to put forward the effectiveness of the proposed approach in clustering. The obtained results indicate that the discrete artificial bee colony with the enhanced solution generator component is able to reach more valuable solutions than the other algorithms in dynamic clustering, which is strongly accepted as one of the most difficult NP-hard problem by researchers.

@&#INTRODUCTION@&#
With the rapid development of computer hardware and software, datasets with great capacities can be stored without more effort. However, these datasets cannot be used or specified by users without of any pre-process. One of the new interdisciplinary fields of computer science, data mining concerns with datasets by basically trying to extract meaningful data and summarizing it into useful information through clustering, feature extraction, statistical tests, etc. [1]. In this study, general motivation just focuses on clustering, which is one of the most appreciated subjects by the researchers and is used in many real-world applications such as bioinformatics, machine learning, image analysis, and pattern recognition and market analysis. In clustering, the main goal is to divide data into groups or clusters based on some similarity measures like distance, intervals within multidimensional data [2]. Through clustering, valuable information can be extracted from enormous quantities of data.Clustering algorithms fall into two main categories, hierarchical and partitional algorithms. Hierarchical algorithms are based on the use of proximity matrix indicating the similarity between every pair of data points to be clustered and its result is “dendrogram representing the nested grouping of patterns and similarity levels at which groupings change and levels are created through bottom up or bottom down approaches [2]”. In agglomerative (bottom up) hierarchical algorithms, each data member is assigned to a unique cluster, then two clusters are found repeatedly according to the proximity matrix and finally they are merged. The basic agglomerative hierarchical algorithm has the following steps [3]: firstly, construct a similarity matrix which shows the difference between each pair of data. After that, assign K=N where N is the number of data and K is the number of clusters. Then repeatedly find the nearest pair of distinct clusters, merge these clusters and decrement K, 1 by 1 while K>1. The process of merging clusters can be applied by different ways, but the well-known are single link and complete link. Single link algorithms are based on merging two groups which have the smallest distance between their closest members. In contrast, complete link algorithms are based on merging groups which have the smallest distance between their most distant members. As for the divisive hierarchical algorithms, all data members are assigned to one cluster and then spitted a cluster at each stage until number of clusters is equal to the number of data points. Although the number of clusters is not predefined and the initial conditions do not affect the clustering process in hierarchical algorithms, these algorithms are not dynamic i.e. after a data point assigned to a cluster, its cluster cannot be updated, and the lack of information about global size and shape might cause overlapping clusters [4].Contrary to hierarchical algorithms, partitional algorithms allow cluster members to be updated if it improves clustering performance. Partitional clustering attempts to decompose the data into a set of disjoint clusters using similarity criterion (e.g. square error function) which is tried to be minimized by assigning clusters to peaks in the probability density function, or the global structure [5]. Therefore, partitional clustering can be regarded as an optimization problem, which is also considered in this paper. In the usage of partitional clustering algorithms, the disadvantages of hierarchical algorithms are the advantages of partitional clustering algorithms, and vice versa [6].Clustering can also be applied in two different modes: crisp (hard) and fuzzy (soft). Crisp clustering algorithms assume that each pattern should be assigned to only one cluster and the clusters are disjoint and non-overlapping. The most well-known example for the crisp clustering is the K-means algorithm. K-means [7] starts with K number of predefined clusters and then assigns each data member to its closest cluster. After the assignment, each cluster centroid is updated and this process is repeated until the termination criterion is satisfied. As in fuzzy clustering, a pattern may be assigned to all the clusters with a certain fuzzy membership function [2] (e.g. fuzzy C-means (FCM) [8]).On account of the fact that K-means and FCM excessively depend on initial conditions, modifications have been proposed to improve the performance of the algorithms [9–12]. Moreover, evolutionary based clustering algorithms have been proposed in order to overcome local minima problem of these clustering approaches. Particle swarm optimization (PSO), proposed by Kennedy and Eberhart in 1995 [13,14], was applied to the clustering problems, and better performances were gained against K-means [15,16]. Omran and Al-Sharban [16] applied Baribones-PSO to image clustering problem. Wong et al. [17] proposed an improved version of the objective function, which was firstly proposed by Omran et al. [6]. Besides PSO, Hancer et al. [18,19] developed an artificial bee colony based brain tumour segmentation methodology from MRI images with the previously proposed objective function [6]. Ozturk et al. [20] determined the drawbacks of the objective functions in the literature and improved a new objective function satisfying well-separated and compact clusters. Moreover, the Ant colony optimization (ACO) was also applied to the clustering problem [21,22]. The detailed information can be found in [23–26].It is clear that the number of clusters cannot be easily specified in many real world applications and datasets; therefore, the above mentioned algorithms requiring number of clusters as a parameter cannot be effectively employed. On behalf of these understanding, finding the “optimum” number of clusters in a data set has become an important research area. Proposed by Ball and Hall [27], ISODATA splits or merges clusters throughout the programme based on certain criteria in order to increase or decrease the number of clusters. However, ISODATA asks the user to specify the values of several parameters (e.g. the merging and splitting thresholds) and it can only merge two clusters under a user specified threshold [6]. Dynamic optimal cluster-seek (DYNOC) [28], similar to ISODATA, is based on maximizing the ratio of minimum inter-cluster distance to maximum intra cluster distance, but it also requires user specified parameters. Snob [29], Wallace's programme for unsupervised classification of multivariate data, uses the minimum [message or description] length [encoding] (MML or MMD) principle to decide upon the best classification of the data in order to assign objects to a cluster.Evolutionary based algorithms have also been applied to the dynamic clustering problem, particularly in last decade. Omran et al. [4] proposed a PSO based dynamic image clustering (DCPSO), which was inspired by the ideas of Kuncheva and Bezdek [30]. In DCPSO, a cluster set (S) is first created and then binary PSO is applied to select cluster centroids from S. After that, the obtained cluster centroids from S within the best solution are refined by K-means. Das et al. [31,32] proposed differential evolution based algorithms (ACDE and AFDE) in which the parameters of F-scale and crossover rate are determined adaptively. In ACDE, each solution is represented by cluster centroids and associated activation values ([0,1]). Through evaluation, cluster centroids and their activators are updated simultaneously. Thus, it does not need to employ K-means to decrease the effects of initial conditions as in DCPSO. Kuo et al. [33] improved a hybrid PSO&GA algorithm to overcome the convergence problem of the PSO algorithm. However, the applied objective fitness function, based only on Euclidean distance, is not very convenient for dynamic clustering problem. Maulik and Saha [34] proposed a modified differential evolution clustering algorithm based on information of local and global best positions (MoDEAFC) to automatically extract information from remote sensing images. Rui et al. [35] employed DE and PSO sequentially for odd and even iterations and presented a comparative study on clustering validity indexes.The main goal of this study is to demonstrate that the improved version of the discrete binary artificial colony algorithm (DisABC) [36] can be applied to the dynamic clustering problem. The novelty of the improved version of discrete artificial bee colony (referred as “IDisABC”) comprises two parts: modified random selection and modified greedy selection. These improved selection mechanisms are applied to search solution space intimately with the help of crossover and swap operators when the number of probable obtained outputs (M’vals) by dissimilarity calculation of two neighbourhood solutions is greater than one. In this way, the computational complexity of the algorithm is not affected so much. Moreover, the performance analysis and performance comparisons of the algorithms have been tested on benchmark problems in terms of the index quality, obtained number of cluster and correct classification percentage (CCP) by applying the static algorithms such as K-means and FCM in addition to the evolutionary computation based algorithms, including the DCPSO, GA and DisABC. It should be noticed that CCP is one of the most significant criterions to measure the quality of clustering; however, not many studies, especially related to dynamic clustering, reported the values of CCP.The rest of the paper is organized as follows; Section 2 describes the ABC algorithm; Section 3 demonstrates the IDisABC algorithm; Section 4 defines the clustering problem; and Section 5 presents the comparative results of the state of the art algorithms with the proposed algorithm. Finally, Section 6 concludes the paper.A model of intelligent behaviours of honey bee swarm introduced by Karaboga in 2005 [37], the artificial bee colony (ABC) algorithm is a novel swarm intelligence based algorithm and has been applied to various problems such as in optimization of numerical problems [38], data clustering [39], neural networks training for pattern recognition [40], wireless sensor network deployment [41] and routing [42] and image analysis [19,43]. The ABC algorithm comprises of three phases: employed bee phase, onlooker bee phase and scout bee phase. In employed bee and onlooker bee phases, a new solution is produced in the neighbourhood of current solution via Eq. (1);(1)Vij=xij+ϕij(xij−xkj)where j and k are randomly chosen indexes and j≠k, respectively, and ϕijis a random number in [−1,1]. If the value of new solution Viis better than the current one Xi, it is replaced with the current one. It should be noted that while employed bees search in the neighbourhood of current solutions, onlookers search in the neighbourhood of the solutions selected by applying probabilistic manner via Eq. (2) and (3). By this way, more qualified solutions are chosen to be improved by the bees;(2)fitnessi=11+fiti,fiti≥01+abs(fiti),fiti<0(3)pi=fitnessi∑SNj=1fitnessiwhere fitiis the cost value (value of objective function) of the food source Xiand SN is the number of solutions.After the processes of the employed and onlooker bee phases, scout bee phase checks whether there exists any abandoned food source, which exceeds a predetermined number of trials called “limit”. If so, a new solution is generated by Eq. (4) instead of the abandoned one;(4)xij=xjmin+rand(0,1)(xjmax−xjmin)where i=1, …, SN, SN is the number of solutions, j=1, …, D, D is the number of parameters.xjminis the minimum andxjmaxis the maximum values of parameter j. More information on bee swarm intelligence and artificial bee colony algorithm can be found in [44,45].The basic ABC is not suitable to the binary optimization problems since ABC was first proposed for the optimization of numerical problems. To apply ABC in binary problems, Pampapa and Engelbrecht [46] improved an angle modulation based binary ABC model (AMABC), in which each solution is represented by four dimensional real tuple and is then transformed into binary form for the evaluation of objective function. Kashan et al. [36] proposed a concept of dissimilarity between the binary vectors as a measure to evaluate how far the two binary vectors are apart from each other and used that concept to construct the binary ABC model (DisABC). Kiran and Gunduz [47] integrated a simple XOR operator based neighbourhood search mechanism to the ABC algorithm to solve the uncapacited facility location (UFLP) problem. Wei et al. [48,49] used two neighbourhoods (as in DE) and round operator to generate binary solutions. To our knowledge, not so many studies on binary ABC models can be reached in the literature, which motivates us to develop a new one.In this paper, the DisABC algorithm is modified by two improved selection schemes through genetic operators in the phase of new solution generation (referred as IDisABC). The applied operators in modified selection schemes are as follows:(1)Crossover operator: Crossover is used in population in order to increase possibility of getting optimal solution. The most widely used crossover operators are one-point, two-point and uniform. In this study, two-point is chosen as a crossover operator, where two positions on binary vectors are randomly selected and then the positions of the vectors between the determined positions are exchanged between binary vectors. An example of applying two-point crossover can be illustrated in Fig. 1.Swap operator: Swap is the act of exchanging information between objects in such a way that two positions, the values of which are 0 and 1, are switched to each other i.e. total number of ones and zeros in the binary vector is not changed, an example of which is presented in Fig. 2.To generate a solution in the neighbourhood of current solution in IDisABC, similarity measure is first calculated between them as in DisABC;(A) Similarity measure: The similarity between two vectors provides information about how far apart they are from each other and how similar they are to each other. Jaccard's coefficient is used to quantify the degree of similarity.Let Xi=(xi1, xi2, …, xiD), Xk=(xk1, xk2, …, xkD) are two binary vectors where D is the dimension of vectors. The similarity between Xiand Xkis defined by Eqs. (5) and (6);(5)Similarity(Xi,Xk)=M11M01+M10+M11(6)Dissimilarity(Xi,Xk)=1−Similarity(Xi,Xk)where M11 is the total number of bits where both Xiand Xkare equal to 1, M01 is the total number of bits where Xiis equal to 0 and Xkis equal to 1 and M10 is the total number of bits where Xiis equal to 1 and Xkis equal to 0.After calculation of the similarity, M’vals are calculated;(B) Calculation of M’vals: Eq. (1) can be defined in the form of Vi−xi=ϕ(xi−xk). The dissimilarity measure is placed with “−” operator. Then, the new differential equation can be defined by Eq. (7);(7)Dissimilarity(Vi,xi)≈ϕDissimilarity(xi,xk)Let m1 is the number of bits with value 1 in Xiand m0 is the number of bits with value 0 in Xi.(8)min1−M11M01+M10+M11−ϕDissimilarity(xi,xk)(9)M11+M01=m1(10)M10≤m0(11)M11,M10,M01≥0and they are integerEqs. (8)–(11) are used to produce new combinations with the values of M11, M10, M01 which will be the total number of bits where both Xiand Viare equal to 1 (M11), the total number of bits where Xiis equal to 0 and Viis equal to 1 (M10) and the total number of bits where Xiis equal to 1 and Viis equal to 0 (M01). Eq. (8) tries to minimize the gap between Xiand Vi. Eq. (9) satisfies the maximum number of combinations of M11, M10, M01 values, that is (m1+1)(m0+1).It should be notified that it is possible to get more than one M’vals combinations satisfying Eq. (8), which is not considered by DisABC i.e. DisABC only gets the first M11, M10, M01 combination value to generate solution. However, IDisABC considers all possible M’vals combinations through the following modified selection mechanisms in a probabilistic manner;1.Modified random selection: Firstly, Viis initialized with zeros. If there exists only one optimal value for tuple M11, M10 and M01, M11 number of positions with value one are randomly selected from Xiand these positions in Viare assigned as one. After that, the M10 number of positions with value zero is randomly selected from Xiand these positions in Viare also assigned as one. On the other hand, if the number of M11, M10 and M01 combinations is more than 1, a new solution is generated (referred as MVi's solution vector) for each tuple M11, M10 and M01, as firstly mentioned in random selection. Then, two point crossover operator is applied to the Xi, Xglobaland MVi's, and then swap operator is applied to the generated solutions obtained by crossover. Finally, the best solution is chosen as Vi.Modified greedy selection: Firstly, Viis initialized with zeros. If there exists only one optimal value for tuple M11, M10 and M01, the positions with value one at both Xiand Xglobalare selected and these positions in Viare assigned as one. After the assignment, if the number of changed ones (referred as index0) is less than M11, M11 – index0 number of positions with value one are chosen from Xiand these positions in Viare updated according to the random selection. After that, all positions which are zero at both Xiand Xglobalare also assigned as one in Vi. If the number of changed zeros (referred as index1) is less than M10, M10 – index1 number of positions with value zero are selected from Xiand these positions in Viare updated according to random selection. On the other hand, if the number of M11, M10 and M01 combinations is more than 1, a new solution Viis generated for every tuple M11, M10 and M01 as firstly mentioned in greedy selection. After that, two point crossover operator is applied among Xi, Xglobaland generated solutions of MVi's, and then swap operator is applied to the generated solutions obtained by crossover. Finally, the best solution is chosen as Vi.To show the difference between IDisABC and DisABC in generating new solution Vi, the same example is used as in [36]. Let consider two vectors;Xi=(1011010100)andXk=(1000101101).Assume that ϕ=0.7; M11=2, M10=3 and M01=3 are founded from Xiand Xk.ϕDissimilarity(xi,xk)=0.71−22+3+3=0.525m0 and m1=5 from Xi. Then by Eqs. (8)–(11);minz=1−M11M01+M10+M11−0.525M11+M01=5M10≤5M11,M10,M01≥0and integerIn DisABC, the optimal output is M11=3, M01=2, M10=1 and z=0.025. Viis initialized with 1x10 zeros. M11=3 number of positions are randomly selected from Xiand these positions are set to 1 in Vi. Then, M10=1 number of positions is randomly selected from Xiand it is assigned as 1 in Vi. The final generated solution is Vi=(1001000110).In IDisABC, all the optimal outputs are selected according to z=0.025:•M11=3, M01=2 and M10=1;M11=4, M01=1 and M10=3;M11=5, M01=0 and M10=5;For every group of M values, a new solution is generated as in DisABC (referred as MVi's solutions). Two point crossover is applied among all Xi, Xglobaland MVi's solutions. After that, swap operator is applied to the solutions generated by two-point crossover. Finally, the best solution within all generated solutions is assigned as Vi. Furthermore, the implementation differences between the DisABC and IDisABC algorithms can be also clearly illustrated in pseudo codes, shown in Figs. 3 and 4.Patterns are fundamental structures of an object used by clustering and are distinguished from each other through their attributes known as features. Given a data set Z={z1, z2, …, zp, …, zN}, where zpis a pattern with Nddimensional feature space and N is the number of patterns in Z. When Z is demanded for partitioning into K clusters as C={C1, C2, …, Ck} through a crisp clustering approach, the following rules needs to be considered:•Each cluster should have at least one pattern assigned,Ci≠∅∀i=1, 2, …, KEach pattern should be assigned to a cluster,⋃k=1KCk=ZEach pattern should be assigned to only one cluster,Ci∩Cj=ϕ wherei≠jIn addition to these rules, measuring similarity between the patterns should be also handled in clustering, where distance measure, especially Euclidean distance given by Eq. (12), is most widely used to evaluate the similarity of patterns [15]:(12)d(zv,zy)=∑j=1Nd(zv,j−zy,j)2=∥zv−zy∥2Moreover, how to evaluate the performances of clustering algorithms and how to determine optimal number of clusters are the other significant subjects in clustering. To measure the quality of clustering relative to others created by other methods or by the same methods using different parameter values and to determine the number of clusters, clustering validity indexes are used. It can be clearly stated that various validity indexes in the literature, including the Dunn's index (DI) [50], Calinski-Harabasz index [51], DB index [52], CS measure [53], VI index [3], etc. have been proposed and all of them, despite some differences, should satisfy the following properties [6,54]:Compactness: Within the cluster, patterns should close (similar) to each other as much as possible.Separation: Clusters should be well separated from each other in such a way that cluster centroids are distant from each other as much as possible.In this study, the VI index is chosen as an objective function on account of its satisfactory performance [3,6]. The components of the VI index are as follows:(1)Intra-cluster distance: The average of all distances between patterns and their related centroids, intra-cluster distance is used to measure compactness. In order to satisfy well compact clusters, intra-cluster distance should be minimized, defined by Eq. (13):(13)intra=1Np∑k=1K∑z∈Ck∥zp−mk∥2where Npis the number of patterns in a dataset, zpis a pattern in cluster Ck, mkis the kth cluster centroid and K is the number of clusters.Inter-cluster distance: Known as minimum Euclidean distance between any pairs of cluster centroids, inter-cluster distance is used to measure the separateness of clusters. It should be maximized to obtain well separated clusters, defined by Eq. (14):(14)inter=min{∥mk−mkk∥2}wherek≠kkWith the knowledge of the intra and inter cluster distances, the VI index can be defined by Eq. (15):(15)VI=(c×N(µ,σ)+1)×intrainterwhere c is a user specified constant (mostly chosen as 20, 25 or 30) and N(μ,σ) is a Gaussian distribution with mean μ and standard deviation σ, defined by Eq. (16):(16)N(µ,σ)=12Πσ2e[(K−µ)2/2σ2]where K is the number of clusters. Turi [3] tried to solve problems including more than 4 clusters; hence, μ was limited to 2. In addition to the higher number of clusters, this paper also concentrates on problems regarding fewer clusters (less than 4); therefore, μ is taken 0 or 1 for fewer clusters and taken 2 or 3 for higher clusters, and σ is assigned as 1 for all cases.It should be noted that when assigning patterns to the clusters, a cluster may become empty or there exist only one cluster. To sort out this problem, new solution is generated until the obtained number of clusters is more than one and there exist no empty cluster. The implementation steps of an evolutionary based algorithm to the dynamic clustering are given as follows:1.Initialize randomly Nccluster centroids;Generate the initial solutions; Xi: i=1…SN by Eq. (17);(17)xij=0ifU(0,1)≤0.51ifU(0,1)>0.5Set cycle to 1For each solution Xia.Activate the centroids regarding to the dimensions the value of which are 1;Calculate the Euclidean distances of each pattern from the activated centroids by Eq. (12);Assign each pattern to its closest cluster;Calculate f(Xi)=VI by Eq. (15);Update solutions by employed evolutionary computation based algorithm;Repeat (3) to (5) until cycle=MCNApply K means to the remained centroidsGenerate a new cluster set as in step 1Merge new cluster set with remained centroidsRepeat steps (2)–(9) until termination criterion (TC) is satisfied.

@&#CONCLUSIONS@&#
