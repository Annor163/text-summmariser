@&#MAIN-TITLE@&#
Exploiting diversity for optimizing margin distribution in ensemble learning

@&#HIGHLIGHTS@&#
Double Rotation is proposed to produce diversity among base classifiers.We construct a margin related loss function to learn the weights of base classifiers.Margin Based Pruning is proposed to improve margin distribution of ensembles.Extensive experiments are conducted to validate the effectiveness of DRMP.We explain the rationality of DRMP from different perspectives.

@&#KEYPHRASES@&#
Ensemble learning,Margin distribution,Diversity,Fusion strategy,Rotation,

@&#ABSTRACT@&#
Margin distribution is acknowledged as an important factor for improving the generalization performance of classifiers. In this paper, we propose a novel ensemble learning algorithm named Double Rotation Margin Forest (DRMF), that aims to improve the margin distribution of the combined system over the training set. We utilise random rotation to produce diverse base classifiers, and optimize the margin distribution to exploit the diversity for producing an optimal ensemble. We demonstrate that diverse base classifiers are beneficial in deriving large-margin ensembles, and that therefore our proposed technique will lead to good generalization performance. We examine our method on an extensive set of benchmark classification tasks. The experimental results confirm that DRMF outperforms other classical ensemble algorithms such as Bagging, AdaBoostM1 and Rotation Forest. The success of DRMF is explained from the viewpoints of margin distribution and diversity.

@&#INTRODUCTION@&#
Ensemble learning has been an active research area in pattern recognition and machine learning domains for more than twenty years [1,29,38,45,59]. Ensemble learning, also referred to as multiple classifier systems, committees of learners, decision forest or consensus theory, is based on the idea of training a set of base classifiers or regressors for a given learning task and combining their outputs through a fusion strategy.A significant amount of works have been focused on designing effective ensemble classifiers [15,20,25,39]. However, an exact explanation of the success of ensemble strategies is still an open problem. Some researchers explored how an ensemble’s effectiveness is related to the large margin principle, which is regarded as an important factor for improving classification [42,51]. In this paper, we propose a novel ensemble learning algorithm named Double Rotation Margin Forest (DRMF), which is designed to improve the margin distribution of ensembles by enhancing the diversity of base classifiers and exploiting this diversity using an optimization technique.In general, there are two well-accepted viewpoints – diversity and margin – to explain the success of ensemble learning. Roughly speaking, the margin of a sample is its distance from the decision boundary and thus reflects the confidence of the classification. The margin distribution is acknowledged as an important factor for improving the generalization performance of classifiers [2,6,11,43,49]. In [43], Shawe-Taylor et al. gave an upper bound of generalization error in terms of the margin, while in [6] a similar bound was derived for neural networks with small weights. The large margin principle has been employed to design classification algorithms in [8,14,21,26,50].The performance of ensemble learning methods, especially boosting, has been attributed to the improvement of the margin distribution of the training set [42,51]. In AdaBoost, each new base classifier is trained by taking into account the performance of the previous base classifiers. Training samples that are misclassified by the current base classifiers play a more important role in training the subsequent one. The success of Adaboost can thus be explained from the margin distribution, where the optimization objective is to minimize a margin distribution based exponential loss function. In [42], an upper bound of the generalization error was derived in terms of the margins of the training samples, and it was shown that the generalization performance was determined by the margin distribution, the number of training samples and the number of base classifiers. The efficacy of AdaBoost thus lies in its ability of effectively improving the margin distribution. In [51], Wang et al. showed that a larger Equilibrium margin (Emargin) and a smaller Emargin error can reduce the generalization error, and demonstrated that AdaBoost can produce a larger Emargin and a smaller Emargin error.It is acknowledged that the diversity among the members of an ensemble is crucial for performance improvement. Intuitively, no improvement can be achieved when a set of identical classifiers are combined. Diversity thus allows different classifiers to offer complementary information for classification, which in turn can lead to better performance [28]. A number of techniques have been proposed to introduce diversity. In general, we can divide these into two categories: classifier perturbation and sample perturbation approaches. Classifier perturbation refers to the adoption of instability of learning algorithms [10,36] such as decision trees and neural networks. Since they are sensitive to initialization, trained predictors may converge to different local minima if started from different initializations, and diversity can thus be generated from trained classifiers. Sample perturbation techniques train classifiers on different sample subsets or feature subsets, and include bagging, boosting, random subspaces and similar approaches [4,7,17,41].Since both diversity and margin are argued to explain the success of ensemble learning, it appears natural to question whether there is a connection between the two. Tang et al. [46] proved that maximizing the diversity among base classifiers is equivalent to optimizing the margin of an ensemble on the training samples if the average classification accuracy is constant and maximal diversity is achievable. Consequently, increasing the diversity among base classifiers is an effective method to improve the margin of ensembles. Our work is motivated by this conclusion, and our aim is to improve the margin distribution of ensembles.In our proposed approach, we enhance the diversity of base classifiers by perturbing the samples using double random rotation. This idea is inspired by the PCA rotation proposed in the Rotation Forest algorithm [39]. In Rotation Forest, a candidate feature set is randomly split into K subsets and Principal Component Analysis (PCA) is conducted on each subset to create diverse training samples. Diversity is thus promoted through the random feature splits for different base classifiers. In our work, the feature sets are also randomly split into K subsets. In order to introduce further diversity between the base classifiers, we apply PCA and Locality Sensitive Discriminant Analysis (LSDA). In particular, we first perform unsupervised rotation with PCA, and then employ supervised large-margin rotation with LSDA. LSDA [12], as a supervised method, is able to derive a projection which maximizes the margin between data points from different classes. Our experimental results show that the applied Double Rotation can consistently enhance the diversity in a set of base classifiers.We further exploit the diversity and improve the margin distribution with an optimal fusion strategy. In principle, there are two kinds of fusion strategies. One approach is to combine all available classifiers, e.g., in simple (plurality) voting (SV) [28] or through linear or non-linear combination rules [5,9,19,48]. The other method is to derive selective ensembles, or pruned ensembles such as LP-Adaboost [23] or genetic algorithm (GA)-based approaches [53], which only select a fraction of the base classifiers for decision making and discard the others. Clearly, the key problem here is how to find an optimal subset of base classifiers [32]. In the GASEN approach [55], neural networks are selected based on evolved weights to constitute the ensemble. In [54], the subset selection problem is formulated as a quadratic integer programming problem, and semi-definite programming is adopted to select the base classifiers. Both GASEN and semi-definite programming are global optimization methods and thus their computational complexity is rather high. Suboptimal ensemble pruning methods were proposed to overcome this drawback, including reduce-error pruning [31], margin distance minimization (MDM) [33], orientation ordering [34], boosting-based ordering [35], and expectation propagation [13]. In practice, users would prefer sparse ensembles since computational resources are often limited [57]. In this paper, we introduce a technique to improve the margin distribution by minimizing the margin induced classification loss. In our pruned ensembles, the weights of base classifiers are trained withL1regularized squared loss [56]. The base classifiers are then sorted according to their weights, and those with large weights are selected in the final ensemble.Our presented work comprises three major contributions. First, since diversity is considered to be an important factor which affects the classification margin, Double Rotation is proposed to enhance the diversity among base classifiers. Second, we present a new pruned fusion method based on the Lasso technique for generating ensembles with optimal margin and sparse weight vectors, where the weights are learned through minimization of the regularized squared loss function. Third, we present an extensive set of experiments to evaluate the effectiveness and explain the rationality of the proposed algorithm. We convincingly show that it can improve the margin distribution to a great extent and lead to powerful ensembles.The remainder of the paper is organized as follows. Related work is introduced in Section 2. Section 3 describes our proposed algorithm, while an analysis in terms of parameter sensitivity and robustness is presented in Section 4. Section 5 presents the experimental results and explores the rationality of DRMF. Finally, Section 6 offers conclusions and future work.

@&#CONCLUSIONS@&#
Ensemble learning is an effective approach to improve the generalization performance of a classification system. In this paper, we have proposed Double Rotation Margin Forest (DRMF) as an effective new ensemble learning algorithm. The idea of DRMF is to improve the generalization performance by improving the margin distribution on the training set. Extensive experimental results on 20 benchmark datasets confirm that DRMF provides a competent ensemble learner, and allows us to draw several conclusions: (1) Double Rotation with PCA and LSDA is able to generate diverse base classifiers; (2) The margin distribution of the ensemble system is improved if a set of diverse base classifiers is exploited by optimizing a regularized loss function, and consequently the classification performance of the ensemble is enhanced; and (3) The DRMF algorithm outperforms classical ensemble learning techniques such as Bagging, AdaBoostM1 and Rotation Forest.Our work is motivated by the idea that diversity among base classifiers can improve the margin distribution of the ensemble. However, no deep discussion on this issue has been reported so far. Further theoretical analysis on the relationship between diversity and margin is thus required. While in this paper, we use Double Rotation to create diversity, some other techniques could also be introduced. A systematic discussion on generating diversity would therefore present an important task. Although DRMF is presented as an approach to create homogenous ensembles (e.g., based only on decision trees as the base classifiers), it is straightforward to use DRMF to learn and prune heterogeneous classifiers for ensemble learning. Exploring the effectiveness of DRMF in such a setting might be an interesting avenue.