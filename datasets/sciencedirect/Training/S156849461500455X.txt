@&#MAIN-TITLE@&#
A sequential learning algorithm for a spiking neural classifier

@&#HIGHLIGHTS@&#
LSNC automatically evolves the architecture.Real valued data is encoded using a 2-D encoding having spike amplitude and time.Sequential learning algorithm developed for SLSNC.Learning algorithm relies on computationally inexpensive operations.

@&#KEYPHRASES@&#
Spiking neural network,Sequential learning,Pattern classification,2-Dimensional coding,

@&#ABSTRACT@&#
This paper presents a biologically inspired, sequential learning spiking neural classifier (SLSNC) for pattern classification problems. It consists of a two layered neural network and a separate decision block which estimates the predicted class label. Inspired by observations in the neuroscience literature, the input layer employs a new neuron model which converts real valued stimuli into spikes with varying amplitudes and firing times. The intermediate layer neurons are modeled as integrate-and-fire spiking neurons. The decision block identifies that intermediate neuron which fires first and returns the class label associated with that neuron as the predicted class label. The sequential learning algorithm for the spiking neural network automatically determines the network structure from the training samples and adapts its synaptic weights by long term potentiation and long term depression. Performance of SLSNC has been evaluated using a number of benchmark classification problems and the results have been compared with other well-known spiking neural network classifiers in the literature as well as with the standard support vector machine (SVM) with a Gaussian kernel and the fast learning Extreme Learning Machine (ELM) classifiers. The results clearly indicate that the described spiking neural network produces similar or better generalization performance with a smaller network.

@&#INTRODUCTION@&#
A network of spiking neurons, termed as the third generation of neural networks, consists of computational units which exchange information by means of discrete events in time called spikes. The temporal aspect of spiking neurons add an extra dimension for information representation and processing [1]. Further, it has been shown that a spiking neural network can emulate any continuous function with an arbitrary level of accuracy [2]. It has also been shown that a neural network with noisy spiking neurons is computationally more powerful than a sigmoidal neural network with equal number of units [3]. These theoretical results have encouraged researchers to examine the utility of spiking neural networks for various applications like stroke prediction [4], emulating the cognitive brain processes for better understanding [5], prediction of response to opiate substitution treatment [6].The occurrence of discrete time events renders the output of a spiking neuron discontinuous and non-differentiable. Hence, it is difficult to extend the existing gradient based learning algorithms from the previous generations of neural networks, to spiking neural networks in a simple manner. This problem was first overcome in SpikeProp [7] by assuming that, around the time of the spike, the postsynaptic potential varies linearly with time, thereby, allowing one to calculate the gradient. Based on this idea, an error back-propagation algorithm was developed for a multi-layer spiking neural network. It should be noted that the SpikeProp learning rule relies on the assumption that each neuron generates one spike during training. If a neuron does not generate a spike during training, then one cannot calculate its error and learning is not possible for that neuron. This is referred to as the ‘silent neuron’ problem and has been discussed in detail in [8]. Several extensions to SpikeProp have been proposed which improve its convergence characteristics by adding a momentum term [9,10] and by using adaptive learning rates [11]. Other variations of SpikeProp have also been proposed which are capable of learning with multiple spikes [12–14].It should be noted here that all the variants of SpikeProp are also susceptible to the ‘silent neuron’ problem. Several other error functions for spiking neural networks exist in literature which are not susceptible to the ‘silent neuron’ problem. Tempotron [15,16], uses the difference between the maximum postsynaptic potential and a constant threshold as an error function. The output of a Tempotron is defined by the occurrence or non-occurrence of a spike, hence, it can only be used for solving binary classification problems. The output of the neurons in ReSuMe [17] is defined by the precise time of the spikes, hence, it can be used for solving multi-class classification problems. It has been further extended to supervised learning in a multilayer spiking neural network in [18]. The learning rule of ReSuMe employs a combination of STDP [19] and anti-STDP for weight estimation. STDP is a local learning rule and therefore, limits the learning capability of ReSuMe, as shown in [20]. An alternate error function based on Victor and Purpura [21] distance to calculate the difference between the actual and the desired output spike train is used to derive the gradient updates in Chronotron [20]. In Spike Pattern Association Neuron (SPAN) [22], the input spike patterns are convolved with a specific kernel function so as to remove the discontinuity associated with spike patterns. Using the convolved signals Widrow-Hoff learning rule is used to estimate the network parameters. A comprehensive review of the existing learning algorithms for spiking neural networks can be found in [23].It should be pointed out that all the above mentioned works employ gradient descent based learning algorithms, which require multiple presentations of the training samples, and the network architecture is fixed a priori. The need for multiple presentations of the training samples renders these algorithms unsuitable for use in domains with large streams of data where it is difficult to store the entire training data a priori. For such problems, the network would require re-training whenever new training sample becomes available. Further, selection of an appropriate network architecture is problem dependent and influences the performance significantly. To overcome these problems, several evolving learning algorithms for spiking neural networks which can learn in a single presentation of the training samples have been proposed in [24–27,5].In [24–26], an evolving spiking neural network (eSNN) and its non-gradient sequential learning algorithm have been developed. The eSNN starts with no neurons in the output layer and evolves the architecture automatically. The neurons in the output layer are added during the training phase based on a similarity measure which evaluates the distance between the current sample and the existing neurons in the network. The learning algorithm employs a rank order learning scheme to update the synaptic weights. In rank order learning scheme, the synaptic weights of newly added spiking neuron are calculated based on the order of the first input spike generated by the input neurons. The subsequent spikes are not used for learning. Recently, an extension to eSNN has been proposed in [27,5], where the subsequent spikes are also used to fine tune the weights calculated using the rank order learning. For both the eSNN and dynamic eSNN, it should be noted that the input data is in the form of spike trains and this is not suitable for pattern classification with population coding which has normally been used for encoding real valued inputs into spike trains. Hence, there is a need to develop an evolving spiking neural classifier to handle real-valued input with a better encoding scheme.In a majority of the real-world pattern classification problems, the input features are real-valued signals and not in the form of spike trains. Encoding real valued data in the form of spikes is a challenging problem as it significantly affects the performance of the algorithm. Population coding [28] has been the most commonly used technique for converting a real valued data (external stimuli) into spike patterns (internal stimuli). The input passes through several receptive fields and each of them produces a spike at a particular time instant. The amplitude of the generated spikes is left arbitrary and is not utilized by the encoding mechanism.In existing neuroscience literature [29], it has been shown that neurons in the mammalian cortex also transmit analog information, i.e. sub-threshold presynaptic input can modulate the amplitude of the action potential generated by postsynaptic neurons. Also, [30] shows that receptors present in a crabs leg transmit information regarding its position and movement to the central nervous system using a graded potential. A graded potential is a signal whose amplitude is proportional to the strength of the stimuli. In [31] it has been shown that small changes in the membrane potential of a presynaptic neuron can lead to a change in the amplitude and duration of the action potential generated by the postsynaptic neuron. Although [30] considers these afferent neurons as non-spiking neurons, the important point to note from [29–31] is that any external input to a biological neuron is also encoded using both the time and amplitude of the spikes. This has inspired us to develop an input neuron model to encode the external real-valued input as a spike pattern with both a varying amplitude and firing time i.e. the external stimuli is encoded in a 2-dimensional space.In this paper, we first propose an input neuron model inspired by observations highlighted in the neuroscience literature. The input neurons are responsible for converting the external real-valued input features (‘external stimuli’) into encoded spike patterns (‘internal stimuli’) with both varying amplitudes and firing times. The input neurons have a fixed number of responders and the output of each responder is defined using both an amplitude and time functions. The amplitude function defines the spike amplitude and the time function defines the firing time. By this mechanism, an input neuron produces a spike pattern with both a varying amplitude and firing time for a given real valued input feature. Using the above input neuron model, we develop a sequential learning spiking neural network for classification problems, named as sequential learning spiking neural classifier (SLSNC). Essentially, SLSNC has two layers, viz an input layer and an intermediate layer of spiking neurons. The intermediate layer neurons employ the well-known ‘integrate-and-fire’ [34,35] spiking neuron model. Similar to [32,33,36], each intermediate neuron in SLSNC is associated with a specific class label. This association between an intermediate neuron and the class label is stored in the decision block. The decision block identifies the intermediate neuron that fires first and returns the class label associated with that neuron as the predicted class label. The training samples are presented one-by-one and only once. SLSNC starts with zero intermediate neurons and evolves its architecture automatically by adding intermediate neurons. Based on the knowledge in the network and the current input pattern, the proposed learning algorithm either adds new intermediate neurons (new connections) or updates the synaptic weights (long term potentiation/depression) of existing intermediate neurons. The resultant SLSNC has intermediate neurons that can have either inhibitory or exhibitory synaptic weights, i.e., they can either suppress or incite the postsynaptic neuron to spike. It may be noted that the weight update rules for long term potentiation and depression rely on elementary linear algebraic manipulations rather than evaluation of complex mathematical operations like gradient. The effect of different SLSNC parameters on classification performance is also highlighted along with some guidelines for selecting these parameters for a particular problem. Finally, we summarize the algorithm in a pseudocode.Performance evaluation of SLSNC has been carried out in two different ways. First, the performance of the SLSNC is compared with that of SpikeProp [7], MuSpiNN [13], Multi-spike learning [14] and Synaptic Weight Association Training (SWAT) [37] using both the Iris flower classification and Wisconsin breast cancer data sets from the UCI machine learning repository [38]. The experiments are conducted in the same setting as given in [14]. From the results, we can clearly see that the proposed SLSNC requires fewer parameters to achieve a better classification accuracy. In the second study, the performance of the SLSNC classifier on different benchmark data sets (binary and multi-category data sets with different imbalance factors) has been evaluated in comparison to the standard support vector machine classifier with a Gaussian kernel [39] and the fast learning Extreme Learning Machine (ELM) classifier [40]. The parameters of SVM, ELM and SLSNC classifiers are determined by cross-validation using the training data. The classifiers are further tested on unseen data. We have conducted 10 random trial experiments and reported both the mean and standard deviation of the overall and geometric mean accuracies for all data sets. The results clearly highlight that SLSNC performs better than or at par with existing algorithms using fewer neurons.The major contributions of this paper are: (1) A neuron model which employs a 2-dimensional encoding mechanism for converting a real valued input into spike patterns, (2) A sequential learning algorithm for a spiking neural network which requires a single presentation of the training samples, (3) A learning algorithm with a capability to automatically determine the network architecture based on the training samples, and (4) The weight update rules which rely on computationally inexpensive linear algebraic manipulations.The rest of the paper is organized as follows: Section 2 describes the SLSNC and its learning algorithm. This section also describes the basic model of an input neuron and an intermediate neuron. Section 3 presents, a study on the impact of different algorithm parameters on the performance of SLSNC and based on this some guidelines are developed for choosing proper values of these parameters for any given problem. Based on well known benchmark problems, Section 4 highlights the performance of SLSNC in comparison to other classifiers in spiking neural network literature, namely, SpikeProp, MuSpiNN, Multi-spike learning and SWAT. Next, the performance of SLSNC is compared with other well known classifiers namely SVM and ELM on benchmark data sets. Finally, Section 5 summarizes the conclusions from this study.In this section, first a brief description of the general pattern classification problem is presented, followed by a detailed description of the SLSNC network architecture along with a description of the models for input neurons and intermediate neurons that make up the first and second layer of SLSNC. Finally, its learning algorithm is presented in detail.The training samples(x1,c1),(x2,c2),⋯,(xr,cr),⋯are presented to the network one-by-one and each sample (xr, cr) is presented only once.xr=xr1,xr2,⋯,xrmTbe an m-dimensional feature vector andcr∈1,2,⋯,Nis the class label. N is the total number of classes. In this paper, it is assumed that each input sample is available to SLSNC for a prefixed duration of time T referred to as the simulation interval [7]. The objective of SLSNC is to approximate the underlying decision function that maps an input xrto the associated class crwithin the time interval T. Note that the proposed SLSNC adapts its architecture to approximate this decision function closely. Similar to [41], SLSNC initially has no connections between the input neurons and intermediate neurons. These connections are established during the learning phase in a sequential manner. The synaptic efficacy (weight) of the connections between the responders of the input neurons and intermediate neurons can be either excitatory or inhibitory. The synaptic efficacies of the network, thus, control the firing abilities of the intermediate neurons. The terms synaptic efficacy and weight are used interchangeably throughout the paper.Fig. 1shows the architecture of SLSNC. SLSNC is a two layer fully connected feed-forward network. The input layer consists of m input neurons which convert the real-valued inputs to spike patterns and the output layer consist of spiking neurons. SLSNC has a separate decision block which monitors the output of the intermediate neurons to determine the predicted class for the presented sample. Intermediate neurons are modeled as ‘integrate-and-fire’ neurons and can generate multiple spikes. A spike is generated when the membrane potential of an intermediate neuron crosses its threshold potential. In contrast to the traditional spiking neural networks, input neurons in SLSNC convert the real-valued inputs into spike patterns with both varying amplitudes and firing times. The intermediate neurons process the input spike patterns from the input neurons. Each intermediate neuron is associated with a particular class and this association is stored in the decision block. The decision block identifies the intermediate neuron that fires first and returns its associated class label as the predicted class label. It should be noted here that although, an intermediate neuron can generate multiple spikes, the decision block uses only the first spike to determine the predicted class.In Fig. 1, each input neuron consists of P responders and these responders may produce at most one spike with a varying amplitude and firing time based on the value of the corresponding input feature. The network is fully-connected and there are P connections between any pair of input neuron and intermediate neuron. The synaptic efficacy of a connection between the hth responder of ith input neuron and the jth intermediate neuron is denoted bywijh.SLSNC starts with no connections between the input neurons and the intermediate neurons. The connections between the input neurons and intermediate neurons are established during the training phase. Without loss of generality, assume that the SLSNC has evolved to K intermediate neurons in the second layer from r−1 training samples.In spiking neural network literature, population coding is often used to encode the external real input into spikes with varying firing times. In neuroscience literature, it has been shown that variations in presynaptic potential modulates the amplitude of action potential generated by the postsynaptic neurons [29–31]. Based on this observation, in this paper, we propose an input neuron model which converts the real valued input (‘external stimuli’) to spikes with both varying firing times and amplitudes (‘internal stimuli’).The structure of the proposed input neuron model is shown in Fig. 2. Each input neuron has P responders and each responder produces atmost a single spike with some firing time and amplitude. The multiple responders used by the learning algorithm increase the resolution of the problem features. For a problem in which the samples are not easily separable i.e., when there is a considerable overlap between the clusters belonging to the different classes, the number of responders needed would be high. On the other hand, few responders would be good enough for a problem which is easily separable. Therefore the value of P for a given problem is dependent on the nature of the problem.The outputuih(t,xri)of the hth responder of the ith input neuron is given by:(1)uih(t,xri)=fih(xri)δih(t−τih),h=1,2,⋯,Pwherefih(.)is the spike amplitude function andδih(.)is the spike firing time function. In Fig. 2, for the sake of brevityuihis used instead ofuih(t,xri). The spike firing time function is a Dirac delta functionδ(.)whose output is zero everywhere except at the origin. The procedure for computing the firing time (τih) and the spike amplitude functionfih(.)is described below.Each responder uses a Gaussian receptive field whose output (ϕih) is given by:(2)ϕih=exp−(xri−μih)22σih2,i=1,2,⋯,mh=1,2,⋯,Pwhereμihandσihare the center and width of the hth responder of the ith input neuron. These two parameters control the response of the responders and are determined based on the range of the input feature. For the ith input neuron with P responders (P>2) whose input feature value varies fromIminitoImaxithe center and width of the hth responder is given by:(3)μih=Imini+(2h−3)2(Imaxi−Imini)P−2(4)σih=1γ(Imaxi−Imini)P−2where, γ controls the percentage of overlap between the responders by varying their widths. A value of γ lower than 2 would result in more than 50% overlap between the two adjacent responders. As a result, multiple responders would provide similar outputs for a given input. This reduces the ability of the network to differentiate between samples. On the other hand, a high value of γ would allow fewer responders to have considerable activation, which defeats the purpose of increasing resolution by using multiple responders. Based on the above facts, in this paper, the value of γ is set to 3 which leads to almost 30% overlap.Using Eq. (2), the firing timeτihis defined as:(5)τih=T(1−ϕih),i=1,2,⋯,m;h=1,2,⋯,Pwhere T is the simulation interval in milliseconds. In the experiments presented in this paper, a value of 10ms is used for T.Existing works in this area use only temporal coding, in which case the firing time of the spike is controlled and the amplitude of the spike is not varied. In contrast, this paper defines an input neuron model which performs an encoding such that both the firing time and amplitude of the output spikes generated are meaningful. The amplitude of the spikes(fih(.))generated by an input neuron is defined by:(6)fih(xri)=(λ)rih1+|xri−μih|,i=1,2,⋯,mh=1,2,⋯,Pwhererihis the rank of the hth responder of the ith input neuron, λ represents the slope of f(.), and |·| is the absolute value function. If the value of λ is kept closer to one, then there will be little difference between the amplitude of spikes generated by responders which differ by a small amount in terms of their firing time. This will again reduce the ability of the network to differentiate between samples. On the other hand a low value will cause the amplitude of spikes to go down very fast which will negate the effect of having multiple responders to increase the resolution. Based on this, we choose the value of λ as 0.8. Rank of a given responder is determined by ordering all responders from a given input neuron in terms of their receptive field outputs. Using this definition of rank, the ranking function is defined as follows:(7)FR(x,y)=0ϕix≥ϕiy1otherwisewhere x and y are the indices of any two responders of the same input neuron. The rank of the hth responder of the ith input neuron is given as follows:(8)rih=1+∑y=1,y≠hPFR(h,y)Note that the spike amplitude function in Eq. (6) and the spike firing time functionδihdefine the encoding characteristics of the input neuron.Ideally the parameters of the input neuron should be different for different features such that the encoded patterns generated for different features take into account the corresponding feature values. For the sake of simplicity, it is assumed here that all the input neurons have the same encoding characteristics. Next we will describe the impact of P, γ and λ on the amplitude and firing time of the spikes.In this section, how the encoding characteristics of a responder change with its parameters μ and σ is shown. Next, the variation of the encoding characteristics with respect to the number of responders P in an input neuron is presented.For illustration purposes, the following parameter settings for the input neuron model have been used: range of the input feature is [0, 6]; number of responders is 6 (P=6); overlap factor (γ) is 3 and λ is set at 0.8. Unless otherwise stated, these parameter values have been used in all the simulations.Fig. 3(a) shows the firing time of the 3rd responder as its input varies from [0, 6]. Fig. 3(b) shows the variation of the spike amplitude of the 3rd responder for the same input variation. From Fig. 3(a), it can be observed that the firing time is close to zero when the input feature value is closer to the center of the responder and is maximum (10ms) when the input is far away from the center of the responder. Note that, since time is discreet, after a certain value of ϕ the firing time will always be at maximum. Therefore, for input values far away from the center, the spike amplitude modulates the contribution of a spike to postsynaptic potential. The firing time function has an inverted Gaussian behavior with respect to the input feature. From Fig. 3b, it can be seen that the spike amplitude function has an exponential decay characteristic with discontinuities. In a given input neuron, the number of discontinuities depends on the value of λ and the number of responders. This discontinuity helps the input neuron to differentiate the output of the responders at different ranges and produces a better encoding of the input feature. The number of responders P and the overlap factor γ influence the spike amplitude and firing time function significantly where as the slope λ, controls the rate of decay of the spike amplitude as one moves away from the center of a given responder.Fig. 4shows the spike amplitude variation for the 2nd and 4th responders. It can be observed from the figure that the spike amplitude variation for any two responders is not identical despite using the same value of σ. Furthermore, the variation in the spike amplitude is not symmetrical around the center of the responder. To understand the reason for this difference, observe the maximum distance between a sample in the input range [0, 6] and the center of the respective responders. The center of the 2nd and 4th responders are 0.75 and 3.75 respectively. Therefore, for a given sample in the input range the maximum distance from the center of the 2nd and 4th responders would be 5.25 and 2.25 respectively. Consequently, the ranks would change in a different manner for the two responders as the distance between a sample and the center of the responder varies. Since the spike amplitude depends directly on the rank, the variation in spike amplitude will not be identical for different responders.Fig. 5shows the graphical illustration of the response of 4th responder in an input neuron in a 3D plot. The figure clearly shows the combined effects of both the spike amplitude function and the firing time function. If an input feature is closer to the center of the responder, then the firing time will be closer to zero and the spike amplitude will be closer to one. Similarly, if an input feature is far away from the center of the responder, then the firing time is closer to the simulation time T and the spike amplitude will be very small. The spikes produced by the responders with smaller amplitudes closer to the simulation time T will have little influence on the post-synaptic potential of the intermediate neuron and this improves the information processing capabilities of the intermediate neuron.The λ parameter directly influences the decay characteristics of the spike amplitude function. Fig. 6illustrates the spike amplitude function with respect to different values of λ (0.8, 0.9, 0.99). From the figure, it can be seen that the spike amplitude is smoother for λ value close to 1. Also, one can observe that the inputs that are far away from the corresponding responder center produce a higher spike amplitude for λ value closer to 1.As explained, the encoding characteristics of an input neuron for a given input are mainly controlled by the number of responders (P) and the overlap factor (γ). To study the effect of P and γ, the following experiments were conducted. Fig. 7shows the variation of the spike amplitude function and firing time of the 3rd responder in ith input neuron for different number of responders, viz. P=6, P=8, and P=10. From Fig. 7(b), it can be observed that as the number of responders increases, the rate at which the firing time of a responder decreases, starts climbing. This is due to the fact that the width of a responder is inversely proportional to the number of responders. The decrement in width as the number of responders increase, also results in a faster decrease of spike amplitude, on moving away from the center of the responder.Next, the effect of the overlap factor (γ) on the firing time of the responders is given in Fig. 8(a and b). γ affects the width of all the responders by the same amount and hence influences only the firing time function. Fig. 8(a) shows the variation of the firing time with respect to the input for the 3rd and 4th responders with γ=3. Fig. 8(b) shows the same for γ=1.5. From both figures, one can observe that the overlap area (shaded region) decreases with increasing γ. This controls the range of input for which both the responders fire, although, at different instants. This is useful for effective temporal coding.In summary, the number of responders (P) controls both the amplitude and time encoding characteristics of an input neuron, where as γ controls only the firing time and does not have any influence on the spike amplitude. The slope (λ) influences only the rate of decrease in the spike amplitude with respect to the input. One can achieve the desired encoding characteristics of any input neuron by properly selecting these parameters.Intermediate neurons process the spike patterns generated by the input neurons. Fig. 9shows the intermediate neuronGjcmodeled as an integrate-and-fire neuron. The superscript c indicates the class associated with that intermediate neuron. As shown in the figure, the output of an intermediate neuron can have more than one spike, but the decision block uses only the first spike to determine the predicted class. A single intermediate neuron has mP connections with input neurons and the synaptic efficacies of these links are estimated using different strategies of the sequential learning algorithm. These strategies lead to either long term potentiation or depression of the synaptic efficacies. The black and red dots in the figure represent the excitatory and inhibitory synapses respectively. An inhibitory synaptic efficacy can lead to a decrease in the membrane potential of an intermediate neuron. In the inset of Fig. 9, the part of post-synaptic potential chart marked by a dashed circle shows a sudden dip in the potential of the neuron. Since the neuron model used is a non-leaky integrate-and-fire neuron, such a dip can only be attributed to inhibitory efficacies.The potential of an intermediate neuron at any time is calculated as follows:(9)Vjt1=∑t=0t1∑h=1,i=1m,Puiht,xriwijh∈t1∈(t1) is the spike response function [7]. A spike will be generated whenever the potential of the intermediate neuronGjccrosses the threshold value θj. The time of the first spike is given by:(10)Γj=argmint(Vj(t)≥θj)0≤t≤T∞otherwiseIn Fig. 1, the output of the intermediate neurons are connected to a decision block, which identifies the predicted class labelcˆ. The decision block stores the class associated with each intermediate neuron. The decision block identifies the intermediate neuron which fired first (referred to as the winner intermediate neuron) and returns the class label associated with that intermediate neuron as the predicted class label. The winner neuron is identified as follows:(11)U=argminjΓjThe firing time of the winner intermediate neuron is indicative of the confidence of the network in the predicted class label. If the current sample is similar to a sample already seen by the network, then the firing time of the winner intermediate neuron will be closer to the beginning of the simulation interval. On the other hand, if a sample is very different from a sample already seen by the network, then the firing time of the intermediate neuron will be closer to the end of simulation interval. Therefore, the posterior probability of the current input belonging to class c can be estimated using the confidence of the network in the prediction as follows:(12)p(xr,c)=1.0−ΓUTIn the SLSNC learning algorithm, the network architecture starts with zero intermediate neurons (k=0) and as the training samples arrive (xr, cr), the network evolves based on the information contained in the current sample and also the knowledge stored in the network, in a sequential manner. Unlike existing gradient based learning algorithm in the spiking neural network literature, the proposed learning algorithm employs a ‘learning on-the-go’ approach as such it requires each sample to be presented only once. The learning algorithm of SLSNC can either choose to add a new intermediate neuron or update the synaptic weights of existing intermediate neurons. As each new sample (xr, cr) is presented to the network, the learning algorithm selects one of the following strategies:•Intermediate neuron addition strategy: create a new intermediate neuron and establish the connections between the input neurons and the new intermediate neuron to store the new/unknown knowledge present in the current sample.Conflict resolution strategy: when there is a misclassification, then the nearest intermediate neuron from the same class undergoes a long term potentiation whereas the nearest intermediate neuron from the other class undergoes a long term depression.Synaptic weight update strategy: when the knowledge contained in a new sample is similar to existing knowledge in the network, then the synaptic weights of the winner neuron from the same class undergo long term potentiation.Similar to other sequential learning algorithms, the training stops when there are no more training samples available to learn.First training sample forms the first intermediate neuron and the synaptic weights between the first intermediate neuron (K=1) and the input neurons is initialized as:(13)w1=fwheref=[f11,⋯,f1h,⋯,f1P,⋯,fmP]′andwK=[w111,⋯,w11h,⋯,w11P,⋯,wm1P]′.The threshold (θ1) of that intermediate neuron is calculated as:(14)θ1=αw1Tw1where α is the threshold fraction that controls the firing time of the intermediate neuron. α controls how easy it is for an intermediate neuron to spike when a similar sample is presented to the network. A high value of α will make the response of an intermediate neuron very localized and vice-versa. Therefore, α is initialized in an intermediate range of [0.5, 0.7].Next, the above mentioned three strategies are explained below when the rth sample is presented to the network.The following criteria must be satisfied to add the current sample (xr, cr) as a new intermediate neuron.(15)cˆ={∅}OR(c≠cˆAND∥f−wnrS∥>βa)where nrS is the nearest intermediate neuron from the class crand βais the addition distance threshold. A very small value for βaleads to an increase in the number of intermediate neurons connected to the input neurons resulting in a low generalization performance. A very high value for βarestricts the number of intermediate neurons connected to the input neurons which results in an inaccurate model. Note, ∅ refers to the situation where none of the intermediate neurons fire for the current sample.The nearest intermediate neuron (nrS) is determined by the Euclidean distance between the spike amplitude response for the current sample and the synaptic weights of all the intermediate neurons from the same class.(16)nrS=argminc==cr;∀j∥f−wj∥If the intermediate neuron addition criteria given in Eq. (15) is satisfied, then a new intermediate neuron (K+1) is added to the network and its synaptic weight (wK+1) and the threshold (θK+1) are initialized as follows:(17)wK+1=f(18)θK+1=αwK+1TwK+1Note that the synaptic weights of a new intermediate neuron are initialized to the spike amplitude of the input neuron and hence, its synaptic weights are always positive.The newly added (K+1)th intermediate neuron is associated with the class label crand this association is stored in the decision block. This associated class label is used to identify the class label for future samples, when the (K+1)th intermediate neuron fires first. Also, the self-adaptive potentiation factor (ηK+1) is set to 0.5. This self-adaptive potentiation factor decays from 0.5 to 0. This factor controls the influence of similar samples in the future.When a class label associated with the intermediate neuron which fires first (nrI) is different from the actual class label and there exists another intermediate neuron from the same class (nrS) which fires subsequently, then one should suppress the synaptic weight of the intermediate neuron which fired first and increase the synaptic weight of the intermediate neuron from the same class. Such a conflict can occur when the weight vector (w) of two intermediate neurons associated with different classes are close to each other. For this strategy, the criterion is given by:(19)c≠cˆAND|f−wnrS|<βaTo resolve the conflict, both synaptic potentiation as well as synaptic depression for these neurons are used. First, the intermediate neuron (nrS) from the same class is subjected to long term potentiation such that this intermediate neuron fires first for the current sample and the update rule is given below:(20)wnrS=(1−ηnrS)wnrS+ηnrSf(21)θnrS=(1−ηnrS)θnrS+ηnrSαfTfwhere ηnrSis self-adaptive potentiation factor. In the beginning, the potentiation factor of the intermediate neuron gives higher importance to similar samples and at later stages decreases potentiation for similar samples. The factor ηnrSfor the intermediate neuron is adapted as:(22)ηnrS=ηnrS1+ηnrSAlgorithm 1Pseudocode for SLSNC learning algorithmRequire:Input samples with class label(x1,c1),⋯,(xr,cr),⋯,(xL,cL)▷ Network has m input neurons indexed by i▷ Each input neuron has P responders indexed by hforq←1 to Ldofih←Amplitude of output spikes generated by input neuron,   ∀i, hδih←The firing time of the output spikes generated by input neuron,   ∀i, hcˆ←Predicted class of the samplenrS← closest same class intermediate neuronnrI← closest different class intermediate neuronifcˆ={∅}OR(c≠cˆAND|f−wnrS|>βa)then▷ Intermediate neuron addition strategywK+1=f,   ∀i, hθK+1←αwTwelse ifc≠cˆAND |f−wnrS|<βathen▷ Conflict resolution strategywnrS=(1−ηnrS)wnrS+ηnrSf,∀i, hθnrS←(1−ηnrS)θnrS+ηnrSαfTfwnrI=(1+κ)wnrI−κfηnrS←ηnrS1+ηnrS▷ Update ηnrSelseifcqˆ=cqthen   ▷ Synaptic weight update strategywnrS=(1−ηnrS)wnrS+ηnrSf,∀i, hθnrS←(1−ηnrS)θnrS+ηnrSαfTfηnrS←ηnrS1+ηnrS▷ Update ηnrSend ifend ifend forNext, the intermediate neuron from the other class is subjected to long term depression such that this intermediate neuron does not fire for similar samples and the update rule is given by:(23)wnrI=(1+κ)wnrI−κfwhere κ is the depression factor which controls the extent to nrI undergoes long term depression and is usually set close to zero. A higher value of κ will result in significant shift in synaptic weight resulting in loss of information stored in the network. Therefore, κ is always initialized in the range [0.01, 0.2].If the actual class label (c) and the class label associated with the intermediate neuron (cˆ) which fires first are the same, then the synaptic weight and threshold of winner intermediate neuron are updated as follows:(24)wnrS=(1−ηnrS)wnrS+ηnrSf(25)θnrS=(1−ηnrS)θnrS+ηnrSαfTfwhere nrS is the intermediate neuron that fires first. Also ηnrSis updated according to Eq. (22).To summarize, the pseudo code for SLSNC learning algorithm is given in Algorithm 1.In this section, two different experiments are conducted. In the first experiment, the impact of noise on the performance of SLSNC is discussed. In the second experiment, the impact of SLSNC architecture (P) and learning algorithm parameters (α, βaand κ) on the classification performance is highlighted. For both the experiments, we consider a benchmark problem of image segmentation (IS) from UCI machine learning repository [38]. Based on the results from second experiment, some guidelines are provided for selecting appropriate values for SLSNC architecture and learning algorithm parameters.The problem of Image Segmentation involves partitioning an image into several constituent components and assigning a specific label to each component. It is an important step in automated image recognition system. In image segmentation problem, each image pixel is classified into any one of the 7 pre-defined classes. For each pixel, 18 different features are extracted as an input vector. Thus, the total number of available samples is 2310. For training, we have used 30 samples from each class and remaining 2100 samples are used for evaluating the performance. Note that the training and testing data are well-balanced. For this problem, the input layer of the SLSNC has 18 input neurons and each input neuron contains 11 (P=11) responders. The simulation interval is set to 10ms for all samples. The learning algorithm parameters are selected as: βa=0.867; α=0.662; and κ=0.025. The training samples are presented one-by-one and only once. The intermediate neuron growth history for the training samples is given in Fig. 10. From the figure, it is noted that SLSNC requires 37 intermediate neurons to approximate the functional relationship between the input features and the class labels. The overall training and testing accuracies are 95.23% and 91.76% respectively. As reported in [42], SVM with Gaussian kernel requires 96 support vectors to achieve 90.62% overall accuracy. From the results, it can be inferred that SLSNC achieves similar performance with a smaller number of intermediate neurons. Next, the ability of SLSNC in handling noisy data is investigated.To examine the capacity of SLSNC classifier in handling noisy data, it is trained with noiseless training data and tested with noisy data. Additive white Gaussian noise with varying levels is added to the testing samples and SLSNC overall classification performance is reported in Fig. 11. From the figure, it can be observed that as the SNR decreases, the deterioration in performance is not very significant. Furthermore, for high noise content (SNR of 5dB), the performance is close to 80%.Next, we study the influence of the number of responders and the learning algorithm parameters on the performance of SLSNC. In order to study their influences, we have conducted a cross-validation study by varying these parameter over specific ranges. The results are shown in Fig. 12.•The factor κ allows SLSNC to handle the overlap between clusters from different classes by inducing a long term depression in the synapses of the neuron that fired incorrectly due to this overlap. κ controls the extent of inhibition that should be applied to a neuron which misfired. If the value of κ is set too high then the network loses information recorded from the already seen samples. And if the value of κ is set too low, then it plays no role in learning. Fig. 12(a) shows the overall training/validation performance (ratio of no. of correctly classified samples to total no. of samples in percentage) when κ is varied over the interval [0.01, 0.2]. From the figure, it can be seen that validation performance varies by about 1-2% whereas change in training performance is very small. Using these observations as the basis, a suitable guideline for setting κ is to choose a value in the range [0.01,0.1].The parameter α controls the firing time of the intermediate neuron as it impacts how easy it is for an intermediate neuron to spike when a similar sample is presented to the network. A high value of α will make the response of an intermediate neuron very localized. Fig. 12(b) shows the overall training/validation performance versus α. From the figure, it can be observed that the validation performance increases slightly upto α=0.66 and then it drops. Note that the training performance varies by about 1–2%. Based on this observation, the suggested guideline is to choose a value for α in the range [0.5−0.7].Two different factors control the value of βa. First, if the clusters of samples belonging to different classes are closer to each other, than a low value of βashould be chosen and vice-versa. Second, a problem that has a large number of features or uses a large number of responders is likely to have a high value of βa. Hence, the value of βais dependent on the problem. Fig. 12(c) shows the overall training/validation performance versus βa. From the figure it can be observed that validation performance increase and training performance almost remains same when βaincrease from 0.5 to 0.87. Beyond this value, both training and validation performances drops significantly. Based on these observations, suggested guideline to choose a value for βais in the range 5–20% of the maximum possible distance between any two samples for a given problem. The maximum distance for a given problem is the distance of the vector1→from the origin. The dimension of the vector1→is equal to mP in the problem.Fig. 12(d) shows the overall training/validation performance for different number of responders. Here, the number of responders is varied from 7 to 14. From the figure, it can be seen that both the training and validation performances increase with increase in number of responders (7–11) and the performance decreases beyond 11. One need to study the effect of P for a given problem and then select the appropriate values.In this section, first the performance of SLSNC is compared with that of other learning algorithms for spiking neural networks, namely, SpikeProp [7], MuSpiNN [13], multi-spike learning [14] and SWAT [37]. The comparison has been made for the problems of Fisher Iris plant classification and Wisconsin breast cancer. Next, the performance of SLSNC is evaluated against support vector machine [39] and Extreme Learning Machine [43] using benchmark binary/multi-category classification problems with varying class imbalances.Among the various spiking neural network architectures in the literature, SpikeProp [7] and SWAT [37] are well-known batch learning algorithms applied to real-valued benchmark classification problems. MuSpiNN [13] and multi-spike learning [14] are two important variants of SpikeProp. SpikeProp and its variants employ the well known population coding and each input feature is encoded with P spikes with spike times between 0 to 10ms. Earlier a performance study had been conducted using these algorithms on Fisher iris plant classification (Iris) and Wisconsin breast cancer (BC) problems. Hence, in this section, we conduct a performance evaluation of the proposed SLSNC on the same Iris and BC problems and compare the results with those reported in the literature for these problems by other algorithms. One should note that SpikeProp, MuSpiNN and multi-spike employ gradient descent learning with a fixed network architecture, whereas the proposed SLSNC employs an evolving architecture with a non-gradient sequential learning algorithm. Here, the performance and compactness of the network architectures is compared using the benchmark data sets (Iris and BC) from the UCI machine learning repository. One should note that for both the problems we have used the same experimental setup as employed by SpikeProp and its variants. The number of training and testing samples used by SLSNC is the same as used by these algorithms. Also results for SLSNC have been generated using 5 responders which is the same number of receptive fields used by MuSpiNN and multi-spike learning.1.The Fisher Iris classification dataset is a well-balanced data set with 50 samples from three classes and each sample has four input features. The results for MuSpiNN and Multi-spike learning are reproduced from [14] whereas for SpikeProp the results are reproduced from [8]. The results for SWAT are reproduced from [37]. Note that SpikeProp, MuSpiNN and multi-spike learning employ an additional bias neuron. SpikeProp used four receptive fields to encode the real-valued input features whereas MuSpiNN and multi-spike learning used five receptive fields. SWAT encodes each input feature using four neurons. The network architecture is represented as ni:nh:ny(niis the number of input neurons, nhis the number of hidden neurons, nyis the number of output neurons). SpikeProp and SWAT use three output neurons to handle three classes; MuSpiNN uses three networks (one for each class) with a single output neuron and multi-spike learning uses one output neuron with multiple spikes in the output layer. In [14], the results are reported for a single spike encoding and a Gradual Multi-spike Encoding Strategy (GMES) and the results for both the strategies are reproduced here for a better comparison. For SLSNC, no bias was employed and each input feature is encoded by the input neuron into spikes consisting of both amplitude and firing times. The intermediate neurons are associated with a particular class and the class association of the intermediate neuron, which fires first gives the predicted class label. The value for the parameters α, βaand κ are 0.52, 0.51 and 0.1 respectively. The performance of SLSNC was evaluated by averaging over ten random trials, wherein each trial consisted of ten randomly chosen samples per class for training and the remaining samples are used for testing. The network architecture and the training/testing performances are presented in Table 1. The table also shows the total number of parameters to be estimated for each algorithm. The numbers in bracket in the table are the standard deviation for the corresponding result. From the table, it can be observed that the proposed SLSNC achieves a compact structure with approximately 2−5% higher generalization performance than other batch learning algorithms. For this problem the performance of a multilayer perceptron with 10 hidden layer neurons trained using backpropagation was 95.5% (reproduced from [7]). SLSNC performs about 2% better than multilayer perceptron using 5 neurons which supplements Maass's statement in [3] that a network of spiking neurons is more powerful than a network of sigmoidal neurons.Wisconsin breast cancer dataset is a binary classification problem which consists of 699 samples. Each sample consists of 9 attributes and has to be classified as a benign or malignant case of breast cancer. The results for SpikeProp and its variants, and SLSNC were generated using 10 randomly selected samples from each class for training and the remaining 679 samples are used for evaluating the generalization performance. The value for the parameters α, βaand κ are set to 0.62, 0.91 and 0.1 respectively. SWAT reported results based on fivefold cross-validation using 135 samples in each set. The network architecture and the training/testing performances are given in Table 2. From the table, it can be observed that the proposed SLSNC requires only four intermediate neurons to approximate the decision function with higher generalization performance. The performance of SLSNC is about 2% better than that of multi-spike learning. The results for SLSNC and SWAT are almost the same, but one should note that SLSNC has considerably less network parameters when compared with each of the algorithms concerned. The performance of multilayer perceptron with 15 hidden layer neurons was 96.3 (reproduced from [7]). The performance of SLSNC was same but it used only four neurons. This observation reinforces the observation made by Maass in [3]. The high standard deviation in SLSNC training performance is due to using only 20 samples for training. Hence, the overall training performance drops by 5% for every sample misclassified.The Iris and Wisconsin breast cancer are the two problems on which different algorithms in spiking neural network literature have been evaluated. For evaluating the performance of SLSNC, we have used the same setting as used by SpikeProp and its variants. From the results, it can be observed that the 2-dimensional encoding by the input neuron, and the non-gradient sequential learning algorithm, which autonomously determines the network architecture, enables SLSNC to achieve a better performance. Another important thing to note is that the number of parameters used by SLSNC are far lower than those used by the other algorithms. In the next subsection the performance of SLSNC is compared with that of SVM and ELM.It has been shown in [44] that a spiking neural network has higher computational power than a sigmoidal neural network with the same number of neurons and it can approximate any continuous function to the desired accuracy. This motivated us to compare the performance of the proposed SLSNC with the benchmark support vector machine [39] with Gaussian kernels and the fast learning Extreme Learning Machine [43] (ELM) classifiers. Note that the cost parameter (C) and Gaussian kernel width (γ) influence the SVM classifier and have been optimized using a cross-validation strategy. ELM is a single hidden layer network with randomly selected input layer parameters and the output layer parameters are determined using a least squares solution. It has been shown that ELM is better than a multilayer perceptron using gradient descent algorithm [43,40]. Also, ELM with a Gaussian activation function performs better than a sigmoidal function [43]. Hence, ELM with a Gaussian kernel is used in this paper. Selection of the number of hidden neurons and input weights influence the performance of the ELM classifier [45] and hence they have been optimized using a cross-validation strategy.For the purpose of performance comparison, twelve classification problems (7 binary and 5 multi-category) were considered. The problems have been chosen such that they vary widely in terms of number of samples, number of features and class-wise balance in the dataset. Except the acoustic emission problem [46], all the problems are taken from the UCI machine learning repository [38]. The number of classes, number of features and training/testing samples for different problems are given in Table 3. Since the data sets considered in this study have varying class imbalances, both the overall classification accuracy (ηo) and the geometric mean classification accuracy (ηg) are used as a performance measures. These performance measures are defined below:Let Q be the confusion matrix representing the statistical measures for both class-level and global performances of the classifier. The average performance of individual classes is determined using geometric mean accuracy (ηg) and is given by:(26)ηg=∏j=1NqjjNj×100Nwhere Njis the total number of samples in the jth class and qjjis the total number of correctly classified samples.The global performance of the classifier is determined using overall accuracy (ηo) and is given by:(27)ηo=100NT∑i=1NqiiwhereNT=∑i=1NNiand N is the total number of classes.The results were obtained by averaging the performance results over 10 random trials for each dataset. The results were generated in MATLAB 2013b on a quad-core system with 16 GB RAM in Windows environment. For SLSNC, the value of P was chosen using constructive-destructive procedure [47] and α, βaand κ were chosen using 10-fold cross validation on training samples.Table 4presents overall and geometric mean testing accuracies (mean and standard deviation), and number hidden neurons (mean and standard deviations) for SVM, ELM and SLSNC classifiers for 9 data sets. The numbers in brackets for SLSNC in the ‘No. of neurons’ column is the number of responders used for each problem.The results for SVM and ELM are reproduced from [48]. From the table, SLSNC performance is similar to SVM in all the binary classification problems. For Liver problem, SLSNC is better than SVM by 3%. However, in all these problems, SLSNC uses fewer number of intermediate neurons (2 order lower) compared to number of support vectors in SVM. In comparing with ELM performance, SLSNC produces 2–10 % improvement in performance in all binary classification problems with smaller number of intermediate neurons. For multi-category classification problems, for well-balanced problems like AE, IS and IRIS, SLSNC produces similar performance with a smaller number of intermediate neurons as that of SVM and ELM classifiers. For unbalanced problems like Wine and GI, SLSNC performance is at least 2–5% better with a smaller number of intermediate neurons than that of SVM and ELM classifiers.In order to statistically validate the results of performance comparison between SLSNC, SVM and ELM, we conducted the non-parametric Friedman test followed by a pairwise comparison using Conover-Inman [49] post-hoc analysis. The Friedman test measures the median of the various groups and tests whether all the medians are equivalent (null hypothesis). Instead of using the original data, the Friedman test ranks the measured parameter for all a given block across the different groups. If the p-value for the test statistic is less than the critical value of 0.05 then the null hypothesis is rejected. In case the null hypothesis is rejected, the post-hoc analysis is done to estimate the medians for which of the two groups are significantly different from each other.In our case, the three algorithms, namely SVM, ELM and SLSNC represent the three different groups and the twelve datasets are the different blocks. For a given data set and algorithm, ten random trials were conducted to measure the mean and variance of the algorithm performance. The test statistic obtained using the Friedman test is equal to 14.14 with a p-value of 0.00011. Hence, the null hypothesis is rejected, implying that the performance of the three algorithms is not equivalent.Next, the pairwise comparison is conducted using the Conover-Inman post-hoc analysis. Using the performance measurements, the mean ranks obtained for SVM, ELM and SLSNC are 2.0, 2.75 and 1.25 respectively. The critical difference of the ranks is equal to 0.5849. Since, SLSNC has the lowest rank and the difference between its rank and that of SVM and ELM is greater than the critical difference, it can be said that SLSNC performs better than the other algorithms, used for comparison, with a confidence interval of 95%.

@&#CONCLUSIONS@&#
In this paper, a new sequential learning spiking neural classifier, referred to as SLSNC, for pattern classification problems has been presented. The SLSNC employs a new neuron model in the input layer which converts any real valued stimuli into spikes with varying amplitudes and firing times, thereby providing a better encoding of the real valued input data. The sequential learning algorithm of SLSNC automatically determines the number of intermediate neurons required and adapts the synaptic weights using both long term depression and long term potentiation strategies. The learning algorithm employs computationally inexpensive weight update rules which require the execution of only elementary linear algebraic operations.The performance of the SLSNC has been evaluated using benchmark classification problems from UCI machine learning repository and is compared with other well-known spiking neural classifiers as well as standard SVM and ELM classifiers. The results of the study indicate that it performs better with a compact network architecture in comparison to other spiking neural classifiers used in this study. The comparison with SVM and ELM classifiers indicate that SLSNC produces similar or better performance with a compact network structure. The results of the performance comparison are validated using the Friedman test for statistical testing, followed by pairwise comparison using Conover-Inman post-hoc analysis. The estimated test statistic indicated that the performance of SLSNC is better than that of SVM and ELM with a confidence interval of 95%.