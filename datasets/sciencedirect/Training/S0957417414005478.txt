@&#MAIN-TITLE@&#
Feature selection and multi-kernel learning for adaptive graph regularized nonnegative matrix factorization

@&#HIGHLIGHTS@&#
Graph has been used to regularize nonnegative matrix factorization (NMF).However, noisy features and nonlinear distributed data effect the graph construction.We proposed to integrate feature selection and multi-kernel learning to this problem.Novel algorithms are developed to learn feature/kernel weights and NMF parameters.

@&#KEYPHRASES@&#
Data representation,Nonnegative matrix factorization,Graph regularization,Feature selection,Multi-kernel learning,

@&#ABSTRACT@&#
Nonnegative matrix factorization (NMF), a popular part-based representation technique, does not capture the intrinsic local geometric structure of the data space. Graph regularized NMF (GNMF) was recently proposed to avoid this limitation by regularizing NMF with a nearest neighbor graph constructed from the input data set. However, GNMF has two main bottlenecks. First, using the original feature space directly to construct the graph is not necessarily optimal because of the noisy and irrelevant features and nonlinear distributions of data samples. Second, one possible way to handle the nonlinear distribution of data samples is by kernel embedding. However, it is often difficult to choose the most suitable kernel. To solve these bottlenecks, we propose two novel graph-regularized NMF methods, AGNMFFSand AGNMFMK, by introducing feature selection and multiple-kernel learning to the graph regularized NMF, respectively. Instead of using a fixed graph as in GNMF, the two proposed methods learn the nearest neighbor graph that is adaptive to the selected features and learned multiple kernels, respectively. For each method, we propose a unified objective function to conduct feature selection/multi-kernel learning, NMF and adaptive graph regularization simultaneously. We further develop two iterative algorithms to solve the two optimization problems. Experimental results on two challenging pattern classification tasks demonstrate that the proposed methods significantly outperform state-of-the-art data representation methods.

@&#INTRODUCTION@&#
Nonnegative matrix factorization (NMF) (Lee & Seung, 2000; Sun, Wu, Wu, Guo, & Lu, 2012; Wang, Almasri, & Gao, 2012b; Wang, Bensmail, & Gao, 2013a; Wang & Gao, 2013; Wang, Wang, & Gao, 2013c) decomposes a nonnegative data matrix as a product of two low-rank nonnegative matrices, one of them is regarded as the basis matrix, while the other one as the coding matrix, which could be used as a reduced representation of the data samples in the data matrix (Kim, Chen, Kim, Pan, & Park, 2011a). This method has become popular in recent years for data representation in various areas, such as bioinformatics (Zheng, Ng, Zhang, Shiu, & Wang, 2011) and computer vision (Cai et al., 2013). Recently, Cai, He, Han, and Huang (2011) argued that NMF fails to exploit the intrinsic local geometric structure of the data space. They improved the traditional NMF to graph regularized nonnegative matrix factorization (GNMF). The basic idea is that the data samples are drawn from a low-dimensional manifold with a local geometric structure (Orsenigo & Vercellis, 2012; Wang, Bensmail, & Gao, 2012a, 2014a; Wang, Bensmail, Yao, & Gao, 2013b; Wang, Sun, & Gao, 2014b). Thus the nearby data samples in the original data space should also have similar NMF representations. In GNMF, the geometric structure of the data space is encoded by constructing a nearest neighbor graph, and then the matrix factorization is sought by adding a graph regularization to the original NMF objective function. The key component of GNMF is the graph. In the original GNMF algorithm, the graph is constructed according to the original input feature space. The nearest neighbors of a data sample is found by comparing the Euclidean distances (Lee, Rajkumar, Lo, Wan, & Isa, 2013a; Merigó & Casanovas, 2011) between pairs of data points, while the weights of edges are also estimated in the Euclidean space, by assuming that the original features could provide a proper representation of the local structure of the data space. However, as is well known that in many pattern recognition problems, using the original feature space directly is not appropriate because of the noisy and irrelevant features and the nonlinear distribution of the samples.To handle the noisy and irrelevant features, one may apply feature selection (Fakhraei, Soltanian-Zadeh, & Fotouhi, 2014; Iquebal, Pal, Ceglarek, & Tiwari, 2014; Lin, Chen, & Wu, 2014; Li, Wu, Li, & Ding, 2013b, 2013a) to assign different weights to different features, so that the data samples could be represented in a better way than using the original features. So far, the most broadly used feature selection method is proposed by Sun et al. (2012). Such an approach is able to determine feature weights from a statistics point of view to automatically discover the intrinsic features. It provides a powerful and efficient solution for feature selection in NMF. So this work has been internationally recognized by the researchers in this field. To handle the nonlinear distribution of the data samples, one could map the input data into a nonlinear feature space by kernel embedding (Cui & Soh, 2010; Yeh, Su, & Lee, 2013). However, the most suitable types and parameters of the kernels for a particular task is often unknown, and selection of the optimal kernel by exhaustive search on a pre-defined pool of kernels is usually time-consuming, and sometimes causes over-fitting. Multi-kernel learning (Chen, Li, Wei, Xu, & Shi, 2011; Yeh, Huang, & Lee, 2011), which seeks the optimal kernel by a weighted, linear combination of pre-defined candidate kernels, has been introduced to handle the problem of kernel selection. An, Yun, and Choi (2011), presented the Multi-Kernel NMF (NMFMK), which learns the best convex combination of multiple kernel matrices and NMF parameters jointly. However, graph regularization was not taken into consideration in their framework. In this paper, we will incorporate feature selection and multi-kernel learning into the graph regularization NMF to obtain novel and enhanced data representation methods. In this way, we could handle the problem of noisy and irrelevant features, nonlinearly distributed data samples, graph construction, and data matrix factorization simultaneously. Compared to the methods reported in the current literature which use a fixed graph for NMF parameters learning, our method can adapt the graph to the learned feature or kernel weights, which improves the NMF by providing it with a more reliable graph.Here, we propose two novel methods, AGNMFFSand AGNMFMK, that incorporate features selection and multiple-kernel learning into graph-regularized NMF, respectively. Feature selection or multi-kernel learning will provide a new data space for the graph construction of GNMF, and at the same time, GNMF will direct feature selection or multi-kernel learning. Both AGNMFFSand AGNMFMKare formulated as constraint optimization problems, each of which has a unified objective function to optimize feature selection/multi-kernel learning and graph-regularized NMF simultaneously. Experimental results demonstrate that the two proposed methods significantly outperform state-of-the-art data representation methods.The rest of the paper is organized as follows: We briefly review the GNMF in Section 2. We then propose the two novel algorithms, AGNMFFSand AGNMFMK, in Section 3. The proposed methods are compared with other NMF learning methods on two challenging data sets for classification tasks in Section 4. Finally, the paper is concluded in Section 5 with some future works.In this section, we will briefly introduce the graph regularized NMF as background knowledge of this paper.Given a training set with N nonnegative data samplesX={x1,…,xN}∈R+Drepresented as a nonnegative data matrixX=[x1,…,xN]∈R+D×N, wherexn∈R+Dis the D-dimensional nonnegative feature vector of the nth sample, NMF aims to find two nonnegative matrices H and W whose product can well approximate the original matrix X as(1)X≈HW,whereH∈RD×R, andW∈RR×N. Accordingly, each samplexnis approximated by a linear combination of the columns of H, weighted by the components of the nth column of W, as(2)xn≈∑r=1RhrwrnTherefore, H can be regarded as a collection of basis vectors, while,wn, the nth columns of W, can be regarded as the coding vector or a new representation of the nth data sample. The most commonly used cost function to solve H and W is based on the squared Euclidean distance (SED) between the two matrices:(3)ONMF(H,W)=||X-HW||2=Tr(XX⊤)-2Tr(XW⊤H⊤)+Tr(HWW⊤H⊤),whereTr(·)denotes the trace of a matrix.Cai et al. (2011) introduced the GNMF algorithm, by imposing the local invariance assumption (LIA) to NMF. If two data samplesxnandxmare close in the intrinsic geometric space of the data distribution,wnandwm, the coding vectors of these two samples with respect to the new basis, should also be close to each other; and vice versa. They modeled the local geometric structure by a K-nearest neighbor graphGconstructed from the data setX. For each data samplexn∈X, the set of its K nearest neighbors,Nn, inXis determined by the SED metric (Lee et al., 2013a) as(4)d(xn,xm)=||xn-xm||2=∑d=1D(xdn-xdm)2=xn⊤xn+xm⊤xm-2xn⊤xmA K-nearest neighbor graph is constructed forX. Each data sample inXwill be a node of the graph, and each nodexnwill be connected to its K nearest neighborsNn. We also define a weight matrixA∈RN×Non the graph, withAnmequal to the weight of the connection between nodesxnandxm. There are many choices to define the weight matrix A. Two of the most commonly used options are as follows:Gaussian kernel weighting(5)Anm=exp-||xn-xm||2σ2,ifxm∈Nn,0,otherwise.(6)Anm=xn⊤xm,ifxm∈Nn,0,otherwise.With the weight matrix A, we can use the following graph regularization term to measure the smoothness of the low-dimensional coding vector representations in W:(7)OG(W;A)=12∑n,m=1N||wn-wm||2Anm=Tr(WDW⊤)-Tr(WAW⊤)=Tr(WLW⊤),where D is a diagonal matrix whose entries are column sums of A, i.e.,Dnn=∑m=1NAnmandL=D-Ais the graph Laplacian matrix. By minimizingOG(W;A)with regard to W, we expect that if two data pointsxnandxmare close, i.e.,Anmis large,wnandwmare also close to each other.Combining this geometry-based regularizer,OG(W;A), with the original NMF objective function,ONMF(H,W), leads to the loss function of GNMF (Cai et al., 2011):(8)OGNMF(H,W;A)=ONMF(H,W)+αOG(W;A)=Tr(XX⊤)-2Tr(XW⊤H⊤)+Tr(HWW⊤H⊤)+αTr(WLW⊤),in which α is a tradeoff parameter. Thus the GNMF problem turns to a constrained minimization problem as(9)minH,WOGNMF(H,W;A),s.t.H⩾0,W⩾0where H and W can be solved in an iterative manner by optimizing and updating them alternately (Cai et al., 2011).In this section, we propose two enhanced data representation methods based on GNMF by encoding feature selection and multi-kernel learning, respectively.Given an input sample x represented as a vector of D nonnegative features asx=[x1,…,xD]⊤∈R+D, feature selection tries to scale each feature to obtain a weighted feature space, parameterized by a D-dimensional nonnegative feature weight vectorλ=[λ1,…,λD]⊤∈R+D, whereλdis the scaling factor for the dth feature (Sun, Todorovic, & Goodison, 2010b). We restrict its scale by∑d=1Dλd=1andλd⩾0. Thus the scaled feature vector of x is represented asx̃=[λ1x1,…,λDxD]⊤=diag(λ)x, wherediag(λ)is aD×Ddiagonal matrix with entries of λ along the main diagonal. The original data matrix and basis matrix for NMF can be represented in the scaled space as (10),(10)X̃=diag(λ)XandH̃=diag(λ)H,s.t.∑d=1Dλd=1,λd⩾0,d=1,…,D.By replacing the original features in X and H of NMF with the weighted featuresX̃andH̃defined in (10), we have the augmented objective function for NMF with feature selection in an enlarged parameter space(11)ONMFFS(H,W,λ)=||diag(λ)(X-HW)||2=Tr[diag(λ)2XX⊤]-2Tr[diag(λ)2XW⊤H⊤]+Tr[diag(λ)2HWW⊤H⊤]HereH,Wand λ are all the variables to solve so that the above objective function can be minimized.After the new feature space defined by feature weight vector λ is defined, the nearest neighbor graph should also be updated to be adaptive to the selected features. First, the K nearest neighbors,Nn, of the nth data point should be re-found according to the λ-weighted SED, i.e.,(12)dλ(xn,xm)=||xn-xm||λ2=∑d=1Dλd2(xdn-xdm)2The K nearest neighborsNnre-found by the λ-weighted distance is denoted asNnλ, and the graph adaptive to λ is donated asGλ. The corresponding weight matrixAλofGλshould also be updated. Here we discuss how to update the Gaussian kernel weighting for adaptive graph with feature selection, which is updated asAnmλexp(-||xn-xm||λ2σ2)ifxm∈Nnλ, and 0 otherwise.With the adaptive graphGλ, we can re-regularize the NMF in the selected feature space. Similar to the GNMF, we propose the adaptive graph regularization term as(13)OAG(W;Aλ)=12∑n,m=1N||wn-wm||2Anmλ=Tr(WLλW⊤),whereLλ=Dλ-Aλis the corresponding graph Laplacian. By minimizingOAG(W;Aλ), we expect that if two data pointsx̃nandx̃mare close with respect to the new features selected by λ, the representationswnandwmwith respect to the selected features should also be close to each other.To perform the feature selection together with the adaptive graph regularized NMF, we first propose the unified objective function for adaptive graph regularized NMF and feature selection for data representation, and then develop an alternately updating algorithm to estimate the basis matrix H, the coding coefficient matrix W and the feature weight matrix λ.•Objective function: Combining the NMF objective function with feature selection defined in (11) with the adaptive graph-based regularizer defined in (13) leads to the objective function of our AGNMF with feature selection — AGNMFFSalgorithm:(14)OAGNMFFS(H,W,λ)=ONMFFS(H,W,λ)+αOAG(W;Aλ)=Tr[diag(λ)2XX⊤]-2Tr[diag(λ)2XW⊤H⊤]+Tr[diag(λ)2HWW⊤H⊤]+αTr(WLλW⊤)The optimization problem (8) of GNMF can now be extended to accommodate the feature selection and adaptive graph:(15)minH,W,λOAGNMFFS(H,W,λ)s.t.H⩾0,W⩾0,∑d=1Dλd=C,λd⩾0,d=1,…,D.Optimization: Since direct optimization to (15) is difficult, we instead adopt an iterative, two-step strategy to alternately optimize(H,W)and λ. At each iteration, one of(H,W)and λ is optimized while the other is fixed, and then the roles of(H,W)and λ are switched. Iterations are repeated until convergence or a maximum number of iterations is reached.–On optimizing(H,W): By fixing λ and updating the adaptive graphGλwith its corresponding Laplacian matrixLλaccording to λ, the optimization problem (15) is reduced to(16)minH,WTr[diag(λ)2XX⊤]-2Tr[diag(λ)2XW⊤H⊤]+Tr[diag(λ)2HWW⊤H⊤]+αTr(WLλW⊤)s.t.H⩾0,W⩾0.The LagrangeLof the above optimization problem is(17)L=Tr[diag(λ)2XX⊤]-2Tr[diag(λ)2XW⊤H⊤]+Tr[diag(λ)2HWW⊤H⊤]+αTr(WLλW⊤)+Tr(ΦH⊤)+Tr(ΨW⊤),whereΦ=[ϕdr]andΨ=[ψrn]are the Lagrange multiplier matrices for constraintH⩾0andW⩾0, respectively. By setting the partial derivatives ofLwith respect to H and W to zero, we have(18)∂L∂H=-2diag(λ)2XW⊤+2diag(λ)2HWW⊤+Φ=0∂L∂W=-2H⊤diag(λ)2X+2H⊤diag(λ)2HW+2αWLλ+Ψ=0Using the KKT conditions, i.e.,ϕdrhdr=0andψrnwrn=0, we get the following equations forhdrandwrn:(19)-[diag(λ)2XW⊤]drhdr+[diag(λ)2HWW⊤]drhdr=0-[H⊤diag(λ)2X]rnwrn+[H⊤diag(λ)2HW]rnwrn+α(WLλ)rnwrn=0These equations lead to the following updating rules:(20)hdr←[diag(λ)2XW⊤]dr[diag(λ)2HWW⊤]drhdrwrn←[H⊤diag(λ)2X+αWAλ]rn[H⊤diag(λ)2HW+αWDλ]rnwrnOn optimizingλ: By fixing H and W, and removing the terms irrelevant to λ, the optimization problem (15) becomes(21)minλTr[diag(λ)2(XX⊤-2XW⊤H⊤+HWW⊤H⊤)]=Tr[diag(λ)2(YY⊤)],s.t.∑d=1Dλdd=1,λdd⩾0,d=1,…,D.whereY=X-HW. Here, the value ofλdindicates the weight of the dth feature. We rewrite the objective function of (21) as follows:(22)minλTr[diag(λ)2(YY⊤)]=∑d=1Dλd2∑n=1Nydn2=∑d=1Dλd2ed,s.t.∑d=1Dλdd=1,λdd⩾0,d=1,…,D.λd⩾0.whereydnis the(d,n)th element of matrix Y anded=∑n=1Nydn2. It could be optimized by using Theorem 1.The closed form solution of the optimization problem in(21)is given by:(23)λd=1/yd∑d=1F1/yd,d=1,…,DGiven the constrain of∑d=1Dλd=1and the Candy-Schwartz inequality, we have(24)1=∑d=1Dλd2=∑d=1Dλdyd·1yd2⩽∑d=1Dλd2yd∑d=1D1ydThus we have the following inequality,(25)∑d=1Dλd2yd⩾1∑d=1D1ydand the equal sign holds ifλdyd=C1yd, or(26)λd=C1yd,d=1,…,DMoreover, since∑d=1Dλd=1, we haveC∑d=1D1yd=1, thereforeC=1∑d=1D1yd, and the minimizer of (21) is (23).□Algorithm: The proposed iterative AGNMF algorithm with feature selection (named as AGNMFFS) is summarized in Algorithm 1.After learning(H,W)and λ via AGNMFFSfor the training data matrix X, we can use the basis matrix H and feature weight matrix λ to infer the coding vector for a new data point. When a new test data samplex∈R+Dcomes in, we first connect it to its K nearest neighborsNλfrom the training setXwhich are found by using the λ-weighted SED (12), and then calculate the weight vector of x asaλ=[a1λ,…,aNλ]∈RNwhereanλ=exp-||x-xm||λ2σ2, ifxn∈Nλ; and 0, otherwise. Assuming the coding of the training samples are not affected by the test sample, we only need to optimize the following objective function regarding to the coding vectorw∈RRof the test sample:(27)minwO(w)AGNMFFS=||diag(λ)(x-Hw)||2+α2∑n=1N||w-wn||2anλ=Trdiag(λ)2xx⊤-2Trdiag(λ)2xw⊤H⊤+Tr[diag(λ)2Hww⊤H⊤]+α2∑n=1NanλTr(ww⊤)-αTrw∑n=1Nanλwn⊤+α2∑n=1NanτTr(wnwn⊤)s.t.w⩾0.By setting the partial derivative of the Lagrange function of (27) with respect to w to zero, and using the KKT conditions, we can have the following updating rule for w:(28)wr←H⊤diag(λ)2x+α2∑n=1NanλwnrH⊤diag(λ)2Hw+α2∑n=1NanλwrwrBy repeating this updating rule, we could have the optimal coding vector, w, for the test sample.Consider a nonlinear mappingxn→φ(xn)orX→φ(X)=[φ(x1),…,φ(xN)], the kernel matrixK∈RN×Nis given byK=φ(X)⊤φ(X). A direct application of NMF to the feature matrixφ(X)yields(29)φ(X)≈HWFor the sake of convenience, we impose the constraint that the vectors defining H lie within the column space ofφ(X):hr=∑n=1Nfnrφ(xn)or(30)H=φ(X)F,wherefnris the(n,r)th element of the matrixF∈R+N×K. Substituting (30) to (3), we have the objective function for the kernelized version of NMF(31)ONMFk(F,W)=||φ(X)-φ(X)FW||2=Tr[φ(X)(I-FW)(I-FW)⊤φ(X)⊤]=Tr[φ(X)⊤φ(X)(I-FW)(I-FW)⊤]=Tr[K(I-FW)(I-FW)⊤]Suppose there are altogether L different kernel functions{Kl}l=1Lavailable for the NMF task in hand. Accordingly, there are L different but associated nonlinear feature spaces. In general, we do not know which kernel space should be used. An intuitive way is to use them all by concatenating all feature spaces into an augmented Hilbert space and associating each feature space with a relevance weightτl, whereτl⩾0,∑l=1Lτl=1. We denote the kernel weights as a vectorτ=[τ1,…,τL]⊤. Performing the NMF in such feature space is equivalent to employing a combined kernel function for the NMF:(32)Kτ=∑l=1LτlKl,s.t.τl⩾0,∑l=1Lτl=1We substitute this relation into (31) to obtain the objective function for Multiple Kernel-based NMF (NMFMK):(33)ONMFMK(F,W,τ)=Tr∑l=1LτlKl(I-FW)(I-FW)⊤To update the graphGregarding the multiple kernel space, given a τ, the K nearest neighborsNnτfor the GNMF algorithm will be re-found by the τ-weighted SED in the multiple kernel space, i.e.,(34)dτ(xn,xm)=||φ(xn)-φ(xm)||τ2=Kτ(xn,xn)+Kτ(xn,xm)-2Kτ(xn,xm)=∑l=1LτlKl(xn,xn)+Kl(xn,xm)-2Kl(xn,xm)The corresponding K nearest neighbor graph adaptive to τ is donated asG. Here we discuss the updating of dot-product weighting for the weight matrixAτof the adaptive graph with multiple kernel learning, i.e,Anmτ=φ(xn)⊤φ(xm)=Kτ(xn,xm)=∑l=1LτlKl(xn,xm), ifxm∈Nnτ; 0, otherwise.With the graphGτadaptive to the multiple kernel space, we then re-regularize the NMFMKin the multiple kernel space. We propose the Adaptive Graph regularization term as(35)OAG(W;Aτ)=12∑n,m=1N||wn-wm||2Anmτ=Tr(WLτW⊤),whereLτ=Dτ-Aτis the corresponding graph Laplacian.To perform the multi-kernel learning together with the adaptive graph regularized NMF, we first propose a unified object function, and then develop an alternately updating algorithm to solve it.•Objective function: Combining the NMF objective function with multiple kernel defined in (33) with the adaptive graph-based regularizer defined in (35) leads to the optimization problem of our AGNMF with multi-kernel learning — AGNMFMK:(36)minF,W,τOAGNMFMK(F,W,τ)=ONMFMK(F,W,τ)+αOAG(W;Aτ)+β||τ||2=Tr∑l=1LτlKl(I-FW)(I-FW)⊤+αTr(WLτW⊤)+β||τ||2=TrKτ(I-FW)(I-FW)⊤+αTr(WLτW⊤)+β||τ||2,s.t.F⩾0,W⩾0,τ⩾0,∑l=1Lτl=1where the regularization term||τ||2is also introduced to prevent the parameter τ from overfitting to one kernel.Optimization: Similar to AGNMFFS, we also adopt an iterative strategy to alternately optimize(F,W)and τ.–On optimizing(F,W): By fixing τ and updating the adaptive graphGτand kernel matrixKτ, the optimization problem (36) is reduced to(37)minF,WTrKτ(I-FW)(I-FW)⊤+αTr(WLτW⊤)s.t.F⩾0,W⩾0Similar to the optimization of H and W of AGNMFFS, we have following rules to update F and W:(38)fnr←(KτW⊤)nr(KτFWW⊤)nrfnrwrn←(F⊤Kτ+αWAτ)rn(F⊤KτFW+αWDτ)rnwrnOn optimizingτ: By fixing F and W, and removing the irrelevant terms, the optimization problem (36) becomes(39)minτTr∑l=1LτlKl(I-FW)(I-FW)⊤+β||τ||2=Tr∑l=1LτlKlZZ⊤+β||τ||2=∑l=1Lτlgl+β∑l=1Lτl2,s.t.τ⩾0,∑l=1Lτl=1whereZ=I-FWandgl=TrKlZZ⊤. The optimization of (39) with respect to the feature weights τ could be solved as a standard quadratic programming (QP) problem.Algorithm: The iterative AGNMF algorithm with multiple kernel learning (named as AGNMFMK) is summarized in Algorithm 2.When a test samplex∈RDcomes in, we first connect it to its K nearest neighborsNτfrom the training setX, which is found by using the τ-weighted SED (34). Then the weight vectoraτ=[a1τ,…,aNτ]∈RNis calculated asanτ=Kτ(x,xn), ifxn∈Nτ; 0, otherwise. We need to optimize the following objective function to solve w with AGNMFMK:(40)minwO(w)AGNMFMK=||ϕ(x)-ϕ(X)Fw||2+α2∑n=1N||w-wn||2anτ=Tr[Kτ(x,x)]-2Tr[Kτ(X,x)w⊤F⊤]+Tr[Kτ(X,X)Fww⊤F⊤]+α2∑n=1NanτTr(ww⊤)-αTrw∑n=1Nanτwn⊤+α2∑n=1NanτTr(wnwn⊤),s.t.w⩾0whereKτ(X,y)=[Kτ(x1,y),…,Kτ(xN,y)]⊤, andKτ(X,X)=[Kτ(xn,xm)]∈RN×N. By setting the partial derivative of the Lagrange function of (40) regarding w to zero and using the KKT conditions, we have the following updating rule for w(41)wr←F⊤Kτ(X,x)+α2∑n=1NanτwnrF⊤Kτ(X,X)Fw+α2∑n=1Nanτwrwr

@&#CONCLUSIONS@&#
