@&#MAIN-TITLE@&#
Can involving clients in simulation studies help them solve their future problems? A transfer of learning experiment

@&#HIGHLIGHTS@&#
We detail a methodology to measure transfer of learning and overconfidence in DES.Results show that transfer of learning from DES is difficult, but possible.Transfer of learning improves when extra time is provided for experimentation.Model builders made less mistakes than model reusers.Model builders made mistakes in higher confidence than model reusers.

@&#KEYPHRASES@&#
Behavioural OR,Psychology of decision,Model building,Model reuse,Discrete-event simulation,

@&#ABSTRACT@&#
It is often stated that involving the client in operational research studies increases conceptual learning about a system which can then be applied repeatedly to other, similar, systems. Our study provides a novel measurement approach for behavioural OR studies that aim to analyse the impact of modelling in long term problem solving and decision making. In particular, our approach is the first to operationalise the measurement of transfer of learning from modelling using the concepts of close and far transfer, and overconfidence. We investigate learning in discrete-event simulation (DES) projects through an experimental study. Participants were trained to manage queuing problems by varying the degree to which they were involved in building and using a DES model of a hospital emergency department. They were then asked to transfer learning to a set of analogous problems. Findings demonstrate that transfer of learning from a simulation study is difficult, but possible. However, this learning is only accessible when sufficient time is provided for clients to process the structural behaviour of the model. Overconfidence is also an issue when the clients who were involved in model building attempt to transfer their learning without the aid of a new model. Behavioural OR studies that aim to understand learning from modelling can ultimately improve our modelling interactions with clients; helping to ensure the benefits for a longer term; and enabling modelling efforts to become more sustainable.

@&#INTRODUCTION@&#
Going back as far as Churchman and Schainblatt (1965) it has been argued that client involvement in operational research (OR) studies, particularly in model building, is beneficial for successfully implementing a study's findings. Implementation can take the form of concrete changes to the system studied (Kotiadis, Tako, & Vasilakis, 2014), or as learning where the clients gain an understanding that can impact their future decision-making (Robinson, 2014). From this belief that involvement leads to implementation emanates a stream of research on client involvement, for instance, group model building (Andersen, Vennix, Richardson, & Rouwette, 2007; Rouwette, Korzilius, Vennix, & Jacobs, 2011; Rouwette, Vennix, & Mullemkom, 2002) and facilitated modelling (Franco & Montibeller, 2010). In the field of interest in this study, discrete-event simulation (DES), there have been a number of recent papers that focus on how to enhance client involvement in the process of developing and/or using a model (Adamides & Karacapilidis, 2006; den Hengst, de Vreede, & Maghnouji, 2007; Kotiadis et al., 2014; Robinson, 2001; Robinson, Radnor, Burgess, & Worthington, 2012; Robinson, Worthington, Burgess, & Radnor, 2014). There is, however, very little evidence, other than the anecdotal, that the hypothesised relationship between client involvement and successful implementation actually exists.In a recent study by the authors, implementation as learning is studied by investingating the so called high involvement hypothesis: that client involvement in a DES study improves client learning (Monks, Robinson, & Kotiadis, 2014). We found that involvement in incremental model development and validation, aids clients in the discovery of improvement options or variables previously not considered. The study also provided empirical evidence about the impact of involvement in model building and experimentation on single loop learning (Argyris & Schön, 1996): a change in client attitudes towards management implementation options brought about by involvement in modelling. Although facilitating single-loop learning through models is important the OR literature often comments on the behavioural assumption that modelling within a social context leads to deeper learning about concepts, such as queuing, that can be transferred and adapted to solve future problems.In this paper we provide a new analysis and new data to test the impact of client involvement in a DES study on the ability of clients to transfer their learning to another context i.e. a degree of learning more in line with a ‘double-loop’ (Argyris & Schön, 1996). Here management actions, deep transferrable understanding about a problem and management norms about how to learn are corrected. We contribute new empirical results that involvement in modelling aids learning for future decision making by such a double-loop. We also provide a novel measurement approach for investigating structural and conceptual learning and confidence effects from models. These contributions go beyond our previous work that focused simply on solving an immediate management problem. Our new results and measurement approach focus on the ability of clients to learn more deeply and to recognise the generic lessons that could apply to situations with a similar structure. For example, having realised for a specific queuing problem that resources should not be loaded at 100 percent, the client then transfers that same lesson to a new context involving a similar queuing problem. The benefit of such learning is that the implementation of the DES study extends beyond the immediate problem and it can repeatedly impact upon many problem situations in which the client is involved without the need for further simulation. Behavioural OR studies that aim to understand such learning from modelling can ultimately improve the community of practice's interaction with clients; helping to ensure that benefits are longer term; and enabling modelling efforts to become more sustainable.The specific objectives of the current study are: to determine if clients can transfer their learning from a DES study to another context; and to determine the effect of client involvement on their ability to transfer that learning. As such, our work aligns with research in behavioural OR that analyses the psychological aspects of model use in problem solving (Hämälläinen, Luoma, & Saarinen, 2013). The problem solving focus means that the study has much in common with research about client learning in group model building in system dynamics (Andersen et al., 2007; Lane, 1994; Rouwette et al., 2002, 2011; Scott, Cavana, & Cameron, 2014). The main difference is that we use experimental methods to explore the impact of study processes at the individual level.The remainder of this paper is organised as follows. We firstly present an overview of the theory associated with transfer of learning. This covers both the conditions required for successful transfer and the difficulties people typically face. Secondly, we provide an explanation of the experimental study and materials used in our research. We follow this with the results of the experiment along with a discussion of both the implications and limitations of the work.A typical behavioural experiment for transfer of learning consists of analogous training and transfer problems. A participant solves the training problem, is given feedback, and then attempts to solve the transfer problem (Bassok, 2003). An often cited study uses the Tower of Hanoi problem in the training task (Gick & Holyoak, 1980). Participants play the role of a general that must use an army to capture the Tower of Hanoi from a rival general. There are four routes to the tower. If the participant attempts to capture the tower using any single route the army is defeated. However, if the participant divides their army and uses all four routes at once they can overpower the enemy forces and capture the tower. In the transfer problem the participant must decide how to kill a tumour in a patient using X-rays. The problem is that the required dosage of X-ray will damage the tissue it passes through on the way to the tumour. The transfer concept is divide and conquer, i.e. apply lower intensity X-rays from different sides of the body simultaneously.There are two important factors in the success of the transfer of learning: structural and perceived surface similarity. Structural similarity refers to the underlying mechanics of the problem being studied. For example, Bakken, Gould, and Kim (1994) study the transfer performance between two system dynamics models with the same underlying feedback structure, but different surface components: a housing market and an oil tanker market. Similarly, in the context of DES both an emergency department and a call centre have an element of structural similarity. They can be considered as queuing systems that are subject to stochastic variation, with much of the variability being driven by arrivals and service times.Surface similarity refers to whether an individual perceives the training task and transfer task to be similar. Surface similarity is an important cue for initiating a transfer attempt. If an individual does not see any similarity to the training task in the transfer task then they will not attempt to transfer the knowledge. If, however, they do see the similarity, then transfer will be attempted (Bassok, 2003).Transfer success is most likely when there is both structural and perceived surface similarity. In this case an individual perceives that the new problem is highly similar to one they have tackled before. In addition, the original and new problems are structurally similar so it is valid to transfer the learning from the one situation to the other.Fig. 1illustrates the surface and structural similarity of four different queuing problems to a hospital emergency department (the training task). For the healthcare walk in clinic there is a high level of both surface and structural similarity; both concern people in a healthcare setting being seen by healthcare professionals, and both have highly unpredictable inter-arrival times. Structurally a call centre is very similar to an emergency department, but given the different context, the similarity on the surface is not as apparent.Fig. 1 shows that transfer of learning is probably much less likely from a healthcare context to a manufacturing context. This is, in part, because surface similarity is low; individuals are likely to perceive a large difference between the patients in a healthcare setting and widgets on a production line. Meanwhile, structurally the problems are quite different: in the manufacturing domain variation is driven from cycle times and machine breakdowns as opposed to arrivals and service times.In a situation where an individual perceives a problem to be highly similar to one they have tackled before, but structurally they are very different, there is a danger of incorrectly transferring learning. One such example is delayed discharges of patients from a hospital. As shown in Fig. 1, this is a problem that might appear very similar on the surface to an emergency department, i.e. there are patients queuing for resource to discharge them. Structurally, however, the discharge of patients is quite different and improving the timeliness of patient discharge involves understanding inter-organisation co-ordination as well as understanding the trade-off between demand and capacity.Transfer of learning is classified as either close or far. The degree of closeness is associated with the perceived surface similarity of the training and transfer problems. For example, a learner may perceive that on the surface a manufacturing domain is very different from a healthcare domain. Successful positive transfer from a healthcare training problem to a manufacturing transfer problem would therefore be classified as far transfer. Empirical studies support the view that learning for far transfer is generally more difficult (Barnett & Ceci, 2002). Although this appears to be a fairly straightforward point to grasp it is actually quite difficult to define far, as perceived similarity is highly dependent on contextual and individual factors (Barnett & Ceci, 2002). Classification of problems as far may refer to different contexts (such as manufacturing and healthcare), the time lag between transfer attempts or the location where transfer is attempted (e.g. a classroom versus a work environment). One method that seems to have a positive effect on transfer is to provide multiple examples of the problem in the training task. Transfer performance appears to improve if both close and far examples are given during training (Gick & Holyoak, 1980). The explanation appears to be that the differences in surface similarity in the examples improve the individuals’ ability to abstract the structure of the problem – giving them a deeper understanding that is transferable.A final aspect to consider is spontaneous versus informed transfer (Bassok, 2003; Gick & Holyoak, 1987). Consider a manager who is involved in a simulation study of a manufacturing production line. In the study the manager learns that she cannot run production line machines at greater than 90 percent utilisation and still achieve lead time targets. Furthermore, the manager finds that there are huge differences in lead times if machines are run at 85 percent and 95 percent utilisation. Sometime later the manager moves organisation and is put in charge of a new production line. If, unprompted, the manager considers making similar decisions in the new context (i.e. considers lower average utilisation to improve lead times), then the transfer was spontaneous. The manager recognised the similarity between the problems, accessed the relevant knowledge and transferred it successfully.Informed transfer is where the manager is told to transfer what they have learnt in training. Informed transfer is an artificial procedure only available in a laboratory setting. Its use is in differentiating between problems in accessing relevant knowledge and problems in learning. Many transfer of learning studies employ a design where one group is given a transfer hint (i.e. now use what you have learnt to solve the following problem) and another group is simply presented with the new problem (Bassok, 2003). If the hint group outperforms the no-hint group then it can be concluded that learning is present, but there is an access problem, i.e. the no-hint group does not recognise the similarity between the training and transfer problems. If neither group perform well then there would appear to be a learning problem.In summary, to achieve transfer an individual must not only possess the relevant knowledge, but also perceive the new problem to be similar to a previous problem. Of course, for transfer to be correct the new problem must also be structurally similar to the previous. Perceived similarity can be subjectively divided into close and far; the likelihood of transfer success declining with distance. Improvement of transfer likelihood to far domains comes with increased exposure to analogous problems in different domains. As an analogy consider an experienced DES modeller who has worked in similar projects across manufacturing, healthcare, the public sector and other domains during their career. They are much more likely to think in terms of the structure of the queuing problem than an individual who has had involvement in only a single simulation study. Meanwhile, in daily life, transfer, if it occurs, is spontaneous; there is no need for prompting. However, in the artificial world of the laboratory hints may be needed to encourage subjects to access the relevant knowledge.To investigate transfer of learning we sought to design an experiment to explore the degree to which novice simulation users could successfully transfer learning from a recent simulation study to analogous problems in the same and different domains. This investigation involved collecting additional data from the experiments reported in Monks et al. (2014). As such, we do not report the full details of the experiment here, but provide an overview with details of the approach used for measuring transfer of learning.The purpose of the experiment is to investigate the hypothesis that greater involvement in model building leads to greater learning about the structure of a problem and so to an improved ability to transfer learning. We created a behavioural experiment making use of novice simulation users (64 business undergraduate students with no simulation training). Bakken et al. (1994) found that MBA students substantially outperformed experts in an experiment that tested for transfer of insights with system dynamics models, since experts tended to refer to their real life experience. As a result, our expectation was that the students would yield a higher rate of transfer success than experts would in our experiment.The context of the training queuing problem is a fictitious hospital emergency department. To solve the training problem and improve the emergency department performance participants must learn about two concepts:•There is a non-linear relationship between resource utilisation and the time an entity spends in a process (T1).Management (or reduction) of process variation can reduce the time an entity spends in a process (T2).The simplified process for developing and using the model that we followed with the students allowed the participants to engage in a simulation study in a similar manner as a client would in the real world. For example, participants could question the assumptions in a model, perform face validation checks, request extra detail and define scenarios to be run. At the end of the simulation study we assessed the participants’ ability to transfer their learning to eight scenarios containing a description of a queuing problem analogous to the problem faced in the emergency department. We now provide an overview of how the participants were involved in model building and experimentation as well as the three experimental conditions used to vary the degree to which clients are involved in model building.In order to involve participants in model building we created a condensed simulation study process with a modeller and a client. Participants took the role of a client while a researcher provided the modelling expertise. The model begins very simply as a single queue and server model where treatment in ED is represented by a single activity with a single first in first out queue. There are six rounds of refinement to the model where detail is added. The final model is then used for experimentation. Although each participant used the same model for experimentation, the model may evolve quite differently depending on participant choices during the training task. By the end of the model building, all participants have interacted with each of the six models.Fig. 2illustrates the procedure used for each refinement of the model. For example, a participant may have chosen to add either doctor resources or split patients into major and minor injuries to the initial single server model. Once this choice had been made a sub-model was opened containing the correct level of detail (all combinations are available to the researcher). The participant was presented with a conceptual model summarising the change that has been made, and the remaining simplifications and assumptions. Once reviewed the participant inspected the visual model and could explore a results spreadsheet. The procedure is repeated when the participant asks for another level of detail to be added to the model (e.g. doctor seniority or prioritisation of major injuries). The model building stage ended after the sixth model was completed and all the detail had been added.Fig. 2 also illustrates that not all participants’ requests could be accommodated by the researcher. A common example involves a simplification of doctor multi-tasking (Günal & Pidd, 2006). Emergency department doctors treat multiple patients concurrently and move between them during the patients’ stay. In the simulation this concurrent treatment is simplified and modelled as slots, i.e. a doctor can treat four patients concurrently therefore we add four doctor slots for every doctor resource available. If a participant asked for this simplification to be removed the researcher had a scripted answer available: ‘it is not possible to remove that simplification as we do not have data available on individual doctor consultations’. In these situations the participant has to reconsider which simplification should be removed. A minority of participants continued to question model simplifications (or requested additional model detail) once the model was ‘complete’. This is not unlike standard client face validation processes in real studies. In the instances where additional detail was requested the scripted answer was that the study did not have sufficient data to include this detail. In the instance where the request related to an assumption, the scripted answer was to advise the participant that assumptions could be explored during the experimentation phase.For the experimentation participants again acted as the clients. Participants directed the researcher with regards to which scenarios were run while the researcher provided guidance on what is possible. Participants were able to watch the model running in visual mode and review batch run results. A spreadsheet tracked all scenarios run and provided a summary so that the participants could review their experimentation history.We manipulated participant involvement in model building and experimentation by including three experimental conditions which are summarised in Table 1. Low involvement was investigated in a condition where participants reuse a pre-existing model (‘model reuse’ – MR). High involvement was investigated in two conditions. In both the participants were involved in the same model building process. However, in the first high involvement condition the budget of time available for building and using the model is equal to that for MR (‘model building with limited experimentation time’ – MBL). As a result they only had limited time for experimentation; enough to explore three scenarios. In the second high involvement condition (‘model building’ – MB), participants were given 45 minutes longer enabling them to carry out much more extensive experimentation. This means they had equivalent time for experimentation as the MR participants. For the experiment the students were randomly assigned to the three conditions to give 22 students in the MR group, 21 in the MBL group and 21 students in the MB group. We make use of randomisation in the allocation of participants to groups to mitigate the risk of imbalances across groups in factors such as intelligence and experience of problem solving.Participants in the MR group were introduced to simulation in the same manner as the MB and MBL groups, as detailed in Section 3.1.1. However, there were no iterative steps in building the model. Instead, once the introduction to the simulation was complete, MR participants were presented with the complete and final model as used by the participants in MB and MBL. The MR participants were then talked through the logic of the model using a script. Participants in all conditions were given the same documentation about the model.We created eight transfer problems with varying distances of surface similarity to the emergency department problem. All participants answered these in the same order. For simplicity we classify these as either close or far transfer problems. The four close transfer problems are set in a healthcare context. These scenarios have high surface similarity and high structural similarity to the training problem. In contrast the far transfer problems have low surface similarity to the training problem, but are still structurally similar (i.e. a queuing problem). The far transfer problems are either set in call centres or a food manufacturing plant. Each problem details a scenario, provides transfer cues, lists multiple choice answers and provides space for a qualitative answer. We include the qualitative answer in order to accurately separate the random chance of selecting a correct answer from the successful transfer of learning. Transfer cues are pieces of information that prompt participants to recall the case study problem. For example, queues of customers in a scenario should prompt participants to recall that the ED problem was based around queue management. Each scenario was designed to test for one of the two transfer concepts listed in Section 3.1.The eight transfer scenarios, and the associated transfer cues and concepts, are summarised in Table 2. We presented these to each participant using a questionnaire following the training task. The training problem contains numerous examples of the two transfer concepts in action. For example, participants ran experiments analysing the allocation of nurses to shifts and the use of resources to learn about resource utilisation and performance. Scenarios S2 and S8 listed in Table 2 test for transfer of this learning (T1). Scenario S2 presents the problem of resource utilisation at another similar hospital where performance is currently good, but demand is about to increase. To transfer the learning successfully participants must correctly reason that the loss of spare (buffer) capacity will likely reduce average performance against targets. Far transfer is tested by Scenario S8 set in a service call centre. For successful transfer of learning the participants must identify that the better performing shifts have more spare capacity.An example of concept T2 in the training problem is the management of the variation of different kinds of patient emergencies by pooling treatment cubicles. Scenarios S4 and S6 test for transfer of this concept. Scenario S4 asks participants to decide how waiting time could be minimised when checking into a co-located walk in centre and GP surgery. For successful transfer participants must correctly reason that combining the queues is a more efficient way to manage the variation in types of patient arriving. Far transfer is tested by scenario S6, a police call centre context, where participants are asked how to reduce caller waiting time to a number of small regional call centres. For successful transfer of learning the participants must recognise that a single larger call centre manages the variation in regional demand more efficiently. Full details of the scenarios can be found in the online supplementary material.We measured two types of dependent variables: success in transfer of learning to problems analogous to the training problem and the overconfidence in the participants’ answers. There are three transfer of learning measures: total, close and far transfer. Overconfidence has two measures, overestimation of ability to correctly transfer and the proportion of errors made in high confidence.Transfer of learning was assessed using a post-test questionnaire consisting of eight scenarios that were developed over a pilot of 16 participants. Each transfer scenario asked participants to select a multiple choice answer, provide a qualitative answer describing their reasoning and report the confidence they had in their answer. All participants were given the same scenarios in the same order and provided the same informationMeasurement of the confidence participants had in their answers to the transfer scenarios is based on scales used to measure metacognitive confidence (Petty, Briñol, & Tormala, 2002). After answering each transfer problem participants were asked to rate the confidence they have in their answer on a nine-point scale (1 = not at all confident to 9 = extremely confident). Participants were told that answers of above five reflected the belief that their answer was correct. Post-hoc analysis revealed that only 18 percent of answers were below five indicating that the vast majority of participants believed they were successful in solving each transfer problem.As an example of a transfer scenario, consider call centre scenario S6 described in Table 3. This scenario provides three options to choose from to reduce call centre waiting times. Participant MR1 believed that waiting time performance would be improved by choosing the option that split the call centres into smaller geographic regions. Participant MR1 provided the following qualitative answer:“[Option A allows] specialisation of tasks increasing focus and speed. Obviously if [the call centres are] still not meeting targets [then] increase staffing and lines. Combining everything (option c) complicates tasks and therefore increases queues”Participant MR1 was very confident that this answer was correct and scored herself as an eight out of nine.The answers were coded twice by one of the authors (Monks) with a six month time gap. To minimise any bias the participant details were hidden from view during coding and the order of the answers was randomised in each coding. The coding had a high reliability, α = 0.87, and it had a high intra-class correlation coefficient, 0.838 < r < 0.895 (91 percent of the codings were the same). The differences between each coding typically reflected cases where the participants had provided minimal detail on their reasoning. The differences were resolved by reviewing the comments made in each coding and agreeing a final score.As an example of the coding procedure consider the information cues provided to a participant by scenario S2 detailed in Table 2. The performance target and percentages provide the cues to recall the performance of the simulation model. The participants were asked if they agree that more staff should be introduced. All participants were asked to give the reasons why they think their choice would improve the performance of the system. An example answer provided by a participant is:“As the workload is going to increase, it is important to increase the number of staff so that they can cope. Depending on the increase in workload it may also be necessary to increase the number of cubicles, but is unlikely as their utilisation is currently lower than staff utilisation and won't reach its maximum”The second sentence provides the most detail on the participants’ reasoning. The participant was thinking in terms of maximum capacity. If the resources are working at less than 100 percent then they can cope. If 100 percent is exceeded then more resource is necessary. Whilst this seems sensible it fails to transfer any learning from the training problem in which it was identified that even with utilisation below 100 percent, resources cannot cope when there is variability in arrival and service times. An example of successful transfer would discuss performance levels and the relationship to utilisation: even if utilisation is increased to 90 percent there will be a change in the performance of the system. Hence the example answer above is coded as a zero – failure to transfer learning.Subjectivity in the coding procedure was minimised by constructing focused transfer scenarios testing a single transfer concept and by issuing a standardised model answer to the coder. As importantly, the pilot experiments provided insight into the phrasing of participant responses and the associated understanding of the transfer concept. We illustrate the potential differences in participant answers for transfer scenario S6 in Table 4. This lists the participant identifier, their qualitative answer, if the correct multiple choice answer is chosen and if the reasoning has been coded as correct. The first three examples are coded as successful transfer and illustrate the range of phrasings that participants used to describe the benefits of queue pooling to handle variability. Although different phrasings are used it is clear that each participant recognises variability in inter-arrival rates and service times lies at the heart of the problem. Incorrect qualitative answers often stood in stark contrast to successful transfer. MB7’s answer in Table 4 illustrates this point i.e. the participant argued that a larger call centre is more efficient (lower average service times) than multiple smaller call centres. The final participant MBL4 in Table 4 chose an incorrect answer (more call operators), but again illustrates the clear cut nature of incorrect answers. Here the participant argues that the demand in each region is ‘uniform’ (meaning constant and equal) and hence there is no benefit in pooling.Rather than simply assessing incorrect transfer we adopted two measures of overconfidence in transfer. This allowed us to distinguish between an erroneous answer made because the participant was unsure what to do and erroneous answers that would potentially be acted upon by the participant. The two overconfidence measures are constructed from the participant's confidence scores on each transfer scenario. Overestimation of their own abilities (Bendoly, Croson, Goncalves, & Schultz, 2010) is the difference between the proportion of problems to which a participant predicts that they have successfully transferred learning and the actual proportion where learning is successfully transferred. For example, if a participant predicts that they have correctly answered six out of the eight problems correctly (75 percent), but have actually only answered four correctly (50 percent) then they have overestimated their ability by 25 percent. Secondly, overconfidence was measured as the proportion of errors that are made with high confidence. High confidence is classed as a reported confidence score (Section 3.4.2) of greater than five. Therefore, a high confidence error is an incorrect answer made with a confidence score of greater than five. As the choice of the cut-off for high confidence errors was a judgement (i.e. the upper half of the scale), we include a sensitivity analysis of results using a more stringent cut-off of eight out of nine (section 4.3).

@&#CONCLUSIONS@&#
Practical simulation studies that aim to help clients and businesses better manage their processes should take note our results. It is often assumed that involvement of clients in modelling leads to (double-loop) learning about the underlying structure of processes and systems under study. Our results demonstrate that, in line with learning theory, such learning is difficult to transfer to analogous problems even in the same domain. We found transfer of learning from a simulation study is difficult, but possible. The main difficulty for participants was recognising the structural similarity of the transfer and training problems; however, access problems were reduced when participants were provided sufficient time for involvement in both model building and experimentation. In the real world, where the surface similarity of analogous problems is even more obscured and budget is tight, we should expect this to be even more difficult for clients. Maybe the primary learning that can be transferred from a DES study is that the problem addressed was non-trivial and that when a similar problem is encountered in the future it would be useful to employ DES again.A novel contribution of our behavioural research is that the level of involvement in model building and experimentation affects overconfidence in subsequent decisions; specifically we saw that model building groups made a higher proportion of high confidence errors than model reusers. Further fieldwork research is needed to replicate and understand these confidence effects. For instance are differences due to model ownership and pride, achieved through involvement in model building, or due to not invented here syndrome, an issue with model reuse.As the field of behavioural OR emerges, the body of experimental and fieldwork studies that analyse modelling and model use offer a unique opportunity to improve the knowledge of how to run modelling projects to achieve maximum benefit both in terms of improved understanding and improved likelihood of implementation of findings. Our study provides results and a novel measurement approach to further these aspirations.