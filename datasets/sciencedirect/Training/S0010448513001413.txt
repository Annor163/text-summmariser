@&#MAIN-TITLE@&#
A hybrid differential evolution augmented Lagrangian method for constrained numerical and engineering optimization

@&#HIGHLIGHTS@&#
A method hybridizing augmented Lagrangian multiplier and differential evolution algorithm is proposed.We formulate a bound constrained optimization problem by a modified augmented Lagrangian function.The proposed algorithm is successfully tested on several benchmark test functions and four engineering design problems.

@&#KEYPHRASES@&#
Constrained optimization problem,Differential evolution algorithm,Modified augmented Lagrangian multiplier method,Engineering optimization,

@&#ABSTRACT@&#
We present a new hybrid method for solving constrained numerical and engineering optimization problems in this paper. The proposed hybrid method takes advantage of the differential evolution (DE) ability to find global optimum in problems with complex design spaces while directly enforcing feasibility of constraints using a modified augmented Lagrangian multiplier method. The basic steps of the proposed method are comprised of an outer iteration, in which the Lagrangian multipliers and various penalty parameters are updated using a first-order update scheme, and an inner iteration, in which a nonlinear optimization of the modified augmented Lagrangian function with simple bound constraints is implemented by a modified differential evolution algorithm. Experimental results based on several well-known constrained numerical and engineering optimization problems demonstrate that the proposed method shows better performance in comparison to the state-of-the-art algorithms.

@&#INTRODUCTION@&#
In real-world applications, most optimization problems are subject to different types of constraints. These problems are known as constrained optimization problems. In the minimization sense, general constrained optimization problems can be formulated as follows:(1)minf(x→)(a)s.t.gj(x→)=0,j=1,2,…,p(b)gj(x→)≤0,j=p+1,…,m(c)li≤xi≤ui,i=1,2,…,n(d)wherex→=(x1,x2,…,xn)is a dimensional vector ofndecision variables,f(x→)is an objective function,gj(x→)=0andgj(x→)≤0are known as equality and inequality constraints, respectively.pis the number of equality constraints andm−pis the number of inequality constraints,lianduiare the lower bound and the upper bound ofxi, respectively.Evolutionary algorithms (EAs) have many advantages over conventional nonlinear programming techniques: the gradients of the cost function and constraint functions are not required, easy implementation, and the chance of being trapped by a local minimum is lower. Due to these advantages, evolutionary algorithms have been successfully and broadly applied to solve constrained optimization problems  [1–10] recently. It is necessary to note that evolutionary algorithms are unconstrained optimization methods that need additional mechanism to deal with constraints when solving constrained optimization problems. As a result, a variety of EA-based constraint-handling techniques have been developed  [11,12].Penalty function methods are the most common constraint-handling technique. They use the amount of constraint violation to punish an infeasible solution so that it is less likely to survive into the next generation than a feasible solution  [13]. The augmented Lagrangian is an interesting penalty function that avoids the side-effects associated with ill-conditioning of simpler penalty and barrier functions. Recent studies have used different augmented Lagrangian multiplier methods with an evolutionary algorithm. Kim and Myung  [14] proposed a two-phase evolutionary programming using the augmented Lagrangian function in the second phase. In this method, the Lagrangian multiplier is updated using the first-order update scheme applied frequently in the deterministic augmented Lagrangian methods. Although this method exhibits good convergence characteristics, it has been tested only for small-scale problems. Lewis and Torczon  [15] proposed an augmented Lagrangian technique, where a pattern search algorithm is used to solve the unconstrained problem, based on the augmented Lagrangian function presented by Conn et al.  [16]. Tahk and Sun  [17] used a co-evolutionary augmented Lagrangian method to solve min–max problems by means of two populations of evolution strategies with annealing scheme. Krohling and Coelho  [18] also formulated constrained optimization problems as min–max problems and proposed the co-evolutionary particle swarm optimization using Gaussian distribution. Rocha et al.  [19] used an augmented Lagrangian function method along with a fish swarm based optimization approach for solving numerical test problems. Jansen and Perez  [20] implemented a serial augmented Lagrangian method in which a particle swarm optimization algorithm is used to solve the augmented function for fixed multiplier values.In the above approaches, the augmented Lagrangian functions were used to deal with the constraints in constrained optimization problems. However, penalty vectors were only considered as fixed vectors of parameter. They were given at the beginning of the algorithms and kept unchanged during the whole process of solution. It is difficult and very important to choose some good penalty vectors. In addition, Mezura-Montes and Cecilia  [21] established a performance comparison of four bio-inspired algorithms with the same constraint-handling technique (i.e., Deb’s feasibility-based rule) to solve 24 benchmark test functions. These four bio-inspired algorithms are differential evolution, genetic algorithm, evolution strategy, and particle swarm optimization. The overall results indicate that differential evolution is the most competitive among all of the compared algorithms for this set of test functions.In this paper, we presented a modified augmented Lagrangian technique, where a differential evolution algorithm is used to solve the unconstrained problem, based on the augmented Lagrangian function proposed by Liang  [22]. The basic steps of the proposed method comprise an outer iteration, in which the Lagrange multipliers and various penalty parameters are updated using a first-order update scheme, and an inner iteration, in which a nonlinear optimization of the modified augmented Lagrangian function with bound constraints is solved by a differential evolution algorithm.The rest of this paper is organized as follows. In Section  2, the modified augmented Lagrangian formulation method is described. In Section  3, the proposed hybrid method is discussed in sufficient detail. Simulation results based on constrained numerical optimization and engineering design problems and comparisons with previously reported results are presented in Section  4. Finally, the conclusions are given in Section  5.In nonlinear constrained engineering optimization, the problem size ranges from a few hundred to several thousands of variables and constraints. Currently, the most frequently used solution methods are the generalized reduced gradient methods, successive quadratic programming methods, and the modified barrier function methods. These approaches are based on the linearization techniques and can be applied to problems with either a few variables, when used in full space, or a few degrees of freedom, when used in reduced space. Also, the presence of many inequality constraints (and bounds) may make their active-set based strategies quite inefficient. The modified barrier function method, which transforms the originally constrained problem to a series of unconstrained ones, has finite convergence as opposed to asymptotic convergence for the classical barrier function methods and their barrier parameters need not be driven to zero to obtain the solution. But the case of equality constraints poses a serious difficulty on the method. All these methods start from an initial point and iteratively produce a sequence to approach some local solution to the studied problem. The purpose of this work is to utilize the modified augmented Lagrangian multiplier method for constrained problems (1).In formula (1), if the simple bound (1)(d) is not present, then one can use the modified augmented Lagrange multiplier method to solve (1)(a)–(c). For the given Lagrange multiplier vectorλkand penalty parameter vectorσk, the unconstrained penalty sub-problem at thekth step of this method is(2)MinimizeP(x,λk,σk)whereP(x,λ,σ)is the following modified augmented Lagrangian function:(3)P(x,λ,σ)=f(x)−∑j=1p[λjgj(x)−12σj(gj(x))2]−∑j=p+1mP̃j(x,λ,σ)andP̃j(x,λ,σ)is defined as follows:(4)P̃j(x,λ,σ)={λjgj(x)−12σj(gj(x))2,ifλj−σjgj(x)>012λj2/σj,otherwise .It can be easily shown that the Kuhn–Tucker solution(x∗,λ∗)of the primal problem (1)(a)–(c) is identical to that of the augmented problem (2). It is also well known that, if the Kuhn–Tucker solution is a strong local minimum, then there exists a constantσ̄such thatx∗is a strong local minimum ofP(x,λ∗,σ)for all penalty vectorσwhich component not less thanσ̄; the Hessian ofP(x,λ,σ)with respect toxnear(x∗,λ∗)can be made positive definite. Therefore,x∗can be obtained by an unconstrained search from a point close tox∗ifλ∗is known andσis large enough.If the simple bound (1)(d) is present, the above modified augmented Lagrange multiplier method needs to be modified. In modified barrier function methods, the simple bound constraints are treated as the general inequality constraintsxi−li≥0andui−xi≥0, which enlarges greatly the number of Lagrange multipliers and penalty parameters. So, we make another modification to deal with the bound constraints. At thekth step, assume that the Lagrange multiplier vectorλkand penalty parameter vectorσkare given; we solve the following bound constrained sub-problem instead of (2):(5){minP(x,λk,σk)s.t.li≤xi≤uiwhereP(x,λ,σ)is the same modified augmented Lagrangian function as in (3). LetS⊆Rndesignate the search space, which is defined by the lower and upper bounds of the variables (1)(d). The solutionx∗to sub-problem (5) can be obtained by searching the search space ifλ∗is known andσis large enough. We will choose the differential evolution algorithm for the global search in (5). The details are discussed below.The proposed hybrid approach is performed in two stages (as shown in Fig. 1). The outer stage is performed, which formulates a modified augmented Lagrangian function, updates the Lagrange multipliers and penalty parameters, checks for convergence and reinitiates another bound constrained minimization (5) accordingly or declares convergence. Following this, the inner stage is the bound constrained global minimization of the modified augmented Lagrangian function, in which a new iterative point near to the global minimum is found via the modified differential algorithm. For given starting guessλ0andσ0of the vectors of Lagrange multipliers and penalty parameters, the framework of the proposed hybrid approach can be described as in Fig. 1. The flowchart of the MAL-DE algorithm is presented in Fig. 2.For the proposed hybrid method (called MAL-DE), the Lagrange multiplier vectorλkand the penalty parameter vectorσkdrive the approximate global minimum of sub-problem (5) to that of the original problem (1) iteratively. It is well known that if the original problem has a feasible solution, the multiplier penalty function method has finite convergence. The options for initializing the Lagrange multiplier vectorλwithin MAL-DE allow two choices. The first is set the initialization of the vector of multipliers to any positive vector, and the second is using the provision of initial guesses of the multipliers for each constraint explicitly by the user (e.g. as stored from a previous run of a similar problem).Assuming that the Lagrange multiplier vectorλkand penalty parameter vectorσkare given andxˆkis the global minimum of sub-problem (5), from the first optimality condition of the original problem (1) and the sub-problem (5), we update the Lagrange multiplier vector as follows:(6)λjk+1={λjk−σjkgj(xˆk)i=1,2,…,pmax{λjk−σjkgj(xˆk),0}i=p+1,…,m.The initial values of these parameters are set to any arbitrary positive values, where typicallyσ0=σ0(1,1,…,1)Tandσ0=10orσ0=100. The updating scheme is(7)σk+1=γσkwhereγ>1and typicallyγ=10orγ=100. Instead of increasing the values of the components of the penalty parameter vector in every iteration, they may be increased only if no sufficient progress is made towards feasibility of the original problem (1) from the previous iteration to the current one. The schemes available to the user are as follows.Scheme 1γ=1if‖g¯(xk+1)‖2≤ζ‖g¯(xk)‖2, otherwiseγ>1, where(8)‖g¯(x)‖2=∑i=1p(gj(x→))2+∑i=p+1m(min{gj(x→),0})2is the feasibility norm and0<ζ<1and typicallyζ=0.25.Scheme 2(9)σjk+1={σjk,if{|gj(xk+1)|≤ζ|gj(xk)|forj∈{1,…,p}|min{gj(xk+1),0}|≤ζ|min{gj(xk),0}|forj∈{p+1,…,m}max{γσjk,k2},otherwisewhereγ>1and typicallyγ=10orγ=100.It is also noted that, in the above schemes, when the current feasibility norm is less than the user-required toleranceε, it is useful to restrain increase ofσfor insignificant values of the norm, which will not be the one determining the dissatisfaction of the convergence criteria in this case. In MAL-DE, the user can couple with a strict upper boundσuon the values of the penalty parameters, which is a fail-safe mechanism for not driving them to an unrealistically large value.Noting that the iterative sequence{xk}generated by the algorithm MAL-DE satisfies the bound constraints, i.e.,l≤xk≤u, we can define the feasibility norm by (8). For user-specified toleranceε, the termination criterion is as follows.If‖g¯(xk+1)‖2≤ε, thenxk+1is an approximate global solution to problem (1), wherexk+1is the global minimum obtained from thekth bound constrained minimization sub-problem (5) via the differential evolution algorithm.The above termination criterion is in some cases coupled with a maximum iteration numberKmof the outer stage. If the feasibility norm is larger than the user-specified tolerance in allKmiterations, the global minimum obtained from the last bound constrained minimization sub-problem (5) will be taken as the approximate global solution to problem (1).To obtain the global minimum for thekth bound constrained minimization sub-problem (5), one can employ many solvers based on genetic algorithm  [23], particle swarm optimization  [24] or differential evolution  [25,26]. Here we choose the modified differential evolution algorithm for the global search in (5). The objective function in (5), i.e., the modified augmented Lagrangian functionP(x,λk,σk)will be taken as the fitness evaluation, and the search space is defined by the lower and upper bounds of the variablesli≤xi≤ui.During the past decade, the characteristics of the trial vector generation strategies of DE have been extensively investigated, and some prior knowledge has been obtained. Such prior knowledge could be used for designing more effective and robust DE variants. In addition, the DE algorithm employing different trial vector generation strategies usually performs differently during different stages of evolution  [27]. Some strategies have better exploration capability, while others favor exploitation. Hence, adaptively selecting a suitable strategy for a certain evolution stage according to the current experience can improve the DE algorithm’s performance.The following trial vector generation strategies are selected to be used in the DE literature:(10)DE/rand/1/bin:ui,j,G={xr1,j,G+F⋅(xr2,j,G−xr3,j,G),ifrand≤CRorj=jrandxi,j,Gotherwise(11)DE/best/1/bin:ui,j,G={xbest,j,G+F⋅(xr1,j,G−xr2,j,G),ifrand≤CRorj=jrandxi,j,Gotherwise(12)DE/current-to-rand/1:u→i,G=x→i,G+rand⋅(x→r1,G−x→i,G)+F⋅(x→r2,G−x→r3,G)wherer1,r2,r3∈{1,2,…,NP(population_size)}are randomly chosen inters, which are different from each other and also different from the running indexi.xbestrepresents the best individual in the current generation.F(>0)is a scaling factor which controls the amplification of the differential vector.randdenotes a uniformly distributed random number between 0 and 1.jrandis a randomly chosen index from{1,2,…,n},Gis the current iteration number, and0≤CR≤1determines the similarity of the offspring with respect to the mutation vector.The above trial vector generation strategies are frequently used in many DE literatures and their properties have been well studied. The “DE/rand/1/bin” strategies are the most commonly used strategies in the literature. In these strategies, all vectors for mutation are selected from the population at random and, consequently, it has no bias to any special search directions and chooses new search directions in a random manner. As a result, they usually demonstrate slow convergence speed with superior exploration capability. “DE/best/1/bin” strategies rely on the best solution found so far. They usually have a faster convergence speed and perform well when solving uni-modal problems. However, they are more likely to get stuck at a local optimum and thereby lead to a premature convergence when solving multimodal problems. “DE/current-to-rand/1” is a rotation-invariant strategy. Its effectiveness has been verified when it was applied to solve multi-objective optimization problems  [22].In general, we expect that the chosen DE trial vector generation strategies show distinct advantages and, therefore, they can be effectively combined to solve different kinds of problems. Unlike the traditional DE algorithm where only one trial vector generation strategy is used to generate the offspring of each vector in the population, the three selected trial vector generation strategies (see Eqs. (10)–(12)) compete to get more vectors to reproduce in this paper. The initial populationP(0)ofNPvectors is divided in three sub-populations (P1(0),P2(0), andP3(0)) of equal size. Each sub-population is assigned to each one of the three DE trial vector generation strategies. Each DE trial vector generation strategy then generates the offspring for each vector in its sub-population. The basic structure of the modified differential evolution algorithm is described in Fig. 3.At first, 13 well-known constrained benchmark test functions mentioned in Runarsson and Yao  [28] are optimized to inspect the performance of the proposed MAL-DE algorithm. The characteristics of these test functions are shown in Table 1, and their expressions are provided in Appendix A. From Table 1, these test problems include various types (linear, nonlinear and quadratic) of objective functions with different number of decision variables(n)and a range of types (linear inequalities (LI), nonlinear equalities (NE), nonlinear inequalities (NI), and linear equalities (LE) and number of constraints). The feasibility ratioρis the ratio between the size of the feasible search space and that of the entire search space, i.e.,(13)ρ=|Ω|/|S|where|S|is the number of solutions randomly generated fromSand|Ω|is the number of feasible solution out of these|S|solutions. In our experiment setup,|S|=1000,000.Note that test functions g02, g03, g08, and g12 are maximization problems, and the others are minimization problems. In this study, the maximization problems are transformed into minimization using−f(x→). In addition, only test functions g03, g05, g11, and g13 contain equality constraints.The following parameters are established experimentally for the best performance of MAL-DE: the population size was set to 100 and the number of cycles to 3000 (120,000 evaluations were carried out per independent run); the scaling factorCR=0.7; the crossover rateF=0.9. The individuals are randomly initialized within the boundaries for each run according to a uniform probability distribution. The maximum numbers of generations, i.e., the maximum iteration number of the outer stage, was set to 30, the user-required toleranceε=1e−08, the initial Lagrange multiplier vectorλ0=(1,1,…,1), the initial penalty parameter vectorσ0=(10,10,…,10), the maximum allowed penalty parameterσu=1e10, the penalty parameter increasing factorγ=10, and the reduction factor for feasibility normζ=0.25.

@&#CONCLUSIONS@&#
This paper has presented a hybrid approach coupling modified augmented Lagrangian multiplier method and modified differential evolution algorithm for solving constrained optimization and engineering design optimization problems. The proposed hybrid method employs the differential evolution (DE) to find global optimum in problems with complex design spaces while directly enforcing the feasibility of constraints using a modified augmented Lagrangian multiplier method. The proposed algorithm has demonstrated better performance than the other approaches in the literature on solving 13 well-known benchmark constrained optimization problems and four engineering design optimization problems. In the future, we will apply MAL-DE to various problems found in the real world. Meanwhile, we are interested in extending our method so that it can deal with multi-objective optimization problems.