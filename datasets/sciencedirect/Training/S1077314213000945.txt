@&#MAIN-TITLE@&#
Multi-agent stochastic level set method in image segmentation

@&#HIGHLIGHTS@&#
A variational framework based on level set theory is introduced to find the global solution for image segmentation problem.A single and a multi-agent stochastic structure are studied in the aforementioned framework.Proof of convergence for single-agent model (active contour with stochastic fronts) is also provided in this article.Comparing with Chan–Vese model, higher chance to find the global solution is achieved.

@&#KEYPHRASES@&#
Image segmentation,Multi-agent,Stochastic level set,Global solution,

@&#ABSTRACT@&#
A stochastic structure for single and multi-agent level set method is investigated in this article in an attempt to overcome local optima problems in image segmentation. Like other global optimization methods that take advantage of random operators and multi-individual search algorithms, the best agent in this proposed algorithm plays the role of leader in order to enable the algorithm to find the global solution. To accomplish this, the procedure employs a set of stochastic partial differential equations (SPDE), each one of which evolves based on its own stochastic dynamics. The agents are then compelled to simultaneously converge to the best available topology. Moreover, the stochastic dynamics of each agent extends the stochastic level set approach by using a multi source structure. Each source is a delta function centered on a point of evolving front. Lastly, while the computational costs of these methods are higher than the region-based level set method, the probability of finding the global solution is significantly increased.

@&#INTRODUCTION@&#
Finding a precise, fast, automated and robust approach to segment each image into meaningful sections is crucial in image processing [1]. There are many classical [2] and advanced methods [3] currently available in image segmentation, including thresholding, clustering, histogram-based, region-growing, graph partitioning. However, among these, only a few can be applied in practical and non-ideal images. Several review papers on image segmentations [3–5] verify that there is no general computational method to cope with all practical images. Therefore, for each class of images, a method should be selected and tuned in order to achieve satisfactory results. Deformable models or active contours are relatively new approaches in image segmentation. As their names imply, active contours provide an explicit representation of the boundary and shape of objects. They have several desirable properties when compared to the aforementioned methods such as inherent connectivity and smoothness, and being capable of adding knowledge domain about the object of interest [6]. The level set method (LSM) [7–9] is one of the most important algorithms in deformable models and has been widely employed in many applications. It has several advantages over other active contour methods. These advantages include: (1) no need for parameterization of the contour; (2) flexibility in topology definition; (3) numerical stability; and (4) easiness of extension to higher dimensions [6,10,11]. Essentially, this method is an extension of steepest descent to the variational framework, and therefore this algorithm inherits the problem of getting trapped in local optima. As a result, the final solution is very sensitive to the initial guess [12].This problem has been relentlessly investigated in recent researches. The aforementioned approaches can be grouped into two categories. The methods in the first category try to reformulate the objective functions used in image segmentation in order to improve the performance of the algorithm. Conversely, the methods in the second division focus on the optimization procedure and the evolution process in the level set method. Kichenassamy et al. [13] and Caselles et al. [14] proposed geodesic active contours as an energy-based algorithm for image segmentation, with intention to improve the capability of non-convex feature detection. Additionally, by this extension, the researchers improved the speed of convergence as well. An interesting review based on geodesic active contours can be found in [15], which evaluates the performance of this method on the convexification of the cost functions in image segmentation. Finding a pure convex function which is appropriate for segmentation is obviously not possible. As such, instead of evolving based on local edge information [16] (as well as other snake approaches [17]), the use of region-based cost functions [18] that have less local optima and consequently, segmentation procedure, would be far less sensitive to the initial guess. Region-based active contours were originated by Mumford–Shah [18]. In this research, they proposed a new segmentation framework in which the minimization of the functional in Eq. (1) was employed in order to segment the image, I, by using a set of contours, ∂Ω[6,18].(1)E(∂Ω,f)=α∫Ω(f-I)2dx+β∫Ω⧹∂Ω|∇f|dx+Hn-1(∂Ω)where Hn−1(∂Ω) is the (N−1) dimensional Hausdorff measure andα,β∈R+are positive real coefficients, f is a smooth approximation of image I and Ω is the region surrounded by ∂Ω.The main drawback of this method is its high sensitivity to in-homogeneities caused by noise and artifacts in practical images. In particular, moving organs in medical images exaggerate these in-homogeneities.Using this framework, Chan and Vese created a new vision of “Active Contour without Edge” (ACWE) in a series of oft-cited papers [19–23]. For them, object detection is not necessarily performed by the gradient at its edges. Rather, the basic idea is based on the minimal partition problem in which the stopping criteria does not depend on the gradient of the image. The problem is not convex yet has less local optima. The level set method, which is a gradient-based algorithm, usually finds the global (or the best possible solution) topology that defines an accurate segmentation. The subsequent paragraphs provide a brief review of the second approach to improving the performance of the level set method.There are some other models that sometimes demonstrate better responses than the Chan–Vese model such as the research by Bresson [24], this model has a different local optimal than ACWE and usually performs better than ACWE in terms of accuracy of segmentation, particularly when the images suffer from low contrast features. The basic idea behind it is to add an edge-based indicator into the region-based model. Obviously, since this method is founded on gradient flow, the algorithm may be stuck in local solutions. However, local solutions are usually more accurate than the solutions of ACWE in terms of indicating low contrast edges. Therefore, global solution is a better solution compared to ACWE. The original definition of global solution is somewhat different from how it is used in this paper. The current paper is looking for an active contour approach that does not get trapped in parts of the image and is capable of exploring the whole image.The idea of stochastic flow, as proposed by Welsh [25], is based on stochastic partial differential equations. This concept has been employed in the modeling of physical phenomena and even computer vision. Yip [26] has performed a literature review on stochastic motion by mean curvature that shows the recent trends in this field. One of the most interesting set of works on the viscosity solution of a stochastic partial differential equation was done by Lions and Souganidis in a series of articles [27,28] that are very helpful for numerical implementation of the stochastic level set method.A simple stochastic curve evolution has been implemented by Juan et al. [12] in a level set framework. By using a Stratonovich differential, the well-posedness of the evolution is achieved. In addition, the final solution almost becomes independent of the initial curve. The stochastic active contour proposed in this reference is able to deal with the local optima problem and even with a complicated circumstance wherein the derivation of the exact gradient is not possible. The only concern is that the general spatial-dependent formulation derived in this article is generally not convergent to the deterministic model.A very comprehensive extension to this work has been done in [29], which investigates several aspects of the simulation of stochastic partial differential equations. In addition to the extension of previous work by Juan et al. [12], a set of definitions for numerical stability of stochastic partial differential equation (SPDE) has been suggested.Law et al. [1] investigated the global minimization of the multi-phase piecewise constant Mumford–Shah model [18]. To do so, a hybrid method containing the gradient-based and stochastic method was proposed to deal with the dependency of the solution on initial guesses. The algorithm seems to be very successful compared to other stochastic algorithms such as simulated annealing.Chen and Radke [30] extended the conventional random walk Metropolis–Hastings method to high-dimensional curves; while also proving the asymptotic convergence of the Markov chain. To accomplish this, the authors used an implicit representation of shapes instead of conventional explicit curve parameterization. They claim that their algorithm can be easily applied to 3D frameworks.Pan and others in [31], proposed a new formulation for active contours, in which the conventional state estimation is employed for evolving the curves. As a result, a hybrid algorithm based on the level set method and particle filters has been created. Results show that this new algorithm works well for complex images. An interesting comparison has been made in [1] about various level set based methods. Based on this article, the pure stochastic level set algorithm has the best performance in most cases in terms of finding the most global solution. In comparison to other algorithms like Gradient Descent, Simulated Annealing, or a hybrid method called Multi-resolution Stochastic Level Set Method, the only drawback associated with the stochastic level set method is that it has no systematic procedure to control the random terms. As a result, this algorithm has to even search the solutions that significantly increase the cost function, thus slowing down the procedure. On the other hand, it is worth mentioning that although all of these algorithms employ stochastic operators in order to escape from local optima, finding the global solution is not yet guaranteed.Unal et al. [32] presented some modifications to the active contour method to preserve certain features in images for noise removing purpose. These modifications consist of a tangential diffusion movement of the particles along the contour. This movement improves the original geometric approach to detect the sharp ends of the objects in the image without destroying the larger scale features.Developing a special type of stochastic crystalline algorithm, Arous et al. [33] proposed a random particle system that evolves on a discretized unit circle. It has been shown that the stochastic evolution is convergent toward the deterministic baseline model. The baseline model for this article is the curve-shortening flows approach.The problem of a global optimum point is one of the most challenging open problems in mathematics. Based on the existing algorithms in optimization theory, there are two main approaches to attacking the problem: using random operators and employing multi-individual (agents) based algorithms.The first approach helps the procedure escape from the local optima it is trapped in, while the second one assists in the exploring of all feasible regions for a solution. Obviously, the more individuals (agents) are involved in the hunt, the higher the probability of finding the global optimum point. This point has been proven in all of the multi-individual (agent) based algorithms such as the GA and Particle Swarm optimization method (PSO), including multi-agent gradient-based methods [35].Essentially, along with the aforementioned operators, an algorithm for convergence is also required. There is always a tendency for researchers to prove the convergence of their algorithm mathematically. In this regard, GA has a drawback in that there is no mathematical proof for its convergence. Even for PSO, the mathematical proofs cope with some simplified models. In level set, there is very strong mathematical proof, even in stochastic framework [27,28]. As long as some conditions are satisfied, the convergence of the stochastic level set method can be proven. Thus, for any further extension, one should ensure that the above-mentioned conditions are not violated.In this article an extension to the stochastic level set method [12] is proposed in which a multi-agent-based random structure for Wiener random operator has been investigated. The main idea in escaping from the local edges is that the procedure starts with several agents (here, stochastic level set equations) that help the algorithms to explore the entire image. Each agent tries to segment the image based on its own stochastic algorithm. In addition, all of the agents are compelled to converge to the same topology.Although the dynamic of each agent can be considered as a special case of the general spatial-dependent formulation in [12], the convergence of the stochastic equation is also proven in the present article. According to [37], the general equations derived in [27,28,12] are not necessarily convergent towards the solution of the deterministic part; the current extension in this article is. Obviously, the deterministic model is still very crucial for segmentation since the segmentation is defined by deterministic models and stochastic terms only help to find the best possible solution, so called global solution.This article is organized as follows: Section 2 provides a brief background on edge-based, region-based formulation and stochastic level set method. Section 3 introduces the new algorithms and discusses their advantages with respect to other optimization methods. Section 4 investigates the results, and Section 5 provides a comprehensive conclusion.The following section will provide a brief overview of edge-based level set method [7–9], re-formulation of LSM for Active Contours without edge [21–23], and stochastic level set method [12,1]. One way to address the segmentation problem is to find curve(s) or surface(s) that surround the region of interest in an image. Essentially, this is the primary idea behind active contours methods. However, in these algorithms, the curve(s) or surface(s) can be directly defined using control points, which can cause some issues in terms of the control points updating, merging, and splitting closed curve(s) or surface(s). Nevertheless, there are remedies for these issues, and level set method can easily cope with them. The concept behind the level set method is to embed the aforementioned curves in a surface, such that the evolution of the curve surrounding the objects in an image can be implicitly represented by the evolution of the surface. Fig. 1a and b shows how a surface embedding a curve can be morphed in order to achieve a desired topology.Implicit description of curves and surfaces is the main concept of the level set method. This capability allows the level set method to be used as a multi-dimensional tool to represent any complex topology. Changes in topology start from a zero level set or isophote as an initial guess, which continuously evolves through a Hamilton–Jacobi PDE formulation based on a certain cost function [7–9,18,19].Let’s assume there is an implicit function, ϕ(x, t), in a given design domain, Ω, which satisfies(2)ϕ(x,t)>0,x∈Ω+Exteriorϕ(x,t)<0,x∈Ω-Interiorϕ(x,t)=0,x∈∂ΩBoundariesand also two other well-known Heaviside, H(s), and Dirac delta, δ(s), functions:(3)H(s)=1s⩾00s<0(4)δ(s)=0s≠0∞s=0The level set function is founded on the basis of the Hamilton–Jacobi equation [34]:(5)ϕt+Vn|∇ϕ|=0The so-called Vnis known as a normal velocity of the evolving boundaries that control the speed evolution of the fronts, and ∣∇ϕ∣ is the amplitude of the gradient vector. Solving this partial differential equation starting from an initial guess transfers the segmentation problem to an initial value problem in PDE framework. A popular formulation [34] for Vnin Eq. (5) is as follows:(6)∇IG=IxG,IyG(7)Vn=exp(-α|∇IG|)where IGis the filtered image by a Gaussian filter G, α is the convergence rate VnandIxGandIyGare the partial derivatives of Image, I, with respect to x and y respectively. As can be seen, Vndepends on a local gradient, which can lead to failure in inaccurate segmentation.In the region-based LSM formulation, an energy functional contains the fitting energy and the energy associated with the length and the area of the initial contour∂Ω,∂Ω⊂Ω,ϕ:Ω⟶R. This functional energy can be represented as follows [19–23]:(8)E(∂Ω)=Ff+μL∂Ωp+υAΩ-where ∂Ω is the boundary of Ω, L∂Ωis the length of∂Ω,p=NN-1,Nis the dimension ofRN, andAΩ-is the area inside the ∂Ω.The LSM as an optimization procedure will minimize the functional in Eq. (8) in order to segment the image. The fitting energy is defined as [19–23]:(9)Ff=F1(∂Ω)+F2(∂Ω)=∫Ω-|I-c1|2dΩ+∫Ω+|I-c2|2dΩin which the c1 and c2 are the average of image I inside and outside of the ∂Ω, respectively.(10)c1(ϕ)=∫ΩI(1-Hε(ϕ))dxdy∫Ω(1-Hε(ϕ))dxdy(11)c2(ϕ)=∫ΩIHε(ϕ)dxdy∫ΩHε(ϕ)dxdyTo see how this formulation works, let us look at the following four options in Fig. 2.As can be seen, the fitting function is always positive except in a case where the evolving curve completely fits the objects in the image, and functional Ffis at its global minimum. Therefore, it can be claimed that minimization of the cost function leads to segmentation of the image. It is known that the steady state solution of Hamilton–Jacobi PDE, Eq. (1), is the optimum solution of the segmentation problem [7–9]. The Hamilton–Jacobi PDE can be re-written in Eq. (12)[19–23]:(12)∂ϕ∂t=δε(ϕ)μp∫Ωδε(ϕ)|∇ϕ|dΩp-1div∇ϕ|∇ϕ|+-υ-λ1(I-c1)2+λ2(I-c2)2where the following conditions are held:(13)p∫Ωδε(ϕ)|∇ϕ|p-1δε(ϕ)|∇ϕ|∂ϕ∂n=0on∂Ωϕ(t,x,y)=ϕ0(x,y)inΩin whichυ,μ,λ1,λ2∈R+are fixed parameters,n,∂ϕ∂ndenotes the exterior vector normal to the boundary and the normal derivative of ϕrespectively, denotes boundary. δε, Hεare approximation for Dirac delta and Heaviside function as follows:(14)δε(x)=1πεε2+x2(15)Hε(x)=121+2πarctanxεwhen ε is a tiny positive parameter and when ε→0, these functions approach H(ϕ(x)), δ(ϕ (x)).Stochastic differential equations (SDE) have been widely used in branches of studies such as biology, economics, finance, chemistry, MEMS and NEMS applications. Advanced probability and stochastic processes are the main prerequisites for complete understanding of SDE but this article will provide a very brief review on concepts. For more details, interested readers are encouraged to refer to more extensive sources such as [39,41,42]. Starting with the idea of Brownian motion, stochastic integration will be discussed as a key to solving the SDE. All the operators are defined in a standard probability space(ω,F,Ft,P). For definitions of standard probability space and other introductory concepts, please refer to [39,41,42].A common representation of stochastic differential equation is shown as [43]:(16)dXt=G(Xt,t)dt+H(Xt,t)dWtwhere Xt=Xt(t) is a realization of the stochastic process. G(Xt, t) and H(Xt, t) are called drift and diffusion coefficient respectively. The drift coefficient is the deterministic part of the equation which determines the local trend of the process. Correspondingly the diffusion coefficient is the stochastic part of the coefficient that shows the average fluctuation of Xt. Assuming that the stochastic part obeys the necessary conditions for the Wiener process, one may be interested to solve the stochastic Eq. (16) in order to find at least one realization, Xt. To find the realization, one should integrate Eq. (16) as [43]:(17)Xt=Xt0+∫t0tG(Xs,s)ds+∫t0tH(Xs,s)dWsThe first integral in Eq. (17) is an ordinary Riemann integral. For the second integral, it should be noted that the Brownian processes are not differentiable. To cope with this situation, Ito, a Japanese mathematician, proposed the Ito Stochastic Integral in 1940, and 20years after that, a Russian physicist named R.L. Stratonovich suggested another stochastic integral to solve Eq. (16). To distinct between Ito and Stratonovich integrals, Stratonovich integral is usually shown by a specific operator “∘” [12].(18)dXt=G(Xt,t)dt+H(Xt,t)∘dWtand correspondingly the integral form can be represented as:(19)Xt=Xt0+∫t0tG(Xs,s)ds+∫t0tH(Xs,s)∘dWsIn general, the second integral can be reformulated as:(20)∫t0tH(Xs,s)dWs=limh→0∑k=0m-1H(Xτk,τk)(W(tk+1)-W(tk))in this equation h=(tk+1−tk) with intermediary points τk=(1−λ)tk−λtk+1, ∀ k∈{0,1,…,m−1}, λ∈[0,1]. Depending on how the parameters of λ are defined, different integrals are introduced. The instance of λ=0 results in τk=tk, meaning that the integral is evaluated at the start point of each interval. This method is called the Ito integral. Additionally, λ=1/2 gives τk=(tk+1−tk)/2, which corresponds to the Stratonovich integral [43].The Ito integral is usually employed in financial mathematics wherein only the information about the past is taken into account for modeling. In contrast, Stratonovich is very common in physical science modeling. Based on the aforementioned formulation, the Ito integral can be written as follows:(21)∫0TH(s)dWs=limh→0∑k=0m-1H(tk)(W(tk+1)-W(tk))Similarly, in the case of Stratonovich, the next two integrals can be employed. The equality of these two integrals is proven [12].(22)∫0TH(s)∘dWs=limh→0∑k=0m-1Htk+tk+12(W(tk+1)-W(tk))(23)∫0TH(s)∘dWs=limh→0∑k=0m-1H(tk)+H(tk+1)2(W(tk+1)-W(tk))According to Eq. (16), the modeling of stochastic processes is generally, based on a deterministic term (drift) and a stochastic term (diffusion). In stochastic flow modeling or curve evolution, the deterministic term can be shown as G=G(D2ϕ, Dϕ, x, t) in which ϕ is an implicit representation of the curve or surface, D is the gradient operator and D2 denotes the Hessian matrix. The general form of a fully nonlinear stochastic partial differential equation inRN×(0,∞)is [27,28] is:(24)dϕ=G(D2ϕ,Dϕ,x,t)dt+∑i=1mHi(Dϕ,x,t)dWi(x,t)In a special case, stochastic model for curve evolution has been proposed as follows [12]:(25)dϕ=G(D2ϕ,Dϕ,x,t)dt+|Dϕ|dW(t)One may notice that in Eq. (25), the Ito operator has been chosen implicitly. Obviously, Eq. (25) in Stratonovich sense can be shown as:(26)dϕ=G(D2ϕ,Dϕ,x,t)dt+|Dϕ|∘dW(t)As can be seen in Eqs. (25), (26), all points have an extra random force (second term) that is not explicitly a function of position because it only depends on ∣Dϕ∣ and Brownian process, which is a function of time. A more flexible structure can be achieved by replacing the Brownian motion, W(t), by a colored spatial noise.(27)W(x,t)=∑i=1mψi(x)Wi(t)whereψi:RN⟶Rare smooth functions with compact support. Thus, the final stochastic evolution model is as follows [12]:(28)dϕ=G(D2ϕ,Dϕ,x,t)dt+|Dϕ|∑i=1mψi(x)∘dWi(t)In order to simplify the model, one can use the same type of function ψ at different centers. For example, ψi(x)=ψ(x−xi), where ψ can be a convenient regular function.A rigorous mathematical convergence analysis has been conducted by Lion and Souganidis [27,28] in which the authors have suggested the notion of stochastic viscosity solution for fully nonlinear, second order, possibly degenerate stochastic partial differential equations. The main result in [27] is presented in Appendix B.As can be seen, the theorem can only guarantee the convergence of Eq. (28) when ψi=1 or basically when the noise is not spatial-dependent [12]. The general case of this theorem is still an open problem in stochastic partial differential equation theory. Recently however, more general cases have been studied and luckily, these new studies can support our modified version of the stochastic level set method presented in Section 3.1.1. These theorems were not available when Juan et al. [12] published their paper. The newer theorems will be discussed in detail in Section 3.1.1.As stated earlier, in contrast to the Ito integral, Stratonovich formulation does not have an explicit numerical scheme because it does not only depend on left value in each time interval. For more clarification, if one assumes the simple evolution dϕ=∣Dϕ∣∘dW(t), the corresponding method for its implicit update scheme would be:(29)ϕi+1=ϕi+12(|Dϕi|+|Dϕi+1|)ΔWiTo avoid encountering implicit update formulation such as Eq. (29), let’s consider the following equation in Stratonovich sense:(30)dϕ=H(Dϕ,x)∘dW(t)A typical example for H(p, x), a real valued function inRN×R, is H(p, x)=∣p∣ψ(x). Here ψ is any regular smooth function. The expanded form of Eq. (30) in Stratonovich sense [12] is:(31)dϕ=H(Dϕ,x)dW(t)+12d〈H(Dϕ,x),W〉(t)This expansion shows that Stratonovich term is actually Ito term plus an additional term called drift.The drift term, second term, can be obtained as follows [12]:(32)12〈H(Dϕ,x),W(t)〉(t)=12∫0t[DpH·(D2ϕDpH)+DpH·DxH]dsAs it is assumed that H=∣p∣ψ(x), the drift can be expanded to:(33)〈H(Dϕ,x),W(t)〉(t)=∫0tψ2(t)Dϕ|Dϕ|·D2ϕ(x,s)Dϕ|Dϕ|+ψ(x)Dψ(x)·Dϕ(x,s)dsThe second order term in Eq. (33) can be simplified to [12]:(34)Dϕ|Dϕ|·D2ϕ(x,s)Dϕ|Dϕ|=Δϕ-|Dϕ|divDϕ|Dϕ|=Δϕ-|Dϕ|κwhere “div” is divergence operator and κ is the mean curvature of level set. Eq. (33) is obtained based on this assumption that “m”, the number of Brownian motions, is 1. For a more general case in which the stochastic evolution PDE is of type:(35)dϕ=G(D2ϕ,Dϕ,x,t)dt+|Dϕ|∑i=1mψi(x)∘dWi(t)the expanded version of Eq. (35) can be represented as follows [12]:(36)dϕ=G(D2ϕ,Dϕ,x,t)dt+|Dϕ|∑i=1mψi(x)dWi(t)+12∑i=1mψi2(x)(Δϕ-|Dϕ|κ)+∑i=1mψi(x)Dψi(x)·DϕdtOne way to improve the chance of finding the global solution with an iterational minimization such as stochastic flows, is to only accept iterations that decrease the objective function. While it is good to escape from local optima, doing so may result in longer convergence time. One intermediate option is to accept iterations with higher objective functions by a slight chance that is exponentially related to how far the value of the current objective function is from the previous one. If the objective function is slightly higher than the previous iteration, it is more likely accepted than cases in which a big jump occurs and objective function increases dramatically. The combination of stochastic flows and annealing scheme is summarized in Table 1. It should be noted that, just by employing the annealing scheme, there is still a chance of being stuck in a local optima when the process cannot find a better solution. Generally speaking, using delta function as weighting function in ACSF helps the active contour to have an independent source of noise at each point of the front, such that each point can find a better location at each iteration if possible.In this scheme, Temp(i) is the annealing temperature, which is usually a monotically decreasing function of time (iteration). Here, this function is defined as:(37)Temp(i)=Temp0iwhere i is the iteration number and i=1⋯NItrin which NItris the maximum number of iterations.One of the most important applications of stochastic partial differential equations is in image processing and segmentation [12]. By employing the stochastic level set method, fronts should be able to escape from local optima. However, the simulation shows that with small number of noise sources, it only works to capture the outer boundary of the features in the image. To improve the capability of the algorithm in bypassing the local edges, each point on the evolving curve should have an independent random jump. This independent jump is useful because during the process, each point on the front can move to a better position independently if possible (possibility will be determined by annealing scheme). Dependent source noise may create a situation in which only some points have this opportunity to select a better position, while other points on the front are forced to move to positions that are not necessarily better than before. This happens even if the overall objective function is finding its way toward a better value. By local edge, we are referring to situations where the evolving fronts are trapped at some locations and cannot evolve further. Additionally real edges are the edges that are detected by the algorithm and actually exist in the real image. It should be noted that, it is not a firm condition to escape from local solutions because the local edges are usually unstable (fake), the algorithm might be able to bypass the local edges by perturbing the edges,. In other cases, like Fig. 6d, although most of the detected edges are real, the algorithm fails to explore the entire image to detect all of the edges because it cannot evolve anymore.One may argue that the final solution of ACWE can be improved in edge detection capability, by tuning the parameters in deterministic Eq. (8) or (12). This is especially true for μ, the coefficient of the length of evolving front. For example, the smaller the value of μ, the more details captured in the image. Nevertheless, in some cases like Fig. 6d, even if μ is set to zero, ACWE cannot detect entire strips on zebras’ bodies.One way to perform this idea is to use the delta function as spatial weighting functions in Eq. (36). This way, each point over the front has an independent random jump. As the finite difference algorithm has been employed for numerical simulations, a delta function is assigned to every node on evolving contour. By substituting delta function and its derivative into Eq. (36), it can be re-written as Eq. (38) as long as ε is small enough.(38)dϕ=G(D2ϕ,Dϕ,x,t)dt+|Dϕ|∑l=1mδε,l(x)dWl(t)+12∑l=1mδε,l2(x)Δϕ-|Dϕ|κdt+12∑l=1mδε,l(x)Dδε,l(x)·DϕdtHere G(D2ϕ, Dϕ, x, t) is the deterministic model. This article uses the Chan–Vese model, Eq. (12), although any other model can be selected as well. Furthermore, m is the number of nodes on evolving contour that, by definition of ACSF, is equal to the number of the delta functions. δε,l(x) is the Dirac delta function centered at lth node of the finite difference grid, l=1⋯m. An approximation for Dirac delta function with width of ε is represented in Eq. (39).(39)δε,l(x)=1πεε2+(x-xl)2Assuming ε≪Lewhere Leis the minimum grid size, δε,l(x) is almost zero in all nodes but lth node. Keeping this point in mind, Eq. (38). The original Eq. (28), can be rewritten in the following form:(40)dϕ=G(D2ϕ,Dϕ,x,t)dt+Dϕ(x,t)·ϒ(x,ε)∘dW(t)where ϒ=[υil] is a matrix of m×m in which the entities are defined as:(41)υil=δε,l(xi),∀i,l=1⋯mOne issue with this algorithm is that the edges continue wobbling for a relatively long time before the process reaches the steady state. As such, therefore an edge solidification expression should be added to the equation. An exponential term is employed here for this purpose. The modified equation is as follows:(42)dϕ=G(D2ϕ,Dϕ,x,t)dt+e-tτDϕ(x,t)·ϒ(x,ε)∘dW(t)in which1τis the solidification rate and should be determined by trial and error to achieve better performance.In the theorem proposed by Lion and Souganidis [27,28], the noise source has no spatial dependency. The general formulation in Eq. (36) is fully dependent on spatial terms represented by ψifunctions. Basically, Theorem 4 cannot support any formulation with spatial-dependent noise. Eq. (42) obtains a simpler form which can luckily be supported by a recent theorem proposed by Caruana et al. in [37]. In spite of what is expected, it has been shown that in the presence of spatial term, the stochastic partial differential equation would not necessarily converge to the solution of the deterministic part. Although the form employed by [37] is not the general form, it can confirm that Theorem 4 is not valid for a general case where the spatial terms exist. To discuss the new theorem and its application in ACSF, some background information is required.Given C∞-vector fieldsHi=∑j=1nHji(x)∂∂xj, 1⩽i⩽q onRn. Lets denote L(H1,…,Hq), the Lie sub-algebra ofI(Rn), expanded by {H1,…,Hq}. HereI(Rn)is the Lie algebra of all C∞-vector fields onRn, in which the bracket product operator is defined as [45]:(43)[X,Y]=X⊗Y-Y⊗X,X,Y∈I(Rn)This operator can be expanded as:(44)[X,Y]i=∑k=1nXkdYidxk-YkdXidxkwherei∈{1,…,q},q<nEq. (44) shows the ith entity of the vector field [X, Y].A Lie algebra L is called Nilponent of step p if pth term of the series in Eq. (45) vanishes [45].(45)[L,L]⊃[L,[L,L]]⊃[L,[L,[L,L]]]⊃⋯in which(46)∀A,B⊂L:[A,B]=∑i=1k[ai,bi];ai∈A,bi∈B,i=1,…,k,k=1,2,…In a paper by Caruana et al. [37] which was preceded by a set of papers by the last two authors [38,?], studied nonlinear parabolic evolution of the form ∂tϕ=G(D2ϕ, Dϕ, x, t) subject to noise of form H(Dϕ, x)∘dB for linear H, with respect to Dϕ. As the authors stated this set of research is motivated by the work done by Lions and Souganidis [27,28] also mentioned in Theorem 4. To deal with the recent case, they proposed the use of rough path analysis suggested by Lyons [44]. The following theorem is the primary result of their work, which is very useful in the convergence analysis of ACSF.Theorem 1Let S=(S1,…,Sm) be a collection of C∞-bounded vector fields onRnand W a m-dimensional standard Brownian motions. Then, for every α=(α1,…,αN)∈{1,…,m}N, N⩾2, there exist (piecewise) smooth approximations (zk) to W, with each zkonly dependent on {W(t): t∈Dk}, where Dkis a sequence of dissection of [0, T] with mesh tending to zero, such that almost surelyzk→Wuniformlyon[0,T]but uk, solution to(47)dϕk=G(D2ϕk,Dϕk,x,t)dt-Dϕk(x,t)·S(x)dzkϕk(0,·)=ϕ0∈BUC(Rn)converges almost surely locally uniformly to the solution of the “wrong” differential equation:(48)dϕ=[G(D2ϕk,Dϕk,x,t)-Dϕ(x,t)·Sα(x)]dt-Dϕ(x,t)·S(x)dWwhere Sαis the bracket-vector field given by:Sα=[Sα1,[Sα2,…,[SαN-1,SαN]]].It can easily be shown that the preceding theorem is also valid when the Stratonovich differential ∘dW is replaced by dz for somez∈C1([0,T],Rm).An outline of the proof for Theorem 1 and Remark 2 is available in [37]. Using this theorem and remark, one is able to verify the convergence of ACSF. The next section is devoted to the convergence analysis of ACSF.A simple comparison between Eq. (40) defining the ACSF evolution equation and Eq. (47) in Theorem 1, shows that the equations have the same structure except that Eq. (40) is written in Stratonovich sense.Considering Theorem 1 and Remark 2, one can conclude that assuming S(x)=−ϒ(x, ε), according to Theorem 1, the SPDE (42) will converge to a “wrong” differential equation such as:(49)dϕ=(G(D2ϕ,Dϕ,x,t)+Dϕ(x,t)ϒα(x,ε))dt+Dϕ(x,t)ϒ(x,ε)∘dW(t)where suppose ϒ=[ϒ1,…,ϒm] thenϒα=[ϒα1,[ϒα2,…,[ϒαm-1,ϒαm]]], α=(α1,…,αm)∈{1,…,m}m, which is basically a bracket-vector field made of Dirac delta functions and Lie bracket operator in Nilpotent fashion.Now we want to show that in ACSF, ϒαis always zero. In other words, the “wrong” differential equation becomes exactly the same as the initial equation, or the stochastic level set equation will converge to the solution of deterministic part, which is what we want.Theorem 3Given the stochastic differential Eq.(40), ACSF evolution equation converges to deterministic solution of Hamilton–Jacobi equation dϕ=G(D2ϕ, Dϕ, x, t)dt.To proceed with the proof, lets focus on ϒα. We claim that the Lie algebra, L, defined over{ϒα1,ϒα2,…,ϒαm-1,ϒαm}is Nilponent of step 1, or simply:(50)[ϒαm-1,ϒαm]=0,∀αi∈{1,…,m}mTo show that this claim is true, lets recall Eqs. (41) and (44) for Dirac delta functions by which the expanded version of Lie bracket is defined.(51)[ϒαm-1,ϒαm]i=∑l=1mυl,αm-1dυi,αmdxl-υl,αmdυi,αm-1dxl=∑l=1mδε,l(xαm-1)dδε,i(xαm)dxl-δε,l(xαm)dδε,i(xαm-1)dxlBy investigation of Eq. (51), one can see that for each particular x, one ofδ(xαm-1)orδ(xαm)is zero. This means that both terms inside the bracket in the last equation are zeros. Thus[ϒαm-1,ϒαm]i, an arbitrary element of[ϒαm-1,ϒαm]is always zero, and therefore the whole bracket in Eq. (50) is zero. It leads to this conclusion that the Lie algebra, L, is Nilponent of step 1. Based on this conclusion, the extra term in the deterministic part of the “wrong” Eq. (48) (second term in the bracket) compared to Eq. (40) is always zero for ACSF evolution. This can confirm that all of the performed simulations based on ACSF differential equations are at least a local solution of the original optimization problem.□As discussed earlier, two approaches have been proposed in order to improve the performance of active contours in image segmentation: active contour without edges [19–23] and stochastic level set method [12,27,28]. The first approach omits the dependency of evolution of front on local image gradient, so it is very successful coping with the images corrupted with noise. However, in some cases the real edges within the image are barriers for evolution of active contour. A good example is the image shown in Fig. 6a. The strips on the zebras’ bodies prevent the contour from completely evolving and extracting the whole bodies. On the other hand, according to Theorem 1 from [37], the general model developed by [12] will converge to a “wrong” equation that is not the deterministic model. This is undesirable, since deterministic models are designed such that the optimization process converges to an accurate segmentation. Essentially, the stochastic algorithms should find the best solution of deterministic models.Active Contour with Stochastic Fronts (ACSF) as a special case of model in [12], is an attempt to achieve all of the previously-mentioned advantages, while mitigating the flaws.It uses both the formulation of region-based and the stochastic level set method. However, a simple but very effective change in order to extract all possible details within the image and outer boundaries. At the same time, the convergence of the resultant equation toward the deterministic solution is also guaranteed. In Section 4, this algorithm has been implemented and evaluated based on its effectiveness compared to the region-based level set method, Tables 2 and 3.In global optimization theory, there are two main approaches to cope with local optima problems: stochastic operators and multi-agent systems. An interesting feature of the work in [12] is the combination of the stochastic operator with the level set method. This innovation creates a global optimization algorithm with the advantages of gradient-based methods. This section intends to use the second approach (multi-agent) while utilizing the advantages from the first approach. In doing so, instead of only one partial differential equation, a set of level set equations are employed. The main idea here is to make sure that the agents are capable of escaping from a local point. Therefore, instead of one H–J equation in Active Contour with Stochastic Front (ACSF), several SPDEs have been employed starting from different initial guesses. This can help the evolving front to escape from local edges when the stochastic term in Eq. (42) is not strong enough for this purpose.Certainly, all of the agents have to converge to the best current topology. Namely, the leader has minimum energy, so the functional in Eq. (9) must be revised in order to make this condition. The revised equation is:(52)Ffj(∂Ωl,∂Ωj)=F1(∂Ωj)+F2(∂Ωj)+λljFcj(∂Ωl,∂Ωj)where:(53)Fcj(∂Ωl,∂Ωj)=∫Ω(H(ϕl)(1-H(ϕj))+H(ϕj)(1-H(ϕl)))dΩThe converging force Fcj(∂Ωl, ∂Ωj) is simply a metric showing the region that is surrounded between the leader, lth agent and the agent jth, l≠j, but does not include the intersection area. To achieve the same topology, the area of this region has to converge to zero. Fig. 3shows the general surrounded area between the two agents, i and j. It is worth mentioning that although Eq. (52) is only written for two agents, (leader and an arbitrary individual), it can be employed for a system with any number of agents because the “converging force” is only applied between the leader and each individual. Obviously, the leader only evolves based on Eq. (12) and the converging force for this agent is equal to zero, Fcl(∂Ωl, ∂Ωl)=0.As stated, the main problem in LSM, and in the stochastic level set method is that when the fronts converge to an edge, there is no guarantee that this edge is a real edge. Even if the edge is real, the segmentation may be stuck over that edge and cannot evolve to segment all of the features in the image. To increase the probability of finding a real edge, the zero level sets should start from different parts of the feasible region; consequently, a multi-agent system can overcome this problem.In level set method, it is very common that one employs an initial level set such that it has enough holes in it. As the level set method is unable to make a new hole, having sufficient holes in the initial level set is crucial. It might seem that a stochastic level set method with multiple holes in initial guess, such as in Fig. 4, is a multi-agent stochastic system by itself, and there is no need for using a complicated system like the one we have proposed in this article. To clarify the difference between an ACSF algorithm initiated with a multiple holes zero level set and the multi-agent structure proposed here, one should notice that every single node on the front in the first case evolves independently. As such optimum solution for this case is similar to a case where one runs a gradient based optimization procedure with different initial guesses, resulting in each agent finding its own local optimum points. However, in the latter case because the agents are forced to finally converge to the same shape, the probability of finding the real edges in the image is higher than the first case. This is similar to the convergence of individuals in multi-agent optimization algorithms such as GA, PSO, and/or SBFO [35], in which each agent starts from different initial guess but they are all supposed to converge to the best possible solution. This approach is more probable to find the global solution than cases where each agent only finds its local solution depending upon the initial condition that it starts with. This point has been demonstrated by an example in Section 4.In addition, using a multi-agent system helps to explore the whole feasible region, which clearly increases the chances of finding the global solution. Although the computational cost of this algorithm is higher than the previous ones, it can be performed in a decentralized fashion, since each agent evolves almost independently based on its own dynamics. Compared to other multi-individual optimization algorithms like GA or PSO, the convergence should be faster because this algorithm is an extension of a gradient-based algorithm called SBFO [35] to the variational framework.In contradiction to GA, PSO, etc., the number of tunning parameters are extremely low, specifically the number of agents and weighting parameter λljin Eq. (52).

@&#CONCLUSIONS@&#
