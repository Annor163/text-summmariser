@&#MAIN-TITLE@&#
Simple change detection from mobile light field cameras

@&#HIGHLIGHTS@&#
We present a framework for solving moving-camera problems with still-camera solutions.Geometry captured by light field cameras is used directly without a 3D scene model.The framework yields a simple, efficient, closed-form solution for change detection.The solution outperforms structure-from-motion methods for commonly occurring scenes.The framework can be generalized to a broad class of computer vision problems.

@&#KEYPHRASES@&#
Change detection,Light field filtering,Plenoptic flow,Light field rendering,

@&#ABSTRACT@&#
Vision tasks are complicated by the nonuniform apparent motion associated with dynamic cameras in complex 3D environments. We present a framework for light field cameras that simplifies dynamic-camera problems, allowing stationary-camera approaches to be applied. No depth estimation or scene modelling is required – apparent motion is disregarded by exploiting the scene geometry implicitly encoded by the light field. We demonstrate the strength of this framework by applying it to change detection from a moving camera, arriving at the surprising and useful result that change detection can be carried out with a closed-form solution. Its constant runtime, low computational requirements, predictable behaviour, and ease of parallel implementation in hardware including FPGA and GPU make this solution desirable in embedded application, e.g. robotics. We show qualitative and quantitative results for imagery captured using two generations of Lytro camera, with the proposed method generally outperforming both naive pixel-based methods and, for a commonly-occurring class of scene, state-of-the-art structure from motion methods. We quantify the tradeoffs between tolerance to camera motion and sensitivity to change, and the impact of coherent, widespread scene changes. Finally, we discuss generalization of the proposed framework beyond change detection, allowing classically still-camera-only methods to be applied in moving-camera scenarios.

@&#INTRODUCTION@&#
Having a static camera simplifies a wide range of important computer vision problems: change/motion detection, object tracking, segmentation, isolation and removal, and a range of spatio-temporal filtering techniques including denoising and velocity filtering [1–4]. If the camera is mobile, however, nonuniform apparent motion complicates these techniques, generally requiring structure from motion approaches which generate explicit 3D models of the scene. These methods are conceptually, computationally and behaviourally much more complex than their still-camera counterparts.We show that light field cameras [5,6] offer a simplification by allowing a virtual, stationary view to be rendered from a dynamic light field sequence. The process, depicted in Fig. 1, does not form an explicit 3D model of the scene. Rather the geometry implicitly encoded in the light field is directly exploited to produce a virtual, stationary camera, effectively reducing moving-camera problems to stationary-camera problems.The proposed framework can simplify a wide range of problems. In this work we demonstrate the simplification of change detection, arriving at the surprising, important and novel result that change detection can be carried out with a single closed-form expression. To our knowledge this is the first published closed-form solution to change detection from a moving camera in a 3D environment.The proposed method outperforms competing single-camera structure from motion approaches for a commonly-occurring class of scene. Because structure from motion jointly estimates camera velocity and scene geometry, changes in the scene can be confused for apparent motion, leading to a significant underestimation of change.In contrast to competing methods, our solution has constant runtime, low computational requirements, predictable behaviour, and is easily implemented in hardware including FPGA or GPU, making it desirable in a range of challenging application domains including robotics.The remainder of this paper is organized as follows: we discuss related work in Section 2 and provide background on the closed-form method of camera motion estimation from plenoptic flow in Section 3. We then describe a linear, additive rendering method based on plenoptic flow in Section 4, and combine the methods to effect change detection in Section 5. Section 6 shows results for imagery captured using two generations of Lytro camera, giving quantitative and qualitative analyses of the method’s performance and limitations, including explorations of the interplay between sensitivity to change and tolerance to camera motion, and sensitivity to widespread scene changes. The paper concludes with discussion and directions for future work in Section 7, including generalization of the proposed framework over an important class of computer vision problems.

@&#CONCLUSIONS@&#
