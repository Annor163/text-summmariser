@&#MAIN-TITLE@&#
Analysis of engagement behavior in children during dyadic interactions using prosodic cues

@&#HIGHLIGHTS@&#
Engagement level of children reflected in their vocalizations as well as the psychologist's.Engagement level reflected in global prosodic cues as well as local prosodic patterns.Engagement level varies with different interaction settings but universal prosodic patterns also exist.

@&#KEYPHRASES@&#
Engagement,Prosody,Global level cues,Local level cues,Classifier decision fusion,

@&#ABSTRACT@&#
Child engagement is defined as the interaction of a child with his/her environment in a contextually appropriate manner. Engagement behavior in children is linked to socio-emotional and cognitive state assessment with enhanced engagement identified with improved skills. A vast majority of studies however rely solely, and often implicitly, on subjective perceptual measures of engagement. Access to automatic quantification could assist researchers/clinicians to objectively interpret engagement with respect to a target behavior or condition, and furthermore inform mechanisms for improving engagement in various settings. In this paper, we present an engagement prediction system based exclusively on vocal cues observed during structured interaction between a child and a psychologist involving several tasks. Specifically, we derive prosodic cues that capture engagement levels across the various tasks. Our experiments suggest that a child's engagement is reflected not only in the vocalizations, but also in the speech of the interacting psychologist. Moreover, we show that prosodic cues are informative of the engagement phenomena not only as characterized over the entire task (i.e., global cues), but also in short term patterns (i.e., local cues). We perform a classification experiment assigning the engagement of a child into three discrete levels achieving an unweighted average recall of 55.8% (chance is 33.3%). While the systems using global cues and local level cues are each statistically significant in predicting engagement, we obtain the best results after fusing these two components. We perform further analysis of the cues at local and global levels to achieve insights linking specific prosodic patterns to the engagement phenomenon. We observe that while the performance of our model varies with task setting and interacting psychologist, there exist universal prosodic patterns reflective of engagement.

@&#INTRODUCTION@&#
During childhood an individual develops critical social, physical, psychological and cognitive skills and abilities. This development is affected by several factors including society (Walker et al., 2007; Davie et al., 1972), family (Biller, 1993; Egeland and Farber, 1984) and peers (Dodge et al., 2003). Furthermore, developmental changes are reflected in behavioral aspects such as joint attention (Akhtar et al., 1991; Tomasello and Farrar, 1986), and ability to engage, among others (Cloward et al., 1960; Göncü, 1999; Morrissey-Kane and Prinz, 1999). Quantitative assessment of these behavioral aspects, while very challenging, can provide tools for understanding important aspects of child development, both typical and atypical. In turn, this can further inform intervention methods targeted toward assisting healthy child development. Several approaches for quantifying a child's behavioral aspects such as social and language development (Volkmar et al., 1993; Coplan and Gleason, 1990) exist and we note that their specific type and nature is very dependent on, and tailored to the corresponding behavior of interest. Given the vast heterogeneity and variability in developmental trajectories, especially in the presence of neuro-cognitive and behavioral disorders, there is an imminent need for quantitative methods and analysis tools that can help shed further light into developmental behavioral processes and mechanisms.Understanding and quantitatively characterizing the engagement patterns of a child, a core behavioral construct, can be useful for both diagnostics and intervention design. Child engagement is defined as the child being involved with his/her environment in a contextually appropriate manner (McWilliam and Casey, 2008). Engagement is a complex internal state externalized and reflected in several modalities including face and body language (Xu et al., 2010; Sanghvi et al., 2011), speech (Yu et al., 2004; Manning et al., 1994) and physiology (Nes et al., 2005). Several studies suggest that a greater engagement has a constructive impact on a child's development (McWilliam et al., 2003; Taylor et al., 2003). For instance, investigations by de Kruif and McWilliam (1999) suggest positive multivariate relationships between developmental age and observed child engagement, where the developmental age is determined over personal–social, adaptive, communication, motor and cognitive domains (Newborg et al., 1984). Göncü (1999) has reported on the impact of a child's engagement during social activities to his/her development and underscores the importance of an interdisciplinary approach to such an endeavor. The importance of engagement is also emphasized in the study of children with developmental disorders like autism (Delano and Snell, 2006; Poulsen and Ziviani, 2004) aiming to enhance behavioral intervention methods (Kasari et al., 2010; Rogers, 2000). Furthermore, improved engagement is associated with success of organizations like child care centers (Maher Ridley et al., 2000) and schools (Skinner and Belmont, 1993). Methods of intervention exist to improve child engagement in different settings such as school (Skinner et al., 1990), parent–child interactions (Casey and McWilliam, 2005) and play with peers (Cielinski et al., 1995).Given that child development and engagement are strongly coupled, several schemes have been proposed to measure engagement. Yatchmenoff (2005) proposes to quantify engagement in child protective services by categorizing it into five dimensions of receptivity, expectancy, investment, mistrust and working relationship. Kishida and Kemp (2006) have proposed measures of engagement specific to practitioners as opposed to researchers motivated by their relation to practicality, sensitivity to the participants and ability to measure across the span of activity types. Libbey (2004) measured engagement of children in schools by defining a few school connectedness measures based on conceptual interrelatedness. Other studies such as in Read et al. (2002) view engagement as a dimension of a higher order construct and propose measures of engagement as a subcomponent to analyze the construct. However, these studies do not extend to the cases involving natural interaction with children and are often limited to artificial settings. Moreover, given that engagement is a latent internal state inferable only using observed cues, a majority of studies rely on a subjective measurement of engagement. Such measures are susceptible to several uncertainties introduced by variability in interaction settings, interpersonal differences, inconsistencies across subjective judgments and even the operational definition of engagement.We aim to address the need for an objective engagement behavior quantification method that is robust to the variations in environmental parameters. We perform a study in which children interact with a psychologist while performing different socio-cognitive tasks. The child–psychologist dyadic interaction provides an opportunity to investigate interaction engagement under various settings introduced by differences in task conditions. In the study, we develop a computational system based solely on the observed vocal cues, specifically vocal prosody during the dyadic interaction. Furthermore, it is hypothesized that the engagement of the child can be predicted from the acoustic prosodic cues of both the child and the psychologist and data-driven methods can be designed to capture this relationship.Our approach is inspired by several previous studies that link speech prosody to human behavior based constructs such as emotion (Austermann et al., 2005; Lee et al., 2011), approach-avoidance (Rozgic et al., 2011; Xiao et al., 2012), entrainment (Lee et al., 2014), blame/acceptance (Black et al., 2013), and empathy (Kempe, 2009; Aziz-Zadeh et al., 2010). These studies present techniques that computationally model prosodic patterns which are otherwise difficult to quantify perceptually. We build upon our previous work in Gupta et al. (2013, 2012) and present a data-driven approach to identify global prosodic patterns (task level statistics) as well as those that last over much shorter time spans (local cues). The local level cues also provide a means for capturing the temporal relationship between the local prosodic patterns. We apply this model for engagement level prediction using prosody and our modeling technique can serve as a generic tool extendible to other modalities. We train separate models on the global and local cues and finally fuse their predictions. We observe that individual models using either the global or the local cues carry statistically significant predictive capability. We subsequently fuse the two components to utilize their complementarity. Our model assigns child engagement to one of the three instrument-defined categories, achieving an unweighted average recall of 55.8%. We also investigate the predictive capabilities of each individual cue used in classification.Besides obtaining an objective decision, we also address the issues of variability introduced by interpersonal differences and dissimilar interaction settings. We show that our methodology captures prosodic patterns that are universally present across various interaction settings in the Rapid ABC protocol. Specifically, we normalize for individual speaker traits and train our model by combining data from all the tasks that comprise an entire dyadic interaction. We present the results categorized per task as well as for each interacting psychologist to estimate the generalizability of our model. We observe that our engagement model performs well over different parameters, but the performance does vary under different settings. This suggests that universal prosodic cues of engagement do exist, but they occur in combination with some setting specific patterns.This paper is organized as follows: Section 2 describes the database. We explain the global and the local cue extraction scheme and the experimental setup in Section 3 and describe the classification approach and its results in Section 4. We present our analysis on the system in Section 5 and conclude in Section 6.We use the Rapid ABC database (Ousley et al., 2013; Rehg et al., 2013) collected at the Georgia Institute of Technology as part of an NSF Expeditions project. The Rapid ABC protocol is a 3-5 minutes long semi-structured interaction between a psychologist and a child during a predefined set of tasks. Concurrently, the psychologist assesses the child's social attention, non-verbal communication using gaze, vocalizations and facial expressions, and perceived engagement. The assessments by the psychologist are recorded on a paper form screener. Specifically, the recorded assessments in the screener are designed to identify behavioral markers of atypical social-emotional development, language and motor development. The primary purpose of this dataset is to aid experiments in designing technological solutions that would facilitate the integration of an autism screening tool into a medical office's work-flow. In the following sections, we describe the interaction settings for the Rapid ABC dataset, the screener form based evaluation followed by data statistics.The Rapid ABC interaction sessions involve semi-structured over-the-table interaction between a child and a psychologist. The session involves five tasks: (i) Smiling and saying hello, (ii) Ball play, (iii) Jointly looking at a book, (iv) Putting a book on the psychologist's head as if it was a hat, and (v) Smiling and tickling. These tasks are designed to capture various aspects related to cognitive, social, language and motor development.11A description of these tasks can be found at https://www.youtube.com/watch?v=89KnHRLz7EQ.The psychologist is provided a script for each task and concurrently records her assessments in the screener. The dataset is recorded using video, audio and physiology (wrist) sensors. In this work, we use the prosodic measures derived from the audio signal captured by a central farfield microphone on the table that records audio (at 16khz sampling rate) from both the child and the psychologist.The Rapid ABC screener form was designed to concisely capture various observable/perceivable cues reflective of behavioral aspects like joint attention and social aptitude during each task in a session. The screener had to be manually completed by the psychologist while simultaneously interacting with the child. Part of the screener corresponding to the Ball play task is shown in Fig. 1. For each of the five tasks, the screener consists of two separate fields:•Behavior annotation: The psychologist annotates a set of observed behaviors that are expected from the child during a task. These sets of behaviors of interest typically include actions reflecting joint attention or a social response. In Fig. 1, the instructions to annotate a set of behaviors for the ball task is provided. The psychologist scores a PLUS if behavior mentioned in the instructions is present and a MINUS otherwise.Ease of engagement: The psychologist annotates the child's engagement state as one of the 3 levels as per the instructions listed in Table 1. Whereas better metrics to quantify engagement may exist, the three levels used (classes) broadly stratify the engagement phenomenon making it easier for the psychologist to provide an objective judgment. A separate block to annotate the engagement levels (E) is provided in the screener form as shown in Fig. 1.In our work, we aim to model the above perceived engagement level from observed cues, specifically, of speech prosody. Attributes related to behavioral annotation in the screener can be reliably annotated, but the engagement level annotation is prone to variabilities introduced by interpersonal differences amongst interacting psychologists; including in their perceptions, differences in settings under which the interaction happens, and the developmental stage of the child. We intend to aid the psychologist's judgment by minimizing the effect of the aforementioned factors.We use 74 sessions containing speech recordings from 63 children in the Rapid ABC data. The Rapid-ABC protocol was repeated for 11 children as a follow up and hence we have two sessions for these children. The children are in the age range of 15–30 months and 39 of them are boys. These children interacted with one of the four psychologists trained for interaction in the Rapid ABC settings. Apart from the psychologist's assessments, these sessions contain manual lexical transcriptions with time alignments for child and psychologist vocalizations. The distribution of the engagement levels for each of the five tasks over the 74 sessions is listed in Table 2.From the table we observe that we have an uneven distribution of engagement levels. E0 is the majority class in all the tasks. This suggests that the occurrence of other levels of engagement is rather atypical and possibly of greater interest with respect to the goals of the Rapid-ABC protocol in providing quick assessments over several aspects of child development. Also, the distribution of engagement levels varies depending on the task. For instance, the proportion of E0 in the Hat task is significantly higher than any other task. Similarly, the proportion of E1 in the Book task is significantly higher than Ball, Hat and Smiling tasks (We use a conservative difference in proportions test for p-value < 5%. The number of samples for the significance testing is computed such that each class is considered to have the same number of samples as the least represented class.22The rationale is to give equal importance to all the classes while performing the significance testing. This is particularly important as we use the unweighted average recall per class as our metric later. Since we reduce the number of samples, this test provides a more conservative significance level. However, we do avoid inflated significance which may arise due to different statistics on the majority class. For more details please refer to Bone et al. (2015).). This indicates that the phenomenon of engagement is contingent on the task at hand and may vary under dissimilar settings.We focus on using data-driven methods to model the psychologist's engagement level assignment using observed cues, specifically the vocal prosody of the participants. Even though the engagement level is conditioned upon the interaction settings, we hypothesize that there exist common vocal prosodic patterns reflective of engagement across these settings. Furthermore, we hypothesize that the child's engagement will be reflected not only in the child's prosodic cues, but also in those of the interacting psychologist. We aim to objectively capture these cues to infer the engagement levels. We construct multiple models performing the engagement evaluation and combine their decisions to obtain a final prediction. In the remainder of this section we describe our data preparation and prosodic cue extraction framework.For each of the 74 sessions we have five tasks with engagement evaluation. We segment the audio files by task to allow for task-wise analysis. Hence we have 370 (74 sessions×5 tasks) audio segments, each with corresponding manual diarization and an engagement score assigned by the interacting psychologist.In order to train our models, we collectively use the 370 files from all the tasks and psychologists. Even though the data statistics suggest that engagement is contingent on the task, we combine the data primarily because of two reasons.(i)First, we aim to capture prosodic patterns that are robust to the variability introduced by task-dependent contexts and different interaction partners.Second, the small number of instances from E1 and E2 classes are not suitable for training specific models for each task and psychologist. As we rely on data-driven techniques, we need sufficient samples to reliably capture patterns.After obtaining these separate audio chunks with an assigned engagement level E∈{E0, E1, E2}, we extract the prosodic signals from the speech as discussed in the next section.We extract a set of prosodic signals that characterize voice source activity: pitch, loudness, jitter and shimmer. All prosodic signals are computed using Praat (Boersma and Weenink, 2001) at a rate of 100frames/s. In our experiments, we consider every 10ms time interval as one analysis frame. Below, we detail prosodic signal computation including utilized signal denoising techniques.•Speaker assignment: We use the manual segmentations to obtain a frame-wise speaker assignment vector S={s1, …, sn, …, sN}, where N represents the total number of analysis frames in the file (assignments are made every 10ms). The element snassigns the nth frame of the audio file to either psychologist speech (Psy), child vocalization (Child), overlap (Ol) or silence (Sil).Pitch: We use an autocorrelation based method to perform pitch estimation (F0, fundamental frequency) as described in Boersma (1993). We use an analysis window with a duration of 40ms to estimate pitch at a time step of every 10ms, which synchronizes with our speaker assignments. Since the extracted pitch may have errors due to audio quality, we smooth and cubically interpolate the pitch signal to reduce such errors. P={p1, …, pn, …, pN} represents the vector of processed pitch values, where pnis the pitch value for the nth frame.Intensity: We obtain the intensity estimates by squaring the audio magnitudes per frame. This is followed by convolution with a Gaussian window to reduce noise effects. The pitch-synchronous intensity ripple is also reduced by this operation to give a smoother intensity contour (Boersma and Weenink, 2001) (as convolution with Gaussian window is also a low-pass filtering operation). We represent the intensity vector as I={i1, …, in, …, iN}.Jitter: Jitter serves as a measure of voice quality and is defined as the cycle-to-cycle variation of the fundamental frequency (F0) (Farrús et al., 2007). We estimate the relative jitter using overlapping windows in the audio files. Relative jitter is computed by normalizing the absolute jitter (the average absolute difference between consecutive periods in speech signal) by the average period. Note that a jitter value for a specified window can only be calculated if it contains multiple F0 value estimates. We chose a window length of one second shifted by 10ms. Smaller window lengths lead to several undefined jitter values and larger window lengths lead to imprecise estimates of local values of jitter as voicing from distant intervals is also incorporated.As jitter measures cycle to cycle variation of periods in the speech signal, its estimation is sensitive to the accurate estimation of F0. We can only estimate jitter when we have several pitch cycles in a window. Since we observed noisy jitter estimates in our data, we choose to smooth the jitter signal using a moving average filter. However, we do not interpolate the values as they are often missing over several windows, leading to poor interpolation. Over such windows with missing values, the jitter is listed to be undefined. We represent the set of jitter values as J={j1, …, jn, …, jN}, where jnrepresents the jitter estimate for a window starting at the nth frame, extending for 1s. Note that jncan also be listed as undefined.•Shimmer: Shimmer provides us with another measure of voice quality (Farrús et al., 2007). Shimmer measures cycle-to-cycle variation in intensity (jitter measured variation in periods). We estimate relative shimmer using the same window-wise approach as with jitter, smoothing but not interpolating the signal (again leading to undefined values). We represent the set of shimmer values as H={h1, …, hn, …, hN}, where hnrepresents the shimmer estimate for a window starting at the nth frame.The characteristics of the prosodic signal based on engagement level can be captured using either (i) time series modeling tools or (ii) discriminative models on statistical measures computed over the prosodic signals. Time series modeling tools such as Gaussian mixture model-Universal background models (GMM-UBM) (Reynolds, 2002), i-vector systems (Dehak et al., 2009; Shum et al., 2010) train on frame-wise features and attempt to model the probabilistic process governing the time series generation (conditioned on the class label). These methods assume that each sample in the time series is generated from a class dependent probability distribution. Given a set of time series from each class, these time series modeling tools estimate the parameters of the probability distribution for that class. On the other hand discriminative models on statistical estimates rely on capturing the differences between target classes using compact statistical representations. The latter is particularly useful in case of smaller datasets as modeling latent generative processes usually requires a large amount of data. Several other studies have also attempted to model similar time series data using compact statistical representations (Gupta et al., 2012; Bone et al., 2013; Hansen and Arslan, 1995). We use a similar discriminative scheme to capture the statistical properties of the prosodic time series at two levels of granularity: over the entire task duration (global cues) and at smaller time scales (local cues). The global cues help model the characteristics of the prosodic time series over the entire duration of a task, while the local cues quantify the local pattern in prosodic signals. We compare the outputs of the discriminative model based on local and global cues against a standard GMM-UBM model. We expect the discriminative model to perform better for two primary reasons, which are that the dataset is small and unbalanced and that the GMM-UBM model cannot account for temporal prosodic patterns. Over the next two section, we describe the local and global cues in detail.Global cues are statistical functional estimates calculated per-speaker over the entire interaction segment. Statistical functionals estimates computed over a sample set represent the characteristics of the underlying probability distribution from which the sample is drawn (Fernholz, 1983). These cues capture the overall characteristics but do not model the temporal evolution of the prosodic signals. Fig. 2describes the extraction procedure and below we provide a step-wise description of our methodology involving speaker-specific signal sampling, normalization and statistical computation.(i)Speaker-specific prosodic signal sampling: Given the speaker assignment S, we initially selectively sample the prosodic signals (pitch, intensity, jitter and shimmer) to contain frames only belonging to that speaker (overlaps excluded; this is shown in Fig. 2 in the block labeled “Signal sampling”). Note that in some of the tasks, we do not have child speech as the experimental design does not require a child to vocalize during interaction. For such tasks, we sample the prosodic signal corresponding to the psychologist speech only.Speaker-wise signal normalization: Next, we perform speaker-wise normalization on the sampled signal values to minimize speaker specific traits. We chose z-normalization (similar to cepstral means–variance normalization (Molau et al., 2003)) with means and variances obtained from all available vocalizations for each speaker. This includes all the five tasks as well as audio recordings before and after the Rapid ABC sessions.Global cue calculation: Finally, we calculate four statistical values over the z-normalized signals for the task at hand. These statistical functionals are mean, median, standard deviation and range. We call these our global cues.Since the computation of global cues does not capture temporal dynamics associated with the interaction between the two participants, we propose a method to overcome these limitations in the following section, utilizing local prosodic cues.The global cues computation treats prosodic signals as time series of independently drawn samples and the cue value will be unaffected even if samples in the time series are interchanged. However, the temporal evolution of prosodic signals may impart further information regarding the engagement behavior of children. Therefore, we propose a “prosody word” based scheme to concisely capture the temporal patterns in prosody along with jointly modeling the prosody of the two speakers. The local cues quantify changes in prosodic signals (e.g. increase in intensity, decrease in pitch) which are later associated with the engagement levels. The local cues are inspired from feature quantization methods (Ahalt et al., 1990) and language modeling techniques (Katz, 1987) in automatic speech recognition (Levinson et al., 1983). Recently, feature quantization has been coupled with other modeling techniques to address problems such as topic modeling (Kim et al., 2012; Nakano et al., 2014) and speaker recognition (Shriberg et al., 2005). We evaluate the utility of these cues in engagement prediction over just using the global cues. Fig. 3summarizes the local cue extraction framework, consisting of five steps which are described next in detail.(i)Signal quantization: Given a prosodic signal and the speaker assignment, we quantize each frame of the signal based on a threshold (T). If the frame is assigned to a unique speaker (Child or Psy), we set T to be the median of the prosodic signal over the entire available vocal activity for the same speaker. As an example, a graphical illustration for pitch signal quantization is shown in Fig. 4. Eq. (1) lists the naming convention for quantized values in the provided example. d(pn) represents the discretized value for pn(the pitch value at the nth frame). Frames assigned to overlap or silence are retained without further quantization.(1)d(pn)=0PsyPitchifpn<T(Psy);iftheframebelongstothepsychologistspeech1PsyPitchifpn≥T(Psy);iftheframebelongstothepsychologistspeech0ChildPitchifpn<T(Child);iftheframebelongstothechildspeech1ChildPitchifpn≥T(Child);iftheframebelongstothechildspeechOiftheframecontainsanoverlapxiftheframecontainssilencewhere T(Psy) is the median pitch value over the entire psychologist vocal activity and T(child) is the median pitch over the entire child vocal activity.Although discretization reduces information, it allows for more complex modeling and learning with limited data. The feature median gives us a balanced distribution of the two binary categories and is not vulnerable to outliers. We do not perform a finer quantization of the prosodic signals as the number of prosody words (defined later) increases exponentially leading to sparsity issues.Signal units: Next, we define a “signal unit” over a window consisting of multiple discretized signal frames, aiming to capture the signal dynamics over a shorter time span. We operate sequentially on the discretized prosodic signal given the window length W and window overlap length V. For a window starting at the nth frame, we define the signal unit based on the following W discretized signal values. A signal unit provides a compact statement about the prosody within a window, such as the window containing “high pitch” or “a transition from high to low intensity”. Using the same example of the pitch signal, we list the signal unit assignment strategy in Table 3.DnPitchis a window starting at the nth frame containing the discretized values {d(pn), …, d(pn+W)}.U(DnPitch)is the signal unit assigned toDnPitch. An example pitch units assignment with W=3 and V=1 is shown in Fig. 5.Prosodic words: Next, we concatenate the sequence of signal units from multiple signals to obtain the joint representation over various prosodic signals. In this work, we use the pitch and the intensity signals. Using more signals leads to an exponential increase in the number of prosodic words33Just adding one more signal increases the count of potential prosodic words from 6 to 29. This leads to a very large number of n-grams as computed next.and also jitter and shimmer are poorly estimated44The jitter and shimmer signals rely upon accurate estimation of F0. In our case, we find that jitter and shimmer estimates for psychologist and child speech are completely absent for about 33% and 50% of the 370 tasks, respectively, as F0 could not be continuously estimated by Praat over long periods of time in these sessions.. Eq. (2) shows the vector obtained after concatenating signal unit for pitchU(DnPitch)and intensityU(DnInt), defined as the prosodic word. The prosodic word provides a combined representation of the dynamics captured by the signal units(2)Rn=U(DnPitch)U(DnInt)Each of the prosodic words give us a crude estimation of the prosodic signal dynamics over a shorter time span.N-grams of prosodic words: Given the sequence of prosodic words for the Rapid ABC tasks, we apply ideas similar to language modeling (Katz, 1987) in automatic speech recognition. We define n-grams on the set of prosodic words. For instance, the bigrams are defined by the pairs of consecutive prosodic words; Rnand R(n+W−V) (note that if a window starts at n, the next window will start at n+W−V to achieve an overlap of V).Similarity computation: Next, we compute empirical occurrence probabilities of n-grams on: (a) a given test task and (b) all the tasks in the training set with a specified engagement level. Let n-gramlbe one of L possible n-grams; then the empirical probability for n-gramlon the test task πtest(n-graml) is as shown in Eq. (3), and the empirical probability on training tasks with engagement level E, πtrain:E(n-graml) are as shown in Eq. (4).(3)πtest(n-graml)=Totalcountofn−gramlinthetesttaskTotalnumberofn−gramsinthetesttask(4)πtrain:E(n-graml)=Totalcountofn−gramlintrainingpartitiontasksw/engagementlevelETotalcountofalln−gramsinthetrainingpartitiontasksw/engagementlevelEA final step in local cue computation consists of computing the cosine similarity CE(Eq. (5)) between vectors of empirical probabilities πtest(n-graml) and πtrain:E(n-graml). In Eq. (5), 〈πtest(n-gram1), …, πtest(n-gramL)〉 represents the vector of πtest(n-graml) over all the n-grams computed on the test set. Similarly, 〈πtrain:E(n-gram1), ldots, πtrain:E(n-gramL)〉 represents the vector of πtrain:E(n-graml) computed on the train set. CEis shown as the cosine distance between these two vectors. cE(n-graml) is simply the product of πtest(n-graml) and πtrain:E(n-graml). The CEand cE(n-graml) measures serve as our local cues and concisely capture the dynamics in prosodic signals. This last step in local cue computation is shown in Fig. 6.(5)CE=〈πtest(n-gram1),…,πtest(n-gramL)〉T|〈πtest(n-gram1),…,πtest(n-gramL)〉|2〈πtrain:E(n-gram1),…,πtrain:E(n-gramL)〉|〈πtrain:E(n-gram1),…,πtrain:E(n-gramL)〉|2(6)cE(n-graml)=πtest(n-graml)·πtrain:E(n-graml)We train separate classifiers on the set of global and local cues as described in the next section. For the local cues, we tune the window parameters W and V using inner cross-validation on the training set. Given the small amount of data, only unigrams and bigrams are extracted for reliable estimation. As the number of n-grams is still large, we perform feature selection on cE(n-graml) during classification via the correlation-based feature selection (CFS) filter algorithm proposed in Hall (1998). This feature selection scheme evaluates the worth of a subset of products cE(n-graml) by considering the predictive ability of each along with the correlation amongst all of them. In the next section, we provide the results.We initially train a GMM-UBM model on frame-wise features as a baseline. This is followed by the description of the discriminative model trained on the global and local cues. We first describe the GMM-UBM model followed by the discriminative model based on global and local cues.GMM-UBM on frame-wise features: In this modeling scheme, we initially sample the prosodic signals belonging to the psychologist and the child speech frames (for speaker specific sampling please refer to Fig. 2). On the sampled prosodic signals, we train separate psychologist and child speech UBMs. These UBMs are then adapted using the data from tasks with specific engagement levels. For example, frames from psychologist speech belonging to all tasks with engagement level E0 are used to adapt psychologist UBM to obtain a psychologist GMM-model for level E0. Therefore we obtain three different GMM-models each for the two speakers, corresponding to the three engagement levels. Note that these models are trained on a 4-dimensional space defined by the frame-wise values of the four prosodic signals. We evaluate the GMM-UBM by using speaker-independent cross-validation, i.e., leaving sessions from one child for testing and training on the rest. On the test set, we evaluate the prosodic signals belonging to psychologist speech frames using psychologist GMMs (likewise for child frames). The final class likelihoods are obtained by summing up likelihoods from psychologist and child GMMs for that class. Given the unbalanced dataset, we use unweighted average recall (UAR) as our performance metric and the results using the GMM-UBMs are shown in Table 4.From the results, we observe that though we beat the chance recall (33.33%) the value is relatively low. This is expected as we use the raw feature values without any regards to their characteristics over the entire task duration. Moreover in this simplistic model, the sequence of feature frames is not accounted for and no interaction between child and psychologist prosody dynamics is captured. We expect to represent the global characteristics and local temporal patterns using the global and local cues as presented in the next section.Discriminative model based on global and local cues: We train individual classifiers on both global and local prosodic cues and then fuse the outputs in a stacked generalization framework (Wolpert, 1992). We chose this multi-layered classification approach for two main reasons. First, separate evaluations for global and local cues provide us with an independent measure of their discriminative power. Additionally, their joint performance helps us evaluate the degree of complementarity between the feature sets. Second, an independent training of global and local cue classifiers helps address data sparsity issues. A general schematic of classification experiments and their presentation in this section is given in Fig. 7.Speaker-independent cross-validation is performed by holding out data segments for each child, leading to 63 splits. Given that the data suffers from class bias, we subsample data points from the majority class (E0, E1) so that each class has the same number of training instances as the least represented class (E2). This effectively optimizes our performance metric unweighted average recall (UAR) as using the same number of samples per class in training assigns equal importance to individual class recalls.In order to determine class boundaries, we train multiclass support vector machine (SVM) classifiers (Cortes and Vapnik, 1995) with pairwise boundaries. We obtain probabilistic decisions for each engagement level by fitting logistic models to data point distances from SVM hyperplanes. However, the logistic models may not yield class probabilities that sum to one. Thus these probabilities are scaled using the coupling method suggested by Hastie and Tibshirani (1998). The assigned class is the one with the highest probability. The parameters (i. e. kernel, boxconstraint) of the SVM classifier are tuned by internal cross-validation on the training set. We observe that all SVM classifiers perform best using a linear kernel as complex kernels may overfit a small dataset easily.We train two classifiers with global cues derived from the psychologist (SVMPsy) and the child (SVMChild). Whereas the psychologist speech is present in all the tasks, the child features are available only in 221 of 370 tasks (172 marked as E0, 34 marked as E1 and 15 marked as E2). If the child speech is present, we fuse the outputs from SVMPsy and SVMChild using another classifier: SVMGlo. We train SVMGlo on the probabilities SVMPsy and SVMChild output on the training set itself. In the absence of child speech, we directly use the probabilities output from SVMPsy to determine the overall UAR for the global system.

@&#CONCLUSIONS@&#
