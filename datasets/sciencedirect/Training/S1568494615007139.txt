@&#MAIN-TITLE@&#
Application of teaching learning based optimization procedure for the development of SVM learned EDM process and its pseudo Pareto optimization

@&#HIGHLIGHTS@&#
Two-stage soft computing ((SVM-TLBO)-(PLM-TLBO-pseudo PARETO)) based virtual system of manufacturing process – EDM is developed.Virtual data generator of EDM process learned by support vector machine (SVM) with internal parameters (C, ɛ and σ) tuned by teaching learning based optimization (TLBO) is reported.Modifications namely population based termination criteria, initialize population with high dispersion and way of choosing teacher in case of multiple best learners performing same score, over standard TLBO are suggested. Further, a comparison between performances of TLBO and PSO in model development is studied.A simple procedure for pseudo Pareto front development by modified TLBO is proposed.Inverse solution procedure for selection of optimum available machine parameter setting corresponding to specific output combination is elaborated.

@&#KEYPHRASES@&#
Teaching learning based optimization (TLBO),Electrical discharge machining (EDM),Support vector machine (SVM),

@&#ABSTRACT@&#
Manufacturing processes could be well characterized by both the quantitative and the qualitative measurements of their performances. In case of conflicting type performance measures, it is necessary to get possible optimum values of all performances simultaneously, like higher material removal rate (MRR) with lower average surface roughness (ASR) in electric discharge machining (EDM) process. EDM itself is a stochastic process and predictions of responses – MRR and ASR are still difficult. Advanced structural risk minimization based learning system – support vector machine (SVM) is, therefore, applied to capture the random variations in EDM responses in a robust way. Internal parameters of SVM – C, ɛ and σ are tuned by modified teaching learning based optimization (TLBO) procedure. Subsequently, using the developed SVM model as a virtual data generator of EDM process, responses are generated at the different points in the experimental space and power law models are fitted to the estimated data. Varying the weight factors, different weighted combinations of the inverse of MRR and the ASR are minimized by modified TLBO. Pseudo Pareto front passing through the optimum results, thus obtained, gives a guideline for selection of optimum achievable value of ASR for a specific demand of MRR. Further, inverse solution procedure is elaborated to find the near-optimum setting of process parameters in EDM machine to obtain the specific need based MRR-ASR combination.average surface roughness (μm)biasregularization parameterlimits of constriction factorlimits of cognitive acceleration coefficientcurrent setting (A)training input space dimensiontarget functioncomponent of best position of swarm along dth dimensionmaximum number of iterationskernel functionmean absolute training errormaterial removal rate (mm3/min)number of learners in class, number of particles in swarmnumber of training datacomponent of best position of ith particle along dth dimensiona pseudorandom number generated following standard uniform distribution within range (0,1)random weight factorslimits of social acceleration coefficientpulse off time (μs)pulse on time (μs)teaching factorvelocity component of kth particle along dth dimension in iterth iterationweight vectortraining input vectorcomponent of velocity corrected position of kth particle along dth dimension in iterth iterationtraining output vectormean of training output setnumber of attributesLagrange multipliersradius of loss insensitive hyper-tubestandard deviation of radial basis function (kernel function)feature spacelimits of inertia factor

@&#INTRODUCTION@&#
Manufacturing industries are playing a vital role to meet the ever growing demand on product variation as well as to boost the economical growth since the early age of industrialization. Non-traditional manufacturing processes widen the horizon in precision manufacturing over the last three decades. The goal of developing and deploying economical supportable machining techniques, capable of meeting almost all performance requirements is difficult to achieve till now. Conflicting type performance measures of any manufacturing process still involve a difficulty to get simultaneous optimum outcomes. Concept of Pareto optimality of multiple outcomes is expected to be useful in this regard. Near exact representation of process and thus setting of control parameters to achieve simultaneous optimum responses are necessary to freeze the procedure at the earliest in pre-production stage. The model building would become even more difficult for stochastic type non-conventional manufacturing processes. Advanced learning based systems being devoid of four problems – efficiency in training, efficiency in testing, over-fitting and algorithm parameter tuning would be effective in such situation.In the present study, experiments are carried out on electric discharge machining (EDM) process in the semi-finishing and roughing zone with different combinations of three significant process parameters – current (cur), pulse on time (ton) and pulse off time (toff). Material removal rate (MRR) and average surface roughness (ASR) are considered as two performance measures. In case of machining, rate of material removal determines the productivity of the process that is higher MRR results higher productivity. Besides, to meet the specific functional aspects of product, quality must be maintained. One of the major surface quality measurements is the average surface roughness (ASR). They are conflicting in nature. Therefore, a trade-off between MRR and ASR is expected to exist. Hence, the process should be controlled to meet both of these features in an optimal way. These responses are trained through support vector machine (SVM) regression procedure for explicit model development. A meaningful physical significance of the insensitive zone of learned system provides a space to allow the tolerances on uncontrollable variations in EDM process. Aich and Banerjee [1] outlined the way of setting all of the internal parameters – regularization parameter (C), radius of loss insensitive hyper-tube (ɛ) and standard deviation of Gaussian radial basis function (σ) chosen as kernel function (K(xi, x)) for SVM learning through particle swarm optimization (PSO) and thereby, the building of separate models of MRR and surface finish of HSS in EDM. However, the behavior of the developed models near the boundary of the experimental domain appears as not very accurate.Performance of the swarm based optimization techniques, rather evolutionary algorithms, are affected by their own control parameter settings [2]. Unlike those probabilistic approaches, algorithm-specific parameter-less teaching learning based optimization (TLBO), introduced by Rao et al. [3] is proved to be more effective for complex type multimodal high-dimensional non-linear objective functions [3,4]. Thus, a comparison between the performances of modified TLBO and modified PSO in searching optimum sets of SVM internal parameters – C, ɛ and σ is also studied.Though, SVM learned system is able to predict the responses very close to their experimental ones, still tractability of this complex representation must be improved. So, process responses are estimated through the learning system and further fitted in power law model (PLM). Power law models are readily amenable for shop-floor use and through inverse solution procedure near optimum process parameter setting could be easily found to meet the customer requirement. Here, SVM learned system itself performs as a virtual data generator of EDM process. Pseudo Pareto optimization of the responses is done by assigning different weight factors to the power law fitted models. As, modified TLBO exhibits better performance in the model development stage, modified TLBO is further employed for pseudo Pareto optimization of responses. In addition, an inverse solution method is proposed to get near optimum process parameter setting (cur, tonand toff) for a desired MRR-ASR combination within the experimental domain. On survey, no literature was found on such two-stage ((SVM-TLBO)-(PLM-TLBO-pseudo PARETO)) optimization based virtual system on EDM.Performance of metaheuristic algorithms in comparison with traditional deterministic approaches for optimization of multimodal, high-dimensional non-linear large scale engineering problems is established as more promising [3]. Algorithms for those trajectory and population based natural phenomena are still suffering from the problem of tuning their own internal parameters [3,4]. Based on the ideology of teaching-learning process, Rao et al. [3] introduced an algorithm-specific parameter-less optimization technique. Though, no such internal parameters are required to fix in TLBO before simulation starts, yet, two crucial conflicting aspects of a metaheuristic algorithm – intensification and diversification are successfully achieved in it. Exploration of the search space is done in learner phase whereas teaching phase does the exploitation. Knowledge gain of a learner depends on both the teaching ability of the teacher as well as the receiving capacity of the learners. Teacher always tries to pull forward the batch of learners aiming to his/her own level. It was reported that, in this teaching phase, overall performance of all learners gradually improves by deploying adaptive teaching factor [5] depending upon the current performance level of the whole batch instead of randomly chosen teaching factor (TF) values. Sharing of knowledge among learners also helps in improvement of their individual knowledge levels. In this way, in every iteration, objective function values i.e. learners’ score in each subject gradually moves toward optimum zone. Some modifications, namely population based termination criterion, initialize population with high dispersion and way of choosing teacher in case of multiple best learners performing same score, of the standard basic algorithm are adapted for more smoothing convergence.Electrical discharge machining (EDM) is a potential process of developing complex surface geometry and integral angles in mold, die, aerospace, surgical components, etc. [6]. The process is applicable to any conductive material (resistivity should not exceed 100Ωcm) regardless of its hardness, toughness and strength [7]. Material is eroded by series of spatially discrete and chaotic [8] high frequency electrical discharges (sparks) of high power density between tool electrode and work piece separated by a fine gap of dielectric fluid. The working zone is completely immersed into dielectric fluid medium for enhancing electron flow in the gap, cooling after each spark and easy flushing of eroded particles. Electrical discharge machining process could be well characterized by two conflicting responses – material removal rate (MRR) and average surface roughness (ASR). From both quantitative and qualitative point of view, higher MRR and lower ASR are always preferred. Basic scheme of EDM is shown in Fig. 1.Experiment was carried out on an EDM machine (Tool Craft A25 EDM Machine) operating with commercially available kerosene oil as dielectric medium and an open circuit voltage of 66V. Standard high speed steel cutting tool of specimen C-0.80%, W-6%, Mo-5%, Cr-4%, V-2% equivalent to grade M2 was chosen as the work-piece material. The measured density of material is 8006kg/m3. The tool material selected is electrolytic copper with density 8904kg/m3 and has cross-sectional diameter of 12mm. Workpiece is connected to the negative terminal. Measurement of responses – MRR and ASR are taken with different combinations of levels of three most dominating process parameters of EDM namely current (cur), pulse-on time (ton) and pulse-off time (toff). In the semi-finishing and roughing zone, based on the availability of the machine settings, levels of the input process parameters are chosen as shown in Table 1.In each experimental run, weight loss is noted and surface roughness measurements are taken. To calculate MRR, work sample weights are taken at standard measuring balance (AFCOSET – ER182A) of least count 0.01mg. It is then divided by the measured density of work-piece material in order to convert it into volumetric term and is further divided by the actual machining time to obtain the MRR in terms of mm3/min. Different surface roughness measurements of the machined surface of work piece along three mutually 120° apart direction are taken by the Taylor Hobson Precision Surtronis 3+ Roughness Checker. Here a sample length of 4mm is taken and stylus tip radius of 5μm is used. Mean value of three-measured centreline average surface roughness (Ra) values are considered as the representative average surface roughness (ASR) of EDM machined surface.Total 64 mutually exclusive combinations of different levels of three process parameters are set to the EDM machine and corresponding process responses are observed. Out of 64 unique combinations, 15% of available population i.e. 10 sets are chosen randomly and kept aside for testing purposes. Rest data sets are used for training of SVM learning system.For the purpose of model building through SVM learning system, 54 data sets are taken for training. To fit the data with reasonable accuracy, internal parameters of SVM are to be properly tuned. Improper choice of SVM internal parameters may lead to under-fitting or over-fitting of the actual process. The internal parameters are tuned by modified TLBO technique to obtain minimum achievable training error. Fitted models are tested through rest 10 data sets.Ultimate insight of modeling process outcomes is to choose a robust trained model with the least possible errors. Statistical techniques like multivariable regression analysis, response surface methodology and artificial intelligence based neural network are rigorously used for modeling empirical data. Suffering from generalization of model estimation, over-fitting might have occurred in ANN. Besides, random variations in process outcomes are obvious in stochastic type machining process. These random fluctuations in experimental results are required to be absorbed with specified tolerance value for intelligent predictions. Structural risk minimization based [9] supervised batch learning system, support vector machine could be a smart way of handling the situation and the trade-off between flatness and complexity of the unknown underlying fixed function.Suppose, with a key assumption of disjoint, independent and identical distributed data set {(x1, y1), (x2, y2), … (xN, yN)} model is to be developed in d dimensional input space (i.e. xRd). Target function may be represented in the form [10]:(1)f(x)=<w,x>+bwhere <> indicates dot product in vector space. Nonlinearity in the relation between input and output pattern (shown in Fig. 2[11]) is handled through mapping the high dimensional input space to a feature space Φ(x) via kernel functions. In most of the model building techniques, data are fitted with least training error calculation to estimate the unknown coefficient or weight vectors associated with training inputs. So all the data are tried to fit as close as possible to the deemed model. Here, an insensitive zone wrapped around the estimated function is improvised. This zone captures the fluctuations within permissible tolerances specified with process outcomes. Thereby, radius of this hyper-tube directly controls the allowable complexity of the learning system. As, this radius increases, model would become more flat being unable to reveal the unseen nature of variation in the outcomes, whereas, lower radius might make the model more complex, Thus a trade-off is required. In nomenclature, the outliers around this tube are named as support vectors. To get the benefit of this feature of SVM regression over other model estimation techniques, a regularization parameter is required to penalize the support vectors whereas the points lie inside the insensitive zone are considered as of zero loss. Thereby, the regularization parameter (C) i.e. the penalty associated with support vector, radius of insensitive tube (ɛ) and kernel function parameter are to be properly tuned.Here, L(y), a loss function is introduced to penalize over fitting of model with training points. ɛ-Insensitive loss function (refer Fig. 3) is mostly used for process modeling problems. This function may be defined as [11]:(2)L(yi,f(xi))=yi,experimental−f(xi)|−ε,if|yi,experimental−f(xi)|≥ε=0,if|yi,experimental−f(xi)|≤εThe idea of kernel function K(xi, x) gives a way of addressing the curse of dimensionality [12]. It helps to enable the operations to be performed in the feature space rather than potentially high dimensional input space. Gaussian radial basis function with σ standard deviation (Eq. (3)) is commonly used for its better potentiality to handle higher dimensional input space.(3)K(xi,x)=exp−||xi−x||22σ2This problem can be efficiently solved by standard dualization principle utilizing Lagrange multipliers (αi,αi∗) [9,10]. Support vectors can be easily identified from the value of difference between Lagrange multipliers (αi,αi∗). Very small values (close to zero) indicate the points inside the insensitive hyper-tube but non-zero values belong to support vector group [13]. Then, weight factor w can be calculated by [10](4)w=Σi=1(1)N(αi−αi∗)Φ(xi)Thus, the final model with optimum choice of C, ɛ and σ may be presented as [10](5)f(x)=Σi=1(1)N(αi−αi∗)K(xi,x)+bCoptimumεoptimumσoptimumTraining of experimental results with proper internal parameters of SVM regression (C, ɛ and σ) is necessary to get a near-exact representation of the process. So, these three internal parameters should be optimally tuned. Here, TLBO with certain modification is employed for this tuning operation. Different set of C, ɛ and σ, reshape the learning system and optimization technique is employed to search optimum internal parameter setting that would train the experimental results with minimum training error. With optimum set of C, ɛ and σ, a set of Lagrange multipliers are calculated and thus models of MRR and ASR are estimated using Eq. (5).Searching techniques should be robust to get a general result. Wide range of search space may be a good choice but irrelevant movement would take lot of time to converge. So, searching range of these three parameters should be logically chosen. Aich and Banerjee [1] reported some experimental data based techniques to choose these ranges. For setting a searching range of C, upper end of six sigma range [14,15] of response values was considered, but some duplication error is there as they again selected a range considering normal distribution over this upper end value of six sigma range. This seems to be erroneous in physical significance. Actually the regularization parameter C should lie within the response values. Thus range of experimental values of responses might be a robust reasonable choice for searching range of C.(6)(MRRexp)min≤CMRR≤(MRRexp)max(ASRexp)min≤CASR≤(ASRexp)maxBesides, searching ranges of ɛ and σ are chosen as [14,15](7)ε=y¯30,y¯10;σ=[(0.1)1/z,(0.5)1/z]Here, z indicates number of most influencing attributes in the process. In EDM it is 3, namely, current (cur), pulse on time (ton) and pulse off time (toff). Using Eqs. (6) and (7), searching ranges of C, ɛ and σ are calculated (Table 2).For better implementation of this estimated search range it was suggested [14,15] to normalize the training inputs within the range (0,1). So, the chosen control parameters of EDM process, i.e. current, pulse on time and pulse off time are normalized using the following formulae.(8)x1,norm=cur−615−6x2,norm=ton−50200−50x3,norm=toff−50200−50Choice of internal parameters C, ɛ and σ, changes the value of Lagrange multipliers and thus shape of the prediction model changes. Best model should be selected for near accurate estimation of responses. Chance of generalization error is reduced in SVM learning and internal parameters must be tuned in such a fashion as to reduce the training error in learning process. Thereby, in this study, mean absolute training error (MATE) in prediction of process responses – MRR and ASR is chosen as objective function.(9)MATE(%)=100N∑i=1(1)N|yi,exp−yi,est|yi,expMinimization of MATE is carried out for selection of SVM internal parameters C, ɛ and σ. In the marching steps for optimization of MATE, different combination of C, ɛ and σ changes the value of Lagrange multipliers. Subsequently, learning system is getting reshaped, responses are predicted and MATE is evaluated using Eq. (9). Here, this optimization is performed by algorithm-specific parameter-less teaching learning based optimization.As discussed in Section 2, TLBO is a parameter less optimization technique. Still, to get this benefit in optimizing any non-linear high-dimensional objective functions, termination criteria should be logically defined. In most of the optimization techniques, a termination criterion is defined by the maximum number of iterations or change in objective function value below a predefined margin. When optimizing a new objective function, it is very difficult to know earlier the required number of iteration to meet a certain target. Even to attain certain accuracy, change in objective function value may vary due to their different scale range. In some cases attainable optimum objective function value is difficult to predict earlier. As such, a general termination criterion is required to propose for population based searching techniques. Though a proposal has been submitted by Aich and Banerjee [1], but this criterion might become erroneous when the optimum lies near zero. Even negative search space needs to be shifted to positive side before application. In the present work, a general meaningful criteria is suggested based on spread of population relative to searching range in different dimension i.e. spread-range (SR) ratio defined as a ratio of standard deviation of population to span of searching range (expressed in %) i.e.(10)SRratio(%)=100×(standarddeviationofpopulation)spanofsearchingrangeThereby, simulation will be stopped when this SR ratio along each of the input parameters’ dimension simultaneously go down below a predefined limit. Here, this limit is chosen as 5% i.e. termination of searching operation would be flagged on when SR ratio along C, ɛ and σ dimension simultaneously drop below 5%.Metaheuristic techniques are marched to the global optimum with some randomly generated probabilistic logical movement. Whatever might be the termination criterion that is considered, if simulation is stopped by watching that the specific user defined measurement just reaches below a certain value in any iteration, then, it may be erroneous. Simulation should be allowed for a few more iterations to finally freeze down below that specified limit. In the present work, termination criteria is defined by SR ratio of latest population i.e. learner in each direction – C, ɛ and σ and simulation is terminated when SR ratio values along all dimensions (C, ɛ and σ) satisfy the termination criteria i.e. go below 5% in last consecutive 5 iterations.In case of population based optimization technique, a widely spread initial population must be assured for better exploration in the whole range. As discussed earlier, a proposal has been raised to consider latest population based termination criteria i.e. SR ratio of the latest population along each dimension and so initial SR ratio of the population must have a high value along all dimensions to ensure proper exploration of the search space. In the present work, considering initial SR ratio as at least 40% along each direction, a set of 20 learners are randomly generated within specified search space (Table 2). For maintaining the repeatability of the simulation steps, initial learners are given in Table A1.In every step of iteration, with different set of learners i.e. set of C, ɛ and σ, shape of trained model changes. Teacher for any iteration is selected as that set of C, ɛ and σ having lowest training error (MATE) value. Delivering capability of teacher is controlled by adapting a teaching factor evaluated from the latest condition of the students in the population. Here, teaching factor is modified as a ratio of mean of the learners’ value to teacher value at latest population [5], instead of any randomly chosen integer either 1 or 2 [3]. This adaptive teaching factor aids in converging the simulation with lesser time.In most of the published study of optimization algorithm [16], it is reported that the best one of the latest population works as the guide for next iteration. The best one is chosen with either minimum or maximum objective function value. However, if more than one best setting in the population with same minimum or maximum objective function value are found then confusion will come to choose only one among all those best settings. Improper choice of the best may guide the following iterations in wrong way and finally might be trapped inside any local optimum. In the present work, a simple methodology is proposed to avoid this condition. A weighted combination of all best settings should be evaluated which either having lower objective function than the second best (for first iteration) or lower (higher for maximization) than the objective function value of the best setting in immediate last iteration. Say, in case of minimization of a two-variable objective function within search space ([0,20], [0,20]), at any iteration two learners (13.4, 2.3) and (6.6, 17.7) score same minimum value 70.85. Minimum score at last iteration was 73.45. Now it is required to choose the teacher among these two learners for next iteration. No such clear guidance is reported till now to choose the right one among these two. Here, a weighted combination of these two learners along their corresponding dimensions is calculated. Randomly two weights (rw1, rw2) are generated between (0,1) such that rw1+rw2=1. A new learner is evaluated as (rw1×13.4+rw2×6.6, rw1×2.3+rw2×17.7). For rw1=0.3 and rw2=0.7, new learner would be (8.64, 13.08) which scores 11.34. New evaluated learner scores less than the minimum score at last iteration (73.45) (in case of first iteration, comparison would be done with second minimum). This learner (8.64, 13.08) would be the teacher for next iteration otherwise the steps are repeated with another random set of weights (rw1, rw2) till the condition is fulfilled. However, there is no need to update current population with this evaluated teacher. This proposition is expected to be effective to avoid ambiguity to choose the right optimum at any iteration.Adapting all the above said modifications, steps of TLBO algorithm used for searching optimum set of C, ɛ and σ by minimizing MATE (Eq. (9)) in estimation of the responses separately are discussed below.Step 1: Set number of learners (n)=20, initial SR ratio along each dimension of C, ɛ and σ ≥40%, final SR ratio along each dimension ≤5%. Randomly initialize the position of 20 learners i.e. 20 set of initial combination of C, ɛ and σ within search space (Table 2).Step 2: Set itermax=250 and iter=1.Step 3: Calculate mean of all the learners and the score of the individual learner i.e. objective function value (Eq. (9)) for each of the 20 set of C, ɛ and σ. Identify the minimum objective function value, set the corresponding learner value as teacher. If number of learners having same minimum score appears more than one, then go to Step 4 otherwise go to Step 5.Step 4: Learners having same minimum score, are identified and make a weighted combination of those such that the new evaluated learner must score lower than either the second minimum score at current set of learners (applicable only for first iteration) or the minimum score gained at last iteration. The new evaluated learner is selected as teacher.Step 5: Calculate adapted teaching factor (TF) as(11)TF=meanofthecurrentlearnerscurrentteacherOnce the teaching factor is determined then evaluate new value of learners according to the following relation(12)Newlearner=oldlearner+rand×(currentteacher−TF×currentmean)Check if each of the new evaluated learners’ score is lower than their corresponding earlier score before this modification, then update the new value with the old one otherwise old learner is kept unaltered.Step 6: For each of the learners, chose another learner randomly. If this randomly chosen learner scores lower or higher than the current learner, then updates the current learner as follows otherwise it is kept unaltered.(13)Lower:newlearner=currentlearner+rand×(currentlearner−randomlychosenlearner)Higher:newlearner=currentlearner+rand×(randomlychosenlearner−currentlearner)Step 7: Calculate SR ratio along all three dimensions C, ɛ and σ. Store the maximum among these three SR ratio values in a separate matrix called “DECISION”. If iter<5, set iter=iter+1 and go to Step 3, otherwise go to Step 8.Step 8: If the maximum of last five consecutive entries in matrix “DECISION” contains SR ratio value lower than 5% then current teacher of the learners would be declared as the optimum setting of C, ɛ and σ, stop the simulation and minimum achievable MATE in estimation of response is calculated with obtained optimum set of C, ɛ and σ, otherwise go to Step 9.Step 9: If iter=itermax then go to Step 2, restart the simulation with higher itermax otherwise iter=iter+1, go to Step 3.Therefore, latest teacher is selected as optimum settings of C, ɛ and σ. With these settings of C, ɛ and σ, set of Lagrange multipliers are calculated and thus the model of the corresponding response could be represented by Eq. (5). Using this equation, estimated response parameter value is calculated and finally achieved training error (MATE) is determined.Now, using the above said teaching learning based optimization algorithm adapted with all discussed modifications, training errors in prediction of MRR and ASR (Eq. (9)) are minimized separately for different settings of C, ɛ and σ. As the simulation marches, with different values of C, ɛ and σ, shape of estimated model of each response is getting modified and training error is changed. Finally, the optimum settings of C, ɛ and σ within the specified searching range (Table 2) with minimum mean absolute training error (MATE) in estimation of MRR and ASR separately are found and reported in Table 3. Modified TLBO algorithm is coded in MATLAB R2012a and LibSVM command line functions are used for the SVM learning process.Optimum values of C for both of the MRR and ASR are shifted toward the upper end of search space. This indicates the complexity of the model which is in favor of the stochastic behavior of EDM process. The random fluctuations could be controlled by proper choice of ɛ. Here, lower values of ɛ for both of MRR and ASR indicate that these complex models could be able to absorb the random variations adequately. Besides, small σ values for both MRR and ASR indicate that oscillatory patterns in output outside the insensitive zones are properly entrapped.With the simulated results of C, ɛ and σ listed in Table 3, set of Lagrange multipliers (αi,αi∗) are calculated (Table A2) and estimated models of MRR and ASR are as follows:(14)MRR:f(x)=Σi=1(1)N(αi−αi∗)K(xi,x)+bC=28.0986ε=0.3968σ=0.4642withK(xi,x)=exp(−||xi−x||2/2σ2)σ=0.4642(15)ASR:f(x)=Σi=1(1)N(αi−αi∗)K(xi,x)+bC=9.00ε=0.2127σ=0.4642withK(xi,x)=exp(−||xi−x||2/2σ2)σ=0.4642Marching steps for optimum selection of C, ɛ and σ in estimation of MRR and ASR separately are given in the following flow chart (Fig. 4). Gradual decaying pattern of MATE in estimation of MRR and ASR are represented in Figs. 5–8.In case of minimizing MATE in estimation of MRR, relative to C and σ, the effect of ɛ is lower, as SR ratio for ɛ decreases at a faster rate than that that of C and σ (Fig. 6). Whereas, all three parameters C, ɛ and σ influence the model of ASR to almost same pattern (Fig. 8).To depict the effects of different process parameters (current, pulse on time and pulse off time) on responses, surface plots for MRR and ASR are generated using the above estimated models (Eqs. (14) and (15)) being trained by their corresponding training data sets and shown in Figs. 9–14.For both MRR and ASR, current shows a strong positive influence, whereas, other two control parameters – pulse on time and pulse off time are found to be not so effective with their changes. At lower values of current, effects of both pulse on time and pulse off time on MRR and ASR are almost insignificant. In the higher zone of current values, higher MRR could be obtained by increasing pulse on time or lowering pulse off time. Though variation of pulse off time does not show significant change in ASR even at higher values of current, but with increase of pulse on time, ASR is found to be increased at upper zone of current space.Though the higher coefficient of determination – r2 (refer Table 3) indicates the high correlation between experimental and estimated response values from the trained models (Eqs. (14) and (15)), still testing of the models with separate data sets from training sets is necessary for using these models as representatives of the EDM process outcomes.Trained models of MRR and ASR – Eqs. (14) and (15) are tested with 10 disjoint data sets obtained from separate follow-up experimental runs.Mean absolute testing errors (Tables 4 and 5) for both MRR (4.57%) and ASR (3.27%) indicate the practical adequacy of the models in their experimental ranges.In searching of best sets of internal parameters of SVM – C, ɛ and σ, MATEs in estimation of process outcomes–MRR and ASR, are to be minimized separately. Particle swarm optimization could also be employed for this optimization process. Here, model development stage is almost repeated with modified PSO algorithm. The simulated results and some performance measurements of modified PSO algorithm are compared with that of modified TLBO algorithm obtained in Section 4.2.Particle swarm optimization is an advanced evolutionary computational intelligence based stochastic optimization method for optimizing real world multimodal problems. Kennedy and Eberhart [17] proposed PSO, mimicking the natural behaviors found especially in flock of birds or school of fish for seeking their best food sources. In standard basic PSO algorithm, set of initial position and velocity vectors are to be randomly generated, inertia factor, constriction factor, social and cognitive acceleration coefficients need to be logically updated for better local exploitation and global exploration in searching operation. For improvement of the performance of PSO, numbers of modifications in estimating the internal parameters are done by theoretical analysis, mathematical inference and empirical research. In this section, internal parameters of SVM – C, ɛ and σ are searched by modified PSO.Searching ranges of C, ɛ and σ are kept same as in Section 4 (refer Table 2). Training input vectors that is control parameters of EDM – cur, tonand toffare normalized using Eq. (8). Mean absolute training errors (MATE, refer Eq. (9)) in estimation of MRR and ASR are separately considered as objective functions for searching optimum sets of C, ɛ and σ. With the corresponding training vectors and optimum sets of C, ɛ and σ, sets of Lagrange multipliers could be evaluated for each of MRR and ASR. Estimation models of MRR and ASR would be constructed separately using these sets of Lagrange multipliers.Termination criteria for stopping the searching operation is defined by suggested SR ratio (refer Eq. (10)). Simulation will be terminated when each of the SR ratio values along three dimensions – C, ɛ and σ, simultaneously go below 5%. After crossing this limit (SR ratio along each dimension <5%), searching of global optimum is further continued for 5 more iterations for better assurance in freezing of simulation process.In the present study, number of particles in swarm (n) is chosen as 20 [1]. In PSO, a widely dispersed set of particles is good for better exploitation and exploration of searching algorithm. Thus, initial position vectors of 20 particles along three dimensions – C, ɛ and σ are randomly generated within specified search space (refer Table 2) with more than 40% SR ratio (see Table A3). Velocity vectors are also randomly generated (given in Table A4) with absolute value lies within their respective range of searching space.Inertia factor influences the degree of maintaining original velocity and constriction factor affects the convergence of PSO. Here, linear variation of inertia factor and constriction factor are considered in the range [0.9, 0.4] [1]. Cognitive and social acceleration coefficients both vary linearly within same range [2.5, 0.5] but in opposite order [1]. A easy way for selecting pbestand gbestin case of multiple particles have same minimum objective function value, as proposed in Section 4, is incorporated in the present modified PSO algorithm.Adapting all the above-mentioned modifications over standard PSO algorithm, simulation steps for searching optimum sets of C, ɛ and σ by minimization of MATEs in estimation of MRR and ASR separately are discussed as follows:Step 1: Set n=20, ωinitial=0.9, ωfinal=0.4, coginitial=2.5, cogfinal=0.5, socinitial=0.5, socfinal=2.5, cfinitial=0.9, cffinal=0.4 and termination criteria as SR ratio along each dimension ≤5% for consecutive 5 iterations.Step 2: Set itermax=250. Randomly initialize the position of 20 particles that is 20 set of initial combination of C, ɛ and σ with SR ratio along each of the three dimensions >40% within search space (Table 2). Initialize velocity vectors corresponding to each of the 20 particles. Absolute value of each of the velocity vectors should lie within the range of respective search space.Step 3: Set iter=1. Each of the current particles is set to the eachpbesti. Go to Step 5.Step 4: Calculate MATE for each of the 20 particles. If the current position of ith particle has lower objective function value (MATE) than the already selectedpbesti, then replacepbestiwith new position of ith particle otherwisepbestiis kept unaltered.Step 5: Identify the particle having minimum MATE and set the corresponding particle as gbest. If number of particles having same minimum MATE then go to Step 6, otherwise go to Step 7.Step 6: Particles having same minimum objective function value, are identified and make a weighted combination of those such that the new evaluated particle must score lower than either the second minimum objective function value at current set of particles (applicable only for first iteration) or the minimum objective function value gained at last iteration. The new evaluated particle is selected as gbest.Step 7: Calculate SR ratio along all three dimensions C, ɛ and σ. Store the maximum among these three SR ratio values in a separate matrix called “DECISION”. If iter<5, go to Step 10, otherwise go to Step 8.Step 8: If the maximum of last five consecutive entries in matrix “DECISION” contains SR ratio value lower than 5% then current gbestwould be declared as the optimum setting of C, ɛ and σ, stop the simulation and minimum achievable MATE in estimation of response is calculated with obtained optimum set of C, ɛ and σ, otherwise go to Step 9.Step 9: If iter=itermax then go to step 2, restart the simulation with higher itermax otherwise go to Step 10.Step 10: Evaluate the dynamic internal parameters as(16)ωiter=ωinitial+(ωfinal−ωinitial)(itermax−1)(iter−1)(17)cogiter=coginitial+(cogfinal−coginitial)(itermax−1)(iter−1)(18)sociter=socinitial+(socfinal−socinitial)(itermax−1)(iter−1)(19)cfiter=cfinitial+(cffinal−cfinitial)(itermax−1)(iter−1)Step 11: Calculate velocity vectors as follows(20)viter+1,dk=ωiterkviterk+cogiter(rand)(pbest,dk−xdk)+sociter(rand)(gbest,d−xiter,dk)k=1(1)nandd=1(1)3Step 12: Update velocity corrected position vector as(21)xiter+1,dk=xiter,dk+cfiterviter+1,dkk=1(1)nandd=1(1)3Set iter=iter+1 and go to Step 4.Therefore, the latest gbestis decided as the optimum set of C, ɛ and σ. Modified PSO algorithm coded in MATLAB R2012a and LibSVM command line functions are used for the SVM learning process. Simulated results and some performance measurements are given in Table 6.Optimum values of C for both of the MRR and ASR are shifted toward the upper end of search space. This indicates the complexity of the model which is in favor of the stochastic behavior of EDM process. The random fluctuations could be controlled by proper choice of ɛ. Here, lower values of ɛ for both of MRR and ASR indicate that these complex models could be able to absorb the random variations adequately. Besides, small σ values for both MRR and ASR indicate that oscillatory patterns in output outside the insensitive zones are properly entrapped.With the optimum sets of C, ɛ and σ, corresponding sets Lagrange multipliers for each of the responses – MRR and ASR are calculated (given in Table A5). Models of MRR and ASR could be represented using Eq. (5). Finally achieved MATEs are determined using the representative models (Eqs. (22) and (23)) fed by their corresponding sets of Lagrange multipliers.(22)MRR:f(x)=Σi=1(1)N(αi−αi∗)K(xi,x)+bC=28.0969ε=0.3699σ=0.4642withK(xi,x)=exp(−||xi−x||2/2σ2)σ=0.4642(23)ASR:f(x)=Σi=1(1)N(αi−αi*)K(xi,x)+bC=9.00ε=0.2127σ=0.4642withK(xi,x)=exp(−||xi−x||2/2σ2)σ=0.4642Marching steps for selection of optimum sets of C, ɛ and σ separately for MRR and ASR are presented in flow chart (see Fig. 15) also. Gradual decaying pattern of MATEs in estimation of MRR and ASR are shown in Figs. 16–19.Though the higher coefficient of determination – r2 (refer Table 6) indicates the high correlation between experimental and estimated response values from the trained models (Eqs. (22) and (23)), still testing of the models with separate data sets from training sets is necessary for using these models as representatives of the EDM process outcomes.Trained models of MRR and ASR – Eqs. (22) and (23) respectively are tested with 10 disjoint data sets obtained from separate follow-up experimental runs.Mean absolute testing errors (Tables 7 and 8) for both MRR (4.80%) and ASR (3.27%) indicate the practical adequacy of the models in their experimental ranges.Simulated results and performances of modified TLBO and modified PSO in searching of optimum sets of C, ɛ and σ separately for MRR and ASR are compared in Table 9.Though the simulated optimum sets of C, ɛ and σ, training errors, r2 values and mean absolute testing errors for MRR and ASR are almost same, still simulation time in modified PSO is much higher (about 30 times for MRR and 50 times for ASR) than simulation time in modified TLBO. More simulation time causes higher computational cost. Thus, the TLBO procedure is much more efficient than the PSO. The learned system (Eqs. (14) and (15)) is subsequently used as the virtual data generator of the EDM process in the experimental domain of present work.Though SVM learning system could be able to capture the random fluctuations in EDM responses and predict the outcomes in a robust way, yet, due to lack of tractable representation of the system, it would become difficult for workshop use by engineers. Thus, using the developed SVM models as the virtual data generators, data sets of the two responses are generated with 64 different combinations of the levels of process parameters. These generated data sets are further used for developing power law models of MRR and ASR. Generated data are fitted through power law model due to its ready amenability and easy back tracking to process parameters from need based requirements of process outputs.Here, power law models (PLM) of MRR and ASR are considered as(24)MRR=ea0cura1tona2toffa3ASR=eb0curb1tonb2toffb3To estimate the unknown coefficients, natural logarithm is taken at both sides.(25)ln(MRR)=a0+a1ln(cur)+a2ln(ton)+a3ln(toff)ln(ASR)=b0+b1ln(cur)+b2ln(ton)+b3ln(toff)The coefficients are estimated by linear regression with 64 non-repeated data sets generated from SVM learned system through Eqs. (14) and (15). Using linear multivariable regression analysis, coefficients of Eq. (25) are estimated as follows.(26)ln(MRR)=−0.9421+1.5264ln(cur)+0.4418ln(ton)−0.5176ln(toff)ln(ASR)=−0.1080+0.5484ln(cur)+0.1203ln(ton)+0.0238ln(toff)Finally, the fitted power law models (PLM) are represented as(27)MRR=e−0.9421cur1.5264ton0.4418toff−0.5176ASR=e−0.1080cur0.5484ton0.1203toff0.0238The above steps are presented in Fig. 20. These models are then used for pseudo Pareto optimization within their experimental range of inputs and outputs. To maintain both the productivity and quality of products, higher MRR and lower ASR are always preferred. So, functions of MRR and ASR (Eq. (27)) are to be maximized and minimized respectively. In general, TLBO is used for minimization purpose. Reciprocal of MRR is therefore, taken to convert it into a minimization problem (Eq. (28)).(28)f1(cur,ton,toff)=1MRR(29)f2(cur,ton,toff)=ASRFor pseudo Pareto optimization, a weighted combination of these functions (Eqs. (28) and (29)) are considered. With different combination of weight factors (w1, w2), objective function is modified. Further, each of the responses is to be normalized to avoid the effect of their different scale range.(30)Objectivefunction=w1f1−(1/MRRexp)min(1/MRRexp)max–(1/MRRexp)min+w2f2–(ASRexp)min(ASRexp)max–(ASRexp)minHere, 1001 different combinations of weight factors (w1, w2) between (0, 1) are chosen subject to the condition of w1+w2=1. As a result, almost all possible practical combinations are considered and near exhaustive search is done. Two extreme combinations are nothing but the normalized f1 (for w1=1 and w2=0) and f2 (for w1=0 and w2=1) respectively. So, starting from one normalized response, as the weight factors change, the objective function gradually shifts to the other normalized response. For minimization of the objective function (Eq. (30)), modified TLBO is again employed.In every iteration during optimization, evaluations of MRR and ASR from Eq. (27) are restricted to 10% of the lower end and the upper end of experimental observations (Eq. (31)), so as to get a Pareto front very close to the exact experimental boundaries.(31)0.9(MRRexp)min≤MRR≤1.1(MRRexp)max0.9(ASRexp)min≤ASR≤1.1(ASRexp)maxThese constraints are tagged with previously discussed modified TLBO in Section 4.2. Number of learners is also kept at 20. Searching range of each process parameter is thus obtained (Table 10).To maintain the repeatability of the simulation process initial setting of learners in TLBO is to be memorized. A wide spread (SR ratio above 40%) initial settings are generated within searching range subject to the constrained criteria (Eq. (31)). The initial settings are listed in Table A6. Objective functions (refer Eq. (30)) with 1001 different weighted combinations are then minimized by modified TLBO. Each different weight factor combination gives a set of optimum MRR and ASR. Thus, 1001 set of optimum MRR and ASR are found (Fig. 21).A typical variation in marching steps to minimize objective function (Eq. (30)) with w1=0.88 and w2=0.12 under constraint Eq. (31) and corresponding change of SR ratio along cur, tonand toffare shown in Figs. 22 and 23. Obtained optimum control parameters are curopt=11.92; ton, opt=200.00; toff, opt=50.00 and corresponding minimum objective function value is 0.0849.As, almost all the points lie on a non-linear curve, a 4th order polynomial is fitted through the optimum MRR and ASR combinations and represented as(32)ASR=3.1883986596+0.2404369670(MRR)−0.0019712434(MRR)2−0.0000808326(MRR)3+0.0000018202(MRR)4This representation of ASR as a function of MRR indicates the Pareto front (Fig. 21) and suggests the limiting situation. Significance of this pseudo Pareto front is that, no other optimum setting of both the responses – MRR and ASR could be achieved simultaneously below this boundary. For a particular MRR, different ASR values are achievable, but lowest possible value lies on that pseudo Pareto front. Steps for pseudo Pareto font development are shown in Fig. 20. Pseudo Pareto front is validated (Table 11) through follow up experimental observations. Optimum ASR that is estimated from Eq. (32) lies within 10% from corresponding experimental values (Table 11).Pseudo Pareto front suggests a boundary for setting simultaneous optimum MRR and ASR. This near exhaustive search meets almost all of the practical possible requirements of customer within the experimental domain. For each combination of optimum MRR and ASR, corresponding to each point of Fig. 21, a set of optimum process parameters (cur, ton, and toff) exists, though, it is difficult to set the exact value in real world EDM machine. So, near optimum settings should be found out to get a particular MRR and ASR combination. Here, the inverse solution of this problem is outlined.Maintaining a specific productivity, products with different quality are possible, that is to meet a particular MRR value, different ASR could be achieved. Lower ASR is always preferred. The pseudo Pareto front guides to select possible minimum ASR for a particular MRR value. Using Eq. (32), optimum ASR is found for a specific MRR requirement. Yet, setting of the process parameters (cur, ton, and toff) is to be determined to get the optimum combination of MRR and ASR. Thus, a back tracking is required. Power law model is therefore, chosen for the inverse solution.Once, the desired value of MRR and corresponding estimated optimum value of ASR value are put in Eq. (26), it just becomes a set of two linear equations with three unknowns (cur, ton, and toff), and has infinite number of solutions. As, pulse off time (toff) has lowest influence on the responses compared to current (cur) and pulse on time (ton), some specific values might be set for this purpose. For the sake of simplicity, four available setting of toffare chosen (50μs, 100μs, 150μs and 200μs). Putting each of these four pulse off times, the equations becomes a set of two equations with two unknowns, and has one exact solution set. With four different pulse off times, four different sets of current and pulse on time are found. Each of them yields the same specific MRR-ASR combination. However, out of range settings are just omitted for the lack of availability in machine settings.As for example, to obtain MRR as 13.50951mm3/min, different ASR values are possible. Using pseudo Pareto front Eq. (32), lowest possible ASR value is found as 5.94μm. These two values are then put in Eq. (26) to yield Eq. (33).(33)ln(13.50951)=−0.9421+1.5264ln(cur)+0.4418ln(ton)−0.5176ln(toff)ln(5.94)=−0.1080+0.5484ln(cur)+0.1203ln(ton)+0.0238ln(toff)The equation has infinite number of solutions and so, we put either of 50μs, 100μs, 150μs or 200μs for toff. In case of 50μs, Eq. (33) becomes(34)ln(13.50951)=−0.9421+1.5264ln(cur)+0.4418ln(ton)−0.5176ln(50)ln(5.94)=−0.1080+0.5484ln(cur)+0.1203ln(ton)+0.0238ln(50)Eq. (34) has an exact set of cur: 8.21A and ton: 207.43μs. Other solutions obtained from putting toffas 100μs, 150μs and 200μs, the estimated combinations of current and pulse on time are far away from available experimental process parameters’ settings.All 64 experimental MRR values are set on pseudo Pareto front equation and corresponding optimum ASR is estimated. Following the above procedure, with four sets of pulse off time, combinations of optimum current and pulse on time are calculated. Some of the results within the experimental zone are listed in Table 12. Nearest available process parameter settings and corresponding outcomes are compared with the solution from above mentioned back tracking steps (also given in Fig. 20). Optimum process parameters lay within 15% of experimental setting and claims in favor of the practical applicability of this whole study.

@&#CONCLUSIONS@&#
