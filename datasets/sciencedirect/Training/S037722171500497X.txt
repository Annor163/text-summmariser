@&#MAIN-TITLE@&#
Do ‘big losses’ in judgmental adjustments to statistical forecasts affect experts’ behaviour?

@&#HIGHLIGHTS@&#
Forecasters’ behaviour changes after performing adjustments that lead to big losses.A big loss is more likely to be followed by another big loss: mistakes perpetuate.Forecasters tend to adjust in the direction opposite to that of the previous error.Simple correction strategies can substantially improve forecasting performance.A new measure for understanding the quality of judgmental adjustments is introduced.

@&#KEYPHRASES@&#
Forecasting,Judgment,Behavioural analytics,Decision support systems,

@&#ABSTRACT@&#
The behaviour of poker players and sports gamblers has been shown to change after winning or losing a significant amount of money on a single hand. In this paper, we explore whether there are changes in experts’ behaviour when performing judgmental adjustments to statistical forecasts and, in particular, examine the impact of ‘big losses’. We define a big loss as a judgmental adjustment that significantly decreases the forecasting accuracy compared to the baseline statistical forecast. In essence, big losses are directly linked with wrong direction or highly overshooting judgmental overrides. Using relevant behavioural theories, we empirically examine the effect of such big losses on subsequent judgmental adjustments exploiting a large multinational data set containing statistical forecasts of demand for pharmaceutical products, expert adjustments and actual sales. We then discuss the implications of our findings for the effective design of forecasting support systems, focusing on the aspects of guidance and restrictiveness.

@&#INTRODUCTION@&#
Accurate product demand forecasting is important to companies as forecasts are used in decisions relating to inventory control, production planning, purchasing, logistics, cash flow planning and other aspects of the business. A typical forecasting process includes pre-processing and analysis of the data, which are usually in the form of time series, extrapolating the series with a suitable statistical method (Petropoulos, Makridakis, Assimakopoulos, & Nikolopoulos, 2014), post-processing the statistical forecasts and monitoring and evaluating the outputs. The latter acts as feedback to inform the calculation of subsequent sets of forecasts. Often, the forecasting process is implemented within specialised forecasting software. This paper focuses on the third, post-processing, stage of the forecasting process, and more specifically on the judgmental interventions on statistical forecasts that are typically performed by demand planners and managers (Fildes & Goodwin, 2007b). Such interventions are common, with Fildes, Goodwin, Lawrence, and Nikolopoulos (2009) reporting that 91 percent of the forecasts examined in one organisation were subject to judgmental adjustments. Franses and Legerstee (2009) reported similar findings.Human adjustments of the outputs of standard forecasting methods, like exponential smoothing or ARIMA models, are primarily made for four reasons. First, managers attempt to incorporate into the forecasts the expected impact of forthcoming special events, such as promotional activities, strikes, or the launch of a competiting new product (Fildes & Goodwin, 2007a). Arguably, more formal methods that include external regressors could sometimes be used for this purpose (Huang, Fildes, & Soopramanien, 2014). However, limitations in the available quantitative data and the complexity of the models often renders judgmental adjustment as the only practical approach. Second, demand planners may tend to change statistical forecasts in order to be in-line with budgeting or politically-related targets set by senior managers (Fildes & Goodwin, 2007b). For example, in a field study Lawrence, O’Connor, and Edmundson (2000) questioned whether forecast accuracy was the primary objective of their company-based forecasters and suggested that their forecasts were heavily influenced by political choices within the company framework. Despite this, a recent survey by Fildes and Petropoulos (2015) showed that accuracy is generally the most important driver in the forecasting process, confirming earlier studies (for example see: McCarthy, Davis, Golicic, & Mentzer, 2006). Third, managers may adjust in order to gain a sense of ownership of the forecasts, possibly as a result of a lack of trust in the statistical methods, which they may regard as “black-boxes” (Önkal & Gönül, 2005). Lastly, humans are liable to confuse the signal with the noise (Harvey, 1995) and introduce unnecessary judgmental adjustments as the result of perceived systematic changes that were not captured by the statistical methods (Goodwin & Fildes, 1999).Previous studies of demand forecasting have focused on the efficiency of judgmental adjustments and the circumstances under which judgmental manipulation of statistical forecasts might be useful. Some studies proposed actions and strategies to prevent unnecessary interventions or to optimally combine statistics with judgment. See Lawrence, Goodwin, O’Connor, and Onkal (2006) and Leitner and Leopold-Wildburger (2011) for reviews of progress in judgmental forecasting. Recently, researchers in behavioural operational research (Hämäläinen, Luoma, & Saarinen, 2013) have focused on finding links between forecasting performance and experts’ behaviour (for example see: de Bruijn & Franses, 2012). Similarly, research on corporate earnings forecasting has examined the behavioural determinants of observed biases (Ramnath, Rock, & Shane, 2008). However, while demand forecasters usually have many products to forecast and obtain rapid feedback on accuracy, earnings analysts in contrast tend to focus more intensively on particular companies and observe the outcomes of their forecasts less frequently.While a number of factors may affect forecasters’ behaviour, the occurrence of a significant event or outcome in the previous period may be particularly influential. In an interesting study by Smith, Levere, and Kurtzman (2009), poker players were found to change their strategy after significant wins or losses. Big losses were followed by playing less cautiously, with players tending to be more aggressive compared to their behaviour after big wins. Similar behaviours have been found in sports gambling (Xu & Harvey, 2014) and in financial markets (Coval & Shumway, 2005; Garvey, Murphy, & Wu, 2007). Here we investigate whether forecasters’ behaviour in relation to judgmental adjustments is affected by the experience of previous poor interventions and, if the effect is damaging to accuracy, how this might be mitigated. Specifically, we address the following research questions:RQ1How do adjustments to statistical forecasts that lead to large losses affect experts’ behaviour in performing interventions for the very next period?If judgmental adjustments are unduly influenced by large losses in the previous period what corrective actions would be likely to result in improved forecasting performance?In order to deal with these two questions, we have to define what a big loss in a judgmentally adjusted forecast is. So, after a review of the background literature in Section 2, in Section 3 we propose a new way for classifying and measuring the quality of judgmental adjustments. Sections 4 and 5 attempt to answer the research questions by analysing a large empirical data set of judgmental adjustments to demand forecasts made by managers in a multinational company. Finally, the last section summarises the findings, offering conclusions as to their managerial implications.A few decades ago, most researchers discouraged managers from making judgmental adjustments to statistical forecasts because it was believed that they would generally damage accuracy (Armstrong, 1985; Carbone, Andersen, Corriveau, & Corson, 1983). These researchers found evidence that judgment was associated with a wide range of biases including over-optimism, anchoring (Eroglu & Croxton, 2010), overconfidence (Blattberg & Hoch, 1990; Kottemann, Davis, & Remus, 1994), inconsistency, and confusion of the signal with the noise (Eggleton, 1982; O’Connor, Remus, & Griggs, 1993).Mathews and Diamantopoulos were the first to show empirically through a series of company-based studies (Mathews & Diamantopoulos, 1986; 1989; 1990) that “forecast manipulation” can lead to improvements in accuracy. Interestingly, they showed that forecasters are more likely to adjust the forecasts that would have produced the largest forecast errors had the statistical forecasts remained unrevised. Other researchers have provided further evidence on the efficacy of judgmentally adjusted forecasts in macroeconomics (Donihue, 1993; McNees, 1990; Turner, 1990), accounting earnings (Brown, 1988) and business forecasting (Vere & Griffiths, 1995; Wolfe & Flores, 1990). Syntetos, Nikolopoulos, and Boylan (2010) showed that in addition to the improvements in performance as measured by traditional error metrics, judgmental adjustments of demand forecasts also result in significant reductions in inventory costs. The common factor in these studies is that when important domain knowledge is missing from the statistical forecasts, this can be integrated efficiently into the operational forecasts by applying judgmental adjustments to improve performance. However, two key elements affecting the success of an intervention are the reliability and importance of the missing information (Goodwin & Fildes, 1999) and the requirement that humans should not discount reliable statistical forecasts (Donihue, 1993).Despite these findings, there is considerable evidence from both field and laboratory studies that relatively accurate statistical forecasts are frequently judgmentally adjusted without reference to domain knowledge or its reliability (Fildes et al., 2009; Goodwin, 2000). A particularly salient cue that the forecasters are likely to be prompted with is the latest error and this raises the question: to what extent are such adjustments a behavioural response to an error resulting from a judgmental adjustment in the previous period, and in particular, a large error, as this is likely to be especially prominent? For example, do forecasters have a propensity to make a large adjustment after a previous adjustment has led to a large error, even when they have no reliable domain knowledge to justify such an intervention?The literature suggests a number of possible behavioural reactions to an adjustment in the previous period that has led to a large error. There are two reasons why the subsequent adjustment might be large: a) greater risk taking behaviour by the forecaster and b) overreaction to outcome feedback. Smith et al. (2009) found that, after large losses in games of poker, players engaged in more aggressive and riskier gambles (see also Xu & Harvey, 2014). They largely attributed this to the break-even hypothesis whereby, after sustaining a large loss, the players were prepared to take risks in an attempt to cancel out the loss. In demand forecasting a large adjustment may be a sign of risk taking behaviour. To make a large adjustment following a previous damaging intervention may be a brave action that risks further compounding both financial costs and damage to the forecaster’s reputation. Significantly, it involves an act of commission. While, not making an adjustment when it was warranted would risk one being guilty of an act of omission, there is evidence that an erroneous act of commission is seen as worse than an erroneous act of omission (Spranca, Minsk, & Baron, 1991).However, decisions in poker games differ from forecasting judgments in several ways so the break-even hypothesis might not apply in the forecasting context. First, the concept of losses and gains differs between the two concepts. In the context of demand forecasting we define a gain as a improvement in accuracy as a result of an adjustment, while a loss is a reduction in accuracy. Thus large losses are to be distinguished from large forecast errors. A forecaster’s adjustment may actually lead to a gain in accuracy compared to the statistical forecast, but a large error may still result. Second, the outcomes of poker games are independent while observations in time series are usually dependent. Intervals between poker games are likely to be shorter than the periods between successive demand forecasts so that immediate emotional reactions to poor judgments are likely to be less prevalent in demand forecasting. Also, traditional poker players are likely to be engaged in one game at a time while most demand forecasters will have the task of forecasting many series (Fildes & Goodwin, 2007b) over long periods so that a large forecast error in one series at one point in time will be less prominent than the consequences of a poor judgment in poker.In particular, errors in forecasts differ from financial losses and gains so that adjustment significantly improves accuracy will not necessarily compensate for a preceding intervention that reduced accuracy. For example, experts’ reputations are more easily lost than gained (Bonaccio & Dalal, 2006). While prospect theory (Tversky & Kahneman, 1992) suggests that people do tend to risk even further losses to try to negate current loss, the idea of forecasters making a large reckless adjustment to a forecast for an individual product in order to recover their reputation, because their previous adjustment had significantly damaged accuracy, seems less plausible. Hence, demand forecasting seems unlikely to be associated with the break-even hypothesis.Nevertheless, there are still reasons to believe that forecasters may tend to make large adjustments following large errors and these reasons are related to the direction of adjustment. First, it is known that forecasters have a tendency to overreact to outcome feedback, which will, in part, reflect the noise in a time series (Lawrence et al., 2006). For example, an outcome that is significantly higher than a forecast may be interpreted as a sign that an upward movement in the signal has occurred even when much of the error can be attributed to noise. As a consequence, the subsequent forecast may be subject to considerable upwards adjustment. This causes it to be too high and a large error in one direction is followed by a large error in the opposite direction. Where the initial large error largely results from the forecaster’s adjustment then the forecaster will incur a loss (as defined above). Assuming that forecasters receive outcome feedback on the success or otherwise of their adjustments then large adjustments following large losses would be manifested in a tendency for large adjustments to be made in the same direction as the previous large error so that a positive adjustment will follow a positive error (where error = actual−forecast) and vice versa.Alternatively, forecasters may persist in making large adjustments in the opposite direction to the previous large error. For example, a significant upwards adjustment, resulting in a negative error and a large loss may still be followed by a large upwards adjustment. This behaviour is likely to incur similarly large errors in the same direction. This may in part relate to the gambler’s fallacy where chance events are perceived to be self-correcting (Smith et al., 2009). For example, a forecaster might expect that a run of lower than expected sales figures that are judged to be due to random factors will be balanced in the future by higher sales because ‘on average half the sales are lower and half are higher than expected’. The probable result would be a persistent tendency to over-forecast as the ‘compensating’ higher-than-expected sales are awaited.However, there may be other reasons for the persistency of large errors of the same sign after big losses including an unforeseen delay in an expected special event and, when decisions are associated with asymmetric loss, a confusion of forecasts with decisions. For example, this may occur when, in order to meet customer service targets, decisions are made to hold inventory at two standard deviations above expected sales but these decisions are represented as forecasts of expected demand (Fildes et al., 2009). There is also evidence that people are reluctant to modify a previous act of commission and persist in pursuing the same action (Staw, 1976). This can occur despite evidence that continuing the action is counterproductive (Lim & O’Connor, 1995). Persistent errors of the same sign represent a rejection of outcome feedback and can result from a belief that what happened in the last period is irrelevant.Of course, there are reasons why big losses might tend to be followed by relatively small adjustments. Following the poker analogy (Smith et al., 2009), a large loss may have negative effects on a forecaster’s confidence in his or her ability to contribute to forecast accuracy for a given product. This would also lead to a propensity to avoid a large adjustment in the subsequent period. In other cases a big loss in the previous period may have been associated with an inability to forecast the effects of a special event. In the subsequent period, when no special event is anticipated, an adjustment might not be considered to be necessary.On balance, the literature suggests the following hypothesis:H1Experts are more likely to make large judgmental adjustments to forecasts in periods following big losses.H1 implies that forecasters are likely to pay attention to the latest loss and this is also fairly supported by responses to a questionnaire administered by Boulaksil and Franses (2009). Here the experts of the pharmaceutical company (on which our later analysis is based) indicated that they review their past forecasting performance when making new forecasts. Also, there is some evidence that these experts compare the performance of the statistical forecasts with that of their own forecasts. As we have seen, outcome feedback will be expected to cause forecasters to adjust in the direction suggested by their previous error. Hence, we have:H2Following a big loss, experts are more likely to make adjustments in same direction as the previous large error.The preceding hypotheses suggest that, following a large loss, large adjustments will be made based largely on the basis of outcome feedback. Since, this feedback relates only to the latest period and is contaminated by noise it is an unreliable basis for these large adjustments which are therefore likely to be seriously detrimental to forecast accuracy. Hence we hypothesise:H3Big losses are more likely in the period following a big loss.Generally, the effects of judgmental adjustments can be divided into three types, graphically depicted in Fig. 1. Wrong direction adjustments are interventions in the opposite direction compared to the sign of the deviation between real outcome and statistical forecast. These adjustments always lead to inferior accuracy in the final forecast compared to the statistical forecast. Undershoots refer to revisions that are to the correct direction, but not enough to fully explain the true outcome. Despite that, undershoots always improve forecast accuracy, as adjustments of this type decrease the difference between statistics and reality. Lastly, overshoots are interventions to the correct direction, but of magnitude larger than the ‘optimal’. Overshoots may lead to either improvements or deterioration in forecasting performance, depending on the magnitude of the adjustment.In this study we are interested in analysing the behaviour of experts in performing adjustments directly after revisions that resulted in big losses. So, we have to first answer the question ‘what is a big loss?’. However, to the best of our knowledge the definition of ‘big losses’ is absent from the literature. Arguably, it could be linked to wrong direction and overshoot adjustments, but a non-arbitrary quantitative measure of the type, quality and magnitude of a single judgmental adjustment is needed. In this section, we define a new measure for understanding judgmental revisions of statistical forecasts. This new measure enables us to analyse the behaviour of experts when performing judgmental adjustments, focusing on the cases after big losses.Let us assume that the statistical output of a forecasting method is unbiased. This means that the cumulative signed forecast error over a large number of periods is zero. In other words, any optimistic statistical point forecasts are balanced off by other pessimistic ones, and vice versa. Let us also assume that there is a deviation between the true outcome and the statistical prediction. In essence, this deviation, or statistical forecast error, is to be reconciled by an ideal judgmental adjustment. In other words, the aim of a judgmental adjustment is to alter the statistical output by the construction of an expert forecast which will be closer or even equal to the actual value. Given the aforementioned assumptions, we can regard the quality of a judgmental adjustment as a percentage of the deviation between the statistical forecast and the actual outcome.We define a scale-free measure for identifying the type, quality, and magnitude of a judgmental adjustment:(1)βt=EFt−SFtYt−SFt=FDtRDtwhere:•Yt: actual value at time t.SFt: statistical forecast at time t.EFt: expert forecast at time t produced given the statistical baseline (SFt). It may be equal to SFt. Most usually, this is used as the final (operational) forecast.RDt: real difference that needs to be reconciled or difference between actual and statistical forecast (Yt−SFt).FDt: forecasts’ difference or difference between expert forecast and statistical forecast (EFt−SFt). This is the actual judgmental adjustment.This measure gives the signed ratio of the judgmental adjustment to the error in the statistical forecast. A positive sign denotes an adjustment in the correct direction, while negative β values refer to wrong direction adjustments. The interpretation of this measure is very intuitive. For example,β=0.5means that only 50 percent of the statistical forecast error has been removed by the judgmental adjustment,β=1refers to a perfect adjustment (100 percent of the statistical forecast error is removed by judgment), whileβ=1.5indicates that judgment is over-compensating (by 50 percent) for the statistical forecast’s error. A linkage of β values with different types of adjustments is provided in Table 1. We also provide a translation of the different values of β with the effect of the adjustment on forecast accuracy, when compared to no adjustment.While some of the critical values derive directly from the definition of this new measure, the values of β for which an adjustment is translated to a ‘big loss’ rather than just a loss may differ in various applications. However, we opt for retaining the symmetry of the critical values and therefore propose that a big loss may be regarded as the result of an adjustment that deviates by more than 200 percent from a perfect adjustment (i.e. has a β value of less than−1 or greater than 3).A limitation of this measure is that it does not distinguish between upwards and downwards adjustments, which proved to be of some importance in other studies (Fildes et al., 2009; Trapero, Pedregal, Fildes, & Kourentzes, 2013). A further limitation is that β is undefined in the extreme case that the statistical forecast coincides with the actual value (the error of the statistical forecast is zero). In this case, we argue that a no-adjustment is the optimal behaviour (maximum gain), resulting in a β coefficient of 1. If an adjustment has been made, then the β coefficient will be infinite, denoting that this adjustment significantly deteriorates the accuracy (compared to the statistical forecast), so it is a big loss.Let us now see how this new measure links to the existing literature on judgmental adjustments. Franses and Legerstee (2011a) examine the effectiveness of linearly combining the statistical and expert forecasts. So, they suggest that a final forecast at time t (FFt) may be derived as:(2)FFt=αtEFt+(1−αt)SFt⇔(3)FFt=SFt+αt(EFt−SFt)⇔(4)FFt=SFt+αtFDt⇔(5)Yt−FFt=Yt−SFt−αtFDtwhere αtis the weight to be assigned on the expert forecast at time t. Obviously, the weight to be assigned on the statistical forecast should be1−αt,so that the summation of the two weights is unity.LettingYt−FFtbeing the forecast error at time t (et), Eq. (5) gives:(6)et=RDt−αtFDtFrom Eq. (6), foret=0:(7)αt=RDtFDt=1βtSo, the optimal weights for combining the statistical forecast (SFt) and the expert forecast (EFt) in order to end up with a zero forecast error are1−1βtand1βt, respectively. For example, if βtreflects a situation where only 0.4 of a required upwards adjustment has been made, a weighted average of the statistical and expert forecast using respective weights of−1.5 and 2.5 would yield a perfectly accurate forecast. It is worth mentioning that the optimal weights, along with the β values, are likely to change over time.Hyndman and Koehler (2006) define the relative error as the ratio of the forecast error deriving from a method whose performance is to be measured divided by the error of a benchmark method. For example, the relative absolute error (RAE) incurred for a statistical forecast, SFt, for time t can be defined as:(8)RAEt=|Yt−SFt||Yt−SFtb|=|et||etb|whereSFtbandetbrefer to the statistical forecast and the respective forecast error of the benchmark method. Following Davydenko and Fildes (2013), we can replace the benchmark method in Eq. (8) with the pure statistical forecast and the method in the numerator with the expert forecast (the one containing the judgmental adjustment). By doing this, we can directly compare the performance of the expert forecast relatively to the statistical forecast:(9)RAEt=|etEF||etSF|=|Yt−EFtYt−SFt|However, Eq. (9) can be rewritten as:(10)RAEt=|Yt−SFt−(EFt−SFt)Yt−SFt|=|1−FDtRDt|=|1−βt|So, the β is also linked with the relative absolute error of the expert forecast, using the statistical forecast as the benchmark. This is an important property which provides a direct relationship of the β with a recently introduced error measure (Average Relative Mean Absolute Error or AvgRelMAE, Davydenko & Fildes, 2013) for evaluating judgmentally adjusted forecasts across different periods and multiple time series.In order to examine the behaviour of experts in performing judgmental adjustments after big losses, we consider a database that was initially introduced in a study by Franses and Legerstee (2009) and was afterwards used in other studies by the same researchers (for example see Franses & Legerstee, 2010; 2011a; 2011b; 2013; Legerstee & Franses, 2014). This database contains the monthly sales of 1,101 pharmaceutical stock keeping units (SKUs). The SKUs come from 37 countries and were the responsibility of 50 different managers.The length of each series is 25 months, spanning from October 2004 to October 2006. Besides the actual sales (Y), in each period the database also contains the statistical (SF) and expert forecast (EF). However, missing values exist for some periods in specific SKUs. We focus on the 774 time series where the triplet Y, SF, and EF is available for all periods. SF is automatically provided by some forecasting software which utilises historical information (lagged sales) and individually (per series) select an optimal method from a set of alternatives (such as Box-Jenkins or Holt-Winters). The method itself and the optimised parameters may change across origins. For more details, please see Franses and Legerstee (2009, 2013).A typical time series from the database is presented in Fig. 2. This shows all different types of judgmental adjustments. For example, undershoots occurred at periods 7 and 17, overshoots are observed at periods 4 and 6, while wrong-direction adjustments are recorded for periods 3 and 5.As the target is to identify the effect of big losses in the very next judgmental adjustment, we first calculate the percentage of judgmental adjustments of each type identified in Table 1. The analysis is performed for the periodst=2,3,…,25,leaving out of the observations for the very first period. This is because periods with lag one will be used later to identify periods where judgmental adjustments occurred after big losses. To simplify the analysis, we excluded the limited number of cases whereβ=0(2 percent of the total cases), indicating that no adjustments were made despite deviations between the actual values and statistical forecasts. So, the effective sample for the current analysis contains more than 18,000 judgmental adjustments (774 time series  ×  24 periods−384 cases whereβ=0). The cases where β ∈ (0, 1] are pooled together, creating the “undershoot or spot-on” group. Similarly, for β ∈ (1, 2] in the case of small overshoots.The relative frequency (percentage of cases) of each type of adjustment is depicted in Fig. 3. Undershoots (and spot-on) are the most common type of adjustments (36 percent of the cases), with small wrong direction adjustments being the second most common. These two categories together constitute 57 percent of the adjustments. Thus, managers have a tendency to perform relatively small adjustments. So, in the majority of cases, the absolute magnitude of the interventions is not enough to remove the difference between the actual outcome and the statistical forecast. This is a result that is consistent with findings in other studies (Fildes et al., 2009).Only 49 percent of the adjustments lead to improvements in accuracy (i.e. they are undershoots, spot-on adjustments or small overshoots). This is in line with previous studies on the same database, where it was found that, when averaged across countries and categories of products, only in 43 percent of the cases expert forecasts were better than statistical ones (Franses & Legerstee, 2010). Hence, for more than half of the cases adjustments to the statistical forecasts result in deterioration in accuracy. On top of that, in 25.4 percent of cases the adjustments led to big losses (i.e. βs had values lower than−1or greater than 3).We first test H1 and investigate whether there is an association between the size of adjustment in a given period and whether or not a big loss occurred in the previous period. To control for different levels of volatility in the series we divided each absolute adjustment by the standard deviation of the statistical forecasts. We then categorised these normalised adjustments as being small if they were below the median of all adjustments in the database, large if they were between the median and 75th percentile and very large if they exceeded this percentile. Given that a few of the adjustments were extremely large, this categorisation led to a more robust analysis and reduced the influence of these extreme observations. Table 2 presents the observed frequencies, where previous moderate losses and gains (−1≤βt−1≤3) are pooled, so that big losses are kept distinct. Also, the percentage differences of the realised frequencies compared to the expected ones (assuming independence) divided by the realised numbers are given in brackets. For example, far more very large adjustments are observed in periods following an extra-large overshoot than would be expected if adjustment behaviour in the second period was independent of what happened in the first.When the chi-squared test of independence was applied to Table 2χ2=116.6with p < 0.0001 suggesting thatβt−1and the size of adjustment at t are dependent. Table 2 indicates that very large adjustments are more probable particularly after a very large overshoot in the previous period and also after a large wrong direction adjustment. Also, it is less likely that a small adjustment will occur after a big loss. This provides support for H1.We next test H2 to see whether, following a big loss, the experts adjusted in the same direction as the previous forecast error. To investigate this, the following mixed effects logistic regression equation was fitted to the 18192 observations in the database. The estimation of the model took into account that we have repeated measures for each SKU. The two tailed p-values assume thatZ=mse(m)follows a standard normal distribution, where m is the estimated coefficient and se(m) is an estimate of its standard error.ln(Π1−Π)=−0.01−0.84βt−1LW−0.84βt−1XL−0.10Lt−0.33VLtp−values:(0.000)(0.000)(0.007)(0.000)where:•Π: the probability that the adjustment at t has the same direction as the error att−1.βt−1LW=1if the loss att−1resulted from a large wrong direction adjustment (i.e.β<−1), 0 otherwise.βt−1XL=1if the loss att−1resulted from a very large overshoot (i.e. β > 3), 0 otherwise.Lt=1if the adjustment at t was large, 0 otherwise.VLt=1if the adjustment at t was very large, 0 otherwise.This logistic regression shows that the probability that the adjustment is in the same direction as the previous error is significantly reduced following large wrong direction adjustments, large overshoots and where the adjustment is large or very large. It suggests that H2 should be rejected and indicates that after a large loss forecasters are more likely to persist in adjusting forecasts in the opposite direction to that suggested by the error.We next examine the consequences of this behaviour on losses by examining the association between βtandβt−1. When the chi-squared test of independence was applied to Table 3χ2=183.7with p < 0.0001 so there appeared to be a dependence between losses in consecutive periods providing support for H3. It can be seen that there is a higher probability of large wrong direction adjustments when a wrong direction adjustment has been made in the previous period. Similarly, very large overshoots tend be more probable following very large overshoots.Table 4 provides further insights. It shows the association betweenβt−1and instances of adjustments at t that are very large in size, contrary to the direction of the previous error and result in another big loss (i.e. a large wrong direction adjustment or a very large overshoot). When the chi-squared test of independence was applied to Table 4χ2=346.4with p < 0.0001 indicating that such adjustments are much more probable following a large wrong direction adjustment or a very large overshoot. Taken together these results support the notion that, following a large loss, forecasters are more likely persist in making large adjustments in a direction contrary to that suggested by their previous error and this behaviour is likely to lead to a serious deterioration in forecast accuracy.

@&#CONCLUSIONS@&#
