@&#MAIN-TITLE@&#
Random Indexing and Modified Random Indexing based approach for extractive text summarization

@&#HIGHLIGHTS@&#
Three summarization techniques RISUM, RISUM+ and MRISUM are proposed.Cosine dissimilarity and Euclidean distance are used for proximity computation.Cosine dissimilarity unlike cosine similarity makes weighted PageRank to converge.MRISUM uses a convolution based scheme for context vector construction.MRISUM outperforms RISUM, RISUM+ and LSA+TRM.

@&#KEYPHRASES@&#
Word Space Model,Random Indexing,PageRank,Convolution,Modified Random Indexing,

@&#ABSTRACT@&#
Random Indexing based extractive text summarization has already been proposed in literature. This paper looks at the above technique in detail, and proposes several improvements. The improvements are both in terms of formation of index (word) vectors of the document, and construction of context vectors by using convolution instead of addition operation on the index vectors. Experiments have been conducted using both angular and linear distances as metrics for proximity. As a consequence, three improved versions of the algorithm, viz. RISUM, RISUM+ and MRISUM were obtained. These algorithms have been applied on DUC 2002 documents, and their comparative performance has been studied. Different ROUGE metrics have been used for performance evaluation. While RISUM and RISUM+ perform almost at par, MRISUM is found to outperform both RISUM and RISUM+ significantly. MRISUM also outperforms LSA+TRM based summarization approach. The study reveals that all the three Random Indexing based techniques proposed in this study produce consistent results when linear distance is used for measuring proximity.

@&#INTRODUCTION@&#
Extractive text summarization (Mani, 2001) aims at selecting the important sentences from the original text comprising single or multiple documents. Traditional extractive summarization techniques typically used simple heuristics, such as, word distribution in the source (Luhn, 1958), cue phrase method (Edmundson, 1969), location method (Edmundson, 1969; Hovy and Lin, 1999), discourse structure (Barzilay and Elhadad, 1999) with limited success. Subsequently, more comprehensive techniques have been developed by resorting to efficient representation of texts. Two major approaches in this regard are: Graph-based Model and Word Space Model (WSM).Graph-based Model: Here a document is represented in the form of graphs, where sentences (or paragraphs) are represented as nodes; while weighted edges represent similarity between the nodes (Salton et al., 1997). Important sentences (or paragraphs) are then extracted by analysing the graph structure. TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) are other graph-based methods proposed subsequently.Word Space Model (WSM): Here words and sentences are represented in the form of vectors in a high-dimensional vector space, called Word Space (Schütze, 1993). The major WSM-based approaches include Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997; Landauer et al., 1998), where orthogonal unary vectors11A unary vector is a vector consisting of one 1 and rest entries 0 (Sahlgren, 2006, p. 43).are used to represent words of a document. The dimension of each such word vector is equal to the number of distinct words in the document. These word vectors are used to form the initial word-by-sentence matrix. For illustration, suppose the input text consists of m distinct wordsw1,w2,…,wm, whose word vectors are m-dimensional unary vectorsw→1=[100⋯0]T,w→2=[010⋯0]T,…, andw→m=[000⋯1]T, respectively. If the input text consists of n sentencesS1,S2,…,Snand the weight assigned to the word wiin sentence Sjis aij, then the vectorS→jcomputed as∑i=1maijw→iprovides the vector representation of the sentence Sj. In this way the document can be represented as a word-by-sentence matrixA=[S→1S→2⋯S→n]of dimensionm×n. Since the number of words is much larger than the number of sentences, and normally each distinct word appears in few sentences only, A is a very high-dimensional sparse matrix. The weightsaij,i=1,…,m;j=1,…,nare usually composed of two components: a local weighting of the word wiin the sentence Sj, and a global weighting of the word wiin the whole collection (Berry et al., 1995).LSA is computationally expensive as it uses Singular Value Decomposition (SVD) (Klema and Laub, 1980) for dimension reduction of the huge and sparse word-by-sentence matrix A. Random Indexing (RI) (Sahlgren, 2005, 2006) has been proposed as an alternative to LSA. RI allows incremental learning of context information by using near-orthogonal index vectors representing words of a given text. This helps in dimension reduction by resorting to index vectors of much lower dimension in comparison with the number of words in the given text. It thereby avoids the computationally expensive dimension reduction technique typically used in LSA.The ability of these schemes to transform a linguistic model into a mathematical model made them amenable to rigorous mathematical treatments. Hassel and Sjöbergh (2006) presented a RI+Greedy Search based approach for extractive text summarization. Starting from a summary with lead sentences of the document, the algorithm at each step investigates all neighbours of the existing summary to create a better summary, where a neighbouring summary is one that can be created by removing one sentence of a given summary, and adding another. When all such summaries have been investigated, the one most similar to the original document is updated as the current best candidate. The process terminates when no summary better than the current candidate is obtained. In this approach, each text (original document/candidate summary) is assigned its own vector for semantic content, which is the (weighted) sum of all the context vectors (created using RI) of the words in the text. Similarity between two texts is measured in terms of the cosine angle between the semantic vectors of the texts. This approach, however, does not rank the individual sentences for their suitability to be included in the summary.Hybridization of WSM and graph-based ranking has also been used for single document extractive summarization. Some of these approaches include:•LSA+TRM (Yeh et al., 2005): This scheme uses LSA along with Text Relationship Map (TRM) for extracting the important sentences from the globally bushy path. The authors claim that when appropriate dimension reduction ratio is chosen, this approach outperforms keyword-based text summarization approaches.RI+PageRank (Chatterjee and Mohan, 2007): Here fixed size, near-orthogonal random index vectors have been used for implicit dimension reduction of WSM. Cosine similarity has been applied to measure similarity between sentences, and weighted PageRank algorithm has been applied to rank the sentences according to their importance. The authors claim that this scheme produces better summaries than commercially available summarizers, viz. Copernic and Word Summarizer. In the rest of the paper this system will be referred to as RI-Baseline.However, this scheme suffers from two major drawbacks: abruptness in the generated summaries, and divergence of weighted PageRank algorithm (Smith and Jönsson, 2011). In our experiments with the RI-Baseline too we found the same problems.The abruptness in summaries produced by RI-Baseline can be attributed to the fact of complete randomness in the creation of index vectors (see Appendix A for an illustrative example). In this work we propose improvements to the RI-Baseline by working on the randomization of index vectors in a restricted way. In our approach we divide the index vectors in two halves. The ‘+1's are placed randomly in the upper half, the ‘−1's are placed in the lower half. This in turn avoids mutual cancellation of ‘+1's and ‘−1's while summing the index vectors to create the context vectors of the constituent words.The divergence of weighted PageRank algorithm in RI-Baseline system is due to the use of cosine similarity measure for computing proximity between two sentences. The cosine similarity measure between the sentences produces both positive and negative similarity values. A mixture of positive and negative similarity values often makes the weighted PageRank algorithm diverge (Appendix B explains this with an example). Smith and Jönsson (2011) tackled this problem using a much larger outside Word Space as a training set, which is expensive in terms of time and space. However, in this paper we handled this problem in a very simple and effective manner by using cosine dissimilarity measure instead of cosine similarity measure.The above mentioned two modifications to RI-Baseline system results a new summarization system termed as RISUM. We made two further modifications to the scheme – which led to two successively improved versions of the scheme, termed RISUM+ and MRISUM, respectively. In RISUM+ we used the focus word as well while forming its context vector. In our experiments with DUC22Document Understanding Conferences: http://duc.nist.gov/ (Accessed January 2012).2002 documents we found both RISUM and RISUM+ to be better than RI-Baseline. In MRISUM we have used convolution (Borsellino and Poggio, 1973) as the operation to form context vectors instead of addition. We found MRISUM to be far superior to RISUM and RISUM+.The paper is organized as follows. In Section 2 we discuss the RI scheme to some detail. Section 3 discusses RISUM and RISUM+; while Section 4 deals with the MRISUM scheme. Section 5 analyses their performance. Section 6 concludes the paper.In Random Indexing each distinct word in a document is assigned a unique, randomly generated vector called the index vector of the word. These index vectors are sparse, high-dimensional, and ternary. An index vector consists of a large number of 0s and a small number (ɛ) of ±1s. Each element of it is allocated one of these values with the following probabilities:(1)+1withprobabilityε2d0withprobabilityd−εd−1withprobabilityε2dwhere d is the dimension of the index vector, and ɛ≪d. For a given document an appropriate value of d is to be chosen depending upon the number of distinct words in the document.Using RI a document is represented as a set of vectors in the Word Space with the underlying assumption that the intended semantics of a word in a document can be found from its context words in the document. The context vector of each word in the document is created by adding the index vectors of all the words occurring within a specified context of the focus word. In this respect we found that random positioning of ‘+1's and ‘−1's may result in creating two index vectors having their respective ‘+1’ and ‘−1’ occurring at the same coordinate position. This in turn creates erroneous context vectors as the ‘+1’ and the ‘−1’ cancel each other. To avoid this situation we restrict the placement of ‘+1's in the upper half positions, and ‘−1's in the lower half positions of an index vector. This ensures that the semantics is not misrepresented in the context vector.Given a document the key steps for application of RISUM to create its extractive summary are the following:•Representation of the sentences of the document in the Word Space.Construction of the Proximity Graph (of the sentences) for the document.Extraction of the summary from the graph.The aim of this step is to create vectors representing the sentences of the document. This is done in four stages:Pre-processing: At this stage the content words of the document are identified by using a list of stop words. We used ‘smart_common_words.txt’33ROUGE (Lin, 2004), the evaluation metric used by DUC also uses the same file for listing stop words.file consisting of 598 stop words for this purpose. Then Porter Stemming Algorithm (Porter, 1980) is used to remove the common morphological and inflectional endings from the words of the document.Forming index vectors: Each distinct word of the document is allotted an index vector in the d-dimensional vector space according to the scheme explained in Section 2. For the present work we have created index vectors with one ‘+1’ and one ‘−1’, and the dimension of the index vector is determined dynamically by using the formuladRI=2m, where m is the number of distinct words in the document, andstands for the ceiling function. In fact dRIis the minimum dimension that is required to accommodate all the distinct words of the document (Appendix C provides a justification of the formula). For illustration, if a document contains 500 distinct words, and the index vectors are constructed with one ‘+1’ and one ‘−1’, and the placement of ‘+1’ is restricted to first half positions and placement of ‘−1’ is restricted to second half positions of index vectors, then, the dimension of the index vectors will bedRI=2500=46. In fact, withd=dRI=46, a maximum of529(23×23)distinct words can be accommodated.Construction of context vectors: Context vectors are formed for each content word occurring in the document. For RISUM, we define the context of a word with the help of a bi-directional window of size 2 (henceforth denoted by 2+2 window) restricted to the sentence in which the word has occurred. Initially the context vector of each content word in the document is initialized to dRI-dimensional null vector. We denote ith content word of jth sentence as wi,j. The context vector of wi,jdenoted byC(wi,j)is computed using Eq. (2):(2)C(wi,j)≔C(wi,j)+∑k=−2k≠0221−|k|I(wi+k,j)whereI(wi,j)denotes the index vector of the word wi,j; and 21–|k| is the weighting factor of a word based on its proximity (in a given sentence) to the content word under consideration. The context vector in a sense is a cue to the inherent semantics of a word derived from the words co-occurring with it in the sentence in which it occurs. Since a content word in a document co-occurs with a set of words in one sentence, and with a different set of words in another sentence, the context vector of the same content word may vary from sentence to sentence.Mapping the sentences into the Word Space: All the sentences of the document are finally mapped into the Word Space with the help of the context vectors of their constituent content words. The sentence vectorS(sj)of the jth sentence is obtained by using Eq. (3):(3)S(sj)=1mj∑i=1mj(C(wi,j)−O)where mjis the number of content words in the jth sentence. HereOdenotes the central theme of the document computed as the arithmetic mean of context vectors of all the content words of the document.While forming the context vectors we have focussed on the content words only, as context vectors of stop words have negligible influence on the semantics derived from the words of a sentence. However, we found that stop words are important for determining the context of a word occurrence. Therefore, while forming the context vector of a content word, all the words within its context window (irrespective of whether it is a stop word or content word) are considered. This in turn calls for a need to subtract the mean vector from the context vectors. Subtraction of the mean vector reduces the magnitude of those context vectors which are close in direction to the central theme of the document, and increases the magnitude of context vectors which are almost in the opposite direction from the central theme. This reduces the influence of the commonly occurring words, such as the auxiliary verbs and articles, on the sentence vector (Higgins and Burstein, 2007).Once the sentences are mapped into the Word Space, the entire document is represented as a proximity graph. The nodes of the graph represent the sentences of the document, and weighted edges represent the proximity between sentences. In our scheme, we experimented separately with two fundamental ways of computing proximity between sentence vectors, viz. angular proximity and linear proximity.(a)Angular proximity is measured in terms of the angle between the two sentence vectors in the dRI-dimensional vector space. For angular proximity we have used cosine dissimilarity measure given by:(4)Cij=1=∑k=1dRIsik⋅sjk∑k=1dRIsik2⋅∑k=1dRIsjk2where Cijis the cosine dissimilarity between ith and jth sentence andS(sp)=[sp1sp2⋯spd]is the pth sentence vector.Linear proximity is measured in terms of the Euclidean distance between the two sentence vectors in the dRI-dimensional vector space using the formula given by:The edge weight ωijof the weighted graph shows the relatedness between the ith and jth nodes of the document. Depending upon the proximity measure used (Cosine or Euclidean) one gets different edge weights, and hence different proximity graphs. Weighted PageRank algorithm (Mihalcea and Tarau, 2004) is now used to determine the importance of a sentence in the whole document. If G=(V,E) is an undirected graph with the set of nodes V and set of edges E, then the weighted PageRank of a node vi, denoted by PRW(vi) is defined as:(6)PRW(vi)=(1−τ)+τ∑vj∈{vi:{vi,vj}∈E}ωijPRW(vj)∑vk∈{vi:{vi,vk}∈E}ωjkwhere ωijis the weight associated with the undirected edge {vi,vj} and ωij=ωjifor all i,j; and τ is a parameter chosen between 0 and 1. In our experiments we set τ at 0.85 as per the recommendation of Brin and Page (1998). Iterative application of the weighted PageRank algorithm on the proximity graph makes the node weights converge. In case of cosine dissimilarity measure, nodes with higher weights are considered for summary generation; while in case of Euclidean distance measure lighter nodes are picked up for the summary.The summarization scheme RISUM has an inherent drawback. While computing the context vector of a word w, it does not take into consideration the word itself. This may result in some identical context vectors for words which are semantically very different from each other. For example consider the two sentences:(i)Police arrested the man.Police awarded the man.Like RISUM, in RISUM+ too one can use both cosine and Euclidean distances. Table 1gives a comparison of the outcomes of RISUM and RISUM+ on DUC 2002 documents. Bold numbers represent the best recall scores in each category (10% and 50%) with respect to the corresponding ROUGE evaluations. It is clear from Table 1 that RISUM+ performs only marginally better than RISUM. This prompts us to investigate with the other method for computing context vectors. As mentioned in Section 1, we have used convolution for this purpose. The resulting scheme MRISUM is discussed in Section 4.Our motivation to use convolution in text summarization is influenced by the successful application of this powerful mathematical technique in various scientific applications, such as: electrical engineering (Senior, 1986), speech acoustics (Harrington and Cassidy, 1999), image processing (Castleman, 1996). Moreover, convolution as a technique has already found its application in Natural Language Processing (NLP), most of these studies are on ‘holographic representation of lexicon’ (Metcalfe, 1990; Plate, 1995; Jones et al., 2006; Jones and Mewhort, 2007; De Vine and Bruza, 2010). The convolution operation on finite vectors can be defined as follows.IfU=[u1u2⋯um]andV=[v1v2⋯vn]are two vectors of dimension m and n respectively, then their convolutionU*Vis a vectorW=[w1w2⋯wm+n−1]of dimension m+n−1 defined as follows:(7)wi=∑juj⋅vi−j+1where1≤i≤m+n−1and j ranges over all legal subscripts for ujand vi−j+1, specificallymax(1,i+1−n)≤j≤min(i,m).In our proposed scheme MRISUM, initially context vector of each word in the document is initialized to the (2dRI−1)-dimensional null vector. The context vector of ith content word of jth sentence is updated using Eq. (8):(8)C(wi,j)≔Cwi,j)+∑k=−2k≠0221−|k|[I(wi,j)∗I(wi+k,j)]Once the context vectors are generated the summary generation proceeds in the same way as RISUM.

@&#CONCLUSIONS@&#
