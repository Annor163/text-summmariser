@&#MAIN-TITLE@&#
New insights on AR order selection with information theoretic criteria based on localized estimators

@&#HIGHLIGHTS@&#
We focus on selecting the order of autoregressions when forgetting factor least-squares algorithms are used.We analyze sequentially normalized maximum likelihood and sequentially discounting normalized maximum likelihood.Some modifications of the criteria are discussed.The implications of the modifications are evaluated.Theoretical results are illustrated by numerical examples.

@&#KEYPHRASES@&#
Piecewise autoregressive processes,Forgetting factor least-squares algorithms,Sequentially normalized maximum likelihood,Sequentially discounting normalized maximum likelihood,Model selection,

@&#ABSTRACT@&#
Selecting the order of autoregressions when the parameters of the model are estimated with least-squares algorithms (LSA) is a well researched topic. This type of approach assumes implicitly that the analyzed time series is stationary, which is rarely true in practical applications. It is known since long time that, in the case of nonstationary signals, is recommended to employ forgetting factor least-squares algorithms (FF-LSA) instead of LSA. This makes necessary to modify the selection criteria originally designed for LSA in order to become compatible with FF-LSA. Sequentially normalized maximum likelihood (SNML), which is one of the newest model selection criteria, has been modified independently by two groups of researchers such that to be used in conjunction with FF-LSA. As the proposals coming from the two groups have not been compared in the previous literature, we conduct in this work a theoretical and empirical study for clarifying the relationship between the existing solutions. As part of our study, we also investigate some possibilities to further modify the criteria. Based on our findings, we provide guidance which can potentially be useful for the practitioners.

@&#INTRODUCTION@&#
The methods used in analysis of time series are traditionally classified into parametric and non-parametric. Parametric analysis assumes that the underlying stochastic process can be described by a model with a small number of parameters. A special class of models is termed autoregressive moving average (ARMA). Their importance stems from the fact that any continuous power spectral density (PSD) can be approximated arbitrarily closely by a model from this class [1,2]. Due to practical considerations, the autoregressive (AR) models are more often used than ARMA. However, before estimating the parameters of the model from the data, one should choose the number of the parameters, which in the AR case is equivalent to selecting the order of the model. The accurate approximation of the PSD can be achieved only if the order of autoregressions is properly selected.During the last decades, many solutions have been proposed for selecting the order of autoregressions, some of them using the newest advances in maximum likelihood theory, namely the normalized maximum likelihood (NML) [3–5]. As the evaluation of NML is difficult from computational viewpoint, it has been introduced a new variant of it which is called sequentially normalized maximum likelihood (SNML) [6]. In fact, the name used in [6] is sequentially normalized least squares (SNLS) and not SNML, but the reason for which we employ SNML is the following: The authors of [6] firstly published the key ideas of the new method in a conference paper [7], where they used the name SNML and since then almost all researchers employ this name when referring to the method. More importantly, the simulation study from [6], which shows the superiority of SNML in comparison with other methods, is focused on estimating the order of autoregressions. Another interesting aspect is that SNML has been originally regarded as an approximation of NML, but very recent publications revealed interesting properties of SNML, especially its relationship to other statistical methods (see [8] and the references therein).An autoregressive model with constant parameters is not appropriate for most of the time series which are measured in practical situations, where is preferable either to consider that the parameters of the model are slowly varying in time or to assume that they are piecewise constant[9, pp. 149–152]. In the latter case, an off-line procedure can be applied for identifying the piecewise AR processes which are called segments. To this end, one should detect the position of abrupt changes and estimate the AR order as well as the AR coefficients for each segment. This is currently done by employing various information theoretic criteria or by using the Bayesian approach (see [9–11] and the references therein). For instance, in [12] the segmentation is performed by introducing a hierarchical Bayesian model for which the posterior distribution is estimated with the reversible jump Markov chain Monte Carlo (MCMC) method [13]. The methodology from [12] was further extended in [14] for the joint segmentation of signals measured by different sensors like, for example, the multiple-track audio records. An algorithm based on sequential Monte Carlo was proposed in [15] for tracking the pole parametrization of time-varying autoregressions.In this work, the segmentation is not the main concern since we adopt an adaptive filtering approach in which the AR coefficients are estimated with forgetting factor least-squares algorithms (FF-LSA) [16]. Based on these estimates, we can analyze how the frequency content of the signal varies over time, but the accuracy of time-frequency analysis depends on the order of the AR model [9, pp. 140–141]. Unfortunately, the SNML cannot be applied straightforwardly for selecting the AR order because it has been devised to work in conjunction with least-squares algorithms (LSA) and not with FF-LSA. The first attempt at altering SNML such that to become compatible with FF-LSA is the one from [17]. The modified SNML is namedSNMLλ, and Ref. [17] provides an extensive theoretical and empirical study which shows the relationship betweenSNMLλand other model selection criteria.Another modified SNML criterion was introduced in [18] under the name sequentially discounting normalized maximum likelihood (SDNML), and we prefer to call itSDNMLλ[1]. The major motivation when proposing this criterion was to address the issue of real-time change point detection in time series. According to [18], a change-point is “the time point at which the statistical nature of time series suddenly changes”, and the detection of the change-point may lead to the “discovery of a novel event”. The performance ofSDNMLλ[1]was demonstrated in [18] by using artificial and real-life datasets. In both cases, the AR model was assumed to have constant order. The aim of analyzing the real-life time series was to detect the malicious software based on the computer access logs. The method from [18] has been recently applied in [19] for detecting oscillatory alpha activity in EEG signals.An improved variant ofSDNMLλ[1], which we callSDNMLλ[2], was proposed in [20,21], where the main interest is to detect the emerging topics from social network streams based on monitoring the mentioning behavior of users. In their paper, a new emerging topic has been defined as “something people feel like discussing, commenting, or forwarding the information to their friends”. Their method has been tested on artificial data sets as well as real-life data obtained from Youtube and Twitter.For the sake of completeness, we briefly explain howSDNMLλ[2]is used in [21] for change point detection in time series. Firstly,SDNMLλ[2]is computed at each time point by applying the formula from [21, Section 3.7]. Note that the same formula is reproduced below in Eq. (7). An intermediate score is obtained at each time point as an average of the most recent κSDNMLλ[2]-values, where κ is chosen by the user. The resulting scores are regarded as a new time series for whichSDNMLλ[2]is evaluated at each time point and then the results are used to compute the final score with the same type of averaging as described above. The final score is converted into binary alarms by thresholding. It is important to mention that the value of the threshold is optimized according to a procedure which was originally introduced in [22] and is also presented in [21, Algorithm 1].According to the best of our knowledge, the previous literature does not contain any comparison ofSNMLλ,SDNMLλ[1]andSDNMLλ[2]. Moreover, the performance ofSDNMLλ[1]andSDNMLλ[2]has only been tested in applications focused on change-point detection in time series, where the order of AR models involved is assumed to be known. The aim of this work is to investigate the capabilities ofSNMLλ,SDNMLλ[1]andSDNMLλ[2]in selecting the order of autoregressions.The rest of the paper is organized as follows. In the next section, we prove some theoretical results that show the relationship between the three criteria. In order to assess the theoretical results, we conduct several experiments which are described in Section 3. In the same section, we provide an interpretation of the empirical results. Conclusions are outlined in Section 4.In many applications, the coefficients of an autoregressive model(1)yi+a1yi−1+⋯+akyi−k=εiare estimated, at each time moment t, from the real-valued measurementsy1,…,ytby minimizing the sum of weighted error squares(2)∑i=1tλt−i(yi+a⊤x¯i)2,wherea=[a1,…,ak]⊤,x¯i=[yi−1,…,yi−k]⊤and the symbol⊤denotes transposition. Random variablesε1,…,εnare statistically independent and assumed to be drawn from a Gaussian distribution with zero mean and constant (but unknown) variance. The exponential weighting factor λ, which is positive and smaller than one, has the role of “forgetting” the past observations. This feature allows the application of the estimation method to nonstationary time series [23]. Efficient algorithms for the minimization of (2) constitute a well-established chapter in signal processing literature (see, for example, [16]). For the sake of brevity, instead of presenting here all the details, we rather prefer to outline in Table 1the most important quantities which are involved in the implementation of such algorithms. The nomenclature within the first column of the table is taken from [16], while the notation outlined in the second column is the same as in [17]. In the third column, we represent the relationship between the definitions from [18] and those from [17]. In relation with the initialization step, note that all measurements indexed by nonpositive integers (y0,y−1,y−2,…) are assumed to be zero. We also remark that the calculation of the time-average correlation matrix in [18] employs only the tap-input vectors whose indexes are larger thant0+1, wheret0is a parameter selected by the user such that to ensure the inverse of∑i=t0+1tλt−ix¯ix¯i⊤exists. The problem concerning the existence of the inverse is generally addressed in adaptive signal processing by using regularization [16].As already mentioned in the previous section, we focus on investigating the performance of three different criteria employed for selecting the order of an AR model (see (1)) when the coefficients are estimated by FF-LSA. First, we consider the selection rule which was originally derived in [17] by modifying the criterion from [6] such that to become compatible with FF-LSA.Given the measurementsy1,…,yn, the strategy applied in [17] is to pick-up the earliest time moment m for which the inverse of∑i=1mλm−ix¯ix¯i⊤exists, and then to select at time momentst∈{m+1,…,n}the AR order by choosing from the pre-defined set{Kmin,…,Kmax}the nonnegative integer k which minimizes the criterion:(3)SNMLλ(k)=nef2ln(1nef∑i=m+1tλt−ieˆλ,i2)+∑i=m+1tln[(1+cλ,i)λk]+12lnnef.The significance of the symbols in the equation above can be found in Table 1. Additionally,nefrepresents the effective number of observations which is defined as∑i=0t−1λiand is approximated bynef∞=1/(1−λ)when t approaches ∞. Remark in (3) that theSNMLλ-formula involves the terms1+cλ,i, wherei∈{m+1,…,t}. The term1+cλ,iequals the inverse of the conversion factor1−dλ,i, which is sometimes named the likelihood variable[16,24].For preparing our next calculations, we reproduce here the following formula which appears as Problem 4 in [16, Chapter 12]:(4)1+cλ,i=11−dλ,i=|Vλ,i−1|λk|Vλ,i|,where the operator|⋅|denotes the determinant of the matrix in the argument.After these preliminaries, we present the second AR order selection criterion which will be considered in our analysis. To this end, we re-write with our own notation the formula in [18, Eq. (8)], which represents the sequentially discounting normalized maximum likelihood (SDNML) code length for the sampleyj(m+2≤j≤n):(5)j−m2ln∑i=m+1jeˆλ,i2−j−m−12ln∑i=m+1j−1eˆλ,i2+ln11−dλ,j+lnΓ((j−m−1)/2)Γ((j−m)/2)+12lnπ.As usual,Γ(⋅)denotes Euler's integral of the second kind. In order to derive a criterion similar to the one in (3), we add together the SDNML code lengths for samplesym+2,…,yt, which leads to the following result:t−m2ln∑i=m+1teˆλ,i2−12lneˆλ,m+12−∑i=m+2tln(1−dλ,i)+lnΓ(1/2)Γ((t−m)/2)+t−m−12lnπ.We replace−ln(1−dλ,i)withln(1+cλ,i)and discard terms in the equation which do not depend on k to further simplify the above formula to:(6a)SDNMLλ[1](k)=t−m2ln∑i=m+1teˆλ,i2+∑i=m+2tln(1+cλ,i)−12lneˆλ,m+12(6b)SDNMLλ[1](k)=t−m2ln∑i=m+1teˆλ,i2+∑i=m+1tln(1+cλ,i)−12lneλ,m+12.The expression in (6b) was obtained from (6a) by using the identityeλ,m+12=(1+cλ,m+1)2eˆλ,m+12. More importantly, the formula in (6b) is the order selection criterion which corresponds to the variant of SDNML introduced in [18].However, SDNML has been further improved by replacing in (5) the sum∑i=m+1jeˆλ,i2with(1−λ)∑i=m+1jλj−ieˆλ,i2[20,21]. A similar change has been operated on the term−[(j−m−1)/2]ln∑i=m+1j−1eˆλ,i2from (5) such that the new expression for the code length ofyjis [21, Section 3.7]:(7)j−m2ln[(1−λ)∑i=m+1jλj−ieˆλ,i2]−j−m−12ln[(1−λ)∑i=m+1j−1λj−i−1eˆλ,i2]+ln11−dλ,j+lnΓ((j−m−1)/2)Γ((j−m)/2)+12lnπ1−λ+j−m−12ln1λ.We sum the code lengths forym+2,…,ytand then neglect the terms which are independent of k. The resulting criterion is:(8a)SDNMLλ[2](k)=t−m2ln∑i=m+1tλt−ieˆλ,i2+∑i=m+2tln(1+cλ,i)−12lneˆλ,m+12(8b)SDNMLλ[2](k)=t−m2ln∑i=m+1tλt−ieˆλ,i2+∑i=m+1tln(1+cλ,i)−12lneλ,m+12.The identityeλ,m+12=(1+cλ,m+1)2eˆλ,m+12was used in (8a) as we have applied it in (6a). This remark concludes the presentation of the criteriaSNMLλ,SDNMLλ[1]andSDNMLλ[2]. Similarly toSNMLλ,SDNMLλ[1]andSDNMLλ[2]can be applied for selecting the order of autoregressions. More precisely, the selected order is the one which minimizes the value of the criterion. All three criteria have been introduced in the previous literature, but the relationship between them was not investigated so far. To fill the gap, we resort next to a theoretical comparative study in whichSDNMLλ[2]is chosen to be the reference.In order to simplify the calculations, we re-write the formulas of the two criteria in a more convenient form. Since the available measurements arey1,…,yn, we taket=nin (3), which makes the effective number of observations to benef=∑i=0n−1λi. We remove the last term in (3) as it is independent of k, and then divide the rest of the terms bynef. These modifications lead to the expression below:(9a)1nefSNMLλ(k)=12ln(1nef∑i=m+1nλn−ieˆλ,i2)(9b)1nefSNMLλ(k)=+1nef∑i=m+1nln[(1+cλ,i)λk].Similarly, after dividing the formula in (8b) byn−m, we have:(10a)1n−mSDNMLλ[2](k)=12ln(1nef∑i=m+1nλn−ieˆλ,i2)(10b)1n−mSDNMLλ[2](k)=+1n−m∑i=m+1nln(1+cλ,i)−lneλ,m+122(n−m).The identities above are instrumental in proving the following result.Proposition 2.1Assumptions:(A1)The sample size is large (n≫1) and λ is close to unity.The observationsy1,…,ynare outcomes of the Gaussian stationary AR process defined in(1), for whichE[x¯ix¯i⊤]=Γ(E[⋅]denotes the expectation operator and Γ is positive definite).∑i=1nλn−ix¯ix¯i⊤≈(1−λ)−1Γ.ProofThe key point is to compute the difference between the formula in (9b) and the formula in (10b). Asn≫1, we usenef∞instead ofnefin both formulas. Then the difference can be expressed as follows:(12)1nef∞∑i=m+1nln[(1+cλ,i)λk]−1n−m∑i=m+1nln(1+cλ,i)=[1n−m∑i=m+1nlnλk]+[1nef∞−1n−m]∑i=m+1nln[(1+cλ,i)λk]=klnλ+[1nef∞−1n−m]ln∏i=m+1n|Vλ,i−1||Vλ,i|(13)=klnλ+[1nef∞−1n−m][ln|Vλ,m|+ln|∑i=1nλn−ix¯ix¯i⊤|](14)≈klnλ+[1nef∞−1n−m]ln|(1−λ)−1Γ||Vλ,m−1|(15)≈−knef∞+[1nef∞−1n−m]ln|nef∞Γ||Vλ,m−1|.Note that in (12) we have applied the identity (4), whilst in (13) we have used the definition ofVλ,nfrom Table 1. The approximation in (14) is an immediate consequence of the assumption(A3). If from the Taylor expansion of lnλ we retain only the first term, then we havelnλ≈λ−1, which leads tolnλ≈−1/nef∞. This approximation has been employed in (15).  □Remark 1The asymptotic approximation in (11) is obtained under the hypothesis that the model in (1) is the correct one and is time-invariant. These assumptions have been already used in the previous literature for analyzing the behavior of adaptive filters (see [17] and the references therein). For instance, in [17, Appendix A] has been demonstrated that λ should be selected such that2[(1−λ)/(1+λ)]k≪1in order to improve the accuracy of the approximation in(A3). In the same reference has been pointed out the equivalence between this condition and another condition found in [25,26], namely1/(1−λ)≫k. In fact, the equivalence can be easily noticed from the fact that λ is close to unity, so1+λ≈2.Remark 2According to [17, Lemma A.5], we have(16)1nef∑i=m+1nλn−ieˆλ,i2=Rλ,nnef∞[1−knef∞O(1)].The result above together with (9a) and (10a) lead to the conclusion thatSNMLλ(k)/nefandSDNMLλ[2]/(n−m)share the same goodness-of-fit (GOF) term which is mainly given byRλ,n. This explains the reason why we have computedSNMLλ(k)/nef−SDNMLλ[2]/(n−m)in (11) instead ofSNMLλ(k)−SDNMLλ[2]. Hence, the difference in (11) measures the dissimilarity between the penalty terms of the two criteria.Remark 3The result in (11) can be extended without difficulties to the case when k is the order of the fitted AR model and does not necessarily coincide with the order of the “true” AR model. Under this hypothesis, we investigate the role played by each of the terms,Δ1,Δ2andΔ3. First we observe thatΔ3depends on initialization, but its influence is negligible whenn≫m. If additionally,n≫m≫1, then assumption(A3)from Proposition 2.1 implies thatΔ2≈0. When the above conditions are satisfied, the difference in (11) reduces toΔ1=−k/nef∞, which shows thatSNMLλpenalizes the high order models less thanSDNMLλ[2]does. This happens in situations when, for example,n→∞,m→∞andn/m=10.Remark 4In practice, m is small and this should be taken into consideration also in our analysis. For small m, we can ignore the termΔ3, but we cannot neglectΔ2. In relation withΔ2, observe that the value of1/nef∞−1/(n−m)is mainly determined by λ and n when m is small. Moreover,1/nef∞−1/(n−m)>0ifn−m≫nef∞. The second factor from the definition ofΔ2depends on how well|Vλ,m−1|approximates|nef∞Γ|. It is easy to show thatE[Vλ,m−1]=(1−λm)(nef∞Γ). Therefore, for small m, it is possible thatΔ2≥0and this might compensate the effect of the negative term that we obtained fromΔ1. For illustration, we have simulated measurements according to the following AR model which is taken from [10,27]:(17)yt−1.32yt−1+0.81yt−2=εt.The driven noiseϵtis zero-mean Gaussian, with unitary variance.We have computed|Vλ,m−1|for 5000 different realizations by takingλ=0.995,m=30andn=3000. Each time seriesy1,…,ymwas produced as follows: (i) Generate 200 samples according to the difference equation in (17); (ii) Discard the first200−msamples. Then, we have evaluated the expression in (14) whenk∈{1,…,15}. In the calculation of|Vλ,m−1|we have assumed the data prior to time moment one are all zero. For computing|Γ|, we have applied the formula from [28]. For each value of k, the difference betweenSNMLλ/nefandSDNMLλ[2]/(n−m)(measured with the formula in (14)) was negative in less than2%of the time series we have simulated. Thus, it is possible that in practical situations when m is small, the penalty term ofSNMLλ/nefis larger than the penalty term ofSDNMLλ[2]/(n−m).For comparing the two criteria, we firstly prove an auxiliary result:Lemma 2.1LetSn=∑i=m+1neˆλ,i2andLλ,t=∑i=m+1tλt−ieˆλ,i2for allt∈{m+1,…,n}. The following identity holds:(18)Sn−Lλ,n=(1−λ)∑t=m+1n−1Lλ,t=1nef∞∑t=m+1n−1Lλ,t.ProofThe result is obtained straightforwardly from the equalities:Lλ,m+1=eˆλ,m+12,Lλ,m+2=λLλ,m+1+eˆλ,m+22,…,Lλ,n=λLλ,n−1+eˆλ,n2.  □This lemma is important in our analysis because, we have from (6a) and (8a) that comparingSDNMLλ[1](k)withSDNMLλ[2](k)at time moment n reduces to the following calculations:SDNMLλ[1](k)−SDNMLλ[2](k)=n−m2lnSnLλ,n=n−m2lnLλ,n+∑t=m+1n−1Lλ,t/nef∞Lλ,n=n−m2ln(1+1nef∞∑t=m+1n−1Lλ,tLλ,n)≥0.To gain more insight on the significance of the above inequality, let us consider that the assumptions from Proposition 2.1 are satisfied. If we employ in (18) the expression ofLλ,n−1/nef∞obtained by replacing n withn−1in (16), then we observe thatSn−Lλ,ndepends onRλ,n−1which is the GOF term at time momentn−1. More precisely, the major difference betweenlnLλ,nandlnSncomes from the fact that the former is influenced only by the GOF term at time moment n (Rλ,n), while the latter depends also on GOF terms corresponding to previous time moments. As the penalty term is the same for bothSDNMLλ[1](k)andSDNMLλ[2](k)(see (6b) and (8b)), it is likely thatSDNMLλ[1]has lower capabilities thanSDNMLλ[2]in detecting the changes of the model order, especially when the order decreases abruptly.For a model selection criterion, it is desirable to be invariant under the scalar change of the observations (see, for example, Remark 5.3 in [29]). In other words, we would like the criterion to select the same model when the set of measurements is{y1,…,yt}or{αy1,…,αyt}, where α is a real-valued constant. In order to verify ifSNMLλhas this property, we compute the quantities involved in (3) for the case when the observations areαy1,…,αyt. An arbitrary quantity denoted by Q when is computed fromy1,…,ytwill be denoted byQ|αwhen its evaluation is based onαy1,…,αyt. It is easy to demonstrate that(19)eˆλ,i|α=αeˆλ,i,1≤i≤t,which leads to(20)nef2lnLλ,t|αnef=nef2lnLλ,tnef+nef2lnα2.Remark in the equation above that we have employed the notation from Lemma 2.1. Furthermore, we use the identity in (4) in the same way in which we have already applied it in (12), and we get(21)∑i=m+1tln[(1+cλ,i)λk]=ln|Vλ,m|−ln|Vλ,t|,which implies that(22)∑i=m+1tln[(1+cλ,i|α)λk]=∑i=m+1tln[(1+cλ,i)λk].It follows from (3), (20) and (22) thatSNMLλ(k)|α=SNMLλ(k)+nefln|α|. The results in (19) and (21) can be used together with (6a) and (8a) to prove thatSDNMLλ[1](k)|α=SDNMLλ[1](k)+(t−m−1)ln|α|andSDNMLλ[2](k)|α=SDNMLλ[2](k)+(t−m−1)ln|α|. Since the differencesSNMLλ(k)|α−SNMLλ(k),SDNMLλ[1](k)|α−SDNMLλ[1](k)andSDNMLλ[2](k)|α−SDNMLλ[2](k)do not depend on k, we conclude that all three criteria are invariant under the scalar change of observations.We show that the scale invariance property holds also for the predictive densities criterion (PDC) which was introduced in [17] by adapting a criterion from [30] to the case when the coefficients of the AR model are estimated with FF-LSA:(23)PDCλ(k)=nef2lnRλ,tnef+12∑i=m+1tln[(1+cλ,i)λk]+12lnnef.For demonstrating the scale invariance property, it is enough to employ (22) and to observe thatRλ,t|α=α2Rλ,t.Given the importance of the identity (21) in proving the scale invariance property for all four criteria, we analyze it in more detail. Remark that it can be re-written as(24)∑i=m+1tln[(1+cλ,i)λk]=ηλ,m−ηλ,t,whereηλ,i=ln|Vλ,i|=−ln|∑j=1iλi−jx¯jx¯j⊤|,m≤i≤t.Obviously,ηλ,mdepends on the forgetting factor and the firstm−1samples. The fact thatηλ,mremains unchanged when t increases seems to be in contradiction with our methodology which relies mainly on the idea of “forgetting” the past. Based on this observation, one might consider discarding the terms depending onηλ,mfrom all the criteria, in order to improve their performance.The negative implication of this change is that the modified criteria do not have anymore the scale invariance property. This can be seen immediately by noticing that∑i=m+1tln[(1+cλ,i|α)λk]−ηλ,m|α=−ηλ,t|α=−ηλ,t+klnα2,which means the modified criteria will prefer to select lower orders after multiplying all samples with a constant α whose magnitude is larger than one. It can be also shown that the modified criteria will penalize the high order models more severely than the original criteria when the variance of the excitation in (1) is multiplied with a constant larger than one.Next we conduct experiments with simulated data for further investigating how the removing of the terms which depend onηλ,mimpacts on the performance ofSNMLλ,SDNMLλ[1],SDNMLλ[2]andPDCλ. In our experiments, we also evaluate the effect of other parameters like, for example, the forgetting factor or the variance of the driven noise.

@&#CONCLUSIONS@&#
