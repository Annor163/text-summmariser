@&#MAIN-TITLE@&#
Using mutual information in supervised temporal event detection: Application to cough detection

@&#HIGHLIGHTS@&#
A method is proposed to enhance classifier abilities for supervised event detection.The approach relies on the use of mutual information-based measures.Applicability is tested in the context of audio-based automatic cough detection.Steps of feature selection and classification are significantly improved.The resulting technique clearly outperforms the baseline and a commercial system.

@&#KEYPHRASES@&#
Event detection,Biomedical engineering,Cough detection,Mutual information,Supervised learning,

@&#ABSTRACT@&#
A large number of biomedical and surveillance applications target at identifying specific events from sensor recordings. These events can be defined as rare and relevant occurrences with a limited duration. When possible, human annotation is available and developed techniques generally adopt the standard recognition approach in which a statistical model is built for the event and non-event classes. However, the goal is not to detect the event in its complete length precisely, but rather to identify the presence of an event, which leads to an inconsistency in the standard framework. This paper proposes an approach in which labels and features are modified so that they are suited for time event detection. The technique consists of an iterative process made of two steps: finding the most discriminant segment inside each event, and synchronizing features. Both steps are performed using a mutual information-based criterion. Experiments are conducted in the context of audio-based automatic cough detection. Results show that the proposed method enhances the process of feature selection, and significantly increases the event detection capabilities compared to the baseline, providing an absolute reduction of the revised event error rate between 4 and 8%. Thanks to these improvements, the audio-only cough detection algorithm outperforms a commercial system using 4 sensors, with an absolute gain of 26% in terms of sensitivity, while preserving the same specificity performance.

@&#INTRODUCTION@&#
This paper addresses the problem of automatic event detection from time series. A proper method for event detection is of interest in various biomedical, surveillance and signal-based applications involving a sensor-based monitoring of any phenomenon. These applications encompass the characterization of molecular events [1], cough detection [2], monitoring of biomedical measures (sleep apnea [3], muscle activity [4], etc.), seismic event detection [5,6], anomaly detection [7], meteorological changes, traffic regulation [8], or event detection in social streams [9]. This paper proposes a new method of event detection based on mutual information in the context of supervised learning. The problem positioning is more precisely presented in Section 1.1. Since the validity of the proposed approach is illustrated in the frame of audio-based cough detection, the background on this issue is given in Section 1.2. The structure of the paper is finally described in Section 1.3.The detection of events from time series data is a problem which gained interest from the research community [10–12]. In most cases, studies refer to the issue of unsupervised event detection, in which the underlying phenomenon is ill-understood, making human annotation impossible [10,13]. In such a context, the goal is to identify the time points at which the system behavior change occurs. This is referred to as the change-point detection problem [10]. This is typically achieved by considering probability distributions from data in the past and present intervals, and by inspecting whether these two distributions are significantly different [12]. Semi-supervised learning has also been addressed in [14] for the detection of rare and unexpected events. This method can be applied when collecting a sufficient amount of labeled training data for supervised learning is practically infeasible (e.g. because manually annotating such a large corpus would be too time-consuming, and consequently too expensive).On the other hand, there is a large number of applications for which human annotation is available and which target at identifying specific events from sensor recordings [2,5,8]. Events can then be defined as rare and relevant occurrences, generally with a limited duration. For such an issue, vectors of features characterizing the signal are extracted at a constant sampling rate. More precisely, the signal is windowed in so-called frames where a short-term analysis is performed [15]. For each frame, a set of characteristics (also called features, measurements or attributes) are extracted. The whole signal is analyzed by shifting the frames by a constant delay, consecutive frames possibly overlapping [15]. Based on these sequences of frames, developed techniques generally adopt the standard approach in which a statistical model is built for each class to be identified (presence or not of an event) [16]. This approach nonetheless suffers from a main drawback: models built at learning stage are based on the whole event duration. However, the goal is not to detect the event in its complete length precisely, but rather to identify the presence of an event, i.e. to detect an event trigger.This induces a dramatic change from the typical training formalism: instead of building models so as to minimize the error rate at the frame level, the supervised learning has to focus on the detection ability at the event level. An extreme case to illustrate this concept would be a classification system which identifies correctly only one single frame among the several contained in each event. In the conventional framework, this would be characterized by low performance since the majority of the frames contained in the event are missed, although it leads to a perfect discrimination at the event level since the event has been properly detected.In parallel, measures derived from the Information Theory [17] have been extensively used in machine learning. Among others, the usefulness of mutual information (MI) for selecting the most relevant features in a given classification task has been proven [18]. This efficiency is nonetheless also impaired if the traditional formalism aiming at detecting events in their whole duration is considered. Indeed, the relevance of a feature at the frame level does not necessarily imply its relevance at the event level, and vice versa.The goal of this paper is precisely to investigate how MI can be used to alleviate the aforementioned drawbacks by localizing the relevant regions of interest in each event and by synchronizing features. Some concepts of the proposed approach and all our experimental results will be illustrated in the context of a particular application: audio-based cough detection.Cough is the commonest reason for which patients seek medical advice to the general practitioner (around 20% of consultations for children below 4 years old), the pediatrician and the pneumologist (for whom chronic cough represents one-third of consultations). The impact of cough, notably chronic coughing, on life quality can be important [19].In order to evaluate the cough severity, a subjective assessment is possible by making use of cough diaries, quality-of-life questionnaires or relying on a visual analog scale [20]. However, it has been shown that the subjective perception of cough is only slightly correlated with objective measurements of its severity [21]. Medical literature on this topic therefore underlines the lack of a tool allowing the automatic, objective and reliable quantification of this symptom [19]. This latter step is notably required prior to any correct evaluation of possible treatments.Some approaches have been recently proposed to address the automatic detection of cough [2]. These systems generally couple various sensors to the audio signal [2]: accelerometer, chest impedance belt, contact microphone, ECG, respiratory inductance plethysmography, etc. Although reported results are encouraging, there is currently neither standardized methods nor adequately validated, commercially available and clinically acceptable cough monitors [2,19]. Besides, following the patient in ambulatory and 24-h-long conditions (while preserving his daily habits) remains an open problem. As a result, cough quantification in the majority of hospitals is still nowadays performed by a tedious task of manual counting from audio recordings, or for validation by comparison using simultaneous video recordings.For respiratory physiologists, cough is three-phase expulsive motor act characterized by an inspiratory effort, followed by a forced expiratory effort against a closed glottis and then by opening of the glottis and rapid expiratory airflow [19]. As shown in Fig. 1, the acoustics of the cough sound is manifested by three phases, where the last one is optional [22]: an explosive phase, an intermediate period whose characteristics are similar to a forced expiration, and a voiced phase. At this point, it can then be understood that even for the detection of short events like cough: (i) it might not make sense to try to detect the cough event in its complete duration, (ii) as the signal properties vary across the duration of an event, the segments where features are particularly discriminative may not coincide. For example, some features might be relevant for detecting the explosive phase, while others would characterize the voiced phase. This might be particularly true when features arise from different sensors which might not be synchronous. This paper aims at addressing both of these issues.This paper is structured as follows. Section 2 describes the proposed approach based on information localization inside events, and feature synchronization. The experimental protocol is detailed in Section 3. Results of our evaluation are reported in Section 4 and the paper is concluded in Section 5.The general workflow of the proposed approach is presented in Fig. 2. The method starts with a sequence of feature vectors and with the initial event labels (resulting from the manual annotation). The algorithm consists of an iterative process aiming at localizing the relevant information inside the events, and at synchronizing features with each other and with labels. The motivation behind these steps is the following. First of all, the relevant segments of the events, i.e. the portions of events which are the most distinguishable from other classes, are only a partial component of the whole event duration and have to be located. Secondly, features extracted from the time signal may characterize different aspects of this latter signal, which may occur at different instants. Besides, in some applications, features might even arise from various sensors, which strengthens this issue. For these reasons, features have to be synchronized such that their relevant segments emerge at the same time, which is expected to enhance the event discrimination capabilities of the classifier.As aforementioned, the period where a feature is particularly discriminative may not perfectly coincide with the class label indicating the presence of an event. Therefore, each feature must be synchronized with the class labels by applying a certain delay. Denoting Cithe class labels at iteration i, the sequence of the jth feature, noted Xj, is synchronized with a delay dijsuch that:(1)dij=argmaxdI(Xj(d);Ci),where Xj(d) represents the feature sequence Xjon which a delay of d frames has been applied (with regard to the initial features). The mutual information I(Xj;Ci) between Xjand the classes Cican be computed as [17]:(2)I(Xj;Ci)=∑xj∑cip(xj,ci)log2p(xj,ci)p(xj)p(ci)and can be viewed as the amount of information that feature Xjconveys about the considered classification problem, i.e. the individual discrimination power of this feature alone.The goal of this step is then to find the optimal delays to apply to each feature so as to maximize its discrimination power. Fig. 3shows the evolution of the normalized MI as a function of the applied delay for a particular feature used in our cough detection application. In this case, MI is normalized by division with the entropy of the classes, such that it is bounded to 1 for a perfect classification [17,23]. In the illustration of Fig. 3, it can be observed that shifting the feature sequence by 3 frames in the past gives a normalized MI reaching 0.21, providing a clear improvement compared to the initial case where a value of only 0.14 is obtained.The key idea of the information localization step is to find the optimal duration of the relevant segments inside the events such that the discrimination abilities of the feature set are maximized. As in Section 2.1, we would like this step to be independent of any classifier and to rely only on measures derived from the information theory. Unfortunately, computing MI from data requires the estimation of probability densities, which cannot be accurately done in high dimensions. This is why the great majority of MI-based methods use measures based on up to three variables (two features plus the class label).Therefore a MI-based assessment of the relevance of a feature set whose cardinality is higher than 2 is impossible to achieve accurately in practice, as it would require a prohibitive amount of data. For this reason, several strategies (mainly of feature selection) have been proposed to deal with the issue of redundancy management, i.e. to estimate the redundancy and the amount of new relevant information of a given feature with an existing feature subset [18,23].In this paper, we use as feature selection method the following algorithm which is known [23] to provide among the best feature selection results. Let us denote F={X1,X2,…,XN} the initial set of N features, and Skthe selected subset (with Sk⊆F) of k features at step k. The method is a greedy algorithm which starts from an empty set and which selects at each step k the feature Ykmaximizing:(3)Yk=argmaxXp∈F\Sk−1[I(Xp;C)−maxYq∈Sk−1I(Xp;Yq;C)],where I(Xp;C) is the relevant information brought by feature Xpseparately (i.e. independently of any other feature), and where I(Xp;Yq;C) is the redundancy of relevant information between features Xpand Yqand can be developed as:(4)I(Xp;Yq;C)=∑xp∑yq∑cp(xp,yq,c)·log2p(xp,yq)p(xp,c)p(yq,c)p(xp,yq,c)p(xp)p(yq)p(c)In other words, the algorithm considers that the redundancy between Xpand the selected subset Sk−1 is dominated by the most redundant feature in it.In a similar spirit, we can consider in the following that, working with a set SMof M features, the relevant information RI(Xp, C, SM) brought by feature Xp(contained in SM) with regard to other selected frames can be expressed as:(5)RI(Xp,C,SM)=I(Xp;C)−maxYm∈SM\{Xp}I(Xp;Ym;C).In this context, information localization inside the events is made at iteration i by fixing the duration of the class labels Cisuch that the discrimination of these events is optimal:(6)Ci=argmaxC∑m=1MRI(Xm,C,SM).In other words, the idea is to set the duration of the new labels at iteration i in a way such that the discrimination abilities of the selected feature set SMare optimized. Finally, it is worth noticing that the sum involved in Eq. (6) does not make sense in the absolute (as this sum might excess H(C)) but allows a comparative evaluation between various feature sets (of the same cardinality), or between class labels as it is the case here.As depicted in Fig. 2, starting from the initial non-synchronized features and manually annotated class labels, an iterative process is used by repeating the steps of feature synchronization and information localization as explained here above. This is done until convergence is reached for the label duration. According to our experiments, this was always achieved in less than 3 iterations. No divergence or oscillation between two or more possible solutions were observed. Note that common schemes used in non-linear optimization to prevent these issues can be applied here straightforwardly.We here detail the protocol used throughout our experiments led in the context of audio-based cough detection. The database is first described in Section 3.1. The recognition framework is presented in Section 3.2. First a large variety of audio descriptors is extracted (Section 3.2.1), among which only the most relevant will be selected as explained in Section 3.2.2. Finally Section 3.2.3 provides details about the methodology used for classification and assessment. Methods compared in our results are summarized in Section 3.3, and metrics employed in our evaluation are introduced in Section 3.4.The study population was divided into two groups. The first set (A) included 22 healthy subjects (9 male, mean age±SD: 22.8±2.44, range: 20–28). The second set (B), consisting of 10 additional healthy subjects (5 male, mean age±SD: 23±1.45, range: 22–26) was designed to compare our system to the commercially available KarmelSonix cough counter [24]. It is worth noting that these recordings were made across several sessions and in different rooms.The aim of the database was to record various cough sounds but also some other sounds which are typically confused with cough. The participants followed a standardized protocol performed in three different situations, as detailed in Table 1: (a) sitting down in a quiet environment, (b) sitting down in a noisy environment and (c) climbing on/going down of a stepladder.This protocol was inspired by the one used to develop and evaluate the Karmelsonix system [24], with the addition of coughs at low and intermediate pulmonary volume as these kinds of cough are more difficult to detect for automatic cough counters. All recordings have been precisely manually annotated by a trained observer. In total, the database contains 2338 coughs (among which 864 are from fits of coughing), 289 forced expirations, 479 throat clearings, 289 laughters, for a total duration of 237min. Note that slight deviations were observed from the strict protocol, but that the manual annotation was made coherently.The key idea here is to extract the largest variety of audio features among which only the most relevant will be selected. These features are extracted every 12ms on a 30-ms-long frames and can be divided into two categories: features describing the spectral contents and measures of noise. We also added the first and second derivatives for each of these features in order to integrate the sound dynamics.Several features characterizing the spectral shape have been proposed in [25]. For a comprehensive description of the magnitude spectrum, we used the widely used Mel-frequency cepstral coefficients (MFCCs), the loudness associated to each Bark band [25] and the relative energy in different frequency subbands. Besides, several parameters describing the spectral shape are also employed. The spectral centroid is defined as the barycenter of the amplitude spectrum. Similarly, the spectral spread is the dispersion of the spectrum around its mean value. The spectral decrease is a perceptual measure quantifying the amount of decreasing of the spectral amplitude [25]. Finally, the spectral variation and spectral flux characterize the amount of variations of spectrum along time and are based on the normalized cross-correlation between two successive amplitude spectra [25]. Besides, we also use the energy and total loudness which are informative mainly about the presence of audio activity.Quantifying the level of noise in the signal is of interest for describing the cough sound. For this purpose, several measures are here extracted. First, the harmonic to noise ratio (HNR) is calculated in four frequency ranges. The spectral flatness measures the noisiness/sinusoidality of a spectrum (or a part of it) in four frequency bands [25]. The zero-crossing rate quantifies the number of times the signal crosses the zero axis. It is expected that the greater the amount of high-frequency noise, the higher the number of zero-crossings. The F0 value and its related measure of periodicity based on the summation of residual harmonics [26] are used as voicing measurements. As a last parameter quantifying the amount of noise in the audio signal, the Chirp group delay is a phase-based measure proposed in [27] for highlighting turbulences during glottal production.A total number of 222 features (including the first and second derivatives) has been extracted in Section 3.2.1. The goal of the feature selection algorithm is to retain the most relevant ones so as to alleviate the effect of the curse of dimensionality [28]. The algorithm of feature selection we use throughout the rest of the paper is the one briefly described in Section 2.2 and whose details can be found in [23]. Probability density functions involved in the calculation of MI measures are estimated by a histogram approach. The number of bins is set to 50 for each feature dimension, which results in a trade-off between an adequately high number for an accurate estimation, while keeping sufficient samples per bin.For each of the issues tackled across experiments, a dedicated artificial neural network (ANN) has been trained. Our ANN implementation relies on the Matlab Neural Network toolbox. Each ANN is made of a single hidden layer consisting of neurons (fixed to 16 neurons in this work, as it gave in [29] a good compromise between high performance and rather low complexity) whose activation function is an hyperbolic tangent sigmoid transfer function. The output layer is a simple neuron with a logarithmic sigmoid function suited for a binary decision.In order to provide contextual information to the ANNs, the feature vector at the considered analysis time is appended with its values 50ms and 100ms both in the past and in the future. Finally note that when testing, the posterior probability of cough detection, provided by the ANN, is smoothed by a median filtering over a period of 50ms so as to remove erroneous isolated decisions.Except for the comparison with the Karmelsonix system (Section 4.2) for which training is performed on set A and testing on set B, a leave-four-subjects-out cross-validation approach is adopted in which models are trained on 28 out of the 32 subjects are used for training, and test is performed on the four remaining. This operation is repeated 8 times so as to cover the whole database for testing, and results are averaged across them.Four methods will be compared in the following, depending upon the steps involved in the proposed approach (see Section 2):•Local=0, Synchro=0 (baseline): denotes the traditional method in which none of the proposed steps is achieved. In other words, this is the classical framework where the initial features and labels are used, and it is considered as a baseline in the following.Local=0, Synchro=1: is the technique where features are synchronized according to the original labels.Local=1, Synchro=0: performs the step of information localization using the initial features.Local=1, Synchro=1: is the proposed approach described in Section 2 for which the complete iterative process has been carried out.In addition to an assessment of these four methods, a part of the results (Section 4.2) will also be devoted to a comparison with the commercial Karmelsonix system [24].Metrics we use at the event level are the standard specificity and sensitivity measures [30]. Specificity is the complement of the so-called “false positive rate”, defined as the proportion of false alarms. A false alarm is an event which is incorrectly identified: in our case, this means that the algorithm detects a cough event when there is actually none. Similarly, sensitivity is the complement of the so-called “false negative rate”, defined as the proportion of misses. A miss is an event which is incorrectly rejected: in our case, that implies that the algorithm does not detect anything where there is actually a cough event. By varying the decision threshold θ (applied on the posterior probability outputted by the ANN), a receiver operating characteristic (ROC) curve is obtained in the specificity–sensitivity plane [30]. Two measures are then employed to characterize the performance of the ROC curve. The first one is the well-known area under curve (AUC), which reaches a value of 1 for a perfect classifier. As a second single measure summarizing the ROC curve, we defined the revised event error rate (REER) as:(7)REER=minθ(1−sens.(θ))2+(1−spec.(θ))2,and which also benefits from a straightforward interpretation: REER is the Euclidean distance in the ROC curve plane from the ideal working point characterized by values of 1 for both specificity and sensitivity. This criterion implies that an equal importance is given to both specificity and sensitivity criteria. Based on a medical advice, one of these aspects could be emphasized by weighting its importance in Eq. (7). Finally, the revised event classification rate (RECR) is defined as the complement of the REER (i.e. RECR=1−REER). As a consequence, the higher AUC and RECR (the lower REER), the better the system performance.

@&#CONCLUSIONS@&#
