@&#MAIN-TITLE@&#
Vehicle matching in smart camera networks using image projection profiles at multiple instances

@&#HIGHLIGHTS@&#
We propose a novel method for vehicle appearance modeling and matching.A key novelty is automatic collection of good observations for matching.Our method is robust in real-world scenarios.Our method outperforms more complex object matching methods.Our method is data and computationally efficient, deployable on smart cameras.

@&#KEYPHRASES@&#
Multi-camera tracking,Feature extraction,Appearance modeling,Object recognition,Traffic surveillance,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
For the purpose of traffic management and fast reaction in cases of traffic accidents it is important to timely detect potential incidents or disturbances in the traffic flow. Therefore, surveillance cameras are typically mounted along roads, for commercial reasons often with non-overlapping fields of view. As an aid to human operators, computer vision algorithms can then be used for automatic detection and tracking of vehicles in the acquired videos. Such algorithms consist of three parts: vehicle detection, tracking of vehicles in a field of view of one camera (single-camera tracking) and vehicle matching, which is used for a “handover” of vehicles between cameras, i.e., for multi-camera tracking. Typically, results of vehicle detections and single-camera tracking are bounding boxes with vehicle images being regions of interest inside the bounding boxes. Such vehicle images are referred to as vehicle detections. Vehicle detections are input to the vehicle matching.In traditional camera networks the cameras send all acquired data to the central server that performs video analysis. However, networks of smart cameras open a possibility to process the acquired video data by the cameras themselves and transfer only the obtained metadata to the other cameras and to the central server. In this context, this paper addresses the problem of matching vehicles as they are imaged by a network of stationary smart cameras with non-overlapping views. We focus on the problem of finding a computationally and data efficient, but still discriminative and robust representation of vehicle appearances that can be computed by cameras themselves and sent between cameras without sending the whole images. We also focus on finding a computationally efficient algorithm for matching such vehicle representations, suitable for execution on cameras themselves. Although the framework proposed in this paper is developed in the context of a vehicle tracking application in tunnels, where the cameras are placed to view the vehicles from relatively similar viewpoints, the basic idea and the associated techniques can be applied to vehicle tracking and matching in general, as well as to matching of other types of rigid objects.In traffic surveillance such placement of cameras is achieved in many cases: in tunnels, along roads, or even at intersections of roads by placing more cameras at the crossroads. Still, vehicle matching remains challenging due to significant appearance changes in between cameras, observation differences and inaccurate and false vehicle detections, which are all common in real-world applications. The vehicle appearance changes are due to various reasons: illumination changes in the environment (e.g., a different lighting in different areas of the environment, shadows, light reflections), changes of the vehicle pose as it moves through the multi-camera environment and turning vehicle lights on or off. The observation changes result from differences in camera settings (e.g., a scale difference due to different zoom settings). Inaccurate vehicle detections are detections of vehicles or their parts together with a part of the background. They cause misalignment of vehicle images. False detections, i.e., detections of the background as vehicle, detections of multiple vehicles as one and multiple detections of one vehicle can cause significant problems to matching algorithms, if not discarded.In our test application there are also some challenges due to a tunnel environment. Firstly, tunnels are often partly dark, artificially illuminated, which makes color unreliable information (the same holds for outdoor environments in general in cases of poor lighting conditions). Secondly, tunnels are tubular environments, so strong light reflections from the walls and ceiling can disturb cameras and “pollute” the images. Fig. 1shows images of six vehicles, acquired in a tunnel by three cameras and automatically detected using the detector proposed by Rios Cabrera et al. [1]. We used this detector in our work because it is a state-of-the-art vehicle detector suitable for tunnels, as demonstrated in [1]. It uses rectangular Haar features (similar to those proposed by Viola and Jones for face detection [2]) and a cascade of strong classifiers, which are combinations of weak classifiers selected using the Ada-Boost algorithm. The detection accuracy is increased by introducing a confidence score accumulated and normalized over all cascade stages. However, even with these improved detections there is still a significant variety of vehicle appearance and observation changes across cameras, illustrated in Fig. 1. Moreover, if vehicle images are of low to medium resolution, which is common in video surveillance, the motion blur and noise in the images are also significant. This imposes an additional challenge for extraction of robust features from vehicle images.Previous work on object appearance matching has mainly focused on extracting robust features from acquired images, so that those features remain invariant to appearance changes [3–13]. Many different features have been proposed, based on color, local features, edges, image eigenvectors or entropy, all with limited success in achieving the goal of invariance. Calculation and matching of such features is also often computationally demanding, so object comparison in real-time is typically done using only one image per camera for each object. Therefore, the accuracy of these approaches strongly depends on the quality of observations and the matching is much more challenging if observations contain disturbances like light reflections, strong shadows or occlusions.In our work we try to overcome these problems by a conceptually different approach, based on two novelties in vehicle matching. Firstly, we use simple descriptors of vehicle appearances that are easy to compute and compare, yet highly informative in low resolution images. For this purpose we model vehicle appearances using signatures that are Radon transform like projection profiles of the acquired vehicle images. Matching of the appearance models is then obtained by a simple combination of 1-D correlations in a coarse-to-fine procedure. The signatures are also used to learn scale differences between the observations from different cameras, which is important for their alignment. The second novelty is to use signatures from multiple images for creating a multiple observation appearance model and for automatic selection of good observations for matching (i.e., informative observations with few disturbances), as shown in Fig. 2. Such an appearance model enables representation of vehicles from multiple views, collected online as they move through the multi-camera environment. This is especially beneficial when vehicles change pose (e.g., by changing lane or moving away from the camera). Finally, since each vehicle has one and only one corresponding vehicle in other cameras, we employ the Hungarian algorithm to resolve ambiguities and to optimize the matching.The remainder of the paper is organized as follows. Section 2 gives related work on multi-camera tracking, object representation and matching. Section 3 briefly formulates the problem of vehicle matching. In Section 4 we propose a novel appearance model based on the vehicle signatures, together with the procedure for collection of good observations along the vehicle trajectory. Matching of vehicle appearances using the proposed appearance model is explained in Section 5. The complete matching algorithm that optimizes the association of vehicle correspondences is given in Section 6. In Section 7 we present and discuss the experimental results and finally, we conclude the paper in Section 8.

@&#CONCLUSIONS@&#
