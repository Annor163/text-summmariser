@&#MAIN-TITLE@&#
Preprocessing noisy imbalanced datasets using SMOTE enhanced with fuzzy rough prototype selection

@&#HIGHLIGHTS@&#
SMOTE is a widely used preprocessing technique for imbalanced data.SMOTE performs badly in the presence of class noise.Existing techniques focus on cleaning the data after applying SMOTE.We clean the data before and after applying SMOTE using fuzzy rough set theory.Tests on real datasets with class noise show the good performance of our method.

@&#KEYPHRASES@&#
Imbalanced classification,SMOTE,Prototype selection,Fuzzy rough set theory,

@&#ABSTRACT@&#
The Synthetic Minority Over Sampling TEchnique (SMOTE) is a widely used technique to balance imbalanced data. In this paper we focus on improving SMOTE in the presence of class noise. Many improvements of SMOTE have been proposed, mostly cleaning or improving the data after applying SMOTE. Our approach differs from these approaches by the fact that it cleans the data before applying SMOTE, such that the quality of the generated instances is better. After applying SMOTE we also carry out data cleaning, such that instances (original or introduced by SMOTE) that badly fit in the new dataset are also removed. To this goal we propose two prototype selection techniques both based on fuzzy rough set theory. The first fuzzy rough prototype selection algorithm removes noisy instances from the imbalanced dataset, the second cleans the data generated by SMOTE. An experimental evaluation shows that our method improves existing preprocessing methods for imbalanced classification, especially in the presence of noise.

@&#INTRODUCTION@&#
Imbalanced classification is an important topic in data mining, as data in real-world applications such as fraud detection [1], oil spill detection [2], text classification [3] and medical applications [4] turn out to be imbalanced. In imbalanced problems, at least one class (the minority class) is under-represented. In general imbalanced problems, there can be more than one minority class and many other classes, but these problems can all be reduced to the basic case where there is one minority class and one other class, called the majority class. The Imbalance Ratio (IR) expresses how imbalanced a dataset is, and is defined as the size of the majority class over the size of the minority class. Datasets with IR 1 are perfectly balanced, while datasets with higher IR are more imbalanced.Using the classification accuracy to measure the performance of data mining techniques for imbalanced classification is not appropriate. E.g., when a dataset has 10% minority examples and 90% majority examples, a classifier that classifies any instance to the majority class will obtain a classification accuracy of 90%, but is clearly not a good classifier for the problem at hand. Instead, one may use the Receiver Operating Characteristic (ROC) curve. It plots the ratio of correctly classified minority instances against the ratio of correctly classified majority instances. The Area Under the ROC Curve (AUC, [5]) then reflects the trade-off between correctly classified minority and majority instances.Many techniques for imbalanced classification do not focus on the classifier itself, but carry out preprocessing on the data before classification. One of the most well-known techniques is the Synthetic Minority Oversampling TEchnique (SMOTE, [6]), that introduces artificial instances using interpolation. After applying SMOTE, the dataset is balanced and common classifiers can be used.We focus on improving SMOTE in the presence of class noise. SMOTE cannot handle noise adequately and can even reinforce it, as the introduced instances might be the result of interpolation between noisy instances. As most of the existing improvements of SMOTE [7–11] clean the data after applying SMOTE, they cannot tackle the class noise problem adequately.We carry out data cleaning before applying SMOTE to improve the quality of the instances generated by SMOTE and apply data cleaning after SMOTE, such that even if some low-quality instances were introduced by SMOTE, they are removed, together with original instances that show a bad cooperation with the artificial ones. The technique is based on two ideas. The first is that when SMOTE is applied using noisy data, it is logical that the data that it generates is of low quality. The second is that, even when SMOTE is applied to clean data, it can still generate low-quality data, as SMOTE generates artificial data using a simple interpolation strategy. Therefore, we propose to carry out data cleaning before and after applying SMOTE to eliminate the potentially noisy instances.As we focus on improving the K Nearest Neighbor (KNN, [23]) classification, we use the term Prototype Selection (PS, [12]) for data cleaning in the remainder of this work. We develop two PS methods based on fuzzy rough set theory: one for imbalanced data, called Fuzzy Rough Imbalanced Prototype Selection (FRIPS) and one for normal (balanced) data, called Fuzzy Rough Balanced Prototype Selection (FRBPS). Both algorithms measure the quality of the instances using fuzzy rough set theory and remove instances for which the quality is lower than a certain threshold. To determine this threshold, FRIPS evaluates candidate thresholds by measuring the training AUC, while FRBPS measures the training accuracy. The final hybrid algorithm subsequently applies FRIPS, SMOTE and FRBPS.We develop an experimental analysis considering 65 datasets with artificial class noise, in which we analyze the components of the hybrid proposal, analyze the positive synergy among components, and show that our method improves state-of-the-art approaches.The remainder of this work is structured as follows: in Section 2, we recall the SMOTE procedure, its improvements and Ordered Weighted Average (OWA) fuzzy rough set theory. In Section 3, we introduce a measure based on OWA fuzzy rough set theory to measure the quality of instances, introduce the PS methods FRIPS and FRBPS, and show how we use them to improve SMOTE. In Section 4 we experimentally show that our method improves state-of-the-art techniques w.r.t. AUC. We conclude and suggest further research directions in Section 5.In this section we provide preliminaries that make the remainder of the paper self-contained. We first recall the SMOTE procedure and its state-of-the-art improvements. Next, we provide the reader with background on classification of noisy data. Finally, we summarize the most important notions of fuzzy rough set theory and its extension to OWA based fuzzy rough sets.The SMOTE procedure [6] is often used to balance data. First, the k nearest neighbors of all minority instances are determined. Next, artificial minority instances are introduced on the lines between the minority instances and their k nearest neighbors, until the dataset is balanced.Although SMOTE improves the classification of imbalanced data substantially, it can still been improved. E.g., SMOTE with Tomek Links (SMOTE-TL, [7]) uses SMOTE to over-sample the data and then cleans the data to get better-defined class clusters, by removing adjacent couples of instances from different classes. SMOTE with Edited Nearest Neighbors (SMOTE-ENN, [7]) over-samples the data using SMOTE and then cleans the data with the ENN prototype selection technique. SMOTE with Rough Set theory (SMOTE-RSB*, [8]) cleans the data after applying SMOTE by removing instances that do not belong to the rough lower approximation. In SMOTE with Fuzzy Rough Set Theory (SMOTE-FRST, [9]), this approach is improved by using fuzzy rough set theory and by iteratively applying SMOTE and data cleaning.Other techniques directly try to improve the quality of the instances generated by SMOTE by adjusting the SMOTE algorithm. Borderline SMOTE (SMOTE-BL1 and SMOTE-BL2, [11]) strengthens the borderline and its nearby points of the minority class by over-sampling instances in the border of the minority class. The difference between the two algorithms is that SMOTE-BL2 generates artificial instances that are closer to the minority class. Safe-level SMOTE (SMOTE-SL, [10]) assigns each minority instance a safe level before generating synthetic instances. When generating artificial instances with SMOTE, these instances are positioned closer to the instance with the largest safe level, such that the artificial instances are generated only in safe regions. A more recent approach to improve SMOTE is presented in [22]. Their algorithm, called Majority Weighted Minority Oversampling Technique (MWMOTE), first identifies the most hard-to-learn minority instances, and then assigns weights to them based on the distance between them and the nearest majority instances. The synthetic instances are then interpolated based on the most informative minority instances in a way that the generated instances lie inside a minority class cluster.When developing classification methods, one should always keep in mind that the data at hand may be noisy. There are various types of noise, which we recall here briefly.The main distinction that should be made is between attribute noise, where errors are introduced to attribute values, and class noise, where certain instances are mislabeled. In [21], three types of attribute noise are distinguished: erroneous attribute values, missing or don’t know values and incomplete or don’t care values. Following [21], class noise can either come from contradictory examples, where two instances have the same attributes but a different class label, and misclassifications, where the instances are incorrectly annotated.In this work, we focus on class noise. Instances that are generated by the SMOTE algorithm might include contradictory examples, which then should be removed by our FRBPS algorithm. To ensure that our techniques can handle misclassifications, we add artificial class noise to the data in the experimental section.In this section we review the main concepts of fuzzy rough set theory [13] and its extension to OWA based fuzzy rough set theory [14].From now on,(X,A∪{d})is a decision system, consisting of n instances X={x1, …, xn}, m attributesA={a1,…,am}and the decision attributed∉A. The value of each instance x∈X for an attributea∈Ais denoted by a(x). We assume that the attributesAare continuous and normalized, that is, for x∈X anda∈A, a(x)∈[0, 1]. The decision attribute d is nominal and assigns a class d(x) to each instance x∈X. In our context, d(x) can take two values for each x∈X: d(x)=1 if x is in the minority class and d(x)=2 if x is in the majority class.We associate a fuzzy indiscernibility relation R:X×X→[0, 1] with the decision system as follows:(1)∀x,y∈X:R(x,y)=T(1−(a(x)−a(y))2)︸a∈A,whereTis a triangular norm11Note that, as t-norms are commutative and associative, they can unambiguously be extended from [0, 1]2→[0, 1] to [0, 1]m→[0, 1] operators.(t-norm), which is an associative and commutative mappingT:[0,1]2→[0,1], increasing in both arguments and such that∀a∈[0,1]:T(a,1)=a. In this paper we use the Lukasiewicz t-norm given by∀a,b∈[0,1]:T(a,b)=max(0,a+b−1)As all attributes are normalized, R(x, y) returns a value between 0 and 1 that expresses to what extent the instances x and y are similar to each other w.r.t. the attribute setA.Fuzzy rough set theory can be used to approximate concepts (fuzzy sets) by means of the indiscernibility relation. Given a fuzzy set S, the lower approximation of S is a fuzzy set X→[0, 1] given by:(2)∀x∈X:(R↓S)(x)=miny∈XI(R(x,y),S(y))It expresses to what extent instances similar to x belong to S. Here,Iis a fuzzy implication, which is a mappingI:[0,1]2→[0,1], decreasing in the first and increasing in the second argument, and for whichI(0,0)=1,I(1,1)=1,I(0,1)=1andI(1,0)=0. In this work we use the Lukasiewicz implication, given by∀a,b∈[0,1]:I(a,b)=min(1,1−a+b)The second part of the fuzzy rough approximation of the fuzzy set S is given by the upper approximation as follows:(3)∀x∈X:(R↑S)(x)=maxy∈XT(R(x,y),S(y)),whereTis a t-norm, we use the Lukasiewicz t-norm in this work. This lower approximation expresses to what extent there exist instances that are similar to x and that belong to S.In [14], it was noted that this classical definition of fuzzy rough sets is highly susceptible to noise: the lower and upper approximation only depend on one instance, as the strict minimum and maximum operators are used. Therefore, it was proposed to soften these minimum and maximum operators by means of OWA operators [15]. Recall that, given a weight vectorW=〈w1,…,wp〉for which∑i=1pwi=1and∀i∈{1,…,p},wi∈[0,1], the OWA aggregation of p values s1, …, spis given by:(4)OWAW(s1,…,sp)=∑i=1pwiti,where ti=sjif sjis the ith largest value in s1, …, sp. This operator resembles the weighted average, but the difference is that the weights are associated with positions rather than with values.The OWA operator can be used to soften the minimum operator. Consider a weight vectorWmin=〈wmin1,…,wminp〉such that weights are increasing. Then small values will be associated with large weights, while high values will be associated with low weights. As a result, theOWAWminoperator behaves like the minimum operator, but does take into account more than one value. In this work we will use the slowly increasing additive weights 〈1, 2, …, p−1, p〉, after normalization this results in the following weights:(5)∀i∈{1,…,p}:wmini=ip(p+1)/2.Completely analogously, we can define theOWAWmaxoperator that softens the maximum operator. Its normalized weightsWmax=〈wmax1,…,wmaxp〉are given as follows:(6)∀i∈{1,…,p}:wmaxi=p−i+1p(p+1)/2.In [15] it was suggested to replace the maximum and minimum operators in the lower and upper fuzzy approximation by OWA operators, leading to the following definitions of OWA fuzzy rough sets:(7)(R↓OWAS)(x)=OWAWminy∈XI(R(x,y),S(y))(8)(R↑OWAS)(x)=OWAWmaxy∈XT(R(x,y),S(y))In this section we describe the procedures we use to improve the SMOTE algorithm. First, we discuss how we can use fuzzy rough set theory to measure the quality of instances. Next, we use this measure to improve SMOTE by means of two fuzzy rough prototype selection techniques.Fuzzy rough set theory can be used to measure the quality of instances. We first define the following fuzzy set for each x∈X:∀y∈X:[x]d(y)=1ifd(x)=d(y)0else,which can be interpreted as the set containing all instances with the same class as x.The quality of an instance x∈X can now be measured by γ:X→[0, 2], defined as follows:(9)γ(x)=(R↓OWA[x]d)(x)+(R↑OWA[x]d)(x).That is, we consider the lower and upper approximation of the class of x by means of R, and then measure how well x belongs to this upper and lower approximation. The lower approximation part expresses to what extent instances similar to x also belong to the same class as x. This means that if many instances of a different class are close to x, the lower approximation membership is low. If the instances from different classes are further away, the lower approximation membership is higher. The upper approximation of [x]dby means of R expresses to what extent there are instances close to x from the same class.To understand the intuition behind the γ measure, we consider the case without OWA generalization and rewrite it as follows:γ′(x)=(R↓[x]d)(x)+(R↑[x]d)(x)=miny∈XI(R(x,y),[x]d(y))+maxy∈XT(R(x,y),[x]d(y))=miny∈X(min(1,1−R(x,y)+[x]d(y)))+maxy∈X(max(0,R(x,y)+[x]d(y)−1))=miny∈{z∈X|d(z)≠d(y)}(1−R(x,y))+maxy∈{z∈X|d(z)=d(y)}R(x,y)=1−maxy∈{z∈X|d(z)≠d(y)}R(x,y)+maxy∈{z∈X|d(z)=d(y)}R(x,y).Here, we clearly see that an instance is penalized if there is an instance close to it belonging to a different class, and that it is rewarded if there is an instance from the same class close to it. By using OWA fuzzy rough sets, we soften this idea: the lower approximation penalizes the instance x for all instances from different classes close to it, and the worse instances (instances from different classes closest to x) are penalized more. On the other hand, the upper approximation rewards the instance x for all instances from the same class close to it, and the best instances (instances from the same class closest to it) are rewarded most.The measure defined in the previous subsection can be used to measure the quality of the instances. We want to remove the low-quality instances, while maintaining the high-quality instances. The main question is now what threshold to use to decide if instances should be removed or retained. We proceed as follows: we calculate γ(x) for all instances x and use each of these values as a threshold. Next, we consider the corresponding instance subsets and select the one with the highest training AUC or training accuracy, depending on the problem (imbalanced or balanced) at hand.First consider the quality of all instances: γ(x1), …, γ(xn). After removing duplicates from these values, we obtain the set of thresholds Tγ:Tγ={τ1,…,τk},where 1≤k≤n. We now associate a subset of instances with each threshold τ∈Tγas follows:Sτ={x∈X|γ(x)≥τ}.Next, we want to measure the quality of these subsets. We consider two cases:•In case of normal (balanced) problems, we can use the training classification error using a leave-one-out procedure, as described in Algorithm 1. In this algorithm, each instance in the training set X is classified using the candidate subset of prototypes S as a pool for possible nearest neighbors. If the instance x∈X to be classified is included in S, we temporarily omit it from S. As we use the 1NN classifier for the final classification, we also use it to calculate the training accuracy, that is, an instance is classified to the class of its nearest neighbor. The value returned by Algorithm 1 is the training accuracy, which can be used to assess the predictive quality of the candidate subset S.In case of an imbalanced dataset, it is better to use the training AUC instead of the training accuracy. For the 1NN classifier, its formula is given by:(10)1+TPTP+FN−FPFP+TN,with TP the number of correctly classified minority instances, TN the number of correctly classified majority instances, FP the number of falsely classified majority instances and FN the number of falsely classified minority instances. In Algorithm 2 it is described how to calculate the training AUC, based on the confusion matrixC=TPFNFPTNthat is updated for each classified instance.Algorithm 1trainACC, procedure to measure the training accuracy of a subset of instances using a leave-one-out approach on balanced datasets.1:input: Reduced decision system(S,A∪{d})(S⊆X).2:acc←03:forx∈Xdo4:ifx∈Sthen5:Find the nearest neighbor nn of x in S\{x}6:else7:Find the nearest neighbor nn of x in S8:end if9:ifd(nn)=d(x) then10:acc←acc+111:end if12:end for13:OutputaccAlgorithm 2trainAUC, procedure to measure the AUC of a subset of instances using a leave-one-out approach on imbalanced datasets.1:input: Reduced decision system(S,A∪{d})(S⊆X).2:Initialize confusion matrixC=00003:forx∈Xdo4:ifx∈Sthen5:Find the nearest neighbor nn of x in S\{x}6:C(d(x), d(nn))←C(d(x), d(nn))+17:else8:Find the nearest neighbor nn of x in S9:C(d(x), d(nn))←C(d(x), d(nn))+110:end if11:end for12:Output AUC based on C.Finally, the subsetsSτi1,…,Sτiswith the highest training accuracy (or training AUC in the case of imbalanced datasets) are considered. Notice that more than one subset can have the maximum training accuracy or AUC. The subset that is finally returned isSmedian(τi1,…,τis). We opt to take the median because it is a compromise between removing possibly useful instances and retaining too many instances. Moreover, experimental evaluation in [16] showed that using the median resulted in the best accuracies.As already mentioned in Section 1, we will use the term FRIPS in case the training AUC is used, and FRBPS in case the training accuracy is used.22Note that FRBPS is similar to the Fuzzy Rough Prototype Selection (FRPS) method introduced in [16]. The difference is that we improved the measure that expresses the quality of instances by using both the OWA lower and upper approximation, whereas in [16] only the fuzzy rough positive region was considered.Summarizing, these algorithms work as follows:1.Calculate γ(x1), …, γ(xn)Remove duplicates, this results in Tγ={1, …, k}, where 1≤k≤nCalculateSτ1,…,SτkCalculatequality(Sτ1),…,quality(Sτk)(quality=trainACC for FRBPS and quality=trainAUC for FRIPS)Select the subsetsSτi1,…,Sτiswith the highest qualityReturnSmedian(τi1,…,τis).These two algorithms can now be used to improve the SMOTE algorithm, as illustrated in Fig. 1. First, the entire imbalanced dataset X is cleaned using FRIPS, resulting in SFRIPS⊆X. Next, SMOTE is applied to balance this cleaned dataset and returns SSMOTE. Finally, FRBPS is applied to remove noise in the balanced dataset and returns SFRBPS⊆SSMOTE.In this section we evaluate the performance of the designed procedure. We use 65 datasets from the Keel dataset repository33www.keel.es.that are described in Table 1. The datasets have relatively many instances such that prototype selection makes sense, but the number of attributes is limited, to keep the running time of the experimentation under control. The IR of the datasets varies over the datasets: the glass1 dataset is almost balanced, while the abalone19 dataset is highly imbalanced.As FRIPS is designed to remove noise, we add artificial class noise to the data. We consider six noise levels, ranging from 0% class noise to 50% class noise in steps of 10%. For instance, when the noise level is 30%, the class labels of 30% randomly picked instances of the training data are swapped. Both the class labels of minority and majority instances can be swapped.We follow a 5-fold cross validation procedure to evaluate the performance of the algorithms. We divide each dataset in 5 folds, of which each fold is once used as test data and the remaining folds as train data, and report the average AUC over all folds. For the final classification, we always use the 1NN classifier. The parameter k in the SMOTE algorithm is fixed to 5 as in the original proposal.The full results are available on our website.44http://users.ugent.be/~nverbies/.First, we analyze the influence of FRIPS and FRBPS on the performance of 1NN and SMOTE. In Fig. 2, we show the average AUC of 4 methods for the six different noise levels. On the left hand side, we show the performance of the 1NN classifier without any preprocessing. It can be seen that the noise highly influences the AUC: there is a drop of 0.15 AUC if 50% class noise is added. The second method applies FRIPS before classifying the data. In case noise was added to the data, this improves the AUC. The more class noise is added, the better the improvement by FRIPS. The third method balances the data selected by FRIPS, which improves the classification in all cases. The last method applies FRBPS after balancing, which improves the AUC even more. Fig. 2 shows that all steps we proposed (FRIPS, SMOTE and FRBPS) do improve the results and are necessary.Next, we compare the hybrid algorithm called FRIPS-SMOTE-FRBPS with state-of-the-art preprocessing techniques for imbalanced classification. Besides the improvements of SMOTE described in Section 2.1, we also consider the SPIDER algorithm which removes majority instances that result in misclassifying instances from the minority class and over-samples minority instances that are surrounded by majority instances [17]. The last algorithm we use is SPIDER2: a two-phase version of the SPIDER algorithm presented in [18]. In the first phase noisy majority instances are removed or relabeled, in the second phase noisy minority examples are amplified.In Table 2, we show the average AUCs over all datasets for all methods. If no noise is added, the performance of FRIPS-SMOTE-FRBPS is equal to that of the best-performing methods SMOTE-TL and SMOTE-ENN. It can be derived from the full results that this result is mainly due to the worse performance of FRIPS-SMOTE-FRBPS for highly imbalanced problems. For instance, when we only consider datasets with IR≤10, FRIPS-SMOTE-FRBPS is the best performing algorithm (AUC is 0.8610 whereas the AUC of SMOTE-ENN and SMOTE-TL are 0.8583 and 0.8560, respectively).When class noise is added, FRIPS-SMOTE-FRBPS performs better than all the other algorithms. The more noise added, the larger the differences. To see if FRIPS-SMOTE-FRBPS significantly outperforms the state-of-the-art results, we perform the statistical Wilcoxon test [19,24–26]. This is a non-parametric pairwise test that aims to detect significant differences between two sample means; that is, the behavior of the two implicated algorithms in the comparison. For each comparison we compute R+, the sum of ranks of the Wilcoxon test in favor of FRIPS-SMOTE-FRBPS, R, the sum of ranks in favor of the other methods, and also the p-value obtained for the comparison. The observed values of the statistics are listed in Table 3, clearly we found very low p-values associated to the comparative analysis for high noise levels.In this paper we proposed a technique to improve SMOTE using fuzzy rough set theory, the hybrid algorithm FRIPS-SMOTE-FRBPS. First, we clean data such that SMOTE generates better artificial data using FRIPS, a fuzzy rough prototype selection method that takes into account the imbalanced character of the data. Next, we apply SMOTE to balance the data, and clean the data again, now using FRBPS, a fuzzy rough prototype selection method for balanced data.An experimental evaluation on real datasets with artificial class noise shows that our method improves state-of-the-art approaches.In the future we would like to make a more general study on how non-fuzzy rough prototype selection techniques could improve SMOTE. That is, in this work we only considered fuzzy rough approaches to prototype selection, but it might be interesting to replace these by, e.g., evolutionary approaches as it was proposed in [20].

@&#CONCLUSIONS@&#
