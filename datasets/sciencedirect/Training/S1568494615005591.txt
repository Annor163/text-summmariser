@&#MAIN-TITLE@&#
Data selection based on decision tree for SVM classification on large data sets

@&#HIGHLIGHTS@&#
This paper describes the development of an algorithm for training large data sets.The algorithm uses a first stage of SVM with a small data set.The algorithm uses decision trees to find best data points in the entire data set.DT is trained using SV and non-SV found in the first SVM stage.In the second SVM stage the training data represent all data points found by the DT.

@&#KEYPHRASES@&#
SVM,Classification,Large data sets,

@&#ABSTRACT@&#
Support Vector Machine (SVM) has important properties such as a strong mathematical background and a better generalization capability with respect to other classification methods. On the other hand, the major drawback of SVM occurs in its training phase, which is computationally expensive and highly dependent on the size of input data set. In this study, a new algorithm to speed up the training time of SVM is presented; this method selects a small and representative amount of data from data sets to improve training time of SVM. The novel method uses an induction tree to reduce the training data set for SVM, producing a very fast and high-accuracy algorithm. According to the results, the proposed algorithm produces results with similar accuracy and in a faster way than the current SVM implementations.

@&#INTRODUCTION@&#
SVM was introduced by Vapnik as a kernel based machine learning model for classification and regression task. The extraordinary generalization capability of SVM and its discriminative power have attracted the attention of data mining, pattern recognition and machine learning communities in the last years. SVM has been used as a powerful tool for solving practical binary classification problems [1–5] and regression [6,7]. However, it is well known that the major drawback of SVM occurs in its training phase [8–10]. This is because in order to train this classifier, it is necessary to solve a quadratic programming problem or QPP, which is a computationally expensive task. Solving the QPP becomes impractical when the data sets are huge because the amount of time and memory invested is between O(n2) and O(n3). In order to show how long the training time of a SVM is, Fig. 1presents a comparative between training times using three popular methods whose source code is publicly available, these are: Sequential Minimal Optimization (SMO) [11], Library of Support Vector Machines (LibSVM) [10] and Simple Support Vector Machine (SSVM) [12]. The data set used in this example contains one million points and eight features; in this case, LibSVM significantly outperforms SMO and SSVM.In this research, we propose a novel method to reduce the size of data sets based on a decision tree (DT). The latter has several interesting properties: they are tolerant to noise, their training is not costly, they are able to partition the input space into regions with low entropy, and in addition, they produce models that humans can interpret easily. By taking advantage of the ability of DTs to model class distributions with the use of partitions, new instances can be assigned to the same class of partition they belong to. Using this feature, we can detect critical instances that determine the decision boundaries of SVM. With proposed method, SVM can be enabled on large data sets. According to the experimental results there is some minor damage in some data sets. However, computational comparisons on benchmark data sets show that proposed method reduces the training time significantly in comparison with other state of the art proposals.DTs have been used for data reduction in some previous works. In [9], each disjoint region (partition) discovered by a DT is used to train a SVM. That method is based on the well-known fact that, in general, the region (hyperplane defined by the solution) found on small data sets are less elaborated than the region obtained by the entire training set [13–15]. Small learning data sets reduce decision tree complexity simplifying the decision rules. A SVM is applied to each one of these regions, so that the computational cost is less expensive compared with training a SVM and with the whole data set.A similar approach to proposed method was recently presented in [16]; it consists in reducing the number of instances to train a SVM. The central idea was to approximate the decision boundary of SVM by using DT, i.e., to capture the objects near the decision boundary; however, the strategy is different to the used in [16]; a SVM is trained twice, the first time with a few examples randomly selected. The hyperplane obtained before is used to select examples close to the Support Vectors of separating hyperplane detected by a DT. The SVM is trained again, but this time with those examples selected by the DT. In [17,18], also DT is used to select examples close to decision boundary, however, the authors does not take in consideration how to optimize the sigma parameter, and the use of the algorithm on imbalanced data sets.The method presented in this research is very efficient with large data sets. The proposed approach, firstly, eliminates nonessential data, and then it recovers the data points that are near to the decision boundaries. The training of SVM is done on those remained “useful” data. The methodology presented in this study applies a decision tree in a unique, yet powerful and effective way to reduce the SVM training time. It is important because in many applications on the real world is necessary a reasonable trade-off between accuracy and training time.The rest of the study is organized in 7 sections. In Section 2, an overview on the methods to train SVM is presented. In Section 3, we review some preliminaries of SVM and DT. Section 4 presents the proposed method in detail. The complete experimental study is carried out in Section 5. A thorough discussion is presented in Section 6. Finally, Section 7 summarizes the work and draws conclusions from it.According to the strategy used, the training methods for SVM can be categorized into data selection, decomposition, geometric, parallel implementations and heuristics. Their core ideas and the most representative algorithms are presented in this section.Data selection methods for SVM intent to decrease the size of data sets by removing the instances that do not contribute to the definition of the optimal separating hyperplane. The latter depends completely on instances which are located closest to the separation boundary [19], and correspond to those whose Lagrange multipliers are greater than zero in the Karush–Kuhn–Tucker conditions (1). These instances are called support vectors (SVs). Generally, the number of SV is a small portion compared with the size of training sets.(1)αi=0⇒yi(〈ω,xi〉+b)≥1andξi=00<αi<C⇒yi(〈ω,xi〉+b)=1andξi=0alphai=C⇒yi(〈ω,xi〉+b)≤1andξi≥0Simple random sampling (SRS) is probably the most basic strategy to reduce the size of training sets. It consists in choosing a number of instances and then train a SVM with them. The works presented in [20,21] and [22] show that uniform random sampling is the optimal robust selection scheme in terms of several statistical criteria. However, although SRS is computationally cheap, the standard deviation of classification accuracy is large in most cases [22].A more sophisticated form of this type of sampling consists in assigning to each instance a probability to be chosen. Once a number of instances is randomly selected, a SVM is trained with them. After this, the probabilities are updated, increasing those whose instances have been miss-classified [23–25]. This process is repeated several times.Some data selection methods have been developed by computing the distance between the instances and the optimal hyperplane. Several metrics for measuring distance have been used in previous works: Euclidean [26,27], Mahalanobis [28] and Hausdorff [29]. Most of the current distance-based methods are inspired on two observations: (1) the instances closest to those ones with opposite label have high chances to be SV [29] and (2) instances far from hyperplane do not contribute to the definition of decision boundary [30]. A problem with naive implementations that require to compute all distances between objects is that this task has a temporal and a spatial complexity of O(n2).The Condensed Nearest Neighbor (CNN) [31] choose instances near to class frontiers, reducing the size of training sets. However, CNN is not noise tolerant. Reduced Nearest Neighbor (RNN) [32], Selective Nearest Neighbor (SNN) [33] and Minimal Consistent Set (MCS) are methods based on CNN, and therefore, they have also problems with noisy data sets. RNN, SNN and MSC are more costly than CNN.Neighborhood properties of SV have also been exploited to reduce size of training sets. Wang and Kwong [34] used neighborhood entropy, in [35] only the patterns in the overlap region around the decision boundary are selected. The method presented in [36] follows this trend but use fuzzy C-mean clustering to select samples on the boundaries of class distribution, whereas [29] uses hyper spheres.The basis for decomposition methods lies in the fact that the training time can be reduced if only the active constraints of QPP are taken into account [37]. A similar idea to active sets methods for optimization is applied in decomposition methods. In the active set approach, two sets are used: the working set and the set of fixed variables. The optimization is made only on working set. For the case of SVM, the working set is usually composed of instances that violate the Karush–Kuhn–Tucker conditions. Apart of the proved convergence [38], a clear advantage of decomposition is that memory requirement is linear in the number of training examples; but on the other hand, because only a fraction of variables is being considered in each iteration, it is time consuming [39,40] if elements in active set are not carefully selected. One of the first decomposition methods was Chunking [30]. It consists in repetitively obtaining the maximum margin hyperplane from an amount of instances (called the chunk) and then forming a new chunk with the SV from the previous solution and some new instances. Probably the most famous decomposing algorithm is the SMO [11]. It considers the smallest size working set: only two training samples. LibSVM [10] is an algorithm based on SMO with the improvement of a more advanced mechanism of selection of working set by using the second order information method previously shown in [41]. The SVMlight[42] is another important state of the art decomposition method.Variants of SVM speed up training time of SVM at expense of loosing accuracy [39]. These methods work by changing the original QPP formulation. Most of the variants methods conclude with a system of linear equations solved efficiently if the number of features is moderate, i.e., around 100. A representative method in this category is the least square SVM (LS-SVM) [43] which changes the original QPP by using a linear system of equations that can be solved explicitly or by using a conjugate gradient method. Other important methods are the PSVM (Proximal SVM) [44] and reduced SVM (RSVM) [45].Parallel implementation of QPP is difficult because there is a strong dependence between data [46]. Most parallel methods for training SVM divide training set into independent subsets to train SVM in different processors, as in [46–48]. In [49], the kernel matrix of SVM is approximated by block diagonal matrices so that the original optimization problem can be decomposed into hundreds of sub problems, which are easy to solve in a parallel fashion. Other parallel implementations can be found in [50–54].Geometric methods for SVM are based on that computing the optimal separating hyperplane is equivalent to find the closest pair of points belonging to convex hulls [19,55,56]. Recent advances on geometric methods can be found in [57–61].Among all heuristic methods, the alpha seeding [62] consists in providing initial estimates of the αivalues for the starting of QPP. Alpha seeding seems to be a practical method to improve training time of SVM. Recently in [63] has been proposed an improvement of this method.According to the reviewed literature, there are currently just few methods that combine DT for instance selection in a similar way to the presented in this research. In [64], the algorithm patterns by ordered projections (POP) is presented. It uses projections of instances on the axis of attributes to find the minimal number of elements to represent hyper-rectangles which contain instances of the same class (entropy zero). A disadvantage of POP is that reduction of the size of data sets is very low [65].In [16], a method that approximates the decision boundary of SVM using a DT to speed up SVM in its testing phase is proposed. There are important differences with respect of our method: first, in [16], a SVM is used in some leaves of a DT. The idea is to reduce the number of test data points that require SVM's decision; and secondly, the other method is not focused in reducing the number of SV.Recently, in [66], the combination of a DT and SVM was proposed. The underlying idea is to train a SVM first, and then use the predictions of the model obtained to modify the class of examples in the training set. A DT is afterward trained using the modified set. The SVM is used as a pre-processor for improving the performance of DT, when dealing with the problem of imbalance. The major drawback of this approach is its inability to deal with large data sets.This section presents some basic concepts of the SVM and decision trees. A more detailed explanation can be found in [67–69] respectively.Considering binary classification case, it is assumed that a training set is given as:(2)(x1,y1),(x2,y2),…,(xn,yn)i.e.X={xi,yi}i=1nwhere xi∈Rdand yi∈(+1, −1). The optimal separating hyperplane is given by(3)y=sign[wTφ(xi)+b]which is obtained by solving the following QPP(4)minw,bJ(w)=12wwT+c∑i=1nξisubjectto:yi[wTφ(xi)+b]≥1−ξiwhere ξiare slack variables used to tolerate miss-classifications ξi>0, i=1, 2, …n, c>0. Eq. (4) is equivalent to the QPP 5 which is a dual problem with the Lagrange Multipliers αi>0,(5)maxαiJ(w)=−12∑i=1,j=1nαiyiαjyjK〈xi·xj〉+∑i=1nαisubjectto:∑i=1nαiyi=0,C≥αi≥0,i=1,2,…,nWith C>0, αi=[α1, α2, …, αn]T, αi≥0, i=1, 2, …, n, are the coefficients corresponding to xi, all the xiwith nonzero αiare called SV. The function K is the kernel which must satisfy the Mercer Condition [8]. The resulting optimal decision function is defined as(6)yi=sign∑i=1nαiyiK〈xi·xj〉+bwhere x=[x1, x2, …, xn] is the input data, αiand yiare Lagrange multipliers. An unpreviously seen sample x can be classified using (6). There is a Lagrangian multiplier α for each training point. When the maximum margin of the hyperplane is founded, only the closed points of the hyperplane satisfy α>0. These points are called support vectors (SV), the other points satisfy α=0, so the solution vector is sparse. Where b is determined by Kuhn–Tucker conditions:(7)∂L∂w=0,w=∑i=1nαiyiφ(xi)∂L∂b=0,∑i=1nαiyi=0∂L∂ξi=0,αi−c≥0αi{yi[wTφ(xi)+b]≥1−ξi}=0Decision tree techniques have become one of the most popular tools for classification and regression. One of the attractiveness of decision trees is that they can extract knowledge by inferring human understandable rules from data. Typically, a decision tree algorithm begins with the entire set of data, splits the data into two or more subsets based on the values of attributes and then repeatedly splits each subset into finer subsets until the split size reaches an appropriate level. The entire modeling process can be represented in a tree structure and the model generated can be summarized as a set of “if-then” rules. Decision trees are easy to interpret, computationally inexpensive, and capable of work with noisy data.We used the C4.5 algorithm [69] to construct the decision tree data filter. The selection of the best attribute node is based on the Gain ratio (S, A) where S is a set of records and A is an attribute. This gain defines the expected reduction in entropy due to sorting on A. It is calculated as(8)Gain(S,A)=Entropy(S)−∑v∈Value(A)|Sv||S|Entropy(Sv)In general, if we are given a probability distribution P=(p1, p2, …, pn) then the information conveyed by this distribution, which is called Entropy of P and it is given byEntropy(P)=∑i=1n−pilog2pi

@&#CONCLUSIONS@&#
In this paper was presented a new algorithm for training SVM for classification. The proposed algorithm works very fast even with large data sets and outperforms the current state of the art SVM implementations without substantial reduction of accuracy.The proposed method applies a data filter based on a decision tree that scans the entire data and obtains a small subset of data points. The proposed approach is conceptually simple, easy to implement, and for some experiments faster than other SVM training algorithms (SMO, LibSVM and SSVM) because it avoids calculating margins for nonsupport vector examples. The superiority of the proposed algorithm is experimentally demonstrated for some real life data sets in terms of training time.The results of experiments on synthetic and real data sets show that the proposed approach is scalable for very large data sets while generating high classification accuracy.As future work, proposed method ideas can be adapted to stream data mining SDM, which is a relatively new research area in data mining. The challenge in SDM for SVM rests in that the underlying model must consider concept drift phenomena and SV must be updated very quickly.