@&#MAIN-TITLE@&#
A soft computing method to predict sludge volume index based on a recurrent self-organizing neural network

@&#HIGHLIGHTS@&#
The structure of RSONN can be self-organized based on the contributions of each hidden node, which uses not only the past states but also the current states.The appropriately adjusted learning rates of RSONN is derived based on the Lyapunov stability theorem. Moreover, the convergence of the proposed RSONN is discussed.An experimental hardware, including the proposed soft computing method is set up. The experimental results have confirmed that the soft computing method exhibits satisfactory predicting performance for SVI.

@&#KEYPHRASES@&#
Soft computing method,Recurrent self-organizing neural network,Sludge volume index,Prediction,Sensitivity analysis,

@&#ABSTRACT@&#
In this paper, a soft computing method, based on a recurrent self-organizing neural network (RSONN) is proposed for predicting the sludge volume index (SVI) in the wastewater treatment process (WWTP). For this soft computing method, a growing and pruning method is developed to tune the structure of RSONN by the sensitivity analysis (SA) of hidden nodes. The redundant hidden nodes will be removed and the new hidden nodes will be inserted when the SA values of hidden nodes meet the criteria. Then, the structure of RSONN is able to be self-organized to maintain the prediction accuracy. Moreover, the convergence of RSONN is discussed in both the self-organizing phase and the phase following the modification of the structure for the soft computing method. Finally, the proposed soft computing method has been tested and compared to other algorithms by applying it to the problem of predicting SVI in WWTP. Experimental results demonstrate its effectiveness of achieving considerably better predicting performance for SVI values.

@&#INTRODUCTION@&#
Sludge bulking, due to excessive growth of filamentous bacteria, is one of the most common solid separation problem in the wastewater treatment process (WWTP) [1,2]. The term bulking is defined as the phenomenon in which the density of activated sludge tends to decrease as a consequence of the overabundance of filamentous bacteria [3]. Despite the fact that sludge bulking has a significant impact on activated sludge system performance, the kinetic properties of filamentous bacteria cannot be considered satisfactory, which makes a mechanistic description of sludge bulking in a general model difficult [4].To circumvent the lack of a mechanistic understanding of WWTP, the soft computing methods are becoming more common for predicting the water qualities in WWTP, even though they are still not as widespread as, for instance, in the process industry, where the soft computing methods are extensively exploited [5,6]. In the soft computing methods, the input information and the internal model are used to return output information associated with the hard-to-measure primary variables [7]. For the soft computing methods, models based on the artificial neural network (ANN) and the fuzzy system are the most popular ones [8,9]. An ANN contains the nodes arranged in layers and connected to each other. The input–output relationship is encoded in the connection weights, which are adapted to minimize the error between the network outputs and the targets [10,11]. In general, the structures of ANN can be classified as feedforward neural networks (FNNs) and recurrent neural networks (RNNs) [12,13]. And most of the publications in the soft computing methods use FNNs with backpropagation or its other variations for modeling nonlinear system. However, one of the main drawbacks of FNNs is that they are essentially static input-to-output maps and their capability for representing nonlinear systems is limited [14]. RNNs, on the other hand, are capable of providing long-range predictions even in the presence of measurement noise and own some other advantages due to their recurrent structures [15]. Therefore, RNNs, which have the capability of capturing various plant nonlinearities are better suitable to predict nonlinear systems for the soft computing methods [16]. However, the number of hidden nodes in RNNs is often assumed to be constant [17]. In fact, if the number of hidden nodes is too large, the computational loading is heavy and the generality is poor; on the other hand, if the number of hidden nodes is too small, the learning performance may not be good enough to achieve the desired performance [18,19]. For these reasons, it is crucial to optimize the structures of RNNs to improve their performance [20].Generally, there are three ways for designing the structures of RNNs: growing, pruning, and a combination of growing and pruning. The growing methods start with a small size network and add new hidden nodes to the network in the training procedure. For example, Subrahmanya et al. proposed a constructive method for simultaneous structure and parameters training of RNNs based on the particle swarm optimization and covariance matrix adaptation strategy [21]. An Elman-based self-organizing RBF neural network (ESRNN) is designed through the simultaneous structure and parameter learning methods by the Mahalanobis distance approach, in [22]. Furthermore, some other growing RNNs have already been proposed, in [23–25] and have been shown to outperform fixed RNNs. However, it is possible that the constructive methods may overestimate the number of hidden nodes required [21]. Moreover, the constructive algorithms tend to build small networks due to their incremental learning nature [26]. On the other side, the pruning algorithms are used to delete unnecessary hidden nodes and connections from the oversized RNNs. The pruning algorithms start with an oversized network and remove unnecessary network parameters, either during training or after convergence to a local minimum [27]. In the literature, many different pruning methods have emerged. Zheng et al. introduce an interplay of spike timing-dependent plasticity and different homeostatic mechanisms for designing a class of recurrent self-organizing networks [28]. Leung et al. proposed a local extended Kalman filtering training approach for pruning RNNs [29]. And a recursive Bayesian Levenberg-Marquardt algorithm, which can evaluate the significance of hidden nodes, is proposed to organize RNNs in [30]. Moreover, some other pruning RNNs have also been proposed in [31,32]. Although these pruning RNNs have several benefits, such as little redundancy in connections or nodes and few parameters. The computational cost is higher since the majority of the training time is spent on networks larger than necessary [33].Recently, the combination of growing and pruning has become popular for designing RNNs. The growing–pruning algorithm is a hybrid approach, which executes a constructive phase (or a pruning phase) first, and then a pruning phase (or a constructive phase). Therefore, the growing-pruning algorithms effectively enable address some drawbacks of the independent growing or pruning algorithms. Based on the firing strength and the significance of each hidden node, Hsu et al. proposed a dynamic recurrent fuzzy neural network, which can generate or prune the hidden nodes dynamically online for achieving the optimal neural structure [34]. The results show that the dynamic recurrent fuzzy neural network can improve the performance. El-Sousy developed a recurrent self-evolving fuzzy neural network (RRSEFNN), which can perform the structure and parameter learning concurrently [35]. The simulation and experimental results confirm that the proposed RRSEFNN grants robust performance regardless of load disturbances. However, many constraint conditions should be assumed for these self-organizing RNNs in the learning process. Wang et al. proposed an optimization algorithm for an Elman-type RNN, combining the advantages of discrete particle swarm optimization algorithm and improved particle swarm optimization algorithm [36]. The results show that this self-organizing Elman-type RNN can obtain a low architectural complexity and good generalization performance. However, the convergence of the neural network is not discussed and the training process may stop in the early phases. Moreover, to deal with the dynamics of RNNs by introducing the information process of neural systems at the network and whole brain levels, in recent years, many biological methods have been developed for adjusting the network structures underlying observed information signals and the effective connectivity [37,38]. However, in general, how to design an effective method for RNNs with the efficient performance is still a challenge [39,40].With above mentioned motivations in this paper, a growing and pruning RNN called recurrent self-organizing neural network (RSONN) is developed for the soft computing method to predict the values of sludge volume index (SVI) in WWTP. The proposed RSONN-based soft computing method contains three major contributions as follows. First, the growing and pruning method is developed by the sensitivity analysis (SA) method, which uses not only the past states but also the current states for computing the contribution of hidden nodes (previous studies provided strong evidence that compatibly used past and current states to be more desirable for optimizing the structures of RNNs [41,42]). Then, the structure of RSONN can be self-organized based on the contributions of each hidden node. Secondly, to guarantee the successful applications of the proposed soft computing method, the appropriately adjust learning rates of RSONN is derived based on the Lyapunov stability theorem. Moreover, the convergence of the proposed RSONN is discussed in this paper. Thirdly, an experimental hardware, including the proposed soft computing method is set up. A real WWTP is employed to test the proposed RSONN-based soft computing method. The experimental results have confirmed that the soft computing method exhibits satisfactory predicting performance for SVI.The rest of this paper is organized as follows: An overview of RNN is given in Section II. The growing and pruning method as well as the parameter learning algorithm of RSONN are described in Section III. The convergence of RSONN is discussed in Section IV. In addition, the appropriately adjust learning rates are given. In section V, an experimental hardware is set up. And the proposed RSONN-based soft computing method is applied to predicting the SVI values in a real WWTP. Finally, the conclusion is given in section VI.A RNN contains input layer, hidden layer, and output layer, as well as feedback connection weights, activation functions, and interconnection weights. In this study, the proposed RNN is designed by the combination of the locally recurrent and globally feed forward structure. The dynamic properties are achieved by utilizing the internal feedbacks as shown in Fig. 1.For a clear understanding of the computational model for the proposed RNN through the proposed structure, the mathematical function of each node is described as follows:Input layer: There are M nodes, which represent the input variables in this layer. The output values of each node can be described as:(1)ui(t)=xi(t),where ui(t) is the ith output value at time t, i=1, 2,…, M, and the input vector isx(t)=[x1(t), x2(t), …, xM(t)].Hidden layer: each node in hidden layer connects with the input nodes and output node of RNN. The output of hidden nodes are:(2)vj(t)=f(∑i=1Mwij1(t)ui(t)+vj1(t)),i=1,2,…,M;j=1,2,…,H,wheref(x)=(1+e−x)−1is the activation function, w1 ij(t) is the input weight connecting the ith node in the input layer with the jth hidden node, H is the number of hidden nodes, and v1 j(t) is the feedback value, which is given by:(3)vj1(t)=wj2(t)vj(t−1),where vj(t−1) is the output value of the hidden layer at time t−1, w2 j(t) is the self-feedback weight of the jth hidden node.Output layer: There is only one node in this layer, the output is:(4)y(t)=∑j=1Hwj3(t)vj(t),where y(t) is the output value of output layer at time t, w3 j(t) is the output weight of the jth hidden node.Moreover, for estimating the performance, the root mean squared error (RMSE) is defined:(5)E(t)=12t∑p=1t(yd(p)−y(p))2,where y(p) and yd(p) are the network output and the desired output at time p (p=1, 2, …, t).The basic idea of RSONN is to utilize the SA method for designing a suitable network structure. In fact, the SA method, which is well-known in the engineering literature, is to ascertain how the model output depends on its input factors [43]. Inspired by the advantages of the SA method, the significances of hidden nodes can be obtained by their input parameters. Then, the hidden nodes can be pruned or the new hidden nodes can be added in RNNs.In order to describe the internal feedback dynamic characteristics of RNN clearly, an RNN can be decomposed into two parts shown in Fig. 2. The first part is the indirect response relationship between the hidden nodes (the self-feedback part) and the output layer. The second part is the direct relationship between the hidden nodes and the output node. The relevance of a hidden node is related to its influence on the RNN response.The quantitative measure of SA used in this study is given by:(6)Sh=Var[E(Y|Zh)]Var(Y),(h=1,2,…,H),where Zhrepresents the hth input factor, Y is the model output, E(Y|Zh) is the expectation of Y conditional on a fixed value Zh, and the variance Var(Y) is taken over all the possible of Zh, the ratio Shis called the first-order index: it represents the main effect of a factor, i.e., the average effect of that factor on the response.For RNN, the input factors for SA contains two parts: the indirect and direct factors. The indirect input factor is Z1(t)=[v1 1(t), v1 2(t), …, v1 H(t)], which is the feedback value of the hidden layer at time t. And the direct input factor is Z2(t)=[v 1(t), v 2(t), …, v H(t)], which is the output of the hidden layer at time t. Then, the quantitative measure of sensitivity with indirect factors is rewritten as:(7)Sh1(t)=VarhE(y(t)|Zh1=vh1(t))Vary(t),where v1 h(t) is the feedback value of the hth hidden node at time t, h=1, 2,…, H, y(t) is the output value of RNN, and the variances Var [y(t)], Varh[E(y(t)| Z1 h= v1 h(t))] are defined from the Fourier translation [44]:(8)Varh[E(y(t)Zh1=vh1(t))]=2(Aωh11)2+(Bωh11)2,Var(y(t))=2∑h=1H((Aωh11)2+(Bωh11)2),where ω1h is the frequency of the hth feedback value. If the range of the factor v1 h(t) is [a, b], the parametric representation of the Fourier translation is given by(9)vh1(s)=b+a2+b−aπarcsin(sin(ωh1s)),and the Fourier coefficientsAωh11andBωh11are:(10)Aωh11=12π∫−ππcos(ωh1s)ds,Bωh11=12π∫−ππsin(ωh1s)ds,the range of s is [–π,π].Meanwhile, the quantitative measure of sensitivity with direct factors is rewritten as:(11)Sh2(t)=VarhE(y(t)|Zh2=vh(t))Vary(t),where v h(t) is the output value of the hth hidden node at time t, and the variances Var[y(t)], Varh[E(y(t)| Z2 h=v h(t))] can be calculated similarly as in (8) and (10).According to the above quantitative measure of sensitivity with indirect and direct factors as (7) and (11), the total sensitivity index SThof the hth hidden node is:(12)STh(t)=αSh1(t)+βSh2(t),where α and β are the judgment constants, α∈[0, 0.2], β∈[0.5, 0.8].Remark 1In general, the total sensitivity indexes are used to represent the necessary information between the hidden nodes and the output layer. This SA method, which is different from the definition in [27], proves that the total sensitivity index depends on current states along with previous states. Previous studies [39,40] provided strong evidence that compatibly used past and current states to be more desirable.This section describes the growing and pruning strategies that employ the sensitivity index to self-organize the structure of RSONN. The general idea of this method is: if the sensitivity indexes of hidden nodes are too small, these hidden nodes will be pruned to simplify the structure of RNN. On the other side, if the learning performance is not good enough to achieve the desired performance, some new hidden nodes will be inserted to the network, based on these hidden nodes whose sensitivity indexes are large.At time t, if there are H nodes in the hidden layer, the RMSE for the system is E(t)>ζ(t) (ζ(t)=t−0.65), which means that the learning performance is not good enough to achieve the desired performance and a new hidden node should be added to the current structure to improve the performance. In order to maintain convergence, the initial parameters of the new hidden node are(13)wH+11(t)=wh1(t)=w1h1(t),w2h1(t),…,wMh1(t),wH+12(t)=wh2(t),wH+13(t)=e(t)vH+1(t),where(14)STh(t)=max{STj(t)},j=1,2,…,H,e(t)=yd(t)−y(t),w3 H+1 is the output weight between the new hidden node and the output layer, w2 H+1 is the self-feedback weight of the new node, and w1 H+1 is the input weight vector between the new hidden node and the input layer, h is the node, which has the largest total sensitivity index.Similar to the growing step, at time t, if the total sensitivity index STh<ξ (ξ is the pruning threshold), then the hth node will be pruned. And the weights of the hth node and its nearest neighbor are(15)wh1'(t)=0,wh2'(t)=0,wh3'(t)=0,wh−n1'(t)=wh−n1(t),wh2'(t)=wh2(t),wh−n3'(t)=wh−n3(t)+vh(t)vh−n(t)wh3(t),where w’1h, w’2h, and w’3h are the weights of the hth hidden node after the structure had been adjusted, h-n is the node, which has the minimum Euclidean distance to the hth node and STh-n≥λ (λ is the active threshold), w1 h−n and w’1 h−nare the input weights of node h-n before and after pruning; w2 h−nand w’2 h−nare the self-feedback weights of the h−nth node before and after the structure has been adjusted, w3 h−nand w’3 h−nare the output weights of the h−nth node before and after pruning. The parameters of the retained nodes are not changed.The proposed RSONN self-organizes the structure by the growing and pruning strategies using the SA method, which determines the total sensitivity index for all the hidden nodes, and then employs the supervised gradient algorithm to further optimize the weights.The adaptation strategy of weights is defined as:(16)wij1(t+1)=wij1(t)+Δwij1(t),wj2(t+1)=wj2(t)+Δwj2(t),wj3(t+1)=wj3(t)+Δwj3(t),where w1 ij(t+1), w2 j(t+1), and w3 j(t+1) are the input weight between the ith input node and the jth hidden node, the self-feedback weight of the jth hidden node, and the output weight between the jth hidden node and the output layer at time t+1, respectively; and(17)Δwij1(t)=−η1∂E(t)∂wij1(t),Δwj2(t)=−η2∂E(t)∂wj2(t),Δwj3(t)=−η3∂E(t)∂wj3(t),η1, η2, and η3 are the learning rates.The basic idea behind RSONN is to improve the precision of the final neural network by taking advantage of the growing and pruning strategies. Initially, random number of hidden nodes is initialized in RSONN, and the RSONN algorithm will be adjusted by the structure learning algorithm. In addition with the structure, all weights in RSONN are also learned simultaneously, including those newly generated and previously existing. For clarification, the proposed RSONN learning scheme can be summarized in Table 1.Remark 2This RSONN algorithm can easily lead to an optimization network structure. At the same time, a supervised training algorithm, which is used to adjust the weights of the network, ensures the exactitude of RSONN. And, because the structure of RSONN can modify online, its performance have greatly improved than the fixed RNNs.For RSONN, the convergence with the structure and weights adjustment is important and requires careful investigation, as it is crucial for the successful applications. To analyze the convergence of the proposed RSONN, this section first discusses the convergence of RSONN with fixed number of hidden nodes. Then, the convergence of RSONN with different number of hidden nodes is studied.In this study, the Lyapunov function candidate is defined as:(18)V(t)=12[e(t)]2,where e(t)=yd(t)−y(t) represents the error in the learning process. Then, the change in (18) is obtained by(19)ΔV(t)=V(t+1)−V(t)=12[e2(t+1)−e2(t)],and the error difference can be represented by:(20)Δe(t)=e(t+1)-e(t)=∂e(t)∂W(t)TΔW(t)+O(ΔW(t)),where, O(ΔW(t)) indicates the higher order terms of the remainder of the Taylor series expansion and can be ignored, ΔW(t) represents the change in an arbitrary weighting vector, which is given by:(21)ΔW(t)=[ΔW1(t),ΔW2(t),ΔW3(t)]T,and W1(t)=[w1 1(t), w1 2(t), …, w1 H(t)]T, W2(t)=[w2 1(t), w2 2(t), …, w2 H(t)]T, and W3(t)=[w3 1(t), w3 2(t), …, w3 H(t)]T.From (17), one has(22)ΔW1(t)ΔW2(t)ΔW3(t)=−e(t)η1000η2000η3∂e(t)∂W1(t)∂e(t)∂W2(t)∂e(t)∂W3(t).Now we have the following convergence theorems.Theorem 1Suppose the number of hidden nodes of RSONN is H, and the learning algorithm for W is given as in (16) and (17). LetPmax be defined asAs shown in (19) and (20), the error difference can be rewritten as:(25)ΔV(t)=12[(e(t)+Δe(t))2−e2(t)]=12[(e(t)+∂e(t)∂W(t)TΔW(t))2−e2(t)].By considering (21) and (22), one has(26)ΔV(t)=12[(e(t)−e(t)∂e(t)∂W1(t)∂e(t)∂W2(t)∂e(t)∂W3(t)Tη1000η2000η3∂e(t)∂W1(t)∂e(t)∂W2(t)∂e(t)∂W3(t))2−e2(t)]⁡≤12[e2(t)(1−η1∂e(t)∂W1(t)T∂e(t)∂W1(t)−η2∂e(t)∂W2(t)T∂e(t)∂W2(t)−η3∂e(t)∂W3(t)T∂e(t)∂W3(t))2−e2(t)].Since the learning rates satisfies (24), it will be(27)ΔV(t)≤0.It follows from (18) and (27) that V is not increasing, one can further conclude that(28)limt→∞e(t)=0,and the convergence result of Theorem 1 is thus completed. ■Theorem 1 is stated for the RSONN algorithm with a fixed structure. The convergence analysis of RSONN with a self-organizing structure will be presented in sequel, that is, at each time, when new nodes are inserted or pruned, it may destroy the convergence of RSONN if the structure design method is not suitable.Suppose the number of hidden nodes of RSONN is self-organized in the learning process, and the learning algorithm forWis given as in (16) and (17). If the learning rates are chosen to satisfy (24), the convergence of RSONN can also uniformly asymptotically converge to 0.When a new hidden node has been added to the network structure (there are H+1 hidden nodes after the growing step), the approximation error eg(t) after the node growing is given by(29)eg(t)=yd(t)−∑j=1H+1wj3(t)vj(t).Based on formula (13), one has:(30)eg(t)=0.Meanwhile, when a hidden node has been pruned in the hidden layer (there are H−1 hidden nodes after the pruning step), the approximation error ep(t) after the node pruning is given by(31)eg(t)=yd(t)−∑j=1H−1wj3(t)vj(t).Based on formula (15), one further has:(32)eg(t)=e(t).From (30) and (32), each time new nodes are inserted, the network has good approximation error. And each time the hidden bodes are pruned, the approximation error will not be influenced. To prove Theorem 2, the Lyapunov function candidate is chosen as (18). If the learning rates are chosen to satisfy (24), following the similar arguments as in the proof of Theorem 1, one concludes that ΔV(t) is negative definite. The convergence result of Theorem 2 can thus be established. ■It is worth pointing out that the convergence of RSONN can be maintained in both fixed structure and self-organizing structure phases. The structure and parameter adjusting algorithms proposed in this paper can guarantee the convergence of RSONN.Theorems 1 and 2 show the convergence of the proposed RSONN. Moreover, one of the key advantages of the approach is that the modeling error can uniformly asymptotically converge to zero. It is crucial for the successful applications of the proposed RSONN-based soft computing method.In order to demonstrate the effectiveness of the proposed RSONN-based soft computing method, the experimental hardware, including the online sensors and the predicting plant is schematically designed in a real WWTP for predicting the values of SVI. The performance of this proposed method is evaluated by comparing with other methods. All the simulations are programmed with MATLAB version 2014, and were run on a Pentium 4 with a clock speed of 2.6GHZ and 1GB of the RAM, under a Microsoft Windows XP environment.The activated sludge process is the most commonly technology in WWTP. It requires the removal of the microbial biomass produced in its biological reactors by sedimentation. And the secondary settling tanks are the most hydraulically sensitive unit operations in the activated sludge process. The effluent qualities of WWTP depend on the efficiency of secondary settling tanks in separating and thickening the activated sludge. A good separation and thickening of activated sludge in the secondary settling tanks is a necessary condition to guarantee a good effluent quality from the activated sludge process. However, in fact, poorly settling biomass, leading to sludge bulking, continues to be a common problem in WWTP [45].Bulking sludge, a term used to describe the excessive growth of filamentous bacteria, is a common problem in WWTP. Despite much research, bulking sludge seems to be a continuous problem in operating WWTP. The condition of the plant operation under which bulking sludge occurs is usually only marginally documented. One reason for not finding a good general solution to bulking sludge might be the absence of a consensus on the exact level at which the problem should be approached. A widely accepted component of sludge bulking theory is the existence of a threshold effect, whereby when filament content exceeds some threshold value, its effect on SVI increases greatly. The threshold concept suggests that there is a discontinuity in the relationship between filament content and settle ability at the threshold filament value. Many previous studies have sought to determine the relationship between filament content and settling ability. Settling ability has been expressed that higher SVI values indicate poorer settling ability. Originally, the SVI is intended to be a rough measure of sludge settling ability in the everyday operation of WWTP as a means for predicting the physical condition of activated sludge.For this proposed soft computing method, the inputs are those variables that are easy to measure and the outputs are the SVI values. Since the input–output relationship is encoded in the data used to calibrate the model, the soft computing method is used to reconstruct it and then to estimate the output variables. In general, the procedure of this soft computing method consists of three parts: data acquisition, data preprocessing, and model design. Remarkable characteristics of the data acquired in WWTP are redundancy and possibly insignificant. And the choice of the input variables that influence the model output is a crucial stage. Therefore, it is necessary to select suitable input variables and prepare their data before using the soft computing method. Moreover, variable selection consists of choosing those easy to measure variables that are most informative for the process, as well as those that provide the highest generalization ability. In this study, the partial least squares method is used to extract the input variables. Based on the partial least squares method, the selected input variables are the effluent pH, chemical oxygen demand (COD) concentration, total nutrients (TN) concentration, dissolved oxygen (DO) concentration, and mixed liquid suspended solid (MLSS). pH is the growth environment for microorganisms. COD and TN are the carbon source and nitrogen source for microorganisms, respectively. DO concentration is an important criterion for sludge bulking measurement. MLSS is the indicator of the amount of activated sludge. The inputs considered are listed in Table 2. In addition, the main apparatus and instruments used to obtaining the online values for these easy to measure variables, are also explained in Table 2.Based on the above analysis, in this work, an experimental hardware is set up. A 75L sequential batch reactor (SBR) process with a pre-anaerobic selector is employed in this system (shown in Fig. 3). This SBR process is established with 110cm height and an internal diameter of 15cm. The process is operated using an automatic timer to start and stop pumps for aeration and a stirrer with variable speed. The SBR hydraulic retention time (HRT) is chosen as 16.5h. The reactor is operated at a sludge retention time (SRT) of 30 days. Aerobic granular sludge is collected from a pilot plant treating sewage.Moreover, in this experimental hardware, a sampling system is installed. The experimental hardware, including the online sensors and the proposed RSONN-based soft computing method is schematically shown in Fig. 3. The online sensors consist of five parts: the pH instrument, the COD instrument, the TN instrument, the DO probe, and the MLSS probe. The experimental hardware contains a PC-computer. Since there is no direct contact between the sensors and the PC-computer, a data transmitting system has been designed. The sensors are operated in continuous/online measurement mode. The historical process data are routinely acquired and stored in the data acquisition system. The process data are periodically collected from the reactor in order to check whether the system was operating as scheduled during the experiments. Samples taken simultaneously at the biological tank are immediately collected by the sensors and transmitted to the PC-computer. Then, the data can be easily retrieved in the soft computing method.In this paper, the RSONN-based soft computing method is used to predict the SVI values: its aim is to own the SVI values online, based on the related parameters. The input–output water quality data, measured between 1/6/2014 and 30/7/2014, were obtained from a real SBR process in Beijing, China. After deleting abnormal data, 220 groups were obtained and normalized, 120 groups were used as testing data, whilst the remaining 100 groups were used as training data. The initial structure of RSONN is 5-5-1. The error measures for the SVI are ±3mg/L confidence limits. In the establishment of the soft computing method, the effectiveness of the proposed method is evaluations by the RMSE values and the predicting accuracy (∑100 p=1(1−e(p)/yd(t))/100) [11]. The learning rates η1=η2=0.01, η3=0.02, the parameters ξ=0.01, λ=0.1, α=0.1, and β= 0.75. The maximum training steps are 104, and the expected RMSE value is 0.01.Moreover, in this study, two aspects of the evaluations are discussed: In case 1, the soft computing method is compared with the other SVI prediction models. In case 2, the soft computing method with RSONN is compared with the soft computing method with other neural networks (self-organizing neural networks and self-organizing recurrent neural networks).The predicting performance of the soft computing method is compared with the mathematic method [46], the dynamic auto-regressive exogenous (ARX) method [47], the fixed feedforward neural network [48], the support vector machine method [49], the fixed fuzzy neural network [50], and the fuzzy k-nearest neighbors method [51]. The parameters of the mathematic method, the dynamic ARX method, the fixed feedforward neural network method, the support vector machine method, the fixed fuzzy neural network method, and the fuzzy k-nearest neighbors method are the same as the initial papers. In this experiment, the number of hidden nodes in the training process is shown in Fig. 4. And Fig. 5describes the RMSE values in the training process. Clearly, the RSONN-based soft computing method can self-organize the network structure and the RMSE can converge to a steady state quickly in this example.In the testing process, Fig. 6shows the testing sample details. Moreover, the predicting results of the SVI values by the soft computing method are depicted together with the real process output in Fig. 7. And the predicting errors, which are less than ±3mg/L, are shown in Fig. 8. The predicting values obtained using the proposed RSONN-based soft computing method with respect to the real output are accurate. In order to check the predicting abilities of the different methods, the performance of the different approaches is compared in Table 3. Table 3 shows different methods for predicting the SVI values. In this table, the average accuracy (minimize accuracy and maximum accuracy), the mean, and deviation (Dev.) of testing RMSE are shown in detail.By analyzing Table 3, the following specific comments can be made: 1) the training RMSE of the proposed RSONN-based soft computing method is the smallest, 2) the accuracy of the proposed RSONN-based soft computing method is better than that of the mathematic method, the dynamic ARX method, the fixed feedforward neural network method, the support vector machine method, the fixed fuzzy neural network method, and the fuzzy k-nearest neighbors method. The comparisons demonstrate that this soft computing method is suitable for the SVI prediction. The error measure of the soft computing method is less than 3mg/L.In order to check the performance of RSONN and to obtain a better understanding on the soft computing method, the predicting values of the RSONN-based soft computing method are compared with those of the soft computing method with the SORBF [11], the ESRNN [22], and the RRSEFNN [35]. In this case, five initial hidden nodes are used for the proposed RSONN, SORBF, ESRNN, and RRSEFNN. The other parameters of the SORBF, ESRNN. and RRSEFNN are the same as in the initial papers to ensure a fair comparison. In the training period, these self-organizing neural networks or self-organizing recurrent neural networks can both adjust the parameters and change the network structure. Then, the suitable network models for the current operating point are maintained and used for the predicting.The predictions by these self-organizing neural networks and self-organizing recurrent neural networks are displayed in Table 4for comparison. The results in this case are averaged on 50 independent runs for each neural network. The average values of the following parameters are used to measure the performance: the number of preserved nodes in the hidden layer, the mean RMSE values, and the mean accuracy. Table 4 shows a detailed comparison of the results with the three other self-organizing neural networks.By analyzing Table 4, the RSONN-based soft computing method has the following advantages for the SVI prediction:(1)The proposed RSONN can self-organize the network structure for the soft computing method. The final structure of RSONN is compact and steady over the initial structure.The RSONN-based soft computing method obtains the best RMSE values and the standard deviation RMSE values, for the SVI prediction in this case. Based on the reference [52], over-fitting problem occurs when the testing error is greater than the training error, but, compared with other methods in Table 4, the over-fitting problem can be well controlled by taking advantage of the proposed method, RSONN.The RSONN-based soft computing method achieves the best minimum accuracy compared with the other algorithms and also produces better maximum accuracy. Moreover, the results show that the SVI values can be predicted by the RSONN-based soft computing method.From the above experimental results on the prediction of SVI, the proposed RSONN-based soft computing method has been proved to be a suitable and efficient method for predicting the SVI values. It can also be seen that, in these two cases, the minimum accuracy of the RSONN-based soft computing method is better than the other methods, the maximum accuracy of the RSONN-based soft computing method is also better than the other methods. The proposed RSONN-based soft computing method is relatively straightforward to implement online, and can be used to make online prediction of SVI.

@&#CONCLUSIONS@&#
