@&#MAIN-TITLE@&#
On feature extraction for noninvasive kernel estimation of left ventricular chamber function indices from echocardiographic images

@&#HIGHLIGHTS@&#
Color-Doppler M-mode images are used to characterize left ventricular function.Estimation of end-systolic peak elastance and time-constant of relaxation rate.Comparison and interpretation of different linear estimators.Conditions where non-linear estimators outperform linear ones.High-fidelity study on mini-pigs with echocardiographic images and invasive measures.

@&#KEYPHRASES@&#
Color Doppler M-mode imaging,Kernel methods,Left ventricular function,Elastance,LV relaxation,Cosine transform,

@&#ABSTRACT@&#
Two reference indices used to characterize left ventricular (LV) global chamber function are end-systolic peak elastance (Emax) and the time-constant of relaxation rate (τ). However, these two indices are very difficult to obtain in the clinical setting as they require invasive high-fidelity catheterization procedures. We have previously demonstrated that it is possible to approximate these indices noninvasively by digital processing color-Doppler M-mode (CDMM) images. The aim of the present study was twofold: (1) to study which feature extraction from linearly reduced input spaces yields the most useful information for the prediction of the haemodynamic variables from CDMM images; (2) to verify whether the use of nonlinear versions of those linear methods actually improves the estimation. We studied the performance and interpretation of different linearly transformed input spaces (raw image, discrete cosine transform (DCT) coefficients, partial least squares, and principal components regression), and we compared whether nonlinear versions of the above methods provided significant improvement in the estimation quality. Our results showed that very few input features suffice for providing a good (medium) quality estimator forEmax(for τ), which can be readily interpreted in terms of the measured flows. Additional covariates should be included to improve the prediction accuracy of both reference indices, but especially in τ models. The use of efficient nonlinear kernel algorithms does improve the estimation quality of LV indices from CDMM images when using DCT input spaces that capture almost all energy.

@&#INTRODUCTION@&#
Characterization of left ventricular (LV) systolic and diastolic chamber function is still a pending issue in the clinical setting. In experimental physiology, peak end-systolic elastance (Emax) is well established as the best available index to measure systolic performance of the LV chamber. In turn, the time-constant of LV relaxation (τ) is accepted as the gold standard method accounting for the rate of relaxation of the chamber, one of the main diastolic properties of LV function. MeasuringEmaxrequires complex measurements of instantaneous pressure and volume inside the LV chamber as well as preload intervention maneuvers. Measuring τ requires invasive catheterization of the LV using high-fidelity micromanometers. For these reasons, neitherEmaxnor τ are only measured in patients for research purposes.A number of noninvasive methods have been developed to obtain surrogate indices that correlate withEmaxand τ. Among them, most research has focused on Doppler-echocardiography, because it is a fully noninvasive, non ionizing, cheap and readily available at the patient's bedside. In a previous work we have shown that τ andEmaxcan be reasonably approximated from CDMM images. Using a fluid-dynamic approach we have shown thatEmaxcorrelates closely to the peak-ejection pressure difference developed inside the ventricle, which can be computed by solving Euler's equation from the CDMM velocity data [27]. Similarly, τ can be approximated by the peak reverse end-ejection pressure difference with reasonable accuracy [26]. Importantly, using a learning from samples approach we have obtained similar approximations without the need of complex fluid-dynamic modeling [13]. Hence, an experimental (animal) setup was used to simultaneously measure the catheter-based curves (pressure and flow) and acquire the CDMM images; and a machine-learning model was designed for straightforward estimation of τ andEmaxparameters from an input space given by the diastolic period of the digitized CDMM image.In that precedent work, a linear estimator was used for the image input space, which raises several questions. On the one hand, nonlinear relations between CDMM images and indices can be expected, as the haemodynamic variables in the cardiac circulatory system are known to be mostly interrelated by nonlinear fluid dynamic equations. On the other hand, linear kernel estimators are often suggested in the machine learning literature as the most appropriate choice for high-dimensional input spaces, and they also provide with easier to interpret, black-box models than their nonlinear counterparts. Therefore, our aim was to test whether alternative algorithms on the machine learning specifications could improve the prediction of invasive indicesEmaxand τ from CDMM images. First, we wanted to study which feature extraction from linearly reduced input spaces yields the most useful information for the prediction of the haemodynamic variables from CDMM images. Second, we wanted to verify whether the use of nonlinear algorithmic versions of those linear methods actually improves the estimation. Accordingly, we benchmarked the performance of several linear kernel estimators, in terms of linear feature extraction transformations, and in addition we analyzed the physical and clinical meaning of the relevant features in these transformed spaces, when possible. We also benchmarked the nonlinear kernel versions of the above analyzed estimators, hence determining the actual improvement obtained by the consideration of nonlinearity in the estimation kernel machine. For this purpose, we chose several kernel methods, namely, Support Vector Regression (SVR), Principal Component Regression (PCR), and Partial Least Squares (PLS), according to different levels of algorithm complexity in terms of the multidimensional output estimation from the multidimensional input. SVR performs one dimensional output robust estimation, PCR performs dimensionality reduction and multidimensional output estimation, and PLS performs a dimensionality reduction according input–output covariance.The rationale for the chosen input features was as follows. First, the RAW input space conveys all the image information, hence it represents a necessary benchmarking. Also, a linear machine working on the input space will be easy to interpret, in terms of the relative temporal and spatial position of the linear weights. Second, DCT input space is a widespread used frequency transform in image problems, and given the smoothness and low-pass frequency content of CDMM images, it can be expected to work well from an image information compression point of view. Third, PCR provides us with features from an intrinsic image decomposition (different from frequency decompositions), with a decoupled regression stage. And finally, PLS provides us with features from an intrinsic image decomposition in which the regression output quality is an embedded optimization criterion.The scheme of the paper is as follows. In the next section, the fundamental theory of the multidimensional kernel machines is summarized for the SVR, PCR, and PLS algorithms. Then, a detailed set of experiments is presented for benchmarking and interpretation of linear vs nonlinear kernel versions of the estimators. Finally, conclusions are drawn.This section first describes the basic equations of SVR, PCR and PLS. These methods allow both linear and nonlinear estimation without explicitly extracting features from the images. Both PCR and PLS implicitly extract features (components or latent vectors) previous to the estimation problem. PCR performs a feature extraction such every new feature captures as much as possible of the remaining variance of the input data, where PLS extracts features that maximize the covariance of the input data and target variables.SVR is a well-studied technique that allows nonlinear mappings of the input space and works well with high dimensional spaces like images [20]. Given a training set{(xi,yi),i=1,…,n}wherexi∈Rdandyi∈R, the SVR finds a function f that estimatesyias(1)yˆi=f(xi)=ϕT(xi)w+b=yi+eiwhereϕ:Rd→His in general a nonlinear mapping to the feature spaceH;(⋅)Tis the matrix transpose operator; w is the weight column vector in this space; b is a bias term; andeiis the residual error. Function f is found by minimizing a functional with a regularization term and a loss term, as follows:(2)L=12‖w‖2+∑i=1nL(ei)whereLis in our case the robust ϵ-Huber loss function [13], which increases the flexibility modeling outliers, given by(3)L(ei)={0|ei|≤ϵ12δ(|ei|−ϵ)2ϵ≤|ei|≤ϵ+δCC(|ei|−ϵ)−12δC2ϵ+δC≤|ei|where ϵ is the insensitive-zone parameter (no loss for errors lower than ϵ), and δC controls the size of the quadratic zone of the loss function. Finally, we minimize the following convex problem:(4)L=12‖w‖2+12δ∑i∈I1(ξi2+ξi⁎2)+C∑i∈I2(ξi+ξi⁎)−∑i∈I2δC22with respect tow,b,ξi,ξi⁎, taking into account the following convex constraints:(5)yi−ϕT(xi)w−b≤ϵ+ξi(6)ϕT(xi)w+b−yi≤ϵ+ξi⁎(7)ξi,ξi⁎≥0fori=1,…,n, and whereξi,ξi⁎are positive slack variables to penalize the positive and negative errors, andI1andI2are respectively the sets of samples that are in the quadratic and linear loss zone.Following the same procedure that solves standard SVR [20], we obtain the following solution:(8)yˆt=f(xt)=∑i=1n(αi−αi⁎)K(xi,xt)+bwhereK(xi,xt)=ϕ(xi)Tϕ(xt)is a Mercer kernel function, which is usually constructed without explicitly projecting inH(i.e., without explicit knowledge ofϕ) andC≥αi,αi⁎≥0are the Lagrange multipliers for the restrictions (5), (6). After optimization, someαi,αi⁎have a non-zero value and their associated sample is named support vector (SV), because it influences function f. Three parameters(C,ϵ,δ)need to be tuned for linear SVR (as well as the kernel width σ for the Gaussian case). Linear kernel is defined askL(x1,x2)=x1Tx2, and Gaussian kernel is defined askG(x1,x2)=e−‖x1−x2‖22σ2.PCR [10] performs a Principal Component Analysis (PCA) on the zero-meann×dpredictor matrix X, and then applies Ordinary Least Squares (OLS) to the resulting g principal components (scores)T=[t1,t2,…,tg]and the zero-meann×kdependent variables matrix Y, where k is the number of dependent variables. X can be factorized as:(9)X=TPTwhereP=[p1,p2,…,pr]is the loadings matrix (PPT=I) and r, is the rank of X. Any numberg≤rof components can be selected in order to estimate Y, however, these components should be selected carefully. Massy [10] proposes using the variance of the components or the dependence (correlation with Y) depending on the purpose of the analysis, but author warns also that low variance components can be useful for predicting Y. Jolliffe [7] and Hadi and Ling [5] discourage the direct application of the high-variance rule for selection components.Predictions for X andXtare done by using the following matrix relations:(10)Y=TB+F(11)Bˆ=(TTT)−1TTY(12)Yˆ=TBˆ=XPBˆ(13)Yˆt=XtPBˆwhere (10) is the regression model based on the principal components; (11) is the OLS estimator forBˆ; and (12) and (13) are the predictions.As described in [15,17], the nonlinear kernel PCR (KPCR) performs kernel PCA [19] and then OLS using the principal components in the feature space. The same concerns appear in KPCR as in PCR, low variance components could be useful for prediction, as shown in [14]. One parameter (g) needs to be tuned for PCR (and the kernel width for Gaussian KPCR).PLS for regression (see [16,24] for a description and an overview) are used for modeling relations between blocks of variables (e.g., a block of d explanatory variables and another block of k response variables), as well as for dimensionality reduction. PLS extracts orthogonal latent vectors (also called score vectors) by maximizing the covariance between blocks of variables; then PLS projects the observed data to its latent structure and use the latent vectors to perform regression of the response variables. PLS techniques assume that the observed data are generated by a process driven by a small number of latent (not directly observed) components.PLS models the zero-meann×dexplanatory (predictor) matrix X and the zero-meann×kdependent variables Y as:(14)X=TPT+E(15)Y=UQT+F=TDQT+(HQT+F)=TCT+F⁎whereT=[t1,t2,…,tg]andU=[u1,u2,…,ug]contain the extracted latent vectors as columns, and g is the number of latent components extracted;P=[p1,p2,…,pg]andQ=[q1,q2,…,qg]are the loading matrices; and E and F represent residual matrices. The second equality in (15) comes from the assumption of T columns being good predictors of Y andU=TD+H, where D is a diagonal matrix.In order to find score vectorstiandui, a procedure based on NIPALS algorithm [23] findswi,civectors such that(16)cov(ti,ui)2=cov(Xwi,Yci)2=max‖r‖=1‖s‖=1⁡cov(Xr,Ys)2wherecov(ti,ui)is the sample covariance between vectortiand vectorui. The procedure starts by a random selection ofuivector (it can be initialized asui=Yifk=1). Then, the following steps are repeated until convergence:1.wi=XTui/‖XTui‖ti=Xwici=YTti/‖YTti‖ui=YciThe estimatedYˆandYˆtfor the training data X and test dataXtare respectively given by:(19)B=W(PTW)−1CT(20)Yˆ=XB=TCT(21)Yˆt=XtBwhere W, P and C respectively are the matrices withwi,piandcias columns.To extend PLS to nonlinear problems, it is common to apply the kernel-trick [18] to the predictor matrix X and then perform a linear PLS in the feature space [17]. The NIPALS-based kernel version results:1.ti=Kui/‖Ku‖ci=YTtiui=Yci/‖Yci‖

@&#CONCLUSIONS@&#
