@&#MAIN-TITLE@&#
Fisher Kernel Temporal Variation-based Relevance Feedback for video retrieval

@&#HIGHLIGHTS@&#
We proposed a novel framework for Relevance Feedback based on the Fisher Kernel.The Fisher Kernel representation makes possible to capture temporal variation by using frame-based features.We experiment on a high variety of scenarios and public datasets (genre classification - Blip10000, action recognition - UCF50 / UCF101 and daily activities recognition - ADL) and show the benefits of the proposed approach which outperforms other state of the art approaches.We prove the generalization power of our approach, i.e., the framework is not dependent on a particular type of content descriptors (experiments were made with text, visual and audio features).

@&#KEYPHRASES@&#
Relevance feedback,Fisher Kernel representation,Multimodal content description,Video retrieval,

@&#ABSTRACT@&#
This paper proposes a novel framework for Relevance Feedback based on the Fisher Kernel (FK). Specifically, we train a Gaussian Mixture Model (GMM) on the top retrieval results (without supervision) and use this to create a FK representation, which is therefore specialized in modelling the most relevant examples. We use the FK representation to explicitly capture temporal variation in video via frame-based features taken at different time intervals. While the GMM is being trained, a user selects from the top examples those which he is looking for. This feedback is used to train a Support Vector Machine on the FK representation, which is then applied to re-rank the top retrieved results. We show that our approach outperforms other state-of-the-art relevance feedback methods. Experiments were carried out on the Blip10000, UCF50, UCF101 and ADL standard datasets using a broad range of multi-modal content descriptors (visual, audio, and text).

@&#INTRODUCTION@&#
Understanding video content is in general a subjective process for a user. Labelling video content with a predefined set of labels can greatly facilitate search, but is unlikely to capture all possible viewpoints of users. Hence finding specific video content in the exponentially growing amount of digital video becomes increasingly difficult. One solution to this problem is to empower the user with personalized search by iteratively having the user refine its search queries. This is called Relevance Feedback (RF) and is the topic of this paper. A general RF scenario for video retrieval can be formulated as follows: First the user does an initial query using either a keyword or a specific video for which he wants related videos. After the system has returned the best matching results, the user indicates which videos are relevant and which are not. Results are updated using the input, and this refinement continues until the user is satisfied.In this paper we propose a novel framework for Relevance Feedback based on the Fisher Kernel (FK) and Support Vector Machines (SVMs). The proposed approach operates on top of an existing retrieval system by refining the initial results. First, we alter the feature space: we train a Gaussian Mixture Model (GMM) on the top retrieved results, after which we obtain the FK representation with respect to the GMM. Hence the new feature space is specialized in representing the top results that are representative. Afterwards, we train an SVM using the user feedback, yielding a specialized classifier in the new feature space. Therefore, we have an unsupervised step which alters the feature space and a supervised step to incorporate user feedback. The entire process is illustrated in Fig. 1.Additionally, we propose to use the FK to capture temporal information as follows: we cut a video up in smaller temporal segments, extract a fixed-size feature representation for each segment, and represent the resulting feature set using the FK. Notice that since the FK captures variation in features in general, and we vary the features in time only, we effectively capture the temporal variation using this representation (but not the temporal order). This differs from other uses of the FK: The representation of images using SIFT [1] and FK leads to a representation of the local visual variation in space only while no temporal information is captured which discards meaningful video information; Representing videos using local Histograms of Oriented Gradients (HoG)/Histograms of Optical Flow (HoF) descriptors [2] and the FK leads to a representation of the local variation in both time and space simultaneously, where space and temporal information is mixed together thus reducing their individual representative power instead of exploiting it. In our approach, by having fixed-sized representations of single frames or small temporal segments with FK we manage to exploit the variation in time only thus capturing that unique video temporal characteristics. As experimental results show, this proves highly efficient in relevance feedback based retrieval scenarios.This paper extends our previous work [3,4] by including evaluation on a new video dataset, evaluating more feature extraction schemes, analysing the influence of multiple relevance feedback iterations, and including a computational complexity analysis. To summarize, our main contributions are as follows:1.We propose a novel method for Relevance Feedback based on a combination of the FK and SVMs. To the best of our knowledge, this is the first work that exploits the benefits of FK representations to video relevance feedback;We explicitly model temporal variation by combining frame-based features with the FK;We demonstrate the generality of our approach by evaluating it on a broad range of modalities: we use visual, audio, and text descriptors. We achieve better performance than other state-of-the-art relevance feedback algorithms on two standard datasets [5,6], which makes the results both relevant and reproducible.The remainder of the paper is organized as follows. In Section 2 we present the current state-of-the-art on relevance feedback and FK and position our contribution. Section 3 presents the Fisher Kernel theory and Section 4 presents the algorithm of the proposed FK relevance feedback. Afterwards, in Section 5 we present an expansion of the method for temporal aggregation with FK. The experimental setup is presented in Section 6, while the experimental results are reported in Section 7. Finally, in Section 8 we conclude the paper.

@&#CONCLUSIONS@&#
In this paper we formulated and analysed a new approach for relevance feedback using Fisher Kernels in the context of video retrieval. Our relevance feedback consists of two steps: (1) altering the feature space by training a Gaussian Mixture Model on the top retrieved results and re-representing those features using Fisher Kernels; (2) using the user feedback to train a personalized Support Vector Machine. Additionally, the Fisher Kernel representation made it possible to capture temporal variation (but not temporal order) by using frame-based features.Our Relevance Feedback experiments showed that our method always performs equal or better to other methods even without using temporal information: Compared to the next best method, RFE [22], we get improvements on Blip10000 between 0% and 11% MAP, averaging 5.2% MAP. For UCF50 the next best method is Random Forest RF [14] for which we get improvements of 0.9%, 1.6%, and 8.5% MAP, respectively for colour naming histograms, HoG, and HOF.If we capture temporal information we get even better improvements at an acceptable computational cost. By using a GMM with only 5–10 clusters a Relevance Feedback iteration becomes 4–6 s, in which the user can give its feedback. Improvements are significant: On Blip10000, we get absolute MAP improvements of 3.3%, 4.6%, and 4.9%, respectively for HoG, MPEG-7, and standard audio features. On UCF50 we get absolute MAP improvements of 3.1%, 4.4%, and 5.0% for respectively colour naming histograms, HoG, and HoF.