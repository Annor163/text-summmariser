@&#MAIN-TITLE@&#
Improved particle swarm optimization algorithm and its application in text feature selection

@&#HIGHLIGHTS@&#
We proposed three improved Particle swarm optimization models based on a common PSO model and two improved PSO models.Selecting Reuters-21578 as the corpus, six experiments are conducted respectively using improved PSO models.Combining asynchronously inertia weight and constriction factor is the best program.Paired-sample T-tests demonstrate the validness of our best program.

@&#KEYPHRASES@&#
Text classification,Text feature selection,Particle swarm optimization algorithm,Constriction factor,

@&#ABSTRACT@&#
Text feature selection is an importance step in text classification and directly affects the classification performance. Classic feature selection methods mainly include document frequency (DF), information gain (IG), mutual information (MI), chi-square test (CHI). Theoretically, these methods are difficult to get improvement due to the deficiency of their mathematical models. In order to further improve effect of feature selection, many researches try to add intelligent optimization algorithms into feature selection method, such as improved ant colony algorithm and genetic algorithms, etc. Compared to the ant colony algorithm and genetic algorithms, particle swarm optimization algorithm (PSO) is simpler to implement and can find the optimal point quickly. Thus, this paper attempt to improve the effect of text feature selection through PSO. By analyzing current achievements of improved PSO and characteristic of classic feature selection methods, we have done many explorations in this paper. Above all, we selected the common PSO model, the two improved PSO models based respectively on functional inertia weight and constant constriction factor to optimize feature selection methods. Afterwards, according to constant constriction factor, we constructed a new functional constriction factor and added it into traditional PSO model. Finally, we proposed two improved PSO models based on both functional constriction factor and functional inertia weight, they are respectively the synchronously improved PSO model and the asynchronously improved PSO model. In our experiments, CHI was selected as the basic feature selection method. We improved CHI through using the six PSO models mentioned above. The experiment results and significance tests show that the asynchronously improved PSO model is the best one among all models both in the effect of text classification and in the stability of different dimensions.

@&#INTRODUCTION@&#
Particle swarm optimization algorithm (PSO) is an evolutionary computing algorithm proposed by James Kennedy and Russell Eberhart in 1995[1]. PSO was originally used to graphically simulate the process of bird folks finding food. By observing flock of birds’ behaviors, the researchers found that sharing information in groups was beneficial to gain advantage during evolution. This made up the basis of PSO [2].The academic research about modifying PSO mainly focuses on improving its parameters by theoretical analysis, mathematical inference and empirical research. For example, Angeline et al. [4] introduced standard selection mechanism into PSO, the optimum particle was selected after each iteration and copied it to next generation. This method was particularly beneficial for complex function optimization. Lovbjerg et al. [5] quoted the concept of the sub-population and the reproductive into PSO in order to get faster convergence. Higashi et al. [6] used Gaussian mutation to redesign the changing rules of particle in terms of position and speed, and got better results on unimodal and multimodal function. Baskar et al. [7] designed two particle swarms which worked in parallel and exchanged information to overcome the PSO shortcoming of premature convergence. In addition, there were studies of quantum PSO [8], Niche PSO [9], Division of Labor in PSO [10], Hierarchical PSO [11] and other improved algorithms. Clerc added constriction factor K into PSO to ensure convergence [12]. Bergh studied the effect of randomness on particle trajectories in iteration [13]. Bergh et al. also proved that the original version of PSO could not converge to the global optimum, and proposed an improved PSO which ensured convergence to local optimal solution [14]. As for the inertia weight in PSO, Chatterjee et al. proposed an inertia weight with nonlinear variation [15]. Besides, Nickabadi et al. came up with an adaptive inertia weight [16]. Bansal et al. conducted comparing experiments for 15 kinds of method of inertia weight calculation [17]. The results showed that the chaotic inertia weight was the best in terms of effect, and the random inertia weight was the highest in efficiency.Classification is the process in which ideas and objects are recognized, differentiated, and understood [18]. Classification implies that objects are usually grouped into required categories for some specific purpose. There are many kinds of classification, including power quality classification, nonstationary power signal time series data classification [19], text classification, etc. Recently, biological evolution algorithms are more commonly used in classification to improve accuracy. For example, B. Biswal et used Bacterial Foraging Optimization Algorithm in classification of power quality data [20]. Luo Xin et used Ant Colony Optimization Algorithm to construct a text classification model [21]. Biswal et used Particle Swarm Optimization Algorithm to improve Fuzzy C-Means Algorithm. This improved algorithm was utilized into Time Frequency Analysis and Non-Stationary Signal Classification and Power Quality Disturbance Classification [22,23]. Lu Yonghe and Liang Minghui utilized Genetic Algorithm to optimize text feature selection method in text classification [24].In this paper, we focus on using Particle Swarm Optimization Algorithm to improve performance of text classification. Under the given classification system, text classification was defined as a process which automatically identify text category according to the text content [25]. Traditionally, scholars put forward many improved methods for text classification by optimizing mathematic model. For example, Lu Yonghe and Li Yanfeng optimized text feature weighting method based on TF-IDF algorithm [26]. Lu Yonghe and He Xinyu added similarity matrix and dimension index table into KNN to improve KNN classification algorithm [27,28]. Wei Tingting et added WordNet and lexical chains to optimize text clustering model [29]. Currently, in the field of text classification research, the improved PSO has three aspects of application. The first one is to optimize the classic text classifier, such as KNN, SVM, etc. [30]. Li Huan et al. proposed a simplified PSO KNN classification algorithm [31]. Tang Zhaoxia used the PSO algorithm to find k neighbors in order to improve the efficiency of web text classification [32]. Tuo Shouheng improved the inertia weight in PSO and added this method into SVM classification [33]. The second one is to build a text classifier using PSO. Luo Xin constructed the text classification model based on PSO [34]. Tong Yala et al. proposed extraction method for classification rule based on chaos PSO [35]. Similarly, Tan Dekun also proposed a text classification method based on chaos PSO [36]. The last one is the use of PSO for text feature selection [30]. Chih-Chin Lai et al. studied the application of PSO in text feature selection for spam classification [37]. This was a specific application using PSO to optimize text feature selection. Yaohong Jin et al. used the improved PSO in Chinese text feature selection [38]. Likewise, Zahran et al. proposed an improved feature selection method based PSO in order to solve the problem of Arabic text feature selection [39]. HK Chantar et al. also analyzed the characteristics of the Arabic text classification and used binary PSO for text feature selection to improve accuracy of text classification [40].In this paper, we firstly improve particle swarm optimization algorithms, and then apply them to text feature selection. Finally, we analyze the application effect of various improved particle swarm programs by using KNN classifier.PSO uses a number of particles, which constitute a swarm moving around in the search space, to look for the best solution. Each particle is treated as a point in a D-dimensional space, which adjusts it's “flying” according to its own flying experience as well as the flying experience of other particles. The particles flight with a certain velocity in the D-dimensional space to find the optimal solution. The velocity of particle i expresses asVi=(vi1,vi2,...,viD), the location of particle i expresses as (xi1, xi2, ..., xiD), the optimal location of particle i expresses as Pi=(pi1, pi2, ..., piD), it is also called pbest. The global optimum position of all particles expresses as Pg=(pg1, pg2, ..., pgD), it is also called gbest. Each particle in group has a fitness function to calculate the fitness value. In standard PSO, the velocity update formula of the dimension d shows in formulae (1) and (2):(1)vid=w×vid+c1×rand()×(pid−xid)+c2×Rand()×(pgd−xid)(2)xid=xid+vidPSO parameters include: Q (Population Quantity), w (inertia weight), C1 and C2 (acceleration constants), vmax (the maximum velocity), Gmax (the maximum number of iterations), rand () and Rand () are random functions with values in [0,1]. The value of C1 and C2 usually takes constant 2 [3].Inertia weight is an important parameter of the standard PSO. It determines the operating results of PSO. Fixed inertia weight make the particles always have the same exploration competence in flight. Formula (3) is the velocity formula with a fixed inertia weight in traditional PSO [3]:(3)vid(t+1)=w×vid(t)+c1×rand()×(pid−xid(t))+c2×Rand()×(pgd−xid(t))According to experience, w generally takes between 0 and 1 [3]. Thus, w is 0.9 in this paper. The velocity formula (3) becomes formula (4). (Here, we call it Program 1):(4)vid=0.9×vid+2×rand()×(pid−xid)+2×Rand()×(pgd−xid)Currently, the conventional strategy of improving inertia weight is LDIW (Liner Decreasing Inertia Weight) [41]. The changing way of w appears in formula (5). (Here, we call it Program 2):vid=w×vid+2×rand()×(pid−xid)+2×Rand()×(pgd−xid)(5)w=wend+(wstart−wend)(1−TGmax)ifpgd≠xidw=wendifpgd=xidwhere T is the number of iterations, T∈[0, Gmax), pgd is global best position, wstart is the initial inertia weight and wend is the value of evolution in the maximum iterations. wstart and wend are calculated in subsequent chapters.Because the dimensions of a text feature vector in Text Categorization are usually very high, the particles in PSO will gather into a point when it is not yet to find the global optimum [42]. Thus, Clerc introduced constriction factor K into PSO to ensure the best convergence. Formula (6) presents the velocity formula (Here, we call it Program 3):(6)vid=K[vid+c1×rand()×(pid−xid)+c2×Rand()×(pgd−xid)]Program 3 in this paper used the formula proposed by Clerc to calculate the constriction factor K. Value c1 and value c2 used 2.05 which were the same with Clerc's experiment. Here we reserve four decimal places of K for experiment. Formula (7) is the specific velocity formula:(7)vid=0.7298×[vid+2.05×rand()×(pid−xid)+2.05×Rand()×(pgd−xid)]In the early iterations, a particle in PSO needs to detect in a wide range to determine the likely location of the optimal solution. In later iterations, it needs to develop locally within a small range to determine the optimal point. Thus, K should take a larger value in the early and take a smaller value in the later. Simultaneously, K should become smaller slowly to the minimum in a longer period of late stage [43]. This pattern of change is consistent with the concave function. In applications of text classification, especially in text feature selection applications, the primary objective of PSO is to find the optimal solution in the search space to get the best result for text classification. To avoid premature convergence, the constriction factor should choose a convex function in the early iterations so that the particles can find optimal solution in a wide range. In the late period, it should choose a concave function so that the constriction factor can change slowly to the minimum in order to develop locally. It ensures convergence of the algorithm. According to this principle, the functional constriction factor structuring on the basis of the cosine function is showed in formula (8):(8)K=cos((π/Gmax)×T)+2.54where T is the number of iterations. Set Gmax=40, the changing curve of value K appeared in Fig. 1.The curve of K in the Fig. 1 is convex function at first and transforms into a concave function at last. The value K is substituted in formula (1), and then formula (1) turns into formula (9). (Here, we call it Program 4). Formula (9) is described below:(9)vid=cos((π×T/Gmax))+2.54×[vid+2×rand()×(pid−xid)+2×Rand()×(pgd−xid)]The velocity formula using only constriction factor does not use inertia weight. The analysis made by Lin Jie showed that the algorithm with constriction factor and the algorithm with inertia weight are equivalent when the constriction factor K and the inertia weight w is equal [44]. The algorithm with constriction factor can be regarded as a special case of algorithm with inertia weight. However, constriction factor and inertia weight have completely different meaning. Therefore, this section discusses the kind of situation using synchronously the inertia weight and functional constriction factor together. Constriction factor K uses directly the formula proposed in Section 2.3. The changing way of inertia weight needs to seek from process of birds foraging. According to the research results of Eberhart [45], in process of bird foraging, the birds slow down to search for the specific location when they are closer to food. In addition, the birds fly rapidly to find the orientation of food as soon as possible. When the particle is in a good position in iterative process, inertia weight should take a small value so as to retain the small part of original velocity for local exploration. When the fitness value of the particle position is poor, inertia weight should take a larger value so as to preserve most of the original velocity for better global optimization. The velocity formula integrating constriction factor and inertia weight becomes formula (10) (Here, we call it Program 5):(10)vid=K[w×vid+c1×rand()×(pid−xid)+c2×Rand()×(pgd−xid)]Considering research results done by Huang Zhongpeng [40], the calculation for specific changes of inertia weight is shown in formula (11):(11)w=wend+(wstart−wend)(1−(T/Gmax))ifpgd≠xidw=wendifpgd=xidwhere T is the number of iterations, T∈[0, Gmax), pgd is the global optimum position.Here we need to calculate value wstart and value wend. The constriction factor K and the inertia weight w are assumed to be constant in order to facilitate calculation. Because each dimension value of PSO does not interfere with each other, we only discuss the situation of one-dimensional value. pb is the individually best position, gb is the globally best position. Formula (12) is obtained according to the formula (10).(12)v(t+1)=K×w×v(t)+K×c1×rand()×(pb−x(t))+K×c2×Rand()×(gb−x(t))The rand () and Rand () are functions to generate a random number between 0 and 1. In order to calculate value K, we replace respectively c1×rand() and c2×rand() with c1 and c2. We can get the formula (13):(13)v(t+1)=K×w×v(t)+K×c1×(pb−x(t))+K×c2×(gb−x(t))We get the v(t+2) similarly:(14)v(t+2)=K×w×v(t+1)+K×c1×(pb−x(t+1))+K×c2×(gb−x(t+1))(15)x(t+2)=x(t+1)+v(t+2)=x(t+1)+K×w×v(t+1)+K×c1×(pb−x(t+1))+K×c2×(gb−x(t+1))=x(t+1)+Kw(x(t+1)−x(t))+Kc1pb−Kc1x(t+1)+Kc2gb−Kc2x(t+1)=(1+Kw−Kc1−Kc2)x(t+1)−Kwx(t)+Kc1pb+Kc2gb(16)MakeA=1+Kw−Kc1−Kc2−KwKc1pb+Kc2gb100001Formula (17) is the homogeneous matrix of formula (15):(17)x(t+2)x(t+1)1=Ax(t+2)x(t)1Formula (18) shows the characteristic equation of the coefficient matrix for formula (15):(18)(λ−1)(λ2−(1+Kw−Kc1−Kc2)λ+Kw)=0The characteristic Eq. (18) has three characteristic roots. Formula (19) shows these roots:(19)λ=1α=1+Kw−Kc1−Kc2+(1+Kw−Kc1−Kc2)2−4Kw2β=1+Kw−Kc1−Kc2−(1+Kw−Kc1−Kc2)2−4Kw2If(1+Kw−Kc1−Kc2)2≥4Kw, α and β are real roots.If(1+Kw−Kc1−Kc2)2<4Kw, α and β are imaginary roots.When α and β are real roots,αandβare all absolute values.When α and β are imaginary roots,αandβare models.We hope that PSO algorithm converges to the global optimal solution as soon as possible. Thus it must be true that max (α,β)≤1 [47], and then we get the inequalities (20):(20)1+Kw−Kc1−Kc2+(1+Kw−Kc1−Kc2)2−4Kw2≤11+Kw−Kc1−Kc2−(1+Kw−Kc1−Kc2)2−4Kw2≤1Solving the inequalities (20), inequalities (21) is the result:(21)w≥c1+c22−1KBecause the right term of the inequality (21) is not constant term, it changes along with the change of iterations. According to the inequality (21), w should be greater than or equal to the maximum value of the right term. We know by the formula (8): when T is 0, −1/K is the maximum value, and its value is−1/cos(π/Gmax×0)+2.5/4. According to the inequality (21),wend=((c1+c2)/2)−1(cos(π/Gmax×0)+2.5)/4. If the values of c1 and c2 are 2, thenwend=2−(1/(3.5/4))=0.857143.In addition, the value w is in the range [0,1], according to the experience, wstart takes 1 in this paper. The velocity formula of Program 2 becomes formula (22) when wend and wstart are put into the formula (5).vid=w×vid+2×rand()×(pid−xid)+2×Rand()×(pgd−xid)(22)w=0.857143+(1−0.857143)1−TGmaxifpgd≠xidw=0.857143ifpgd=xidThe obtained parameters are put into the formula (22) (Program 5), the formula (22) becomes formula (23):vid=cos(π×T/Gmax)+2.54×[w×vid+2×rand()×(pid−xid)+2×Rand()×(pgd−xid)](23)w=0.857143+(1−0.857143)1−TGmaxifpgd≠xidw=0.857143ifpgd=xidConstriction factor affects convergence of PSO and inertia weight affects the degree of maintaining the original velocity. Based on the different characteristics of constriction factor and inertia weight, we use them in different periods of PSO. In the early iterations of particles, inertia weight keeps the original velocity in a certain tactics in order to balance the global search and local exploration. In the later, functional constriction factor ensures that PSO can converge to the optimal point. Because the first half-cycle of functional constriction factor is a convex function so that it could take a global search; the second half-cycle is concave function so that the velocity of particle slowly decreases to the minimum, it ensures both local exploration and the convergence. In the iterative process, inertia weight should take a small value so that the particle retains the small part of original velocity for local development when the position of particle is closed to the optimal solution. Inertia weight should take a larger value so that the particle preserved the most of original velocity to find better global optimal solution when the fitness of particle is poor [45]. Thus, the change of inertia weight is showed by formula (24)[46]:(24)w=0.857143+(1−0.857143)1−TGmaxifpgd≠xidw=0.857143ifpgd=xidEq. (25) is the adjusted constriction factor:(25)K=cos(2π/Gmax×(T−Gmax/2))+2.4285714The equation of velocity change is showed in formula (26). (Here, we call it Program 6):(26)vid=w×vid+2×rand()×(pid−xid)+2×Rand()×(pgd−xid)ifT<Gmax2vid=K[α×vid+2×rand()×(pid−xid)+2×Rand()×(pgd−xid)]ifT>=Gmax2The α in the formula (26) is to measure the degree of reserved original velocity of particle. In the case where the particle velocity needs to be reserved, Program 6 uses constriction factor for convergence of velocity. The initial value α is wend, but we found by experiments that value wend retained too much original velocity and leaded instability of the final convergence. So we decide to set α in interval (0, wend] and find that α being 0.7 shows the best performance. Therefore, the change of velocity for Program 6 is shown in formula (27):(27)vid=w×vid+2×rand()×(pid−xid)+2×Rand()×(pgd−xid)T<Gmax2vid=K[0.7×vid+2×rand()×(pid−xid)+2×Rand()×(pgd−xid)]T>=Gmax2The experiment's main goal is to apply the improved PSO to the text feature selection. In the experiment, we used the improved PSO to randomly search text feature set, and then constructed the text feature vector based on this set [30]. The KNN was selected as classifier due to its simplicity. We evaluate advantages and disadvantages among the different improved PSO programs by comparing the classification accuracy and MacF1. The core of the experiment was to compare programs about velocity change. Some parameters in the experiment need to be explained: the K in KNN was 10; Q (population quantity) was 10; Gmax (maximum generation) was 40. Text preprocessing uses the Standard Analyzer in Lucene 3.0. The corpus applied the Reuters-21578.Six programs described in chapter 3 made comparative experiments. Table 1described the six programs.In order to evaluate the convergence of all programs, we recorded each program's specific convergence process in the experimental conditions of feature dimensions 300 and feature dimensions 450. Under feature dimensions 300, the convergence curves are showed in Fig. 2. Program 6 has the best final fitness in all Programs and Program 1 has the final worst fitness. Convergence speed of Program 6 is fast, it merely need less than 20 times iteration to achieve the optimum. Convergence speed of Program 3 is the fastest but its final fitness is poor. Convergence speed of Program 5 is the slowest but its final fitness is the best except for Program 6. Convergence curves are similar between Program 2 and Program 4. Under feature dimensions 450, the convergence curves are showed in Fig. 3. Program 6 also has the best fitness in all Programs and its convergence speed was very rapid. The fitness of Program 1 is merely worse than Program 6, but its convergence speed is the slowest. It needs to iterate over 25 times to achieve the optimum, but Program 6 only needs 15 times. The convergence speed of Program 3 is the fastest, but its fitness is the worst. The optimum of Program 5 is the same to Program 2, and the convergence speed of Program 5 is faster. Program 4 has general performance in optimum and convergence speed. In summary, Program 6 has the best convergence in all programs.Tables 2 and 3are the all experimental results. We can draw some conclusions from the experimental results. First of all, Program 6 presents the highest mean in both accuracy and MacF1, and shows the most excellent results. Meanwhile, all programs have low standard deviations for accuracy and MacF1, it means that accuracy and MacF1 of all programs are stable and balanced. Especially, as for accuracy, the standard deviation of Program 6 is the lowest; as for MacF1, it is the second lowest. So Program 6 presents the most stable performance and it is also the optimal solution.Through the above analysis, the means of Program 6 is highest among six programs for both accuracy and MacF1. In order to verify the means of Program 6 are significantly higher than the ones of other programs, paired-sample T-test was used for significance test and the results are shown in Tables 4 and 5.Table 4 shows that the T-test results of 1 vs 6, 2 vs 6 and 5 vs 6 reject the null hypothesis while the ones of 3 vs 6 and 4 vs 6 do not at a significance level of 0.05. Therefore, Program 6 is significantly different from other programs except for Programs 3 and 4 in accuracy.Table 5 shows that all T-test results reject the null hypothesis at a significance level of 0.05, thus Program 6 is significantly different from other programs in MacF1.Therefore, we can draw conclusion that Program 6 is superior to other programs in the effect of text classification and the stability of different dimensions.To sum up, the Program 6 can achieve the best effect and stability of text classification. In addition, constriction factor and inertia weight have no equivalence because they update the velocity from different aspects. Constriction factor generates effect after the calculation of velocity so that the improved PSO ensures convergence. Inertia weight measures the degree of particles keeping original velocity. Inertia weight cannot act on the part of information sharing and the part of social cognition in velocity formula. Thus, constriction factor and inertia weight have different targets to adjust the velocity of particle and these two adjustments are very necessary to integrate together.

@&#CONCLUSIONS@&#
