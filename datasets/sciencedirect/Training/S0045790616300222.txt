@&#MAIN-TITLE@&#
Hardware implementation of real-time Extreme Learning Machine in FPGA: Analysis of precision, resource occupation and performance

@&#HIGHLIGHTS@&#
Extreme Learning Machine (ELM) on-chip learning is implemented on FPGA.Three hardware architectures are evaluated.Parametrical analysis of accuracy, resource occupation and performance is carried out.

@&#KEYPHRASES@&#
FPGA,Extreme Learning Machine - ELM,Neural network training,Neural network hardware,On-chip machine learning,Embedded systems,

@&#ABSTRACT@&#
Extreme Learning Machine (ELM) proposes a non-iterative training method for Single Layer Feedforward Neural Networks that provides an effective solution for classification and prediction problems. Its hardware implementation is an important step towards fast, accurate and reconfigurable embedded systems based on neural networks, allowing to extend the range of applications where neural networks can be used, especially where frequent and fast training, or even real-time training, is required. This work proposes three hardware architectures for on-chip ELM training computation and implementation, a sequential and two parallel. All three are implemented parameterizably on FPGA as an IP (Intellectual Property) core. Results describe performance, accuracy, resources and power consumption. The analysis is conducted parametrically varying the number of hidden neurons, number of training patterns and internal bit-length, providing a guideline on required resources and level of performance that an FPGA based ELM training can provide.

@&#INTRODUCTION@&#
The Extreme Learning Machine (ELM) algorithm has revived the interest in Single Layer Feedforward Neural Networks (SLFN) since it eases the neural network training procedure. Hardware implementations have explored other computing paradigms, [1–3], but ELM makes SLFN possible in applications where it was not allowed before due to computational requirements, especially in on-line systems with real-time data processing and tight timing constraints.ELM algorithm is rooted in the previous idea defined as Random Vector Functional Link (RVFL) networks, [4], but introduces new concepts and advantages that outperforms RVFL, [5]. Thus, ELM is based on random hidden layer weights and a linear adjustment for the output layer. As advantages, it provides similar or better results in generalization performance than previous iterative training methods without the need for manually adjusting parameters to find the best behavior. Further, ELM can use a wide range of activation functions, including those piece-wise linear. Finally, it is remarkable the combination of reduced computation requirements and extremely fast learning speed, surpassing previous training algorithms.The reduced and fixed training time is one of the ELM properties that has contributed to its wide application in different fields, especially in battery operated devices where training time is a key factor. ELM has been successfully applied in the power energy area for failures and overload detection, in electricity distribution lines, and electricity price prediction. Other applications are related to biometrics, soldering inspection, or very fast object detection. In the field of control systems, ELM was already applied to real-time computation of friction in automobiles, new control strategies in non-linear control, or non-linear behavior compensation in fiber optic communications. Additional applications are reported for bank client classification, multi-categories classification applications, discrimination of evoked potentials, [6], or epileptic EEG detection, amongst others.Moreover, the characteristic fixed and tight temporal requirements enable ELM to land in the hardware implementation arena to obtain efficient classifiers, usually implementing the forward phase. As an example, an FPGA implementation of the neural network forward phase was proposed in [7], which training phase was calculated on a PC. Others approaches have used VLSI techniques, [8]. However, its temporal requirements make ELM a good candidate to drive also on-chip training in real-time applications.There is a current trend to implement on-chip learning for pattern recognition, facial recognition and complex learning behaviors. As an example, [9] proposed an on-line learning ANN performing a model training every new arriving data, to monitor and forecast the indoor temperature in a smart home, improving the energy efficiency of the HVAC (Heating, Ventilating and Air Conditioning); [10] focused the problem of sequential learning in mobile devices in real-time by a real-time learning algorithm for face recognition related applications; or [11], that proposed a real-time learning of neural networks to the prediction of future opponent robot coordinates in a specific environment.The interest in hardware implementation of ELM training arises in the applications where input dataset does not increase excessively during the application, as in DBS (Deep Brain Stimulation) surgery, [12]. However, much more potential real-time applications can be driven by an on-line sequential learning algorithms as OS-ELM, which is a variation of basic ELM, and that anyway needs the implementation of the basic ELM training for initialization. Diverse OS-ELM sequential learning applications have been proposed to the date, as an example [5] adapted a gesture recognition model to new users automatically, getting high recognition accuracy.As the expansion of on-chip learning systems is just beginning, and ELM is a good candidate to implement it, then arises the research question of how many resources usage the ELM training algorithm need, and which performance can it provide on current FPGA devices. Answering these questions parametrically (it is as a function of parameters like word width, number of training patterns or number of hidden neurons) would offer a quick sight to the needs and requirements of any application to be implemented on FPGA using on-chip ELM learning.To answer this questions, we propose a hardware implementation of an SLFN, including ELM training, and the analysis of the performance, accuracy, resource occupation and power consumption that an FPGA implementation demands. The analysis was conducted parametrically varying the number of hidden neurons, number of training patterns and bit-length. The subject of the analysis was three proposed hardware implementations, all three under FPGA with a fixed-point implementation of the ELM training of a generic SLFN neural network. At the end, the results provide a guideline on expected resources, accuracy and performance that an FPGA implementation can provide for ELM batch training; proposes different hardware implementations for ELM training, describing a customizable core to fit multiple applications by simple parameter modification; and allows to compare between computational methods and parallelizing approaches, balancing among accuracy, logic occupation and performance. The results are applicable to any classification or regression application based on an SLFN neural network, providing a guideline on required resources and level of performance that an FPGA-based ELM learning can demand.This paper is organized as follows. Details of the proposed architectures and computational methods are described in Section 2. Section 3 describes the hardware structures. Results of the analysis and discussion are presented in Sections 4 and 5, respectively. Finally, Section 6 concludes the paper.The basic principle of ELM is that parameters of hidden nodes (the input weights and biases for additive hidden nodes or kernel parameters) need not be traditionally tuned by conventional learning algorithms (e.g. gradient descent-based method). The hidden nodes parameters can be randomly assigned. Right after, the output weights linking the hidden layer to the output layer can be analytically determined through simple generalized inverse operation of hidden layer output matrices [13].We briefly delineate the principles of the ELM algorithm (Section 2.1) and explain and justify the computation method selected (Section 2.2).LetD=(xi,oi); i = 1,…,N, be a set of N patterns where{xi}∈Rd1are the input and{oi}∈Rd2the output data for the training set, so that the goal is to find a relationship between {xi} and {oi}. If there are M nodes in the hidden layer, the general jth SLFN output is given by oj:(1)oj=∑k=1Mhk·f(wk,xj)where 1 ≤ j ≤ N, wkstands for the parameter of the kth element of the hidden layer (e.g. weights and biases in a SLFN) and can be denoted as the matrix W, f is the activation function generating the output of the hidden layer (applied to the scalar product of the input vector and the hidden weights), and hkare the weights connecting the kth hidden element with the output layer (1 ≤ k ≤ M), which are the target values to obtain. For the output vector o, Eq. (1) can be expressed in matrix notation aso=G·h,where h is the vector of weights of the output layer and G is given by:(2)G=(f(w1,x1)…f(wM,x1)⋮⋱⋮f(w1,xN)⋯f(wM,xN))After random initialization of the M parameters of the hidden layer, wk, the weights of the output layer are obtained by the Moore–Penrose generalized inverse [14] according to the expressionh=G†·o,whereG†=(Gt·G)−1·Gtis the pseudo-inverse matrix (superscript t means matrix transposition). Once G† is calculated, the weights h are obtained and thus the system is trained according to the proposed set of training patternsD.Typically, the number of patterns is large compared to the number of hidden nodes (N > >M). In this caseh=G†·obecomes an over determined equations system as we have more equations than unknowns. They can be solved using generalized inverse matrices, which create the normal or least square equations where the unknowns are those minimizing the mean square error [15]. The obtained solution meets three important properties [16]: provides the least training error, the least norm of all possible solutions, and guarantees that the minimum norm solution is unique.The main steps in hardware implementation of the ELM algorithm are those involved in computing the matrixG†=(Gt·G)−1·Gt,where G is known once input weights and hidden node biases W are randomly assigned. Thus, required computations are matrix inversion, matrix transposing and matrix multiplication.Matrix inversion is one of the most important operations when dealing with ELM training. There exist different decomposition methods for matrix inversion computation, such as QR, LU or Cholesky. The selection of the decomposition method depends on the characteristics of the given matrix. For non-square matrices or when direct inversion to recover the data performs poorly, the QR decomposition is used to generate an equivalent upper triangular matrix.All three architectures proposed in this work make use of QR decomposition (QRD) for matrix inversion, since it simplifies and reduces the computational complexity of the matrix inversion operation [17]. The other methods were discarded due to greater complexity ([18]), especially in the case of high dimension rectangular matrices as is the case of ELM.The basic goal of QR decomposition is to factor a matrix as a product of two matrices typically called Q and R. For a given square matrix A, its inverted matrix can be computed asA−1=(Q·R)−1=R−1·Qt. If A is a non-square Matrix, the pseudo-inverse of A is found asA†=R−1·Qt,where Q is also a non-square matrix. In both cases, matrix Q is orthonormal withQ·Qt=I(t shows transpose, I is the identity matrix) and R is a triangular matrix.Three different algorithms are possible [19] to perform QR decomposition:•Gram–Schmidt. Is one of the key algorithms in linear analysis of finite spatio vectorial systems. This method leads to polynomials and orthogonal functions in the function space.Householder reflections. Is a linear transformation that describes a reflection about a plane or hyperplane containing the origin. It is typically used when least square methods need to be solved.Givens rotations. Are orthogonal plane rotations used to eliminate elements within a matrix. By applying a series of successive Givens rotations, a matrix can be triangularized by eliminating the elements beneath the diagonal. It is used for adaptive filter implementation, requiring twice the computational cost compared to previous methods [20].Givens rotations are frequently used to solve QR decomposition in hardware implementations due to the IP cores availability [20]. At the same time, Givens rotations is most accurate and stable than classical Gram–Schmidt algorithm. Moreover, the classical Gram–Schmidt algorithm is unstable under hardware implementation, [21]. On the other hand, Gram–Schmidt uses less resources than Givens Rotations or Householder reflections. Nevertheless, stability and optimization of resources can be combined using a slightly modified version of classical Gram–Schmidt, referred to as Modified Gram–Schmidt (MGS), [16,17]. This MGS algorithm has proven to be numerically equivalent to Givens Rotations QR factorization, [15], requiring less operations and being able to be implemented with fewer resources.The proposed implementation follows a fixed-point architecture to optimize resources usage and boosting the maximum frequency of operation. This implies eventual quantization and round-off errors that induce losses in accuracy and hence, in orthogonality of Q. However, [22] established that Modified Gram–Schmidt (MGS) algorithm is still stable and accurate to the integer multiples of the machine precision under fixed-point precision, for a well conditioned non-singular matrix. For the above reason, the proposed implementation follows Modified Gram–Schmidt (MGS) to solve QR decomposition as other works before, [23].Thus, the computation process to invert a square matrix (the pseudo-inverse matrix in the general case) is as follows:•Apply the QR decompositionInvert R through a Triangular Matrix Inversion (TMI). Since R is triangular, its inverse computation procedure is simpler than the inverse computation of A.Use the matrix multiplicationA−1=R−1·Qt(back-substitution) to obtain the required matrix inversion.Once matrix inversion has been done, output layer weights h are obtained by a matrix multiplication between the resulting matrix G† and the known output values for the training set,h=G†·o.Usually, many applications use online learning algorithms due to their ability to learn by the sequentially arriving data. An algorithm that allows this continuous learning is OS-ELM, a variation of the ELM algorithm, [24,25]. If we pay attention to the hardware implementation of OS-ELM learning method, we can see that the implementation of the ELM training is also required. It is due to the necessary initial calculation of output weights h0 as the first iteration of the algorithm, using batch learning for the initial training dataset.Some applications use sequential learning during a short span, in which not a big amount of sequential patterns will incorporate during the process. These applications can take profit of ELM training either iteratively increasing the training dataset or replacing some less significant patterns in the original dataset. If this is the case, the new information can be learnt on-chip by using an IP core implementing the ELM algorithm proposed in this work.An example are medical or surgery applications seeking to locate or detect the point in which an electrode must be placed to apply certain therapy. In this case, the use of a decision support tool is of great help. The learning starts from an initial dataset which is continuously changing by adding new collected data as surgery is taking place. This learning contributes to fine-tuning the location support tool for the patient through data obtained sequentially from himself, progressively increasing the power of the decision support. As it can be seen, these applications imply the collection of data, but the real-time retraining of the support tool lasts a short time span until the physicist/surgeon decides the optimal point applying the therapy.A more specific example is Deep Brain Stimulation (DBS) surgery. In this therapy, a microelectrode stimulator is inserted into the brain to face neurological disorders such as Parkinson’s disease. But the key of the process is to find the adequate brain place where stimulation must be done. To obtain better precision and physiological validation of the target detection, surgeons use microelectrode recordings (MER) which are introduced step by step until the inner brain, recording action potential signals at each step. Fig. 1illustrates the penetration trajectory of a MER microelectrode, first registers correspond to thalamus, then pass through the subthalamic nucleus and finally lasts registers corresponds to substantia nigra brain area. First and lasts registers from each trajectory can fed the system labelled automatically. Moreover, as several trajectories are normally done, registers from previous trajectories can be used to generate the patterns that increment the dataset. All this new information can be used to launch ELM training repeatedly, being its hardware implementation of great help to surgeons, [12].In this work, we propose three hardware architecture alternatives for the implementation of the ELM training of a single layer feedforward neural network (SLFN). All three proposals use QR decomposition and Modified Gram–Schmidt methods to cope with matrix inversion since these methods optimize resources usage and stability, as seen in Section 2.2.It is remarkable that all three proposals provide general hardware implementations. It implies that the hardware can be adapted for the application requirements modifying implementation parameters as: neural network size (number of hidden neurons), internal computation architecture, internal and external data bit-length and size of the training dataset (number of training patterns). Thus, all three architectures are fully parameterizable to be easily adapted to any classification or regression application requirements.All three approaches use fixed-point arithmetic to reduce resource usage and increase performance. Typically, floating-point arithmetic is used when the computation takes place on a desktop PC. However, in embedded systems it is desirable to use fixed-point arithmetic to optimize resource usage and performance. Nevertheless, to obtain accurate results avoiding the use of floating-point arithmetic, the fixed-point arithmetic needs a flexible word-length scheme for internal computations. It is an important characteristic that helps to improve accuracy. Hence, all architecture proposals use bit-length of internal and external computations as adjustable parameters. The main objectives of the developed hardware implementation are the following:•Parameterizable. The ELM characteristics can be defined for each problem. The matrix inversion computation is offered as a stand-alone computation module for general usage, easily parameterized so that different applications can make use of it.Accurate. Accuracy must be comparable to floating-point arithmetic.Balanced performance. Due to the code optimization for arithmetic computations, a good trade-off between accuracy, logic occupation and performance must be achieved.To sum up, the aim of proposing three approaches is: to study how novel computation architectures cope with ELM training on FPGA; to parametrically measure accuracy, performance and logic occupation; and to outline a comparison between them. Thus, for all proposals it has been used novel computation architectures and optimized control data flow, in order to obtain a fast and accurate hardware processing unit, according to selected parameters.All three versions of ELM training hardware implementations (from now named ELMv1, ELMv2 and ELMv3 versions) differ basically in the use of parallelism and the computation procedure. The computation procedures consist on different approaches to solve the equations system given byG·h=o.Despite three implementation approaches having been proposed, they are based on two computation procedures:1.Square Matrix Decomposition: MatrixZ=Gt·Gis built, and the QR decomposition of Z is done.Rectangular Matrix Decomposition: The QR decomposition of matrix G is directly calculated.The difference between hardware implementations is in the training procedure referred as TRAIN_MODULE, Section 3.3. The rest of the modules, the external hardware interface and the configuration parameters are the same in all versions. Differences between hardware implementations are in the used computation procedure and in the parallelization effort. Table 1lists the computation summary for each proposed method, readily:1.ELMv1. Use serial computation procedure for square matrix. Computations are performed sequentially in order to share arithmetic units and reduce logic occupation.ELMv2. A parallelized version of ELMv1. Parallelization efforts are focused on the most time consuming computations, i.e. the QR decomposition and the Triangular Matrix Inversion (TMI) algorithm. This implementation requires duplication of the matrix multiplication unit. Meanwhile, as shown in Table 1, step 5 (inverse Z matrix computation) and step 6 (G† matrix computation) are indirectly obtained by concurrent calculation of temp1 and QRD of Z in step3, andR−1computation and temp2 in step 4. Since two computation algorithms are carried out in parallel, two computation steps are saved compared to ELMv1.ELMv3. The computation procedure uses the rectangular matrix decomposition. As rectangular QR decomposition is a time consuming process, similar parallelization to ELMv2 is done with one more computation step reduction compared to ELMv2 since no square matrix is required (step 2).As a summary, it is remarkable to note that first ELM version (ELMv1) is computed in a sequential fashion using square matrices; second ELM version (ELMv2) uses parallelization and computes using square matrices; and finally, third ELM version (ELMv3) uses parallelization and computes using rectangular matrices. Parallel processing in ELMv2 and ELMv3 allows faster processing by reducing the number of steps in the algorithms (Table 1).The proposed hardware implementation is conceived as an IP core (Intellectual Property block). Hence, it can be used as an standalone module, or as a part of a more complex System on Chip with additional peripherals, embedded microprocessor, etc.The hardware structure is based on four main blocks:1.TRAIN_MODULE : ELM training block where weight calculation is carried out during batch learning.ANN_MODULE : On-line working mode for output data calculation once the weights are obtained.RAM memories for data storage. These blocks are shared by different computation units and consist of:(a)RAM_W: Stores hidden layer weight matrix W values.RAM_T and RAM_D: Stores desired output values vector o for training input data x (T stands for target and D for data).RAM_Wo: Stores weight vector h for the output layer values.Data flow control logic.Run or On-Line mode. After the NN is trained, this mode is executed for classification or regression of the targeted application. Input data is accepted and computation procedure begins. When computation is done, output data is ready to be read and a new input data can be accepted.Training mode. Data required for training are loaded (matrix W and vectors x and o) and, after that, the training process is carried out.Memory Access. If no training or run mode is active, read/write of RAM stored data can be done. This is very useful for debugging and initialization.Finally, as it was mentioned before, the system is configurable by modifying the implementation parameters. This reconfiguration capabilities allows the hardware implementation to suit any application. Thus, the hardware can be easily adapted to any training dataset, number of hidden neurons, number of outputs, and internal or external word length, amongst others. Hence, the training process and neural network structure can be parameterized by the following parameters:•N_FIELDS: size of data input vector.N_PATTERNS: number of training patterns (NP).N_NEURONS: number of hidden neurons (NH).N_OUTPUTS: number of outputs.N_DATA, M_DATA: bit-length of input data (integer and fractional).NBITS_INTEGER, NBITS_FRACTION: bit-length of internal data (integer and fractional).WIDTH_RAMi, i = 1,…,7: bits needed for the defined RAM.NORMALIZE: Activation of the scaling function.NORM, NORM1, NORM2: size of scaling in different steps of ELM algorithm.L, K1, K2: Constants for the fuzzy activation function.These parameters must be specified by the IP core user to generate the desired hardware. From this point we will talk about bit-length to refer to the internal data representation word width.This module performs the ELM training, calculating the weights that on-line working mode needs to run properly. TRAIN_MODULE is different for each implementation version. However, for code reuse, all versions use the same basic computation modules: matrix multiplication, QR decomposition and TMI.The TRAIN_MODULE block is subdivided as Fig. 2shows. Basically, it is composed of a general control unit TRAIN_CTRL, OPS_UNIT block containing the basic arithmetic units, ADDR_UNIT for RAM address generation, small size RAM blocks for temporary storage (in addition to external RAM blocks) and matrix operation state machines (FSM_MM for matrix multiplication, FSM_QRD for QRD and FSM_TMI for triangular matrix inversion).Additionally, OPS_UNIT include low level computation blocks as simple multiplier, vector multiplier, division and square root units, used in different computation algorithms. Multiplication operation is designed to take advantage of embedded multipliers in FPGA devices. Vector multiplication is using dual port RAM and two parallel multipliers to perform two multiplications in a single clock cycle. Division and square root use sequential algorithms where N clock cycles are required for an N-bit operation. Additionally, bit scaling in the vector multiplication and division modules must be performed at certain stages of the calculation process to avoid overflow.Note that, as fixed-point arithmetic is used, the accuracy is sensitive to the number of bits in the fractional part, and the integer part is important to avoid data value overflow.One of the most important arithmetic modules is related to the QR decomposition algorithm (step 3 in Table 1). The block FSM_QRD allows the wise use of computation units and distributes the calculation process among different stages, obtaining a reduced calculation time and an intensive use of computation units for each of the states. Memory management is also very important during QRD calculation, which is solved by allocating specific memory areas and using them iteratively once the stored data are no longer needed. The logic resource usage is kept low by using specific memory addressing [26].The decomposition process for a given matrix X is structured in different computation steps:1.The diagonal elements of R matrix are obtained according toRii=Xi·XiThe column vectors of matrix Q are calculated byQi=XiRii.The rest of R matrix elements are obtained in a loop.Matrix X is updated asXj=Xj−Rij·Qiand the process is repeated for all columns.Matrix X is used to store initial values, serving also as intermediate memory storage during the computation process. This matrix, together with the vector multiplication and the square root module, is used to calculate the Euclidean norm. The result is stored in matrix R, then, division of the first column of X by the Euclidean norm is done to obtain the first column of matrix Q. The vector multiplication module is used to multiply Q and X matrices in order to calculate the first row of R. The signed multiplication and a subtraction unit updates the values of matrix X; this procedure is repeated for all the columns in Q and rows in R until the process is completed.State machine FSM_QRD implements the steps of the algorithm and controls specific operation units inside the general block OPS_UNIT.Along all the computation steps, the algorithm involves the calculation of vector modulus and vector division by a constant value, requiring computations as vector product, root square and division, being performed by the OPS_UNIT.As mentioned before, this version use serial computation for square matrix, being the most time consuming steps the QR decomposition and the TMI algorithm (steps 3 and 4 in Table 1). Fig. 2 shows the block organization and control flow.The most important block affecting occupation, performance and accuracy of results is related to the computation units. This block is called OPS_UNIT and it is controlled by the general control system TRAIN_CTRL (Fig. 2). Internally, it includes specific control machines for different arithmetic operations. The OPS_UNIT block contains the four main blocks to operate: Vector Multiplication (MultVec_Unit), Root Square (SQRT), Division (DIV) and Multiplication–Subtraction (SM), able to assess two operations in a single clock cycle. The DIV unit is used in QRD and TMI algorithm computation, SQRT and SM is used in QRD, and MultVec_Unit is used in QRD, TMI and matrix multiplications.Data scaling is used to avoid data overflow or underflow when operation results are too high or low, reducing the accuracy but allowing the usage of reduced data bit-lengths. After QRD is finished, the Triangular Matrix Inversion (TMI) procedure is performed (step 4 in Table 1), involving basic multiplication and division operations. Finally, basic matrix algebra is intensively computed in steps 5 to 7 using the OPS_UNIT, specifically, MULT_VEC.In the second version, parallelization of computations in ELMv1 are done when possible. In this case, the control module is modified (OPS_UNIT2) replicating some arithmetic units in the OPS_UNIT block from ELMv1, to add concurrency. The parallelization allows to perform different vector multiplications (states S3b and S4b) and the QRD and triangular matrix inversion (states S3 and S4) concurrently (Fig. 3). The nature of computation procedure prevents greater degree of parallelism.The control module is modified by splitting address space and control signals. It assigns one entire address unit to the FSM_MM block and applies changes in the internal control machines for the TMI, QRD, memory access and other control signals for the synchronization of all processes in the modified data flow.As stated in Table 1, the third ELM version operates on rectangular matrices. This fact makes a difference with previous versions as the QRD algorithm is applied directly to the matrix G, and no additional matrix multiplication is required to obtain the final result for output layer weights h. As a result, S2 and S3b states from ELMv2 (blue in Fig. 3) are removed in the training control scheme of ELMv3. This procedure reduces the computation steps, but the rectangular matrix has a higher dimension. It is necessary to modify the control unit TRAIN_CTRL for proper data addressing and flow into computation units.States S4b and S4 are performed concurrently. The arithmetic block OPS_UNIT2 was adapted in order to adapt new control signals and data scaling. In this case, an important modification has to do with scaling. As it is not required to construct the square matrix, high values are not present in the input data matrix to the QRD algorithm. Thus, initial scaling is performed inside the QRD process.This module was developed to work on-line, providing neural network output results efficiently. It uses the results of the ELM training to calculate the output according to the desired application for classification, prediction, estimation, etc. The calculation is done independently of the TRAIN_MODULE, allowing multiple tests of different learning algorithms. Thus, ANN_MODULE remains unchanged for all three ELM versions.ANN_MODULE follows a serial procedure to obtain the output values. First, the hidden layer weight matrix W is multiplied by the input vector x and the neuron activation function is applied to the result, obtaining the hidden layer output. Second, the output layer weight vector h is multiplied by the hidden layer output, and the result is applied to the activation function to obtain the output values.Fig. 4shows the block diagram of ANN_MODULE, including the data flow of the ANN_CTRL synchronization module, the FSM_MM as the state machine controlling matrix multiplication performed by ‘MultVec_Unit’, the FUZZY_FUN module implementing a fuzzy activation function [27] for the neuron, and the ADDR_UNIT module which generates the proper addresses for data access to RAM memories: RAM_W for hidden layer weight values, RAM_Wo for output layer weights h, RAM_I for input data, and RAM_O for hidden layer output data.

@&#CONCLUSIONS@&#
Implementing the Extreme Learning Machine (ELM) training algorithm, a Virtex 6 based system can train a Single Layer Feedforward Neural Network (SLFN) using a large dataset (250 training patterns) up to 100 times per second while other computing platforms as a PC can perform 50 at 22x more power consumption. This fact, together with a high power efficiency, small size and portability, offers new possibilities for on-chip learning in neural networks with applications in many different fields.Three computational methods for the ELM algorithm are proposed to fit real-time operation, offering a generic code to be implemented under FPGA and, in general, in any VLSI system. All three methods, including sequential and coarse-grain parallel implementations, are compared in performance. The analysis shows that the parallel version computing QR decomposition of square matrices (ELM_v2) offers the best balance among accuracy, logic occupation and performance.Finally, this study provides a guideline to the implementation of neural network batch learning using the ELM algorithm. It shows the level of performance that can be obtained using ELM batch learning under embedded systems as FPGA for different neural network topologies, and the amount of required hardware resources.