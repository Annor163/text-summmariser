@&#MAIN-TITLE@&#
Co-trained generative and discriminative trackers with cascade particle filter

@&#HIGHLIGHTS@&#
We formulate our visual tracker in a co-training framework to label incoming data continuously.We employ the cascade particle filter framework to improve the speed of our tracker significantly.Our tracker adaptively learns the appearance of the object; thus, it allows reacquiring an object after total occlusion.We carefully tested the components of our tracker and compare with many recent state-of-the-arts

@&#KEYPHRASES@&#
Visual tracking,Cascade particle filter,Co-training,Discriminative tracker,Generative tracker,

@&#ABSTRACT@&#
Visual tracking is a challenging problem, as the appearance of an object may change due to viewpoint variations, illumination changes, and occlusion. It may also leave the field of view (FOV), then reappears. In order to track and reacquire an unknown object with limited labeling data, we propose to learn these changes online and incrementally build a model that encodes all appearance variations while tracking. To address this semi-supervised learning problem, we propose a co-training framework with cascade particle filter to label incoming data continuously and online update hybrid generative and discriminative models. Each of the layers in the cascade contains one or more either generative or discriminative appearance models. The cascade manner of organizing the particle filter enables the efficient evaluation of multiple appearance models with different computational costs; thus improves the speed of the tracker. The proposed online framework provides temporally local tracking that adapts to appearance changes. Moreover, it provides an object-specific detection ability that allows to reacquire an object after total occlusion. Extensive experiments demonstrate that under challenging situations, our method has strong reacquisition ability and robustness to distracters in clutter background. We also provide quantitative comparisons to other state of the art trackers.

@&#INTRODUCTION@&#
This paper aims at automatic visual tracking, i.e. once an object of interest is selected, our algorithm automatically tracks the object and reports a confidence which can be used to determine if the object is lost or out of field of view (FOV). When the object reappears, our algorithm reacquires the object and continues tracking.We address three challenges in this problem:1.Appearance Changes: Varying appearance, which can be caused by the changes in viewpoints, poses and illumination conditions, is one of the major challenges in visual tracking. New instances of the initially labeled object may constantly appear during tracking. Thus, visual tracking problem can be regarded as a weakly supervised learning problem. Very little supervised data is available in visual tracking. Improper updates of the appearance model (or no update) is the main reason of tracking drift, which is the most commonly seen failure in tracking.Reacquisition: Persistent visual tracking requires the tracker to have the self-awareness of the status of tracking. A track is supposed to know if the object is out of FOV or is occluded, then reacquires the object when the object reappears. The solution requires an object-specific appearance model, in other words, a particular detector for “the” object, which has to be learned on-the-fly.Time Performance: The success of visual tracking in recent years is mainly due to the powerful appearance models that have been used in visual tracking, such as [3,5,7,12,33]. However, the real-time performance of visual tracking is also an important factor in practice. A good balance of between the complexity of appearance models and the efficiency is desired.We propose a co-training framework of generative and discriminative trackers with cascade particle filtering to address the above challenges. First, we formulate the appearance based object tracking as a semi-supervised learning problem: the process of selecting the object of interest before the automatic tracking can be considered as a process of providing labeled data in semi-supervised learning. Due to the appearance changes, the initially labeled data cannot fully represent the characteristics of entire distribution. A visual tracking approach needs to “learn to adapt” to the new appearance changes. Many visual tracking approaches are performed in a self-learning manner, where the sample with the most confident score evaluated by its own model is used to update itself. Here, we consider an example shown in Fig. 1with a simple one dimensional distribution, where positive samples have two modes and negative samples contain one mode. All training samples are given sequentially. Except the few labeled training samples given at the very beginning, the rest of training samples are given as unlabeled. The dilemma of self-learning is shown in Fig. 1. If one adopts a strict threshold to update its model, the final model never learns new characteristics different from the initial labeled data. Thus, it will end up with a single mode either in zone a or zone c. On the other hand, if one adopts a loose threshold, its model is contaminated by outliers quickly and it will end up in zone b. Thus, self-learning is not a good way of weakly supervised online learning. Co-training proposed by Blum and Mitchell [18] is a principled semi-supervised training method. The basic idea is to train two classifiers on two conditionally independent views of the same data (with a small number of exemplars) and then use the prediction of each classifier to enlarge the training set of the other. It is proved that co-training can find an accurate decision boundary, starting from a small quantity of labeled data as long as the two feature sets are independent [18]. Empirical results [19] show that co-training also works well in the case where the independence is not perfectly satisfied. In our visual tracking setting, although our initial tracking samples are limited, if we regard multiple complementary features as approximated conditionally independent views of the same data, we can apply the co-training framework to combine multiple models to avoid the issues in the self-learning. One can certainly transform the semi-supervised learning problem to a supervised-learning problem for some specific applications: for instance, if the category of the object of interest is known, one can incorporate a model trained with a large amount of offline labeled data to compensate for limited online data, as in [30], or if tracking is allowed to be performed offline with human interaction, eg. directly adding new training data in a bootstrap manner as in [36]. These approaches go beyond the scope of general automatical visual tracking problem and require further information provided from user interaction.Second, instead of combining multiple cues in a linear way, which increases the complexity linearly, we adopt cascade particle filter [30] to balance robustness and computational efficiency from multiple models. Instead of evaluating all models equally, this approach evaluates computationally cheaper models at earlier stages and more expensive models at later stages where much fewer particles remain. The cascade particle filter naturally combines with the co-training framework where multiple models need to be learned and evaluated on-the-fly. We call this proposed framework Co-trained Cascade Particle Filter (CCPF). Compared with co-training all features at the same stage, CCPF benefits from the robustness in co-trained multiple models and reduces the computational costs of different models. The CCPF framework is shown in Fig. 2.Third, while the CCPF framework separates various features into different stages, the last stage of the CCPF makes the final decision for object reacquisition. Thus, besides the tracking capability, the end-product of the tracker is also a detector of the particular object that has been tracked. The detector contains all the appearance variations of the object that have been observed since tracking is started, and can be used to reacquire the object once it reappears.The rest of this paper is organized as follows. The related work is presented in Section 2. The overview and the advantages of our proposed framework are presented in Section 3. All of the online appearance models of trackers are described in Section 4. Then the experiments are shown in Section 5, followed by summary and future work.

@&#CONCLUSIONS@&#
