@&#MAIN-TITLE@&#
A new knowledge-based constrained clustering approach: Theory and application in direct marketing

@&#HIGHLIGHTS@&#
This paper proposes a straightforward way to apply constrained clustering.Soft attribute-level constraints are generated based on feature order preferences.Practitioners can formalize and use their a priori knowledge.A methodology implementing this approach is applied in a direct marketing context.

@&#KEYPHRASES@&#
Data mining,Constrained clustering,Customer profiling,Business knowledge,Direct marketing,

@&#ABSTRACT@&#
Clustering has always been an exploratory but critical step in the knowledge discovery process. Often unsupervised, the clustering task received a huge interest when reinforced by different kinds of inputs provided by the user. This paper presents an approach giving the possibility to incorporate business knowledge in order to guide the clustering algorithm. A formalization of the fact that an intuitive a priori prioritization of the variables might exist, is presented in this paper and applied in a direct marketing context using recent data. By providing the analyst with a new approach offering different clustering perspectives, this paper proposes a straightforward way to apply constrained clustering with soft attribute-level constraints based on feature order preferences.

@&#INTRODUCTION@&#
Data mining techniques and tools have been responsible for many of artificial intelligence's recent successes (e.g. [1–3]). Amongst these techniques, clustering has always been an exploratory but critical task in the knowledge discovery process and has been applied in nearly all domains in which the grouping of similar objects makes sense (e.g. [4–6]). Ranging from the most simple techniques, such as the k-means algorithm (e.g. [7,8]), to the most advanced approaches, such as kernel methods [9] and spectral approaches [10], clustering techniques have received interest from both the scientific and the business community. The users of such techniques are sometimes in possession of background knowledge and would like to include it into the clustering exercise. The usage of constraints in order to integrate this knowledge into the clustering task, i.e. constrained clustering, is a current active research topic leading to different approaches and techniques (see e.g. [11–13]). Amongst the different constraints’ levels, instance-level constraints, based on pairwise information, such as the famous must-link and cannot-link[11], are widely discussed in the literature [14–17].Although this kind of constraints is quite known, consider the myriad of papers dealing with semi-supervised clustering, other interesting levels have emerged in the literature and are of great interest when dealing with knowledge integration. In [18], the authors propose a variant of the k-means algorithm with cluster-level constraints, ensuring that no empty clusters or clusters with very few data points are obtained. In [19], an algorithm constrained by feature order preferences is presented, in which attribute-level constraints are used as part of the to-be-optimized objective function. Attribute-level constraints are also present in [13] where must-link and cannot-link are adapted by creating constraints on the attributes’ values. Moreover hybrid approaches integrating different levels have been proposed, see e.g. the approaches of [20] in which both instance- and attribute-level constraints are used in order to guide the clustering task.Another important dimension concerns the degree to which the constraints have to be satisfied, leading to the concepts of hard and soft constraints. On the one hand, hard constraints are constraints that are required to be fully satisfied. For example, the COP-KMEANS algorithm, presented in [11], requires the assignment of each point to a cluster such that instance-level constraints are not violated. If no such cluster exists, the algorithm fails. The same reasoning is used in [13] in which the mlx-k-Medoids algorithm is presented which fails if no cluster not violating a set of attribute-level constraints can be found. Bradley et al. [18] proposed a cluster-level constrained version of the k-means algorithm, which has to satisfy k hard constraints imposing that a cluster k has to have at least τkdata points. On the other hand, soft constraints are used to guide the algorithm while accepting a partial violation (satisfaction) of the constraints. For example, Wagstaff [21] introduces the notion of soft-constraints in a soft version of COP-KMEANS, SKOP-MEANS, in which a strength factor α is used to indicate the reliability of a constraint. The objective function is penalized if constraints are violated proportionally to the value of their respective α, allowing a violation of the constraints while trying to minimize it. The same approach is used in [22] in which the authors apply soft-constraints in mixture clustering by using fuzzy constraints and a strength factor γ and optimize an objective function minimizing the constraints’ violations. In [19,12], parameters λ andware respectively used in order to fix the significance of the added constraints. Finally, in [23], the authors are using pairwise judgments of similarity and dissimilarity in a soft way by trying to find a partitioning of the vertices into clusters so that the number of violations is minimized.In this paper, a new approach based on business considerations is proposed in order to incorporate business knowledge into the clustering task in an easy and efficient way. The goal of this approach is to focus on the perceived value of the partitioning resulting from the clustering task and not only on the statistical aspects of it (see e.g. [12]). This approach is based on the fact that business people, experts or not, have some insights regarding the importance of the variables before actually starting the analysis. If this insight is limited, an unconstrained approach has to be used, which has already been widely discussed in the literature. However, if this insight is considered sufficient, new approaches are needed in order to consider the a priori knowledge as a critical input for the clustering task. By incorporating this knowledge, the comprehensibility and the perceived value of the clustering will increase. From a more technical point of view, we propose a straightforward approach to transform background knowledge about features’ importance into a metric that is used, as an example, to constrain the Self-Organizing Map algorithm in a soft way, leading to a soft-constrained attribute-level clustering approach based on metric learning. In a related work, Sun et al. [19] propose a solution to a problem which looks quite similar to the one this paper is tackling but which, in fact, is totally different. Sun et al. [19] propose a formulation of a clustering objective function penalizing the violation of feature order preferences making use of background knowledge about the importance of the features in order to create soft attribute-level constraints by parameterizing a weighted distortion measure. This objective function is further incorporated into a prototype-based clustering algorithm in which an iterative approach is used to converge to an accurate partitioning of the data points. Although the approach of Sun et al. [19] and the one proposed in this paper make use of feature order preference, the purposes of both methods are different. In [19], the approach and the algorithm lead to a better capturing of the ground truth if some background knowledge about the importance of the variables is available. This notion of importance is objective and is purely data-driven. Therefore, during their experiments, the “true” number of clusters is provided and simulated feature order preferences are generated using the ground truth class information. This idea of ground truth information is also exploited in [24], where a semi-supervised clustering method for incorporating instance-level and attribute-level information is proposed. In their work, attribute-level constraints are in the form of order preferences, generated using ground truth class information which enables the calculation of rough estimates of the optimal attribute weights leading to the order preferences. For their experiments, 6 UCI data sets with known class information are used to evaluate the proposed methods. In contrast, the proposed paper introduces a subjective, goal-driven notion of importance. Indeed, the soft attribute-level constraints of this paper are based on feature order preferences that reflect the importance of the variables as perceived by the analyst, hence introducing a bias guiding the algorithm and providing the analyst with a powerful exploratory knowledge-based tool.The remainder of this paper is structured as follows. In section “Techniques used”, the necessary background for this paper is introduced. Section “Prioritization approach” presents the approach for the prioritization of the variables in the clustering task. In section “Methodology implementing the prioritization approach”, a 5-step methodology implementing the proposed prioritization approach is described. In section “Application”, the theory is illustrated in a direct marketing context by a comparison between the results of the proposed approach and the traditional approach. The main conclusions are summarized in section “Conclusion”.In this section, the SOM algorithm, the k-means algorithm and the concept of salient dimension extraction are presented.Kohonen maps, also called self-organizing maps (SOM), have been introduced in 1981 by Kohonen. Fields like data exploratory analysis, web usage mining [25], industrial and medical diagnostics [26], and corruption analysis [8] are contemporary examples of SOM analysis applications and successes. This section is based on [27] and aims at giving a theoretical background to the reader, whereas an application of the technique can be found in section “Application”. The main objective of the SOM algorithm is the representation of a high dimensional input dataset on lower dimensional maps. This enables to explore the data and to use techniques like visual correlation analysis or clustering analysis in an intuitive manner. In the first step, a feedforward Neural Network (NN) is trained on the input data. The output layer is a map with a lower dimensionality and a given number of neurons. During each iteration of the algorithm, an input data vector niis compared with the neurons mrof the output map using Euclidian distances. The neuron mcwith the smallest distance with regard to the input vector is identified as the Best Matching Unit (BMU):(1)∥ni−mc∥=minr{∥ni−mr∥}.The weights of the BMU are then modified in the direction of the input vector, leading to a self-organizing structure of the neurons. A learning rate α(t) and a neighborhood function hcr(t) are defined as parameters of the learning function:(2)mr(t+1)=mr(t)+α(t)hcr(t)[ni(t)−mr(t)].The learning-rate will influence the magnitude of the BMU's adaptation after matching with an input vector ni, whereas the neighborhood function defines the range of influence of the adaptation. In order to guarantee the stability of the final output map, decreasing learning rates and neighborhood functions are often used at the end of the training. An exhaustive discussion of the influence of the parameters such as the number of neurons, the shape of the map, or the initial weights of the neurons is to be found in [27].The k-means algorithm is a typical iterative distance-based clustering approach which iteratively creates k clusters based on the distances between data points. First, the number of clusters, k, has to be specified and k initial points are chosen as initial cluster centers. Different approaches exist in order to fix the number of clusters and to choose the initial centers (see [28]). However, the parameter k is often based on business knowledge and the k initial points are typically k data points randomly selected in the data set at hand. In a second step, all data points are assigned to their closest center according to the Euclidian distance. The mean, also called centroid, of each cluster is then calculated and used as new centers for the k clusters. The process is repeated using the updated centers until the algorithm converges, meaning that the data points are assigned to the same centers in consecutive iterations. By choosing the cluster centers to be the centroids, the algorithm minimizes the total squared distance from each of the cluster's points to its center. This clustering method is simple and effective but it is important to notice that the obtained partition depends on the original cluster centers. Different initializations of the algorithm can lead to different results being local optima. It is thus advised to run the algorithm multiple times with different seeds as initial centers in order to augment the probability to find a global optimum.Extracting salient dimensions (SD) for automatic SOM labeling is a methodology developed by Azcarraga et al. [29] and aims at identifying salient dimensions for clusters of SOM nodes. These salient dimensions are then used to label a SOM in an unsupervised way. The methodology is based on five main stages and starts with the training of a SOM using preprocessed data normalized within an input range of 0–1, followed by clustering the resulting nodes with any clustering technique. Pruning the nodes within the different clusters leads to more homogeneous clusters and is the aim of the second step. This pruning phase is based on the mean and the standard deviation of the Euclidian distance between the centroid and the neurons of the different clusters. A parameter z1 is used to identify the neurons to be pruned (the outliers or unlabeled neurons) and the neurons to be kept. The higher the value of z1, the smaller the number of neurons pruned. The third step consists of identifying two sets for each cluster. The in-patterns set is defined and gathers all the individual training patterns belonging to the cluster. The out-patterns set, on the other hand, consists of all the individual training patterns belonging to the other clusters or being attached to an unlabeled neuron identified in the second step. Using the sets defined in the previous step, the salient dimensions can then be identified for the clusters using a measure of deviation in the statistical sense of the term. A difference factor is calculated for each dimension of all clusters and is used to identify the salient dimensions. A second parameter, z2, is used to build a confidence interval around the mean of the difference factors of a cluster. A salient dimension will then be a dimension dj, belonging to the setDgathering all the dimensions, for which the difference factor differs too much with regard to other dimensions within a cluster. In order to identify the salient dimensions, the following steps have to be processed for each cluster k:1.For each dimension dj, compute μin(k, dj) and μout(k, dj) as respectively the mean input value for the set of in-patterns ϕin(k) and out-patterns ϕout(k), wherenidjis the jth dimension of the input vector ni:(3)μin(k,dj)=∑ni∈ϕin(k)nidjϕin(k),(4)μout(k,dj)=∑ni∈ϕout(k)nidjϕout(k).Compute the difference factor df(k, dj) of each dimension djas:(5)df(k,dj)=μin(k,dj)−μout(k,dj)μout(k,dj).Compute the difference factors’ mean μdf(k) over all dimensions djas:(6)μdf(k)=∑j=1Ddf(k,dj)D.Compute the difference factors’ standard deviation σdf(k) over all dimensions d as:(7)σdf(k)=∑j=1Ddf(k,dj)−μdf(k)2D.A dimension djof a cluster k is considered as salient if:(8)df(k,dj)≤μdf(k)−z2σdf(k)or(9)df(k,dj)≥μdf(k)+z2σdf(k).The smaller the value of z2, the larger the number of salient dimensions identified. In order to fully comprehend the application in section “Application”, the reader should understand the meaning of Eqs. (8) and (9). For an exhaustive discussion of the different formulas in this section, the reader is referred to Azcarraga et al. [29]. In some cases, the analyst is only interested in finding the dimensions characterizing a cluster because of their high values compared to the other clusters. In such a case (see e.g. [30]), Eq. (9) can be omitted. The final step uses the different salient dimensions to label clusters with input from domain-specific experts. The result gives the possibility to label a new pattern using the label of the cluster to which it is attached.This section deals with the prioritization approach which is the main contribution of this paper. It consists of the formalization of the fact that a clustering task, such as segmentation, should not be entirely unsupervised if interesting insights into the importance of the variables exist. In other words, if the analyst is able to assign priorities to the variables he is using, he should be able to do so. The following sections are discussing approaches enabling the integration of this knowledge. In section “Incorporating business knowledge”, an approach to integrate the knowledge using clustering techniques based on an adapted weighted distance metric is proposed. In section “From priorities to weights” a method to transform the knowledge from priorities to weights is presented.The context required to use the knowledge in the clustering task can be described as follows. Define a setNgathering n input vectors nicomposed of the d quantitative dimensions djof the setD, resulting from a data preparation step. In the next step, define a vectorw, wherewdjis the weight of the jth dimension ofD. The weightwdjrepresents the importance of the variable for the analysis as perceived by the business expert. A higher value ofwdjimplies, assuming other weights are fixed, a higher importance for the analysis. Finally, apply any clustering technique that uses the Euclidian distance as similarity or dissimilarity measure and replace the traditional Euclidian distance calculation by the following adapted distance between an input vector niand another input vector nm:(10)dist(ni,nm)=∑j=1dwdj(nidj−nmdj)2,withwdjrepresenting the importance level of the jth dimension ofDandnidjandnmdjthe values of the jth dimension of the input vectors niand nmrespectively.The idea behind Eq. (10) and the proposed approach can be illustrated using the two situations shown in Fig. 1. Consider the 4 points represented in Fig. 1(a) which are the corners of a square with a unit length. It is given that the x-axis represents a binary variable in the range [0, 1] while the y-axis represents a continuous variable in the range [0..1]. Applying the k-means algorithm presented in section “k-means” with two clusters and a random seed initialization of the centroids, 6 unique partitions can be obtained using the 4 points A, B, C and D of Fig. 1(a) as can be seen in Fig. 2(a). The well known Davies–Bouldin index [31] is calculated as follows:(11)index=1c∑i=1cmaxi≠jσi+σjd(ci,cj),where c is the number of clusters, cyis the centroid of cluster y, σyis the average distance of all elements of cluster y and d(ci, cj) is the distance between the centroid of cluster i and cluster j. Since a good partitioning corresponds to a situation where the intra-cluster distances are low and the inter-cluster distances are high, the lower the Davies–Bouldin index, the better the obtained partitioning. Using this index as evaluation metric for the 6 partitions previously obtained, the two best partitions are partitions 1 and 2, as their clusters are dense and far from each other. Of the 2 pairs of centroids representing the two best partitions, only one is meaningful considering the knowledge about the variables represented by the x- and y-axis. Indeed, only a partition grouping the point A with C and the point B with D leads to centroids having coordinates respecting the ranges of the original variables. The idea of the proposed approach is to incorporate the business knowledge into the algorithm in order to guide it to a solution which will be potentially better. One way to do it in the context presented in Fig. 1(a) is to transform the knowledge in priorities such that a variable with higher priority should be more structural in the resulting partition. Indeed, the variable x, a binary variable, can be assigned a higher priority than the variable y, a continuous variable, since the algorithm should create partitions mainly structured by the binary variable, hence leading to more meaningful results. This can be achieved by artificially increasing the scale of the variable x as can be seen in Fig. 1(b). In this context, applying the k-means algorithm with two clusters and a random seed initialization of the centroids will lead to 2 unique partitions. Taking the best partition using the Davies–Bouldin index approach, the partition grouping the point A with C and the point B with D is obtained and leads to meaningful centroids given the available knowledge.Coming back to Eq. (10), a dissimilarity between two input vectors will be proportional to the weights and the ranges of the dissimilar attributes so that it becomes easy to include business knowledge by tuning the Euclidian distance with higher or smaller weights. The next section will discuss an approach to fix those weights when the goal is to perform unsupervised learning with an a priori knowledge about the importance of the variables. The approach leads to clustering techniques able to generate clusters mainly structured by the variables with a higher priority for the analyst, which offers new perspectives into the data set at hand.In this section, a general approach for fixing the weights of Eq. (10) is presented. The purpose of this is to obtain clustering techniques able to generate clusters mainly structured by the variables with a higher priority for the analyst by using Eq. (10) with the weights resulting from this approach instead of the classical Euclidian distance. Moreover, a specific case of this approach is proposed in a setup involving only categorized variables represented by dummies, leading to a straightforward method which is used in section “Application” when training Self-Organizing maps.The general approach requires the definition of d quantitative variables djfromDshowing some variability and a priority vector p of size d that captures the priorities assigned to the different dimensions ofD, withpdjrepresenting the priority of the jth dimension ofD. A dimension djwithpdj=1is considered as a dimension with the highest priority and has then a higher priority than a dimension dowithpdo=2, etc. In order to derive the weightswdj, different definitions are necessary and proposed in what follows. The contribution Δdjof a dimension djto the Euclidian distance between two vectors niand nm,(12)dist(ni,nm)=∑j=1d(nidj−nmdj)2,is defined as:(13)Δ(ni,nm)dj=(nidj−nmdj)2.The maximal contribution of a variable dj,Δmaxdj, is defined as:(14)Δmaxdj=maxni,nm∈NΔ(ni,nm)dj.The maximal Euclidian distance distmaxis defined as:(15)distmax=∑j=1dΔmaxdj.The weighted maximal contributionψdjis defined as:(16)ψdj=wdjΔmaxdj.The maximal weighted Euclidian distancedistmaxwis defined as:(17)distmaxw=∑j=1dwdjΔmaxdj=∑j=1dψdj.Given those definitions, the goal of this approach is to find the variables’ weights such that the weighted maximal contribution of a variable is proportional to the sum of the weighted maximal contributions of the variables with lower priorities. In order to formalize this idea, a setLPdjis created which contains the dimensions diwith lower priorities than dj:(18)di∈LPdj,∀di:pdi>pdj,and the weightswdjhave to satisfy the following inequality:(19)αψdj>∑di∈LPdjψdiwith α being a positive parameter inversely proportional to the desired intensity of the prioritization. This parameter is used as a strength factor influencing the magnitude of the constraints like the α in [21], the τijin [22], the λ1 and λ2 in [20] or thewin [12].A specific case of this approach is proposed in a setup involving only categorized variables represented by dummies, leading to a straightforward method applied in section “Application”. In order to meet the requirements of this specific context, a qualitative variable with t different values should be transformed into t dummy variables. A value of 0 or 1 reflects whether or not the input vector is characterized by the value represented by the dummy variable, so that only one of the t dummy variables can be equal to 1. Concerning the quantitative variables, a categorization is possible using intervals represented by dummy variables as done for qualitative variables. Only one of the dummies obtained by the categorization can be equal to 1 if the intervals are not overlapping. Once this data preparation is performed, define a setGof g non overlapping groups gkof dimensions ofDso that there is no dimension ofDnot belonging to one group ofGand no dimension ofDbelonging to two different groups ofG. The function g(dj) returns the group gksuch that dj∈gk. The notion of group is introduced in order to capture the fact that only one of the dummies obtained by the categorization of a variable can be equal to 1 if the categories are not overlapping. This information is used in what follows in order to fix the weights assigned to the different variables. An example of such a group could be the different dummies resulting from the categorization of a quantitative variable. For example, consider the initial variable Age, where the categories, such as [18..25], [26..35], [36..50], [51..65], and [66..], are the dimensions of the group represented by dummies. In order to complete the definition of a group, which is a real-world subdivision of the dimensions, the same priority should be assigned to all the dimensions of the same group, so that(20)∀dj∈gk,pgk=pdj,with gkrepresenting the kth group ofGandpgkthe priority assigned to it. Completing this context, a setLPgkis defined and gathers the groups glsuch that:(21)gl∈LPgk,∀gl:pgl>pgk.Finally, given this context and the definitions of section “General approach”, the weightswdjcan be obtained using the following equation:(22)wdj=1+|LPg(dj)|α,with|LPg(dj)|being the number of groups having a lower priority than the group of the variable dj. This equation provides a solution which, given the requirements of the proposed specific context are fulfilled, respects the inequality of Eq. (19). The way to calculate the weights used in Eq. (10) in the proposed context is summarized in Algorithm 1.Algorithm 1Fixing the weights1:Given a set of variablesP.2:Define a set of d dummy variablesDby transforming the variables ofP.3:Define a set of g groupsGpartitioning the variables ofD.4:Define a vector p of size d by assigning priorities to the different variables ofD.5:fork=→gdo6:Define a setLPgksuch thatgl∈LPgk,∀gl:pgl>pgk.7:end for8:Fix the value of the parameter α.9:Define a vectorwwithwdjrepresenting the weight of the jth dimension ofD.10:forj=1→ddo11:wdj=1+|LPg(dj)|α12:end for13:returnw.In this section, a 5-step methodology implementing the prioritization approach proposed in section “Prioritization approach” is described assuming the specific context of section “Specific approach”. This methodology is further applied in section “Application” and compared with a methodology which is not incorporating the available business knowledge. Fig. 3shows the five steps which are explained in the next sections.The first step of the methodology is a data preparation step required to meet the specific context necessary to calculate the weights as proposed in section “Specific approach” with Algorithm 1. The different variables are transformed into dummies and groups are defined based on this transformation as discussed in section “Specific approach”, leading to a setDof d dummies and a setGof g non-overlapping groups.In the second step, priorities are assigned to the different variables as explained in section “General approach”. The parameter α is fixed and the weights of the different variables are calculated using Algorithm 1 leading to a vector of weights that can be used in the next step of the methodology. Important to note is the fact that the priorities are an input from the analyst and reflect the subjective importance of the variables. The parameter α is to be fixed and will impose the magnitude of the prioritization. The lower the value of it, the higher are the weights and the higher is the difference of impact between variables having different priorities.In the third step, an adapted version of the SOM algorithm presented in section “Self-organizing maps” is proposed in order to implement the prioritization approach by using Eq. (10) during the identification of the BMUs with the weights obtained in step 2. The Eq. (1) is hence replaced by the following equation:(23)∑j=1dwdj(nidj−mcdj)2=minr∑j=1dwdj(nidj−mrdj)2,withwdjthe weight of the jth dimension,nidjthe jth dimension of input vector niandmcdjthe jth dimension of the neuron mc. This new approach for identifying the BMUs leads to a new algorithm, Prioritized SOM (P-SOM), which is summarized in Algorithm 2 and motivated in what follows. By applying the classical SOM algorithm in a setup involving only dummy variables, the obtained neurons will have values in the range [0..1]. Some variables may be difficult to interpret if the values of the neurons are close to 0.5. This situation can happen when those variables do not structure the data. While it is not a problem for a blind exploration, it is not desirable when an analyst is aware of an a priori prioritization among the variables. Indeed, if a variable is perceived as important by the analyst, the algorithm should be able to incorporate this knowledge and guide the learning process in order to create a relative structure given the existing knowledge. By applying Algorithm 2, this knowledge is incorporated in the learning process, leading to neurons mainly structured by the variables with higher priorities, hence improving the perceived quality of the resulting maps. Indeed, the same phenomenon as the one outlined in section “Incorporating business knowledge” with the k-means algorithm will lead to neurons with extremer values for the variables with higher priorities.Algorithm 2P-SOM1:Given a set of input vectors niand a set of output neurons mr.2:Fix the weightswdjof the different dimensions of the input vectors.3:Initialize the neurons.4:repeat5:Select an input vector ni.6:Find the BMU mc:∑j=1dwdj(nidj−mcdj)2=minr{∑j=1dwdj(nidj−mrdj)2}.7:Update the nodes weights: mr(t+1)=mr(t)+α(t)hcr(t)[ni(t)−mr(t)].8:until Stopping criterion is satisfiedIn the fourth step, the k-means algorithm is applied to the neurons trained in the previous step. Although the P-SOM algorithm offers advanced visualization facilities by reducing the dimensionality to two-dimensional maps, a formal analysis of the resulting neurons offers advanced insights into the structure of the data (see e.g. [32,30]). The output of this step is a partitioning of the neurons into groups sharing some characteristics. Note that this clustering technique will capture the prioritized structure.In the final step, the obtained clusters are described by extracting their salient dimensions as explained in section “Salient dimensions extraction”. Thanks to the characterization of the clusters, it is possible for the analyst to name and assess the clustering of the neurons. Note that variables with higher priorities should be at the origin of the clusters’ structures such that the salient character of a variable should be impacted by its priority.The application involves the clustering of a data set originating from the concert industry which is provided by Ticketmatic, a leading ticketing company in Europe. The data set gathers attributes about clients of one concert organizer which are summarized in Table 1and discussed in what follows. Different business considerations motivated the use of the methodology proposed in the previous section. Firstly, the business involved wanted to use a technique allowing them to use their knowledge to guide the clustering algorithm and analyze different perspectives of the data. Some a-priori knowledge about important variables was available and could thus be transformed into priorities. Secondly, feature selection algorithms were rejected by the business because all input variables were considered valuable from a business perspective, discarding a purely statistical unsupervised feature selection strategy. Thirdly, since the analysts were marketing experts, visualization for exploration and reporting was considered as a key feature of the to-be-used technique. These different business considerations were guiding for the design and the application of the proposed methodology. In section “Two approaches”, a comparison between a traditional two-step clustering approach and the prioritized methodology proposed in section “Methodology implementing the prioritization approach” is illustrated, using the available data. To do so, a real business context is described and a solution to a relevant decision problem is proposed using both approaches. In section “Impact of the parameter α”, the impact of the parameter α of Eq. (22) on the clustering quality and on the relative importance of the variables in the resulting clustering structure is discussed, based on the results of extensive experiments.The goal of the application is to perform an analysis of the data at hand in order to capture the profile of the customers that may be interested in a future concert, called The Concert in the remainder of this paper. The different steps of the application are shown in Fig. 4and are discussed in what follows. Note that steps 1, 4 and 5 of Fig. 4 are similar for both approaches in order to allow a comparison of the results.The first step of the application concerns the data preparation according to the criteria presented in section “Specific approach”. An index total RFM is computed by summing the three values of the RFM variables (Recency, Frequency and Monetary) calculated for each client, leading to a score between 3 and 15 for each of them. The interested reader is referred to [33] for an extensive discussion of the RFM framework, used in this paper to assess the value of a customer to the company based on his past behavior. Five categories of customers were defined based on their respective total RFM indices, using the following intervals: [3..5], [6..8], [9..11], [12..13] and [14..15]. The birthdate of the different customers was used to generate an index capturing the age. Five categories were defined using the following intervals in years: [18..25], [26..35], [36..50], [51..65] and [66..]. The gender of the customers was used to build two extra categories. Using the IP addresses of the booking computers, an index representing the geographic distance separating the customer from the concert location was created. Categories were defined using the following intervals in km: [0..5], [6..10], [11..15], [16..25], [26..50] and [51..]. In order to define an index capturing the interest of a given customer for a tag, an artist or a concert, the last.fm API data was used in combination with data of the previous concerts involving the given customer. Considering that a concert consists of a series of artists characterized by a series of tags, it is then possible to rank the customers according to their score for a given concert as described in [30]. Note that this interest-based variable is based solely on tags of previously attended concerts, and not on information about the customers who really attended The Concert. Five categories are then obtained by categorizing this interest variable, from The Concert 1, representing customers with a low interest, to The Concert 5, representing those with a high interest.As a second step of the application, the specific prioritization approach presented in section “Specific approach” is performed. Groups are formed as follows: g1=(d1, d2), g2=(d3, d4, d5, d6, d7), g3=(d8, d9, d10, d11, d12, d13), g4=(d14, d15, d16, d17, d18) and g5=(d19, d20, d21, d22, d23). Note that these groups gather the dummies related to the same original variables. A priority is assigned to the different variables respecting the definitions of section “From priorities to weights”. In this case, the business is interested in analyzing the profiles of the potential participants of future concerts and considers the five variables related to the interest for The Concert as more important than the other variables. This business knowledge is difficult to integrate in the clustering task with traditional clustering techniques, but the approach proposed allows for an easy translation in terms of priorities and weights. Using the logic presented in section “From priorities to weights”, a priority of 1 is assigned to the variables The Concert 1, The Concert 2, The Concert 3, The Concert 4 and The Concert 5 and a priority of 2 is assigned to the other 18 variables, meaning that they are considered as less important than the variables capturing the interest for The Concert. The weights are then calculated using Eq. (22) with α equal to 0.2 resulting in a weight equal to 21 for the variables of group 5 and a weight equal to 1 for the other variables. The details of the calculation of the weights of the dimensions d1 and d19 are illustrated in what follows.1.The dimension d1 has been assigned a priority of 2 and is part of group 1 so thatpd1=2and g(d1)=g1.LPg(d1)is empty because 2 is the lowest priority in this case such thatLPg(d1)=∅. Finally, with α equal to 0.2, the weight of the dimension d1,wd1, is calculated aswd1=1+(|LPg(d1)|/α)=1+(0/0.2)=1.The dimension d19 has been assigned a priority of 1 and is part of group 5 such thatpd19=1and g(d19)=g5.LPg(d19)is equal to (g1, g2, g3, g4). Finally, with α equal to 0.2, the weight of the dimension d19,wd19, is calculated aswd19=1+(|LPg(d19)|/α)=1+(4/0.2)=21.For the next steps of the application, a distinction will be made between the traditional and the prioritized approach. The traditional approach consists of applying the SOM-algorithm introduced in section “Self-organizing maps” whereas the prioritized approach includes the available business knowledge with the P-SOM algorithm as described in section “Prioritized self-organizing maps”.In the third step, a 10×12 SOM is trained (batch training) using both traditional and prioritized approaches. By doing this, a large set of prototypes is created. According to best practices [32,27], this number should be larger than the expected number of clusters. This expected number being unknown, business expectations are then used to have an idea of the maximum number of clusters they are willing to handle. In this case, because 10 clusters are expected and the first clustering step is followed by a second one, it has been opted to choose a number substantially higher than 10. According to the same best practices [27], a rectangular shape of the output map is preferred, which explains the choice of a 10×12 SOM instead of e.g. a 10×10 SOM. The visualization power of the SOM is illustrated in Figs. 5 and 6, which show the component planes for the traditional approach and for the prioritized approach respectively. The dark red and dark blue neurons of a component plane represent, respectively, the neurons with relatively high and low values for the variable represented by the component plane. Fig. 5 gives the analyst the possibility to analyze the data at hand in a general way and provides him with valuable patterns. An example of such a pattern is the fact that, based on Fig. 5(n) and (m), the analyst can see a correlation between the variables Distance 50+ and Total rfm 1 which is a valuable pattern indicating the fact that people coming from far are not perceived as clients with a high customer lifetime value. This is a logic pattern that may convince the analyst to spend more effort on people located near to the concert place. Although this pattern is interesting and the fact that an expert in SOM would be able to find a lot of other patterns using these component planes, the limits of the unsupervised clustering task appear if an analyst is interested in some of the variables and would like to focus on them. Some advanced techniques, such as e.g. the one proposed in [30], can deal with this problem by using statistical approaches, which gives the possibility to explore the data by specifying targeted dimensions. However, this goal can also be achieved by using the proposed prioritized approach which overcomes those limitations by providing the analyst with a way to focus on specific variables by defining priorities for the analysis. Using Fig. 6, which focuses on the variables reflecting the interest of the clients for a future concert, the analyst is able to perform a specific exploration of the data, formalizing his knowledge and effectively guiding the algorithm. Richer and cleaner patterns appear when focusing on the variables of group 5 as can be concluded from Fig. 6. An example of a more specific analysis can be the analysis of the profile of the clients having the most interest in the future concert. Those clients are summarized by the neurons having a high value for the variable The Concert 5, and, based on the other sub-figures of Fig. 6, one can easily conclude that those clients are relatively young people (see Fig. 6(i), (l) and (m)), have a low RFM value (see Fig. 6(n), (o), (p), (q) and (r)) and are used to travel to attend a concert (see Fig. 6(c), (f), (g) and (h)). Given that, in this context, the analyst is interested in the understanding of the profiles capturing the interest for The Concert, the reader can easily assess the usefulness of both approaches.In the fourth step of the experiment, a second clustering step is performed in order to capture the structure of the SOMs obtained using both traditional and prioritized approaches. The k-means algorithm is thus applied twice: a first time to the neurons obtained by the traditional approach and a second time to the neurons obtained by the prioritized approach. The maximal number of clusters is fixed to 10 and the number of epochs to 50 so that a stable clustering is obtained. The Davies–Bouldin index is used in order to select the best clustering for each approach. Fig. 7(a) shows the 9 clusters ck:k∈[1..9] obtained when applying the k-means to the neurons resulting from the traditional approach whereas Fig. 7(b) shows the 8 clusters ck:k∈[1..8] obtained when the prioritized approach is used.Finally, the extraction of the salient dimensions of the different clusters obtained in the previous step is performed with the parameters z1 and z2 respectively equal to 10,000 and 0 so that the sensitivity is maximal [30]. The characteristics of those clusters can thus be analyzed and are summarized in Tables 2 and 3. Both tables show the dimensions characterizing the different clusters in descending order of their salientness using the difference factors calculated with Eq. (5). For example, based on Table 2, it can be said that the third cluster obtained with the traditional approach, c3, has as main characteristic the dimension d15, as second main characteristic the dimension d2, etc. This means that the centroid of cluster c3 represents customers characterized by a low RFM value (TotalRFM2), a female gender (GenderWoman), etc. Although this information is interesting in order to explore the data without any specific goal, the clustering task often hides pre-defined objectives that are difficult to satisfy using a classical unsupervised approach. In this case, such an objective consists of the understanding of the customers’ interest in The Concert, which is quite difficult given the results obtained and shown in Table 2. This goal can be achieved by exploring the data while focusing on the variables of group 5. The reader can see the position of those dimensions, namely the dimensions d19, d20, d21, d22 and d23, represented in bold in Tables 2 and 3. It can be seen that this final step has led to 9 clusters offering a general view on the one hand, and to 8 clusters offering a targeted view on the other hand. Given the precise task the business is involved in, the first partitioning is difficult to use while the second is really suitable. It can indeed easily be concluded, based on Table 3, that the customers potentially interested in the future concert (summarized by the cluster c3), are coming from far, are relatively young people and are perceived as clients with a low value for the company. This might be an incentive for the concert organizer to think about other music groups, assuming that his main objective is to maximize his profit, until he finds a suitable artist which matches his profitable customer base's interest.In this section, the impact of the parameter α is discussed and illustrated using the prioritized setup described in section “Application”. This parameter is used as a strength factor inversely proportional to the amplitude of the desired prioritization. The smaller the value of α, the higher the difference of weights between variables having different priorities. By reducing this parameter, the soft constraints used during the clustering algorithm are strengthened, hence leading to partitions mainly structured by the variables of higher priorities while changing the resulting partitioning of the data points. By artificially constraining the clustering algorithm, it is expected that the subjective and the objective quality of the obtained partitioning will be impacted. In the remainder of this paper, two metrics will be used to measure this. For the first measure, the Davies–Bouldin index is chosen to capture the objective, data-driven, quality of the clustering. The lower the value of this index, the better the separation of the clusters and the within-cluster density. The second measure, the subjective measure, captures the relative importance of the variables of a given priority with regards to the variables with a lower priority. The relative importance of a set of variables with priority pi,ϕpiis calculated as(24)ϕpi=1k∑s=1k∑di∈DPpidf(s,di)∑dj∈LPdidf(s,dj),with k the number of clusters,DPpithe set of variables with priority pi, df(s, di) the difference factor of cluster s for dimension djandLPdithe set of dimensions with a lower priority than di. Both quality measures are used to monitor the impact of the parameter α using the same setup as in section “Two approaches” and are discussed in what follows.As a first step of the experiment, a benchmark is created by applying the classical two-step clustering approach involving the training of a 10×12 SOM on the available data followed by the k-means algorithm with a maximal number of clusters equal to 10 and 50 epochs. The best clustering is selected using the Davies–Bouldin index, leading to a benchmark partitioning that is used to analyze the impact of the prioritization. Note that the characteristics of the obtained clusters are studied by extracting the salient dimensions. The prioritized approach is applied on the same data set while assigning a higher priority to the variables of g5 as in section “Two approaches”. Starting with the training of the 10×12 P-SOM followed by the k-means algorithm with a maximal number of clusters equal to 10 and 50 epochs, the best clustering is selected using the Davies–Bouldin index and is characterized by extracting the salient dimensions of its clusters. The amplitude of the prioritization is iteratively increased by reducing the parameter α used during the calculation of the weights. The initial value of α is set to1α=0.01and is reduced at iteration i using the following schema:1α=0.01+(1−i)0.05during 200 iterations leading to values of α ranging from 100 to 1/9.96. Fig. 8shows on the y-axis the relative importance,ϕp1, of the variables representing the interest of the customers for a future concert. The P-SOM is iteratively applied, focusing more and more on these variables by augmenting the value of 1/α, represented on the x-axis.It can be seen on this figure that the parameter α has the expected impact on the clustering structure considering the relative importance of the variables with higher priorities. Indeed, the lower the value of α, the more the variables with higher priorities are structuring the clustering output. Starting with values around 0.06 for the unconstrained approach, the relative value increases inversely proportionally to the value of α until converging to values around 0.55, as can be seen on Fig. 8. Fig. 9shows on the y-axis the ratio between the Davies–Bouldin index of the unconstrained clustering, DBnp, and the index of the constrained clustering, DBp. The lower the value of the Davies–Bouldin index, the better the obtained clustering, hence a ratio lower than 1 indicates a decrease in the clustering quality when constraining the clustering algorithm while a ratio higher than 1 indicates an increase in the quality. As expected, a relative decrease in the clustering quality is identified when artificial constraints are added to the clustering task. However, the results of Fig. 9, with a mean value of 0.9433 and some peaks above 1, put into perspective the decrease in the quality of the clustering output. Indeed, while the subjective quality is significantly improved by reducing α, the objective quality is only slightly diminished, leading to an acceptable constrained clustering output from both objective and subjective points of view.

@&#CONCLUSIONS@&#
