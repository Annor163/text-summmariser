@&#MAIN-TITLE@&#
Induction of ordinal classification rules from decision tables with unknown monotonicity

@&#HIGHLIGHTS@&#
We propose the dominance and fuzzy preference inconsistency rates.These indices can be used to discover global monotonicity in decision tables.Sometimes, the transformation of cloning all ordinal attributes is overcomplex.We propose a refined transformation method to avoid the issue of over-fitting.

@&#KEYPHRASES@&#
Rough sets,Inconsistency rates,Multiple criteria decision aiding,Dominance relations,

@&#ABSTRACT@&#
We are considering induction of ordinal classification rules, which assign objects to preference-ordered decision classes, within the dominance-based rough set approach. In order to extract such rules, it is necessary to define dominance inconsistencies with respect to a set of condition attributes containing at least one ordinal condition attribute. Furthermore, it is also assumed that we know if there exist increasing or decreasing monotonicity relationships between the values of ordinal condition and decision attributes. Very often, however, this information is unknown a priori. One solution to this issue is to transform the ordinal condition attributes with unknown directions of preference to pairs of attributes with supposed inverse monotonic relationships. Both local and global monotonicity relationships can be represented by decision rules induced from transformed decision tables. However, in some cases, transforming a decision table in this way is overcomplex. In this paper, we propose the inconsistency rates based on dominance and fuzzy preference relations that have the capacity of discovering monotonic relationships directly from data rather than induced decision rules. Moreover, we propose a refined transformation method by introducing an additional monotonicity checking using these inconsistency rates to determine whether an ordinal condition attribute should be cloned or not. Experiments are also provided to evaluate the usefulness of the refined transformation method.

@&#INTRODUCTION@&#
Rough sets theory, proposed by Pawlak (1982); 2004), has proved to be an effective tool to deal with vagueness and inconsistency in information systems. The original rough set model is constructed based on indiscernibility relations. It fails to operate on multi-criteria decision systems, in which there exist monotonicity relationships between the values of condition and decision attributes. To resolve this problem, dominance-based rough set approach (DRSA) was proposed by Greco et al. (1999a, 2001a, 2002b) and Słowiński et al. (2005); 2009). In DRSA, the equivalence relation is replaced with a dominance relation.Until now, numbers of extensions of DRSA have been proposed. DRSA with missing values has been considered by Błaszczyński et al. (2012b), Dembczyński et al. (2009),Greco et al. (1999b); 2000a), and Yang et al. (2008). Rough sets based on rough-graded preference relations were presented by Tsai et al. (2006). Greco et al. (2000b); 2000c); 2002a) proposed the fuzzy set extension of DRSA. Hu et al. (2008); 2010) introduced the fuzzy preference rough sets (FPRS) model where crisp dominance relations were generalized to fuzzy preference relations.Furthermore, as claimed by many studies, the definition of lower approximations in DRSA is too strict. To overcome this drawback, variable consistency dominance-based rough set approaches (VC-DRSA; Błaszczyński et al., 2006; 2009; Greco et al., 2001c) and variable precision dominance-based rough set approach (VP-DRSA; Inuiguchi et al., 2009) were proposed. In these approaches, the conditions for assignment of objects to lower approximations are relaxed to allow some inconsistencies also in lower approximations in a controlled way.One application of rough set theory is classification. In this study, we concern induction of dominance classification rules, which assign objects to preference-ordered decision classes, within dominance-based rough set approach. In order to extract such rules, it is necessary to define dominance inconsistency with respect to a set of condition attributes containing at least one ordinal condition attribute. Furthermore, it is also assumed that we know if there exist increasing or decreasing monotonicity relationships between the values of ordinal condition and decision attributes. Very often, however, this information is unknown a priori. Moreover, ordinal condition attributes may have different kinds of monotonicity relationships in different parts of the whole evaluation space. This is called local monotonicity. Błaszczyński et al. (2012a) proposed a transformation method which is able to discover global and local monotonicity relationships represented by monotonic decision rules. Additionally, this method allows application of DRSA and VC-DRSA to non-ordinal decision tables as well. In this methodology, all ordinal condition attributes with unknown directions are cloned to pairs of attributes with supposed inverse monotonic relationships. However, sometimes, this transformation is overcomplex.In this paper, we propose the conceptions of inconsistency rates based on dominance and fuzzy preference relations, which have the capacity of discovering the directions of preference directly from data rather than decision rules. Furthermore, we refine the Błaszczyński et al. transformation method by introducing an additional step to determine whether an ordinal condition attribute should be cloned or not according to its inconsistency rates.The remainder of this paper is arranged as follows. Section 2 reviews some notions and conceptions of DRSA and inconsistency rates in non-ordinal decision systems. In Section 3, we propose the conceptions of inconsistency rates based on dominance and fuzzy preference relations. In Section 4, we show that for some attributes, Błaszczyński et al. transformation method is overcomplex and present a refined transformation method. An illustrative example is provided in Section 5. Section 6 contains experiments to evaluate the usefulness of the refined transformation method in terms of predicative accuracy. This paper concludes in Section 7 along with our summaries and conclusions.In this section, we briefly review some conceptions and notions of DRSA and inconsistency rates based on equivalence relations.An information system (table) is a quadruple DS = 〈U, A, V, f〉, where U is a non-empty finite set of objects, A is a non-empty finite set of attributes,V=⋃a∈AVaand Vais a domain of attribute a, and f: U × A → V is a total function such that f(x, a) ∈ Vafor every a ∈ A, x ∈ U, called an information function. Furthermore, the set A of attributes can be divided into set C of condition attributes and set D of decision attributes. For simplicity, we assume that the set of decision attributes D is a singleton {d}. Furthermore, given a set P⊆C, the Cartesian product of the domains of all attributes in P is called the P-evaluation space.We suppose that the decision attribute d makes a partition of U into a finite number of decision classes. Let Cl = {Clt, t ∈ T}, T = {1, …, n}, be the set of these classes. We also assume that these classes are preference ordered, i.e., for all s, t in T, such that t > s, the objects from Cltare preferred to the objects from Cls. The sets to be approximated are called upward and downward unions of decision classes, respectively,Clt≥=⋃s≥tCltandClt≤=⋃s≤tCls,t = 1, …, n.Let q ∈ C be a condition attribute. We say that q is an ordinal condition attribute, if its value set is preference ordered, i.e. for any x, y ∈ U, we have f(x, q)≽f(y, q) or f(x, q)⪯f(y, q). Furthermore, if the evaluations of objects on ordinal condition attribute q have monotonic relationships with the evaluations on decision attribute d, then q is a criterion. If a condition attribute is not a criterion, it is a regular condition attribute.Moreover, we have two kinds of criteria: cost criteria (the smaller the better) and gain criteria (the greater the better). We also say that criterion q ∈ C has a negative monotonicity relationship with decision attribute d, if q is a cost criterion, or q has a positive monotonicity relationship with d, if q is a gain criterion.An information table, which has an ordinal decision attribute and at least one criterion, is called a multi-criteria decision table.For a subset of condition attributes, the dominance relation is defined as follows: for x ∈ U and P⊆C, x dominates y with respect to (w.r.t.) P, denoted by xDpy, if the following conditions are met simultaneously:•for every gain criterion q ∈ P, f(x, q) ≥ f(y, q),for every cost criterion q ∈ P, f(x, q) ≤ f(y, q),for every regular attribute a ∈ P, f(x, a) = f(y, a).In DRSA, the granules of knowledge used for approximation are dominance cones that are defined as follows:•the set of objects dominating x w.r.t. P:DP+(x)={y∈U:yDpx},the set of objects dominated by x w.r.t. P:DP−(x)={y∈U:xDpy}.Finally, the upper and lower approximations of unions of decision classes w.r.t. P are calculated as follows:•The P-lower approximation ofCli≥:P̲(Cli≥)={x∈U:DP+(x)⊆Cli≥}The P-upper approximation ofCli≥:P¯(Cli≥)={x∈U:DP−(x)∩Cli≥≠⌀}The P-lower approximation ofCli≤:P̲(Cli≤)={x∈U:DP−(x)⊆Cli≤}The P-upper approximation ofCli≤:P¯(Cli≤)={x∈U:DP+(x)∩Cli≤≠⌀}The P-boundaries ofCli≥andCli≤are the following:BnP(Cli≥)=P¯(Cli≥)−P̲(Cli≥),BnP(Cli≤)=P¯(Cli≤)−P̲(Cli≤).Dash and Liu (2003) proposed an attribute selection algorithm for non-ordinal decision systems based on the notion of inconsistency rates. Let DS = 〈U, C∪{d}, V, f〉 be a non-ordinal decision system, and P⊆C be a subset of condition attributes. A pattern w.r.t. to P is defined as a subset of objects that have equal values on all the attributes in P. Let Ptn(P) = {Ptnt(P), t ∈ T}, T = {1, …, l} be a set of patterns of a decision system. It is clear that we haveU=⋃t=1lPtnt(P)andPtnt(P)∩Ptns(P)=⌀,∀s, t ∈ T and s ≠ t.It is possible that one pattern contains objects belonging to different decision classes. We suppose that the decision attribute d makes a partition of sth pattern Ptns(P) into h decision subclasses. Let these subclasses be denoted as Clt, t = 1, …, h. The inconsistency count of Ptns(P), denoted by ICs(P), is calculated as follows: ICs(P) = |Ptns(P)| − max{|Clt|: t = 1, …, h}, where | · | denotes the number of objects in a set.The inconsistency rate w.r.t. P is the ratio of the sum of inconsistency counts of all patterns to the number of objects in U. Formally,IR(P)=∑s=1lICs(P)|U|.In Dash and Liu (2003), the authors try to reduce the redundant attributes by finding a minimal subset of C without increasing the inconsistency rate.In this section, we present the conceptions of inconsistency rates based on dominance and fuzzy preference relations.In a multi-criteria decision system, let x, y ∈ U be objects and P⊆C be a subset of condition attributes. We say that y is an inconsistent object of x w.r.t. P, if y is dominating x w.r.t. P and belongs to a worse decision class than x, or y is dominated by x w.r.t. P and belongs to a better decision class than x. Formally, ∀x, y ∈ U, P⊆C, ify∈DP+(x)∩[x]d<,ory∈DP−(x)∩[x]d>,where[x]d<={y∈U:f(y,d)<f(x,d)}and[x]d>={y∈U:f(y,d)>f(x,d)},then y is an inconsistent object of x. Furthermore, ify∈DP−(x)∩[x]d>,thenx∈DP+(y)∩[y]d<. So, for all x ∈ U, we only concern the inconsistent objects inDP+(x)∩[x]d<,for simplicity.Definition 1Let P⊆C be a subset of condition attributes. Dominance inconsistency rate (DIR) w.r.t. P is calculated as:(1)DIRP=∑x∈U|DP+(x)∩[x]d<|∑x∈U|[x]d<|Of course, there is another way to calculate the dominance inconsistency rate w.r.t. P by counting the inconsistent objects dominated by x ∈ U, i.e.DIRP=∑x∈U|DP−(x)∩[x]d>|∑x∈U|[x]d>|. In fact, we have the property:∑x∈U|DP+(x)∩[x]d<|∑x∈U|[x]d<|=∑x∈U|DP−(x)∩[x]d>|∑x∈U|[x]d>|.ProofFirstly, we define two inconsistency matricesI+=(γij+)m×mandI−=(γij−)m×mas:γij+={1ifxj∈DP+(xi)∩[xi]d<0otherwiseγij−={1ifxj∈DP−(xi)∩[xi]d>0otherwisewhere m is the number of objects.As it is known that ifxj∈DP+(xi)∩[xi]d<,thenxi∈DP−(xj)∩[xj]d>,we haveγij+=γji−.For object xi∈ U, the number of inconsistent objects dominating xiw.r.t. P is:|DP+(xi)∩[xi]d<|=∑j=1mγ(ij)+,and the number of inconsistent objects dominated by xiis|DP−(xi)∩[xi]d>|=∑j=1mγ(ij)−.So the numerators of two kinds of DIR are calculated as follows:∑x∈U|DP+(x)∩[x]d<|=∑i=1m∑j=1mγ(ij)+∑x∈U|DP−(x)∩[x]d>|=∑i=1m∑j=1mγ(ij)−=∑j=1m∑i=1mγ(ji)+=∑i=1m∑j=1mγ(ij)+=∑x∈U|DP+(x)∩[x]d<|.Analogously, we have∑x∈U|[x]d<|=∑x∈U|[x]d>|.Finally, we conclude that∑x∈U|DP+(x)∩[x]d<|∑x∈U|[x]d<|=∑x∈U|DP−(x)∩[x]d>|∑x∈U|[x]d>|.□This property means that we only need to calculate one of the two kinds of DIR.Let us assume that q ∈ C is an ordinal condition attribute and we do not know whether there exists a positive or negative monotonicity relationship between the values of q and decision d. If q is considered a gain criterion,D{q}+(x)=[x]q≥={y∈U:f(y,q)≥f(x,q)},while if q is considered a cost criterion,D{q}+(x)=[x]q≤={y∈U:f(y,q)≤f(x,q)}. In order to determine the direction of preference of q, we calculate twice the dominance inconsistency rate of {q}, respecting the assumptions that q is a gain or cost criterion respectively. So we define the dominance inconsistency rates w.r.t. a single ordinal condition attribute with unknown direction in two directions.Definition 2Let q ∈ C be an ordinal condition attribute. The upward and downward dominance inconsistency rates w.r.t. q are calculated as:•The upward dominance inconsistency rate (UDIR) w.r.t. q:(2)DIRq↑=∑x∈U|[x]q≥∩[x]d<|∑x∈U|[x]d<|The downward dominance inconsistency rate (DDIR) w.r.t. q:(3)DIRq↓=∑x∈U|[x]q≤∩[x]d<|∑x∈U|[x]d<|Furthermore, we define the difference betweenDIRq↑andDIRq↓as follows:(4)ξq=DIRq↑−DIRq↓Notice that the conceptions of UDIR and DDIR are only meaningful for a single ordinal attribute, because a subset of condition attributes may contain gain and cost criteria simultaneously.Intuitively, we should adopt the assumption that corresponds to the smaller inconsistency rate. More precisely,DIRq↑andDIRq↓can be decomposed as follows:DIRq↑=∑x∈U|[x]q>∩[x]d<|∑x∈U|[x]d<|+∑x∈U|[x]q∩[x]d<|∑x∈U|[x]d<|DIRq↓=∑x∈U|[x]q<∩[x]d<|∑x∈U|[x]d<|+∑x∈U|[x]q∩[x]d<|∑x∈U|[x]d<|where[x]q>={y∈U:f(y,q)>f(x,q)},[x]q= {y ∈ U: f(y, q) = f(x, q)} and[x]q<={y∈U:f(y,q)<f(x,q)}.WhenDIRq↑approaches 0, both∑x∈U|[x]q>∩[x]d<|and∑x∈U|[x]q∩[x]d<|approach 0. This means that for all x ∈ U, most of the objects in[x]d<do not belong to[x]q>and [x]q. On the other hand, as[x]q<=U−([x]q>∪[x]q),most of the objects in[x]d<must belong to[x]q<. So∑x∈U|[x]q<∩[x]d<|approaches∑x∈U|[x]d<|andDIRq↓approaches 1. In consequence, ξqapproaches − 1. WhenDIRq↑=0,q is strictly monotonically non-decreasing w.r.t. d. In this case, ξqgets its minimum − 1. Analogously, whenDIRq↓=0,q is strictly monotonically non-increasing w.r.t. d and ξqgets its maximum 1. Notice that whenDIRq↑approaches 1, it is not always the case thatDIRq↓approaches 0, because it is possible that only∑x∈U|[x]q∩[x]d<|∑x∈U|[x]d<|approaches 1. In such a case, bothDIRq↑andDIRq↓approach 1, and ξqapproaches 0.In general, the absolute value of ξqpresents the possibility that q has a global monotonicity relationship with d, and the sign of ξqdenotes the direction of q. Furthermore, for a highly inconsistent ordinal condition attribute q, where there are a number of inconsistent objects w.r.t. q both in upward and downward directions, bothDIRq↑andDIRq↓may have high values. This means that the absolute value of ξqis very small. In this case, it is hard to say that q is a gain or cost criterion. Namely, the possibility that q is globally monotonically related to d is very low. So, we introduce a threshold α ∈ [0, 1]. When ξq∈ [ − α, α], we consider there does not exist globally monotonous relationship between d and q. Otherwise, q is a cost criterion, if ξq∈ (α, 1], or a gain criterion, if ξq∈ [ − 1, −α).We continue to consider the inconsistency rates of an ordinal condition attribute in the context of a subset of condition attributes.Firstly, let us consider two subsets of condition attributes, P and Q. If P⊆Q, then for any object x, we haveDQ+(x)⊆DP+(x)andDQ+(x)∩[x]d≤⊆DP+(x)∩[x]d≤. This means that DIRQ≤ DIRP. So we say that DIR w.r.t. a set of condition attributes is monotonic w.r.t. the set of considered attributes.Furthermore, we assume that q ∈ C is an ordinal condition attribute with ξq∈ [ − 1, −α). So q should be labeled as a gain criterion, q↑, andDIRq↑=DIR{q↑}. Given an additional subset of attributes, Q, we haveDIRQ∪{q↑}≤DIR{q↑},as DIR is monotonic w.r.t. the set of considered attributes. In other words there is no new inconsistency against the assumption that q is a gain criterion is introduced when additional condition attributes are considered. So if q is considered a gain criterion individually, it should also be considered a gain criterion in the context with additional attributes.Analogously, we have that if ξq∈ (α, 1], q should be considered a cost criterion individually and in the context of any subset of condition attributes. Namely, the possibility that an ordinal condition attribute has a global monotonicity relationship with the decision attribute can be estimated independently.On the other hand, the preference direction of an ordinal condition attribute cannot be evaluated by comparing the inconsistency rates, corresponding to supposed different directions, calculated within a subset of attributes.Let us consider the data set shown in Table 1, which contains eight objects and is characterized by two ordinal condition attributes q1 and q2 and an ordinal decision attribute d.Furthermore, we assume that it is known that q1 is a gain criterion, labeled asq1↑,and want to know the direction of q2. According to the information presented in Table 1, it is easy to see that q2 is highly likely to be a gain criterion. In fact, q2 is strictly monotonically non-decreasing w.r.t. d, if x1 is removed.When the direction of q2 is calculated independently, we haveDIRq2↑=0.2381,DIRq2↓=0.9048andξq2=−0.6667. So q2 should be considered a gain criterion. This is consistent with our intuitive conclusion.However, when q2 is considered in company withq1↑,there are two possible combinations of attributes with different directions:P={q1↑,q2↑},corresponding to the assumption that q2 is a gain criterion, andP′={q1↑,q2↓},corresponding to the assumption that q2 is a cost criterion. We have DIRP= 0.0952,DIRP′=0.0000,andDIRP>DIRP′. It is seemed that q2 should be a cost criterion, assuming there exists a global monotonicity relationship between q2 and d.The reason that we have an irrational conclusion is that the inconsistencies only in a part of the whole evaluation space are considered. For object x, inconsistent objects areDP+(x)∩[x]d<=D{q2↑}+(x)∩(D{q1↑}+(x)∩[x]d<)w.r.t. P andDP′+(x)∩[x]d<=D{q2↓}+(x)∩(D{q1↑}+(x)∩[x]d<)w.r.t. P′. However for most the objects in this data set, we haveD{q1↑}+(x)∩[x]d<=⌀,exceptDq1↑+(x4)∩[x4]d<=Dq1↑+(x5)∩[x5]d<={x1}. In consequence, for most the objects, there is no inconsistency no matter w.r.t. P or P′. In fact only the inconsistencies in subset {x1, x4, x5} w.r.t.q2↑(orq2↓) are concerned, when calculating DIRP(orDIRP′). It is unreasonable to speculate global monotonicity relationships just only depending on the inconsistencies in a part of the whole evaluation space.From the above analysis, we conclude that the direction of each ordinal condition attribute can be calculated individually.In this subsection, we introduce another way to check monotonicity based on fuzzy preference relations.Definition 3Let U be a finite nonempty set of objects. A fuzzy preference relation P over U is a fuzzy set over the product set U × U, that is characterized by a membership function: μP= U × U → [0, 1].Let us assume that there are m objects in U. The fuzzy preference relation P can be denoted by a matrix as follows: P = (pij)m × m, where pij= μp(xi, xj), ∀xi, xj∈ U. pijpresents the preference degree of xiover xj. If xiis preferred to xj, pij> 0.5; if xjis preferred to xi, pij< 0.5; pij= 0.5 indicates indifference between xiand xj. Usually, a fuzzy preference relation is additive reciprocal, i.e., pij+ pji= 1.There might be a number of approaches to construct fuzzy preference relations. Hu et al. (2008) introduce the Logsig sigmoid transfer function to calculate preference degrees as follows:•The upward fuzzy preference relation:(5)Rq↑(xi,xj)=11+e−k(f(xi,q)−f(xj,q))The downward fuzzy preference relation:(6)Rq↓(xi,xj)=11+ek(f(xi,q)−f(xj,q))where k is a positive constant to control the gradient.Obviously,Rq↑(xi,xj)=Rq↓(xj,xi),∀xi, xj∈ U.Definition 4Let DS = 〈U, C∪{d}, V, f〉 be a multi-criteria decision system, and q ∈ C be an ordinal condition attribute. Fuzzy preference inconsistency rates (FPIR) w.r.t. q are calculated as:•The upward fuzzy preference inconsistency rate (UFPIR) w.r.t. q is:(7)FPIRq↑=∑x∈U∑u∈Umin(Rq↑(u,x),Rd<(u,x))∑x∈U∑u∈URd<(u,x)whereRd<(xi,xj)is the less-than relation w.r.t. decision d, which is calculated as follows:Rd<(xi,xj)={1f(xi,d)<f(xj,d)0otherwiseThe downward fuzzy preference inconsistency rate (DFPIR) w.r.t. q is:(8)FPIRq↓=∑x∈U∑u∈Umin(Rq↓(u,x),Rd<(u,x))∑x∈U∑u∈URd<(u,x)Additionally, FPIR has the following property:Proposition 1FPIRq↑+FPIRq↓=1As ∀x, u ∈ U,Rq↑(u,x)+Rq↑(x,u)=1andRq↑(x,u)=Rq↓(u,x),we haveRq↑(u,x)+Rq↓(u,x)=1.Furthermore,FPIRq↑+FPIRq↓=∑x∈U∑u∈Umin(Rq↑(u,x),Rd<(u,x))∑x∈U∑u∈URd<(u,x)+∑x∈U∑u∈Umin(Rq↓(u,x),Rd<(u,x))∑x∈U∑u∈URd<(u,x)=∑x∈U∑u∈Umin(Rq↑(u,x),Rd<(u,x))+min(Rq↓(u,x),Rd<(u,x))∑x∈U∑u∈URd<(u,x)We have known that the value ofRd<(u,x)is 1 or 0.WhenRd<(u,x)=0,min(Rq↑(u,x),Rd<(u,x))+min(Rq↓(u,x),Rd<(u,x))=0=Rd<(u,x). WhenRd<(u,x)=1,min(Rq↑(u,x),Rd<(u,x))+min(Rq↓(u,x),Rd<(u,x))=Rq↑(u,x)+Rq↓(u,x)=1=Rd<(u,x).So, we get the conclusion thatFPIRq↑+FPIRq↓=∑x∈U∑u∈URd<(u,x)∑x∈U∑u∈URd<(u,x)=1.□Proposition 1 indicates that ifFPIRq↑<0.5,thenFPIRq↑<FPIRq↓. So q should be considered a gain criterion. Analogously ifFPIRq↑>0.5,q should be considered a cost criterion. This means that we only need to calculate the UFPIR of an ordinal condition attribute when checking its monotonicity.Finally, we consider the parameter k in Eqs. (5) and (6), which is used to control the preference degree by users. In this paper, we only concern in which direction the inconsistency rate of an ordinal condition attribute is smaller. So it is not important what value k takes. Assuming k = +∞ and defining + ∞ · 0 = 0, we have the following fuzzy preference relations w.r.t. q.•The upward fuzzy preference relation:(9)Rq↑(xi,xj)={1f(xi,q)>f(xj,q)0.5f(xi,q)=f(xj,q)0f(xi,q)<f(xj,q)The downward fuzzy preference relation:(10)Rq↓(xi,xj)={0f(xi,q)>f(xj,q)0.5f(xi,q)=f(xj,q)1f(xi,q)<f(xj,q)On the other hand, we haveFPIRq↑+FPIRq↓=1. SoFPIRq↑=(ξq+1)/2. This means thatFPIRq↑can be considered a linear transformation of ξq. However, using DIR, we have to calculate both UDIR and DDIR of an ordinal condition attribute. While using FPIR, we only need to calculate UFPIR. In other words, using FPIR, we would get the same result with less computation. So in the following, we only calculate the UFPIR to check the monotonicity of an ordinal condition attribute.Furthermore, we have:−α≤ξq≤α⇔(1−α)/2≤FPIRq↑≤(1+α)/2,ξq<−α⇔FPIRq↑<(1−α)/2,andξq>α⇔FPIRq↑>(1+α)/2.We introduce a new threshold α′ = α/2, α′ ∈ [0, 0.5]. In the remainder of this paper, the direction of preference of an ordinal condition attribute is determined as follows: whenFPIRq↑∈[0.5−α′,0.5+α′],q has no global monotonicity relationship with d; whenFPIRq↑∈[0,0.5−α′),q is monotonically increasing related to d; whenFPIRq↑∈(0.5+α′,1],q is monotonically decreasing related to d.DRSA and VC-DRSA can be applied to multi-criteria classification problems for inducing decision rules (Greco et al., 2001b; Błaszczyński et al., 2011) and assigning objects to classes according to these rules (Błaszczyński et al., 2007). In these problems, data sets are generally described in the form of multi-criteria decision tables, which are also called classification tables. Induction of decision rules involves calculating lower and upper approximations and inducing a set of minimal decision rules from them. The rules induced from lower approximations are called certain decision rules and those induced from upper approximations are called possible decision rules. As objects are classified mainly according to certain rules, we only concentrate on certain decision rules in this paper.After a transformation proposed by Błaszczyński et al. (2012a), DRSA and VC-DRSA can also be applied to non-ordinal classification problems. This method transforms a classification table S to n binary ordinal classification tablesSt′,t = 1, …, n, if the decision attribute of S is non-ordinal and has n distinct values in its value set. Moreover, each condition attribute is cloned individually (possibly binarized to g 0-1 number-coded ordinal condition attributes before being cloned, if the condition attribute is nominal and has g distinct values in its value set) to a pair of ordinal attributes with supposed inverse monotonic relationships to the class assignment, except the ordinal condition attributes with clearly positive or negative directions. This method has the capacity of discovering global and local monotonicity relationships that are unknown a priori.Błaszczyński et al. method clones all ordinal condition attributes with unknown directions. However, in some cases, the transformed table is too complex. Let us consider the following classification table shown in Table 2, in which there are eight objects described by an ordinal condition attribute q and an ordinal decision attribute d.According to the available data, it appears reasonably to consider q as a gain criterion, if we suppose that q is globally monotonically related to d. In this case, the granules of knowledge w.r.t. P = {q↑} are:DP+(x)={y∈U:f(y,q↑)≥f(x,q↑)}andDP−(x)={y∈U:f(y,q↑)≤f(x,q↑)}. The lower and upper approximations w.r.t. P are:P̲(Cl1≥)={x7,x8},P¯(Cl1≥)={x4,x5,x6,x7,x8},P̲(Cl0≤)={x1,x2,x3},P¯(Cl0≤)={x1,x2,x3,x4,x5,x6}. Notice that x4, x5 and x6 are inconsistent objects and belong to P-boundaries:BnP(Cl1≥)=BnP(Cl0≤)={x4,x5,x6}. The global relationship between q↑ and d is imperfect and can be reflected using VC-DRSA. For simplicity, we only concern monotonic VC-DRSA in this study. (In fact, we would get the same result using standard VC-DRSA).Let us assume that the monotonic consistency level ε is, for example, 0.25. The lower and upper approximations within VC-DRSA are:P̲0.25(Cl1≥)={x4,x5,x7,x8},P¯0.25(Cl1≥)={x4,x5,x6,x7,x8},P̲0.25(Cl0≤)={x1,x2,x3},P¯0.25(Cl0≤)={x1,x2,x3,x6},and P-boundaries are:BnP0.25(Cl1≥)=BnP0.25(Cl0≤)={x6}. Applying VC-DomLEM (Błaszczyński et al., 2011), we have the following certain rules:1:if f(x, q↑) ≤ 1.3, then f(x, d) ≤ 0, with confidence 1.0,if f(x, q↑) ≥ 1.4, then f(x, d) ≥ 1, with confidence 0.8.All the objects covered by the first rule (no. 1), {x1, x2, x3}, are consistent. So rule no. 1 has a confidence of 1. While the second rule (no. 2), which covers one inconsistent object, {x6}, has a confidence of 0.8.On the other hand, if we suppose that q has local monotonicity relationships with d, we have the conclusion that q has positive monotonicity relationships in subsets {x1, x2, x3, x4, x5} and {x6, x7, x8}, and a negative monotonicity relationship in subset {x5, x6}.In this case, attribute q is cloned to a pair of attributes P′ = {q↑, q↓}, where q↑ is a gain criterion and q↓ is a cost criterion. The granules of knowledge w.r.t. P′ are:DP′+(x)={y∈U:f(y,q↑)≥f(x,q↑)∧f(y,q↓)≤f(x,q↓)}={y∈U:f(y,q)=f(x,q)}andDP′−(x)={y∈U:f(y,q↑)≤f(x,q↑)∧f(y,q↓)≥f(x,q↓)}={y∈U:f(y,q)=f(x,q)}. The lower and upper approximations w.r.t. P′ are:P′̲(Cl1≥)=P′¯(Cl1≥)={x4,x5,x7,x8},P′̲(Cl0≤)=P′¯(Cl0≤)={x1,x2,x3,x6}. Notice that all objects in the transformed table are consistent w.r.t. P′. This means that the upper and lower approximations would never change even the definition of lower approximations is relaxed by introducing variable consistency level. Applying VC-DomLEM with ε = 0.25 to the transformed table, we have a more complex set of rules as follows:1:if f(x, q↑) ≤ 1.3, then f(x, d) ≤ 0, with confidence 1.0,if f(x, q↑) ≥ 1.4, then f(x, d) ≥ 1, with confidence 0.8,if f(x, q↑) ≤ 1.6 and f(x, q↓) ≥ 1.6 (or f(x, q) = 1.6), then f(x, d) ≤ 0, with confidence 1.0,if f(x, q↑) ≥ 1.7, then f(x, d) ≥ 1, with confidence 1.0.We say that the transformed set P′ of condition attributes is more complex than the original set P, because P⊆P′. Then the granules of knowledge w.r.t. P′ become smaller, i.e.DP′+(x)⊆DP+(x)andDP′−(x)⊆DP−(x). This means that for x ∈ U, ifDP+(x)⊆Clt≥,thenDP′+(x)⊆Clt≥and ifDP−(x)⊆Clt≤,thenDP′−⊆Clt≤. In other words, a consistent object w.r.t. P must be a consistent object w.r.t. P′, but not vice versa. So we haveP̲(Clt≥)⊆P′̲(Clt≥)andP̲(Clt≤)⊆P′̲(Clt≤). As the induced set of decision rules should be complete which means all objects in lower approximations are covered by the induced rules, we get a more exact set of rules which explains the class assignments of more number of objects. If too many objects, in this example, all objects, are included in lower approximations, we would get an over exact set of rules. Many studies in machine learning have shown that over exact knowledge learned from learning sets would not work well in real applications. This is called “over-fitting” (Mitchell, 1997; Alpaydin, 2010).Furthermore, even applying VC-DRSA with consistency level ε > 0 to a transformed decision table, only inconsistent objects w.r.t. P′ would be reassigned. In fact, we always haveP′̲(Clt≥)⊆P′̲ϵ(Clt≥)andP′̲(Clt≤)⊆P′̲ϵ(Clt≤). However, the problem here is too many objects are assigned to lower approximations. It is impossible to exclude the objects consistent with P′ from lower approximations within VC-DRSA. We would still extract an over exact set of rules from the transformed table.To solve this problem, we propose a refined transformation method with an additional step to determine whether an ordinal condition attribute with unknown preference direction should be cloned or not. In this refined method, a classification table is transformed in the following way. For all nominal condition attributes, they are transformed in the same way as Błaszczyński et al. methodology. For all ordinal condition attributes that are clearly monotonically increasing or decreasing related to the decision attribute, no transformation is needed. For an ordinal condition attribute q with unknown direction, we calculate its inconsistency rateFPIRq↑,comparing with threshold α′. IfFPIRq↑lies in interval [0.5 − α′, 0.5 + α′], we consider that q has no global monotonicity relationship with the decision attribute d ( but may have local monotonicity relationships). So q is cloned to a pair of attributes {q↑, q↓}, where q↑ is a gain criterion and q↓ is a cost criterion. Otherwise, we consider that q has a global monotonicity relationship with d and has not to be cloned. Furthermore, ifFPIRq↑∈[0,0.5−α′),q is labeled as a gain criterion, q↑, while ifFPIRq↑∈(0.5+α′,1],q is labeled as a cost criterion, q↓.We are considering an illustrative classification table in this section. Classification table S, presented in Table 3, is composed of eight objects characterized by set C of ordinal condition attributes and ordinal decision attribute d, where C = {q1, q2, q3}. Furthermore, it is unknown whether there are monotonicity relationships between the values of attributes from C and decision attribute d.Firstly, we calculate UFPIR of q1, q2 and q3, getting the following results:FPIRq1↑=0.1667,FPIRq2↑=0.8810,FPIRq3↑=0.1667. Let threshold α = 0.25. We have that q1 and q3 are gain criteria and q2 is a cost criterion. So it is unnecessary to clone these condition attributes. The set of condition attributes labeled with the calculated directions isP={q1↑,q2↓,q3↑}. The dominance cones w.r.t. P are calculated asDP+(x)={y∈U:f(y,q1↑)≥f(x,q1↑)∧f(y,q2↓)≤f(x,q2↓)∧f(y,q3↑)≥f(x,q3↑)},DP−(x)={y∈U:f(y,q1↑)≤f(x,q1↑)∧f(y,q2↓)≥f(x,q2↓)∧f(y,q3↑)≤f(x,q3↑)}. In addition, the downward and upward unions of decision classes are the following:Cl1≤={x1,x2,x3},Cl2≤={x1,x2,x3,x4,x5},Cl2≥={x4,x5,x6,x7,x8},Cl3≥={x6,x7,x8}. Therefore, we have that the P-lower and upper approximations of unions of decision classes are:P̲(Cl1≤)={x2,x3},P¯(Cl1≤)={x1,x2,x3,x4},BnP(Cl1≤)={x1,x4},P̲(Cl2≤)={x1,x2,x3,x4,x5},P¯(Cl2≤)={x1,x2,x3,x4,x5},BnP(Cl2≤)=⌀,P̲(Cl2≥)={x5,x6,x7,x8},P¯(Cl2≥)={x1,x4,x5,x6,x7,x8},BnP(Cl2≥)={x1,x4},P̲(Cl3≥)={x6,x7,x8},P¯(Cl3≥)={x6,x7,x8},BnP(Cl3≥)=⌀.We apply VC-DomLEM to these P-lower approximations, using the implementation from jRS and jMAF frameworks (see http://www.cs.put.poznan.pl/jblaszczynski/Site/jRS.html). The induced set of minimal certain decision rules, denoted by R, is listed in Table 4.On the contrary, we transform S using Błaszczyński et al. method, in which all ordinal condition attributes are cloned. S is transformed to a new classification table S′ (see Table 5). Let the set of transformed condition attributes beP′={q1↑,q1↓,q2↑,q2↓,q3↑,q3↓},whereq1↑,q2↑,q3↑are gain criteria,q1↓,q2↓,q3↓are cost criteria.In S′, the P′-lower and upper approximations are the following:P′̲(Cl1≤)=P′¯(Cl1≤)={x1,x2,x3},BnP′(Cl1≤)=⌀,P′̲(Cl2≤)=P′¯(Cl2≤)={x1,x2,x3,x4,x5},BnP′(Cl2≤)=⌀,P′̲(Cl2≥)=P′¯(Cl2≥)={x4,x5,x6,x7,x8},BnP′(Cl2≥)=⌀,P′̲(Cl3≥)=P′¯(Cl3≥)={x6,x7,x8},BnP′(Cl3≥)=⌀. Note that there is no inconsistent object in S′ w.r.t. P′. (In fact,DP′+(x)andDP′−(x)are calculated based on indiscernibility relations rather than dominance relations. We haveDP′+(xi)=DP′−(xi)=xi,i=1,…,8.)Applying DomLEM to these P′-lower approximations, we get another set of minimal certain decision rules, denoted by R′ and listed in Table 6. Comparing with Table 4, there are two added rules, marked by *, in Table 6. These added rules are supported by objects {x1} and {x4, x5}, respectively, where x1 and x4 are inconsistent objects in S but consistent objects in S′. In general, we believe that R is better than R′, because R′ is so exact that all the objects in the learning set are covered. So R′ has a high possibility to over-fit.In this section, some experiments are provided to demonstrate the effects of the threshold α on the monotonic classifiers applied to the transformed data sets and compare the refined transformation method with Błaszczyński et al. method in terms of mean absolute error (MAE) and percentage of correctly classified objects (PCC).In essence, both Błaszczyński et al. and the refined transformation methods are a preprocessing approach which can be followed by any monotonic classification algorithm. In this work, we apply VC-DomLEM algorithm with monotonic consistency level ε, proposed by Błaszczyński et al. (2011), to the transformed data sets. Consider the following factors: (1) as analyzed in Section 4, redundancies introduced by cloning attributes cannot be ruled out by introducing the variable consistency level, because relaxing the definition of lower approximations does not exclude any object from them, but possibly make more objects be included, for example, see Table 2; (2) this work focuses on how to preprocess ordinal data sets with unknown preference directions in order to make it possible to apply monotonic classification algorithms to them, but the effects of ε on the induced classifiers are more relevant to VC-DomLEM itself, rather than to the preprocessing and beyond the scope of this work. So ε is fixed to 0.25 in the tests presented in this section.We still use the implementation of VC-DomLEM from jRS and jMAF frameworks in the following tests. Note there is a built-in conversion which subtracts the input value from 1 as the actual monotonic consistency level. So the input value of monotonic consistency level should be set to 0.75.Twelve real-life ordinal data sets are estimated in the following experiments. Data sets: eucalyptus and grub-damage are taken from Weka (http://www.cs.waikato.ac.nz/ml/weka/). Others are downloaded from the UCI repository (http://archive.ics.uci.edu/ml/). All of them are shown in Table 7. Furthermore, we assume that we do not know the directions of preference for all the ordinal condition attributes in these data sets.The first experiment is to demonstrate the effects of threshold α on the performances of the classifiers trained on the transformed data sets using 10-fold cross-validation. In each fold, the FPIR of each ordinal condition attribute is calculated on the training set. Then the training set is transformed following the refined cloning method with α ranging from 0.00 to 0.50 with a step 0.05. So there is a set of training sets in each fold. VC-DomLEM algorithm is applied to these training sets and the induced decision rules are validated on the validation set. We perform such 10-fold cross-validation 5 times repeatedly on each data set and calculate the average of PCC and MAE. The results of this experiment are depicted in Fig. 1.There is a point worth being clarified that when α is big enough, all ordinal condition attributes would be doubled, then the transformed data set would never change even α keeps increasing. For example, on data set “tae”, the greatest valid value of α is 0.15. Furthermore, we find that the curve of MAE is roughly a mirror of that of PCC on each data set. This means that it is enough to only focus on the effects of increasing α on PCC.On data sets “wine quality red”, “diabetes” and “grub-damage”, PCC is demoted as α increases, which matches our conclusion very well that when a data set is in the state of over-fitting, increasing α, then increasing the dimension of the transformed data sets, will make the performances of classifiers become worse.It is also possible that PCC is promoted at first when α increases, see the curves of PCC on data sets “tae” and “eucalyptus”, because when α is too small, the transformed data sets may be in the state of under-fitting. In this context, the performances of classifiers would be improved when data sets become more complex. However PCC will not keep increasing forever. After α becomes big enough, PCC is unchanged, see the curve of PCC on “tae” when α ≥ 0.05, or even becomes worse, see the curve of PCC on “eucalyptus” when α ≥ 0.25.On data sets “credit approval”, “breast-w”, “housing”, “ionosphere” and “heart disease”, PCC changes in a more complex way that there is an increasing phase even PCC has begun to decrease. Although it is yet not clear what makes this happen, this is not in contradiction with our theory. Because the fact that a data set is in the state of over-fitting only predicts that the performances of classifiers running on it are very likely to be demoted, does not guarantee that there are strict monotonic relationships between the number of condition attributes and the performances of classifiers. Furthermore, even on these data sets, PCCs on totally doubled data sets are still worse than those on partly doubled data sets with a small α equals to 0.00 or 0.05.Only on data sets “car” and “cpu”, increasing α has almost no effect on PCC, i.e. PCC is neither promoted, nor demoted significantly as α increases.Fig. 1 shows that the refined transformation method is, at least, not worse the Błaszczyński et al. method over these 12 data sets, considering the fact that the refined transformation boils down to Błaszczyński et al. method when all ordinal attributes are cloned.The above experiment provides an visualized, but qualitative demonstration of the effects of α. In the second and third experiments, we will compare the two methods statistically using the Wilcoxon signed-ranks test (Wilcoxon, 1945).We use 5 × 2 cross-validation to estimate the performances of the two transformation methods on each data set. The steps of one round are as follows:The original data set S is randomly divided into two subsets, S1 and S2 with equal size. Firstly S1 is served as training set and S2 is served as validation set. For Błaszczyński et al method, each ordinal condition attribute in S1 is cloned to a pair of attributes and labeled with inverse preference directions. The transformed training set is denoted asS1′.For the refined method, the threshold α is tuned for the optimal PCC, i.e. PCC is calculated with 5-fold cross-validation on S1 for α = 0.0, 0.05, 0.10, 0.15, …, 0.50, then the value of α which generates the greatest PCC is selected. The training set transformed following the refined method with the optimized α is denoted asS1′′.VC-DomLEM algorithm is applied toS1′andS1′′respectively, inducing two sets of dominance decision rules,R1′andR1′′. Both of them are validated on the same validation set S2.The roles of S1 and S2 are changed and the two transformation methods are re-validated. After five rounds, the averages of PCC on the data sets transformed using Błaszczyński et al method and the refined method respectively are listed in Table 8in which the forth column is the differences between the results of the two transformation methods and the fifth column is the ranks of the differences.We use the Wilcoxon signed-ranks test to compare the performances of the two preprocessing approaches over multiple data sets. In this case, the null hypothesis is that the PCC of VC-DomLEM running on data sets transformed using the refined method is equal to that on data sets transformed using Błaszczyński et al. The sum of positive ranks is T+ = 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 60, the sum of negative ranks is T− = 1 + 2 + 3 = 6, and T = min(T+, T−) = 6. For a confidence level of α = 0.05 and N = 11, the critical value is 10. So the null hypothesis is rejected with confidence 0.95. In other words, the refined transformation is significantly better than Błaszczyński et al method.Similarly, the third test aims to get a classifier which generates the best MAE. So α is tuned by choosing the value of α which generates the smallest MAE on training sets. The results of the third test are listed in Table 9in which the forth column is the differences between the results of the two transformation methods and the fifth column is the ranks of the differences.In this test, the null hypothesis is that the refined and Błaszczyński et al. methods run equally well in terms of MAE. Here we have T+ = 3 + 5 = 8, T− = 1 + 2 + 4 + 6 + 7 + 8 + 9 + 10 + 11 = 58 and T = min(T+, T−) = 8. T is equal or less than the critical value 10, for a confidence level of α = 0.05 and N = 11. So the null hypothesis is rejected as well.

@&#CONCLUSIONS@&#
When extracting ordinal classification rules from multi-criteria decision tables within DRSA or VC-DRSA, one has to know if there exist positive or negative monotonicity relationships between the values of ordinal condition and decision attributes. Błaszczyński et al. transformation method that allows application of DRSA and VC-DRSA to non-ordinal classification problems is able to discover global and local monotonicity relationships represented by decision rules. In this method, all ordinal condition attributes with unknown directions of preference are cloned to pairs of attributes. However, in some cases, this transformation is overcomplex.In this paper, we propose the dominance inconsistency rates (DIR) and fuzzy preference inconsistency rates (FPIR), which have the capacity of discovering global monotonicity relationships directly from data rather than induced rules. We also present a refined transformation method, in which an additional step is introduced to determine whether an ordinal condition attribute should be cloned or not according to its inconsistency rates.Some experiments are performed on 12 real-life data sets under the hypothesis that the directions of preference of all the ordinal condition attributes are unknown. For all the data sets tested in the first experiment, although the performances of VC-DomLEM applied on them are not always demoted significantly as α increases, there is no one on which the performances are promoted. The second and third tests statistically compare the refined and Błaszczyński et al. transformation methods using the Wilcoxon signed-ranks test, in which the threshold α of refined method is tuned for optimal PCC and MAE respectively. These tests show that the refined method is better than Błaszczyński et al. method in terms of PCC and MAE.There are still some interesting problems to be addressed. The first experiment shows that α affects the performances of following monotonic classifiers more significantly on some data sets than on others. It is unclear on which type of data sets the effects of α would be more significant. On the other hand, let the set of condition attributes of a transformed data set following Błaszczyński et al. method be the universal set. The refined method can be considered a dimensionality reduction algorithm which generates a subset of the universal set by selecting a part of ordinal attributes to be cloned based on their FPIR, in order to reduce the redundancies introduced by cloning attributes. Whether the cloning redundancies can be reduced in other ways, for example, removing at most one attribute in each doubled pairs from the universal set, is an interesting question for future studies.