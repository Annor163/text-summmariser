@&#MAIN-TITLE@&#
HOPE: A Python just-in-time compiler for astrophysical computations

@&#HIGHLIGHTS@&#
We discuss the reasons for the lower execution speed of Python.We present HOPE, a specialised Python just-in-time compiler.The package combines the ease of Python and the speed of C++.HOPE improves the execution speed up to a factor of 120 compared to plain Python.The code is freely available under GPLv3 license on PyPI and GitHub.

@&#KEYPHRASES@&#
Python,Just-in-time compiler,Benchmark,

@&#ABSTRACT@&#
The Python programming language is becoming increasingly popular for scientific applications due to its simplicity, versatility, and the broad range of its libraries. A drawback of this dynamic language, however, is its low runtime performance which limits its applicability for large simulations and for the analysis of large data sets, as is common in astrophysics and cosmology. While various frameworks have been developed to address this limitation, most focus on covering the complete language set, and either force the user to alter the code or are not able to reach the full speed of an optimised native compiled language. In order to combine the ease of Python and the speed of C++, we developed HOPE, a specialised Python just-in-time (JIT) compiler designed for numerical astrophysical applications. HOPE focuses on a subset of the language and is able to translate Python code into C++ while performing numerical optimisation on mathematical expressions at runtime. To enable the JIT compilation, the user only needs to add a decorator to the function definition. We assess the performance of HOPE by performing a series of benchmarks and compare its execution speed with that of plain Python, C++ and the other existing frameworks. We find that HOPE improves the performance compared to plain Python by a factor of 2 to 120, achieves speeds comparable to that of C++, and often exceeds the speed of the existing solutions. We discuss the differences between HOPE and the other frameworks, as well as future extensions of its capabilities. The fully documented HOPE package is available at http://hope.phys.ethz.ch and is published under the GPLv3 license on PyPI and GitHub.

@&#INTRODUCTION@&#
In recent years, the Python programming language has gained a wide acceptance beyond its original use of simple scripting for system administration and test automatisation. Python has evolved to become a primary programming language for various industries and many scientific research fields, notably in astrophysics (TIOBE Software, 2014; Diakopoulos et al., 2014). The reasons for this success are the simplicity, flexibility and maintainability of Python code as well the wide range of its libraries. In particular, the widely used numerical Python packages NumPy (van der Walt et al., 2011) and SciPy (Jones et al., 2001) allow for fast prototyping and development of new applications. The flexible runtime and the dynamic typing are further features of the interpreter language that are valued by developers. However, these advantages come with a drawback: typically the execution time of Python programs can be slower than native compiled languages such as C or Fortran by orders of magnitudes.While for many applications the performance of the software is not a priority, in astrophysics and cosmology where large simulations and data analysis over large data sets are often required, speed can be crucial (see e.g. Refregier and Amara, 2014, Akeret et al., 2013 and references therein for cosmology). In order, to increase the overall performance one can parallelise the programs to take advantage of multicore CPU architectures. An alternative and complementary approach is to focus on improving the single thread performance. The problem of improving the single thread performance of dynamic languages, such as Python, has been addressed in various different ways that can be grouped broadly into two categories: (1) development of optimising just-in-time (JIT) compilers and (2) development of faster interpreters (Arnold et al., 2005). The concept of the latter is to reduce the overhead introduced by the dynamic nature of the language. The idea of a JIT compiler is to produce faster machine or byte code during runtime when needed (Cuni, 2010). In the Python landscape both approaches have been implemented in various projects e.g. the PyPy11http://www.pypy.org.interpreter, the Numba22http://numba.pydata.org.JIT package or the Cython33http://www.cython.org.C-extension and others.While most of these approaches aim to support the complete language or a large portion of it, we find that some of the solutions are often intrusive (i.e. require the user to tailor the code to the framework) and that only a few of them are able to reach the full speed of an optimised native compiled C++ code. To fully combine the ease of Python and the speed of C++, we have therefore developed the HOPE package. HOPE is a specialised Python JIT compiler that supports a subset of the Python language–primarily numerical features commonly used in astrophysical calculations–and aims to reach the highest possible execution speed. The package translates Python code into C++ and is able to perform numerical optimisation on mathematical expression at runtime. By using HOPE, the user benefits from being able to write common numerical code in Python while having the performance of compiled implementations. To enable the HOPE JIT compilation, the user only needs to add a decorator to the function definition. The package does not require additional information, which ensures that HOPE is as non-intrusive as possible.We used the HOPE package in astrophysics applications such as the development of PyCosmo (Refregier et al., in preparation) and to rewrite UFig (the Ultra fast image generator) (Bergé et al., 2013) into Python. PyCosmo is a Python cosmology package that numerically integrates the Einstein–Boltzmann differential equations and computes various cosmological observables. The HOPE package allowed us to improve the performance of the integration by a factor of50×compared to the pure Python implementation by JIT compiling the integrated function. UFig is an image generator developed to simulate wide field cosmological galaxy surveys thus enabling tight control of systematics effects through forward modelling (Refregier and Amara, 2014; Bergé et al., 2013). HOPE has allowed us to rewrite the original C++ implementation of UFig into Python. Benchmarks show a performance comparable to the earlier C++ code, but the new implementation has increased the modularity, extensibility and readability of the code.This paper is organised as follows. In Section  2, we discuss the reason for the lower execution speed of dynamic languages, and review the existing solutions to address the performance implications in Python. Section  3 explains how we address the performance requirements using our JIT compiler package and describe the HOPE architecture and design. In Section  4 we introduce the benchmarks we used to compare the performance of HOPE to the existing solutions. We show and discuss results in Sections  5 and 6 and finally conclude in Section  7. Information for downloading the HOPE package and on its performance on an alternative platform are described in Appendices A and B, respectively.Python is a dynamic interpreted language, which requires an interpreter for the execution of a program. Typically the performance of Python is much slower than C or comparable compiled languages. There are several reasons for this: Python has not been designed to be a fast language, but, instead, readability has been defined to be more important. In addition, everything in Python, including simple values, are represented as objects in memory. Therefore, the evaluation of a numerical expression requires the interpreter to unbox every value and to load the appropriate operation, as operators can be overwritten dynamically.Currently four main implementation of interpreters exist: CPython, Jython, IronPython and PyPy. CPython is the reference implementation written in C and the most widely used Python interpreter. Jython and IronPython are alternative implementations of Python interpreters targeting the Java virtual machine and the .NET frameworks, but they typically do not improve performance (Cuni, 2010). PyPy is the most prominent effort to develop an alternative interpreter in order to increase the execution speed. In the following, we always refer to the CPython implementation if not explicitly stated otherwise.If performance is critical, one approach is to write the critical code in C or C++ and interface the implementation with Python bindings. With this approach the performance requirements can typically be met but often at the cost of the readability. Experience shows that this makes it difficult for diverse users to maintain and extend the C/C++ code. A further drawback of this approach is that every change in the code requires a recompilation. During development this has to be performed repeatedly, which lowers the productivity and the development velocity of the team.To be able to implement a code in Python but nevertheless be able to properly address the performance needs, different alternative approaches exist, including: Numba, Cython, Nuitka and numexpr (see Table 1). The Numba package is a JIT compiler that generates optimised machine code at runtime using the LLVM compiler infrastructure. A similar but different approach is to translate and compile the Python source code. The Cython C-extension for Python is widely used for this. The Nuitka project44http://www.nuitka.net.is a static Python compiler that focuses on full support of the language. A further approach is the numexpr package55https://github.com/pydata/numexpr.that specialises in evaluating numerical expressions and additionally allows parallel execution on multiple cores. Besides the frameworks listed in Table 1 further solutions exist. Pyston66https://github.com/dropbox/pyston.is a very new project under current development at Dropbox and is also built on the LLVM compiler infrastructure. Further frameworks are topic to research projects such as Pythran (Pythran, 0000; Guelton et al., 2014), a static Python compiler and parakeet (Rubinsteyn et al., 2012), which follows a similar approach as HOPE.HOPE is a specialised method-at-a-time JIT compiler written in Python. It translates Python source code into C++ and compiles the generated code at runtime. In contrast to other existing JIT compilers, which are designed for general purpose, we have focused our development of the subset of the Python language that is most relevant for astrophysical calculations. By concentrating on this subset, HOPE is able to achieve very high performance for these applications.HOPE is able to translate commonly used unary, binary and comparison operators as well as augmented assign statements. Currently the supported native built-in data types are bool, int and float and their corresponding NumPy data types (e.g.  int32, float32 resp. int64, float64, etc.). This applies to scalar values as well as the NumPy arrays with these types. Several features of NumPy and mathematical functions likesin,cos,exp, etc. are also supported. HOPE, for instance, allows operations on NumPy arrays with the common slicing syntax (e.g. a[5:, 5:] = x). A full list of the functionality backed by the package is published online77http://pythonhosted.org/hope/lang.html.and a set of examples is provided on GitHub.88https://github.com/cosmo-ethz/hope/tree/master/examples.The package can be used with Python 2.7 as well as with Python 3.3+. We have tested HOPE in Linux and Mac OSX environment with gcc and clang compilers.HOPE infers the data types by analysing the function signature and inspecting the Abstract Syntax Tree (AST) of the function at runtime. This information is used to translate the Python function into statically typed C++ code. Using Python’s native extensions for C API99https://docs.python.org/2/extending/extending.html.the code is compiled into a shared object library, which is then loaded into the interpreter runtime. During the process of generating the C++ code, HOPE is able to apply various numerical optimisations in order to improve the execution performance of the compiled function (see Section  3.1). Once compiled, the function is cached to disk to minimise lag on future executions of the same code.We have chosen to generate C++ code and compiling the code into a library over other approaches, such as the direct generation of byte code. This design decision was made as the intermediate product–the generated code–is human readable, which greatly simplifies the development and debugging process and, furthermore, allows the use of automatic hardware specific optimisation of modern compilers without additional effort. The just-in-time compiling process is described in Fig. 1. A function call undergoes the following several steps:StartThe Python interpreter loads a function or method previously decorated with the @hope.jit decorator.HOPE checks if a compiled version of the requested functions has previously been cached. In case the code is executed the first time, HOPE returns a wrapper function containing a reference to the original function.The first time the decorated function is called, the wrapper generates an abstract syntax tree (AST) by parsing the function definition using the Python built-in ast package.Using the visitor pattern, the Python AST is traversed and a corresponding HOPE specific AST is generated. During the traversal we use the Python built-in inspect package to infer the data types of the live objects such as parameters, variable and return values. Using this information we are able to statically type the HOPE AST i.e. scalar variables will be assigned a type and array liked variable will receive a data type and a shape information. Operations on arrays with the same shape will be grouped into blocks. Furthermore a scope is assigned to each variable in order to identify if it will be passed as parameter or if it has to be instantiated. In the latter case HOPE distinguishes between temporary variables, used only once (block scope), and variables used multiple time in the function (body scope).HOPE traverses the new AST in order to identify numerical optimisation possibilities and alters the tree accordingly. A detailed explanation of the optimisation is discussed in Section  3.1.A C++ code is generated from the HOPE AST. First the function signature is created using the name of the function, the type of the return value and the names of the parameters including their type and shape information. If necessary, statements for the instantiation of new variables are generated. Next, each block is turned into a loop statement according to the shape information of the contained arrays. By grouping array operations, we are able to evaluate the operation element-wise, which improves cache locality. For variables with block scope we can avoid allocating a whole array and instead a scalar value can be allocated (an example is discussed in Section  3.2). Finally, the generated code is augmented with Python’s native extensions API statements, so that the code can be called from the Python interpreter.The Python built-in setuptools package is then used to compile a shared object library from the generated code. Using this package and defining the generated code as an extension greatly simplifies the handling of compiler and ensures compatibility with the interpreter.Using the extracted information from the function signature and a hash over the function body the compiled shared object library is cached for future calls.The shared object library is dynamically loaded into the runtime and the pointer to the wrapper is replaced with the pointer to the function in the shared object library to avoid unnecessary overhead.A call to the function is directed to the function in the shared object library and executed with the passed parameters.HOPE analyses the types of the passed arguments and queries the cache for a function matching the requested name and arguments. If the system registers a cache hit for a function, the shared object library is then loaded into the runtime and the compiled function is evaluated, otherwise a new function will be generated and compiled.After the HOPE specific AST has been created the package performs a static recursive analysis of the expressions to introduce numerical optimisation. The supported possibilities are divided into three groups: (1) simplification of expressions, (2) factorising out subexpressions and (3) replacing the pow function for integer exponents. To simplify expression we have used the SymPy library (SymPy Development Team, 2014). SymPy is a Python library for symbolic mathematics and has been entirely written in Python. To apply the optimisation, the AST expression is translated into SymPy syntax AST and passed to the simplify function. The function applies various different heuristics to reduce the complexity of the passed expression. The simplification is not exactly defined and varies depending on the input. For instance, one example of simplification is thatsin(x)2+cos(x)2will be simplified to 1. Furthermore the SymPy library is used to factorise out recurring subexpression (common subexpression elimination) using the previously created SymPy AST and SymPy’s csefunction.From C++11 on, the pow function in the C standard library is not overloaded for integer exponents.1010http://en.cppreference.com/w/cpp/numeric/math/pow.The internal implementation of the computation of a base to the power of a double exponent is typically done using a series expansion, though this may vary depending on the compiler and hardware architecture. Generally this is efficient for double exponents but not necessarily for integer exponents. HOPE therefore tries to identify power expressions with integer exponents and factorises the expression into several multiplications e.g.y=x5will be decomposed intox2=x2andy=x2×x2×x. This reduces the computational costs and increases the performance of the execution.In this section, we show the translation process of HOPE using the following simple example:As soon as this function is called, HOPE will create a statically typed AST using the type information available at runtime. Assuming that the function call was done with x and y defined as one dimensional float64NumPy array with same length, the resulting AST can be visualised as the following pseudo code:Using the inspected data types of the parameters and by analysing the mathematical expressions, HOPE is able to identify that operations are performed on arrays with compatible data types and dimensions and will group those expressions into a block. The result of this block is a new array with the same data type and dimension, which will also be used as return value. This information tells HOPE that a new array has to be instantiated and how the function signature will be defined. Next, the block is turned into a loop over every element of the arrays, which includes the power operation as well as the addition operations. Finally, the power operation on the x value with the integer exponent is optimised into a multiplication of the x value.The HOPE package has been developed using the test-driven development (TDD) approach, allowing us to ensure a high level of code quality and numerical accuracy up to type-precision. For every supported language feature we have written a unit test that is executed for all the supported data types and array shape combinations. This results in over 1600 unit tests being executed on our continuous integration server per supported Python version.In this section, we describe benchmarks aimed at assessing the performance of HOPE and at comparing it to the other packages described in Section  2. Since a standardised set of benchmarks for testing the performance of software does not exist, we have generated our own series of tests. The first set are three numerical micro-benchmarks published on the Julia language website1111http://julialang.org.and which are supported by the current version of HOPE. These benchmarks have already been applied to various languages. We omitted benchmarks with matrix manipulations, as good performance typically depend on the proper use of specialised matrix libraries such as Intel’s Math Kernel Library (MKL) or the Basic Linear Algebra Subprograms (BLAS). Additionally, we defined two benchmarks favouring numexpr as they test the ability to compute polynomials and mathematical expression which can be simplified. Finally, we have specified two special purpose benchmarks representing simplified versions of common problems in astrophysics to compare the performance of the general-purpose frameworks. To have a reference and baseline, we additionally implemented all the benchmarks in C++.For our benchmark tests we have chosen packages where the user should, in principle, be able to write a code in Python and that only minor modifications would needed to enable the performance improvements. All the benchmarks have been made available online as IPython notebooks.1212http://hope.phys.ethz.ch.The following describes the benchmarks and provide the associated Python source code.The Fibonacci sequence is a common measurement of the execution speed of repeated recursive function calls. The sequence is defined as:Fn=Fn−1+Fn−2,F0=0,F1=1.We executed all the benchmarks withn=20resulting in the answer 6765. The Python implementation is as follows:Quicksort is a powerful yet simple sorting algorithm that gained widespread adoption in Unix as the default sorting function and in the C standard library. It extends the complexity of the previous recursive benchmark with a loop, multiple comparison and different data types. The algorithm was implemented as follows:The algorithm is used in the benchmarks to sort an array of 5000 uniform random float64.This benchmark is a simple approximation ofπ/2, testing the run time behaviour of two nested for-loops.p=∑j=1501∑k=1100011k∗kIn this numerical benchmark a logarithm is approximated using a Taylor expansion resulting in a 10th order polynomial. The approximation is defined as follows:ln(x)≈∑i=19=(−1)i−1(x−1)iiand can be implemented in Python as such:For our benchmarks, a slightly optimised version of this implementation has been used:where X as been defined as an array of 10,000 uniform random float64.This benchmark has been specified asy(x)=sin2(x)+(x3+x2−x−1)(x2+2∗x+1)+cos2(x)where X as been defined as an array of 5000 uniform random float64. The benchmark tests the ability of the packages to efficiently compute polynomial expressions. As the expression can be simplified toy=xthis benchmark will favour frameworks, which analyse and optimise the expression such as numexpr and HOPE.Computing the two-point correlation function for a set of points is a common problem in many areas of astronomy and cosmology. A similar but simplified problem is the computation of the distance between all the points. Using thel2norm this results in an×nmatrix for a given input array of sizen. The distance is defined as:Di,j=∑k=0N(Xi,k−Xj,k)2,∀i∈I,∀j∈JwhereN=3is the number of dimensions andI=J=1000is the number of points in the array. The naive solution used in the benchmark is implemented as follows:Additionally we benchmark the vectorised implementation of this test using the NumPy package:This benchmark has been inspired by calculations that simulate astronomical imaging data. One step in these simulations is to make realistic simulations of the images of stars. For many astronomical applications stars are smaller than the resolution of the instrument. Therefore, the resulting images are realisations of the Point Spread Function (PSF) coming from the finite resolution and atmospheric effects. A good model for the PSF of ground-based telescopes is a circular Moffat profile (Moffat, 1969), given byI(r)=I0[1+(rα)2]−βwhereI0is the value at the origin (r=0),αandβare parameters depending on the conditions of the observation (see Appendix B in Bergé et al., 2013). The numerical integration in thexandydirection is done using Gauss–Legendre integration with 7th order coefficients:The density and w2D variables are 20×20 and 7×7Numpy arrays, respectively, while the other parameter are defined as scalar values. A very similar approach has been implemented in the UFig project (Bergé et al., 2013).

@&#CONCLUSIONS@&#
