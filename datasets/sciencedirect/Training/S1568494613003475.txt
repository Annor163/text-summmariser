@&#MAIN-TITLE@&#
A new constructive neural network method for noise processing and its application on stock market prediction

@&#HIGHLIGHTS@&#
Some noise data are considered as singular values of a continuous function.RBF neural networks are constructed to fit the singular value with every ɛ error.A theorem about a function with m jumping discontinuity points has been proved.The constructive part has no generalization influence to the learning system.A real world problem has been presented to verify the correctness of the theory.

@&#KEYPHRASES@&#
Neural network architecture,Decay RBF neural networks,Overfitting,Noise,Stock market prediction,

@&#ABSTRACT@&#
In this paper, in order to optimize neural network architecture and generalization, after analyzing the reasons of overfitting and poor generalization of the neural networks, we presented a class of constructive decay RBF neural networks to repair the singular value of a continuous function with finite number of jumping discontinuity points. We proved that a function with m jumping discontinuity points can be approximated by a simplest neural network and a decay RBF neural network inL2(ℝ)by each ɛ error, and a function with m jumping discontinuity pointy=f(x),x∈E⊂ℝdcan be constructively approximated by a decay RBF neural network inL2(ℝd)by eachε>0error. Then the whole networks will have less hidden neurons and well generalization in the same of the first part. A real world problem about stock closing price with jumping discontinuity have been presented and verified the correctness of the theory.

@&#INTRODUCTION@&#
Neural networks have attracted increasing attention from researchers in many fields, including information processing, computer science, economics, medicine and mathematics, and have been used to solve a wide range of problems such as data mining, function approximation, pattern recognition, expert system and data prediction, etc. The widespread popularity of neural networks in many fields is mainly due to their ability to approximate complex multivariate nonlinear functions directly from the input samples. Neural networks can provide models for a large class of natural and artificial phenomena that are difficult to handle using classical parametric techniques [1–5].One of the most important problems that neural network designers face today is choosing an appropriate network size for a given application. However, the process of selecting adequate neural network architecture for a given problem is still a controversial issue.And it is empirically known that the problem is particularly serious when the size of the network is large. When a network is trained with noisy data, it may have a very small training error that is caused by fitting the noise rather than the true function underlying the data. In such situations, the generalization error tends to be larger than its optimal level because the trained network may deviate from the true function [6]. We also call this phenomenon overfitting [7–9].The overfitting problem is a critical issue that usually leads to poor generalization [10–12]. One of the main reasons of over-fitting is the excessive noise data or singular value in the practical problems [13,14]. So the traditional methods of processing noise data are to remove noise data before approximation through various algorithms such as wavelet transform [15–18], principal component analysis [19,20] and various filtering algorithms [21–24]. But sometimes, the “noisy” data we think of ways to remove are often some singular values of a process which contains important information [25].In this paper, in order to optimize neural network architecture and generalization, after analyzing the reasons of poor generalization and overfitting of neural networks, we presented a class of constructive decay RBF neural networks to repair the singular values of a continuous function with finite number of jumping discontinuity points. And a real world problem about stock closing price with a jumping discontinuity have been presented and verified the correctness of the theory. First, we will consider some noise data as singular values of a continuous function - jump discontinuity point. The continuous part can be approximated using less size neural network, which have optimal architecture and good generalization performance, by traditional algorithms such as constructive algorithm for feedforward neural networks with incremental training [26,27], BP algorithm [28,29], ELM algorithm [30,31], various constructive algorithm[32–34], RBF approximation [35–37] and SVM [38]. Then, we will construct a RBF neural network to fit the singular value with every ɛ error inL2(ℝd), and then the whole network will have optimal architecture and generalization.This paper is organized as follows; Section 2 investigates the phenomenon of neural network overfitting caused by noisy data. Section 3 gives some previous works on approximation of functions by neural networks. Section 4 investigates constructive multidimensional approximation of a function with one jump discontinuity point. Theorems 6–9 are proved. Section 5 investigates constructive multidimensional approximation of a function with finite number of jump discontinuity points, Theorems 10–13 are proved. In section 6, a real world problem about stock closing price with a jumping discontinuity point have been presented and verified the correctness of the theory. Section 7 provides some conclusions.Overfitting is a well-known generalization problem for neural network due to the finite training set, which greatly reduce its generalization ability in practical applications [39]. But one of the important factors to cause overfitting is noisy data[40]. In the real world, some underlying function relationship between the input and desired output are simple, but the sample data are always corrupted by noise to some degree. Consequently learning with noisy data would need too many hidden neurons and then results in poor generalization ability. We will describe this phenomenon through a simple experiment.Example: the following sample dataset A comes from the functiony=cosxwhich contained 60 points (Fig. 1). And sample dataset B comes from functiony=f1(x)=cosxx≠π−2x=π,which has a noise data in functiony=cosxand also contained 60 points (Fig. 2).In the following experiments, we will approximate the dataset A and B using traditional single hidden layer feedforward neural networks trained by BP algorithm. Although there are many variants of BP algorithm, a faster BP algorithm called Levenberg–Marquardt is used in our experiments. All the experiments are carried out in MATLAB 7.10 (R2010a) environment running in a Intel(R) Core(TM) i3-2120 3.30GHz CPU. We will use 54 points of the dataset to train the neural networks and the other 6 points to test the networks.In the experiments, we will compare the following performance index: training time; the ratio of the training time with the training time of dataset A (RTT)RTT=trainingtimeofdatasetBtrainingtimeofdatasetA;training and testing root mean square error (RMSE)RMSE=1n+1∑i=0n(f(xi)−y˜i)2wherey˜iis the output value of the neural networks in the simulation point; the ratio of training RMSE (RTRR)RTRR=traingingtimeofdatasetBtrainingtimeofdatasetAthe ratio of testing RMSE (RTER)RTER=testingtimeofdatasetBtestingtimeofdatasetAand the number of the hidden neurons.In Fig. 3, the neural network only need 4 hidden neurons to fit the functiony=cosxvery well with 0.234 CPU training time,9.2318×10−7training RMSE and 0.00026123 testing RMSE.In Fig. 4, because there is one noise point in dataset B, 4 hidden neurons in the neural networks is not enough to fit the dataset B, which it spent 5.8926 CPU time to train the networks with 0.012537 training RMSE that is 13580 times of that in Fig. 3, and 0.0057322 testing RMSE which is 21.943 times of that in Fig. 3.So we increased hidden neurons to 10 to fit it well, in Fig. 5, it spent 2.4024 CPU time to train the networks with9.9419×10−7training RSME which is almost equals that in Fig. 1, but the testing RMSE is 0.00083815 that is larger 3.2085 times than that in Fig. 1. So with the increase of the complexity of the networks and the precision of training RMSE, the generalization of the neural networks was reduced instead because of the noise data.The results above three experiments are concluded into the Table 1.In this paper, the dataset with noisy data is divided into two parts, one part contains simple function relationship, and the other part consists of jumping discontinuity points. The first part can be approximated with the optimal neural network architecture, which has less number of hidden neurons and good generalization performance, by traditional algorithm such as BP, ELM and SVM. At the same time, we will construct a RBF neural network to approximate the singular value with every ɛ error inL2(ℝd)which has no influence to the generalization of the first part and the whole neural networks.There are many good results on approximation of continuous functions relationship without noisy data by neural networks that have best performing architecture and well generalization by properly using traditional algorithm such BP, ELM, SVM and some constructive approaches.Letϕ:ℝ→ℝ.Define(1)∑d(ϕ)≡spanϕ(w.x+b):b∈ℝ,w,x∈ℝd,ThenN∈∑d(ϕ)if and only if(2)N(x)=∑j=0ncjϕ(wj.x+bj),wherecj,j=0,⋯,nare real numbers and n is a positive integer.We say thatϕis a sigmoid function, if it verifieslimt→−∞ϕ(t)=0andlimt→∞ϕ(t)=1, then (2) is called single-layer feed-forward neural network. Ifϕ(x)=φ(x-x0)x∈ℝd, we callϕ(x)RBF function, then (2) is called RBF neural networks.In the case of continuous functions we have the following density resultsTheorem 1([41]) Letϕ∈C(ℝ). Then∑d(ϕ)is dense inC(ℝd)in the topology of uniform convergence on compact if and only ifϕis not a polynomial.([42])y=f(x)∈C(ℝd)can be approximated by a simplest neural networks (such as with minimum number of hidden neurons).In the case of not necessarily continuous functions we also have some density results.Theorem 2([43]) Letϕbe bounded, measurable and sigmoidal. Then∑d(ϕ)is dense inL1([0,1]d).The following theorem is a generalization of the above results.Theorem 3([44]) Letϕbe a Lebesgue measurable function, not a.e. equal to a polynomial, satisfying∫abϕ(x)pdx<∞for alla,b∈ℝ. Let K be a compact set inℝd. Then for any functionf∈Lp(K)(p≥1)and everyε>0, there is a networkN∈∑d(ϕ)such thatN−fK,p<ε, wheregK,p≡(∫Kg(x)pdx)1p.For RBF neural networks we have the following results.Theorem 4([35]) Letϕbe a RBF function, Then for any functionf∈Lp(ℝ)(p≥1)and everyε>0, there is a networkN∈∑d(ϕ)such that([32]) 1. Letx∈[a,b]k⊂ℝk, andf(x)be a multivariate continuous function,xi∈[a,b]k,i=0,1,⋯,nbe an uniform grid partition to[a,b]k, where[a,b]is divided to s equal partition, and arrange by breadth-first such thatx0,x1,⋯,xn(n=sk). Then2. A depends on n, that isA=A(n).3. The real numbersfiare the images ofxiunder a multivariate continuous functionf(x), that isfi=f(xi),i=0,1,2,⋯,n.For eachε>0, we can construct a decay RBF neural networkWa(x,A(n)), and there exists a functionA(n)and a natural number N such that, whenn>N, we havef(x)−Wa(x,n,A(n))<ε, for allx∈[a,b]k.The above theorems on approximation of continuous function can be carried out by many traditional machine learning systems with good generalization such as in Fig. 3.In this section, we introduce a new constructive approach to reduce the overfitting phenomenon of machine learning system, especially neural networks, which can reduce the hidden neurons to optimal neural network architecture. The sample dataset with one noisy data is divided into two parts, the first part is considered to come from a simple continuous function. The other part is consisted of noisy data and is considered as one jumping discontinuous point of the continuous function. Then we can use many traditional methods to fit the continuous function with optimal architecture and good generalization, for the noisy part, we can construct a decay RBF neural network to fit it without influence the generalization to the first part and the whole machine learning system.Definition 1Considerψ(x)be a continuous real function, also the condition is given aslimx→∞ψ(x)=0=o(e−x2), andψ(0)≠0. We callψ(x)is a decay RBF, and the decay RBF neural networks (DRNNs) can be written asAs we known that Gaussian functionψ(x)=e−x2and wavelet functions, e.g. Mexican Hat waveletψ(x)=(2/3)π−1/4(1−x2)e−x2/2and Morlet waveletψ(x)=(2/3)e−x2/2cos5xbelong to decay RBF. The definition and nature of wavelet function are introduced in [45,46].Replaceφ(x)=ψ(x)/ψ(0)to (3), we can getφ(0)=1,limx→∞φ(x)=0then we can rewriting (3) as followNW(x)=∑j=0ncjψ(λjx−tj)=∑j=0ncjψ(0)ψ(λjx−tj)ψ(0)=∑j=0nkjφ(λjx−tj)Lemma 1Considerφ(x)is a decay RBF function, there exist real numbersk1,k2and positive real number A, such that whenx>A, we havek1e−x2<φ(x)<k2e−x2, andφ(x)is bounded inℝ.Aslimx→∞φ(x)=0=o(e−x2), thenlimx→∞φ(x)e−x2=k, and for eachε>0there exists a positive real number A, such that whenx>A, we haveφ(x)e−x2−k<ε, that is,−ε<φ(x)e−x2−k<ε, thenk−ε<φ(x)e−x2<k+εandNow, considery=f(x)x∈ℝdis continuous exceptx=x0, that is meaning,limx→x0f(x)≠f(x0), we can decomposef(x)as two partsfc(x)andfd(x), it is shown asf(x)=fc(x),+fd(x)wherefc(x)=f(x)x≠x0limx→x0f(x)x=x0is continuous andfd(x)=0x≠x0f(x0)−limx→x0f(x)=h0x=x0Example 1For one-dimensional function:f(x)can be decomposed as tofc(x)andfd(x), that is meaningf(x)=fc(x)+fd(x), wherefc(x)=cosxandfd(x)=0x≠π−1x=π.Example 22-D functionz=f(x,y)We can decomposef(x,y)tofc(x,y)andfd(x,y), sof(x,y)can be denoted asf(x,y)=fc(x,y)+fd(x,y),wherefc(x,y)=f(x,y)x2+y2≠0limx2+y2→0f(x,y)=1x2+y2=0is continuous andfd(x,y)=0x2+y2≠0f(0,0)−limx2+y2→0f(x,y)=1=h0x2+y2=0Theorem 6For eachε>0, there exists a constructive RBF neural networksNWd(x,A)and a positive real numberA′, such that whenA>A′we havefd(x)−NWd(x,A)<ε.ProofConsiderNWd(x,A)=h0φ(Ax−x0)is a RBF neural networks with one neuron, soNWd(x0,A)=h0φ(Ax−x0)=h0fd(x)−NWd(x,A)=NWd(x,A)=(∫Rh0φ(Ax−x0)2dx)12=h0(∫Rφ(Ax−x0)2dx)12=h0[(∫x−x0<δφ(Ax−x0)2dx)12+(∫x−x0≥δφ(Ax−x0)2dx)12]By Lemma 1,φ(x)<M>0, thenh0[(∫x−x0<δφ(Ax−x0)2dx)12<h0Mδ<ε2,onlyifδ<ε2h0M,and whenAx−x0>A1that is,A>A1x−x0>A1δ, we can geth0(∫x−x0≥δφ(Ax−x0)2dx)12<h0∫Rk2e−A2x2dx<h0k2∫Re−A2x2dx,=h0k2A∫Re−x2dx=h0k2Aπ<εThen, whenA>h0k2επ, we can getfd(x)−NWd(x,A)<ε2+ε2=ε.Remark 1The decay neural networkNWd(x,A)can be constructed to fit the noisy data without influence to the generalization of whole machine learning (Figs. 6–9).A function with one jumping discontinuity point can be repaired to a continuous function inL2(ℝ)by decay RBF neural networks with each ɛ error.It is obvious by Theorem 6.For Example 1, function (Fig. 6)y=cosxx≠π−2x=πcan be repaired byy˜=cosx+e−A2(x−π)2with Gaussian RBF functione−x2for each ɛ whenA>A1, havingy−y˜<ε(Fig. 8)Example 2, the function (Fig. 7)z=sinx2+y2x2+y2x2+y2≠02x2+y2=0can be repaired byz˜=fc(x,y)+e−A2(x2+y2)with Gaussian RBF functione−(x2+y2)for each ɛ whenA>A1, havingz−z˜<ε(Fig. 9)Theorem 8A function with one jumping discontinuity point can be approximated by a simplest neural networks with a decay RBF neural networks inL2(ℝ)by each ɛ error.It is obvious by Theorem 4 and Corollary 1.A function with one jumping discontinuity pointy=f(x),x∈E⊂ℝdcan be constructively fitted by a decay RBF neural networks inL2(ℝd)by eachε>0error.Asf(x)=fc(x)+fd(x),fc(x)∈E⊂C(ℝd), from Theorem 4, for eachε>0, we can construct a decay RBF neural networkNWc(x,A), and there exists a natural numberA1such that, whenA>A1, we havefc(x)−NWc(x,A)<ε2, for allx∈E⊂ℝd.And from Theorem 6 for eachε>0, there exist a constructive RBF neural networksNWd(x,A)and a positive real numberA2, such that whenA>A2we havefd(x)−NWd(x,A)<ε2.Then whenA>A0=maxA1,A2we have(4)f(x)−(NWc(x,A)+NWd(x,A))≤fc(x)−NWc(x,A)+fd(x)−NWd(x,A)<ε2+ε2=εAndNW(x,A)=NWc(x,A)+NWd(x,A)is also a constructive RBF neural networks.Based on the above theorems, a sample dataset with one noisy point can be divided into two parts; the first part can be considered from a simple continuous function which can be fit by traditional machine learning system such as Fig. 1, the noisy part can be fit by a decay neural network with constructive approach. Then the whole machine learning system has optimal architecture and well generalization of part one, which can reduced the overfitting phenomenon of machine learning system.The content of this section is the extension of Section IV to the situation ofmnoisy points which has the same conclusion of optimizing neural network architecture and generalization.Lety=f(x)x∈ℝdhasmjumping discontinuity pointsxj,j=1,2,⋯,m, that is,limx→xjf(x)≠f(xj)j=1,2,⋯,m. We can decomposef(x)tofc(x)andfd(x), that is,f(x)=fc(x)+fd(x), where(5)fc(x)=f(x)x≠xjj=1,2,⋯,mlimx→xjf(x)x=xjj=1,2,⋯,mis continuous and(6)fd(x)=0x≠xjj=1,2,⋯,mf(xj)−limx→xjf(x)=hjx=xjj=1,2,⋯,mTheorem 10Ify=f(x)x∈ℝdhasmjumping discontinuity pointsxjj=1,2,⋯,m, that is,limx→xjf(x)≠f(xj)j=1,2,⋯,mFirstly letfd(x)=∑j=1mfdj(x), whereThen for each jumping discontinuity pointxj, by theorem 6, for eachε>0, there exist a constructive RBF neural networksNWdj(x,A)=hjφ(Ax−xj)with one neuron and a positive real numberA′j, such that whenA>A′jwe havefdj(x)−NWdj(x,A)<εm.Then, we constructNWd(x,A)=∑j=1mNWdj(x,A)=∑j=1mhjφ(Ax−xj),whenA>A′=maxA′1,A′2,⋯,A′m, we havefd(x)−NWd(x,A)≤∑j=1mfdj(x)−NWdj(x,A)<mεm=εTheorem 11A function withmjumping discontinuity points can be repaired to a continuous function inL2(ℝ)by decay RBF neural networks with eachεerror.It is obvious by Theorem 10.A function withmjumping discontinuity point can be approximated by a simplest neural networks and a decay RBF neural networks inL2(ℝ)by eachεerror.It can be proved similarly to Theorem 8.A function withmjumping discontinuity pointsy=f(x),x∈E⊂ℝdcan be constructively approximated by a decay RBF neural networks inL2(ℝd)by eachε>0error.It can be proved similarly to Theorem 9.In this section, a practical problem of stock data is presented to verify above theory. All stock market trends are fast changing. It is affected by not only the individual investors and many institutional investors, but also impacted by domestic political, economic situations and many other factors. Therefore, it is very difficult to build a classical parameter model to predict the market movement [47]. But it is easy to build a NNs model to fit the stock dataset to predict the stock closing price.We first collected the sample data of closing price of Chongqing Iron & Steel (601005) in Chinese Shanghai stock market from internet stock database, which is called as dataset C. The collection period is from 4 January 2012 to 8 October 2012 and the number of data totaled 120 (Fig. 10).From Fig. 10, we can observe there is a noise inx=35(2012.6.1), which have too much information to move it. And we can create a mathematical model to detect the noisy data as follows:Iff(xi)−f(xi−1)f(xi−1)>k>0,f(xi)−f(xi+1)f(xi+1)>k>0and(f(xi)−f(xi−1))(f(xi)−f(xi+1))>0, wherekis a control parameter based on specific problems, we considerf(xi)as a noisy point. In Fig. 10, we takek=0.08. Then we can define the follow detection function:d(xi)=1,f(xi)−f(xi−1)f(xi−1)>k>0,f(xi)−f(xi+1)f(xi+1)>k>0and(f(xi)−f(xi−1))(f(xi)−f(xi+1))>00,otherwiseSo in the noisy pointxi,d(xi)=1, otherwise,d(xi)=0. We applied the detection function to Fig. 10, and get the results in Fig. 11.From the result of Fig. 11, we can seef(x35)is a noisy point. In the following experiments, we will use 115 points of the dataset to train the NNs and the other 5 points to test the NNs.In Fig. 12, the NNs with 6 hidden neurons fit the dataset C with 17.4565 CPU training time, 0.0016164 training RMSE and 0.0013742 testing RMSE.In order to fit dataset C more well, if we increase the hidden neurons to 9 neurons, we get the results as Fig. 13. Although the training RSME decreased to 0.0014821, but the testing RMSE increased to 0.017929 that is larger 13.05 times than that in Fig. 12 because of the noisy point overfitting.Now, we consider the dataset C comes from a functiony=f(x)with a jumping discontinuous point atx=35. Then we decompose functionf(x)as two partsfc(x)andfd(x), so,f(x)=fc(x)+fd(x), wherefc(x)=f(x)x≠35limx→35f(x)=12(f(34)+f(36))=3.42x=35is continuous part, which dataset D comes from, andfd(x)=0⁡x≠353.72−3.42=0.3x=35.For the dataset D which came from a smoother functionfc(x), a BP NNs with 6 hidden neurons can fit it very well in Fig. 14with 0.0012357 training RMSE and 0.00083884 testing RMSE that is smaller 21.37 times than that in Fig. 13 and 1.64 times that in Fig. 12 because of no overfitting the noisy point with less hidden neurons.Then we constructed a constructive RBF NNsNWd(x,A)withA=50to approximate the functionfd(x), and the comprehensive results are presented in Fig. 15, which have less hidden neurons and smaller training and testing RMSE that verified the correctness of above theory.Remark 1In real practical problems, because of many factors, noise data always exist. And we always assume that noise data can be distinguished from non-noise one based on some practical principle or experiences such as above example. These noise data can make the mathematical model such as neural networks very complex or overfitting. So the traditional methods of processing noise data is to remove them before approximation through various algorithms such as wavelet transform [15–18], principal component analysis [19,20] and various filtering algorithms [21–24]. But sometimes, the “noisy” data we think of ways to remove are often some singular values of a process which contains important information [25] which have other uses in the next step and would not be removed.In above real stock price example, based on the practical experience, the data point atx=35changes so much bigger than others that makes the neural network for prediction overfitting. So we consider this data is a noise data, but we does not remove it in the prediction process.

@&#CONCLUSIONS@&#
In this paper, after analyzing the reasons of poor generalization and overfitting in neural networks, we consider some noise data as a singular values of a continuous function - jump discontinuity point. The continuous part can be approximated with the simplest neural networks, which have good generalization performance and optimal network architecture, by traditional algorithms such as constructive algorithm for feed-forward neural networks with incremental training, BP algorithm, ELM algorithm, various constructive algorithm, RBF approximation and SVM. At the same time, we will construct RBF neural networks to fit the singular value with everyεerror inL2(ℝd), and we prove that a function withmjumping discontinuity points can be approximated by the simplest neural networks with a decay RBF neural networks inL2(ℝ)by each ɛ error, and a function with m jumping discontinuity pointy=f(x),x∈E⊂ℝdcan be constructively approximated by a decay RBF neural networks inL2(ℝd)by eachε>0error and the constructive part have no generalization influence to the whole machine learning system which will optimize neural network architecture and generalization performance, reduce the overfitting phenomenon by avoid fitting the noisy data. And a real world problem about stock closing price with jumping discontinuity have been presented and verified the correctness of the theory.