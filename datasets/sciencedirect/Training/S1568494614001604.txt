@&#MAIN-TITLE@&#
Least learning machine and its experimental studies on regression capability

@&#HIGHLIGHTS@&#
Least learning machine LLM is proposed for training both SLFN and MLFN with a single or multiple outputs.Only the parameters in the last hidden layer of MLFN require being adjusted, all the parameters in other hidden layers can be randomly assigned.LLM is much faster than BP for MLFN in training the sample sets.

@&#KEYPHRASES@&#
Feedforward neural network,Extreme learning machine,Hidden-feature-space ridge regression,Least learning machine,

@&#ABSTRACT@&#
Feedforward neural networks have been extensively used to approximate complex nonlinear mappings directly from the input samples. However, their traditional learning algorithms are usually much slower than required. In this work, two hidden-feature-space ridge regression methods HFSR and centered-ELM are first proposed for feedforward networks. As the special kernel methods, the important characteristics of both HFSR and centered-ELM are that rigorous Mercer's condition for kernel functions is not required and that they can inherently be used to propagate the prominent advantages of ELM into MLFN. Except for randomly assigned weights adopted in both ELM and HFSR, HFSR also exploits another randomness, i.e., randomly selected examplars from the training set for kernel activation functions. Through forward layer-by-layer data transformation, we can extend HFSR and Centered-ELM to MLFN. Accordingly, as the unified framework for HFSR and Centered-ELM, the least learning machine (LLM) is proposed for both SLFN and MLFN with a single or multiple outputs. LLM actually gives a new learning method for MLFN with keeping the same virtues of ELM only for SLFN, i.e., only the parameters in the last hidden layer require being adjusted, all the parameters in other hidden layers can be randomly assigned, and LLM is also much faster than BP for MLFN in training the sample sets. The experimental results clearly indicate the power of LLM on its application in nonlinear regression modeling.

@&#INTRODUCTION@&#
In recent years, feedforward neural networks have been extensively used for nonlinear regression modeling. Its widespread popularity in regression modeling is mainly due to their strong ability to approximate complex nonlinear mappings directly from the input samples. From a mathematical viewpoint, existing works about the approximation capability of feedforward neural networks can be categorized into two types: universal approximation on compact input sets and approximation of a finite set of samples. Theoretical results about the universal approximation of feedforward neural networks have been obtained by Hornik and Lesino, see [28,29]. In real-world applications, since feedforward neural networks are trained on a finite set of samples, much more endeavors should be taken for the approximation capability of the second type. Typically, gradient descent based learning algorithms like BP [1–5] of feedforward neural networks have been developed and extensively applied in the last decades. When these learning algorithms are used, all the parameters of feedforward neural network need to be adjusted in a backward way and thus there exists the dependence relationship between different layers of parameters in the network. Due to iterative learning steps, these learning algorithms generally converge very slowly and even to local minima. On the other hand, cross-validation and/or early stopping are sometimes adopted to circumvent the over-fitting phenomena.In order to overcome these shortcomings of the existing learning algorithms, Huang et al. demonstrated and proved that the traditional iterative techniques are not required in adjusting parameters of SLFNs at all. Based on the universal approximation capability of SLFNs with random hidden nodes, Huang et al. proposed a simple and efficient learning method referred to as extreme learning machine (ELM) [1–14,17–21]. They proved that the input weights and the hidden layer biases can be randomly assigned if the activation function in the hidden layer is infinitely differentiable. Once the input weights and the hidden layer biases are randomly assigned, SLFN can be considered as a linear system and the output weights of SLFN can be analytically solved by using the simple generalized inverse operation of the hidden layer output matrix. With its easy implementation, ELM can tend to reach both the smallest training error and the smallest norm of weights and thus provide good generalization performance at extremely fast learning speed, for example, thousands of times faster than BP in many applications [5].Up to now, many variants of ELM have been developed. Huang et al. [2,10,11,33] gave an intensive survey on ELM and its variants, especially on batch learning mode of ELM [4,5,32], fully complex ELM [12], online sequential ELM [3], incremental ELM [2,10,11], and ensemble of ELM [8,31]. As stated by Huang [5], however, ELM at its present form can only be applied to SLFNs. For many real-world applications, a multiple hidden layer feedforward neural network is more suitable for nonlinear regression modeling since it can approximate large number of samples with less hidden nodes than SLFN can [34].Although extreme learning machine is able to learn thousands of times faster than conventional popular learning algorithms for SLFNs, developing a fast learning method for MLFNs is still an open problem. In this paper, this problem is well investigated by building the link between extreme learning machine (ELM) and the variants of ridge regression. Two variants of ridge regression, i.e., the hidden-feature-space ridge regression HFSR and centered ridge regression Centered-ELM, for both SLFN and MLFN are first proposed. As the special kernel methods, the virtues of both HFSR and Centered-ELM exist in that rigorous Mercer's condition for kernel functions is not required and that it plays a bridging role in naturally propagating the prominent advantages of ELM into MLFN by using randomly assigned parameters and randomly selected samples for kernel activation functions. Through constructing the transformed data set from the training dataset in a forward layer-by-layer way, we can easily extend HFSR and Centered-ELM to MLFN. Accordingly, as the unified framework for HFSR and Centered-ELM, the least learning machine (LLM) is proposed for both SLFN and MLFN with a single or multiple outputs. LLM keeps the same virtues of ELM only for SLFN, i.e., only the parameters in the last hidden layer require being adjusted, all the parameters in other hidden layers can be randomly assigned, and LLM is much faster than BP in training the sample sets. The experimental results on regression datasets clearly indicate the power of LLM on nonlinear regression modeling.It should be worth pointing out that the objective of this paper does not pursue for the performance advantage of LLM over ELM. Our contribution exists in two aspects: (1) Through LLM, we can extend ELM to MLFN with keeping the same virtues of ELM only for SLFN; (2) LLM indeed gives a new forward encoding learning way rather than a backward gradient-descent learning way in the widely used learning algorithm BP. It views the behavior of MLFN between the last hidden layer and the input layer as the successive encoding procedure for the input data in a difficult-to-understand way. To large extent, this new understanding can also help us answer why MLFN behaves like a black box.The remainder of this paper is organized as follows. In Section 2, we briefly review ELM for SLFN. In Section 3, we first propose the hidden-feature-space ridge regression HFSR and Centered-ELM, and then build the link between ELM and them for SLFN. Finally, we give the least learning machine LLM as the unified framework of HFSR and Centered-ELM for SLFN and MLFN with a single or multiple outputs. In Section 4, we report the obtained experimental results about Centered-ELM for SLFN and LLM for MLFN on artificial or benchmarking datasets. Section 5 concludes the paper.In this section, we give a brief review of the extreme learning machine for a single hidden layer feedforward neural network. For easy interpretation and derivation hereafter and without loss of generality, we first consider a single hidden layer feedforward neural network (SLFN for brevity) with a single output here. Given N arbitrary distinct samples (xj, tj),xj=[xj1, xj2, ..., xjn]T∈Rn, tj∈R, j=1, 2, ......, N, SLFN withN˜hidden nodes and the activation function g(x) and a single output can be mathematically modeled as(1)∑i=1N˜βigi(xj)=∑i=1N˜βig(wiT⁡xj+bi)=Ojj=1,2,…,Nwherewi=[wi1,wi2,…,win]Tis the weight vector connecting the ith hidden node and the input nodes,β=[β1,β2,…,βN˜]Tis the weight vector connecting all the hidden nodes and the output node, biis the threshold of the ith hidden node, andwiTxjdenotes the inner product ofwiandxj.We desire that the above SLFN with a single output can approximate these N samples with zero error, that is to say,(2)∑j=1N||Oj−tj||2=0,i.e.∑i=1N˜βig(wiTxj+bi)=tj,j=1,2,…,NThe above N equations can be compactly written as the following linear system(3)Hβ=TwhereH(w1,w2,...,wN˜,b1,b2,...,bN˜,x1,x2,...,xN)(4)=g(w1Tx1+b1)…g(wN˜Tx1+bN˜)⋮…⋮g(w1TxN+b1)…g(wN˜TxN+bN˜)N×N˜(5)β=β1β2...βN˜N˜×1T=t1t2...tNN×1whereHis called the hidden layer output matrix of SLFN, whose ith column is the ith hidden node output with respect to the inputsx1,x2, …,xN.According to Theorem 2.1 and Theorem 2.2 in [5], for the linear system in Eq. (4), its unique solution, i.e., the smallest norm least squares solutionβˆcan be computed as follows:(6)βˆ=H†TwhereH† is the Moore-penrose generalized inverse of the matrixH. Accordingly, Huang et al. proposed the following extreme learning machine ELM [5].Extreme learning machine ELM.Given the sample set D={(xj, tj)|xj∈Rn, tj∈R, j=1, 2, …, N}, the infinitely differential activation function g(x) and the hidden node numberN˜of SLFN with a single output.Step1: Randomly assign the weight vector and the biaswi,bi,i=1,2,…N˜Step2: Compute the hidden layer output matrixHStep3: Compute the output weight vector of SLFN, i.e.,βˆ=H†T, whereT=[t1, t2, …tN]T.In fact, letβ=β1Tβ2T⋮βmT,βi=[βi1,βi2,…,βiN˜]T,T=t1Tt2T⋮tmT,ti=[ti1, ti2, …, tiN]T, i=1, 2, …, m, according to Huang's theory [5], the above ELM still holds for SLFN with m multiple outputs.The distinctive advantages of ELM exist in its easy implementation with an analytical solution and extremely fast learning capability. Unlike the widely used back-propagation algorithm BP, after the input weights and the hidden layer biases are determined randomly, SLFN can be simply considered as a linear system and the output weights of SLFN can be solved by using the simple generalized inverse operation of the hidden layer output matrix, which results in the fact that ELM is generally thousands of times faster than BP. Besides, ELM not only tends to reach the smallest training error but also the smallest norm of weights, which indeed results in better generalization performance of SLFN than BP [5]. However, ELM in its present version is only valid for SLFN, and it is still an open issue how to extend it to a multiple hidden layer feedforward neural network with a single output (MLFN with a single output for brevity) or design a fast learning machine with all randomly chosen parameters in all hidden layers and a linear system in a single output layer. We will concentrate on this challenging issue in this paper.It is should be noted that according to ELM theory, the activation function of SLFN must be infinitely differentiable. Fortunately, the extensively used activation functions such as sigmoid, RBF and many wavelet functions are both infinitely differentiable and of kernel function types, which provides us the very feasibility of designing the least learning machine (LLM for brevity) below.In this section, we first define a new hidden-feature-space regression HFSR for SLFN and MLFN, and then explore the intrinsic relationship between HFSR and ELM. We also propose the learning machine Centered-ELM using HFSR for SLFN and MLFN. Finally, we develop a unified framework called least learning machine LLM to unify HFSR and Centered-ELM for both SLFN and MLFN and discuss the virtues of LLM.In this subsection, we will first propose the hidden-feature-space ridge regression HFSR with a single output and then reveal the relationship between it and ELM. Given the same training set D={(xj, tj)|xj∈Rn, tj∈R, j=1, 2, …, N} as above, letX=[x1,x2, …,xN]T, T=[t1, t2, …, tN]T, the classical ridge regression with a single output [16], as a QP problem, realizes a linear regression model based on the least square error:(7)β=(XTX)−1XTTFor arbitrary inputx, the corresponding single output t:(8)t=x(XTX)−1XTTIn order to enhance the performance of the classical ridge regression, we can apply the kernel trick to it. The extensively used kernel trick, like in SVM and SVR [15,26,27], is the direct use of certain kernel functions in the form of the inner products of the inputxand every sample in D after obtaining Eq. (8).However, here we explore an alternative special kernelized implementation called HFSR of the above classical ridge regression. For arbitraryxin D, we can construct a newN˜dimensional hidden feature space from all the n-dimensional input samples in D by presettingN˜infinitely differential kernel functionsg(x,θ1),g(x,θ2),…,g(x,θN˜)as the corresponding mapping functions, which are the same as the activation functions in the hidden layer of SLFN, whereg(x,θi)(i=1,2,…,N˜)takes certain kernelized form of the inner product of the inputxand some randomly selected distinct examplarxk∈D(k∈{1, 2, …, N}), andθ1,θ2,…,θN˜respectively denote the kernel parameter vectors inN˜kernel functions. In other words, this hidden feature space corresponds to the space generated by the activation functions of the hidden layer of the above SLFN with a single output. We should indicate that most of commonly used activation functions in feedforward neural networks satisfy this mild requirement about infinite differentiability, for example, sigmodial functions such as1/(1+e−xiTx+b); decaying RBF functions such as Gaussian functione−||x−xi||2/σ2, Mexican Hat wavelet function(2/3)π−1/4(1−||x−xi||2/σ2)e−||x−xi||2/σ2, Morlet wavelet function(2/3)e−x−xi2/σ2cos(5||x−xi||2/σ2); fuzzy basis functions [24]. In the above way, the approximation problem between inputs and outputs of the samples in D based on the activation functions of SLFN is transformed into the corresponding approximation problem in the hidden feature space, which can be effectively solved by using the following ridge regression with a single output.(9)min12β2+C∑j=1Nξj2s.t.(g(xj,θ1),g(xj,θ2),…,g(xj,θN˜))βT=tj+ξj,j=1,2,…,Nwhere C is a regularized parameter. We call the above regression in Eq. (9) as the hidden-feature-space ridge regression HFSR. Obviously, HFSR is the QP problem in a special form. We construct the training seth=[h1,h2, …,hN]T, wherehi=[g(xi,θ1),g(xi,θ2),…,g(xi,θN˜)],T=t1t2...tNN×1, according to the ridge regression in [19], the solution for Eq. (9) becomes(10)βˆT=hThhT+12CIN×N−1TPlease note, for randomly assigned parameter vectorsθi,(i=1,2,…,N˜), with enough large C,(hhT+12CIN×N)−1always exists, soβˆTin Eq. (10) always exists. For arbitrary inputx, the corresponding output is(11)t=hxβˆT=(g(x,θ1),g(x,θ2),......,g(x,θNˆ))hT(hhT+12CIN×N)−1TwhereIN×Nis an N×N identity matrix.We can easily see the virtues of HFSR as follows:Firstly, unlike most of SVMs or SVRs [26,27] where rigorous Mercer's condition [26,27] for kernel functions is required, HFSR does not require this rigorous Mercer's condition since kernel functions here have been preseted explicitly in the mapping functions, and the kernel trick as a means of implicitly defining the feature mapping, like in SVMs or SVRs, are not required even after the inner products appear in Eq. (11), and thus we may view HFSR and SVMs or SVRs as two distinct approaches for classification or regression tasks, and can easily extend the set of usable kernel functions in HFSR. For example, except RBF kernel function is often used in SLFN, the following sigmoidal kernel function(12)k(x,y)=tanh(θ1xTy+θ2),θ1,θ2∈Ris also often used in SLFN. Obviously, it is not positive definite, thus according to Mercer's condition, strictly speaking, it can not be used in SVMs or SVRs. However, we can adopt it here in HFSR.Secondly, we can observe the following properties from Eq. (10). These properties help us reveal the close link between ELM and HFSR.Property 1When C becomes large enough,βˆTin Eq. (10) can be approximated asβˆT=hT(hhT)−1T; furthermore, whenN=N˜,βˆT=hT(hhT)−1T=hT(hT)−1h−1T=h−1T. In terms of Theorem 2.1 and Theorem 2.2 in [5], since the kernel functionsg(x,θ1),g(x,θ2),…,g(x,θN˜)are infinitely differentiable, then with probability one,h−1 exists, thusβˆTbecomesh−1Twith probability one.WhenN˜≤N, sinceA†=limδ→0(ATA+δI)−1AT=limδ→0AT(AAT+δI)−1(see [30]), so when C is large enough, 1/2C becomes small enough, thus Eq. (10) can be approximately written as(13)βˆT=h†TAccording to Theorem 2.2 in [5], since the infinitely differential kernel functionsg(x,θ1),g(x,θ2),…,g(x,θN˜)are adopted, with probability one, HFSR here can approximate the outputTwithin the given approximation accuracy, for all randomly assigned parameter vectorsθ1,θ2,…,θN˜.Comparing Eq. (13) with (10), referring to Property (1), we can draw the following conclusion: for SLFN with a single output, ELM is approximately equivalent to HFSR with a enough large C. Besides, as we may know well from SVMs [26,27], the term (1/2)β2 in Eq. (9) actually has the generalization role forβ, which actually helps us well explain why ELM has good generalization capability.Thirdly, ifN=N˜, and g(x,θi) takes certain kernelized form about the inner product of the inputxand some randomly selected distinct examplarxk∈D(k∈{1, 2, …, N}, in terms of Eq. (11), for arbitrary inputx, the inner producthxhTcan be computed only in terms of the inner productsxTxj, (j=1, 2, …, N) andxiTxj,(i,j=1,2,…,N). Therefore, such a HFSR can be viewed as a special kernel method in the sense of fully kernelizing allxiTxj,(i,j=1,2,…,N). Conversely, whenN˜<N, g(x,θi) takes certain kernelized form about the inner product of the inputxand some randomly selected examplarxk∈D(k∈{1, 2, …, N}), let us recall that kernel methods can be scaled up by:(1)Sampling some representative samples from the sampling set, for example, SVM and SVR can be scaled up by CVM [24,25], using various sampling strategies.Randomly projecting on the high dimensional datasets [22,23]. In terms of Theorem 5 in [22], underN˜<N, whenN˜becomes large enough, kernel methods can be effectively approximated or scaled up. If we consider thatg(x,θ1),g(x,θ2),…,g(x,θN˜)are constructed by randomly projectingN˜dimensions from the sampling set, we can view HFSR withN˜kernel functionsg(x,θ1),g(x,θ2),…,g(x,θN˜)as the random-projection scaling-up version, i.e., the approximate implementation based on random projection, of the kernel method HFSR whereN=N˜, and g(x,θi) takes certain kernelized form about the inner product of the inputxand some randomly selected examplarxk∈D(k∈{1, 2, …, N}). In summary, HFSR in essence goes one more step than its kernelized version, and except for the fact that randomly assigned weights can result in so fast ELM, this new perspective can also help us understand why ELM (as HFSR with enough large C) can run so fast from another aspect. What is more, except for randomly assigned weights adopted in both ELM and HFSR, HFSR still exploits one more randomness, i.e., randomly selected examplars from the training set D for kernel activation functions.Fourthly, HFSR plays a bridging role in naturally propagating the prominent advantages of ELM for SLFN to MLFN. Through referring to the structure of the hidden layers of a multi-layer feedforward neural network (MLFN for brevity), we can easily extend the above HFSR to MLFN by constructinghiandhx(for the testing samplex) successively in the forward layer-by-layer way from the input layer among all the hidden layers of MLFN, see Fig. 1. In this figure, when we presentxito the input layer, the outputs in the last hidden layer will constitutehi; when we present a testing samplesxto the input layer, the output in the last hidden layer will constitutehx. Since all the activation functions among all the hidden layers are infinitely differential kernel functions, which take certain kernelized form of the inner product of the input vector from the previous hidden layer and given vector with the same dimension. After successive computations,hiandhxin the last hidden layer become two vectors in which every element is of the kernel type in the form of the inner product ofxin the input layer and some randomxk∈D(k∈{1, 2, …, N}). In term of partial differential theory, the kernel functiong(x,θi),(i=1,2,…,N˜)with respect toxin the last hidden layer is still infinitely differential, so according to Theorem 2.1 and Theorem 2.2 in [5], with probability one, such a HFSR for MLFN still can approximate the outputTwell within the given approximate accuracy for all randomly assigned parameter vectors in all the infinitely differential kernel functions of all the hidden layers.In fact, according to the idea of centered inputs in [18,19], HFSR in Eq. (9) can be equivalently realized by carrying out the corresponding HFSR for centered inputs. Leth¯=(1/N)∑i=1Nhi,t¯=(1/N)∑i=1Nti, the centralization matrixL=IN×N−(1/n)11T, where1∈RNis the vector in which all elements are equal to 1. Obviously,LT=L. After centralization withL, the training set becomeshL,T=t1−t−t2−t−...tN−t−. With HFSR forhLandThere, we have(14)βˆT=hTLLhhTL+12CIN×N−1TObviously, when C becomes large enough,βˆTwill degenerate into(15)βˆT=(Lh)†TFor arbitrary testing inputx, the corresponding output is(16)t=(hx−h¯)βˆTWe can call the above HFSR in Eqs. (15) and (16) the centered HFSR or CHFSR for brevity.In terms of the close link between HFSR and ELM as above, and by comparing Eq. (15) with (13), we can easily design the following extreme learning machine called Centered-ELM through CHFSR. Obviously, Centered-ELM has the same virtues of ELM for SLFN with a single output. Please note, when C is generally set to be a large constant, we can computeβˆin this learning machine by using Eq. (14) or using Eq. (15) instead of Eq. (10).Centered-ELM for SLFN with a single output.Given the training set D={(xj, tj)|xj∈Rn, tj∈R, j=1, 2, ......, N}, the kernel activation function g(x, θ) forN˜hidden nodes of SLFN with a single output, a large constant C used in CHFSRStep1: Randomly assign the parameter vectorsθ1,θ2,...,θN˜in the last hidden layerStep2: Compute the training sethStep3: Compute the output weight vectorβˆTof SLFN using Eq. (14) or (15)Just like the above HFSR, we can easily extend the above Centered-ELM to MLFN by constructinghi,h¯andhx(for a testing samplex) successively in the forward layer-by-layer way from the input layer among all the hidden layers of MLFN. This obviously does not violate the fact that such a Centered-ELM for MLFN with a single output still is a special kernel method.What is more, as done in Section 2, if we defineβ=β1Tβ2T⋮βmTβi=[βi1,βi2,......,βiN˜]T,T=t1Tt2T⋮tmT,ti=[ti1,ti2,......,tiN]T,i=1,2,…,m, we can easily know that HFSR and Centered-ELM still hold for both SLFN and MLFN with m multiple outputs. In the sequel, we assume that HFSR and Centered-ELM hold for both SLFN and MLFN with a single or multiple outputs.As pointed out as above, both HFSR and Centered-ELM can be extended to MLFN. In order to give a unified framework of the above learning methods for both SLFN and MLFN, we summarize them as the least learning machine (LLM) in the sense of the fact that we only require adjusting the parameters in the last hidden layer rather than all the hidden layers of feedforward neural networks for both SLFN and MLFN.LLM for both SLFN and MLFN.Given the training set D={(xj, tj)|xj∈Rn, tj∈R, j=1, 2, ......, N}, the kernel activation functionsgl(x,θ1),gl(x,θ2),…,gl(x,θN˜)as the transformed dataset from the (l−1)th hidden layer. When l=1, this transformed dataset is the training set D in the input layer, l=1, 2, …, M−1 in the lth hidden layer of MLFN, and a large constant C used in CHFSR. Suppose MLFN has taken M hidden layers, i.e., MLFN takes the n−M−1 architecture. When M=1, MLFN degenerates into SLFN.Step1: (skip this step for SLFN): for l=1, 2, …, M−1, doBeginStep1.1: Randomly assign all the parameters ingl(x,θ1),gl(x,θ2),…,gl(x,θN˜)in the lth hidden layer.Step1.2: Present the transformed dataset by usinggl(x,θ1),gl(x,θ2),…,gl(x,θN˜)in the lth hidden layer to the (l+1)th hidden layer.EndStep2: Randomly assign the parameter vectorsθ1,θ2,…,θN˜in the Mth hidden layerStep3: Compute the training sethStep4: Compute the output weight vectorβˆTby using Eq. (10) or (13) for HFSRor by using Eq. (14) or Eq. (15) for Centered-ELMWe can refer to the above learning machine LLM as a kind of kernel methods, however, as pointed out in the above, the rigorous Mercer's condition for kernel functions is not required in this learning machine.Let us see the benefits from the learning machine LLM for both SLFN and MLFN.(1)Since all the parameters in the kernel activation functions among all the hidden layers can be randomly assigned and all the examplars in the kernel activation functions among all the hidden layers can be randomly selected, and without any iterative calculation input data in every hidden layer are transformed into the next hidden layer only once in a forward layer-by-layer way, LLM for MLFN has the advantages of both easy implementation and very fast learning capability, compared with BP for SLFN and MLFN where all parameters in the network need to be iteratively adjusted in a backward gradient-descent way such that BP generally converges very slowly and even sometimes converges to local minima.When MLFN with a single output is trained using LLM rather than BP, the training performance including the approximation accuracy and running speed of MLFN actually depends on C and the output weights in CHFSR. In particular, for the given network architecture, the output weights of the last hidden layer play the most crucial role in the approximation accuracy of LLM for SLFN and MLFN. Since all the parameters between all the hidden nodes can be randomly assigned, parameters between different hidden layers are independent with each other! This fact indeed violates the popular thinking that just like BP, all the parameters in MLFN need to be tuned and thus the dependence between different hidden layers of parameters exists, which will heavily determine the training performance of the network.In fact, LLM views the behavior of a feedforward neural network between the last hidden layer and the input layer as the successive encoding procedure for the input data in a difficult-to-understand way. When we understand the training behavior of MLFN from this new LLM perspective, to large extent, LLM can also help us answer why MLFN behaves like a black box.In this section, several experiments were arranged to study the performance of LLM for regression tasks. Two implementation of LLM were tested in our experiments. The first one was implemented with Centered-ELM and the second one was implemented with HFSR. We compared the performance of two LLM implementations with that of BP. In the experiments, LLM was implemented with MATLAB. BP was implemented by calling the MATLAB function NEWFF() and the training function BTF was set as “TRAINLM”, which utilized the Levenberg–Maquardt (LM) method to optimize the cost function. In other words, the adopted BP algorithm was based on MLP+LM. We denote it as BP using MLP+LM.Another problem for multiple-hidden layer feedforward neural networks is the determination of the number of hidden nodes. In our work, we determined the optimal number of hidden nodes for each layer by the best training accuracy after grid searching within a range.Without loss of generality, we consider two-hidden-layer MLFNs where RBF functiong1(x)=exp(−(||x−xi||2/σ2)),i=1,2,…,N˜, is adopted in the first hidden layer,N˜is the number of the hidden nodes, and sigmoid function g2(x)=(1/1+e−x)) is adopted in the second hidden layer.The nonlinear regression modeling problems often involve high dimensional datasets. Therefore, in this regression experiment, the artificial dataset Friedman [35] was used. The input attributes (x1, x2…, x10) are generated independently, and each of which is uniformly distributed over [0,1]. The target function is defined by(17)y=10sin(π⋅x1x2)+20(x3−0.5)2+10x4+5x5+σ(0,1)where σ(0,1) is the noise term which is normally distributed with zero mean and unit variance. Note that only x1, x2…, x5 are used in the above function and x6, x7, …, x10 are noisy irrelevant input attributes. Table 1shows the experimental settings of this dataset.Ten trials have been conducted for each algorithms. The average experimental results are summarized in Table 2. We can easily see from Table 2 that LLM using HFSR or centered-ELM for the two-hidden-layer MLFN have comparable performance with BP and are much faster than BP for the regression tasks.In order to show the practical feasibility of the proposed algorithms, we applied three algorithms to the regression tasks on three benchmarking datasets which can be downloaded from http://www.liaad.up.pt/∼ltorgo/Regression/DataSets.html, as shown in Table 3. Ten trials have been conducted for all three algorithms on the randomly sampling training sets with the given number of training samples. The average experimental results are summarized in Table 4, in which the best reults are in bold. Obviously, the same conclusion as above in Section 4.1 holds for these regression datasets. This experiment indicates that the proposed algorithms can be successfully applied in the fields such as finance, electronics and manufacture.To further evaluate the regression performance of the proposed algorithm, an experiment was conducted by applying the comparison algorithms to model a biochemical process. The adopted dataset originates from the glutamic-acid fermentation process, which is a multiple-input–multiple-output dataset. The input variables of the dataset include fermentation time k, glucose concentration S(k), thalli concentration X(k), glutamic acid concentration P(k), stirring speed R(k), and ventilation Q(k) at time k. The output variables are glucose concentration S(k+1), thalli concentration X(k+1), and glutamic-acid concentration P(k+1).The performance of the proposed algorithms as well as its rivals is tabulated in Table 5, from which LLM is found to be effective for biochemical process modeling. When compared with BP, the training speed of both LLM using centered ELM and LLM using HFSR is competitive and their obvious advantage in terms of running time for training is also demonstrated.

@&#CONCLUSIONS@&#
Due to the fact that ELM is much faster than BP for SLFN, ELM has been attracting more and more attentions recent years. However, ELM in its present form cannot be directly applied to multiple hidden layer feedforward neural networks. In this paper, by proposing the hidden-feature-space ridge regression HFSR and centered ridge regression Centered-ELM for both SLFN and MLFN, we build the link between extreme learning machine (ELM) and them for SLFN, which can help us understand the essence of ELM. As the special kernel methods, the advantages of both HFSR and Centered-ELM exist in that rigorous Mercer's condition for kernel functions is not required and that we can naturally use them to propagate the prominent advantages of ELM into MLFN. Compared with ELM, except for randomly assigned weights, HFSR still exploits one more randomness, i.e., randomly selected examplars from the training set for kernel activation functions. Through constructing the transformed datasets from the training dataset in a forward layer-by-layer way, we extend HFSR and Centered-ELM to MLFN. Accordingly, as the unified framework for HFSR and Centered-ELM, the least learning machine (LLM) is proposed for MLFN with a single or multiple outputs. LLM keeps the same prominent virtues of ELM for SLFN, i.e., only the parameters in the last hidden layer require being adjusted while all the parameters in other hidden layers can be randomly assigned, and LLM is much faster than BP in training the sample sets. We experimentally demonstrate that the effectiveness of the proposed learning machine LLM for regression tasks.In essence, LLM gives a new forward encoding learning way which is completely different from a backward gradient-descent learning way in BP. It views the behavior of MLFN between the last hidden layer and the input layer as the successive encoding procedure for the input data in a difficult-to-understand way, which can also help us answer why MLFN behaves like a black box from an alternative perspective.There still exists a big room for us to explore LLM in near future. For example, a challenging research topic for LLM is how to determine appropriate number of the hidden nodes in the last hidden layer and even in other hidden layers of MLFN. When LLM is taken to train MLFN, no any dependence of parameters between different hidden layers exists, which arises another challenging issue: whether the number of the parameters in the output layer must be equivalent to the number of the hidden nodes in the last hidden layer, or, can we determine appropriate number of the parameters in the output layer which is less than the randomly assigned comparatively big number of the hidden nodes in the last hidden layer.