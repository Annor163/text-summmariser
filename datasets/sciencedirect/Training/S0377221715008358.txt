@&#MAIN-TITLE@&#
Obtaining cell counts for contingency tables from rounded conditional frequencies

@&#HIGHLIGHTS@&#
We determine all possible cell counts for a contingency table of rounded conditional frequencies.Redundancies among cell calculations are exploited in a decomposition method.The method allows exploration of risk trade-offs for table configuration and rounding precision.

@&#KEYPHRASES@&#
OR in government,Integer linear programming,Statistical disclosure control,Tabular data,Fast Fourier transform,

@&#ABSTRACT@&#
We present an integer linear programming formulation and solution procedure for determining the tightest bounds on cell counts in a multi-way contingency table, given knowledge of a corresponding derived two-way table of rounded conditional probabilities and the sample size. The problem has application in statistical disclosure limitation, which is concerned with releasing useful data to the public and researchers while also preserving privacy and confidentiality. Previous work on this problem invoked the simplifying assumption that the conditionals were released as fractions in lowest terms, rather than the more realistic and complicated setting of rounded decimal values that is treated here. The proposed procedure finds all possible counts for each cell and runs fast enough to handle moderately sized tables.

@&#INTRODUCTION@&#
Statistical disclosure control is concerned with privacy guarantees when releasing data that might otherwise identify, or reveal information about, specific individuals or organizations. Releasing data in summarized form greatly reduces disclosure risk, but does not eliminate the risk altogether. In particular, a contingency table of frequency counts potentially reveals information if the table includes any cells having small or zero counts. On the other hand, a table is less useful for statistical inference if it overly aggregates information. This trade-off generally leads to the release of a modified or aggregated version of the table that carries somewhat less information than the original. In deciding which particular form to release, a crucial step is to determine the tightest possible bounds that can be inferred about the individual cell counts in the original table. Such bounds can be used to assess both the disclosure risk and the statistical utility of the released information. The present paper addresses the open question of how to calculate these bounds when the contingency table is released in the form of a two-way table of rounded conditional frequencies.A two-way contingency table is a two-dimensional array of whole numbers, and the row and column sums of such an array are referred to as marginal counts. The conditional row probabilities are given by dividing each entry in the array by the marginal sum for its row. As an example, the contingency tables shown in Tables 1(a) and 1(b) have the same sample size (sum of all entries) and also lead to the same conditional row probabilities, which are shown in Table 1(c). Wright and Smucker (2014) showed that Tables 1(a) and 1(b) are the only contingency tables of sample size 48 having the row conditionals shown in Table 1(c). Because Tables 1(a) and 1(b) have the same second row, knowledge of Table 1(c) and the sample size therefore exposes the actual counts in the second row. The question is whether those counts could be obscured somewhat by converting the exact fractions in Table 1(c) into rounded decimal expressions. In Section 3 we show that two-digit rounding does obscure the counts in this example whereas three-digit rounding does not.Various approaches are used to limit the disclosure risk of contingency tables, such as choosing a more heavily aggregated summary (e.g., marginal counts only), suppressing some cells altogether, or perturbing cell counts slightly (Hundepool, Domingo-Ferrer, Franconi, Giessing, Nordholt, Spicer, & de Wolf, 2012). These often rely on operations research techniques to limit risk and evaluate risk-utility trade-offs (Almeida & Carvalho, 2005; Castro, 2006; 2011; 2012; Cox, 1995; Cox & Ernst, 1982; Fischetti & Salazar-González, 1999; Hernández-García & Salazar-González, 2014; Kelly, Golden, & Assad, 1990; Kelly, Golden, Assad, & Baker, 1990; Muralidhar & Sarathy, 2006; Salazar-González, 2004; 2005; 2006; 2008). Over the past decade researchers have also explored the possibility of releasing observed conditional probabilities, which retain some statistical utility insofar as odds and ratios of odds are preserved (Slavković, 2010). To date, the work on disclosure risk for tables of conditionals has focused primarily on two-way contingency tables, such as two-way rearrangements of multi-way tables in which some subset of the observed variables are treated as the conditions (i.e., predictors), some are treated as the responses, and perhaps others are omitted altogether (by aggregating over their values) (Fienberg & Slavković, 2005; Slavković, 2004; Smucker & Slavković, 2008; Smucker, Slavković, & Zhu, 2012).Our problem description and methods are framed in terms of two-way contingency tables, but the same approach can also be used to obtain cell bounds on multi-way contingency tables that have been reshaped as two-way tables by designating some variables as predictors, other variables as responses, and perhaps omitting some variables altogether. We refer the reader to Smucker et al. (2012), Wright and Smucker (2013), Wright and Smucker (2014) for more information on how and why such reshaping might be performed.Here is a concrete example of a multi-way table using public data that nevertheless provides a nice illustration of how several variables might be grouped into predictor, response, or omitted variables. We consider an 8-way table (N=48,842) from the 1993 U.S. Current Population Survey (CPS), a monthly survey that collects demographic and other data of interest. Table 2gives information about the qualitative variables measured in this study. Imagine that data such as these comprised, say, all the adults in a given small city. There might be some concern in releasing it in full detail. In particular, small cell counts (including zeros) could potentially be used to identify the salary range of specific individuals. Other variables in this data set might also be considered sensitive information for some people, depending on their personal circumstances. Among the eight variables here, we would almost always consider age, race and sex as natural choices for possible predictors, whereas the other variables (except salary) could be considered either a predictor or response depending on the question at hand. Likewise, any of the variables might conceivably be omitted (implying an aggregation of counts). Moreover, levels within a variable might be combined into a smaller number of levels, such as reducing the three age ranges (for variable X1 of Table 2) to two age ranges. In any case, the value of row conditionals now becomes clearer: over all people in the study who satisfy a given set of demographic predictors, the row conditionals tell us what proportion have a given (say) education level and salary level. The actual counts could well be of less interest than the proportions to decision-makers and the general public. But privacy considerations and statistical inference would require recovering information about those underlying counts from the stated proportions.The mathematical structure of the corresponding cell-bounding problem was recently elucidated for the simpler context in which conditional probabilities are presented as unrounded fractions Slavković, Zhu, and Petrović, Wright and Smucker (2013). Under that idealization, it was demonstrated that the (upper or lower) bounding problem for each cell can be reduced to an integer linear knapsack problem. Wright and Smucker (2014) subsequently showed that cell bounds and possible counts for an entire two-way table of unrounded conditionals can be obtained quickly with an algorithm that shares intermediate solution information among large groups of cells in the table. They used that capability to explore disclosure risk for various rearrangements of the data represented in Table 2 under the assumption that a data snooper somehow determined the true unrounded fractions for each conditional probability.The present article examines the two-way cell-bounding problem in the more complicated and realistic setting of rounded conditionals. Several of the works cited above (especially Smucker et al., 2012, Slavković, Zhu, & Petrović) provide light commentary, heuristics or preliminary results on issues relating to cell-bounding from rounded conditionals. So far there have been no substantive attempts to provide a general procedure for identifying tightest cell bounds. Here we formulate the bounding problem for each cell as a pair of integer linear optimization problems and show how these can be decomposed into two types of simpler subproblems. One type of subproblem is solvable in closed form and the other can be addressed by adapting some of the ideas presented in Wright and Smucker (2014). The decomposition provides useful knowledge about the cell-bounding problem structure and identifies all possible cell counts rather than merely the cell bounds (which is the most that could be obtained with integer programming methods). We also provide simple examples showing that: (a) rounded conditionals can lead to considerably wider tightest bounds than the corresponding unrounded fractions, and (b) a single-digit change in the rounding precision sometimes means the difference between revealing many cell counts and revealing none.A full discussion of how to evaluate statistical utility and disclosure risk goes beyond the scope of this paper, but the basic issues are as follows. On the one hand, disclosure risk is minimized by avoiding the revelation of small counts (magnitude 3 or smaller, say) in the contingency table, and this in turn suggests that very narrow cell bounds are undesirable. This is easily understood, so our discussion of the examples in this paper tends to focus on disclosure. On the other hand, statistical utility refers to the ability to perform estimation or hypothesis testing and obtain results only slightly weaker than what one could obtain by applying standard statistical methods to the actual table of counts. Intuitively, such utility is maximized by having a more accurate representation of the underlying table, but the picture is more complicated than that. Salazar-González (2004) (also Salazar-González, 2005) has presented a fine overview of statistical disclosure limitation and its trade-offs from an operations research perspective.Our method finds bounds on row marginals along with information about individual cell counts, with the latter then implying bounds on the remaining marginals. Performing inference on the basis of such bounds is an area of ongoing research, and a few methods have been proposed for the purpose (see Slavković, Zhu, and Petrović, Dobra, 2012 and references cited therein). Given that the implied bounds on marginals could be quite wide when disclosure risk is avoided, we are led to ask how badly data utility might be compromised. Slavković and Lee (2010) have provided some encouraging results on this last point. They proposed a method for disclosure limitation based on the following idea: round the actual row conditional probabilities to nearby decimal-place approximations, rewrite those approximate conditionals as fractions themselves, and then randomly select a table of counts from the collection of contingency tables for which the new fractions are the conditional probabilities. Releasing such a “synthetic” contingency table is similar to, but slightly different than, releasing rounded conditionals themselves. They examined the utility-risk trade-off between releasing such a table and the traditional approach of releasing a table that preserves marginals (rather than conditionals) by swapping variables in the underlying microdata; this latter approach was used with data tabulations from the 2000 U.S. Census. For the particular example they considered (see Section 3 of the current paper) they found that statistical inference performed directly on the released table was not compromised when the table preserved conditionals, whereas it was seriously affected when marginals were preserved instead. At present, there is still a lot of scope for determining efficient and effective methods for inference based on conditionals, and for evaluating risk versus utility.In the next section, we define the mathematical problem, formulate it as an integer programming problem, and then describe a decomposition method for solving it. In Section 3, we give some examples illustrating features that distinguish the solutions obtained from rounded versus unrounded conditionals. The fourth section presents some computational results illustrating actual running time versus the theoretical value from Section 2, along with a preliminary comparison to the naive use of a commercial solver. The final section provides some additional discussion of the results within the larger context of statistical disclosure limitation.Consider a two-way contingency table having I rows and J columns, for which the actual (observed) count in cell ij is denoted oij. The data provider plans to release only the sample sizeN=∑i,joijand the conditional probabilities oij/∑koik, which give the relative proportions within each column across the row i. However, the conditionals will first be rounded or perturbed to nearby numbers pj|iexpressible with a fixed number of decimal places:|pj|i−oij/∑koik|≤ϵfor all i, j. Assuming a value has been specified for ϵ (either provided with, or inferred from, the released table), the problem we address is to identify the tightest bounds for the nonnegative integers nijsatisfying∑i,jnij=Nand|pj|i−nij/∑knik|≤ϵ. In fact, it is often useful to know precisely which values each cell may take; the method described below can do that as well. With a small adjustment, we can also address the related situation in which the rounding or perturbation procedure guarantees a strict inequality of the form|pj|i−oij/∑koik|<ϵinstead of the weak inequality indicated above.To formulate the cell-bounding problem, we let Nidenote the possible row sum (i.e., denominator) for row i. For greater generality, we also allow for the possibility that prior bounds might be known for some row sums or cell counts. Our task is to solve the following collection of 2IJ integer linear optimization problems, one pair for each cellı¯j¯:(1)min/maxnı¯j¯overallintegersnij,Nisubjectto(2)∑iNi=N,(3)Li≤Ni≤Ui,foralli,(4)∑jnij=Ni,foralli,(5)(pj|i−ϵ)Ni≤nij≤(pj|i+ϵ)Ni,foralli,j,(6)lij≤nij≤uij,foralli,j.The inequality (5) expresses the constraint on the rounding error in cell ij, whereas (3) and (6) are a prioribounds on row sums and cell counts. The bounds are assumed to be integer-valued with Ui≥ Li> 0 and uij≥ lij≥ 0.We now comment briefly on the computational complexity of these problems. Determining integer feasibility of (2)–(6) is NP-hard for arbitrarily chosen positive integers Li, Ui, lij, uij, rational numbers pj|i> 0 (with or without1=∑jpj|i) and rational ϵ ≥ 0. To see this, it suffices to show that (2)–(6) capture as a special case the following well-known NP-complete decision problem (Nemhauser & Wolsey, 1988): given positive integersa1,…,aIand b, determine if there exist valuesx1,…,xI∈{0,1}satisfying∑iaixi=b. For an instance of this knapsack-equality decision problem, we can define data for an instance of (2)–(6) byJ=2,N=b+∑iai,Li=ai,Ui=2ai,ϵ=1/(4maxi{ai2})lij={1,ifj=1,ai−1,ifj=2,uij={2,ifj=1,2(ai−1),ifj=2,pj|i={1/ai,ifj=1,1−1/ai,ifj=2.The equivalence of the resulting instance of (2)–(6) with the standard decision problem above is then given by the correspondencexi=ni1−1,where we also takeNi=aini1andni2=Ni−ni1. These last two equations must be satisfied by all integer solutions of (2)–(6) because of the particular choice of ϵ > 0; in other words, we might as well takeϵ=0. Conversely, as noted previously (Smucker et al., 2012), Slavković, Zhu, and Petrović, the special case of unrounded conditionals (i.e., any instance withϵ=0) admits a direct reformulation as a collection of equality-constrained knapsack problems.To summarize, we expect the problems (1)–(6) to be a good deal more challenging than linear programming and at least as hard to solve in practice as knapsack problems. On the other hand, omitting the constraint (5) leaves a totally unimodular system that could be handled with linear programming or network optimization. In fact, it can be addressed in closed form as a special case of Theorem 2.1 below.In the next two subsections we describe a procedure for solving these optimization problems.We propose a solution framework based on the observation that each instance (1)–(6) can be decomposed into two families of subproblems:•Given a value of Ni, what cell counts nij(if any) within row i yield Nias a row sum?Given a set of Ni-values for each i, which of those Nican be included in a sum equal to N?The solution procedure begins by addressing the if-any aspect of the first subproblem to identify a set of Ni-values, which is then considered in the context of the second subproblem. For each Niadmitted by the second subproblem, we follow up by solving the first subproblem to obtain the actual cell counts. In other words, the framework is roughly analogous to eliminating all nijin favor of Ni, then solving a system that involves only the Ni-values, and finally substituting back to recover nij.The key to this approach is that the first subproblem above admits closed-form solutions in both directions, namely, when restricting the choice of Niduring the elimination of nijand again when recovering nijfrom each such Ni. The former amounts to some easily derived inequalities relating Nito nij, whereas the latter relies on the ability to guarantee that those inequalities are tight and that nijcan take all consecutive integer values within the bounds implied by a given choice of Ni. The rest of this subsection presents these closed forms, proves their validity, and indicates their formal role in the framework outlined above. Examples involving small contingency tables are sketched in Section 3.The decomposition makes use of tight explicit bounds on nijimposed by Nithrough the constraints (4)–(6). To calculate those bounds, we first rewrite the conditions (5 and 6) equivalently in the form(7)lij(Ni)≤nij≤uij(Ni),foralli,j,where the left and right sides are defined aslij(Ni):=max{lij,⌈(pj|i−ϵ)Ni⌉},uij(Ni):=min{uij,⌊(pj|i+ϵ)Ni⌋}.Here we use the notation ⌊z⌋ for the greatest integer that is no larger than z, and ⌈z⌉ for the least integer that is no smaller than z. Taking into account the possible gap between the left and right sides of (4) when the nij-values are at their bounds (6), we definenij−(Ni):=max{lij(Ni),Ni−∑j′≠juij′(Ni)},nij+(Ni):=min{uij(Ni),Ni−∑j′≠jlij′(Ni)}and then consider the inequalities(8)nij−(Ni)≤nij≤nij+(Ni).With this notation, the decomposition is described by the following result. It shows how to select row sums Nifor consideration and how to recover cell counts nijfrom Ni.Theorem 2.1Consider a positive integer Ni for a fixed row i. There exist integers nij satisfying(4)–(6)if and only if Ni satisfies(9)lij(Ni)≤uij(Ni),forallj,and(10)∑jlij(Ni)≤Ni≤∑juij(Ni).Moreover, for each such Ni, the attainable integer values of nij in(4)–(6)are those satisfying(8).First suppose there are integers nijsatisfying (4)–(6), so that (7) and therefore (9) both hold. Combining (4) with the left inequality in (7) gives usNi=∑jnij≥∑jlij(Ni),and hence we obtain the left inequality of (10). At the same time, (4) and the right inequality in (7) yieldnij=Ni−∑j′≠jnij′≥Ni−∑j′≠juij′,so the left inequality of (8) holds. In a similar fashion, we can verify the right inequalities of (10) and (8). This proves the claimed necessity of (8)–(10).Conversely, suppose that Nisatisfies (9) and (10). We need to express Nias a sum of integers nijsatisfying (7). Also, the final statement of the theorem requires showing that each integer value of nijindicated in (8) can be attained. Without loss of generality (by permuting column indices as needed), we demonstrate such attainment for the final columnj=J. To this end, consider a value niJbetweenniJ−andniJ+. Definingmk:=∑j≤kuij(Ni)+∑k<j<Jlij(Ni)fork=0,1,…,J−1,we see thatmJ−1≥Ni−niJand that mkis nondecreasing in k. For the smallest k such thatmk≥Ni−niJ,it is readily verified that the choicenij={uij(Ni),if0<j<k,lij(Ni),ifk<j<J,Ni−niJ−mk+uik(Ni),ifj=k>0satisfies both (4) and (7).□The decomposition presented in Theorem 2.1 validates the three-phase solution process outlined earlier, which can now be described more completely.For each row i, the conditions (9 and 10) are easily checked for each value ofNi∈{1,…,N},as are the simple bounds (3). In this way we identify the collection, denotedNi,of row sums that accommodate the rounding constraints and bounds for all cells in row i. This eliminates the cell counts nijmomentarily and constitutes the first phase of the solution procedure.In the second phase, we extract those elementsNi∈Nithat can be combined with admissible sums from other rows to satisfy the remaining constraint (2). More precisely, we calculate the setNifeas:=Ni∩(N−∑i′≠iNi′),where the sum of sets is understood in the Minkowski sense (i.e., all numbers expressible as sums∑i′≠iνi′withνi′∈Ni′). As described in Section 2.3, this calculation be accomplished quickly for fairly large two-way contingency tables.The third and final phase of the solution procedure consists of substituting eachNi∈Nifeasinto the left and right sides of (8) to find the attainable values of nij. Clearly, the extremes sought by the objective in (1)–(6) lie among the valuesnij−(Ni)andnij+(Ni)for some choices of Ni. It should be noted that the functionsnij−(·)andnij+(·)are not necessarily monotonic, so cell-specific optima might not correspond to extreme elements ofNifeas.We also note that Niand ϵ together determine how many values nijlie in the range (8). It can be shown that⌊pj|iNi⌋≤nij−(Ni)≤nij+(Ni)≤⌈pj|iNi⌉whenever Ni< 1/ϵ, so there are at most two integer values nijsatisfying (8) when Niis sufficiently small. For Ni≥ 1/ϵ, the right and left sides in (8) differ by no more than 2⌈ϵNi⌉.The first and third phases above can be carried out in closed form with O(J) comparisons and arithmetic operations for each Niconsidered, and therefore they require O(IJN) such operations overall. As will be seen in the next subsection, the second phase requires at most O(INlog Ilog N) and so that component tends to dominate the running time. Section 4 presents the results of some computational tests to see whether this time estimate might be realized in practice. Note that all calculations can be carried out using integer arithmetic, if desired.We now indicate how to adapt this method to address strict rounding inequalities of the form|pj|i−oij/∑koik|<ϵ,which are sometimes used in consistent rounding instead of the corresponding weak inequalities. As an example, for 3-digit rounding withϵ=0.001we might allow any number strictly between 0.134 and 0.135 to be rounded to either of those two values, but 0.1340 could not be converted to 0.135 (as would be allowed by the weak inequality). In the problem formulation, this clearly amounts to replacing the weak inequalities in (5) with strict inequalities. Although this potentially makes the integer programming representation more complicated, it is handled easily by the decomposition presented above. The only modification needed is to replace the definitions of lij(Ni) and uij(Ni) with these:lij(Ni):=max{lij,⌊(pj|i−ϵ)Ni⌋+1},uij(Ni):=min{uij,⌈(pj|i+ϵ)Ni⌉−1}.All the remaining details in this section apply without further change.We now look more closely at the second phase of the solution procedure presented in Section 2.2. Given setsN1,…,NIof positive integers, we must calculateNifeas=Ni∩(N−∑i′≠iNi′)for each i. First we note that the calculation ofNifeasrequires only those elements of∑i′≠iNi′that do not exceed the sample size N. Hence, when adding two sets of the formNi′it suffices to use a pair of nested loops over subsets of{0,…,N}. Doing this for each row i and eachi′≠iwould amount to O(I2N2) operations overall, which is reasonable if I and N are not too large. When I or N is large (in the thousands, say), it is worthwhile to seek greater efficiency.For eachNi∈Niwe need to determine whetherN−Nican be expressed as a sum∑i′≠iNi′for some choice ofNi′∈Ni′. If each setNiwere a uniformly spaced set of the form{r0+λr∣λ∈[λ−,λ+]∩Z},then this subproblem would constitute an equality-constrained, bounded knapsack problem. Indeed, the cell-bounding problem for unrounded conditionals leads precisely to such a situation, in which case each setNisimply consists of consecutive multiples of a single number ri(the lowest common denominator for row i). Wright and Smucker (2014) therefore proposed a refinement of a standard dynamic programming algorithm for knapsack problems. A dynamic programming perspective is particularly attractive in this context because we must address multiple values of Niwithin each row i and we also hope to exploit redundancy in the calculations used for distinct rows.Unfortunately,Niin the present setting does not have such a nice form, but in general admits arbitrarily small or large integer gaps between its elements. We note, however, that one special case can be treated by a direct extension of the approach cited above: in the absence of a priori finite upper bounds on the cells in row i, we can treatNias an additive set by momentarily ignoring the bound imposed by N. In this case, we can cheaply identify the minimal generating set (with respect to additivity) ofNiand treat each generator as a separate term in the knapsack constraint. We implemented such an approach and found that its performance was similar to the method described next. However, the method below has the significant advantage of working just as well when a priori upper cell bounds are imposed or even when an explicit list of possible cell counts is specified beforehand.To replace the dynamic programming approach, note that it suffices to calculate a sum of sets∑iNiinvolving summands of cardinality on the order of N, but restricting the result so that we exclude elements exceeding N. Conceptually, we start with a setS={0},then sequentially updateSby including sums of elements of a setNiwith elements ofS,truncating the result to remove elements larger than N as they occur. For this purpose, we storeSandNias indicator vectors with entries sk∈ {0, 1} fork=0,1,…,N. The convolution of these indicators is the vector whose entry at indexk=0,1,…,2Ngives the multiplicity of the pairwise sums fromS+Nithat result in a value of k. In particular, the support of this convolution is the desired sumS+Ni. It is well known that we can compute the convolution efficiently as follows (Loan, 1992): pad both indicators with N additional zeros for indices k > N, calculate the discrete Fourier transform of the padded indicators, take the entry-wise product of the transformed vectors, and then calculate the inverse transform of that product to get the desired convolution. With fast Fourier transforms, the convolution requires only O(Nlog N) operations. Once the convolution has been obtained, its nonzero (rounded) entries for indicesk=0,…,Nconstitute the support of the updatedS.Next, note that most of the work needed to calculate the summation∑i′≠iNi′for a given value of i is repeated for many other values of i, because only one value of i′ is omitted from each summation. We exploit this redundancy by separately building the partial accumulations∑i′<iNi′and∑i′>iNi′. Specifically, we compute and store the values of (say) the former for all i in a single forward pass and then backtrack with a simple update of a single copy of the latter. If the required storage size(I+1)Nis too great to fit into computer memory then we employ the following divide-and-conquer scheme, which is similar to that presented by Wright and Smucker (2014) for their dynamic programming approach.In this algorithm, the collection of row indices{1,…,I}is iteratively bisected to create a binary tree of nested partitions. The values of Ldand Udgive the ranges of row indices considered in each iteration of the while-loop. The subscript d denotes the depth within the tree and δ ∈ { ± 1} is the direction of motion (deeper or shallower) as the tree is traversed. A sumSof setsNiover some subset of rows is calculated at each stage of the bisection and a list of selected such sumsSdis stored for subsequent reuse. The core task of updatingSin part (c) of step 2 is performed O(Ilog I) times; the contribution toSof part (c) in step 3 can kept separate until needed and therefore only performed I times. Using this arrangement of the work, the entire second phase of the decomposition needs O((Ilog I)(Nlog N)) operations.Here are few more comments comparing the above with the approach taken by Wright and Smucker (2014) for calculating cell bounds based on unrounded conditionals. Exploiting the particularly simple form ofNiin that setting, they presented a dynamic programming algorithm in which the update ofSuses O(N) operations instead of the O(Nlog N) needed by the update described here. Moreover, the first and third phases listed in Section 2.2 of the present paper can be replaced by almost trivial calculations in the unrounded case. Therefore, it generally takes at least one order of magnitude longer (sometimes much longer) to solve the cell-bounding problem for rounded conditionals than for unrounded conditionals.We end this section with a note about an important special case, namely, that in which some rowı¯has a lone nonzero entry (necessarily unity for the table of conditionals) and no a priori bounds are associated with that row. This situation was also singled out by Wright and Smucker (2013)2014). For such a special rowı¯,the setNı¯consists of all integers from 1 through N. This considerably simplifies the calculation ofNifeasfor alli≠ı¯,because it implies that∑i′≠iNi′consists of all integers from∑i′≠iminNi′through∑i′≠imaxNi′. In other words,Nifeasis simply the restriction ofNı¯to those elements ranging fromN−∑i′≠imaxNi′throughN−∑i′≠iminNi′. Consequently, we see that the convolution-based calculation of∑i′≠iNi′is needed only wheni=ı¯,and that too can be avoided if there is a second such row. A further ramification is that tables with one or more such rows tend to have very wide bounds, which is good news for confidentiality but perhaps bad news for statistical utility.This section presents a few examples to highlight the difference in information provided by rounded versus unrounded conditionals. Throughout the section, we only consider tables in which the rounding or perturbation is consistent, meaning that the decimal values of the approximate conditionals sum to unity within each row. However, consistent rounding is not required for the decomposition procedure introduced in Section 2.2.First we consider the artificial Table 1 introduced in Section 1, for which some rounded conditionals are shown in Table 3. These correspond to the standard nearest-value rounding, which leads to the unique choice for 3-digit rounding shown in Table 3(b) and two possibilities for 2-digit rounding, as shown by the second row in Tables 3(c) and 3(d).When determining possible cell counts based on the three-digit values in Table 3(b), the first phase of the method of Section 2.2 identifies eachNias consisting of all multiples (no larger thanN=48) of the following values: 7 for the first row, 8 for the second, and 5 and 9 for the third and fourth rows, respectively. Note that these are also the corresponding denominators shown in Table 1(c). Next, the second phase restricts the possibilities to these row sums:N1feas={7,21},N2feas={8},N3feas={10,15},N4feas={9,18}.The third phase then determines that Tables 1(a)–(b) are the only possibilities, and so the 3-digit rounded conditionals yield the same disclosure risk as the unrounded conditionals. In particular, the true counts in the second row are disclosed by this 3-digit rounding.Now suppose the same calculations are performed under the slightly looser proximity condition havingϵ=0.001,for which there are 36 possible tables of 3-digit conditionals corresponding to Table 3(a). It turns out that all 36 lead to exactly the same results as above in each phase of the decomposition method. Consequently, a 3-digit release of these conditionals must include perturbations with ϵ > 0.001 if we wish to avoid exposing the small count of 3 in the second column.Taking a large step in that direction, we consider releasing either of the two-digit Tables 3(c) and 3(d). Applying the first phase of the solution procedure to these yieldsN1={7,a14,16,19,a21,23,25,26,a28,30,32,33,a35,37,38,39,40,41,a42,44,45,46,47,48},N2={8,a16,a24,27,29,a32,35,36,37,a40,43,44,45,47,a48}∪{{11,19,22,25,30,33,38,41,46},Table3(c),{13,18,21,26,31,34,39,42},Table3(d),N3={5,a10,a15,a20,22,23,a25,27,28,a30,32,33,a35,37,38,a40,41,42,43,44,a45,46,47,48},N4={9,16,a18,20,23,25,a27,29,30,32,34,a36,37,38,39,40,41,43,44,a45,46,47,48},where the numbers in bold type indicate the values obtained previously in the 3-digit case (to facilitate a comparison). The second phase of the decomposition restricts these toN1feas={7,14,16,19,a21,26}∪{{23},Table3(c),∅,Table3(d),N2feas={8,16,27}∪{{11,22},Table3(c),{13},Table3(d),N3feas={5,a10,a15},N4feas={9,16,a18,20,23}∪{{25},Table3(c),∅,Table3(d).The corresponding lists of numerators obtained in the third phase are shown in Table 4. The results of all three phases are unchanged if we process Table 3(c and d) usingϵ=0.01instead of 0.005.The next example illustrates an even wider gap between 2-digit and 3-digit roundings, along with a few other noteworthy possibilities. Table 5, taken from Report on statistical disclosure limitation methodology (2005), was shown by Slavković (2004) to be the unique 4 × 4 contingency table of sample sizeN=135having the unrounded row conditionals given in Table 6. In other words, all cell counts are revealed by these unrounded conditionals and the many small values might pose a serious disclosure risk. Slavković and Lee (2010) used a particular one-digit rounding of Table 6 for the purpose of creating and enumerating synthetic tables of counts preserving those rounded values (as if they were the true conditionals). Their findings suggested no compromise in data utility for the use of standard statistical hypothesis tests of independence. Here we focus on how disclosure is affected by the rounding precision.Two sets of rounded conditionals are shown in Table 7(a and b), where all entries were calculated by first using nearest-value rounding (to 3 and 2 digits, respectively) and then adjusting one entry in the second row so that the row sum is unity. From the 3-digit rounding in Table 7(a), the decomposition completely recovers the true cell counts. On the other hand, the two-digit rounding in Table 7(b) leads to the cell bounds shown in Table 8. Many of these cell bounds are quite wide. In the six cells where the true counts are 3 or less, all integers in the bounding intervals are possible. The remaining cells can each take at least 12 distinct values, and the upper left cell can take 31 values.Notice that each row-sum upper bound in Table 8 is less than the corresponding sum of the cell upper bounds in that row, an impossibility for bounds obtained from unrounded conditionals. This suggests a lack of monotonicity in how the numerators within a row are matched to the feasible denominators inNifeas. In fact, it is also possible that the tightest upper (or lower) bound for an individual cell does not occur at the upper (or lower, respectively) bound for the row sum. Although this cell-wise difficulty does not affect the bounds in the present example, the underlying cell-wise non-monotonicity of numerator to denominator does occur. Fig. 1shows how the tightest lower bound on row 3, column 4 changes with respect to the row sums inN3feas. This is why the second phase of the decomposition procedure must return the specific allowable row sums rather than merely bounds on those sums.In this section, we present results of some computational tests showing the running time of the proposed decomposition. Some of the tests used real-world contingency tables from the literature. In general, the original table of counts in each case is a k-way table for which each dimension denotes a discrete random variable, usually taking a small number of values. In creating a summary table of conditional probabilities, some of the random variables are designated as predictor variables and some as response variables; the former then correspond to the rows of the summary table and the latter to its columns. Aggregation is performed over any random variables omitted from those two designations. This always yields a two-way contingency table, from which the original table can be recovered directly when there is no aggregation over variables. It is also possible to aggregate over the levels taken by the discrete variables. Wright and Smucker (2014) provide a detailed example of such table reshaping and aggregation in the context of unrounded conditionals.We constructed such two-way tables and their corresponding row conditionals from five data sets as follows.•A 3-way table (Haberman, 1978) with a 32 × 3 summary andN=1055,using dimension 3 as the response (columns) and the other two dimensions as predictors (rows).A 4-way table (Koch, Amara, Atkinson, & Stanish, 1983) with a 23 × 3 summary andN=193,using dimension 4 as the response and the other three dimensions as predictors.A 6-way table (Edwards & Havranek, 1985) with a 25 × 2 summary andN=1841,using dimension 6 as the response and the other five dimensions as predictors.An 8-way table from the U.S. Census Bureau’s 1993 Current Population Survey (see, for instance, Gomatam, Karr, & Sanil, 2003; Sanil, Gomatam, Karr, & Liu, 2003) with a (22 · 3) × 2 summary andN=48842. Dimension 8 was used as the response, dimensions 4, 6 and 7 as predictors, and aggregation was performed over dimensions 1, 2, 3 and 5.A 16-way table from the National Long Term Care Study (see, for instance, Erosheva, Fienberg, & Joutard, 2007) with a 23 × 2 summary andN=21574. Dimension 4 was used as the response, dimensions 1, 2, and 3 as predictors, and aggregation was performed over the other twelve dimensions.Although the six-way table includes a row with a lone nonzero entry (as discussed at the end of Section 2.2), we did not exploit that opportunity because the computational tests focused on how decomposition time varies with problem dimension and rounding precision. Likewise, larger and less-aggregated rearrangements of the 8- and 16-way tables tend to have many of those special rows and were therefore omitted here because they are handled much more efficiently by the lone-nonzero procedure.In the formulations considered here, we used the trivial a priori bounds on all cell and row counts:lij=Li=1anduij=Ui=N. Consistent roundings were employed withϵ=10−din (5) for d-digit roundings; note that standard roundings withϵ=0.5×10−dneed not give values summing to unity within rows. Table rearrangement, problem formulation, and decomposition were all performed in the MATLAB 7.14 environment. Details about the test instances are given in Table 9. The running times reported in Table 9 were obtained on a 2 × quad-core Intel 64-bit (3.40 GHz, 8GB RAM) platform with the Windows 7 Enterprise operating system. For a given table of conditionals, the decomposition requires essentially the same amount of effort regardless of the rounding precision. Recall from Section 2.2 that we expect the operation count to be dominated by O((Iln I)(Nln N)). Fig. 2shows a scatterplot of the running times for each of the contingency tables along with the curvet=[(IlnI)(NlnN)]0.6·5.25×10−5. This suggests that the bound in Section 2.2 is perhaps overly pessimistic, at least for tables with just a few dozen rows.For the purpose of a very simplistic comparison, each cell-bounding problem (1)–(6) was also solved by a separate call to CPLEX 12.5 (with default settings), a popular commercial software package for integer and linear optimization. As shown in the columns labeled “decomp.” and “all IPs” of Table 9, the decomposition outperforms this direct application of CPLEX by at least an order of magnitude in almost every case. The CPLEX times also show relatively little dependence on sample size. The number of branch-and-bound nodes per cell optimization ranged from 0 (i.e., root node only) to 10413. For some cells, even the optimizations involving only the root node occasionally made heavy use of the default CPLEX preprocessing and cut-generation to obtain an integer solution. Careful tuning of CPLEX parameters and workflow might well overcome these difficulties; see Section 5 for some possibilities. Nevertheless, calling a general-purpose solver has drawbacks such as the difficulty of obtaining all possible cell counts (in addition to the extremes) and a potential lack of portability. On the other hand, the proposed decomposition approach consists of fewer than 150 lines of MATLAB code and can therefore be ported to a variety of programming environments and easily modified to address related questions.Contingency tables lacking a priori bounds on individual cell or row counts sometimes exhibit symmetry insofar as multiple cells within a row have the same count or multiple rows have the same (sorted) list of cell counts. In such cases, we need only solve a subset of the full collection of IJ integer programming problems. Moreover, we can improve the performance of branch-and-bound in CPLEX by introducing symmetry-breaking constraints, such as forcing nij≤ nikif cells j and k (for j < k) within row i are symmetrically equivalent. The final column in Table 9, labeled “asymm.”, gives the running times when such considerations are taken into account. This shows that CPLEX can perform much better when the symmetry aspects are removed, but it is still an order of magnitude slower than the proposed decomposition on these small tables. The tables derived from the 3-way and 4-way examples have several repeated cell counts whereas those based on the 6-way example also have some repeated rows. For these instances, many more of the cell optimizations involved only the root node (alongside cuts and preprocessing) and the largest node-count was reduced to 467. On the other hand, the given arrangements of the 8-way and 16-way tables exhibit no such symmetry and therefore no improvements were possible in those cases.The contingency tables considered so far are relatively small examples, so it is worth investigating algorithm performance on a somewhat larger scale. Although the 8-way and 16-way tables above can be rearranged to create tables with hundreds or thousands of rows and much symmetry, such rearrangements generally have several rows with lone nonzero entries and can be handled trivially (as discussed at the end of Section 2.2). We therefore consider randomly generated tables instead. For each of several 2-way table dimensions I and J, we generated five instances with integer cell counts randomly selected from 1 through 25 and built tables of conditionals with 2-digit rounding. All of these instances exhibited quite a lot of symmetry, so the improvements discussed in the previous paragraph were again employed. The results are shown in Table 10, where we also indicate the number of symmetry-breaking constraints available and employed for the various instances. The gap between the proposed decomposition and CPLEX is narrow for contingency tables with two columns and many repeated rows, but it becomes substantially wider as the number of columns is increased and the symmetry moves from being mainly within rows rather than among rows. At the other extreme, in which the number of rows is far smaller than the number of columns, the gap is again less pronounced. In those instances, most of the computing time for the proposed method is spent on the closed-form column-wise calculations (7)–(10) of phases 1 and 3. Note that such instances have much greater within-row symmetry because the cell counts are capped at 25.This results in Table 10 raise further questions about how the proposed method performs when many cell counts are not small and the sample size is very large, or when the contingency table is square in shape. To test these, we generated larger instances in which only 5% of cells are given counts in [0, 4] and the remaining 95% have counts uniformly distributed in [5, 499], again with conditionals given by two-digit rounding. Sample sizes for such tables can be in the hundreds of thousands or millions. Because such instances are large and time-consuming to solve by either method, only one instance was generated for each of the configuration sizes considered. The results are shown in Table 11. As with the medium-sized tables, the gap between the proposed method and the naive use of CPLEX is often an order of magnitude or more. Again, this suggests that the fruitful application of a commercial solver might require parameter tuning.We end this section by reminding the reader that none of the computational tests reported here involve a priori bounds on cell counts or row sums. When such bounds are imposed by knowledge of another table of different shape and arrangement based on the same underlying data, the symmetry can disappear entirely. In that case, the proposed method requires (by its nature) even less time to solve the cell-bounding problems. This matter is a topic for future research, given that simultaneous consideration of both tables together is of greater interest than the effect that bounds from two tables have upon each other separately.

@&#CONCLUSIONS@&#
