@&#MAIN-TITLE@&#
Single imputation with multilayer perceptron and multiple imputation combining multilayer perceptron and k-nearest neighbours for monotone patterns

@&#HIGHLIGHTS@&#
Imputation data for monotone patterns of missing values.An estimation model of missing data based on multilayer perceptron.Combination of neural network and k-nearest neighbour-based multiple imputation.Comparison of the performance of proposed models with three classic procedures.Three classic single imputation models: mean/mode, regression and hot-deck.

@&#KEYPHRASES@&#
Hot-deck model,Multiple imputation,Mean/mode model,Multilayer perceptron,Regression model,

@&#ABSTRACT@&#
The knowledge discovery process is supported by data files information gathered from collected data sets, which often contain errors in the form of missing values. Data imputation is the activity aimed at estimating values for missing data items. This study focuses on the development of automated data imputation models, based on artificial neural networks for monotone patterns of missing values. The present work proposes a single imputation approach relying on a multilayer perceptron whose training is conducted with different learning rules, and a multiple imputation approach based on the combination of multilayer perceptron and k-nearest neighbours. Eighteen real and simulated databases were exposed to a perturbation experiment with random generation of monotone missing data pattern. An empirical test was accomplished on these data sets, including both approaches (single and multiple imputations), and three classical single imputation procedures – mean/mode imputation, regression and hot-deck – were also considered. Therefore, the experiments involved five imputation methods. The results, considering different performance measures, demonstrated that, in comparison with traditional tools, both proposals improve the automation level and data quality offering a satisfactory performance.

@&#INTRODUCTION@&#
Computer assisted personal interview (CAPI), computer assisted telephone interview (CATI) or web assisted personal interview (WAPI) are some of the most common data collection systems. However, none of them guarantees perfect data sets and a certain risk of error generation is always present. In particular, missing or inconsistent values might appear because of the lack of response or an inaccurate answer recording.Missing values can be estimated using data imputation techniques, so the gaps are filled and a complete data set is obtained. The treatment of non-response errors is a fundamental step of data cleaning, in data knowledge discovery process, to improve the information quality. Statistical agencies usually have to apply imputation techniques on the data sets resulting from their survey process, proved to be a time-consuming task.Artificial neural networks (henceforth termed ANNs) constitute flexible computing frameworks and universal approximators that can be applied to a wide range of prediction and classification problems with a high degree of accuracy. The application of different ANN approaches to data imputation has been studied previously from different points of views.Kuligowski and Barros [12] introduced a backpropagation neural network for missing data estimation by using concurrent rainfall data from neighbouring gauges. Refs. [6,25] dealt with self-organizing maps (SOM) as data imputation tools in different application areas. In other works, such as [22], the use of multiple imputations for the analysis of missing data was considered.Kalteh and Hjorth [10] imputed missing values with SOM, multilayer perceptron, multivariate nearest neighbours, the regularised expectation maximization algorithm and multiple imputation in the context of a precipitation–runoff process database. Kaya et al. [11] carried out a comparison of the neural networks, the expectation maximization algorithm and the multiple imputation techniques, while the application of genetic algorithms was proposed in [15].Subasi et al. [23] presented a new imputation method for incomplete binary data and in [21] a methodology for data imputation by ANNs was proposed and empirically compared with other data mining model, evaluating the performance of the imputation process by employing a variant of k-nearest neighbours (k-NN) method to the classification task on imputed databases.García-Laencina et al. [8] presented a multi-task learning (MTL) based approach using multilayer perceptron to impute missing values in classification problems. They combined classification and imputation in only one neural architecture, being classification the main task and imputation the secondary task.Rahman and Islam [18] proposed two techniques for the imputation of both categorical and numerical missing values, using decision trees and forests. The missing values were imputed using similarity and correlations, and they merged segments to achieve a higher quality of imputation. Azim and Aggarwal [2] described a two-stage hybrid model to fill missing values using fuzzy c-means clustering and multilayer perceptrons.Aydilek and Arslan [1] used a hybrid neural network and weighted nearest neighbours to estimate missing values. The estimation system involved an auto-associative model to predict the input data, coupled with the k-nearest neighbours to approximate the missing data.The present work focuses on a particular missing values pattern, the monotone pattern, where a set of variables is missing on the same set of records. This pattern is appropriate to build imputation models based on the aggregation of a set of predictions. A multiple imputation approach (MIMLP from now onwards) is proposed, relying its implementation on the combination of the multilayer perceptron (hereinafter MLP) and k-nearest neighbours. It is compared with an estimation model of missing values, also based on a multilayer perceptron (IMLP in what follows) and studied in [21].Tusell's work [24] has been followed, but, regarding variable types, the present research extends to qualitative variables, so that a case is imputed considering complete cases with Gower's distance, further explained below. Additionally, this study also provides insights into the selection of parameter values. To compare the efficiency of both methods, the classical models Hot-deck, mean/mode substitution and regression models have been also implemented.This paper is organised as follows. In Section 2, missing data patterns and mechanism, as well as imputation methods, are described. Section 3 introduces general aspects of artificial neural networks. Section 4 deals with experiments carried out. The automatic procedure to impute missing values based on ANNs: IMLP and MIMLP models, are described in Sections 5 and 6, respectively. The comparison with other well-known methods is presented in Section 7, where the neural network configuration is extensively studied on both MIMLP and IMLP models. The results and conclusions shown in Sections 7 and 8 reveal a clear improvement in the data set quality for this machine learning approach.In Section 2.1, the different mechanisms that generate missing values are shown, and the missing data patterns are described. In Section 2.2, the three classical single imputation procedures (mean/mode imputation, regression and hot-deck), implemented to compare the efficiency of the proposed methods, are explained.Missing data can arise by different mechanisms and with different patterns. Refs. [13,19] define three types of missing data mechanisms:•MCAR (missing completely at random). The probability that the value of a variable Xjis observed or missing for any individual does not depend on any variable:P[Xj=mis∣X1,…,Xp]=P[Xj=mis]MAR (missing at random). The probability that the value of a variable Xjis observed for any individual depends on the value of the other variables, not on the variable itself:P[Xj=mis∣X1,…,Xp]==P[Xj=mis∣X1,…,Xj−1Xj+1,…,Xp]NMAR (not missing at random). The probability that the value of a variable Xjis observed for any individual depends on the value of that variable, being this value unknown.Moreover, two types of missing data patterns, monotone and non-monotone, are usually distinguished. In the former, the lack of response is observed for the same records and variables; and in the latter, any variable for any record presents a missing value.The main objective of this work is to analyze the performance of the considered imputation techniques in presence of monotone patterns. Therefore, a wide empirical study has been conducted, introducing randomly generated monotone patterns in 18 data sets, as explained in Section 4. The experiments are based on the random selection of a set of variables which are set to missing values in a set of randomly selected records. Thus, this procedure agrees with the MCAR mechanism.A data set S is considered, where all the variables have been measured. However, it is assumed that certain S cells have been missed during the data collecting process. T is the true data set, where all the values have been completely recorded. In this way, a data imputation model is defined by a set of rules and procedures aimed at obtaining an approximation T* to T, working on the available data set S.Table 1illustrates the monotone missing data pattern. A p-sized variable X=(Xo, Xm) has been tried to collect for n records, obtaining a data set S where f variables Xoare completely observed, and p−f variables Xmare missing in a set of m records. The shaded cell in Table 1 (Z4 matrix) is missing, while Z1, Z2 and Z3 are complete data. Therefore, C denotes the set of the n−m records with observed data for the p variables and M is the set of the m records with incomplete data. This pattern of missing values suggests to fit some prediction model where Z1 provides the inputs and Z2 defines the set of outputs. Once the model has been trained, it is applied to Z3, obtaining an imputed set Z4.Let us denote the set of records with values observed for the variable j asXjoand the set of records with missing values for the variable j asXjm, with j=1, …, p. Let us represent, for the record i, the variable values of the set XoasX_ioand the variable values of the set XmasX_im.Different methods and tools may be used for data imputation. Little and Rubin [13] describe methods for analyzing missing data, several of them considered for continuous and normally distributed data. Some methods, such as listwise, casewise and pairwise data deletion techniques, try to make use of the available information with the omission of all the records containing errors in one or more variables, depending on the population parameters to be estimated. Other methods calculate appropriate values for replacing missing data.According to Little and Rubin [13], methods for handling missing data could be classified depending on their complexity degree. Listwise and pairwise data deletion and mean/mode are inferior in terms of accuracy; regression methods are somewhat better, but not as good as hot-deck or procedures based on multiple imputation. Consequently, three of these methods for an empirical comparison with MIMLP and IMLP have been implemented: mean/mode substitution, regression imputation and hot-deck imputation.•Mean/mode imputation: Simple method where for a quantitative variable any missing value is replaced by the mean of the observed values, and for a qualitative variable the missing value is replaced by the statistical mode. As there are variables of any kind, numerical and categorical, distinguishing between variable types is necessary. Thus, if a variable presents several missing values in different records, all of them are imputed with the same value. Therefore,Xˆjm=X¯jo.Regression models: A regression model is fitted to predict missing data for a variable from the predictors. The fitted model provides a prediction for the initial missing values of the variable. The process is repeated for each variable with incomplete data.Three multiple regression procedures have been considered in this study: multiple linear regression for quantitative variables, logistic regression when the dependent variable is dichotomous, and multinomial logistic regression used to handle categorical variables with more than two categories.Multiple linear regression. For a dependent variable or response Xjwith missing values, a population model Xj=β0+β1X1+β2X2+⋯+βfXf+ɛ is assumed using records with observed data for the variable Xjand the independent or predictor variables X1, X2, …, Xf, being f>1. ɛ denotes a random disturbance or error representing the absence of an exact relationship and being β0, β1, …, βfunknown coefficients or parameters that define the regression hyperplane β0+β1X1+β2X2+⋯+βfXf.When a qualitative variable involves c categories, c−1 dichotomous variables are added to the model:dj10ifj∉category11ifj∈category1dj20ifj∉category21ifj∈category2⋮dj,c−10ifj∉categoryc−11ifj∈categoryc−1The category c is the base category. None of the variables for this category is defined, but all observations having value 0 for the other c−1 variables. For example, if Xfis qualitative and X1, …, Xf−1 are quantitative, the multiple linear regression model would be:xij=β0+β1xi1+β2xi2+⋯+βf−1xi,f−1+α1di1++α2di2+⋯+αc−1di,c−1+ɛiLogistic regression. This method is applied when the dependent variable Xjis dichotomous, being a Bernoulli random variable whose probability parameter (its mean) is given by a functionμ(X_i), the mean and variance of Xjdepend on the value of the predictors vector.E[Xj∣X=X_i]=μ(X_i),V[Xj∣X=X_i]=μ(X_i)[1−μ(X_i)].The logistic function is one of the most used models to express this relationship:μ(X_i,β)=P[Xj=1∣X_i]=eβ0+β1xi1+⋯+βpxif1+eβ0+β1xi1+⋯+βpxif.Multinomial logistic regression. This method is used when the dependent variable Xjis a categorical variable with more than two categories, being a generalization or extension of the previous one. Assuming Xj≡1, …, C, the log odds ratio between categories c and C (base category) is defined asθ(c∣X_i)=logP[Xj=c∣X_i]P[Xj=C∣X_i], c=1, …, C. This model assumesθ(c∣X_i)=βc0+βc1xi1+⋯+βcfxif. Thus,P[Xj=c∣X_i]=eθ(c∣X_i)eθ(1∣X_i)+⋯+θ(C∣X_i).Hot-deck: This method performs the missing values estimations on the incomplete records from values of similar complete records belonging to the same data set. The nearest neighbours technique 1-NN has been used with Gower's general similarity coefficient [9] to measure the proximity between records containing mixed data types.The potential donor records are found in the complete records, the proximity between these records and the receptor records is calculated by Gower's general similarity coefficient, choosing the most similar case to the case with missing values. Hot-deck imputation method allows us to estimate the missing value xijfrom the value of the variable Xjof the complete record set, which makes maximum Gower's general similarity coefficient G:xˆij=xtj∣G(X_i,X_t)=maxG(X_i,X_γ)X_γ∈XjoFor the sample with n records and p variables, Gower's general similarity coefficient sijis defined as:sij=∑k=1pwijksijk∑k=1pwijkrepresenting bywijkthe number of variables which have observed values for both records:wijk=1ifXkis known ini,j0otherwiseand sijkis the contribution provided by the kth variable, distinguishing between different data types.For continuous and ordinal variables:sijk=1−|xik−sjk|rkdenoting rkthe range of values for the kth variable and xikthe value of the record i for the variable k. For nominal variables, if both records i and j present the same category for the variable k, the value sijkis equal to 1 and 0 otherwise:sijk=1ifxik=xjk0ifxik≠xjkThe outputs ojof the considered three-layered perceptron are:oj=w0j+∑h=1Hwhjg(v0h+∑i=1pvihxi),j=1,2,…,qfor p inputs x1, …, xp, denoting by H the size of the hidden layer,{vih,i=0,1,2,…,p,h=1,2,…,H}the synaptic weights for the connections between the p-sized input and the hidden layer and{whj,h=0,1,2,…,H,j=1,2,…,q}the synaptic weights for the connections between the hidden and the q-sized output layer.The hyperbolic tangent activation function g(u)=(eu−e−u)/(eu+e−u) is used in the hidden layer and the identity function in the output layer.In MLP, the input layer only accepts numeric values. However, data files often contain categorical variables and quantitative variables, so each qualitative variable must be codified by a binary variable 0–1 for each class. Consequently, the number ptof inputs to the MLP is usually larger than the number p of variables in the data file. To impute the value of a categorical variable, the largest predicted binary variable provides the associated category as the prediction.A supervised scheme is used, thus the network learns by modifying the synaptic weights values. Examples of both input and output values comprise a training data set which are repeatedly presented to the model, the weights are adapted looking for the maximum possible similarity between the network responses and the actual output values. The rest of the technical issues of multilayer perceptron are addressed in Section 5 for IMLP model and in Section 6 for MIMLP model.Section 4.1 provides certain aspects of the extensive suite of experiments carried out. Section 4.2 describes the data sets used to study the models. In Section 4.3, preprocessing tasks and perturbations applied to the original data sets are explained. Finally, Section 4.4 contains details about the computed measures to evaluate the different data imputation models.In general, a fixed data set T was assumed for the whole process of data imputation experiments with monotone patterns of missingness. Each correct data set T was randomly split into training (70%) and test (30%) sets, obtaining two files (T1 and T2) and achieving reliable measures of the models performance.A 30% of variables, p−f, were randomly selected, the resulting selection was Xm, and a perturbed version Tm2 of T2 was defined by setting to missing all the values of all the variables in Xm. As described above, the remaining variables not included in Xmwere represented by Xo. In this way, an imputation model to predict Xmfrom Xocould be fitted on T1. The rows of Tm2 were fed to the fitted model and the output records of the model, contained in T*2, were compared with the true records contained in T2.This same perturbation pattern was repeated 50 times to avoid that the obtained results depended simply on the performance of a single imputation process. Consequently, 50 perturbed files were obtained for each data set, repeating the imputation procedure for each one of them, i.e., the five imputation methods were applied on each one of the 50 perturbed files. The details for each proposed model are explained in Section 5 for IMLP model and in Section 6 for MIMLP model.The source code employed in this work for all the different imputation methods was written by the authors in Matlab 6.0, using the neural network toolbox [3] for the implementation of the MLP.This study was performed on 18 data sets, whose features are shown in Table 2. These data sets include different variable types: quantitative, ordinal and nominal qualitative. Moreover, they were selected to cover several domains such as biology, medicine, chemistry, electronics, social surveys, census and business. The majority of the data sets are frequently used in the scientific community, available in the UCI machine learning repository [7].Table 2 contains the following columns. Name: name of the data set; n: number of records; pt: total number of inputs and outputs for the machine learning models (including the added auxiliary variables for the qualitative attributes); pq: number of quantitative variables; pc: number of categorical variables; so, p=pq+pc.The first group corresponds to databases with qualitative and quantitative variables, the second group represents databases with only categorical variables and the third group of databases contains only quantitative variables.Several preprocessing tasks were carried out for each data set. All constant variables were removed. The quantitative variables were normalized, computing for the quantitative variable Xithe following value for the record j:§ij=xij−xi,minxi,max−xi,minAs aforementioned in Section 3, each of the categorical variables was codified by a vector formed by binary variables 0–1, one for each class. Given a categorical variable Xj, with c modalities dj1, dj2, …, djc, it was codified with c dichotomous variables 0–1, such that the lth dichotomous variable took the value 1 if the value of the variable Xjwas equal to the lth category and 0 otherwise.Following the works by Ding and Simonoff [4], missing values were generated for the purpose of this study. A perturbed data file was made up from an assumed correct and complete original data set.As described previously, a set of variables and records, randomly chosen, were set to non-response errors. Thus, a disturbed variableXjdwas defined for each original variable Xj. Since the quantitative variables were normalized, all the values ranged from 0 to 1, then non-response was reflected by assigning the value −1 to the disturbed variable. For a qualitative variable Xj, the non-response was reflected by assigning the value 0 to all the dummy variables used in the coding, making 0 the value of that variable.We tried to use a realistic probability of missing value. Thus, several data sets obtained from surveys were analyzed. Most of missing value rates were not greater than 5%. Nonetheless, some preliminary studies with 1%, 5% and 10% did not reveal differences among the models.Depending on the variable type, several measures were computed to evaluate the imputation models.For quantitative variables, each model was measured by computing the average of the squared linear correlation coefficient R2 expressed as a percentage between 0 and 100%. For each variable, where m values were set to non-response errors, R2 was computed measuring the association between the m real values and the m imputed values.For qualitative variables other measures were adopted. In Ref. [5] recommends a Wald-type statistic W to analyze whether a data imputation process preserves the marginal distribution of a qualitative variable with c categories:W=(Q−A)t[diag(Q+A)−L−Lt]−1(Q−A)where Q is the c−1 vector of counts for the first c−1 categories of the imputed variable, A represents the c−1 vector of actual counts for these categories and L denotes the square matrix of order c−1 corresponding to the cross classification of actual vs. imputed counts, all of them computed over a test set where the actual counts are known.Under weak assumptions the large sample size distribution of W is chi-square with c−1 degrees of freedom. Defining the p-value as the right tail probability of a chi-square distribution with c−1 degrees of freedom computed for the observed value W, a statistical test may be carried out for each categorical variable.If the hypothesis of marginal distribution preservation was accepted, the preservation of the true value of the categorical variable could be evaluated. In Ref. [5] a method based on the statistic D defined by the error rate is proposed. For a fixed categorical variable, this incorrect imputation rate was computed as one minus the proportion of records where the true value was equal to the imputed value. The variance of D could be estimated by V(D)=(1−D)/m when the marginal distribution was preserved.A rule to decide if D was significantly greater than 0 was developed, based onɛ=max{0,D−2V(D)}, whose value is suggested in Ref. [5]. If ɛ=0 there was not statistical evidence supporting the alternative hypothesis that D>0, therefore it was accepted that the true value of the categorical variable was preserved. Otherwise,D>ɛ+2V(D)and the true value of the categorical variable were not accepted to be preserved.In the case of an ordinal categorical variable, the preservation of the true value order was studied. The statistic for an ordinal variable Y is now:D=1m∑i=1md(Yˆi,Yi)where d represents a distance between categories of Y, suggested in Ref. [5] as:d(Yˆi,Yi)=12|Yˆi,Yi|Lt(Yˆi,Yi)max(Y)−min(Y)+Lt(Yˆi,Yi)When the order of the true value was preserved, the previous rule could also be applied.The coefficient of preservation CPR was calculated as an overall measure based on these statistics. This coefficient was defined as the categorical variables percentage which preserve the marginal distribution and the true value (for nominal variables) or the true value order (for ordinal variables):CPR=NCVN+NCVONCVNCVN denotes the number of nominal categorical variables which preserves the marginal distribution and true value. Let NCVO be the number of ordinal categorical variables which preserves the marginal distribution and true value order, and NCV represents the number of qualitative variables.A global criterion for the whole data imputation process GCD was computed as the mean of R2 (when available) and CPR (when available).The IMLP fitting requires to take several decisions such as the random initialization of the MLP weights, the number of hidden units or the number of iterations (epochs) of the learning algorithm and, of course, the training algorithm. The parameter values can affect the imputation quality. Thus, different architectures were studied to obtain the best configuration or at least a suitable range of values.It is a well-known fact that the random initial configuration of weights offers very different solutions. Hence, performing five times each learning algorithm we reduced the associated uncertainty, using five random initial weights vectors and selecting the minimum mean squared network error. This was done for each of the 50 perturbed data sets obtained according to the described process in Section 4.1.Three sizes of hidden layer, 5, 10 and 15 nodes, were selected, on account of previous ad hoc experiments which revealed that the use of larger sizes led us to worse results.In the same way, regarding the number of iterations in the training process, it was also observed that after the epoch 300 the network error decreased very slowly. So the number of studied training epochs ranged from 5 to 30 with an increase of 5 and from 50 to 300 with an increase of 25.Given that a wide number of learning rules for the multilayer perceptron are available but none assuring a global minimum of the error function, the most representative learning algorithms was considered. They are shown in Table 3.The network size was (pt, H, pt), being the number of outputs ptof the MLP equal to the number of inputs. As H represented the number of hidden units, the total number of weights for each IMLP architecture of imputation was (2pt+1)H+pt. For example, for the Zoo database with 15 qualitative attributes and 1 quantitative variable, pt=31 when the qualitative variables were codified with binary variables, considering H=15, the IMLP model comprised 976 weights.Different parameter configurations (7×3×17=357) for the IMLP model were considered. Hence, a 10-fold validation procedure was applied for each perturbed data set to select the best configuration of the learning algorithm, as well as the optimal number of hidden units and number of epochs. Thus, for each configuration, 10 values of the GCD criterion (explained in Section 4.4) were available through the 10-fold procedure, and their mean value was also computed.According to the ANN configuration setup described above, with 50 perturbed data sets, seven training algorithms, three sizes of hidden layers, 17 different values for epochs, five different initial weights sets and 10-fold validation splits, the total number of considered IMLP architectures for each data set was 892.500, which involved working with a large number of network weights. This huge amount of architectures for the multilayer perceptron allowed us to investigate the effect of the parameters on the model performance: number of hidden neurons, number of the training epochs and training algorithm.The multiple imputation method consists in obtaining a vector of MI>1 imputed data for each missing value. These imputed data are alternative values to fill the value of the incomplete data. From the imputation vectors, MIsets of complete data are generated. Each missing value is replaced by the first element of its imputation vector, obtaining the first complete data set. Then, each missing value is replaced by the second element of its imputation vector, obtaining the second complete data set, and so on [20]. Each obtained complete data set is analyzed with the same statistical methods used for complete data in the absence of non-response. Afterwards, the results of such individual analysis are appropriately combined.These MIdifferent values can be created in several ways [20,13]. Following the publication of these works, this imputation method has been studied in many articles. Various proposals are suggested, such as carrying out the multiple imputation by classification trees [16,24]. We propose to use a combination of multilayer perceptron and k-nearest neighbours to estimate these MIvalues: MIMLP model.One common method of missing value imputation for some record R is to use the k-nearest neighbours algorithm as follows: a similarity measure is computed using only the observed variables in R, and the k most similar completely observed records to the incomplete record R are identified according to similarity measure. These k records provide k possible values to impute each missing value in R, and some aggregation criterion (for example, the average, the median or the mode) is applied to obtain a final imputation.This approach includes a previous step in this aforementioned process: the training of a MLP on the complete records C=(Z1, Z2). Therefore, for each incomplete record R in the set M=(Z3, Z4), a completed version R′ can be computed applying the perceptron. The similarity function G now involves the whole set of variables, but it is defined as a weighted sum of two similarities, one is computed with the observed values, while the other similarity is computed with the imputed values. The k-nearest records to R′ are consequently identified in C, providing the k possible imputations. As described above, a similar work is available in Tusell [24], which can only be applied on continuous variables. In the present work, the imputation of missing values is made in the case of quantitative and/or qualitative variables.The built model ANN-based is a user-friendly and flexible method. Moreover, this model is also free from assumptions on distributions and allows the process automation, being possible to calculate the variability introduced in the imputation process. MIMLP model has a single hidden layer with the Levenberg–Marquardt learning algorithm. In previous ad hoc studies, the greater MLP generalization capacity was achieved with different parameter values for each database. The optimal values for these parameters, number of training epochs and hidden nodes, are shown in Table 4.The implemented model MIMLP is represented in Algorithm 6.1. The first step of the algorithm is to build the prediction model based on ANNs approximating the values of the set M from the set C. The ANN is trained with the n−m cases of C:Xo→Xˆm=ψ(Xo,ω)being ω the coefficients vector of the model. In step two, the trained model is applied to each recordi≡(X_io,X_im)with missing data, i.e., from the set M, generating the predictionsXˆ_im=ψ(X_io,ω)for the completed recordZ_i=(X_io,Xˆ_im).Algorithm 6.1Algorithm of MIMLP model.1: Train the ANN from the set C:Xo→Xˆm=ψ(Xo,ω)2: fori∈M(i=n−m+1, …, n) do3: ANN is applied to each recordi≡(X_io,X_im)calculatingXˆ_im=ψ(X_io,ω)and obtaining the completed recordZ_i=(X_io,Xˆ_im)4:λ←0.15:repeat▷ Beginning of the loop λ6:fort∈C(t=1,…,n−m),t≡(X_to,X_tm)7:Calculate the similarity functionG˜(Z_i,X_t)=(1−λ)G(X_io,X_to)+λG(Xˆ_im,X_tm)8:kmax←19:repeat▷ Beginning of the loop kmax10:forj←1, kmax11:Select the record s∈C such thatG˜(Z_i,X_s)=Maxt∈CG˜(Z_i,X_t)→Xˆ_sjm12:C←C∖{s}13:end for14:Impute the record i from the previous selectionXˆ_s1m,…,Xˆ_skmaxm15:kmax←kmax+116:untilkmax=0.05n▷ End loop kmax17:λ←λ+0.218:end for19:  untilλ>0.9▷ End loop λ20: end for▷ End main loopThe similarity functionG˜permits to determine the degree of similarity between two individuals, the similarity of the values of their variables. For two records x andy,G˜is defined as:G˜(x,y)=(1−λ)G(Xo,Yo)+λG(Xm,Ym)where G is Gower's similarity function. λ ranges from 0.1 to 0.9 with an increase of 0.2, steps 4–17, where the similarity functionG˜between the completedZ_iand all the n−m records of the complete set C is calculated, line 7. This is, for each individualt∈C(t=1,…,n−m),t≡(X_to,X_tm), the similarity function is calculated asG˜(Z_i,X_t)=(1−λ)G(X_io,X_to)+λG(Xˆ_im,X_tm). From this set, the kmax nearest neighbours to completed individualZ_iare determined. The case i is imputed with the kmax obtained vectors, steps 10 through 14. The nearest individual is obtained by calculating the similarities between one and the other individuals of the observed data set, step 11.With respect to the number of neighbours, the k-NN variant is used with all the individuals equally important. A fixed size neighbourhood is not set, it depends on the database size, considering kmax=0.05n of nearest individuals toZ_i. Thus, the imputations for values of kmax ranging from 1 to 0.05n, steps 9–16, have been studied. For each increment, the error is calculated and its evolution compared. Therefore, kmax imputations are obtained for each completed individualZ_i. For each possible value of kmax from 1 to 0.05n, the search algorithm for the kmax nearest neighbours established is to seek the most similar record s toZ_i, such thatG˜(Z_i,X_s)=Maxt∈CG˜(Z_i,X_t)obtainingXˆ_sjm. From these estimationsXˆ_s1m,…,Xˆ_skmaxm, the case i is imputed.The way in which the information of the k-nearest neighbours is combined into a prediction is crucial, given that the imputed value depends on it. If the value corresponds to a quantitative variable, the completed individualZ_iis imputed with the mean of the observed values from the kmax nearest neighbours. On the contrary, if the variable with missing data is qualitative, the missing value ofZ_iis imputed with the mode of the observed values from the kmax nearest neighbours. In the event of a tie for the most frequently occurring value, i.e., there are multiple modal values, one value is randomly selected.For each (λ, kmax):ECMλ,kmax=∑i=n−m+1n∥Xˆ_iλ,kmaxm−X_i∥2mFor each test set, a grid search for their parameters λ and kmax was conducted. As described in Section 4.1, 50 tests were performed, then the average of these 50 values was calculated, obtainingECM¯λ,kmax.Finally, the target of the multiple imputation was achieved: for each missing data i∈M(i=n−m+1, …, n) a vector of MI>1 possible values is calculated. These MIalternative values for filling the incomplete data is equal to kmax=0.05n for each possible value of λ, and λ can take values 0.1, 0.3, 0.5, 0.7 and 0.9, which would add a total of 5·MIimputations to perform. Rubin [20] specifies that between 3 and 10 imputations is enough, and Table 9 reveals that with three imputations the best performance is usually achieved.The comparison of the different methods for missing data imputation on the different data sets is shown in this section: classical imputation procedures such as mean/mode, regression and hot-deck, and both models ANN-based: IMLP and MIMLP.To complete the study, the evaluation of all the models was carried out from two different perspectives. Firstly, the performance of the imputing methods was according to the aforementioned evaluation criteria. Secondly, the performance of several classification rules on the imputed data sets was also analyzed.To have comparable results, it is important to use the same input sequence to verify the prediction accuracy of the methods. In the same empirical series all the methods are trained and tested with the same values, allowing us to carry out more rigorous comparisons among the different imputation models. Hence, when a method shows a lower error value, this is not due to having an easier data set to impute more cases, but because that model is actually able to impute more cases correctly than others, since all of them are in identical situations, trained and tested on the same examples.Table 5shows the GCD values obtained by the five approaches (models) considered over 18 databases, where the use of categorical or quantitative variables has been taken into account to show the results.IMLP offers the highest GCD values for 9 of the 10 data sets with categorical variables, being MIMLP the second option with the same values in Solar database. Regression models offer the lowest values in 6 of these 10 data sets.The five models provide similar GCD values in the eight databases with only quantitative variables, although mean/mode and hot-deck approach are never superior to the rest of models. Table 5 reveals that IMLP is the best method only in the Iris data set, but in this case with the same value as MIMLP and regression models. In four of these data sets, regression models offer the best value, competing with IMLP and MIMLP on the Iris database and with MIMLP in the rest of the databases. Finally, MIMLP is the best method in seven of the eight databases.In summary, considering all the databases, regression model is the preferred technique in one data set, competing with IMLP and MIMLP in other three. The simple mean/mode algorithm is the best model in only one data set, whereas hot-deck model would be selected in none of the data sets. IMLP and MIMLP are the best models in 14 of the 18 data sets, competing with the other models in three data sets. These results suggest that for databases with only quantitative variables MIMLP is the best option, and for databases with categorical variables, IMLP tends to provide better results in comparison with the other procedures.Table 6contains the mean test values for each criterion for the IMLP model. It shows low values for MSE, being the values of the criterion R2 very high for all the data sets. The mean value of the preservation measure CPR is greater than 80% in 6 of the 10 databases with categorical variables. The last column of this table shows that the global criterion GCD is greater than 83% in the most of data sets, being very high in databases with only quantitative variables.Table 7contains the mean test values for each criterion for the MIMLP model. Table 7 shows low values for MSE. The values of the criterion R2 are very high for all the data sets. The mean value of the preservation measure CPR is greater than 80% for some data sets with categorical variables. The last column of this table shows that the global criterion GCD is greater than 80% for almost all the data sets. Both models offer reasonable results.Table 8contains the distribution for IMLP of the 10-fold learning algorithms, hidden layer size and number of epochs selected, computed as percentages over the disturbed data sets. This table suggests that LM tends to provide the preferred learning algorithm, being GC the main alternative. The learning algorithms GD and GDM are never selected. As regards the number of hidden nodes, in general five hidden units are sufficient. The number of epochs appears to be more variable, setting a valid range from 150 to 250.Table 9 contains the distribution of the variable parameters λ and kmax for the MIMLP model, computed as percentages over the disturbed data sets. The column kmax represents the percentage of the population corresponding to the value kmax taken. This table show that the optimal configuration (λ, kmax) corresponds to a λ value equal to 0.1 and taking 3% of the population for kmax would be sufficient.One of the goals of data imputation is to obtain imputed data aimed at improving the accuracy of the classification results. Thus, to test the behavior of the proposed models considering classification tasks, we conducted experiments comparing the resulting imputed databases by means of different classifiers.In this way, the good or bad estimations performed by the imputation data techniques were analyzed according to the accuracy obtained by some classification methods: Naïve Bayes, K-Nearest Neighbours and J48 Decision Tree, selected as a set of popular classifiers in data mining community. These three classifiers are a part of the WEKA System Witten and Frank [26], used to perform the simulations.The classifiers can be briefly described as follows:Naïve Bayes:Naïve Bayes Classifier is supported on Bayes’ theorem, considering a simple form for the density or probability function for each component and with the supposed variables independence. The naïve bayes classifier is considerably effective in practice, in spite of its simplicity and unrealistic assumption, competing with more sophisticated classification methods. The naïve Bayes classification method uses all attributes and allows them to make contributions to the decision, equally important and independent from one another given the class [14]. Training is very effortless and agile, only requiring consideration for each attribute in each class separately. In the test phase, new cases are classified, deciding to which class label they belong and calculating conditional probabilities with normal distributions.K-nearest neighbours classification method is a simple algorithm for predicting a target variable from all available cases. A dependent variable and several independent variables are distinguished. Hence, the test data set is the set to be classified and the training data set is formed by the predictor variables. The algorithm is as follows. For each observation in the test data set, the k-nearest neighbours of the training data set are taken accordingly to some metric. In particular, Euclidean distance is applied as the similarity measure, habitually used [14], calculating how close each case of the training set is to the case of the test set. These k closest cases are examined, obtaining to which category most of them belong. This category is assigned to the test set case being studied. This procedure is repeated for the remaining observations in the test set. Several values for k, from 1 to 15, have been performed for all the databases, obtaining with k=1 the highest percentage of cases correctly classified.C4.5 Decision tree learning method is one of the robust algorithms of decision tree learning algorithms family, broadly used for classification problems. These algorithms are based on classifying instances into one of a discrete set of possible categories. So the training set is formed by a set of cases belonging to known classes, the cases are analysed for patterns allowing the classes to be accurately separated. The patterns are represented in the form of decision trees or sets of if-then rules, forming the models used to classify new observations. It must ensure that the models are as understandable and accurate as possible [17]. In this study, the WEKA implementation of the C4.5 decision tree, J4.8, is used.To each imputed data set obtained with the best configuration, according to evaluation criterion, classifiers are applied and their results compared. These results do not only rely on the previous imputation step, but also on the parameter configurations of the classification methods.Table 10shows the parameter configurations of each classification method, in cases where the method needs a parameter. The parameter values were chosen according to ad hoc experiments. The experimental results were also averaged on the 10-fold cross validation scheme.The study of all the classifiers for each data set, comparing the results between the different databases, are shown in Tables 11–13. The values correspond to the percentage (%) of correctly classified instances. The first column shows the obtained results by the classifiers on original data sets; the second and third columns show the obtained values by the classifiers on IMLP imputed and MIMLP imputed data sets, respectively. The difference between columns indicates the extent to which the imputation process is able to impute/fill missing values.The results for original databases are only useful for illustrative purposes, because in practice these data would not be available. These results can be compared with the obtained results by the classifiers on MLP-based imputed data, offering an idea of how good the estimations performed by the imputation data techniques are. It is observed that for each of the classifiers, the values between the different data sets (original, IMLP and MIMLP) are similar to each other. Moreover, the obtained success rate is also similar between the classifiers. In general, a high percentage of missing values is estimated. The MLP technique-based-imputed data sets show values higher than the ones obtained with original data sets for some databases. Although it is true that in other databases the opposite occurs, the obtained accuracy with original data sets is slightly higher than the one obtained with imputed data sets.In this section, we are not interested in evaluating the best classifier for each database, but in evaluating, in the context of classification tasks, the proposed imputation data methods. From this study, we can point out that the behaviour of both models for the different databases is homogeneous and the powerful capability of the imputation process to estimate non-response stands out.The NB classifier offers similar results for imputed data sets in all the models; there is no inclination towards choosing the best choice, although with much accuracy, the highest percentage was obtained for six IMLP-based imputed data sets of the 18 databases. k-NN classifier offer the best results for eight IMLP-based imputed data sets of the 18 databases, being more divided for C.45 classifier.According to GCD criterion, the IMLP model was the best method in 10 of the 18 databases. If we focus on the comparison of imputed data sets considering the data types, as well as we did with the previous evaluation criteria in terms of the global criterion GCD, the results are as follows.In the five databases with both variable types, qualitative and quantitative, for NB and k-NN classifier the highest percentages correspond to IMLP-based imputed data sets, without a clear difference for C4.5.For databases with only qualitative variables, the results in the three classifiers are very similar for all the models, offering the IMLP-based imputed data sets the highest percentages in k-NN.For databases with only quantitative variables, the best results in terms of prediction are obtained with the MIMLP model in almost all databases. In terms of classification, the clearest differences between the imputed data sets are observed for the k-NN classifier, corresponding the highest percentage of correctly classified cases for IMLP-based imputed data sets in three of the nine databases and MIMLP-based imputed data set in other three. In the rest of the classifiers, the values coincide for all the imputed data sets.In general, the results indicate that the best classifications are carried out from the IMLP-based imputed data sets. This same fact occurred with the evaluation criterion GCD.

@&#CONCLUSIONS@&#
Two models for data imputation IMLP and MIMLP were proposed and empirically compared with three classical methods: mean/mode imputation, regression models and Hot-deck. In the IMLP model, based on artificial neural networks, several architectures and training algorithms for the multilayer perceptron were tested. In the MIMLP model, a multiple imputation technique combining multilayer perceptron and k-nearest neighbours, a wide range of parameter configurations λ and kmax was explored. This amount of architectures and parameter configurations were studied with the aim of fitting parameter values, evaluating the influence of these parameters on the models.Eighteen real and simulated data sets were exposed to a perturbation experiment, following the monotone pattern to generate missing values, and all the imputation methods were rigorously tested. Several criteria to evaluate the imputation of non-response were computed.The proposed ANN-based imputation models did not only offer automatic imputation, but also very good results. The experimental results showed that for data sets with only quantitative variables MIMLP model provided the best results, being IMLP the best method for data sets with categorical variables. In the first case, the optimal configuration (λ, kmax) corresponded to a λ value equal to 0.1 and taking 3% of the population for kmax would be sufficient. In the second model, Levenberg–Marquardt learning rule tended to be selected, whereas simple architectures (only five hidden units) were usually preferred.Future works could include other ANN models or other machine learning tools, different missing data mechanisms, and new evaluation criteria. Another future research topic would be the treatment of missing values in data sets varying in time, for example panel surveys or econometric time series.