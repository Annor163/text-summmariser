@&#MAIN-TITLE@&#
A differential evolution algorithm with constraint sequencing: An efficient approach for problems with inequality constraints

@&#HIGHLIGHTS@&#
An optimization algorithm is introduced based on a partial evaluation policy.The population are evaluated based on a random sequence of constraints.The search using multiple constraint sequences offers the potential to reach different regions of the search space.

@&#KEYPHRASES@&#
Differential evolution,Constraint handling,Constraint sequencing,Fitness evaluation,

@&#ABSTRACT@&#
Differential evolution (DE) refers to a class of population based stochastic optimization algorithms. In order to solve practical problems involving constraints, such algorithms are typically coupled with a constraint handling scheme. Such constraints often emerge out of user requirements, physical laws, statutory requirements, resource limitations etc and are routinely evaluated using computationally expensive analysis i.e., solvers relying on finite element methods, computational fluid dynamics, computational electromagnetics etc. In this paper, we introduce a novel scheme of constraint handling, wherein every solution is assigned a random sequence of constraints and the evaluation process is aborted whenever a constraint is violated. The solutions are sorted based on two measures i.e., the number of satisfied constraints and the violation measure. The number of satisfied constraints takes precedence over the amount of violation. We illustrate the performance of the proposed scheme and compare it with other state of the art constraint handling methods using a common framework based on differential evolution. The results are compared using CEC-2006, CEC-2010 and CEC-2011 test functions with inequality constraints. The results clearly highlight two key aspects (a) the ability to identify feasible solutions earlier than other constraint handling schemes and (b) potential savings in computational cost offered by the proposed strategy.

@&#INTRODUCTION@&#
Constraint handling is an important and active area of research as the performance of all optimization algorithms are dependent on it. The non-linearity, multi-modality and the feasibility space associated with each constraint are likely to be different. Constraint handling methods can be broadly categorized into six different types [1,2]: (i) use of penalty functions, (ii) repair schemes, (iii) feasibility-first, (iv) ϵ-constrained method, (v) dominance based method and (vi) adaptive method and ensembles. The discussion below provides a brief of each approach.Penalty functions are one of the most commonly adopted approaches for constraint handling. These methods penalize an infeasible solution with predefined penalty factor(s) and aggregate into a scalar value. In the context of minimization, the primary aim is to decrease the fitness of infeasible solutions in order to favor the selection of feasible solutions. Variants of the penalty function based approaches include static penalties [3], dynamic penalties [4], annealing penalties [5], adaptive penalties [6,7], death penalties [3], superiority of feasible points [8] and faster adaptive penalties [9]. While some of these methods have resulted in competitive performance, the choice of an appropriate penalty factor(s) is known to be non trivial.Population based stochastic algorithms are known to perform well for unconstrained and box constrained problems. However, they face difficulties in solving problems involving constraints as the search operators (i.e., crossover and mutation) are blind to constraints [10]. A number of repair schemes have been introduced in [11,2,12] wherein an infeasible solution is repaired to a feasible solution. Development of repair schemes is often problem dependent and may also involve additional computational cost.Feasibility-first schemes are yet another form of constraint handling where a feasible solution is always preferred over an infeasible solution. The common form of preference rules are governed by the following: (a) any feasible solution is preferred over an infeasible solution (b) among two feasible solutions, the one with better objective is preferred (c) among two infeasible solutions, the one with lowest constraint violation is preferred [13]. While these feasibility-rules were originally developed in the context of evolutionary algorithms, they were also used in the context of DE [14,15]. An adaptive version of the above scheme is referred as ϵ-constraint handling proposed in [16]. The method essentially designates a selected set of infeasible solutions as feasible i.e., by accepting a certain level of constraint violation. The method delivered highly competitive results on a set of constraint optimization problems of CEC-2010 [17]. Motivated by its performance, the approach has been successfully adopted in the works such as in [18–20]. Although the method performed well on the benchmark suite, the prescribed values of the parameters could potentially affect the performance for an unknown problem.In spite of the fact that dominance based methods face great difficulties with an increase of number of objectives, there are some competitive constraint handling methods which rely on such concepts. Infeasibility driven evolutionary algorithm (IDEA) [21] utilizes constraint violation as an additional objective and preserves a set of infeasible solutions throughout the search. Notable concepts in this direction include the works reported in [22–25]. Others include spherical-pruning multi-objective optimization differential evolution (sp-MODE) [26], hybrid constrained EA (HCOEA) [27], steady state EA [28], adaptive trade-off model (ATM) evolution strategy (ATMES) [29], ϵ-dominance concept [30] and adaptive bacterial foraging algorithm (ABFA) [31]. In the backdrop of No-Free-Lunch (NFL) theorem [32], ensemble of constraint handling schemes have also been introduced [33–35]. Such schemes still require a number of user defined parameters. In order to eliminate the need of user defined parameter(s)/penalty factors, a number of adaptive strategies have been proposed in the literature. They include adaptive penalty technique [36], parameter less penalty function [37], a self-adaptive fitness formulation [38], stochastic ranking [39] etc.However, in all such formulations and implementations, a full evaluation policy is adopted, i.e. for every solution, its constraint violation (CV) is measured. The constraint violation is calculated as follows:(1)CV=∑i=1pmax(gi,0)+∑j=1qmax(|hj−ϵ|,0)where giand hjare the p and q number of inequality and equality constraints. An important question is “why do we spend computational resources to evaluate constraints of a solution, when it has already violated a constraint?”. Assuming that one is only interested in a feasible solution (preferably optimum) at the end of the search process, it is important to investigate the worth of evaluating infeasible solutions i.e. the cost of evaluation versus the knowledge gained to steer the search. Other followup questions include “what is the best sequence to evaluate the constraints?” and “is there a benefit in using different sequences of constraints?”.This paper attempts to understand the cost-benefits of partial evaluation policy i.e. aborting evaluation of constraints if the solution has already violated one constraint. The above discussion becomes more relevant in the context of optimization problems involving computationally expensive constraint evaluations. The study assumes that the constraints can be evaluated independently of one another. The feasibility-first scheme is perhaps the most popular method to improve the computational efficiency of an optimization algorithm. In the feasibility first scheme, the objective function of a solution is only evaluated if the solution has satisfied all its constraints. The approach has been successfully adopted in the works such as in [13,40,41]. The area of constraint sequencing has received less attention and there are no reports to the best of our knowledge in the area of constraint handling in real-life optimization problems utilizing partial evaluation policy, i.e. evaluate constraints of a solution until it violates one. Identification of a subset of constraints have been attempted in [42] using principles of dimensionality reduction, while an effective constraint list was generated in [43,44]. None of the above mentioned approaches investigated in depth to the effects of partial evaluation.In this paper, we introduce an optimization algorithm based on a partial evaluation policy. The solutions in the population are evaluated based on a random sequence of constraints. The search using multiple constraint sequences offers the potential to reach different regions of the search space. The proposed scheme has been implemented using a framework based on differential evolution [45,46]. The benefits of using multiple constraint sequences are illustrated using two examples in Section 2. The detailed analysis of the performance of the scheme on g-series[47] test instances is presented in Section 3. The summary of key findings are presented in Section 4.The algorithm is described in the context of a constrained minimization problem defined as follows:(2)Minimizef(x),xmin≤x<xmaxS.t.gk(x)≤0,k=1,…pwhere x={x1, x2, ....xD} is the design variable vector bounded by upper (xmax) and lower (xmin) bounds ofx∈RDand p is the number of inequality constraints. The pseudo code for the proposed Differential Evolution algorithm with Constraint Sequencing (DE-CS) is presented in the following section while the components of the algorithm are discussed in subsequent subsections. The algorithm consists of four major components – (a) initialization of the population, (b) evaluation of the individuals, (c) parent selection and offspring generation, and (d) replacement of the individuals.Algorithm 1DE-CSSET:NFEsmax{Maximum number of function evaluations},GENmax{Maximum number of generations}, N{Size of population}, CR{Crossover rate}, F{ Mutation scale factor}1:FEs=0,Gen=12:Initialize the population, Pop;3:Assign a random constraint sequence (e.g., different sequence of 1 to p number of constraints) to each individual in the population;4:Evaluate the solutions following the above assigned sequence of constraints;5:Update(FEs);6:whileGen≤GENmax|| FEs≤NFEsmaxdo7:fori=1:Ndo8:Select p1=i i.e., the ith parent and two other parents p2 and p3 randomly s.t., p1≠p2≠p3;9:Generate the trial vector, ui=DE/rand/1/bin10:Evaluate the trial vector, uiin the ith sequence of constraints;11:Update(FEs);12:ifξ(ui)≤ξ(Popi) then13:Popi=ui14:end if15:end for16:Gen=Gen+117:end while*FEs denotes the sum of objective and all individual constraint evaluations.A population of N individuals is initialized. The variables of ith individual are initialized as follows:(3)xi,j=xj,min+randi,j[0,1).(xj,max−xj,min)where j=1, 2, …D is the number of variables; xj,maxand xj,minare the upper and the lower bounds of jth variable. For a problem with p constraints, each individual is assigned a random sequence of constraints for evaluation.Every individual of the population is evaluated using its prescribed sequence of constraints. In the sequence, if a constraint is violated, the evaluation is aborted. The term NFEs referred in the paper is the sum of the number of evaluated constraints and objective function evaluations. An objective function evaluation is only undertaken if the solution is found feasible i.e., all the constraints are satisfied.In order to generate an offspring solution, the first parent is selected sequentially, the second and third parents are selected randomly from the entire population. In the recombination process, a binomial crossover [48] has been used to generate the offspring solution. The binomial crossover is defined for jth variable of a ith individual as follows:(4)ui,j=vi,j,ifrand[0,1)≤CRorj=jrand,xi,j,otherwisewherevi,j=xp1,j+F(xp2,j−xp3,j)is referred as the difference vector and F is a mutation scale factor.A parent individual is replaced if the fitness of an offspring is better than the fitness of current individual. The fitness of an offspring is determined as follows:(5)fitness(ξ)=f(x),x∈Rngk(x),where gkis the constraint violation for k=1:p number of constraints. For possible replacement, the objective function value is only compared if the solution is found feasible, otherwise the solutions are ranked as described in the following.For every solution in the population, one can compute the number of satisfied constraints (NS) and the amount of violation (V) i.e., the sum of all gk's. The number of satisfied constraints has a precedence over the violation value. For example assume a population, containing 4 solutions (S1, S2, S3, S4). The constraint violation values would assume a form illustrated in Table 1. After ranking the individuals, S3 is identified as rank 1 as the value of NS is higher and the value of V is lower, whereas S1 is identified as rank 4 with a lower value of NS and the higher value of V. The final ranking of the solutions would be S3, S2, S4, S1.While we have presented the algorithm and the details of the components in the above section, it is important to identify if there are significant differences in the underlying search process. To observe the underlying search behavior, we created two examples with 3 constraints. In the first problem, the feasible space of the problem is the feasible space dictated by constraint g3, while in the next example the feasible space of the problem is the intersection of the feasible spaces of the individual constraints i.e., (g1∩g2∩g3). For the first example, the constraints and the feasible spaces corresponding to each constraint is depicted in Fig. 1, with the optimum located at x*={0.65, 0.70}. The mathematical formulation of the problem (Example1) is presented below:(6)Minimizef1(x)=(x1−0.65)2+(x2−0.7)2Subjecttog1(x)≡(x1/2.6)2+(x2/2.6)2−1≤0,g2(x)≡x12/2.6+x22/2.6−1≤0,g3(x)≡x12+x22−1≤0,To understand the differences in the dynamics of the search behavior between two strategies, (i.e. constraint sequencing adopting a partial evaluation policy (CS) (discussed in Section 2.4) and constraint violation (CV) (see Eq. (1)) adopting a full evaluation policy), both the strategies have been implemented within the same DE based optimization framework. A population size of 50 individuals were allowed to evolve using a crossover rate of 0.9 and mutation scale factor of 0.5 with the maximum number of function evaluations (i.e. the sum of constraints and objective function) capped at 30,000. It is important to highlight that the same starting population has been used for both the strategies for a consistent comparison.One can see from Fig. 1 that for NFEs=1950, DE-CS identifies a number of feasible solutions in the feasible region whereas, the solutions of DE-CV are still in the infeasible region. With the known optimum for this example, the Feasibility (Eq. (7)) and the Distance (Eq. (8)) from the known optimum is plotted against NFEs in Fig. 2. One can clearly observe from Fig. 2 that, DE-CS identifies feasible solutions close to the optimum better than DE-CV and at the same time offers a better feasibility. The plots are based on the median run among 50 independent runs.(7)Feasibility=NumberoffeasiblesolutionsNFEs(8)Distance=||xb−x*||where ||.|| is the Euclidean distance between the best solution found so far xband the optimum x*.In our second example, the feasible space for the problem lies at the intersection of the feasible spaces of the individual constraints i.e. g1∩g2∩g3. The mathematical formulation of the problem (Example2) is presented below:(9)Minimizef1(x)=(x1−2.35)2+(x2+0.05)2Subjecttog1(x)≡(x1/2.6−0.96)2+(x2/2.6−0.96)2−1≤0,g2(x)≡(x1/2.4)2+(x2/2.4)2−1≤0,g3(x)≡(x1−3.1)2+(x2+0.6)2−1≤0,The problem has a small feasible region with the optimum solution of x*={2.35, −0.05}. The same set of parameters as listed above has been used for this example. One can observe from Fig. 3, that DE-CS once again identifies more solutions close the optimum over DE-CV with a better distance and feasibility. It is important to highlight that using multiple sequences, DE-CS is able to reach different regions of the feasible space as opposed to DE-CV which tends to have limited diversity of solutions spanning the feasible space. The same behavior of DE-CS can also be observed in this example as depicted in Fig. 4.Since the primary contribution of the proposed scheme is to identify feasible solutions faster which eventually leads to better convergence to the optimum solution i.e., with less computational effort, we present the results of 50 independent runs. The number of function evaluations required to obtain the first feasible solution and the distance of the first feasible solution from the optimum is presented in Table 2. The results clearly indicate the superiority of DE-CS over DE-CV, i.e. the ability of the constraint handling scheme to identify feasible solutions faster.In both the examples, g3 is the most difficult constraint to satisfy i.e. one which has the smallest feasible space. Since the solutions are ranked using the precedence of number of satisfied constraints, individuals attempting to use the sequence i.e. (g3,g2,g1) or (g3,g1,g2) tend to be less successful as the dominant sequence (Fig. 5).

@&#CONCLUSIONS@&#
