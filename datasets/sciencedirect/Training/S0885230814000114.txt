@&#MAIN-TITLE@&#
Feature enhancement by deep LSTM networks for ASR in reverberant multisource environments

@&#HIGHLIGHTS@&#
Deep recurrent neural networks are used for data-based speech feature enhancement.The approach is complementary to state-of-the-art ASR (e.g., discriminative training).Best results on the 2013 2nd CHiME Challenge task (track 2) are achieved.The superiority of the BLSTM network type is shown for the CHiME enhancement task.

@&#KEYPHRASES@&#
Automatic speech recognition,Feature enhancement,Deep neural networks,Long Short-Term Memory,

@&#ABSTRACT@&#
This article investigates speech feature enhancement based on deep bidirectional recurrent neural networks. The Long Short-Term Memory (LSTM) architecture is used to exploit a self-learnt amount of temporal context in learning the correspondences of noisy and reverberant with undistorted speech features. The resulting networks are applied to feature enhancement in the context of the 2013 2nd Computational Hearing in Multisource Environments (CHiME) Challenge track 2 task, which consists of the Wall Street Journal (WSJ-0) corpus distorted by highly non-stationary, convolutive noise. In extensive test runs, different feature front-ends, network training targets, and network topologies are evaluated in terms of frame-wise regression error and speech recognition performance. Furthermore, we consider gradually refined speech recognition back-ends from baseline ‘out-of-the-box’ clean models to discriminatively trained multi-condition models adapted to the enhanced features. In the result, deep bidirectional LSTM networks processing log Mel filterbank outputs deliver best results with clean models, reaching down to 42% word error rate (WER) at signal-to-noise ratios ranging from −6 to 9dB (multi-condition CHiME Challenge baseline: 55% WER). Discriminative training of the back-end using LSTM enhanced features is shown to further decrease WER to 22%. To our knowledge, this is the best result reported for the 2nd CHiME Challenge WSJ-0 task yet.

@&#INTRODUCTION@&#
Decoding of large vocabulary speech in unfavorable acoustic conditions, especially in hands-free scenarios involving interfering noise sources and room reverberation, is still a major challenge for today's automatic speech recognition (ASR) systems despite decades of research on this topic. Robustness of ASR systems can be addressed at different stages of the recognition process (Schuller et al., 2009), and successful systems usually employ a combination of them (Barker et al., 2013). Popular techniques comprise front-end speech enhancement, such as by microphone array processing (Maas et al., 2011; Nesta et al., 2013) or monaural speech de-noising techniques (Rennie et al., 2008; Raj et al., 2010), as well as improvements in the back-end by model adaptation (Gales and Wang, 2011) or improved ASR architectures taking into account additional sources of information, such as neural networks (Hinton et al., 2012; Seltzer et al., 2013; Geiger et al., 2013). ‘In between’ one can also address noise-robust features – a popular expert crafted feature extraction scheme is RASTA-PLP (Hermansky et al., 1992) – or feature enhancement, defining a mapping from noisy to noise free speech features. An example for a data-based, non-parametric technique for feature enhancement is histogram equalization (de la Torre et al., 2005; Wöllmer et al., 2011a).Furthermore, feature enhancement by recurrent neural networks has been considered (Parveen and Green, 2004; Maas et al., 2013). In particular, bidirectional Long Short-Term Memory (BLSTM) recurrent neural networks (RNNs) have been employed by Wöllmer et al. (2013) for feature enhancement in highly non-stationary noise, by mapping noisy cepstral features to clean speech cepstral features, and have been shown to outperform traditional RNNs on this task. In Weninger et al. (2013), we have successfully applied the BLSTM methodology to both ASR tasks (small and medium vocabulary) of the 2013 2nd CHiME Speech Separation and Recognition Challenge (Vincent et al., 2013), which features highly non-stationary convolutive noise recorded from a real home environment over a period of several weeks. There, our BLSTM approach outperformed a similar approach using conventional RNNs on a small vocabulary task (Maas et al., 2013). In this article we proceed to a larger scale evaluation of BLSTM-RNNs and other types of neural networks – including feedforward neural networks – in a medium vocabulary task.With respect to our earlier study (Weninger et al., 2013), this article presents several improvements of network topology and training, resulting in further performance gains. Furthermore, a goal of our present study is to clarify which parts of the performance gain can be attributed to refined ASR back-ends and which to better feature enhancement. In particular, we consider feature mappings from noisy and reverberated to close-talk features with deep network topologies, as well as feature enhancement in the logarithmic Mel frequency domain instead of the cepstral domain. We also investigate whether measures of the network regression performance are correlated to ASR performance, which involves much more complicated likelihood functions than typically used in network training. We also take into account the effect of using multi-condition training with reverberated and noisy speech, feature transformations, and discriminative back-end training separately. All these points have not been addressed in our earlier work (Weninger et al., 2013).In the following, we will first outline our feature enhancement methodology before describing the experimental setup including a brief outline of the CHiME Challenge data and presenting the results.In this article, we use deep LSTM recurrent neural networks (RNNs) for speech feature enhancement. By that, we combine several ideas that have been successfully applied to speech recognition tasks: using multiple hidden layers for increasingly higher level representations of the input features (Hinton et al., 2012; Graves et al., 2013), exploiting temporal context by using recurrent neural networks with an internal state that is preserved over time by using the LSTM architecture (Gers et al., 2000; Graves, 2008), and supervised learning of non-linear mappings from noisy and reverberant to clean features (Maas et al., 2013; Weninger et al., 2013).Let us denote the noisy input features in time frame t by xtand the corresponding clean features by st. We use deep LSTM-RNNs with N layers to generate an estimate of the clean speech featuressˆtby the following iterative procedure:(1)ht(0):=xt,(2)ht(n):=Lt(n)(ht(n−1),ht−1(n)),(3)sˆt:=W(N),(N+1)ht(N)+b(N+1),for n=1, …, N and t=1, …, T, where T is the number of frames in the utterance.ht(n)denotes the hidden feature representation of time frame t at level n. In the above and in the ongoing, W(n),(n+1) denotes the feed-forward connection weights from layer n to the next layer (n=0: input layer, n=N: output layer), while W(n),(n), n>0, contains the ‘self-loop’ weights implementing the recurrent structure; b denotes bias vectors.From Eqs. (1)–(3), it is obvious that the enhanced speech framesˆtdepends on the previous inputs and also the previous enhanced framessˆt−1,sˆt−2,…. This way, recurrent neural networks are able to model speech feature dynamics both in the input and output, rather than doing frame by frame enhancement. In contrast to other studies using recurrent neural networks for speech de-noising (Maas et al., 2013), our networks employ the LSTM activation functionLt(n)instead of the typically used simple sigmoid-like functions. The crucial point is to augment the activation function of each cell with a state variable ctthat is preserved by means of a recurrent connection with weight 1. This enables the network to store inputs over longer periods of time; for example, noise frames without speech can be valuable of enhancing noisy speech frames in the future. It also resolves the ‘vanishing gradient problem’ where the influence of inputs on the output would decrease exponentially over time in conventional RNNs, making them difficult to train using gradient descent (Bengio et al., 1994). The hidden layer activations correspond to the states of the cells scaled by the activations of the ‘output gates’,ht(n)=ot(n)⊗tanh(ct(n)),where ⊗ denotes element-wise multiplication and tanh is applied element-wise. Forct(n), the following definition holds:(4)ct(n)=ft(n)⊗ct−1(n)+it(n)⊗tanh(W(n−1),(n)ht(n−1)+W(n),(n)ht−1(n)+bc(n)).There,ft(n)is the activation of the ‘forget gate’ that can scale the state variable and probably reset it to zero. Furthermore,it(n)is the activation of the input gate that regulates the ‘influx’ from the feedforward and recurrent connections. Similarly to (4), the activations of the output gates ot, input gates itand forget gates ftare non-linear functions of weighted combinations ofht(n−1)(feedforward connections) andht−1(n)(recurrent connections). In particular, instead of multiplying the hidden layer activations from the previous time step with a static weight as in a traditional RNN, the network ‘learns when to forget’ (Gers et al., 2000). Details can be found in Graves (2008), Graves et al. (2013). It has been shown in the context of speech recognition that using the LSTM activation function provides a self-learnt amount of temporal context to the network, which seems to be superior to relying on a manually defined amount of ‘stacked’ input feature frames (Wöllmer et al., 2011).The parameters W and b are learned by backpropagation through time from noisy and clean training data (cf. Section 3.3). The sum of the squared deviations betweensˆtand the original clean speech st(sum of squared errors, SSE) is used as error function,(5)d=∑t,f(st,f−sˆt,f)2.In case thatsˆtand stare log spectra, this function is related to the log spectral distance (Gray and Markel, 1976).So far, the automaton structure given by (1)–(3) can exploit acoustic context from previous frames. For automatic speech recognition, where whole utterances are decoded, future context can be used as well. This results in the concept of bidirectional networks. Each layer of a bidirectional network consists of two independent layers, one of which applies (2) and (3) in the order t=1, …, T as above (forward layer) and the other in the reverse order, i.e., replacing t−1 by t+1 for the recurrent connections and iterating over t=T, …, 1 (backward layer).For each time step t, the activations of the nth forward (→) and backward (←) layer are collected in a single vector(6)ht(n)=ht(n)→;ht(n)←.Both the forward and backward layers in the next level (n+1) ‘see’ this entire vector as input. Thus, conceptually, in a deep BLSTM network one processes the sequence forward, then backward, collects the activations and uses them as input for a forward and backward pass on the sequence on the next level, etc. Alternatively to (6), one can consider ‘subsampling layers’ (Graves, 2008) performing the operation(7)ht(n)=tanhWsub,(n)ht(n)→;ht(n)←,with trainable low-rank weight matrices Wsub,(n), for n=1, …, N−1. We found this very useful for information reduction between the layers, reducing training time without decreasing performance, in contrast to simply using less hidden units.In this article, we perform evaluations on the medium vocabulary (5k) task of the 2013 2nd CHiME Challenge (Vincent et al., 2013). It consists of reverberated and noisy utterances corresponding to artificially degraded versions of the speaker independent development and evaluation test sets of the Wall Street Journal corpus of read speech (WSJ-0). It is split into disjoint sets with 84, 10, and 8 training, development, and test speakers, each comprising different prompts (si_tr_s, si_dt_05 and si_et_05). The monophonic original utterances have been convolved with stereophonic room impulse responses measured in a domestic environment, and overlaid with realistic, stereophonic noise recorded in the same environment at signal-to-noise ratios (SNRs) from −6 to 9dB, in steps of 3dB. Instead of artificially scaling speech and noise to resemble various SNRs, segments matching a specific SNR are selected from the noise recordings. Thus, noise types differ among SNRs and range from household appliances to music and to interfering speakers. The full set of utterances is used at all SNRs in each of the development and test sets. Thus, there are 6×409=2454 development, and 6×330=1980 test utterances. A noisy training set is provided in addition, which comprises randomly selected, disjoint subsets of WSJ-0 training utterances at each SNR. Thus, the number of training utterances in the noisy training set is the same as in the original WSJ-0 corpus (7138). The training and development sets are also provided in a noise-free, but reverberated version to allow for evaluation of de-noising algorithms. The total length of the training, development, and test set is 14.5, 4.5, and 4h. While the Challenge data is stereophonic, in our study we only consider simple beam-forming and subsequent monaural processing (cf. below). The 2nd CHiME Challenge corpus is made publicly available for WSJ-0 licensees.11http://spandh.dcs.shef.ac.uk/chime_challenge/ – last retrieved January 2014.Our contribution to the 2nd CHiME Challenge itself (Weninger et al., 2013), and a related contribution using standard RNNs (Maas et al., 2013) considered only Mel frequency cepstral coefficients (MFCCs) as input and output of the feature enhancement networks. Using MFCCs is mainly an ad-hoc solution motivated by their use in the speech recognition back-end; in particular, HMMs with diagonal covariance Gaussian mixtures.However, recent studies on deep neural network based speech recognition (Hinton et al., 2012; Graves et al., 2013) directly use logarithmic Mel filterbank outputs (Log-FB). The rationale behind using Log-FB is to let the network derive a suited higher-level feature extraction strategy by itself. Furthermore, we also consider Log-FB as training targets. Since Log-FB are correlated with each other, this resembles multi-task regularization of the network and is thus expected to help generalization. 26 Log-FB covering the frequency range from 20 to 8000Hz are used, as is often done in ASR. We add delta coefficients both to the input and output; using them as targets is similar in spirit to the proposal by Seltzer and Droppo (2013) to use multi-frame information as training targets in neural network based speech recognition, which again serves to improve generalization. As additional feature in input and output, we use root-mean-square (RMS) energy with deltas. For the MFCC features, we also add acceleration coefficients (second order deltas), and we perform cepstral mean normalization (CMN) to (partially) compensate channel effects. Thus, in the MFCC case, the network input and output exactly correspond to the ASR front-end used in the HTK CHiME baseline (Vincent et al., 2013). In the Log-FB case, the outputs can be converted to MFCCs by simply applying a Discrete Cosine Transformation (DCT) (Young et al., 2006), cf. below. Log-FB features are investigated with and without log spectral subtraction, which is the Log-FB domain equivalent of CMN (Gelbart and Morgan, 2001). For transparency, feature extraction is done using HTK, using the MFCC_E_D_A_Z, FBANK_E_D and FBANK_E_D_Z types of features with the default parameters (Young et al., 2006).Prior to feature extraction, the stereophonic signals are down-mixed to monophonic audio by averaging channels, corresponding to simple delay-and-sum beam-forming. This is useful for the CHiME Challenge track 2 data where the speaker is positioned at a frontal position with respect to the microphone, and is hence exploited in the baseline system by Vincent et al. (2013).All features are globally mean and variance normalized. To this end, we compute the global means and variances of the noise-free and the noisy training set feature vectors and perform mean and variance normalization of the network training targets and the network inputs accordingly. This normalization was found to be very important for performance; in particular, it ensures that features with large variance due to noise do not ‘mask’ important information in features with lower variance such as delta coefficients.Feature enhancement BLSTM networks are trained on the task to map the features of the noisy training set of the above-mentioned corpus to a noise-free training set. In a first set of experiments, we consider de-noising only, i.e., learning mappings of noisy to clean features within the same acoustic environment. As a consequence, the output features will still be reverberated, and they will be used for decoding with a model adapted to the reverberated training data. This corresponds to our contribution to the 2nd CHiME Challenge (Weninger et al., 2013). In this article, we additionally consider learning mappings from noisy and reverberated to ‘fully clean’ (noise-free, close-talk microphone speech) features, i.e., the network also learns feature-space de-reverberation. There, we also consider deep learning with pre-training, where the first layers are trained to de-noising and subsequent layer(s) are trained to perform de-reverberation. While the CHiME WSJ-0 corpus also contains noise context for each utterance, we use only the ‘isolated’ utterances, i.e., the end-pointed speech segments.We train the networks through on-line gradient descent with a learning rate of 10−5 and a momentum of 0.9. Prior to training, all weights are randomly initialized with Gaussian random numbers (mean 0, standard deviation 0.1). The on-line gradient descent algorithm applies weight changes after processing each utterance, using a random order of utterances in each training epoch to alleviate overfitting. Using on-line learning was found to drastically speed up convergence and increase generalization compared to batch learning. Zero mean Gaussian noise with standard deviation 0.1 is added to the input activations in the training phase, and an early stopping strategy is used in order to further help generalization. The latter is implemented as follows: we evaluate the overall SSE (5) on the development set after every fifth epoch. We abort training as soon as no improvement of the SSE on the development set has been observed during 30 epochs. The network that achieved the best SSE on the development set (across all six SNRs) is chosen as the final network.Most of the applied BLSTM networks have three hidden layers consisting of 2M, 128, and 2M LSTM cells as described above, where M is the input and output feature dimension (39 for MFCC, 54 for Log-FB). Each memory block contains one memory cell. This topology was empirically determined on a similar speech feature enhancement task (Wöllmer et al., 2013). In case that noisy features are mapped to clean features, we also consider networks with four hidden layers incorporating 2M, 128, 2M, and 2M LSTM cells. The rationale behind this is that mapping to clean features is a more complex task than just removing noise, which also involves de-reverberation. Besides training the four hidden layers without additional constraints, we also aim at enforcing structure by pre-training of the first three hidden layers. In particular, we add a fourth hidden layer to the three-layer network which has been trained to map noisy and reverberated to noise-free reverberated features, and then run additional training epochs using the same inputs, but clean features as targets. For the sake of consistency, the training parameters are set based on our previous experience with RNN-based enhancement of conversational speech in noise (Wöllmer et al., 2013). Our LSTM training software is publicly available.22https://sourceforge.net/p/currennt – last retrieved January 2014.To verify the effectiveness of BLSTM networks for feature enhancement, we also consider simpler network architectures: bidirectional RNNs (BRNNs) and feedforward neural networks (FNN). Bidirectional RNNs are obtained by replacingLt(n)in Eq. (2) by the hyperbolic tangent functiontanh(W(n−1),(n)ht(n−1)+W(n),(n)ht−1(n)), and in the case of FNN,tanh(W(n−1),(n)ht(n−1)). Since the latter does not take into account context which is vital for speech processing tasks, in the case of FNN we replace xtby[xt−T;…;xt+T]in Eq. (1) whereTis a fixed parameter representing the context length, i.e., features are stacked into a column ‘super’ vector. We useT=4, i.e., nine frame context windows. In analogy to RNNs, FNNs are trained on the task to provide a clean speech estimatesˆtof xt, which is the center frame of the context window.As FNN topologies, we investigate both ‘symmetric’ hidden layers (3 × 256 units) as well as a structure that reduces information layer by layer (486, 256, and 108 hidden units), matching the size of the first hidden layer to the input layer and the size of the third layer to two times the size of the output layer. BRNNs have the same size as BLSTM-RNNs (108, 128, 108 hidden units). Since BLSTM-RNNs have many more parameters than FNNs or BRNNs of the same hidden layer size, we also investigate a smaller BLSTM net (81, 96, and 81 hidden units) whose number of parameters compares to the simpler architectures. For a fair comparison, both BRNNs and FNNs were trained using the same stochastic gradient descent algorithm as BLSTM-RNNs, using random initialization. We tuned the learning rate for FNNs and BRNNs on the development set and found that best performance with FNN was obtained with 10−7, as opposed to 10−5 for the BLSTM-RNNs, requiring more training epochs until convergence. BRNNs required setting the learning rate as low as 10−8 in order for training to converge.As detailed above, the first step of ASR feature extraction is presenting the frame-wise noisy features (MFCC or Log-FB) x to the trained network and computing the denoised featuressˆas the output activations. In principle, cepstral mean normalized MFCC features with deltas output by a network can be used ‘as is’ in the speech recognizer. However, due to the normalization of the training targets,sˆwill be (approximately) mean and variance normalized, which does not match the features used to train the baseline models. Thus, to be able to use the enhanced features in a ‘plug-and-play’ fashion, i.e., without any recognizer modification, the global mean and variance normalization is reverted after obtaining the enhanced MFCC features, to ensure compatibility with the means and variances of the trained recognition models. More specifically, each enhanced feature vector is multiplied element-wise with the corresponding variances of the noise-free training set, and the mean feature vector of the noise-free training set is added. For the Log-FB features, deltas output by the network are thrown away, the MVN is reverted as above, and cepstral mean normalized MFCC features with delta and acceleration coefficients are computed from the Log-FB features output by the network.In the following, we now describe the speech recognition back-ends we use for evaluating our feature enhancement procedure.We evaluate the performance of the enhanced features using the baseline models provided by the Challenge organizers, as well as re-trained models using enhanced features. The baseline is implemented using HTK (Young et al., 2006) based on the WSJ-0 ‘recipe’ by Vertanen (2006). From these models, a ‘reverberated’ baseline model is generated by an Expectation Maximization (EM) Maximum Likelihood (ML) algorithm on the reverberated training set. Four EM-ML iterations are used. The ‘noisy’ baseline model is created by four additional EM-ML iterations using the training set with convolutive noise. From these ‘noisy’ models, we derive ‘re-trained’ models simply by repeating the multi-condition training step using features that have been processed by our enhancement networks. This is done to investigate to which extent distortions by enhancement can be compensated by model re-training. Furthermore, it is expected that feature de-noising and de-reverberation results in lower feature variance, requiring model adaptation. In contrast, using the baseline models without modification serves to estimate the ‘compatibility’ of enhanced features with their clean counterparts used to train the ASR models. From an application point of view, it corresponds to a ‘plug-and-play’ configuration – in other words, a scenario where the recognizer back-end is a ‘black box’ and only the feature extraction front-end is known.The training procedure used to generate the CHiME baseline models does not use many state-of-the-art ASR techniques, such as feature transformations and discriminative training. Thus, it is of crucial interest to investigate whether the performance of state-of-the-art ASR, such as the back-end used by Tachioka et al. (2013) for their (winning) contribution to the CHiME Challenge track 2, can also be improved by our feature enhancement technique. This system is implemented with the Kaldi speech recognition toolkit (Povey et al., 2011). The ‘recipe’ for training the back-end is publicly available.33http://spandh.dcs.shef.ac.uk/chime_challenge/WSJ0public/CHiME2012-WSJ0-Kaldi_0.03.tar.gz – last retrieved January 2014.Discriminative training is performed using boosted Maximum Mutual Information (MMI) as proposed by Povey et al. (2008). The MMI principle aims at maximizing the posterior probabilities of the correct utterances, given the trained models. Boosted MMI (bMMI) introduces a weight, strengthening the influence of hypotheses with a higher error. For bMMI, the objective function is(8)FbMMI(λ)=∑r=1Rlogpλ(Xr|Msr)κpL(sr)∑spλ(Xr|Ms)κpL(s)e−bA(s,sr),where r=1…R are the training utterances andXrthe corresponding feature sequences,Msis the HMM sequence of sentence s, sris the reference transcription of utterance r, κ is the acoustic scale, pλis the likelihood of the acoustic model with the parameters λ, and pLis the language model likelihood. The last term in the denominator is the boosting weight, where b>0 is the boosting factor and A(s, sr) is the phoneme accuracy of sentence s given the reference sr.Furthermore, techniques for feature transformation are employed. Feature transformation can improve the class separation and address the speaker variability in the training data. Linear discriminant analysis (LDA) is applied on stacked MFCCs and reduces the resulting high-dimensional feature vector to a smaller dimension. The necessary classes are obtained by aligning the tri-phone HMM states. By that, robustness to noise and reverberation can be addressed, assuming that these distortions occur in regular temporal patterns which can be expressed as feature dimensions not related to phonetic information, and hence be discarded. There are too few data to train full-covariance models, because of the high-dimensional acoustic feature space. Therefore, diagonal-covariance models, which do not consider correlations between features, are used instead. Several transformations for decreasing the correlations between features have been proposed. We use maximum likelihood linear transform (MLLT), as described in Saon et al. (2000). Additionally, large variations among speakers degrade the performance of the acoustic models. To address this problem, speaker adaptive training (SAT) (Anastasakos et al., 1997) is applied: before the ML training procedure, feature-space maximum likelihood linear regression (f-MLLR), which is the same as constrained MLLR (Gales, 1998), is applied to estimate a speaker-dependent transform for each speaker. The estimated transform is then used during model re-estimation in training. During decoding, speaker identities are assumed to be known. First, a tight-beam decoding is performed on all test utterances of a single speaker to obtain a first pass transcription, which is used to re-estimate the SAT transform, before doing a final decoding.Parameterization and training of acoustic models follows (Tachioka et al., 2013) and works as follows: 40 phonemes (including silence) are integrated in context-dependent triphone models with 2500 states and a total number of 15,000 Gaussians. First, models are trained with clean training data applying the ML principle. Next, ML training is continued with reverberated training data, using the alignments and triphone tree structures from the clean models. Then, isolated noisy training data are used for training. Another set of ML training iterations is then performed after applying the described feature transformations, using the noisy training data. Here, first, the 13 static MFCC coefficients of nine consecutive frames are concatenated together and LDA is applied to reduce the resulting 117 dimensional vector to 40 dimensions. The LDA uses the 2500 aligned tri-phone HMM states as classes. Subsequently, features are transformed using MLLT and model re-estimation is done. Afterwards, an f-MLLR transform is estimated for SAT, leading to another set of model re-estimation iterations. Based on the resulting acoustic models, discriminative training is performed with the noisy training data, using bMMI with a boosting factor of b=0.1.

@&#CONCLUSIONS@&#
