@&#MAIN-TITLE@&#
Similarity-balanced discriminant neighbor embedding and its application to cancer classification based on gene expression data

@&#HIGHLIGHTS@&#
A similarity-balanced discriminant neighborhood embedding (SBDNE) is proposed.SBDNE is applied to cancer classification using gene expression data.SBDNE constructs two adjacent graphs using a new similarity function.Experimental results on six microarray datasets show that SBDNE is promising.

@&#KEYPHRASES@&#
Discriminant neighborhood embedding,Adjacent graph,Gene expression data,Cancer classification,Microarray data,

@&#ABSTRACT@&#
The family of discriminant neighborhood embedding (DNE) methods is typical graph-based methods for dimension reduction, and has been successfully applied to face recognition. This paper proposes a new variant of DNE, called similarity-balanced discriminant neighborhood embedding (SBDNE) and applies it to cancer classification using gene expression data. By introducing a novel similarity function, SBDNE deals with two data points in the same class and the different classes with different ways. The homogeneous and heterogeneous neighbors are selected according to the new similarity function instead of the Euclidean distance. SBDNE constructs two adjacent graphs, or between-class adjacent graph and within-class adjacent graph, using the new similarity function. According to these two adjacent graphs, we can generate the local between-class scatter and the local within-class scatter, respectively. Thus, SBDNE can maximize the between-class scatter and simultaneously minimize the within-class scatter to find the optimal projection matrix. Experimental results on six microarray datasets show that SBDNE is a promising method for cancer classification.

@&#INTRODUCTION@&#
Recently, cancer classification using gene expression data has attracted a lot of attention in microarray data [1–11]. Usually, the gene expression data has only a few dozen sizes but has thousands to tens of thousands of genes among which only a very small fraction of them are informative for cancer classification. As a result, to achieve the analysis and visualization of gene expression data, we have to reduce the dimension of gene expression data. There are two ways to implement dimension reduction of gene expression data, feature selection [5–10] and feature extraction [1–4]. Here, we focus on feature extraction which maps the high-dimensional data into a low-dimensional feature subspace using a projection matrix.In feature extraction, the most classical methods are principal component analysis (PCA) [12] and linear discriminant analysis (LDA) [13]. Both methods or their variants have been applied to gene expression data [1,5,14,15]. PCA obtains an optimal projection matrix by maximizing the total scatter of training samples. Since PCA aims to optimally represent training samples themselves in an unsupervised way, PCA may not provide a good discriminant projection for classification tasks. LDA can find an optimal discriminant projection matrix by maximizing the ratio of the between-class scatter to the within-class scatter. These projection directions would be of benefit to classification tasks. However, LDA fails to explore the manifold structure of the given data when projecting them into a lower-dimensional subspace and requires the data to obey a Gaussian distribution. Margin Fisher analysis (MFA) [16], an extension of LDA, is able to efficiently solve the drawbacks discussed above. For MFA, the local structure of samples can be preserved by constructing the homogenous and heterogeneous neighbor adjacency graphs, and the optimal projection directions are found by minimizing the ratio of the sum of distance between the samples with the same class and the sum of distance between the samples with the different classes.PCA and LDA are linear methods, while MFA is a kind of manifold learning methods. It is well known that the classical manifold learning methods are locally linear embedding (LLE) [17], isometric feature mapping (ISOMAP) [18] and Laplacian eigenmap [19]. These methods shows that data may locate in some subspace and the nonlinearly inner structure of data cannot be learned by traditional methods. However, these classical manifold learning algorithms cannot perform mapping for an unseen data, which is also called the out-of-sample problem. To cover this shortage, many new methods have been proposed, such as locality preserving projection (LPP) [20], neighborhood preserving embedding (NPE) [21], and discriminant neighborhood embedding (DNE) [22]. Both NPE and LPP find an embedding to preserve local information and can be simply extended to unseen samples. The main difference between NPE and LPP is that they solve the generalized eigenvalue problem subject to different constraints. Since both LPP and NPE do not make full use of the class label information, they cannot work well in classification tasks. Thus, supervised methods for LPP and NPE have been developed, such as supervised locality preserving projections (SLPP) [23], discriminant locality preserving projection (DLPP) [24], neighborhood discriminant projection (NDP) [25], null space discriminant locality preserving projection (NDLPP) [26], and supervised neighborhood preserving embedding (SNPE) [27]. Different from LPP and NPE, DNE is a supervised manifold learning method itself. In DNE, the adjacency graph is constructed to distinguish between homogenous neighbor points and heterogeneous neighbors to keep the local structure. However, DNE cannot preserve the detailed position relationship between the samples and their neighbors. Thus, the recognition rate in the low-dimensional subspace is not good enough for a particular purpose when the data are unbalanced. Hidden space DNE [28] was developed to be a nonlinear version of DNE by introducing a nonlinear hidden function mapping into DNE. Locality-based discriminant neighborhood embedding (LDNE) was proposed in [29], which is to optimize the difference between the between-class distance and the within-class distance under constructing the adjacent graph being different from DNE and endowing different weights.The application of manifold learning methods to the gene expression data have been reported in [3,4,30,31]. In [3], an improved supervised orthogonal discriminant projection (SODP) was presented based on LPP for tumor classification. SODP can not only maximize the weighted difference between the non-local scatter and the local scatter but also preserve the locality information of data. Locally linear discriminant embedding (LLDE) was proposed for performing feature extraction on gene expression data [30]. In [31], three discriminant analysis methods: locality sensitive discriminant analysis, spectral regression discriminant analysis, and supervised neighborhood preserving embedding are applied to microarray data classification. For the projected data, we still need a classifier to deal with them. Many methods have been applied to cancer classification, such as naive Bayes classifier (NBC) [32], partial least squares discriminant analysis (PLSDA) [33], support vector Machines (SVMs) [34], kernel sparse representation-based classifier (KSRC) [34], and k nearest neighbor (kNN) [3,4,30,31]. In theory, any existing classifier could be applied to cancer classification. Among these methods, kNN is widely used for its simplicity.Since DNE-like methods have been successfully applied to face recognition tasks, it is possible to apply them to cancer classification using gene expression data. As far as we know, the possible application has not been reported. Here, we consider the application of a novel DNE algorithm to cancer classification. This paper proposes a new supervised dimension reduction method, called similarity-balanced discriminant neighborhood embedding (SBDNE), for cancer classification using gene expression data. By introducing a new similarity function, SBDNE deals with the between-class data and the within-class data in different ways. The homogeneous and heterogeneous neighbors are selected according to the new similarity function instead of the Euclidean distance. SBDNE constructs two adjacent graphs, or between-class adjacent graph and within-class adjacent graph, using the new similarity function. According to these two adjacent graphs, we can generate the local between-class scatter and the local within-class scatter, respectively. The goal of SBDNE is to find the optimal projection matrix by maximizing the difference between the between-class scatter and the within-class scatter.The remainder of this paper is organized as follows. Section 2 introduces methods available and proposes the novel algorithm. Simulation experiments are presented in Section 3 and conclusions are provided in Section 4.

@&#CONCLUSIONS@&#
