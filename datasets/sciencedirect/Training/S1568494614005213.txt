@&#MAIN-TITLE@&#
Training and testing a self-adaptive multi-operator evolutionary algorithm for constrained optimization

@&#HIGHLIGHTS@&#
Measure the performance of SAMO-GA on a diverse set of constrained problems.Extend the idea of training and testing with EAs for solving COPs.The mean square error measure is used to quantify the results.The results provide interesting insights and a new way of choosing parameters.

@&#KEYPHRASES@&#
Constrained optimization,Genetic algorithm,Cross validation,

@&#ABSTRACT@&#
Over the last two decades, many different evolutionary algorithms (EAs) have been introduced for solving constrained optimization problems (COPs). Due to the variability of the characteristics in different COPs, no single algorithm performs consistently over a range of practical problems. To design and refine an algorithm, numerous trial-and-error runs are often performed in order to choose a suitable search operator and the parameters. However, even by trial-and-error, one may not find an appropriate search operator and parameters. In this paper, we have applied the concept of training and testing with a self-adaptive multi-operator based evolutionary algorithm to find suitable parameters. The training and testing sets are decided based on the mathematical properties of 60 problems from two well-known specialized benchmark test sets. The experimental results provide interesting insights and a new way of choosing parameters.

@&#INTRODUCTION@&#
Constrained optimization is a challenging research area in the computer science, and optimization domains. In order to solve the complex decision problems, many problems are defined as constrained optimization problems (COPs). COPs can be divided into many different categories based on their characteristics and mathematical properties.Evolutionary algorithms (EAs) have a long history of successfully solving optimization problems regardless of whether or not they have nice mathematical properties. The EAs family contains a wide range of algorithms that have been used to solve optimization problems, such as the genetic algorithm (GA) [1,2], differential evolution (DE) [3], evolution Strategies (ES) [4], evolutionary programming (EP) [5]. A comparative study among some of these EAs is found in [6]. In this research, we consider DE for solving optimization problems, because DE usually converges quickly, incorporates a relatively simple and self-adapting mutation, and the same settings can be used for many different problems [3]. EA has the ability to deal with both continuous and discrete variables, and can deal with extremely complex fitness landscapes, including under noisy and dynamic environments.Due to the variability of problem characteristics, and their underlying mathematical properties, an algorithm that demonstrated to work well for one problem, or a class of problems, does not guarantee that it will work for another problem or a range of problems. This behavior can be argued as consistent with the no free lunch (NFL) theorem [7]. For this reason, new algorithms are continuously reported in the literature for solving new problems with different characteristics. In developing any new evolutionary algorithm, it is required to decide on two main aspects: (i) designing an appropriate algorithm and (ii) choosing the required algorithmic parameters. In reality, the algorithm design is not a single step process, but rather an iterative process. After the initial design, the algorithm is redesigned and refined until it meets some criterion. In most cases, the criterion is set as the quality of the solutions for a given set of test problems. Once the criterion is met, only the final form of the algorithm is reported. The parameters to be used in the algorithm are usually determined through learning from the test problems. Traditionally, numerous trial-and-error runs are usually performed to choose suitable parameters. However, even by trial-and-error, one may not find appropriate parameters. Such a parameter selection process is sometimes called parametric analysis. We can now argue that an algorithm with the chosen parameters may demonstrate adequate solution quality on the test problems that may not perform well with unknown problems. However, in evolutionary optimization domain, the usual practice is to compare the performance of any new algorithm with the existing algorithm(s) through solving a set of benchmark problems.From the above discussions, we can state that there is no single algorithm or an algorithm with a known search operator and given parameters, that will consistently perform best for all classes of optimization problems. This motivates us to consider multiple search operators for a better coverage of problems and a train-and-test based approach for appropriate parameter selection.Murata and Ishibuchi [8] examined the performance of different variants of GA with two types of genetic operators (each variant with one crossover and one mutation) in solving flow shop scheduling problems. As they have shown, an independently evaluated good operator may not perform well when it is used in conjunction with another operator (here a given crossover with a given mutation). They also revealed that the combined effect of two operators, which determines the effectiveness of the algorithm, could be either positive or negative. This means that the choice of operators is important in designing a high performing GA, further, this choice is often made by trial-and-error. In this research, our focus is on multi-operator based evolutionary algorithms, where multiple operators, of differing types, will be considered together within the framework of one algorithm. This type of algorithm can be seen not only as a better alternative over trial-and-error based designs, but also as a means to generate better coverage of problems. Multi-operator based evolutionary algorithms are not new in the literature. However, their actual ability for solving constrained optimization problems has not been fully explored. Also, the choice of operators and their appropriate mix, and strategies for their effective use, has not been well studied. A brief review of multi-operator based EAs is provided below.In the case of multiple operators based EAs, it is a popular practice to use adaptive approach. For example, multi-operator based EAs are not new in the literature. In multiple operators based EAs, it is a popular practice to use an adaptive approach. Spears [9] applied an adaptive strategy using two different crossovers for solving N-Peak problems. Eiben [10] developed an adaptive GA framework with multiple crossover operators for solving unconstrained problems. In their algorithm, the population was divided into a number of sub-populations, each of which used a particular crossover. Based on the success of the crossovers, the sub-population sizes were varied. However, their adaptive GAs did not outperform the standard GA using only the best crossover. Hyun-Sook and Byung-Ro [11] investigated whether a combination of crossover operators could outperform the usage of only the best crossover operator by solving the TSP and the graph bisection problem. They used an adaptive strategy to assign the probability of using different crossovers.In DE, Mallipeddi et al. [12] proposed an ensemble of mutation strategies and control parameters with DE (EPSDE), in which a pool of distinct mutation strategies, along with a pool of values for each control parameter, coexisted throughout the evolutionary process and competed to produce offspring. This algorithm was used to solve a set of unconstrained problems. Mallipeddi and Suganthan [13] proposed using a mix of four constraint handling techniques (CHTs) based on a DE algorithm (ECHT-DE) to solve COPs in which different populations were initialized and such that one of each CHT was assigned to each population. Furthermore, mixes of mutation strategies and amplification factor (F) and crossover rate (Cr) values were also used, along with the two mutation strategies “DE/rand/2/bin” and “DE/current-to-rand/1/bin”. The pools of Cr and F values were in the ranges of 0.1–0.9 and 0.4–0.9, respectively, and in steps of 0.1. This algorithm came second in the CEC2010 competition for COPs. However, it was expensive in terms of computational time. Mallipeddi and Suganthan [14] extended their previous work to solve a set of real-world applications [15]. However, their obtained results were not very good in comparison with those from other algorithms. Tasgetiren et al. [16] proposed a discrete DE algorithm with a mix of parameter values and crossover operators, in which parallel populations were initialized, to solve the TSP. Each parameter set and crossover operator was assigned to one of the parallel populations. Furthermore, each parallel parent population competed with the same population's offspring, as well as the offspring populations generated by all of the other parallel populations. Although this algorithm showed better results than other state-of-the-art-algorithms, it was computationally at least twice as expensive as those against which it was compared in the paper. Yong et al. have recently proposed a composite DE algorithm (CoDE) [17], in which the algorithm randomly combines several trial vector generation strategies with a number of control parameter settings at each generation to create new trial vectors. CoDE has been tested on a set of unconstrained problems and showed competitive performance in comparison to other state-of-the-art algorithms. Mallipeddi et al. [18] proposed an EP algorithm that used different mutation strategies, in which each mutation operator has its associated population and every population benefits from every function call. The algorithm has shown superior performance in comparison to other EP algorithms. Elsayed et al. [19] proposed a mix of four different DE mutation strategies within a single algorithm framework to solve COPs, and that algorithm showed good performance by solving a set of small scale theoretical benchmark constrained problems. The algorithm was further extended in [20,21]. Elsayed et al. [22] proposed two novel DE variants, in which each variant utilized the strengths of multiple mutation and crossover operators for solving 60 constrained problems. The algorithm showed competitive, if not better, performance in comparison to the state-of-the-art algorithms.We consider a self-adaptive multi-operator genetic algorithm (SAMO-GA) for the constrained optimization problem, which is much more complex than its unconstrained counterpart. In SAMO-GA, each combination of search operators has its own sub-population. Further, the subpopulation sizes vary adaptively, as the evolution progresses, depending on the reproductive success of the search operators. However, the sum of the size of all of the subpopulations is fixed during the entire evolutionary process. To do this, three equations have been introduced for determining the reproductive success based on the fitness values and the constraint violations. Lastly, to deal effectively an operator may perform very well at earlier stages of the evolution process and do badly at later stages or vice versa, a lower bound on the size of each subpopulation has been set.Therefore, the aim of this research is to measure the performance of SAMO-GA on a diverse set of constrained problems, in which we extend the idea of training and testing with EAs for solving COPs, in which we use a set of test problems that contain different properties for their objective functions and constraints [23,24]. Cross-validation [25] is then used to estimate the generalization ability of SAMO-GA. This is done, in such way that the problems are divided into three groups, with the consideration that each group contains the most diverse types of problems, as possible. Each two groups are then used for training, while the third one is used for testing. Finally, the mean square error measure [26] is used to quantify the difference between the obtained results and the best known results.We could not find any research dealing with the training and testing concept with evolutionary algorithms for the purpose of solving constrained optimization. One of the challenging issues for this concept, is how can we design the training and testing problems? As it is known in machine learning domain, the idea of separating the data into training and testing sets comes with a number of assumptions, such as both training and testing datasets come from the same distribution. In this paper, we have divided the test problems, from two well-known problem sets, into three groups based on their mathematical properties.This paper is organized as follows. After the introduction, Section 2 describes the design of SAMO-GA, and the constraint handling technique that is used in this research. The cross-validation and its common types are described in Section 3. The general framework of testing SAMO-GA is shown in Section 4. The experimental results and the analysis of those results are presented in Section 5. Finally, the conclusions are given in Section 6.In this section, we first describe a multi-operator genetic algorithm with self-adaptive parameter selection. We then introduce the improvement measure strategy for the adaptive selection of parameters. Finally, we describe the constraint handling technique that is used in this paper.In the evolution process, the relative performance of the search operators may vary with the progress of generations. This means that one operator may work well in the early stages of the search process, but may perform poorly at the later stages, or vice versa. So it is inappropriate to give equal emphasis on all the operators throughout the entire evolution process. To give higher emphasis on the better performing operators, we propose to change the subpopulation sizes through dynamic adaptation, basing those changes on the relative performances of the operators. We call this version of MO-GA as Self-Adaptive Multi-Operator Genetic Algorithm (SAMO-GA).Algorithm 1A self-adaptive multi-operator genetic algorithm (SA-MOGA)The basic steps of SAMO-GA are presented in Algorithm 1. SAMO-GA starts with a random initial population P which is divided into four subpopulations of equal size. Each subpopulation of individuals (pi) that evolves through their own crossover and mutation operators and the generated offspringOz={Oz1,}Oz2,…,Ozj,…,OzDare evaluated according to the fitness function value and the constraint violation of the problem under consideration. In SAMO-GA, an improvement index for each subpopulation is calculated using the method discussed below. Based on the improvement index, the subpopulation sizes are either increased, decreased or kept unchanged. As this process may abandon certain operators which may be useful at the later stages of the evolution process, we set a minimum subpopulation size for each operator. Also, after every few (WS) generations the best solutions among the subpopulations are exchanged. The algorithm continues until the stopping criterion is satisfied.To measure the improvement of each operator (/subpopulation) in a given generation, we consider both the feasibility status and the fitness value, where the consideration of any improvement in feasibility is always better than any improvement in the infeasibility. For any generation t>1, there arises one of three scenarios. These scenarios, in order from least to most desirable, are discussed below:(A)Infeasible to infeasible: For any subpopulation i, the best solution was infeasible at iteration t−1, and is still infeasible in iteration t, then the improvement index is calculated as follows:(1)VIi,t=|Vi,tbest−Vi,t−1best|avg⋅Vi,t=Ii,twhereVi,tbestis constraint violation of the best individual at iteration t andavg⋅Vi,tis the average violation. Hence, VIi,t=Ii,tabove represents a relative improvement as compared to the average violation in the current iteration.Feasible to feasible: For any subpopulation i, the best solution was feasible at iteration t−1, and still feasible in iteration t, then the improvement index is:(2)Ii,t=maxi(VIi,t)+|Fi,tbest−Fi,t−1best|×FRi,twhere Ii,tis the improvement for subpopulation i at generation t,Fi,tbestis the objective function for the best individual at iteration t, and FRi,tis the feasibility ratio of operator i at iteration tTo assign a higher index value to a subpopulation with a higher feasibility ratio, we multiply the improvement in fitness value by the feasibility ratio. To differentiate between the improvement index of feasible and infeasible subpopulations, we add a term maxi(VIi,t) in Eq. (2). If all the best solutions are feasible, then maxi(VIi,t) will be zero.Infeasible to feasible: For any subpopulation i, the best solution was infeasible at iteration t−1, and it is feasible in iteration t, then the improvement index is:(3)Ii,t=maxi(VIi,t)+|Vi,t−1best+Fi,tbest−Fi,t−1bv|×FRi,twhereFi,t−1bvis fitness value of the least violated individual in iteration t−1. To assign a higher index value to an individual that changes from infeasible to feasible, we addVi,t−1bestwith the change of fitness value in Eq. (3). We also keep the first term as Eq. (1).After calculating the improvement index for each subpopulation, the subpopulation sizes are calculated according to the following equation:(4)ni,t=MSS+Ii,t∑i=1NoptIi,t×(PS−MSS×Nopt)where ni,tis the subpopulation size for the ith operator at generation t, MSS is the minimum subpopulation size for each operator i at generation t, Nopt is the number of operators.In this paper, we consider the selection of individuals for the purposes of a tournament [22], as follows: (i) between two feasible solutions, the fittest one (according to fitness function) is better, (ii) a feasible solution is always better than an infeasible one, (iii) between two infeasible solutions, the one having the smaller sum of constraint violations is preferred. The equality constraints are also transformed to inequalities of the following form, where ɛ is a small value.(5)|hj(x→)|−ε≤0,forj=q+1,…,mAs of the literature, the goal of cross-validation is to estimate the expected level of fit of a model to a data set, independent of the data that was used to train the model. This is possible to design a quantitative measure of fit that is appropriate for the data and model under consideration. In cross-validation, the data or test problems are divided into two segments: one used to learn or train a model, and the other used to test that model. In typical cross-validation, the training and validation sets must cross-over in successive rounds, such that each data point has a chance of being validated against.There are different forms of cross-validation, such as (i) k-fold cross-validation [27], where the data is first partitioned into k equal (or nearly equal) sized segments or folds. Subsequently k iterations of training and testing are performed, such that within each generation a different fold of the data is held-out for validation while the remaining k-1 folds are used for learning. (ii) Leave-One-Out Cross-Validation (LOOCV) which is a special case of k-fold cross-validation where k equals the number of instances in the data. In other words, in each generation nearly all the data, except for a single observation, are used for training and the model is then tested on that single observation. An accuracy estimate obtained using LOOCV is known to be almost unbiased, but it has high variance which leads to unreliable estimates [25]. LOOCV is widely used when the available data are very rare, (iii) repeated k-Fold Cross-Validation [27]: a commonly used method to increase the number of estimates is to run the k-fold cross-validation multiple times, where the data is reshuffled and re-stratified before each round, (iv) hold-Out cross-validation [28] which splits the available data into two non-overlapped parts: one for training and the other for testing. The test data is then held out and not looked at during training. Hold-out validation avoids the overlap between training data and test data, thus yielding a more accurate estimate of the generalization performance of the algorithm. The drawback is that this procedure does not use all the available data and that the results are highly dependent on the choice made for the training/test split.In this section, we first describe the proposed framework to test and train any evolutionary algorithm.1.Divide the test problems into k segments, in which each segment contains a set of different constrained problems.For k iterations:2.1Train the evolutionary algorithm by using different parameter values to solve the test problems of k−1 segments.Select the best combination of parameters, based on the obtained results of the k−1 segments.Use the selected parameters, and solve the last segment.Calculate the mean square error by using the following equation:(6)MSEk=∑i=1n(xi,opt−xi)2nwhere xi,optis the known best value for problem i, xiis the obtained result of problem i using the evolutionary algorithm, and n is the number of test problems that are used for testing.Calculate the total average of MSE as follows:(7)MSE=∑1kMSEkkIn this section, we first present our approach for dividing the test problems into segments, and the computational results and analysis for SAMO-GA.Form a machine learning prospective, when training and testing a model, it is important to group a set of test problems that contain different properties. In our work, the available number of constrained problems in the literature is small. We grouped all of the test problems from two data sets, the first set contains 24 test problems that were presented in CEC2006 [18], while the other set which was presented in CEC2010 [19] contains 36 test problems (18 test problems with 10 dimension and another 18 test problems with 30 dimensions). The details of these test problems are shown in Tables 1 and 2. These problems contain different characteristics, such as unimodal, multi-modal, separable, non-separable, shifted and/or rotated functions, equality and/or inequality constraints with tiny feasible regions, and problems for which the optimal solution lies on the boundary.The process of dividing the division into groups can be done randomly, in which each problem is assigned to a random segment. Alternatively, in our work we need to divide them more effectively, so that each group should contain different types of characteristics. In this work, we use 3 segments as shown in Table 3.In this sub-section, we train SAMO-GA using different parameter values: the minimum sub-population size (MSS=5%, 10%, 15% and 20% of PS), the window size parameter (WS=1, 25, 50 and 75 iterations), and the population size (PS=40, 60, 80, and 100). For the other parameters the tournament size is set to 2 if the subpopulation size is less than or equal to 20, otherwise it is 3, crossover rate is 100% and mutation rate is 10%. 25 runs have been used for each problem, and the total number of fitness function evaluations (FEs) is set to240,000 (less than half of what was used in CEC2006) for g-problems, while FEs for C-problems are set to 200,000 and 600,000 for 10D and 30D, respectively. Unfortunately, because of page limitations, we cannot present the detailed results (best, median, mean, worst, and standard deviation) for all training and testing iterations, but they are available upon request.To determine the best parameter, we use the following method:Rank SAMO-GA with each parameter, from 1 to 4, based on their solution's quality using a simple scoring scheme. To rank the variants, we assign a score of ‘1.0’ if a variant obtains the best fitness value for a given test instance and ‘0.0’ if a variant fails to achieve any feasible solution If a variant achieves a feasible solution, but not the best fitness, it will receive a fractional score (between 0 and 1) as discussed below. We assume that all the problem instances have a minimization objective function.For an algorithm, r, a test instance, y, and a total number of test problems, Y, Fryis defined as the actual fitness and BFy=minr(Fry), and WFy=minr(Fry) the overall best and worst fitness values for y, respectively. Then, the score of r for y is:(8)Sry=1−|Fry−BFy|a×(|BFy−WFy|),ifFrjisfeasible0otherwisewhere a≥1 and ζ>1. A value of a>1will differentiate between the worst feasible and any infeasible solution by having a small positive value of Sry. Higher values of ζ will place greater emphasis on good solutions. In this thesis, a=1.1 and ζ=2 are used. In a similar way, scores for averages can also be calculated, in which case, the final score for an algorithm, r, can be determined as:(9)FSry=(υ×∑y=1YSrybest+(1−υ×)∑y=1ySryaverage)×FRrywhere FSryis the final score of algorithm r for test problem y, FRrythe feasibility ratio of variant r for test problem y,Srybestthe score based on the best solution,Sryaveragethe score based on the average values and υ a constant with a value between 0 and 1. It is worthy to mention here that the mutual relationship among all algorithms has been considered in Eq. (9). Therefore, any change in α and/or ζ will affect all other algorithms, and thus the best algorithm will still have the highest score. As a consequence, there is no need to analyze these parameters.Higher values of υ (1 or close to 1) will place greater emphasis on the best solutions which is appropriate when only the best fitness value is of interest. Lower values of υ (0 or close to 0) will more highly emphasize average solutions which are appropriate when there is interest in a number of good alternative solutions. In this thesis, υ=0.5 is used to strike a balance between the best and average results.The overall score, OSr, for each algorithm can then be calculated using the following equation:(10)OSr=∑yFSryAs we mentioned earlier, we are using 3-fold cross validation, and therefore there are three main iterations. Below we will show the analysis of each iteration.1.Iteration one:In this iteration, group one and two are used for training, while group three is used for testing.During the training process, we vary one parameter and fix the others, until we found the best value. Then we use it while varying second parameters, and then the same for the third parameter.Iteration two:In this iteration, group one and three are used for training, while group two is used for testing.During the training process, we vary one parameter and fix the others, until we found the best value. Then we use it while varying second parameters, and then the same for the third parameter.Iteration three:In this iteration, group two and three are used for training, while group one is used for testing.During the training process, we vary one parameter and fix the others, until we found the best value. Then we use it while varying second parameters, and then the same for the third parameter.

@&#CONCLUSIONS@&#
