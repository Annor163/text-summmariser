@&#MAIN-TITLE@&#
Audiovisual temporal integration in reverberant environments

@&#HIGHLIGHTS@&#
Explores perceived audiovisual synchrony in reverberant environments.The temporal smear caused by reverberation can alter a signal’s acoustic signature.Reverberating acoustics is a common problem in teleconferencing systems.Reverberation may not have adverse effects on the perceived synchrony for continuous audiovisual speech.The temporal integration of speech syllables and isolated events is affected by the acoustic phenomenon.

@&#KEYPHRASES@&#
Audiovisual speech,Audiovisual asynchrony,Temporal integration,Reverberation,

@&#ABSTRACT@&#
With teleconferencing becoming more accessible as a communication platform, researchers are working to understand the consequences of the interaction between human perception and this unfamiliar environment. Given the enclosed space of a teleconference room, along with the physical separation between the user, microphone and speakers, the transmitted audio often becomes mixed with the reverberating auditory components from the room. As a result, the audio can be perceived as smeared in time, and this can affect the user experience and perceived quality. Moreover, other challenges remain to be solved. For instance, during encoding, compression and transmission, the audio and video streams are typically treated separately. Consequently, the signals are rarely perfectly aligned and synchronous. In effect, timing affects both reverberation and audiovisual synchrony, and the two challenges may well be inter-dependent. This study explores the temporal integration of audiovisual continuous speech and speech syllables, along with a non-speech event, across a range of asynchrony levels for different reverberation conditions. Non-reverberant stimuli are compared to stimuli with added reverberation recordings. Findings reveal that reverberation does not affect the temporal integration of continuous speech. However, reverberation influences the temporal integration of the isolated speech syllables and the action-oriented event, with perceived subjective synchrony skewed towards audio lead asynchrony and away from the more common audio lag direction. Furthermore, less time is spent on simultaneity judgements for the longer sequences when the temporal offsets get longer and when reverberation is introduced, suggesting that both asynchrony and reverberation add to the demands of the task.

@&#INTRODUCTION@&#
Teleconference systems have evolved from being a direct communication platform between two individuals to becoming an extended meeting arena for larger groups of people. With larger groups and larger meeting rooms come larger challenges to tackle, such as reverberating sound components that tend to extend in time as the room size and source distance increase (de Lima et al., 2009). Reverberation is the consequence of the acoustic response from an enclosure (ITU, 2009), characterised by the temporal smearing of an auditory signal. Unlike an echo, which returns one distinct acoustical response, reverberation arises as a mix of acoustical responses from the multiple surfaces of the enclosed space (de Lima et al., 2009). Thus, the sound that finally reaches the ear is a combination of the acoustic waves that have been conveyed directly, and the reflected ones that have been delayed in time (Assmann and Summerfield, 2004). Both the strength and the length of reverberations contribute to influence the experience of audiovisual (AV) quality (Jumisko-Pyykkö et al., 2007). In speech, the resulting effect not only disturbs the experienced quality (de Lima et al., 2008), but also alters the signature and intelligibility of the spoken sounds (Cox et al., 1987).Specifically, reverberation may transform dynamic speech phonemes into more static elements, thereby flattening formants and blurring the onset and offset of certain consonants and vowels, while extending others (Assmann and Summerfield, 2004). Compared to quiet conditions, reverberation makes it difficult to discriminate pitch (Sayles and Winter, 2008) and it can create confusion among vowels (Cox et al., 1987). For example, the perception of reverberant speech will typically merge the two-vowel sound of a diphthong into a single-vowel monophthong (Nábĕlek, 1988). Furthermore, confusion related to consonant place of articulation and voicing has also been established (Cox et al., 1987), especially for consonants that follow a vowel at the end of a word (Gelfand and Silman, 1979). In line with Kurtovic’s model (1975, described in Gelfand and Silman, 1979), the energy reflected from the preceding vowel is believed to mask the subsequent consonant and thereby make the articulation features less intelligible. This masking would be far less detrimental for a consonant in a word-initial position.In addition to altering speech sound intelligibility, reverberation leads to confusion in the arrival of an auditory signal, hampering the perceptual capacity to discern the precedence to one ear before the other (Hartmann, 1983). Because this precedence, or interaural time difference, is an important cue for localising sound sources, reverberation contributes to difficulties in establishing the origin of a sound and even retaining attention to it (Culling et al., 1994; Darwin and Hukin, 2000). Relatedly, when tone series are presented in simulated reverberation, as opposed to quiet conditions, it is harder to keep in synchrony with the presented tempo (Naylor, 1992). According to Naylor, the tone envelopes become smoothed to the extent that the tail of one could overlap the onset of the next. This implies that reverberation not only alters the acoustical properties of speech sounds, but acts also on the auditory perception of less complex signals. Moreover, a reverberant environment can hinder sound localisation processes. In a natural environment with several people engaged in a conversation, binaural cues would normally assist in locating the speaker; however, in a teleconference, the reverberation that could arise from the transmission would be detrimental to this process (Nunes et al., 2011). Moreover, the potential disturbance from background noises and voices may serve to enhance the problem of reverberation in teleconferences.The current study considers simulated reverberation and reverberation recorded from two distinct teleconference rooms. However, instead of looking into the established effect on auditory speech intelligibility, we here explore the potential influence of auditory smearing on temporal perception. Whereas many earlier works have been restricted to auditory perception (Culling et al., 1994; Darwin and Hukin, 2000; de Lima et al., 2008), the current investigation extends this line of research to include not only auditory perception, but also the visual modality. While background noise typically will increase perceptual dependence on visual input (Alm et al., 2009; Sumby and Pollack, 1954), less is known about the perceived correspondence between vision and hearing in the presence of reverberation. One study used reverberant depth cues to demonstrate perceptual alignment to simulated source distances, where greater distances required auditory signals to lag further behind the visual signals for perceived subjective synchrony (Alais and Carlile, 2005). In other words, when the auditory and visual signals happened at the exact same moment in time, participants would not perceive that the two happened simultaneously. Another study found less accurate temporal order judgements for spatially and temporally separated AV signals in reverberant conditions, compared to anechoic conditions (Sankaran et al., 2013). Combined, these findings point to a decreased sensitivity to temporal offsets in the presence of reverberation. Despite the relevance to teleconference systems, as far as we know, no study has been carried out to directly explore the impact of reverberation on the perceived synchrony between auditory and visual speech signals.Synchrony remains a highly relevant challenge in teleconferencing. Due to the encoding, compression and transmission of audio and video, a temporal misalignment can take place and the two streams will be separated in time (Bang et al., 2009). Short temporal offsets are rarely noticeable, but once they exceed certain durations, they can be detrimental to both the subjective experience of quality (Steinmetz, 1996) and the intelligibility of spoken sounds (Grant and Greenberg, 2001). Nevertheless, no fundamental thresholds mark the transition from perceived synchrony to perceived asynchrony (Roseboom et al., 2009); instead, they vary with the measure and the nature of the AV event (van Eijk et al., 2008). For instance, perceptual tolerance to asynchrony is typically greater for spoken words and sentences than for more action-oriented events, such as a hitting hammer (Conrey and Pisoni, 2006; Dixon and Spitz, 1980). Moreover, the perceptual tolerance to temporal offsets is inherently asymmetric (Maier et al., 2011). Thus, the points of detection tend to reflect a lesser tolerance to asynchrony where the auditory signal precedes the visual signal (audio lead) than to asynchrony where the visual signal arrives first (audio lag) (Dixon and Spitz, 1980). These points are typically represented as thresholds, and are defined by the temporal offset required for synchrony to be perceived at a given rate, for instance 50% of the time (Conrey and Pisoni, 2006). The thresholds also define the window of temporal integration (Keetels and Vroomen, 2012), within which sensory inputs from two modalities are considered to be aligned in time.Perceived synchrony in speech varies depending on the sound, with asynchrony noticed at shorter offsets for bilabial stops than for the less visibly articulated velar and alveolar stops (Vatakis et al., 2012). Although the two sounds are distinct in their manner of articulation, the temporal integration of labiodental fricatives is fairly similar to that of bilabial stops (Vatakis et al., 2012). However, the more gradual frequency increase in the voicing of fricatives (McMurray et al., 2008) and the more noise-like nature of their articulation (Schwartz et al., 2012) could make fricatives more vulnerable to acoustic disturbances such as reverberation. For continuous speech stimuli, the window of temporal integration tends to be fairly extensive. For example, in a temporal order experiment where stimuli included a spoken sentence, the audio lead and lag thresholds were established at 118ms and 190ms, respectively (Vatakis and Spence, 2006). Another study assessing the perception of simultaneity for spoken sentences found thresholds at 131ms for audio lead and 225ms for audio lag (Conrey and Pisoni, 2006). Yet another experiment used a discrimination task where participants had to indicate which of two spoken sentences was out of synchrony, for the unfiltered speech stimuli the audio lead threshold was approximately 60ms and the audio lag threshold was around 230ms (Grant et al., 2003). These results illustrate how diverse windows of temporal integration can be, but they still have two things in common. One is a characteristic asymmetry, which portrays how the perceptual system is more sensitive to asynchrony when the auditory signal arrives before the visual, than vice versa. The other relates to the extent of the reported thresholds for continuous speech stimuli, the audio track can lag behind the video with more than 200ms and still go unnoticed. Thus, the perceptual tolerance to asynchrony in continuous speech is asymmetric, but fairly robust.This study addresses two technical challenges relevant to teleconference systems, reverberation and asynchrony, and explores the temporal integration of visual signals with reverberant auditory signals in two experiments. Knowing that reverberation alters the temporal signatures of certain speech characteristics (Cox et al., 1987), and may even mask subsequent speech sounds (Gelfand and Silman, 1979), we assume that increased reverberation will lead to increased temporal distortion. By creating a temporal ambiguity, we expect reverberation to contribute to a greater perceptual tolerance to AV asynchrony by widening the temporal window of integration. Furthermore, due to the extended masking that may result when the audio precedes the video, we believe that the increased tolerance will be most prominent for auditory lead asynchrony. To explore whether the temporal distortion expected from reverberation may differ between speech sounds and less complex auditory signals, the first experiment looks at two speech scenarios, as well as an action-oriented scene. This experiment also addresses the time spent by participants when evaluating the simultaneity of AV presentations at different levels of asynchrony and reverberation. Because uncertainty and cognitive load may vary across conditions, the evaluation times could contribute with insights on the effort required to make simultaneity judgements. The second experiment follows up the first findings by focusing on isolated speech events to investigate the possibility that the continuity of spoken sentences can serve to mask the reverberating elements that follow the speech signals. The inclusion of two syllables with dissimilar manners of articulation allows for an exploration of the potential difference in temporal integration, along with the influence from reverberation.The first experiment explores the perception of synchrony for continuous speech in reverberant conditions that simulate two different teleconference rooms. We aimed to make the setting realistic and to use stimuli with natural speech production and a familiar action event.

@&#CONCLUSIONS@&#
Following the premise that reverberation leads to an alteration of the temporal signature of an acoustic signal, this study has investigated the influence of a reverberant environment on AV temporal perception. Considering the technical challenges related to teleconference platforms, and the associated perceptual consequences, the study has specifically addressed how the co-occurrence of asynchrony and reverberation affects the temporal integration of both continuous and isolated AV speech. In addition, an action-oriented sequence was included to shed light on the differences in temporal integration that are often found between speech and culminating events.With respect to the two long speech sequences, the temporal smearing resulting from reverberation had no impact on the temporal integration of the auditory and visual signals. Similar to previous findings (Conrey and Pisoni, 2006; Dixon and Spitz, 1980; Grant et al., 2003; Vatakis and Spence, 2006), the current thresholds for perceived synchrony fell in favour of a temporal robustness when it comes to continuous speech. Moreover, if these results are applicable to a broader speech context, they imply that the temporal ambiguity that arises from reverberations could be masked by subsequent speech events. In natural and dynamic speech, one syllable tends to follow the other, and this could possibly cover up or work against the temporal smearing. All in all, the established windows of temporal integration, with and without reverberation, indicate that the perceptual system operates with a temporal buffer when integrating sensory signals across modalities and this buffer is particularly resilient to auditory signals lagging behind. The implications of such a theoretical buffer offer good news to providers of teleconference systems, suggesting that perfect objective synchrony is not required for these services.On the other hand, when speech is presented as isolated syllables, with no preceding or succeeding speech sounds to mask the acoustic tail, the temporal smearing from reverberation becomes influential on the perceptual integration process. Under reverberant conditions, the windows of temporal integration extended further in the audio lead direction for both the bilabial stop and the labiodental fricative. This finding adds support to our assumption that the continuity of everyday speech may contribute to a masking of the acoustical tail that arises from reverberation. Although this assumption could alleviate concerns with respect to the applied teleconference scenario, we would rather emphasise the implications of the uncovered effect. Possibly, the impact of reverberation is exclusive to isolated events, speech or otherwise, but even when absent, the effect may merely be masked. If so, the presence of reverberation is still affecting the perceptual system and may even add to the cognitive load, which in turn would make a conversation in reverberant environments more demanding for the listener. Despite all the efforts put into making teleconferences as natural as possible, people still tend to find these systems more exhausting in the long run than face-to-face communication. As a consequence, reverberation may decrease the endurance of those engaged in a teleconference.As surmised from related studies (Dixon and Spitz, 1980; Vatakis and Spence, 2006), the temporal integration does indeed differ between the AV action-oriented event and the continuous speech sequences. In the absence of reverberation, the temporal thresholds for the Chess sequence show a more symmetrical distribution between audio lead and lag asynchrony. Likely, the distinctiveness and relative slow movement of the chess piece touching the board, as compared to the dynamics of continuous speech, create an uncertainty regarding the specific moment of impact. This ambiguity may in turn extend the window of temporal integration. Further uncertainty arises from the reverberant environments, with the extension of the isolated signals’ acoustical tails. Combined with the visual ambiguity of the chess move, the extended tail of the chess sound could facilitate an auditory capture of the delayed visual event. In fact, with reverberation smearing the sound of the Chess sequence, the magnitude of the window of temporal integration does not change much. However, it is shifted even further in the audio lead direction. Although audio lead asynchrony could go almost unnoticed for an event with an ambiguous moment of impact, the reverberant environment has a clear detrimental effect on AV temporal integration.Judging from the windows of temporal integration, particularly without reverberation, we assume that audiovisual synchrony is harder to perceive for isolated syllables than for continuous speech. Most noticeable are the audio lead thresholds for the /ba/ and /va/ syllables, that are approximately 100ms longer compared to the two long speech sequences. This greater temporal tolerance is likely linked to the single audiovisual event contained within the syllable stimuli. In contrast, the 13s duration of the long speech stimuli provide numerous audiovisual articulations that serve as reference points to the temporal alignment of the two modalities. With regards to the syllables, we also observed a difference in the temporal integration, where the significant interaction with asynchrony points to greater temporal sensitivity for the voiced, bilabial stop /ba/ compared to the voiced, labiodental fricative /va/. Previous studies on the temporal integration of isolated speech syllables have not found the same difference between similar speech sounds (Vatakis et al., 2012). We surmise that our uncovered effect could be the result of the experimental task. We asked our participants to judge the audiovisual synchrony, whereas Vatakis and colleagues used temporal order judgements. Some have speculated that the SJ task could be more sensitive to temporal variations between audiovisual stimuli (Vatakis et al., 2008), and this could explain how we found a difference between the bilabial and labiodental syllables.With evaluation times as an added measure, we also assessed potential variations in cognitive load that could be attributed to asynchrony and reverberation. Our analyses show that the time spent on judging simultaneity did differ according to perceived synchrony. With the exception of the Chess sequence presented with audio leading the video, time spent on judging conditions with extreme temporal offsets was shorter than for presentations closer to objective synchrony. Fully synchronous presentations also tended to be evaluated more quickly than presentations with asynchrony levels close to the temporal thresholds, which are harder to discern. For the Chess sequence, where reverberation led to an extremely tolerant temporal perception of audio lag asynchrony, evaluation times were shorter when compared to the quiet condition. Again the presumed cognitive load of the simultaneity judgement task was associated with time spent on the task. Consequently, these results demonstrated perceptual consequences that go beyond temporal integration. AV presentations that were perceived as synchronous more than half of the time were still affecting the cognitive processing, adding load to an already hardworking system. As mentioned, the potential added cognitive load caused by reverberant environments might have adverse consequences for teleconference participants.The increased tolerance to audio lead asynchrony observed for the syllables and the Chess sequence is bound to affect perceptual processes beyond temporal integration. In the best case, reverberation may only affect the perceived quality; in the worst case, it may also affect the intelligibility of the AV content. Indeed, past research has demonstrated the detrimental effect of reverberation on many auditory perceptual processes, among them vowel (Cox et al., 1987; Nábĕlek, 1988) and consonant (Cox et al., 1987; Gelfand and Silman, 1979) comprehension, along with sound localisation (Culling et al., 1994; Darwin and Hukin, 2000). Accordingly, semantic and phonetic AV processes could very well suffer in the presence of reverberation. While teleconferencing was the main focus of the study, few enclosed environments are free from this acoustical disturbance. With our findings we have demonstrated the potential implications on the perception of AV synchrony, but we assume that the temporal integration process does not take place in isolation. Other perceptual processes are likely to be affected, and the cognitive demands are equally likely to escalate.