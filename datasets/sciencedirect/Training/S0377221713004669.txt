@&#MAIN-TITLE@&#
On the convergence of inexact block coordinate descent methods for constrained optimization

@&#HIGHLIGHTS@&#
We provide an inexact decomposition scheme for large scale optimization.We state global convergence results.We show that different line search methods can be embedded in the framework.

@&#KEYPHRASES@&#
Nonlinear programming,Block coordinate descent methods,Inexact decomposition methods,Gradient projection,Frank–Wolfe,

@&#ABSTRACT@&#
We consider the problem of minimizing a smooth function over a feasible set defined as the Cartesian product of convex compact sets. We assume that the dimension of each factor set is huge, so we are interested in studying inexact block coordinate descent methods (possibly combined with column generation strategies). We define a general decomposition framework where different line search based methods can be embedded, and we state global convergence results. Specific decomposition methods based on gradient projection and Frank–Wolfe algorithms are derived from the proposed framework. The numerical results of computational experiments performed on network assignment problems are reported.

@&#INTRODUCTION@&#
Let us consider the problem(1)minxf(x),s.t.x∈F=F1×F2×⋯×FL⊂Rn,wheref:Rn→Ris a continuously differentiable function,Fh⊆Rnh,h∈{1,…,L}, are compact convex sets, and n1+⋯+nh+⋯+nL=n. Accordingly to the structure of the feasible set we partition the vector of variables asx=(x(1),…,x(h),…,x(L))T,wherex(h)∈Rnh,h∈{1,…,L}, is the hth block component.We assume that the dimension nhof each block h is extremely large, so we are interested in studying decomposition-based methods.In some cases, related to the structure of the objective function and/or to the structure of the factor setsFh, it could be convenient to sequentially operate on the block components x(h). With this in mind, in this work we focus on the class of block coordinate descent methods. In this context, the Gauss–Seidel algorithm is one of the most popular method: at each iteration k, given the current point xk, the block components x(h) are sequentially updated by exactly solving the corresponding subproblems, that is, by setting for h∈{1,…,L}(2)x(h)k+1∈argminx(h)∈Fhfx(1)k+1,…,x(h-1)k+1,x(h),x(h+1)k,…,x(L)k.The literature on the convergence of exact decomposition algorithms (as the above Gauss–Seidel algorithm) is wide (see, e.g., [3,9,13,15]). Note that the computation of the exact solutions of the generated subproblems may be expensive whenever these latter do not have a particular structure. This has motivated several studies on inexact decomposition methods [14,21,4,17–19,23,20].We observe that in some applications (for instance, like those concerning network equilibrium problems [7]) it is convenient to work on a lower-dimensional subsetFh(xk)ofFh, that in general depends on the current feasible iterate xk. This will allow to solve problems in which the number of variables in each block is so huge that it is not reasonable to completely enumerate them a priori. In these cases it is suitable to adopt a column generation strategy, for which only the variables needed to reach optimality are iteratively added. Another case that can greatly benefit from this strategy is when a restriction of the feasible set can give some kind of computational advantages, as for instance the possibility to compute efficiently the objective function.On these bases we can define, for instance, a Gauss–Seidel decomposition method operating on the restrictions of the factor setsFh. Formally, the updating rule (2) could be modified as follows:(3)x(h)k+1∈argminx(h)∈Fh(xk)fx(1)k+1,…,x(h),…,x(L)k.A key issue regards the properties of the setsFh(xk)(replacingFh) needed to ensure convergence to the above scheme based, as the standard Gauss–Seidel method, on exact minimizations. Furthermore, for the reasons already explained, we are interested in designing decomposition methods that do not necessarily require the computation of the exact solutions of the generated subproblems.We will define a general decomposition framework with guaranteed theoretical convergence properties, such that:(i)it is possible to consider restrictions of the factor setsFhin order to manage the huge number of variables nh;the generated subproblems can be inexactly solved whenever the computation of their exact solutions is too expensive;there is some degree of freedom in the selection of the blocks.The above points distinguish the present work that of some cited papers. Convergent inexact block-descent methods for smooth problems are proposed in [4,18]. This latter work, as well as [17], presents also convergence rates results of the defined algorithms. The very recent paper [19] provides a unified convergence analysis for a general class of inexact block-descent methods even for nonsmooth problems. Various types of updating rules can be embedded in the presented general framework, including the cyclic updating rule, the Gauss–Southwell update rule or the overlapping essentially cyclic update rule. However, the general inexact decomposition schemes proposed in [4,18,19] do not admit the possibility of operating on restrictions of the factor sets, thus precluding the possibility of employing column generation strategies.Inexact decomposition algorithms for specific (linearly constrained) problems are presented in [14,21,23,20], but, differently from our general framework, they require specific assumptions on the factor sets and on the objective function.Concerning point (i), we state some properties to ensure that the restrictions of the factor sets yield a suitable representation of these latter in a lower dimensional subspace. Regarding points (ii) and (iii), we introduce general conditions on the block selection, and on the iterative mappings operating on the restrictions of the factor sets to guarantee global convergence properties. As a result, we present a general inexact decomposition framework whose key elements are the definition of lower dimensional restrictions of the factor sets, and the employment of line search based iterative minimization mappings defined on the introduced restrictions of the factor sets. Starting from the decomposition framework, specific convergent algorithms, based on gradient projection and Frank–Wolfe directions are designed. Summarizing, the main aim of the paper is to develop a unifying global convergence theory for inexact block coordinate descent methods possibly applied in connection with column generation strategies.The paper is organized as follows. The decomposition framework is presented in Section 2, as well as its convergence properties and the required assumptions. In Section 3 we describe two known line search mappings, the standard Armijo-type line search and a derivative-free line search, and we recall their theoretical properties. Two inexact decomposition methods, based on gradient projection and Frank–Wolfe directions, are defined in Section 4. In Section 5 we analyze a wide class of problems that fulfill the assumptions required for the convergence of the proposed framework. Finally, in Section 6 we report the numerical results of computational experiments performed on network assignment problems.Notation. We suppose that the vectorx∈Rnis partitioned into component vectorsx(h)∈Rnh. Note that we use bracketed subscripts to denote a subvector. An additional subscript is used to identify a variable of a specific block, e.g. we denote by x(h), ithe variable i of block h. The partial gradient of f with respect to x(h), evaluated at x, is indicated by∇(h)f(x)∈Rnh. A critical point for problem (1) is a pointx¯∈Fsuch that∇f(x¯)T(x-x¯)⩾0for everyx∈F, where∇f(x)∈Rndenotes the gradient of f at x. Finally, we indicate by ∥·∥ the Euclidean norm (on the appropriate space).In this section we propose an inexact decomposition framework in which at each iteration a block of variables is chosen and the corresponding sub-problem is inexactly solved. The key issues of the proposed decomposition framework are the introduction of suitable low dimensional restrictions of the sets whose Cartesian product yields the feasible set, and the employment of block descent methods to inexactly solve the generated subproblems. First we formally define the restriction of a setFh.Definition 1Given a pointy∈F, for h∈{1,…,L}, we denote byFh(y)the restriction ofFhat y, a closed convex set such thatFh(y)⊆Fh.The proposed algorithm, named IDA, is depicted in Algorithm 1.In order to perform the convergence analysis of Algorithm IDA, we need to introduce suitable assumptions on the setsFh(·), on the block selection, on the search direction dk, and on the line search procedure that computes the stepsize αk.The first assumption is on the properties of the setsFh(·)for h∈{1,…,L}.Assumption 1(i)The number of possible restricted setsFh(x), with x varying inF, is finite.LetK⊆Nbe an infinite subsequence such that, for allk∈K,xk∈FandFh(xk)=Fh★∀k∈K.Assume thatxk→x¯for k∈K and k→∞. If∇(h)f(x¯)T(x(h)-x¯(h))⩾0∀x(h)∈Fh★,then it holds∇(h)f(x¯)T(x(h)-x¯(h))⩾0∀x(h)∈Fh..We observe that (i) of Assumption 1 is needed to ensure that along an infinite sequence of iterates it is possible to extract a subsequence for which the same restriction of the factor set is selected. Condition (ii) requires that the restrictionFh(xk)captures, in the limit, the geometry of the setFhin terms of optimality conditions. Examples of restrictions satisfying Assumption 1 are defined in Section 5 with reference to a general class of linearly constrained problems (see Proposition 8).A second assumption requires that each block component x(l) for l∈{1,…,L} is periodically considered at Step 3 within a prefixed maximum number of iterations.Assumption 2There exists an integer M>0 such that, for all k⩾0 and for all l∈{1,…,L}, we can find an index l(k), with 0⩽l(k)⩽M, such that at Step 1 we have hk+l(k)=l.The next assumption needs to be satisfied by the line search mapping, and by the choice of the search direction.Assumption 3At every iteration k, the line search procedure computes a value of αksuch thatf(xk+αkdk)⩽f(xk).Furthermore, if {xk} is a sequence of feasible points convergent to a pointx¯and(4)limk→∞(f(xk)-f(xk+αkdk))=0,then we have(5)limk→∞∇f(xk)Tdk=0,limk→∞‖αkdk‖=0.Line search mappings are described and analyzed in the appendix. We remark that the condition (5) stated in Assumption 3 requires, as usual in the context of decomposition methods, that the distance ∥xk+1−xk∥ tends to zero. Indeed, at each iteration, a single block of variables is updated, so that, to attain convergence it may be necessary that “consecutive” points xk,xk+1,xk+2,…, tend to the same limit point.This requirement can be satisfied by an iteration of the formxk+1=xk+αkdk,where αkis determined by an Armijo-type line search (see Section 3.1) provided, for instance, that dkis obtained by a gradient projection. In other cases, for instance, whenever dkis a Frank–Wolfe-type direction, the adoption of an Armijo-type line search is not sufficient to guarantee that the distance between successive points tends to zero. This motivates the adoption of a line search (see Section 3.2) based on an acceptance condition different from that of Armijo’s rule.Finally, we state the following assumption on the search direction.Assumption 4Let {xk}Kbe a subsequence of feasible points convergent to a point x★, and such that(6)Fhk(xk)=Fh∗∀k∈K.We have that(7)limk∈K,k→∞∇f(xk)Tdk=0,implies that(8)∇(h)f(x★)Tx(h)-x(h)★⩾0∀x(h)∈Fh★.We are ready to state global convergence properties of IDA. Note that the following theoretical result is a generalized convergence theorem that follows directly from the stated assumptions, and can be used to design algorithmic strategies for specific classes of problems whose convergence can be proved by satisfying the required assumptions.Proposition 1Let{xk}be the sequence generated by IDA. Suppose thatAssumptions 1–4are satisfied. Then{xk}admits limit points and each limit point is a critical point for problem(1).The sequence {xk} belongs to the feasible compact set, so {xk} admits limit points. Let x★ be a limit point of {xk}, i.e., there exists an infinite subsetK⊆Nsuch that(9)limk∈K,k→∞xk=x★.By Assumption 3 it holds f(xk+1)⩽f(xk) so that, as f is bounded below, we can write(10)limk→∞f(xk+1)-f(xk)=0.Using (10) and (5) of Assumption 3 we obtain(11)limk∈K,k→∞∇f(xk)Tdk=0.From (5) of Assumption 3 it follows:(12)limk∈K,k→∞‖αkdk‖=limk∈K,k→∞‖xk+1-xk‖=0.Then, by induction, we can write for anyl∈N,(13)limk∈K,k→∞xk+l=x★,(14)limk∈K,k→∞∇f(xk+l)Tdk+l=0.From (i) of Assumption 1 it follows that, for every j∈{1,…,L}, we can find an infinite subset Kj⊆K and a setFj★such thatFj(xk)=Fj★∀k∈Kj.Recalling Assumption 2 we have that there exists an index j(k), with 0⩽j(k)⩽M, such that hk+j(k)=j, so that, using (13) and (14) we can write(15)limk∈Kj,k→∞xk+j(k)=x★,(16)limk∈Kj,k→∞∇f(xk+j(k))Tdk+j(k)=0.From (15) and (16) and Assumption 4 we have(17)∇(j)f(x★)Tx(j)-x(j)★⩾0∀x(j)∈Fj★.Taking into account (17) and recalling (ii) of Assumption 1 we have∇(j)f(x★)Tx(j)-x(j)★⩾0∀x(j)∈Fj.This equation holds for every j∈{1,…,L}, and hence the proposition is proved. □In this section we describe the well-known Armijo-type line search algorithm, and a line search previously introduced in the context of decomposition methods for unconstrained optimization [8]. We state the convergence properties of the described line search mappings.Letdk∈Rnbe a feasible direction atxk∈F. Let βkbe the maximum feasible step length along dk. Taking, for instance,xˆk∈Fanddk=xˆk-xk≠0, it follows βk⩾1.Assumption 5Assume that {dk} is a sequence of feasible search directions such that(a)for all k we have ∥dk∥⩽M for a given number M>0;for all k we have ∇f (xk)Tdk<0.An Armijo-type line search algorithm and its properties are described below (see, e.g., [3]).Proposition 2Let {xk} be a sequence of points belonging to the feasible setF, and let {dk} be a sequence of search directions satisfyingAssumption 5. Then:(i)Algorithm ALS determines, in a finite number of iterations, a scalar αksuch that(18)f(xk+αkdk)⩽f(xk)+γαk∇f(xk)Tdk;if {xk} converges tox¯and(19)limk→∞(f(xk)-f(xk+αkdk))=0,then we have(20)limk→∞βk∇f(xk)Tdk=0.The properties of an Armijo-type line search stated in Proposition 2 do not guarantee, without further assumptions on the search direction dk, that the distance between successive points tends to zero (which is a usual requirement, as discussed in Section 2, of decomposition methods). Such a property can be satisfied by the line search algorithm described below and based on an acceptance conditionf(xk+αkdk)⩽f(xk)-γ(αk‖dk‖)2,replacing the Armijo’s conditionf(xk+αkdk)⩽f(xk)+γαk∇f(xk)Tdk.The properties of Algorithm QLS are stated in the next proposition (whose proof can be derived, with minor modifications, from the one of Proposition 4.2 in [8]).Proposition 3Let {xk} be a sequence of points belonging to the feasible setF, and let {dk} be a sequence of search directions satisfyingAssumption 5. Then:(i)Algorithm QLS determines, in a finite number of iterations, a scalar αksuch that(21)f(xk+αkdk)⩽f(xk)-γ(αk‖dk‖)2;if {xk} converges tox¯and(22)limk→∞(f(xk)-f(xk+αkdk))=0,then we have(23)limk→∞αk‖dk‖=0,limk→∞βk∇f(xk)Tdk=0.In this section we present two decomposition algorithms derived from the IDA framework. The two algorithms are based on gradient projection and Frank–Wolfe directions respectively, and on the adoption of suitable line searches. In both algorithms we implicitly assume that, if ∇f(xk)Tdk=0, then we consider a null step along the search direction, that is αk=0.The gradient projection-based algorithm is formally defined by Algorithm 4. We denote byPFhk(xk)x(h)k-∇(h)f(xk)the projection of the pointx(h)k-∇(h)f(xk)onto the setFhk(xk).The global convergence of Algorithm 4 is stated in Proposition 4.Proposition 4Let {xk} be the sequence generated by GP-IDA. Suppose thatAssumptions 1 and 2are satisfied. Then {xk} admits limit points and each limit point is a critical point for problem(1).We will show that the search direction dkand the stepsize αkare such that Assumptions 3, 4 hold, so that the thesis follows from Proposition 1. Hence, we assume that there exists an infinite subset K such that(24)limk∈K,k→∞xk=x★,(25)limk∈K,k→∞(f(xk+αkdk)-f(xk))=0.From the properties of the projection mapping we get(26)∇f(xk)Tdk=∇(hk)f(xk)Td(hk)k⩽-‖xˆ(hk)k-x(hk)k‖2⩽0,where(27)xˆ(hk)k=PFhk(xk)x(hk)k-∇(hk)f(xk).For all k we have either∇f(xk)Tdk<0andαk>0,where αkis determined by means of the Armijo line search along the search direction dk, or(28)∇f(xk)Tdk=0andαk=0.Note that, due to the convexity ofFhk(xk), the maximum feasible step length βkalong dkis greater than or equal to 1. Furthermore, as the closed convex setFhk(xk)belongs, by assumption, to the compact setF, we have that the search direction dkis bounded.Using (25) and assertion (ii) of Proposition 2, taking into account (28), we obtain(29)limk∈K,k→∞∇f(xk)Tdk=0.From (29) and (26) it follows(30)limk∈K,k→∞‖xˆ(hk)k-x(hk)k‖=limk→∞‖dk‖=0,and hence, as αkis bounded above, we can write(31)limk∈K,k→∞‖xk+1-xk‖=limk∈K,k→∞‖αkdk‖=0.Thus, (29) and (31) imply that Assumption 3 holds. Now let Kh⊂K be an infinite subset such thatFhk(xk)=Fh★∀k∈Kh.From (30), recalling the continuity of the projection mapping, we obtain(32)x(h)★=PFh★x(h)★-∇(h)f(x★),which implies that(33)∇(h)f(x★)Tx(h)-x(h)★⩾0∀x(h)∈Fh★.Thus, Assumption 4 is satisfied, and this concludes the proof. □Note that the convergence analysis of GP-IDA cannot be derived from results stated in [4], since GP-IDA involves restrictionsFh(xk)of the prefixed subsetsFh.The decomposition method based on the Frank–Wolfe direction is described in Algorithm 5.The global convergence of Algorithm 5 is established in Proposition 5.Proposition 5Let {xk} be the sequence generated by FW-IDA. Suppose thatAssumptions 1 and 2are satisfied. Then {xk} admits limit points and each limit point is a critical point for problem(1).We will show that the search direction dkand the stepsize αkare such that Assumptions 3 and 4 hold, so that the thesis follows from Proposition 1. Hence, we assume that there exists an infinite subset K such that(34)limk∈K,k→∞xk=x★,(35)limk∈K,k→∞(f(xk+αkdk)-f(xk))=0.For all k we have either∇f(xk)Tdk<0andαk>0,where αkis determined by means of the quadratic line search along the search direction dk, or(36)∇f(xk)Tdk=0andαk=0.Note that, due to the convexity ofFhk(xk), the maximum feasible step length βkalong dkis greater than or equal to 1. Furthermore, as the closed convex setFhk(xk)belongs, by assumption, to the compact setF, we have that the search direction dkis bounded. Using (35) and Proposition 3 (reported in the appendix), and taking into account (36) we obtain(37)limk∈Kk→∞∇f(xk)Tdk=0,limk∈Kk→∞‖αkdk‖=0,which implies Assumption 3.Let Kh⊂K be an infinite subset such thatFhk(xk)=Fh★∀k∈Kh.The direction dkis such that, for everyx(h)∈Fh★(38)∇f(xk)Tdk⩽∇(h)f(xk)Tx(h)-x(h)k.From (34), (37) and (38), recalling the continuity of the gradient, we obtain(39)∇(h)f(x★)Tx(h)-x(h)★⩾0∀x(h)∈Fh★.Thus, Assumption 4 is satisfied, and this concludes the proof. □We remark that global convergence results of inexact block coordinate descent methods (in the general case of nonconvex objective functions) based on Frank–Wolfe iterations were not known. A randomized block-coordinate variant of the classic Frank–Wolfe algorithm for convex optimization with block-separable constraints has been presented in [12], where convergence rates results in the duality gap have been established.In this section we will show how a wide class of problems has a structure that allows restrictions on the factor sets which satisfy Assumption 1.Let us consider problem (1) with(40)Fh=x(h)∈Rnh:a(h)Tx(h)=b(h),l(h)⩽x(h)⩽u(h),where we assume that a(h), i≠0 for i∈{1,…,nh}. Problem (1) with factor setsFhdefined by (40) includes, for instance, Network Equilibrium (NE) problems [7], training problems of Support Vector Machine (SVM) [22,6], which represent an important tool both for classification and regression problems, portfolio selection problems [10], optimal control problems [2].We first state the optimality conditions in a compact form. To this aim, given a pointx∈F, we define the index setsLh(x)={i:x(h),i=l(h),i},Lh-(x)={i∈Lh(x):a(h),i<0},Lh+(x)={i∈Lh(x):a(h),i>0},Uh(x)={i:x(h),i=u(h),i},Uh-(x)={i∈Uh(x):a(h),i<0},Uh+(x)={i∈Uh(x):a(h),i>0}.We introduce the following index setsRh(x)=Lh+(x)∪Uh-(x)∪{i:l(h),i<x(h),i<u(h),i},Sh(x)=Lh-(x)∪Uh+(x)∪{i:l(h),i<x(h),i<u(h),i}.We recall the following results [14].Proposition 6Let {xk} be a sequence of feasible points for problem(1), with factor setsFhof the form(40), convergent to a pointx¯, and let h∈{1,…,L}. Then for sufficiently large values of k we haveRh(x¯)⊆Rh(xk)Sh(x¯)⊆Sh(xk).Letx¯be a feasible point for problem(1), with factor setsFhof the form(40), and let h∈{1,…,L}. Then∇(h)f(x¯)T(x(h)-x¯(h))⩾0∀x(h)∈Fh,if and only if(41)maxi∈Rh(x¯)-∇(h),if(x¯)a(h),i⩽minj∈Sh(x¯)-∇(h),jf(x¯)a(h),j.Given a pointxk∈F, let(42)Ih(xk)=i∈{1,…,nh}:i∈argmaxi∈Rh(xk)-∇(h),if(xk)a(h),i,Jh(xk)=j∈{1,…,nh}:j∈argminj∈Sh(xk)-∇(h),jf(xk)a(h),j.Givenihk∈Ih(xk)andjhk∈Jh(xk), we define the working set(43)Wh(xk)=ihk∪jhk∪(Rh(xk)∩Sh(xk)),and we introduce the restrictionFh(xk)of the factor setFhas follows(44)Fh(xk)={x(h)∈Fh,xi=xik∀i∉Wh(xk)}.Note that the working set contains the pairihk,jhkthat most violates the optimality conditions, and the indexes corresponding to all the variables with inactive bounds. The presence of the maximal violating pair allows us to satisfy requirement (ii) of Assumption 1, while the inclusion of all the variables with inactive bounds is needed to ensure that also requirement (i) is satisfied. Now we will show that Assumption 1 holds on the restrictionFh(xk).Proposition 8For any pointx∈F, letFh(x)be the restriction of the feasible setFhdefined as in(44)(replacing xkwith x). ThenAssumption 1holds.(i)Letx∈F. Note that an index i∈{1,…,nh} does not belong to Wh(x) only if either x(h), i=l(h),ior x(h), i=u(h), i. Then we have|∪x∈F{Fh(x)}|⩽3n.Now let K be an infinite index subset such thatFh(xk)=Fh★for all k∈K. We can extract a further infinite subset, relabelled by K, such thatIh(xk)=Ih★Jh(xk)=Jh★∀k∈K,where the index sets Ih(xk), Jh(xk) are defined in (42). If∇(h)f(x¯)T(x(h)-x¯(h))⩾0∀x(h)∈Fh★,then, using the optimality conditions stated in Proposition 7, we can write(45)-∇(h),i★f(x¯)a(h),i★⩽-∇(h),j★f(x¯)a(h),j★i★∈I★,j★∈J★.Finally, we remark that there exist algorithms (see, e.g., [11,16]) that perform projections onto feasible sets of the form (40) in linear time. Then, the GP-IDA algorithm, described in Section 4, can be effectively applied to classes of problems whose feasible set has the structure considered in this section.In this section we report the results obtained by the proposed inexact decomposition methods on network assignment problems, which is a class of convex large-scale optimization problems.Network assignment problems are a widely studied subject in many research fields, as for instance transportation and data transmission. The aim of a network equilibrium model is to predict the link flows of a network whose arc costs depend on the origin/destination routes chosen by its users (travellers or data package). Denoting by L the number of Origin/Destination (OD) pairs, and by x the vector of path flows, under suitable assumptions (see, e.g., [1,7] for the technical details), network equilibrium model leads to an optimization problem of the form(48)minf(x),s.t.e(h)Tx(h)=bh∀h∈{1,…,L},x(h)⩾0∀h∈{1,…,L},wheref:Rn→Ris a convex continuously differentiable function; nhis the number of paths of the hth OD pair,e(h)∈Rnhis a vector whose components are all ones, bhis the demand of the h-th OD pair, with h∈{1,…,L}, n=n1+n2+⋯+nL, and the partial derivative of f with respect to x(h), iis the cost of path i of the hth OD pair.The convex problem (48) has a very simple structure, as its feasible setFis the Cartesian product of simplices. However, problem (48) can be considered a “virtual” formulation: indeed, in any real application it is not reasonable to completely enumerate, a priori, all the paths, since this would be too expensive. This motivates the need of introducing restrictions of the factor sets formally defined in the preceding section. More specifically, given the current feasible point xk, for each h∈{1,…,L}, the restricted set of variables contains the variables whose current value is strictly positive and the one corresponding to the cheaper path to the destination, that is, the variablex(h),πh(xk), being(49)πh(xk)∈argmini∈{1,…,nh}∂f(xk)∂x(h),i.In the decomposition schemes of GP-IDA and FW-IDA we consider a number of block components equal to the number of origins, so that each given block component groups the variables of the OD pairs associated to the given origin. The line search parameters δ and γ have been set equal to the values 0.5 and 10−4 respectively. We have performed numerical experiments on the freely available data sets from the repository of Hillel Bar-Gera.2http://www.bgu.ac.il/bargera/tntp/.2Some additional information about the problem sizes are reported in Table 1.For all tests we report the so-called relative gap (see for instance [1] for more details), a widely used quality function defined for anyx∈Fas(50)rgap(x)=1-∑h=1Lπh(x)bh∇f(x)Tx=∇f(x)Tx-∇f(x)Txˆ∇f(x)Tx,wherexˆ∈argminy∈F∇f(x)Ty.Note that rgap(x)⩾0 and rgap(x)=0 if and only if x is a solution of the network equilibrium problem.The performances of GP-IDA and FW-IDA have been compared with those of OBA (Origin-Based Assignment), a well established specialized algorithm for the Traffic Assignment Problem (see [1]), for which an executable is freely available.3http://www.openchannelsoftware.org/projects/Origin-Based_Assignment/.3Furthermore, in order to measure the computational improvements brought by the inexact decomposition, we implemented the GP-EDA algorithm, where the subproblems are solved exactly, and the GP algorithm, which represents the standard projected gradient algorithm without any decomposition.All algorithms, except OBA, have been coded in C++ using the Boost Graph Library4http://www.boost.org/doc/libs/1_46_1/libs/graph/doc/index.html.4implementation of the Dijkstra algorithm for the shortest path computation, as well as the graph representation. All tests have been performed on a Intel Core i7 2.93gigahertz standard desktop machine with 3gigabytes of RAM. For each test problem and for each algorithm we report in Tables 2–5the CPU time required to satisfy the stopping criterion, i.e., for attaining a value of the relative gap (see (50)) less than or equal to the tolerance ∊, which has been fixed to different values. The symbol * indicates that the algorithm was not able to satisfy the stopping criterion within 5×103seconds for Winnipeg, 104seconds for Barcelona and 2×104seconds for Berlin Central and Chicago Sketch.From the results reported in Tables 2–5, we can observe that GP-IDA algorithm is competitive with OBA code, and that the Frank–Wolfe based-method, as well-known, is poorly effective for the considered class of problems. We may note that the performances of GP-IDA are clearly better than those of OBA code on the biggest network considered, that is, Berlin Central network. For the other networks, OBA code outperforms GP-IDA when a relatively high accuracy (say a relative gap lower than 10−5) is required (the difference is particularly significant in the Chicago Sketch network). However, as pointed out in [5], in practical cases, requiring a relative gap lower than 10−4 does not change by any significant amount the link flow values of the solution. On the whole, as OBA code is highly specialized for the particular problem data and structure, the results of the numerical experiments point out the validity of the proposed inexact decomposition approach. We also notice that the results show the effectiveness of the inexact decomposition strategy compared with an exact decomposition method (GP-EDA) and with a method (GP) not using a decomposition technique.

@&#CONCLUSIONS@&#
