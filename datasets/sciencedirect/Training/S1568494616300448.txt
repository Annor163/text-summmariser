@&#MAIN-TITLE@&#
Parallel VNS for Bus Terminal Location Problem

@&#HIGHLIGHTS@&#
Parallel variable neighborhood search (PVNS) is implemented for solving Bus Terminal Location Problem (BTLP).Improved local search based on the neighborhoods fast interchange is combined with reduced neighborhood size based on the covering characteristic of the problem.Parallelization gives significant time improvement based on processor core count.All existing results from the literature are improved by PVNS with notably less time.New larger instances based on rl instances from TSP library are introduced and computational results for those new instances are given.

@&#KEYPHRASES@&#
Combinatorial optimization,Bus Terminal Location Problem,Variable neighborhood search,Metaheuristic,Parallelization,

@&#ABSTRACT@&#
This paper considers the Bus Terminal Location Problem (BTLP) which incorporates characteristics of both the p-median and maximal covering problems. We propose a parallel variable neighborhood search algorithm (PVNS) for solving BTLP. Improved local search, based on efficient neighborhood interchange, is used for the p-median problem, and is combined with a reduced neighborhood size for the maximal covering part of the problem. The proposed parallel algorithm is compared with its non-parallel version. Parallelization yielded significant time improvement in function of the processor core count. Computational results show that PVNS improves all existing results from the literature, while using significantly less time. New larger instances, based on rl instances from the TSP library, are introduced and computational results for those new instances are reported.

@&#INTRODUCTION@&#
The Bus Terminal Location Problem (BTLP) is special case of the facility location problem, first introduced in [1]. Since it is formulated as a combination of two NP-hard problems, i.e. p-median and maximal covering problems, we assume it to be also NP-hard. We consider a set of public transportation stations, such as bus or metro station, under the assumption that the passenger number is given, as well as the distances between each station and candidate terminals. A candidate terminal can be an existing station, or a newly built one. For each candidate terminal, the reachable area (radius) around it is given. An established terminal can service only stations in its reachable area. It is also assumed that, in order to use an appropriate public service, passengers from each station are allocated to the closest established terminal, such that their destination is within the corresponding radius. The aim is to locate a pre-specified number of terminals, from the given set of terminal candidates, so that the public service is maximized.Formulation of the BTLP is presented in [1]. Let J={j1, j2, …, jn} be the set of nodes (stations, for example bus stops in a city) and I={i1, i2, …, im} be the set of candidate terminals, where I ought not to be a subset of J.Let C=[cij], denote the distance matrix between node i∈I and the candidate terminal (potential facility) j∈J. The radius of the reachable area is constant and denoted with r, while p is the exact number of terminals to be opened. In addition, let D={d1, d2, …, dn} be the set of potentials corresponding to the nodes in J. More precisely, dkrepresents the maximum number of passengers corresponding to node jk∈J.In what follows,Ji*stands for a subset of J, which contains all nodes that can be serviced from terminal i∈I. In this paper it is considered thatJi*={j∈J:cij≤r}, andJ*=∪i∈SJi*, where S⊂I represents established terminals.The goal is to select a subset S of I, such that |S|=p, and to maximize the objective function, i.e. the service function. In BTLP nodes should be as close as possible to the terminal which services them. The service function as a quality measure can be defined as follows:F(S)=∑j∈J*dj×fmini∈S{cij},where f represents the decreasing function. Note that decreasing function f is an important factor of the objective function due to the fact that with the increase of the distance, the value of the objective function should decrease.BTLP in [1] was considered as a p-uncapacitated facility location problem (p-UFLP) with distance constraint. As the p-UFLP was NP-hard they proposed evolutionary algorithms as a solving technique. In order to solve BTLP the authors in [1] implemented new evolutionary and memetic algorithms, namely genetic local search algorithms (GLS) [2]. Furthermore, the authors defined the potential objective function (POF) for the nodes and used it to design a new mutation operator. To make the memetic algorithm faster, the authors estimated the variation of the objective function based on POF in the local search, as part of an operator in the memetic algorithm. Additionally, the authors proposed a multi start simulated annealing heuristic for solving BTLP.Facility location problems, and many of their variations, have been widely studied in the literature [3–10]. In the paper [8], approximation algorithms for several NP-hard facility location problems are studied, and it is proven that a simple local search heuristic yields polynomial-time constant-factor approximation bounds for metric versions of the uncapacitated p-median problem and uncapacitated facility location problem. In [11], around 160 papers on the covering problems in facility location are reviewed and classified in terms of models, solutions and applications. The maximal covering location problem (MCLP) and its variants are among the most studied covering location problems [3,12–14].The paper [1] has attracted significant interest. A Survey of Transportation Problems [15] placed BTLP in the group of Land Transportation Problems. Moreover, the BTLP problem is mentioned in the paper [16] where the authors considered the hub location problem in urban public transit network design, which includes two phases: to determine the candidate nodes and to optimize the distribution of transit hubs. The proposed two-phase optimization approach was tested on real data of the urban area of Dalian city. Moreover, in [17] authors presented two models for the Bus Terminal Location Problem with fuzzy parameters (FBTLP). The first model differs from the classical BTLP in the fact that the potential of each node is a fuzzy number. In the second formulation, an additional assumption of fuzzy reachable area is considered. Furthermore, the authors presented an efficient GA which is practical for solving FBTLPs.Although the BTLP is related to the optimization of public transportation service, it does not belong to the group of classical bus network design problems [18–22]. As authors in [1] stated, it is clear that a BTLP is a special facility location problem. More precisely, BTLP represents a generalization of well known location problems: the p-median problem [23] and MCLP [3].If in the formulation described in Section 1.1 we set the decreasing function f to be f(x)=−x, all potentials of the nodes to 1 (i.e. dj=1, j=1, 2, …, n) and the radius r to a large number (e.g.r=maxi=1,…,m,j=1,…,n(cij)), the formulation would be equivalent to the p-median problem. On the other hand, if the function f is set to be constant function f(x)=1, the formulation would be equivalent to MCLP.In [24], the p-median problem with an additional coverage constraint (CONPMP) is considered. The aim of CONPMP is the maximization of efficiency subject to required minimum coverages. The BTLP represents a generalization of the CONPMP, since CONPMP formulation can be obtained from the BTLP formulation by setting f(x)=−x.In this paper, a parallel implementation of the VNS algorithm for solving BTLP is presented. To reduce the computational time, we used swap based fast interchange and a newly introduced reduced neighborhood based on the covering characteristics of the problem. Moreover, a low-level parallelism strategy is applied to the most computationally consuming parts of the algorithm, which were distinguished by using CPU profiler.The proposed algorithm presented in this paper achieved better solutions in significantly shorter CPU times compared to the best solutions from the literature. Furthermore, new instances for BTLP are generated based on TSP LIB instances [25] and the appropriate results are presented.Algorithms based on Variable Neighborhood Search (VNS) methodology have been successfully used for solving various combinatorial optimization problems: [26–29]. In this study, we propose a heuristic based on Parallel Variable Neighborhood Search (PVNS) to solve the BTLP. A solution represents an array of established terminals. According to the array of established terminals, it can be easily determinated which node is connected to which terminal.In the proposed VNS implementation a swap-based neighborhood structure is used. One swap represents establishing one currently unestablished terminal and closing one currently established terminal. Moreover, solution sol1 is in the kth neighborhood of solution sol2, if it is possible to construct sol2 from sol1 by applying exactly k swaps.The VNS method consists of two main phases:1.shake phase,local search phase.Algorithm 1VNSThe local search phase is the most CPU consuming part of VNS. Many different advancements of swap based local search have been used to solve various location problems [30–32]. Swap based fast interchange was firstly introduced in [30]. Later, it was applied for the first time to the p-median problem in [31]. The main idea in [31] is to consider which facility is the best to be closed, while establishing one of the unestablished facilities, taking into account the effect of the swap on the objective function value.In what follows, tinstands for the terminal that will be established, and toutfor the most convenient terminal to close in case of establishing tin. Moreover, for each node from J, the notation tfcrepresents its first closest terminal among the established terminals, while tsccorresponds to the second closest one.Let the variable gain denote the improvement of the objective function value. The array loss represents deterioration of the objective function value, such that loss[i] represents deterioration in case of closing the terminal i.Let us introduce a new function fr, based on the decreasing function f and reachable area of the terminal candidates, defined as follows:fr(x)=f(x)ifx≤r,0otherwise.Now, we analyze the influence of the established terminal tinand the closed one on some node l. Three different cases are distinguished:1.ctin,l<ctfc,l≤ctsc,l: The case that terminal tinis closer to the node l than its closest established terminal tfc, l shifts from tfcto tinand gain becomes:gain=gain+(fr(ctin,l)−fr(ctfc,l))dlctfc,l≤ctin,l<ctsc,l: The case that the first closest established terminal tfcto node l is closer to l than terminal tin, and tinis closer to the node l than its second closest established terminal tsc, l shifts from tfcto tinin the case of closing tfcand loss becomes:loss[tfc]=loss[tfc]+(fr(ctin,l)−fr(ctfc,l))dlctfc,l≤ctsc,l≤ctin,l: The case that the second closest established terminal tscto node l is closer to l than terminal tin, l shifts from tfcto tscin the case of closing tfcand loss becomes:loss[tfc]=loss[tfc]+(fr(ctsc,l)−fr(ctfc,l))dlCalculating the first and the second closest established node for each node is needed before each local search iteration.In BTLP, establishing one terminal tinhas influence only on the nodes inJtin*, as opposed to the p-median problem, where all the nodes are from J. Thus, in case of establishing tin, it is enough to recalculate how this affects nodes fromJtin*. For all the nodesl∉Jtin*, l remains connected to its first closest terminal tfc, if tfcstays established, or otherwise l shifts to its second closest terminal tsc. Additionally, when calculating first and second closest, the array loss is being initialized such that it is assumed that in the case of closing the first closest terminal of node l, l shifts to the second closest. Moreover, if the shift from the first to the second closest does not happen, a correction is made: the appropriate added value is taken off the corresponding element of the array loss.The worst case complexity of exploring the newly introduced reduced neighborhood is obviously in function of the radius. More precisely, when using the reduced neighborhood, the time complexity of finding the best swap isO(∑tin∈I\S|Jtin*|), while the complexity without the reduced neighborhood isO(∑tin∈I\S|J|). If the minimum radius is sufficiently large then the size of reduced and original neighborhood are equal. However, this is not the case for real-life instances and the time complexity becomes notably shorter by using reduced neighborhood.Parallel computing techniques are widely used to improve efficiency, especially when facing large problem instances, in an attempt to get a more thorough exploration of the solution space [33–35].The parallelization methods of metaheuristics can be classified into three strategies: low-level parallelism, domain decomposition and multiple search [35]. In [36] the authors introduce three approaches of parallel VNS for solving the p-median problem. The first approach may be classified as low-level parallelism, while other two could be classified as multiple search. Computational results in [36] show that the latter two strategies obtained better solutions. Furthermore, in [35] authors introduced new asynchronous cooperative multi-search parallel strategy for the p-median problem.Generally, in multiple search implementations, using the same input parameters does not guarantee the same result in multiple runs of the algorithm. On the other hand, using a convenient implementation of low-level parallelism strategies guarantees the same results for the same input parameters on non-parallel and parallel implementations. In [35] authors tested the algorithm using a time constraint of tmax/np seconds, where tmaxrepresents constant time and np the number of used processors. However, their computational results showed that with increasing value of np the results became worse. Our idea was to get the same results regardless of the number of used processors, and to obtain a speed-up factor with respect to the number of used processors.In order to keep the fundamental idea of simplicity of the VNS concept, we did not want to parallelize every segment of the algorithm presented in this paper. We wanted to explore which parts of the algorithm are most computationally intensive, and parallelize only those parts. For this purpose we use the CPU performance and diagnostics tool embedded within Microsoft Visual Studio Ultimate 2013.We considered samples of algorithm execution for some of the instances. Sampling is a statistical profiling method that reveals which functions perform the most work in the application [37]. The sampling method collects information about the functions that are executed in the algorithm at specified intervals. Once the profiling is finished, the summary view of the profiling data shows the inclusive and exclusive samples of all the functions implemented in the application. Inclusive samples indicate how much work was done by the function, including the work done by any functions called by it. Exclusive samples indicate how much work was performed by the code in the function body, excluding the work done by functions that were called by it.Inclusive samples from the CPU profiler indicated that the non-parallel version of the algorithm, presented in this paper, spent more than 90% of its time on the following two functions of the local search phase:1.The function that calculates the first and the second closest established terminal for each node in J.The function that calculates which terminal is the best to be closed if terminal tinis established.This was a motivation to add parallelization to these two functions. The parallel version of local search consists of repeating the following steps while better solutions are being successfully found in each iteration. Firstly, in each iteration, arrays representing the first and the second closest established terminals of each node (firstClosest and secondClosest) are calculated and the loss array is initialized. Then, within the parallel loop, for each closed terminal tinwhich may be established, the best terminal to be closed toutis computed using already calculated arrays firstClosest, secondClosest and loss. The difference Δ from the objective function value is calculated. Finally, if the best found difference Δmaxrepresents an improvement (Δmax>0, i.e. the objective function increases), the corresponding terminalbesttinis established andbesttoutis closed. The described parallel implementation of Local search is presented in Algorithm 2.The function that calculates the first and the second closest, and initializes the loss array, is also parallelized. Firstly, arrays firstClosest and secondClosest are initialized such that all the values are set to an undefined value. Secondly, in the parallel for each loop, for each established terminal t and each node l in the reachable area of t, arrays firstClosest and secondClosest are updated if l is closer to t than to the current terminals firstClosest[l] and secondClosest[l]. Then, after being initialized with zeros, array loss is modified in a parallel loop, with the assumption that in the case of closing the first closest terminal of node l, l shifts to the second closest. This algorithm is presented in Algorithm 3.Computation of the best closing terminal in case of establishing terminal tinis done by the function CalculateBestOut described in Section 2.2. The CalculateBestOut function is presented in detail in Algorithm 4.Algorithm 2Local searchAlgorithm 3Calculate first and second closest bus terminal and initialize lossAlgorithm 4Compute Best OutIn this section, the computational results of the proposed methods and their comparison with the results from [1] are presented. The proposed methods were implemented in C♯ language,.NET framework, and run on an IntelCore i7-860 2.8GHz with 8GB RAM memory under the Windows 7 Professional operating system. In implementing parallelization elements, we followed the concept of the shared memory architecture with threads, executed on different processors. We used the.NET Task Parallel Library [38]. The Task Parallel Library allows us to use the parallel foreach loop, which creates a separate thread for each loop iteration, and executes those threads on different processor cores.To evaluate the computational effectiveness of the proposed approach, a comprehensive computational experiment was performed. The decreasing function f(x)=e−xis used, as in [1]. Computational experiments were performed on the following two data sets:1.Fifteen large scale problem test instances specially constructed for the BTLP problem and described in [1], with the following parameters: r=20 and•p∈{64, 125, 187}, forn=m=250,p∈{125, 250, 375}, forn=m=500,p∈{187, 375, 562}, forn=m=750,p∈{250, 500, 750}, forn=m=1000.Eighteen problem test instances created for the purpose of this paper, using the well known data set rl from TSPLIB,11TSPLIB instances are available at http://www.iwr.uni-heidelberg.de/groups/comopt/software/TSPLIB95/tsp/.originally used for the Travelling Salesman Problem [25]. These instances were adapted for the BTLP problem.22Introduced BTLP instances are available at http://www.matf.bg.ac.rs/~nina/BTLP.Since original rl instances contain only one set of nodes, we have randomly chosen half of these nodes correspondents to the set I, and the rest is used as the set J. In order to generate the potential of a node, random values are chosen from three different sets:•[100, 1000],[700, 1000],[100, 400]∪[700, 1000],Due to the characteristic of the chosen decreasing function f=e−x, witch decreases quickly as x increases, results would not be realistic in the cases with large distances (e.g. 2000 or 3000). For example, in the case where the distance between an established terminal i and a node j is cij=1000, the objective function value is increased by dje−1000, where the second member of the product is very close to zero. Therefore, while initializing the problem instance, the coordinates are divided by r, and then r is set to 1. Thus, the reachable neighborhoods of each terminal stay the same in the sense of the nodes that can be allocated to them. Applying the function f=e−xto the distances in the reachable neighborhoods produces a number from the interval [1/e, 1], which is more suitable for calculation.The initial solution of RVNS is randomly generated in our experiments. All tests are performed until a stopping condition is met, which is 1000 successive iterations without improvement.In order to specify the VNS parameters kmaxand krvns_maxtests are performed on 6 different instances with 20 different seeds and the following parameters:1.rl1304_100_400_700_1000, p=163, r=2000,rl1304_100_400_700_1000, p=217, r=2000,rl1323_100_400_700_1000, p=165, r=2000,rl1323_100_400_700_1000, p=220, r=2000,rl1889_100_400_700_1000, p=236, r=2000,rl1889_100_400_700_1000, p=315, r=2000.Note that in the latter analyses we used the notation krvns_max=0 to specify when the RVNS phase is omitted.For each parameter setting and each instance, the best obtained result is considered among all the results obtained from different seed values. In the Tables 1and 2results for the kmaxand krvns_maxparameters analysis are presented, respectively. For all parameter values the same best solutions are achieved, so they are not presented in the tables. In the first column of the presented tables different values of the appropriate parameter values are given. The second column represent the sum of times tsolneeded to find the solution for all the instances with the given parameters. The third column represent the sum of overall running times texecfor all the instances with the given parameters. The evaluation of the solution quality, in all the algorithm runs with different seeds, is performed by calculating a percentage gap (agap) with respect to the best solution and a standard deviation (σ) of the average gap. The last two columns contain average values of the average gap and standard deviation for all the instances with the given parameters, respectively.Values of agap and σ, for each instance, are computed using the following formulas:•agap=1n∑i=1ngapi,σ=1n∑i=1n(gapi−agap)2,In order to find the optimal value of parameter kmax, several tests were performed using krvns_max=0. Analysis presented in Table 1 shows that the solution time is greatest for kmax=5, and offers stable value around 5s for other values of the parameter kmax. According to the solution time analysis, the value for kmaxshould not be 5. Since larger values of kmaxcorrespond to overall longer running times, the value kmax=10 is chosen.The results presented in Table 2 are obtained by setting kmaxto 10. The best results are acquired for krvns_max=0. Therefore the RVNS phase is omitted from the algorithm and the initial solution is randomly generated.In order to present the efficiency of the parallel VNS algorithm in comparison with the non-parallel one, comparative tests were performed on all test instances and the results are reported in Table 3. In the first column of Table 3 the instance group is given. The second and the third column present the sum of the time needed to find solution tsoland the sum of overall running times texecfor the non-parallel implementation of the algorithm, respectively. Moreover, the appropriate times tsoland texecfor the parallel implementation of the algorithm are provided in the forth and the fifth column. The sixth and the seventh column present the improvement factors of the parallel implementation in comparison with the non-parallel implementation for the time needed to find the solution and for the overall running time. Since the purpose of these tests was to compare the execution time between the parallel and non-parallel versions of the algorithm, and not to find the best solutions, tests are performed by doing one repeat of the algorithm for each instance, i.e. using one seed. The objective function value is not presented in Table 3 because the algorithm gave the same solution for both parallel and non-parallel versions. It can be seen that the parallel version overall gives about 3.4 times faster results than the non-parallel one. Note that all tests are executed on a quad core processor.In [1] authors proposed the following algorithms for solving BTLP:•nine algorithms based on evolutionary algorithm and genetic local search,multistart simulated annealing,hybrid algorithm combining evolutionary algorithm and genetic local search.In Tables 4and 5the comparison between the best obtained results from [1] and the results obtained by PVNS on test instances from [1] are presented. For fair comparison PVNS is tested on five seeds. In the first column the instance name is given. The second column presents the best obtained solutions by the PVNS algorithm. In the third and the forth column, the average time needed to find solution and the average overall running time of the PVNS algorithm are given. The fifth and the sixth column present the average gap and standard deviation of the results obtained by the PVNS algorithm. In the seventh column, the best results from [1] are presented. In order to compare our results with the results from [1], we used the best out of three results they achieved. The results for each instance individually are not presented in [1]. Instead the average results for all three algorithms on each instance group are given. The eight column contains the average execution time on the instance group from [1] in seconds. We can conclude that the proposed PVNS method reached better solutions on each group of instances faster than the best result presented in [1]. Moreover, it is noticeable that the average gap and standard deviation are equal to zero or all, except two instances. This indicates that most of the instances, different runs of the proposed algorithm found the best known solution regardless of the initial seed. Therefore, it can be concluded that the proposed PVNS is very stable on instances from [1]. We aimed to make a fair comparison between the results we obtained and the results from [1], but a scaling factor between the used processors cannot be precisely calculated since the exact model of processor is not provided in [1]. However, comparing average execution times of the PVNS and the best algorithm from [1], it can be seen that PVNS execution time is around 32 times smaller and it can be assumed that the scaling factor would not make a significant difference.Tables 6–8contain the computational results of the PVNS algorithm on the instances from [1] and adapted rl instances from the TSPLIB. Results are obtained by executing tests using 20 different seeds. The columns in Tables 6, 7 and 8 have the following notation. The first column presents the instance name, while the second contains the best found solution. In the third and the forth column, the time needed to find the solution and the overall running time of the PVNS algorithm are presented. The fifth and the sixth column contain the average gap and standard deviation of the results obtained by the PVNS algorithm. Note that results from Table 6 are quite similar to the results from Tables 4 and 5, but the difference is the number of seeds used for the experiments since the results in Tables 4 and 5 are used for the comparison with the results from [1]. It can be seen from Tables 7 and 8 that the values of the average gap and standard deviation are very small, which suggests that the proposed PVNS algorithm is stable even when applied to the large instances.

@&#CONCLUSIONS@&#
