@&#MAIN-TITLE@&#
A new matching strategy for content based image retrieval system

@&#HIGHLIGHTS@&#
The integration of the color and texture features enhances the retrieval performance.The dimensionality reduction technique selects the effective features from original one.The artificial neural network in our proposed model serves as a classifier technique.The introduced matching strategy depends on the idea of the minimum area between two vectors.The proposed model achieves improvement in performance compared with other models.

@&#KEYPHRASES@&#
Content-based image retrieval,Color co-occurrence matrix,Dimensionality reduction,Artificial neural networks,Similarity measure,

@&#ABSTRACT@&#
Adopting effective model to access the desired images is essential nowadays with the presence of a huge amount of digital images. The present paper introduces an accurate and rapid model for content based image retrieval process depending on a new matching strategy. The proposed model is composed of four major phases namely: features extraction, dimensionality reduction, ANN classifier and matching strategy. As for the feature extraction phase, it extracts a color and texture features, respectively, called color co-occurrence matrix (CCM) and difference between pixels of scan pattern (DBPSP). However, integrating multiple features can overcome the problems of single feature, but the system works slowly mainly because of the high dimensionality of the feature space. Therefore, the dimensionality reduction technique selects the effective features that jointly have the largest dependency on the target class and minimal redundancy among themselves. Consequently, these features reduce the calculation work and the computation time in the retrieval process. The artificial neural network (ANN) in our proposed model serves as a classifier so that the selected features of query image are the input and its output is one of the multi classes that have the largest similarity to the query image. In addition, the proposed model presents an effective feature matching strategy that depends on the idea of the minimum area between two vectors to compute the similarity value between a query image and the images in the determined class. Finally, the results presented in this paper demonstrate that the proposed model provides accurate retrieval results and achieve improvement in performance with significantly less computation time compared with other models.

@&#INTRODUCTION@&#
As digital images quickly increase in number, researchers are continually developing improved access methods to retrieve images from a large database. Generally, image retrieval procedures can be roughly divided into two approaches: annotation-based image retrieval (ABIR) and content-based image retrieval (CBIR). In ABIR, images are often annotated by keywords. Although ABIR potentially offers the most accurate information when images are well-named or annotated, it still has some drawbacks such as: the manual image annotation is time-consuming, human annotation is subjective, and some images could not be annotated because it is difficult to describe their content with words. The CBIR tends to index and retrieve images based on their visual content. CBIR avoids many problems associated with traditional ways of retrieving images by keywords. Thus, a growing interest in the area of CBIR has been established in recent years [1,2]. The performance of a CBIR system mainly depends on the particular image representation and similarity matching function employed [3]. A set of low level features called feature vector is generated to accurately represent the content of each image in the images database [4]. Most CBIR systems are similarity based, where the similarity between query and target images in a database is measured according to some similarity matching functions to return an ordered list of images that are perceptually similar to the query image [5]. A common scheme to represent the image is to extract different types of features such as color, texture, and shape [6]. Color features include the color histogram, color correlogram, dominant color descriptor, the color coherence vector, the color co-occurrence matrix, vector quantization and color moments [7]. Texture features are derived from the gray-level co-occurrence matrix, the tamura feature, wavelet coefficients and Gabor filter [8]. The used shape features are normalized inertia, Zernike moments, histogram of edge direction and edge map [9]. In comparison with other image features such as texture and shape, color features are very stable and robust. It is not sensitive to rotation, translation and scale changes. Moreover, the color feature calculation is relatively simple [10]. Color co-occurrence matrix (CCM) is a kind of commonly used color feature representation to capture color variation in an image [11]. It calculates the probability of the occurrence of the same pixel color between each pixel and its adjacent ones in each image, and this probability is considered as the feature of the image. According to the sequence of motifs of scan patterns, the difference between pixels of scan patterns (DBPSP) calculates the difference between pixels and converts it into the probability of occurrence on the entire image. Thus, the DBPSP is used as the texture features [12]. Although the integration of multi features enhances the image detection rate and increases the image retrieval performance, but the speed of the content-based image retrieval is slower mainly due to the curse of high dimensionality of the feature space. Therefore, to enhance image detection rate and simplify computation of image retrieval, a dimensionality reduction technique must be adopted such as feature selection [13]. Feature selection is a one solution for computational cost problem of CBIR. It refers to selecting the most important features and their combinations for describing and querying items in the database in order to reduce retrieval complexity in terms of process time while maintaining high retrieval performance. The key operation in the feature selection processes is to evaluate the discrimination power of the individual features. Mutual information is the most common method used for evaluating the discrimination power of a feature [14]. A successful categorization of images into semantically meaningful categories based on low-level visual image features will greatly enhance the performance of CBIR systems by filtering out images from irrelevant classes during matching. Neural network has been utilized for improving image classification problems because of its property called ‘black-box’ learning [15,16]. A classifier technique using ANN performs two stages which are training stage and testing stage. In the training stage, the input data and target data need to be fed into the network. Therefore, the extracted color and texture features from images database give the input data and the class label of images gives the target data. The back-propagation learning rule is applied until the network convergence is reached. In the testing stage, for a query image we use the same technique to extract its features to build a feature vector which then becomes an input to the trained neural network for the retrieval process. The network assigns the feature vector to one or more similar classes. Image matching is a crucial step in content-based image retrieval. Therefore, finding a good similarity measure between images based on some feature set is a challenging task and leads to affect the effectiveness and the efficiency of the retrieval technique. Once features values associated to images database have been computed and stored, queries may be done. There are various kinds of similarity measurement techniques in CBIR applications [17]. The kind of feature vectors selected determines the kind of measurement that will be used to compare their similarity [18]. If the features extracted from the images are presented as multi-dimensional points, the distances between corresponding multi-dimensional points can be calculated. Euclidean distance, weighted Euclidean distance, Manhattan distance, cross correlation distance, minimum mean distance rule, and statistical distance are the most common metrics used to measure the distance between two points in multi-dimensional space [19]. Otherwise, these distances may not be an ideal similarity metric or may not be compatible with the human perceived similarity for other kinds of features such as color histogram. Therefore, histogram similarity measures such as histogram intersection, histogram Euclidean distance and histogram quadratic distance are preferred [20]. The present paper introduces a novel matching strategy for enhancing the performance of the CBIR based on the idea of the minimum area between two vectors. The suggested matching strategy deduces a scheme for deriving the proposed similarity measure. The artificial neural network serves as an intelligent search engine in our model to extract the corresponding class of a query image based on extracting its low level features. Then, the similarity measure between a query vector and each image vector in the extracted class is performed. Finally, similar images are ranked by their similarities and returned as the retrieval results.The organization of this paper is as follows: The feature extraction is performed in Section 2. The dimensionality reduction technique is outlined in Section 3. The neural network classifier is introduced in Section 4. The proposed matching strategy is discussed in Section 5. The performance evaluation is described in Section 6. The experimental results are reported in Section 7. Finally, the conclusion and future work are presented in Section 8.CBIR is greatly affected by the choice of features type. In general, features fall into two categories: global features and local features. Global features include color and texture histograms and color layout of the whole image. Local features include color, texture, and shape features for sub-images, segmented regions, and interest points. The present paper integrated local color and texture features to improve the system performance. These features are color co-occurrence matrix (CCM) and difference between pixels of scan pattern (DBPSP), respectively. Motif co-occurrence matrix (MCM) is the conventional pattern co-occurrence matrix that calculates the probability of the occurrence of same pixel color between each pixel and its adjacent ones in each image. This probability is considered as the attribute of the image. MCM describes the direction of textures but not the complexity of the textures. The aim of DBPSP method is to compute the differences between all pixels within motifs of scan pattern and is considered as one of the texture features. The proposed image features are not affected by image displacement and rotation [12,21,22].The image is processed and converted into four images of motifs of scan pattern based on overlapping 3×3 windows scanning from top to bottom and left to right. Each 3×3 window is divided into four grids with the central pixel G(x,y) located at four different corners as shown in Fig. 1. These 2×2 grids are then replaced by motifs of scanning pattern traversing the grid in the optimal sense. The optimality of the scan is achieved by traversing the path with respect to the incremental difference in intensity along the scan path by minimizing the variation of the intensities at a local neighborhood. Peano space filling curves are used to traverse pixels along a specific local path made out of the seven primitive scans as shown in Fig. 2. One of the seven possible motifs will represent the 2×2 pixel grid optimally with respect to a suitable criterion. Generally, there are 25 different scan patterns in a grid if the traversal goes from four different corners along the paths. If we only consider the scan starting from the top left corner, then the number of motif patterns are reducing to only 7 including motif number “0” as shown in Fig. 2 which signifies the special situation where a motif cannot be formed due to the two or more adjacent pixels that have the same value in a grid. Fig. 3illustrates an example of various patterns of motif “0”. The patterns can be classified into ascending and descending patterns based on whether the values of scanning sequence are increasing or decreasing.After the image is processed and converted into motif patterns, four two dimensional Nx×Nymatrix Pi[Nx, Ny] for delineating motif scan patterns can be obtained. The CCM considers the probability of the co-occurrence between the two adjacent motifs corresponding to a pixel (x, y) and its adjacent pixel (x+δx, y+δy). The probability is then used as the attribute of image color variation. The co-occurrence probability between two motifs u and v can be computed for all possible co-occurring motif number pairs in a motif matrix as follows:(1)Mi(u,v)=Mi(u,v|δx,δy)=Mi(Pi(x,y),Pi(x+δx,y+δy))where u=0, 1, …, 6, v=0, 1, …, 6, u=Pi(x, y), v=Pi(x+δx, y+δy), 1≤i≤4, 1≤x≤Nx, 1≤y≤Ny, 1≤x+δx≤Nxand 1≤y+δy≤Ny.The feature values of color co-occurrence matrix (CCM) where the co-occurring probabilities of the number ith motifs of scan pattern matrix are determined by dividing by the total number of counts (Ni) across all u and v as follows:(2)mi(u,v)=Mi(u,v)Niwhere(3)Ni=∑u=06∑v=06Mi(u,v)As a result, there will be a 7×7 two-dimensional CCM grids in total. Consequently, the total number of CCM elements used as attributes are 49.The feature of DBPSP is mainly to calculate the differences among all pixels within motifs of a scan pattern. Therefore, these differences are taken as one of the texture features. The adopted motifs of a scan pattern do not take motif number “0” into account. The total pixel value difference of any coordinates (x, y) within the image is Δ(x, y), where Δi(x, y) is the total pixel value difference of all scan directions of ith motif number of any coordinate (x, y) within the image. It is possible to generate six motifs of a scan pattern that can be, respectively, shown as(4)Δ1(x,y)=|P1−P2|+|P2−P4|+|P4−P3|Δ2(x,y)=|P1−P3|+|P3−P4|+|P4−P2|Δ3(x,y)=|P1−P2|+|P2−P3|+|P3−P4|Δ4(x,y)=|P1−P3|+|P3−P2|+|P2−P4|Δ5(x,y)=|P1−P4|+|P4−P3|+|P3−P2|Δ6(x,y)=|P1−P4|+|P4−P2|+|P2−P3|Finally, the appearance rate of the total pixel value differences (Δ(x, y)) in the whole image is taken as the feature of DBPSP. Therefore, six DBPSP feature values can be obtained as follows:(5)ARi=1Ni∑jNiΔji(x,y)where i is the motif number; Niis the total appearance of ith motif number within the whole image.Dimensionality reduction of content-based image retrieval features has gained momentum for swift image retrieval. When some features are irrelevant to the prediction task, Langley et al. [23] pointed out that appropriate feature weights or feature selection can substantially increase the learning rate. Many techniques are used to perform the dimensionality reduction in image features as discussed in [24,25]. A typical feature selection process consists of four basic steps which are subset generation, subset evaluation, stopping criterion and result validation. Subset generation is a search procedure that produces candidate feature subsets for evaluation based on a certain searching strategy. Each candidate subset is evaluated and compared with the previous best subsets according to a certain evaluation criterion. If new subsets turn out to be better, they replace the previous ones. The process of subset generation and evaluation is repeated until a given stopping criterion is satisfied. Then, the best selected subset usually needs to be validated using a test dataset [26]. Feature selection algorithms can be categorized into two groups: filter and wrapper. Filter algorithms evaluate the “properness” of the feature subset by using intrinsic characteristics of the data. Popular criteria employed are distance, information, dependency, and consistency measurements [27]. Consistency measurements aim at finding a minimum number of features that separate classes as consistently as the full set of features does. On the other hand, wrapper algorithms use the final mining algorithm (classification, clustering and others) to evaluate the candidate feature subsets. The subset evaluation for wrapper feature selection algorithms is normally expensive and the search space grows exponentially with the number of features. Therefore, filter algorithms are more faster and have lower computational cost when they are compared with the wrapper algorithms. Representative filter feature selection algorithms are the correlation-based feature selection algorithm [28], the fast correlation-based filter algorithm [29], RELIEF algorithm [30] and the maximal relevance and minimal redundancy algorithm [31].In the present paper, we use the maximal mutual information criterion as a feature selection algorithm [32]. Suppose two random multidimensional variables x and y, their mutual information is defined in terms of their probabilistic density functions p(x), p(y) and p(x, y) [33]:(6)I(x,y)=∑i,jp(xi,yj)logp(xi,yj)p(xi)p(yj)From Eq. (6), the value of I(x, y) will be very high, if x and y are closely related to each other; or otherwise, I(x, y)=0 which denotes that these two variables are totally unrelated. The purpose of feature selection algorithm is to find a feature set S with m features, which jointly have the largest dependency on the target class c.(7)maxD(S,c)=I(xi,i=1,2,…,m;c)The problem of this feature selection algorithm is that the jointly probabilistic density function, p(x, y) results in a considerably high computational complexity. To overcome this problem, an alternative feature selection method based on maximal relevance and minimal redundancy criterion has been proposed [31], which provides more practical solutions in terms of computational complexity.This method involves the integration of maximal relevance and minimal redundancy of two aspects. Maximal relevance is to search for features satisfying Eq. (8), which approximates D(S, c) in Eq. (7) with the mean value of all mutual information values between individual feature x and class c.(8)maxV=1|S|2∑i∈SI(c,xi)In this way, all potential features having large mutual information values are possible to be selected even if they are highly dependent on each other. Therefore, it is unavoidable that the selected feature subset has high redundancy which decreases the discriminated capability. In order to overcome this shortcoming, the second criterion of minimal redundancy condition is applied to select mutually exclusive features.(9)minW=1|S|2∑i,j∈SI(xi,yj)The criterion combining the above two constraints aims to define the operator which combines V and W and considering the following simplest form to the optimal V and W simultaneously:(10)maxϕ=(V−W)To unload the computation of selecting features by Eq. (10), an incremental search approach is developed to find the near optimal features defined by Eq. (11). Suppose the feature set with m−1 features is obtained, the task is to select the mth feature from the exclusive set. This is done by selecting the feature that maximizes Eq. (11). So, the respective incremental algorithm optimizes the following condition:(11)maxxj∈X−Sm−1I(xj,xi)−1m−1∑xi∈Sm−1I(xj,xi)The computational complexity of this incremental search method is O(|S|m). As a result, we can obtain the ranked features rapidly even if the dimension of features is very high.Artificial neural networks are well known as powerful tools in the area of pattern classification. In principle, multi-layer feed-forward networks with just a single hidden layer are universal approximators for arbitrary finite-input environment measures [34]. Various types of neural networks techniques have been employed for image classification problems because of their generalization ability [16,35–37]. The process of feed-forward, back-propagation is repeated until the output reaches a desired accuracy, or until a given number of training cycles have been completed [38]. Assume that, a fully connected feed-forward network is chosen for our implementation and the images database is divided into two different samples which are training, and testing samples. The training samples are used to train the network, and the network is adjusted according to its error. Testing samples are then used to provide an independent measure of the network performance during and after training [39]. Suppose that, the input–output of the training sample can be represented by the extracted features’ vectors and their corresponding vectors classes (Xip, Yio), where “p” is the number of input units in the input layer which represents the features of the input image vector, “o” is the number of output units in the output which represents the target classes that the problem is concerned with, and i={1, 2, …, N}, where N is the number of patterns in the training sample. The hidden and the output neurons are sigmoid activation function and all bias input values are set to 1. In the testing stage, for a query image “q”, we extract its features to build a query features vector which then becomes an input vector to the trained neural network and then the network assigns it to one or more similar classes. The performance of the designed classifier is measured in terms of accuracy. This term refers to the ability of the model to correctly predict the class label of new unseen data. Classification accuracy is calculated by determining the percentage of cases in which the test sets are correctly classified. A good classification test always results from high values of accuracy. The accuracy value can be calculated as follows:(12)Accuracy=True positive+true negativeTotal number of imageswhere true positive is the number of correct predictions when an instance is positive; true negative is the number of correct predictions when an instance is negative.A major goal of content-based retrieval is finding the best matched (most similar) images from the multimedia database with respect to a query image. This section describes the traditional similarity measures which are used for matching vectors and introduces a new technique for improving the similarity results. Let the database {x1, …, xj, …, xn} be a set of images represented by features. To retrieve images similar to a query image (q), each image in the database (xj) is compared with the query image using an appropriate distance function d(q, xj). Then, the database images are sorted according to the distances such that d(q, xj)≤d(q, xj+1). So, different similarity/distance measures affect the retrieval performances of an image retrieval system significantly. The choice of distance is extremely important, that in some cases a Euclidean metric will be sensible while in others using a Manhattan metric will be a better choice. Generally, some experience or subject matter knowledge is very helpful in selecting an appropriate distance for a given project. Many similarity measures have been developed for image retrieval process based on empirical estimates of the distribution of features in recent years and we list here some of the most commonly used [40]. One of the most popular similarity distance functions is the Euclidian distance. It is just the sum of the squared distances of two vector values. Assume that the features vector corresponding to ith feature and jth image in images dataset can be denoted by xijand the query vector can be denoted by qi, where i=1, …, m (m is the number of extracted features) and j=1, … n (n is the number of images in dataset). Therefore, the Euclidian distance measure can be formulated as(13)d(q,xj)=∑i=1i=m(xij−qi)2A weighted Euclidian distance function can be developed if it is known that certain features are more valuable than others. The distance score has to be very small for two feature vectors belonging to the same individual and at the same time it needs to be as large as possible for feature vectors coming from different individuals. The formula of weighted Euclidean distance measure can be written as follows:(14)d(q,xj)=∑i=1i=mρi(xij−qi)2where ρidenotes the weighting coefficient of ith feature and can be expressed as(15)ρi=n∑j=1n(xij−z¯i)2where n is the number of images in database and the value ofz¯ican be calculated as(16)z¯i=∑j=1nxijnManhattan distance between the two vectors is calculated as follows:(17)d(q,xj)=∑i=1i=m|xij−qi|The cross correlation can be used as a measure for calculating the degree of similarity between two vectors. Its mathematical definition for two vectors is as given below:(18)r(q,xj)=∑i=1i=mxijqi∑i=1i=m(xij)21/2∑i=1i=m(qi)21/2The value of j which makes r closest to 1 is chosen as the matched image index.The minimum mean distance rule is modified version of the Euclidean distance measure d(q, xj). It is applied with the mean valuez¯jin place of xijand the weights ρi. The mean valuez¯jof each feature vector is computed as(19)zj=∑i=1i=mxijmThe statistical measure compares the maximum and the minimum values among the observation qiand the reference xij. Therefore, it selects the vector which has minimum difference between the Min and the Max values.(20)sj=∑i=1i=mMax(xij,qi)Min(xij,qi)−12The value of j which minimizes sjis selected as the matched image index.The present paper proposes an effective feature matching algorithm in order to compute the similarity value between feature vectors. The proposed algorithm depends on the idea of the minimum area between two vectors. Assume that the images database consists of n images and m features. Therefore, the extracted value of the ith feature and jth image is xijas shown in Table 1.For each feature, we can represent the xijvalues for all images database by a nonlinear regression function fi(xij) as shown in the following equation:(21)fi(xij)=a0+a1xij+a2xij2+⋯+arxijrTo find the coefficients (a0, a1, …, ar) of the polynomial regression model, we set the derivatives with respect to aiequal to zero and the following simultaneous equation are solved for the (r+1) coefficients a0, a1, …, ar.(22)a0a1…ar=n∑j=1nxij……∑j=1n(xij)r∑j=1nxij∑j=1n(xij)2……∑j=1n(xij)r+1……………∑j=1n(xij)r∑j=1n(xij)r+1……∑j=1n(xij)2r−1∑j=1nyi∑j=1nxij⋅yi…∑j=1n(xij)r⋅yiAssume that the non-linear regression function of the feature tiis represented as shown in Fig. 4. The value xijrepresents the value of ith feature and jth image from a given images database, and the value xiqrepresents the value of ith feature of a query image. Then, the area (Δ=abc) under the curve which is resulted from two compared values xijand xiq. This area can be calculated by the integral as follows:(23)Δ=∫xiqxij(f(xiq)−f(xij))dxTherefore, the two values xijand xiqof the ith feature can be identical iff the area under the curve Δ=abc tends to zero.Otherwise, one of the simplest and most popular modeling methods is linear regression. Therefore, in order to simplify the calculation, we can use a linear regression function to represent the proposed model. The form of the linear regression function is(24)fi(xij)=a0+a1xijwhere a0 and a1 are the coefficients of the linear regression model. Fig. 5shows the area (Δ=abc) which is represented by a triangle and can be computed through:(25)Δ=abs|0.5(ab)(ac)|We can substitute (ab) by (xij−xiq) and (ac) by (yq−yi), then the equation can be written as(26)Δ=abs|0.5(xij−xiq)(yq−yi)|Δ=abs|0.5(xij−xiq)[(a0+a1xiq)−(a0+a1xij)]|Δ=abs|0.5(−a1)(xij−xiq)2|Δ=0.5a1(xij−xiq)2Consequently, the total distance measure between the query vector and jth vector of images database can be represented by the total area under all features and can be calculated according to the following equation:(27)D(xij,xiq)=∑i=1m0.5a1i(xij−xiq)2Therefore, the goal is to search for jth vector from a given images database which minimize the total distance measure D(xij, xiq). Finally, we sort the images in the database in ascending order according to the distance measure D(xij, xiq) values to get the retrieval results of the query image. To validate the effect of the proposed matching strategy, we compare its performance with the other six distance metrics methods as shown in the experimental results section.

@&#CONCLUSIONS@&#
