@&#MAIN-TITLE@&#
The exact Gaussian likelihood estimation of time-dependent VARMA models

@&#HIGHLIGHTS@&#
We consider VARMA models with time-dependent coefficients.These coefficients and the error variance can be deterministic functions of time.We present an efficient algorithm for computing their exact Gaussian likelihood.It is based on an algorithm for traditional VARMA models.An illustration on financial data is provided.

@&#KEYPHRASES@&#
Vector VARMA,Time-varying models,Cholesky decomposition method,

@&#ABSTRACT@&#
An algorithm for the evaluation of the exact Gaussian likelihood of anr-dimensional vector autoregressive-moving average (VARMA) process of order (p,q), with time-dependent coefficients, including a time dependent innovation covariance matrix, is proposed. The elements of the matrices of coefficients and those of the innovation covariance matrix are deterministic functions of time and assumed to depend on a finite number of parameters. These parameters are estimated by maximizing the Gaussian likelihood function. The advantages of that approach is that the Gaussian likelihood function can be computed exactly and efficiently. The algorithm is based on the Cholesky decomposition method for block-band matrices. It is shown that the number of operations as a function ofp,qandn, the size of the series, is barely doubled with respect to a VARMA model with constant coefficients. A detailed description of the algorithm followed by a data example is provided.

@&#INTRODUCTION@&#
Let{xt:t∈Z}be anr-vector valued time series. We assume thatxtfollows the time dependent vector autoregressive moving average process of order (p,q), in short tdVARMA(p,q), defined by the following equations:(1.1)xt=∑i=1pAt,ixt−i+yt,(1.2)yt=gtϵt+∑j=1qBt,jgt−jϵt−j,wherepandqare positive integer constants, the coefficientsAt,i,i=1,…,pandBt,j,j=1,…,q, andgt, arer×rmatrices and their elements are deterministic functions of time and assumed to depend on a finite number of parameters, and theϵt’s are independent Gaussianr-dimensional random variables, with zero mean and a positive definite covariance matrixΣ. A stochastic equation like (1.1)–(1.2) requires initial conditions. They will be presented later in Section  3. The vector of parameters of interestθ=(θ1,…,θm)belongs to an open setΘ⊂Rm.Σis considered as a nuisance parameter. In the following we useTanddet(.)as the transpose and the determinant, respectively. If theAt,i’s,Bt,j’s, andgtdo not depend ont, thengtcan be omitted and the data generating process is the classical VARMA process of order (p,q), e.g. Reinsel (1998) or Lütkepohl (2005), where the standard stationarity, invertibility and identifiability conditions are stated. By the way, Lütkepohl (2005, Chapter 17) covers the tdVAR(p) processes. Whenr=1, we are in the case of classical ARMA processes. Note that tdARMA processes have been more deeply studied in the literature, see references in Azrak and Mélard (2006), than tdVARMA processes. Presence ofgtallows for (unconditional) heteroscedasticity. Note that some recent papers deal with time-varying models using a Bayesian approach (Triantafyllopoulos and Nason, 2007), periodic coefficients (Hindrayanto et al., 2010), or focusing on tests (Chen and Hsu, 2014).In principle, Eq. (1.1) should contain an intercept term or a deterministic trend. This has not been done here in order to simplify the related estimation algorithms. It is assumed that the possibly time-dependent mean has been subtracted prior to estimation. There would be no problem to estimate these trend parameters simultaneously with the tdVARMA parameters.Now consider a sample sizenand let thenr×1vectorx=(x1T,…,xnT)Tbe an observed time series of the process defined in (1.1)–(1.2) from which we want to estimateθ. The likelihood function is a density function evaluated as a function of the parameters andx. It is given by(1.3)L(θ;x)=(2π)−nr/2det(S)−1/2exp(−12xTS−1x),withS=covθ(x)thenr×nrcovariance matrix ofx. Mélard (1982) showed in the scalar case,r=1, how to use the Cholesky decomposition method to obtain the exact likelihood of an ARMA process with time dependent coefficients. Azrak and Mélard (1998) developed an algorithm based on the Kalman filter. In this article we show in detail how to compute efficiently the exact likelihood (1.3) by a generalization of Mélard (1982) to the multivariate case. The idea is to transformxinto another vectorwsuch that the matrixSis transformed into a block band matrix depicted in Eqs. (3.2) and (3.3). The block band structure allows for an efficient evaluation of the likelihood. This is done by modifying an algorithm for standard VARMA models, i.e. with constant coefficients, developed by Jónasson and Ferrando (2008). The submatrices in (3.2) and (3.3) were used in that article without the first subscript. Consequently our paper is also a generalization of that algorithm to models with time-dependent coefficients using the Matlab implementation of Jónasson (2008).The asymptotic properties of the estimation methods are the subject of ongoing work. The Gaussian specification is not a restriction. Indeed, the properties are valid for other distributions provided some assumptions are satisfied, such as existence of 8th-order moments. Note also that identifiability conditions should be verified onΣt=gtΣgtT, the covariance matrix ofgtϵt. Here we impose that the non-zero elements(i,j)ofgtare such that∏t=1n(gt)ij=1. Alternatively, a parametrization ofΣtcan be used.Example 1.1For simplicity, we consider the following tdVMA(1) model withr=2xt=Bt,1gt−1ϵt−1+gtϵt,whereBt,1,gtandΣare given by{Bt,1=(B11′B12′B21′B22′)+1n−1(t−n+12)(B11″B12″B21″B22″),gt=(exp{η11n−1(t−n+12)}00exp{η22n−1(t−n+12)})andΣ=(1001).Then, the vector of the parameters to be estimated isθ=(B11′,B12′,B21′,B22′,B11″,B12″,B21″,B22″,η11,η22)T.Example 1.1 can be considered as a vector generalization of the tdMA(1) univariate process given in Azrak and Mélard (2006, Example 4). Here the coefficients depend also on the lengthnof the series but this has no effect on the algorithm.We consider a zero meanr-dimensional Gaussian VARMA(p,q) process, which satisfies the following equation and can be considered as a special case of (1.1) and (1.2):(2.1)xt−∑i=1pAixt−i=ϵt+∑j=1qBjϵt−j,where the matricesA1,…,ApandB1,…,Bqare of dimensionr×rand theϵt’s are like in Section  1. Let us denote byθthe(p+q)r2-dimensional vector of all the parameters to be estimated except forΣ. In our context, we considerΣas a nuisance parameter, not as a parameter of interest. We assume stationarity and invertibility, which require that(2.2)det{A(z)}≠0,det{B(z)}≠0,∀z∈C,such that|z|≤1,whereA(z)=Ir−A1z−A2z2−⋯−Apzp,B(z)=Ir+B1z+B2z2+⋯+Bqzq,Iris the identityr×rmatrix. There are also conditions for identifiability, see e.g. Hannan and Deistler (1988) and Caines (1988). Under these conditions a single model is selected from a set of models that are likelihood equivalent. The likelihood function is given by an expression similar to (1.3), whereShas a simpler form than in Section  1, to be discussed below, and the log-likelihood function of (2.1) is given by(2.3)l(θ;x)=−12[nrlog(2π)+log{det(S)}+xTS−1x].The evaluation of (2.3) requires computing both the inverse and the determinant of the symmetric matrixS, hence the number of multiplications by direct evaluation is roughly proportional to(nr2)3. Some algorithms were developed to reduce substantially the computational effort, e.g. Shea (1984, 1989), Penzer and Shea (1997) via a Kalman filter and Mauricio (2002), Jónasson and Ferrando (2008) via the Cholesky decomposition method. We present an algorithm based on a Cholesky factorization of a block band matrix instead of the block matrixS, like in Jónasson and Ferrando (2008). For other references, see Jónasson and Ferrando (2008). Related references are also Mauricio (2006) and Mélard et al. (2006). In the latter paper, VARMA models in echelon form are handled by maximum likelihood. In principle, this can also be done in the present context.Ansley (1979) has obtained a band covariance matrix for a univariate ARMA model, which enables a simple Cholesky decomposition and computationally efficient evaluation of the likelihood. Mauricio (2002) has adapted Ansley’s algorithm to the multivariate case and argues that it is more efficient for likelihood evaluation than the Kalman filter methods. The fundamental device used by Ansley is a transformation fromxto a vector of random variablesztdefined as follows:(2.4)zt={xtfort≤s∗yt=xt−∑i=1pAixt−ifort=s∗+1,…,n,wheres∗=max(p,q). In our method, we prefer to use the slightly more efficient variant wheres∗=p, first used in the univariate case by Mélard (1982) and Penzer and Shea (1997), and in the multivariate case by Jónasson and Ferrando (2008).The idea behind it is to transformxTS−1xtowTw, withwa vector of uncorrelated random variables. Letz=(z1T,…,znT)T, then the transformation fromxtozhas unit Jacobian so that (2.3) can be expressed as:(2.5)−12[(nrlog(2π)+log{det(Ω)}+zTΩ−1z)],whereΩis a symmetric block-band matrix with blocksΩ(t,s)r×r=cov(zt,zs). That matrixΩcontains the following covariances matrices:(2.6)Wj=cov(yt,yt−j)=∑i=jqBiΣBi−jT,j=0,1,…,q,(2.7)Gj=cov(yt,xt−j)=Wj+∑i=1min(p,q−j)Gi+jAiT,j=q,q−1,…,0.LetB0=Ir,Wj=0andGj=0forj<0andj>q. We haveSj=cov(xt,xt−j),forj=0,1,…,p,which satisfy the following linear system (the vector Yule–Walker equations)S0−A1S1T−A2S2T−⋯−ApSpT=G0S1−A1S0−A2S1T−⋯−ApSp−1T=G1(2.8)S2−A1S1−A2S0−⋯−ApSp−2T=G2⋮Sp−A1Sp−1−A2Sp−2−⋯−ApS0=Gp.We have a linear system with ther(r+1)/2+pr2elements ofS0,…,Sp(noting thatS0is symmetric). The solution of this linear system is dealt with in Jónasson and Ferrando (2008, Appendix B) which is close to that given by Mauricio (1997). The computation of the theoretical autocovariance of a VARMA process has been considered by Ansley (1980), Hall and Nicholls (1980), Kohn and Ansley (1982) and Mauricio (1995).The matrixΩis a block band matrix, withmax(p,q+1)diagonal blocks along each side of the main diagonal blocks. Its precise form in the casep≥qandp<qcan be seen in Section  3.2 by omitting the first subscript everywhere. Now taking the Cholesky decompositionΩ=LLT, whereLis a block band lower triangular matrix, the rows ofLare evaluated recursively using a standard algorithm for block matrices and the entries inw=L−1zcan be calculated by block forward substitution. Thendet(Ω)=∏i=1nrlii2, whereliiis(i,i)thelement ofLand the log-likelihood function is given by(2.9)−12{nrlog(2π)+2∑i=1nrlog(lii)+wTw}.Remark 2.1The method used by Jónasson and Ferrando (2008) is slightly different from ours in the calculation ofGj. Their method is based on the calculations of a set of covariancesCjdefined byCj=cov(xt,ϵt−j)=BjΣ+∑i=1min(p,j)AiCj−i,whereB0=IrandCj=0forj<0andj>q. Then from (2.7) we get(2.10)Gj=∑i=jqBiCi−jT.Determining theWj’s,Cj’s andGj’s by using (2.6), (2.7) and (2.10) costs aboutr3(s2/2+q2)multiplications (Jónasson and Ferrando, 2008) wheres=min(p,q). However, by using our algorithm the determination ofGj’s andWj’s costs aboutr3q2multiplications ifp≥qand otherwise aboutr3(q2/2+qp−p2/2)multiplications. Thus, our variant has two advantages, since it is slightly faster, and also simpler by calculating the covariancesGjby a simple recurrence formula.Thus an efficient algorithm for computing the exact likelihood function defined in (2.3) for a standard VARMA(p,q) process exploiting the full symmetric block-band property ofΩis as follows. For a given value ofθ, the log-likelihood is computed through the following stages.Stage 1:Compute the covariancesWjandGjforj=0,1,…,qby using (2.6)–(2.7), and compute the covariancesSjforj=0,1,…,p−1, by solving the linear system of vector-Yule–Walker equations defined in (2.8).Computeztfort=p+1,…,nby (2.4).Compute the matrixLby a block triangularization algorithm for the block band matrixΩ.Determinew=L−1zusing block forward substitution.ComputewTw,log{det(Ω)}=2∑i=1nrlog(lii)and (2.9).As explained in Section  1, the tdVARMA(p,q) model requires initial condition. For startup reasons, as we have no information on the past behavior of the process, we assume(3.1){At,i=Ai≔A0,ifori=1,…,pandt=0,−1,…,−p+1,Bt,j=Bj≔B0,jforj=1,…,qandt=0,−1,…,−q+1,gt=g1fort=0,−1,…,−q+1,the last line for compatibility with the LIKAMT program (Mélard, 1982), instead ofg0which would have been more coherent. We suppose in addition (2.2) so that the process defined in (1.1) and (1.2) is causal and invertible in the past (t≤0). The assumption of causality in the past implies that we can calculate the covariance of the stationary process att=0. Fort≤0the covariancesSt,j=Sj≔S0,jfor0≤j≤pare obtained like in the stationary VARMA(p,q) models by solving the linear system of equations given in (2.8), but withΣin (2.6) replaced byg1Σg1T. We refer the reader to e.g. Lanne and Saikkonen (2013) for the studies of non-causal VAR models, and for (univariate) AR models, see e.g. Brockwell and Davis (1991, Chapter 3). The assumption of invertibility in the past implies that the non-stationary process is invertible (Hallin, 1978), i.e. for all finitet,ϵtcan be expressed as a mean-square convergent limit of linear combinations of(xt,xt−1,xt−2,…). Without that property, the process would remain unidentifiable and inefficient forecasts would be produced by the model, see Hallin (1986).Using a transformation like in (2.4) for the model defined in (1.1)–(1.2) but withAireplaced byAt,i, we obtainz=ΛxwhereΛis thenr×nrlower triangular block-band matrix given by:Λ=[Ir0⋯00⋱Ir⋮−Ap+1,p⋯−Ap+1,1Ir⋱⋱⋱00−An,p⋯−An,1Ir].SinceΛhas unit diagonal, the log-likelihood (1.3) can be expressed as:l(θ;z)=−12[nrlog(2π)+log{det(Ω)}+zTΩ−1z],but the blocks ofΩ=covθ(z)are now ther×rcovariances matrices:{Wt,j=cov(yt,yt−j),j=0,1,…,q,Gt,j=cov(yt,xt−j),j=0,1,…,q,St,j=cov(xt,xt−j),j=0,1,…,p,which depend ont. We will provide relations to evaluate these matrices in Section  3.3. More precisely, ifp≥q,(3.2)whereas ifp<q, the depiction is slightly different with(3.3)Let us now detail the computation of these blocks.The following relations are a generalization of Mélard (1982) to the vector case.–The covariancesWt,jFort=1,…,nandj=0,1,…,q,Wt,j=∑k=0q∑l=0qBt,kgt−kcov(ϵt−k,ϵt−j−l)gt−jTBt−j,lT,and sincecov(ϵt−k,ϵt−j−l)=Σδk,j+l, withδk,j=1ifk=jand 0 otherwise, lettingBt,0=Ir, we have(3.4)Wt,j=∑k=jqBt,kgt−kΣgt−kTBt−j,k−jT,j=0,1,…,q,withWt,k=0fork>q.–The covariancesGt,jFort=1,…,p+qandj=q,q−1…,0, the covariancesGt,jcan be obtained by the recurrence formula:(3.5)Gt,j=Wt,j+∑i=1min(p,q−j)Gt,i+jAt−j,iT,andGt,j=0forj>q, becauseytis composed ofgt−1ϵt−1,…,gt−qϵt−qwhich are independent fromxt−j.–The covariancesSt,jFort=1,…,pandj=0,1,…,p, the autocovariancesSt,jof the process are obtained by the recurrences(3.6)St,j=Gt,j+∑i=1pAt,iSt−i,j−i,recalling thatSt,j=St,0fort≤0and noting that forj−i<0(3.7)St−i,j−i=cov(xt−i,xt−j)=cov(xt−j,xt−i)T=St−j,i−jT.To our knowledge, this is the first algorithm which extends the Cholesky decomposition method to the tdVARMA case. In scalar tdARMA models there exist only two algorithms to compute the exact likelihood function. The first one denoted LIKAMT (see Mélard, 1982) was also based on a Cholesky decomposition. The second one TKALMAR is based on the Kalman filter (see Azrak and Mélard, 1998). In addition Azrak and Mélard (1998) have compared the two algorithms in the univariate case. Their conclusions are that LIKAMT is faster for some models. On the contrary, for models with orderpandqequal to or larger than 13, they can observe a superiority of TKALMAR.Now we can summarize the algorithm in the case of tdVARMA models in the 6 stages below:Stage 1:Starting conditions:•The coefficientsAt,iandBt,jfort≤0,i=1,…,p,j=1,…,qand the values ofgtfort≤1are determined by using (3.1);The covariancesW0,j,G0,jforj=0,…,qare determined using (2.6) and (2.7), and the covariancesS0,jforj=1,…,pare obtained by solving (2.8).We compute:•The covariancesWt,jby using (3.4) forj=0,1,…,qandt=1,…,n;The covariancesGt,jby using (3.5) forj=q,q−1,…,0andt=p+1,…,p+q;The covariancesSt,jby using (3.6) forj=0,1,…,pandt=1,…,p.zt=ytis computed fort=p+1,…,n.Compute the matrixLby a triangularization algorithm by blocks of the block band matrixΩ.Determinew=L−1zusing block forward substitution.ComputewTw,log{det(Ω)}=2∑i=1nrlog(lii)and (2.9).Remark 4.1In Mélard (1982) and its implementation for the univariate caser=1only four square matrices of sizemax(p,q)+1are needed.Remark 4.2In the case where there is a deterministic trend, as discussed in the Introduction, none of the matrix calculations in the Algorithm of Section  4 would change; only the vector computations in stages 3, 5 and 6 would be affected.Remark 4.3It could be argued that it would be more numerically stable to use QR rather than Cholesky factorization. However, extensive numerical experiments done in Jónasson (2008), and following that work, indicate that the Cholesky method is unlikely to suffer instabilities for practical VARMA time series modeling. We see no reason why adding time dependence should change that.We consider the following mixedr-variate tdVARMA(2, 1) process,xt=At,1xt−1+At,2xt−2+Bt,1gt−1ϵt−1+gtϵt,where the coefficientsAt,1,At,2,Bt,1andgtare deterministic functions of time, thenzt={xtift=1,2,ytift=3,…,n,andΩ=(S1,0S2,1TS2,1S2,0G3,1TG3,1W3,0W4,1TW4,1W4,0⋱⋱⋱⋱⋱Wn−1,0Wn,1TWn,1Wn,0).By using the previous algorithm we have:•Stage  1:{At,i=A0,it=−1,0,i=1,2,Bt,j=B0,jt=0,j=1,gt=g1t=0,and the covariancesW0,0,W0,1,G0,0andG0,1like in Section  2.2, but withΣin (2.6) replaced byg1Σg1T. Then theS0,j’s forj=0,1,2are determined by solving (2.8).Stage  2: Fort=1,2, we should start by calculating theWt,j, by using (3.4):W1,0=g1Σg1T+B1,1g0Σg0TB1,1T,W1,1=B1,1g0Σg0T,W2,0=g2Σg2T+B2,1g1Σg1TB2,1T,W2,1=B2,1g1Σg1T.Note thatg1=g0. SimilarlyWt,0=gtΣgtT+Bt,1gt−1Σgt−1TBt,1TandWt,1=Bt,1gt−1Σgt−1Tfort=3,…,n. For the covariancesGt,kwe have by using (3.5):G1,1=W1,1,G2,1=W2,1,G3,1=W3,1,G1,0=W1,0+G1,1A1,1T,G2,0=W2,0+G2,1A2,1T.Taking care of the covariancesSt,jby using the covariances computed before, we have fort=1:S1,2=A1,1S0,1+A1,2S0,0,S1,1=G1,1+A1,1S0,0+A1,2S0,1T.Knowing that by (3.7)S0,−1=S1,1TandS−1,−2=S1,2Twe haveS1,0=G1,0+A1,1S1,1T+A1,2S1,2T.Then, fort=2:S2,2=A2,1S1,1+A2,2S0,0,S2,1=G2,1+A2,1S1,0+A2,2S1,1T,and, finally knowing thatS0,−2=S2,2TandS1,−1=S2,1Twe haveS2,0=G2,0+A2,1S2,1T+A2,2S2,2T.By using the remaining stages of the above algorithm we can evaluate the log-likelihood function of this tdVARMA(2, 1) process.Remark 4.4We can notice thatΩdoes not contain some of the computed covariances likeS1,1,S1,2,…. However they are involved in the calculation of the other covariances.Table 1gives the total number of multiplications required by our algorithm. Ifpis small compared toqand ifqis small enough with respect ton, the stages 4, 5 and 6 will take the most of the computation time and the total number of operations to computezTΩ−1zisO(n).Finally, the total number of multiplications is of ordern[r3q2+r2(3p/2+q)]+r6p3/3for a tdVARMA(p,q) andn[r3q2/2+r2(3p/2+q)]+r6p3/3in the case of a VARMA(p,q), by neglectingr,pandqwith respect tonbut not their powers or products. In addition, for the term proportional ton, the total number of multiplications for tdVARMA models is about less than twice what is needed for VARMA models of the same order(p,q)(not counting operations for the computation of the coefficients).In this part we illustrate the use of the algorithm and its implementation in Matlab, called AJM, which represents a generalization of Jónasson (2008) Matlab’s program to tdVARMA models. The estimation of the parameters by AJM is based on the maximization of the exact Gaussian likelihood function using the fminunc Matlab function belonging to the optimization toolbox, see Coleman et al. (1999). This function uses the BFGS quasi-Newton method with a mixed quadratic and cubic line search procedure. AJM can be summarized in three steps. In Step 1 we estimate the mean and the observation covariance matrix which will be used as initial values forμandΣin Step 2. Step 2 estimates all the parameters of the model. Finally Step 3 estimates only the parameters of interest in order to derive the covariance matrix of the estimates. This is an alternative to the Tunnicliffe Wilson (1973) method for standard VARMA models.The series (Example 8.1 in Tsay, 2005) considered here is a bivariate (r=2) time seriesxt=(xt,1,xt,2)Twherext,1andxt,2represent respectively the monthly log returns of IBM stock and S&P500 index from January 1926 to December 1999 withn=888observations. Fig. 1shows the time plots ofxtusing the same scale.Tsay (2005) showed, by using the cross-correlation matrices and other techniques (selection criteria, …), that a VMA(3) should be adequate. It is given by(6.1)(xt,1xt,2)=μ+(ϵ1,tϵ2,t)+B1(ϵ1,t−1ϵ2,t−1)+B3(ϵ1,t−3ϵ2,t−3),whereμandΣrepresent respectively the mean ofxtand the covariance matrix ofϵt,B1andB3are the coefficients. The estimation results of complete and simplified models of (6.1) are given in Table 2. The simplified models were obtained by removing one by one the non-significant parameters at the 5% significant level, starting with the least significant, until all the parameters are significantly different from zero. The justification is to reduce the number of parameters to the minimum.We can see that the estimation results found by our program and those given in Tsay (2005, Table 8.6, p. 370) and obtained by using SCA version 8.1 are very close.Let us now assume that the bivariate series follows a tdVMA(3) model, a special case of (1.1)–(1.2). Instead of being constant like in (6.1) we assume that the coefficients are slowly varying deterministic functions of time. We use linear functions of time except for the scalegtwhich is supposed to be exponential to avoid negative values:(6.2)(xt,1xt,2)=μ+gt(ϵ1,tϵ2,t)+Bt,1gt−1(ϵ1,t−1ϵ2,t−1)+Bt,3gt−3(ϵ1,t−3ϵ2,t−3),with(6.3){Bt,1=B1′+(t−n+12)B1″,B1′=(B11′1B12′1B21′1B22′1)andB1″=(B11″1B12″1B21″1B22″1);Bt,3=B3′+(t−n+12)B3″,B3′=(B11′3B12′3B21′3B22′3)andB3″=(B11″3B12″3B21″3B22″3);gt=(exp{η11(t−n+12)}00exp{η22(t−n+12)}).We have first considered a model without heteroscedasticity i.e. withoutgt, and then a model with heteroscedasticity i.e. withgt. Table 4 shows the estimation results of the parameters of interest of (6.2)–(6.3) using the program based on the algorithm described in this paper. For example, for the full model without heteroscedasticity 14 iterations were needed and the computation time on a computer with a 3 GHz Core Duo E8400 processor was around 385 s. The simplified model was obtained again by removing one by one the least non-significant parameters. The theory behind the use of the standard errors andt-statistics for tdVARMA models is based on Alj et al. (in preparation), a generalization of Azrak and Mélard (2006) to multivariate models. Table 3gives the information criteria of each model, where AICc is the corrected Akaike information criterion (Hurvich and Tsai, 1989), SBC is the Schwarz Bayesian criterion (Rissanen, 1978; Schwarz, 1978), HQC is the Hannan–Quinn criterion (Hannan and Quinn, 1979) and finally FPE represents the final prediction error criterion. Note thati.Most of the parameters inB1″andB3″were significantly different from 0.It even happens that the final simplified models involve noB1′andB3′parameters, showing that the trend in the coefficients is more important than the intercept.The criteria in Table 3 agree in favor of the simplified model with heteroscedasticity.Consequently, the suggested kind of models seems useful for at least one pair of series and the proposed algorithm seems justified.Here we have modeled the time dependence with a linear function. There would be no problem to use polynomials int. For example, using a quadratic function of time would produce an increase of about 50% for the number of parameters in the full model.

@&#CONCLUSIONS@&#
