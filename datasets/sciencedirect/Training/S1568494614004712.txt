@&#MAIN-TITLE@&#
A study on visual sensor network cross-layer resource allocation using quality-based criteria and metaheuristic optimization algorithms

@&#HIGHLIGHTS@&#
Optimal allocation of source and channel coding rates and power levels of nodes.Minimization of the average and maximum distortion of video received by all nodes.Mixed integer optimization problems.Mixed-integer optimization problems arise.The particle swarm optimization (PSO) algorithm is used.A hybrid algorithm that combines PSO and active set algorithm is used.

@&#KEYPHRASES@&#
Hybrid algorithms,Metaheuristic optimization,Particle swarm optimization,Power control,Resource allocation,Visual sensor network,

@&#ABSTRACT@&#
Visual sensor networks (VSNs) consist of spatially distributed video cameras that are capable of compressing and transmitting the video sequences they acquire. We consider a direct-sequence code division multiple access (DS-CDMA) VSN, where each node has its individual requirements in compression bit rate and energy consumption, depending on the corresponding application and the characteristics of the monitored scene. We study two optimization criteria for the optimal allocation of the source and channel coding rates, which assume discrete values, as well as for the power levels of all nodes, which are continuous, under transmission bit rate constraints. The first criterion minimizes the average distortion of the video received by all nodes, while the second one minimizes the maximum video distortion among all nodes. The resulting mixed integer optimization problems are tackled with a modern optimization algorithm, namely particle swarm optimization (PSO), as well as a hybrid scheme that combines PSO with the deterministic Active-Set optimization method. Extensive experimentation on interference-limited as well as noisy environments offers significant intuition regarding the effectiveness of the considered optimization schemes, indicating the impact of the video sequence characteristics on the joint determination of the transmission parameters of the VSN.

@&#INTRODUCTION@&#
Wireless sensor networks have constituted a very active research topic in computer science and telecommunications over the past few years. Initially, such networks were mainly concerned with the transmission of unidimensional signals (e.g., temperature, sound etc). Nowadays, their applications have been expanded to the transmission of visual data, such as images or videos. This type of wireless sensor network that conveys visual data is the well-known visual sensor network (VSN).In VSNs, each node is equipped with a camera for imaging different fields of view and detecting events of interest. VSNs support a plethora of applications, ranging from security and teleconference systems to environmental monitoring [1]. Security monitoring includes the surveillance of large areas such as motorways, airports, banks, and other public or private places, mainly for the prevention of unpleasant events or illegal activities. Environmental monitoring is concerned with the monitoring of natural environment in remote locations where human presence is often impossible. These monitoring systems deter illegal construction, logging, hunting etc. Concerning teleconference systems, they enable users to participate remotely in events of interest, such as attending a meeting or a lecture that is taking place in a different location, or even virtually visit an exhibition or a museum.In the present work, we consider a direct-sequence code division multiple access (DS-CDMA) VSN, where the sensor nodes record scenes with varying motion levels, which is a common approach in real-time VSN applications. All the nodes of the network communicate directly with a central processing server, called the centralized control unit (CCU), and they exchange information in order to achieve the ideal tradeoff between the transmitted video quality and energy consumption. This policy is based on a cross-layer design that differs from usual layered network architectures like the open systems interconnection (OSI) [2], and the transmission control program/internet protocol (TCP/IP) models. In a layered network design, each layer is responsible for executing specific tasks, sequentially transferring information from layer to layer according to their hierarchical ordering. On the contrary, the cross-layer design employed in our study is more flexible since it allows for different network layers to interact with each other in order to exchange information, even they are not adjacent. Hence, it relaxes the austere limits of a layered network infrastructure, ameliorating the overall system performance.Various cross-layer designs have been proposed in the literature, based on application-specific requirements [3–8]. Our study is based on such a network, where the physical layer, data link layer, network layer, and application layer are allowed to cooperate with each other with the goal of overcoming the delay latency difficulties and improving the quality of service (QoS) of real-time applications. Indeed, the majority of VSNs applications require real-time data from sensor nodes [1], i.e., data shall be transmitted from nodes to the end-user within an extremely limited time frame, such that the total transmission delay is imperceivable. Yet, there are some negative factors that affect real-time system performance. Data imaging, processing, and transmission, in conjunction with the constraints that govern wireless channels (e.g., available bandwidth or bit rate, modulation scheme, video coding standard, and wireless access method) can dramatically slow down real-time system response, thereby deteriorating the overall efficiency of the VSN.Wireless channels are generally unreliable due to frequent errors that occur during wireless data transmissions. Thus, if our primary concern is to maintain a good level of video quality, we should aim at maintaining a low transmission bit error rate (BER). To this end, suitable dynamic adjustment of sensor nodes’ transmission parameters is required to maximize the VSN's performance. Specifically, each sensor node has a bit rate that can be used for both source coding and channel coding, while it also has an amount of power necessary for sensing, processing, and transmission of the captured data. Hence, the source coding rate, channel coding rate, and power constitute the transmission parameters of each node. Naturally, each node compresses the captured data at a different source coding rate according to the detected amount of motion in each scene. Thus, channel coding rate shall be different for each node. Under a total bit rate constraint, a higher source coding rate results in a lower channel coding rate, and vice versa. Consequently, higher levels of power are required for data transmission due to the lower protection from channel errors.Data transmission is scourged mainly due to multi-path fading, shadowing at the physical layer, and co-channel interference at the medium access control layer [9]. Hence, channel coding is used to increase the reliability of transmissions. Automatic repeat request (ARQ) and forward error correction (FEC) are two widely used schemes, suitable for error-correction over wireless channel transmissions [10]. ARQ is based on the retransmission of missing data packets. Its weak points are the need for a feedback channel and the time required for recovering missing packets, which impose significant limitations in real-time applications. Instead, FEC schemes can tolerate some amount of losses, allowing data transmissions under lower power consumption. However, in the special cases where there are too many losses, they can be handled by usual ARQ techniques.In addition, given that sensor nodes are battery-operated systems, energy control determines the lifetime of their battery. The high complexity of data processing and analysis that accompanies large amounts of video information, along with the real-time application requirements for data transmissions, result in rapid battery drain of the sensor. Energy conservation can be achieved through proper power control in order to maximize the sensors’ energy-efficiency [11–13]. Besides that, the multimedia content transmissions that require high bandwidth demands shall be taken into consideration. In [14] a bandwidth management framework is proposed to coordinate multiple video flows in order to overcome wireless channel resource limitations. Further VSN challenges are highlighted in [15].In the present study, a cross-layer multi-node optimization design accounts for the overall system performance through all network layers. Particularly, it is responsible for the optimal determination of the source coding rates, channel coding rates, and power levels at the application layer, data link layer, and physical layer, respectively, while the CCU lies at the network layer. The source coding rate determines the compression rate of a video sequence, while the channel coding rate defines the relative protection of the transmitted video sequence. Regarding the transmission power, on one hand, it should be adequately high to permit data transmission and maintain the quality of the video reception. On the other hand, it needs to be adequately low in order to prolong battery lifetime, keep interference at low levels among nodes, and efficiently exploit channel capacity, resulting in high system QoS.In order to optimally and jointly allocate system resources to all nodes, we considered two quality-based optimization criteria aiming at video distortion minimization. The first one, called the minimized average distortion (MAD), minimizes the overall average video distortion of the network, neglecting fairness among the nodes. The second criterion, called the minimized maximum distortion (MMD), minimizes the maximum distortion among all nodes of the network, promoting a rather unbiased treatment of the nodes. For both criteria, we require that the total bit rate is identical for all nodes. These criteria have been previously used in similar resource allocation problems [16–18]. However, in these works, they were applied to purely discrete optimization problems, since the source coding rates, channel coding rates, and power levels assumed discrete values solely. Contrary to this, in the present work we considered continuous (and bounded) power levels, while source coding rates retain discrete values, since channel coding rates can take values only within a finite discrete set [19]. Promising preliminary results have offered strong motivation for the further investigation of the specific problems [20].The solution methodology for our problem was a significant issue for both our optimization criteria. Deterministic mixed integer programming methods can be used to tackle such problems. Indeed, branch and bound approaches combined with outer approximation algorithms have been proposed in the literature [21]. Despite the robustness that usually accompanies deterministic algorithms, various issues may arise. For instance, they may require significant implementation effort and their required memory and running-time can be exponentially increased with the number of integer variables. In addition, sensitivity to initial conditions shall be taken into consideration.On the other hand, established population-based optimization algorithms can offer satisfactory solutions at the cost of reasonable computational requirements and minor implementation effort. Also, they concurrently evolve a population of candidate solutions that may constitute useful suboptimal alternatives with slightly different characteristics than the optimal one. Applications of population-based metaheuristics such as evolutionary algorithms, ant colony, greedy randomized adaptive search procedure, and particle swarm optimization in wireless sensor networks can be found in [22–25]. The highly appreciable properties of metaheuristic algorithms triggered our interest in using such methods to the resource allocation problems under investigation.In particular, we employed the particle swarm optimization (PSO) algorithm, which is a computational intelligence approach that draws inspiration from social dynamics. PSO has proved to be very efficient in a plethora of complex engineering problems [26]. This fact, along with its stochastic nature, relieves the user from the burden of presenting an appropriate initialization to the algorithm. The promising preliminary experimental results in [20] suggested the use of PSO as the optimizer for solving our resource allocation problems.For comparison reasons, the performance of PSO was compared to that of the deterministic active-set (AS) algorithm [27–29]. In addition, motivated by the promising performance of hybrid algorithms that combine population-based approaches with deterministic schemes (often called memetic algorithms) [30], we also considered a hybrid algorithm that combines PSO with AS. The new algorithm, henceforth denoted as HPSOAS (Hybrid PSO-AS), aims at exploiting the benefits of both PSO and AS, thereby increasing efficiency.The rest of the paper is organized as follows: in Section 2, we present the transmission parameters of the network under consideration along with procedures for their determination, the wireless channel access method, as well as the universal rate distortion characteristics model. The two optimization criteria that minimize either the average or the maximum distortion of the network are analyzed in Section 3. The employed optimization algorithms are described in Section 4. In Section 5, the system and algorithms’ parameters are defined, followed by experimental results meticulously presented and discussed. Finally, Section 6 summarizes the key-concepts and conclusions of our study.In this section, we discuss the considered source and channel coding techniques for encoding video sequences, as well as the basic characteristics of the considered wireless DS-CDMA VSN. Moreover, we present the estimation procedures for the expected video distortion.Video compression is essential in communications due to limitations in the bandwidth of the communication channel. Different video sequences have different bit rate requirements for their compression. Generally, these requirements depend on the amount of motion in the video sequence. Clearly, video sequences with less motion can be source encoded at a lower bit rate while still maintaining good perceptual quality. On the other hand, video sequences that contain intense motion activity shall be compressed at a higher bit rate in order to avoid significant degradation of the video quality.Thus, assuming that the total bit rate is fixed, if a node needs a higher source coding rate, a lower percentage of the total bit rate is assigned to channel coding for error correction. Hence, in order to keep the BER at acceptable levels, the transmission power must be increased. In our study, we assumed that each node has the power required for video transmission over the VSN. Inevitably, the energy consumed for data transmission leads to shortening of the battery life and, due to the nature of DS-CDMA, to increased interference imposed to the other nodes of the network.The CCU at the network layer plays the role of the coordinator of the overall process of resource allocation. Specifically, it communicates directly with each node, performing source and channel decoding so as to receive the transmitted video sequences. Depending on the amount of motion detected in each video sequence, the central server can request from the nodes to properly adjust their transmission parameters, namely the source coding rate, channel coding rate, and power levels. For instance, if the CCU considers that a node is imaging scenes of great interest, it tries to maximize the picture quality of the specific video by appropriately adjusting its transmission parameters.We employed the H.264/advanced video codec (AVC) video coding standard to compress the video sequences imaged by the nodes, using the High profile for 4:2:0 color format video. This standard is targeted at many applications such as video telephony, storage, broadcast, and streaming [31]. The coded video data is organized in network abstraction layer (NAL) units, which are packets containing an integer number of bytes. These units are grouped into video coding layer (VCL) NAL units and non-VCL NAL units. The VCL NAL units contain the data that represents the values of the pixels in the video pictures, while non-VCL NAL units contain parameter sets and supplemental enhancement information [32]. Concerning the H.264/AVC High profile for the aforementioned color format, it has proved to be extremely efficient in coding, taking into consideration the available coding tools for the encoder [31].Channel coding is used to increase the communication channel reliability by increasing resistance to channel errors. Specifically, it adds redundancy in the video bitstream, unlike source coding which intends to represent data with the smallest possible number of bits. In our paper, an adaptive FEC scheme using rate compatible punctured convolutional (RCPC) codes is utilized for channel coding [19]. These are families of codes with different rates, which can be decoded by the same Viterbi decoder. However, other channel coding schemes can also be used.The use of RCPC codes allows the use of Viterbi's upper bounds on the bit error probability. Thus, for the bit error probability Pb, it holds that [19],(1)Pb⩽1P∑d=dfree∞cdPd,where P is the period of the code; dfree is the free distance of the code; cdis the information error weight; and Pdis the probability that the wrong path at distance d is selected.Let us assume that information is sent over a channel subjected to additive white Gaussian noise (AWGN). Also, let binary phase shift keying (BPSK) be the employed modulation scheme. Then, the probability Pdbecomes [19],(2)Pd=12erfcdRcEkN0,where,(3)erfc(x)=2π∫x∞exp(−t2)dt,is the complementary error function; Rc is the channel coding rate; and Ek/N0 is the energy per bit to multiple access interference (MAI) ratio. The index k denotes the corresponding node of the network.DS-CDMA is the wireless VSN access method that we adopted in the current study. This method allows all nodes to transmit over the same channel, sharing the same bandwidth. Furthermore, since all nodes transmit over the same channel, transmissions are affected by generated interference from the other nodes, mainly due to non-orthogonal spreading codes, possible asynchronous transmissions, and multi-path fading. The target is to limit the interference as much as possible in order to ameliorate the video quality, retain low power consumption, and achieve the effective exploitation of the system's capacity without affecting the integrity of the data transmission procedure.After source and channel coding, each data signal is assigned a spreading code, usually orthogonal or pseudo-random to the codes assigned to the other signals, such that the interference between two signals is minimized. In order to transmit a single bit, a node actually transmits L chips, where L is the spreading code length. Usually, the chip rate (number of transmitted chips per second) is identical for all nodes. We assume that the spreading code length is identical for all nodes. Thus, a constraint on the chip rate corresponds to a constraint on the bit rate. DS-CDMA systems are usually interference-limited systems and therefore, it is common for the thermal noise and background noise to be neglected. The power level for each node k=1, 2, …, K, is given by,(4)Sk=EkRk,and it is measured in Watts (W), where Ekis the energy per bit, and Rkis the total bit rate used for both source and channel coding.In fact, Skrefers to the power received by the CCU from node k. Therefore, for given power levels, the required transmission powers for the nodes can be determined through a propagation model. Assuming the two-ray ground reflection model as the propagation model, the transmission power for node k is given by [33],(5)Sktrans=Skdtr4GtGrht2hr2,where dtr is the distance between the transmitter (node) and the receiver (CCU); Gt is the transmitter antenna gain; Gr is the receiver antenna gain; ht is the height of the transmitter; and hr is the height of the receiver.Concerning the total bit rate Rk, it is defined as,(6)Rk=Rs,kRc,k,and it is measured in bits per second (bps). The quantity Rs,krepresents the source coding rate, also measured in bps, while Rc,kis the channel coding rate of k-th node. Obviously, since Rc,kis the ratio of the number of information bits over the total number of bits, it is a scalar within the range (0, 1) [3].In our investigation, we followed the assumption that the interference can be approximated by AWGN [34,35]. Thus, the energy per bit to MAI ratio is given by,(7)EkI0=Sk/Rk∑j≠kK(Sj/Wt),k=1,2,…,K,where I0/2 is the two-sided noise power spectral density due to MAI, measured in Watts/Hertz (W/Hz), and Wtis the total available bandwidth, measured in Hertz (Hz). Again, k refers to the corresponding node, while j refers to each interfering node.In Eq. (7), the following fundamental assumptions were made:(a)The thermal and background noise were ignored.The spreading codes used are random and do not have any special properties.Interference suppression filters are not used.Assumptions (b) and (c) suggest that no means is used to suppress or limit the co-channel interference, implying that each node admits the power of the other nodes totally as interference. If we drop assumption (a), i.e., assuming that thermal and background noise are rather significant, then, instead of Ek/I0, we must use the following energy per bit to multiple access interference and noise ratio,(8)EkI0+N0=Sk/Rk∑j≠kK(Sj/Wt)+N0,k=1,2,…,K,where N0/2 is the power spectral density of the AWGN.Image quality is highly related with the number of occurred errors during data transmission. Therefore, the BER determines the expected video distortion, which has an immediate impact on the video quality. In this paper, in order to compute the expected video distortion, universal rate-distortion characteristics (URDC) are used [3]. URDC express the expected distortion as a function of the bit error probability after channel decoding. Specifically, having compressed the video sequences with the H.264/AVC video codec at specific source coding rates, and for given BERs at each time, we can estimate the expected distortion value [18].Similarly to relevant studies [18,20,36–38], the model we considered for the expected video distortion per node k is given by,(9)E[Ds+c,k]=αlog101Pb−β,where Pb corresponds to the bit error probability or, in other words, to the BER. The parameters α and β are positive, and they are determined through a mean squared error optimization procedure using a limited number of pairs (E[Ds+c,k], Pb), which are experimentally obtained for specific BERs. The values of these parameters depend on the source coding rate and the video sequence characteristics. Specifically, the parameter α usually takes lower values for video sequences with low amount of motion and higher values for video sequences with high amount of motion [39].Due to the fact that channel errors and packet drops occur randomly, the video distortion attributed to the lossy compression and channel errors is a random variable. To this end, the video distortion is averaged over a number of independent experiments. Another assumption made in our study was that all nodes transmit data using the same total bit rate. In fact, this constraint results from a fixed overall transmission chip rate and the same processing gain (spreading code length) for all nodes, since it holds that,(10)Rk=RchipL,where Rchip is the chip rate, measured in chips per second, and L is the spreading code length, measured in chips.We further assumed that the channel coding rates, Rc,k, k=1, 2, …, K, can take only discrete values [19] from a set Rc. Assuming that Rkis fixed, from Eq. (6) it follows that the source coding rates Rs,k, k=1, 2, …, K, must also take discrete values from a set Rs. Namely,Rc,k∈Rc,Rs,k∈Rs,k=1,2,…,K.Let the index cb=1, 2, …, CB, denote the admissible source coding rate-channel coding rate combinations. Then, the combination (Rs,k, Rc,k) assumes discrete values from a set,Rs+c=(Rs,k,1,Rc,k,1),…,(Rs,k,cb,Rc,k,cb),…,(Rs,k,CB,Rc,k,CB).The cardinality of Rs+c is CB. Evidently, the cardinalities of the sets Rs, Rc, and Rs+c shall be equal. Increasing the cardinality of these sets, results in significant augmentation of the search space with a consequent impact on the corresponding problem's complexity.Regarding the power levels of the nodes, unlike previous work [18], in this study we assumed that they can take real values within a predetermined continuous range,Sk∈S=[smin,smax]⊂ℝ,k=1,2,…,K.Moreover, the parameters α and β of Eq. (9) are functions of the source coding rate and video content characteristics, as previously discussed. Concerning the parameters dfree and cdof Eq. (1), they are functions of the channel coding rate. Thus, α, β, dfree, and cdare functions of the source coding rate-channel coding rate combinations. Substituting Eq. (2) (with I0 instead of N0) into Eq. (1) (considering equality in order to use the BER's upper limit), and Eq. (1) into Eq. (9), the expected video distortion becomes,(11)E[Ds+c,k](Rs,k,Rc,k,S)=α(cb)log101Pb−β(cb),where,Pb=1P∑d=dfree(cb)∞cd(cb)12erfcdRc,kSk/Rk∑j≠kK(Sj/Wt)+N0,and k=1, 2, …, K. Obviously, the expected video distortion for node k is a function of the source coding rate, Rs,k, channel coding rate, Rc,k, as well as of the power levels, S=(S1, S2, …, SK)⊤, of all nodes of the network. Therefore, we eventually need to determine the source-channel coding rate combinations, and the power levels of all nodes, in order to compute the expected video distortion.We assumed that the sensor nodes participating in the network image scenes that include various motion levels. This is a common feature for the majority of real-time VSN applications. The two optimization criteria that we considered to tackle the problem of optimal resource allocation among the nodes of the wireless VSN are both based on the concept of minimizing video distortion and they are analyzed in the following paragraphs.According to the minimized average distortion (MAD) criterion, we need to determine the optimal vectors of source coding rates, Rs=(Rs,1, Rs,2, …, Rs,K)⊤, channel coding rates, Rc=(Rc,1, Rc,2, …, Rc,K)⊤, and power levels, S, such that the overall average distortion Dave(Rs, Rc, S) of the network is minimized, subject to the constraint of equal target bit rate Rtarget for all nodes. This problem can be formally given as follows,(12)minRs,Rc,SDave(Rs,Rc,S),subject toR1=R2=⋯=RK=Rtarget,where Dave(Rs, Rc, S) is defined as follows:(13)Dave(Rs,Rc,S)=1K∑k=1KE[Ds+c,k](Rs,k,Rc,k,S),where k is the node's index and K is the total number of nodes in the VSN. Obviously, this criterion does not assert fairness among the nodes. Hence, distortion is allowed to vary significantly from node to node as far as the average distortion is kept to minimal levels.The minimized maximum distortion (MMD) criterion requires the determination of the optimal vectors of source coding rates, Rs=(Rs,1, Rs,2, …, Rs,K)⊤, channel coding rates, Rc=(Rc,1, Rc,2, …, Rc,K)⊤, and power levels, S, such that the maximum distortion Dmax(Rs, Rc, S) among all nodes is minimized subject to the constraint of equal target bit rate Rtarget for all nodes, i.e.,(14)minRs,Rc,SDmax(Rs,Rc,S),subject toR1=R2=⋯=RK=Rtarget,where Dmax(Rs, Rc, S) is defined as follows,(15)Dmax(Rs,Rc,S)=maxk∈{1,2,…,K}E[Ds+c,k](Rs,k,Rc,k,S),where k denotes the corresponding node. The MMD criterion may also exhibit deviations of the distortion from node to node, but, in contrast to the MAD criterion, it guarantees that all distortions are kept within acceptable ranges.An additional assumption in our study is that the K nodes of the network are clustered into C motion classes, based on the amount of motion in the detected scenes. Without loss of generality, let us consider the case of C=2. In this case, there are two motion classes: a high-motion class, which includes the nodes that detect high levels of motion, and a low-motion class consisting of the nodes that image relatively stationary fields. Each class has its own set of parameters α and β (see Eq. (11)), since they are affected by the amount of motion of each considered video sequence.A reasonable question that follows the aforementioned assumption, is what happens in case of a possible change in the motion level of a scene. For example, what happens if the relatively stationary scenes of a forest-monitoring application are disturbed by an unexpected passage of an animal or, in a motorway-surveillance application, the scenes that capture intense traffic succeed scenes with infrequent vehicle passing? In such cases, a new classification of the scenes into high- and low-motion classes is required, corresponding to a new optimal resource allocation that is adjusted to the current state of the observed system.Regarding the node clustering into two classes, the quantities that need to be determined for each class under a total transmission bit rate constraint, are the following,Rs+c,high=Rs,high,Rc,high⊤,Rs+c,low=Rs,low,Rc,low⊤,S=Shigh,Slow⊤,whereRs,high,Rc,highand Shigh, are the source-channel coding rate combination, and the power level, respectively, for the high-motion class of nodes, whileRs,low,Rc,lowand Slow are the corresponding quantities for the low-motion class of nodes.Since the source-channel coding rate combinations assume discrete values, while the power levels are continuous, the resulting optimization problems for both the above criteria are of mixed integer type. Fig. 1illustrates the contour plot of the corresponding landscape for a fixed source-channel coding rate combination (the one that corresponds to the best solution). The two axes stand for the real-valued power levels of the two motion classes. Darker lines denote lower objective values. As we can see, the function has extremely steep regions (upper left part of the figure) as well as almost flat regions (middle to lower part). The star mark denotes the globally optimal power level vector (for the specific source-channel coding rate combination), which lies in a small region near the right lower bound of the search space. The figure refers to the MMD criterion. Similar landscapes are produced for all bit rate-bandwidth combinations, and for both the MAD and MMD criteria. Note that the optimization algorithm optimizes the source-channel coding rate combinations as well as the power levels, concurrently. It is easily conceived that the interplay of the discrete variables along with the remarkable changes in slope for the real-valued variables, as well as the discontinuities that may be produced by criteria as the MMD can impose serious difficulties for any optimization algorithm.In the following paragraphs we present the employed optimization algorithms, namely PSO, AS, and the hybrid HPSOAS.Particle swarm optimization (PSO) was introduced in 1995 by Eberhart and Kennedy [40]. It is based on models that simulate flocking behavior and it has close ties with the concurrent concepts of emergent and collective behavior [41]. PSO is categorized as swarm intelligence algorithm within the wider field of intelligent optimization [26,42]. Its ongoing increasing popularity can be attributed to its efficiency in tackling a plethora of scientific and technological applications, as well as to its easy implementation, which renders it accessible to researchers from various disciplines [26].PSO uses a population, called swarm, of search points, called particles, to probe the search space. The particles are randomly initialized (usually uniformly) in the search space. Each particle has three essential features: its current position in the search space, a memory where it retains the best position it has ever visited, and an adaptable velocity (position shift) that iteratively defines its new position. Also, it assumes a neighborhood consisting of other particles, i.e., a subset of the swarm, with which it interacts by means of information exchange. The information originating from the particle's own experience as well as the collective experience, are the main sources of influence for its move in the search space.Let the general minimization problem,minx∈X⊂ℝnf(x),with f(x) being the objective function. Let the set I={1, 2, …, N} denote the indices of the N particles of the swarm. Then, the swarm can be represented as a set of search points,S=x1,x2,…,xN.Each particle is an n-dimensional vector,xi=xi1,xi2,…,xin⊤∈X,i∈I,and its velocity is defined as,vi=vi1,vi2,…,vin⊤,i∈I.Its best position is also an n-dimensional vector,pi=pi1,pi2,…,pin⊤∈X,i∈I,stored in the memory and iteratively updated as long as the particle moves in X.The neighborhood,Ni, of the i-th particle can be defined in various ways. A straightforward approach considers as neighbors the closest particles in the search space. However, this approach was shown to produce clusters of particles that rapidly collapse on local minimizers, thereby reducing the (collective) exploration ability of the swarm. An alternative idea is the determination of neighborhoods in abstract spaces instead of the actual search space. An instance that has proved to be very efficient assumes that the particles are ordered on a ring based on their indices. In this case, the neighborhoods consist of particles with neighboring indices, having the form,Ni=i−r,…,i−1,i,i+1,…,i+r⊆I,where r∈{1, 2, …, N/2} is called the neighborhood's radius. The indices are assumed to recycle at the ends, i.e., index 1 follows immediately after index N. Evidently, increasing r results in neighborhoods that approximate the whole swarm. Different neighborhood topologies have been proposed in the literature [43,44]. The neighborhoods control the information flow among the particles as well as the available information that influences the particles’ position shifts at each iteration. Therefore, they can have a tremendous impact on PSO's performance.Let gidenote the index of the best in the neighborhood of the i-th particle, i.e.,(16)gi=argminj∈Nif(pj),i∈I,and let t denote the iteration number. Then, the swarm is updated at each iteration as follows [45]:(17)vij(t+1)=χvij(t)+c1R1pij(t)−xij(t)+c2R2pgij(t)−xij(t),(18)xij(t+1)=xij(t)+vij(t+1),where i∈I; j=1, 2, …, n; and χ is a parameter called the constriction coefficient, which can deter the swarm explosion effect, i.e., the rapid divergence of the particles due to excessively large velocities [45–47]. Regarding c1 and c2, they are two positive acceleration parameters called the cognitive and social parameter, respectively. These parameters control the influence of the personal and collective experience (memory) on the particle's move, with equal values promoting a fair tradeoff between them. Finally,R1andR2are random variables uniformly distributed in the range [0, 1]. They introduce stochasticity in PSO and assume a different value for each i and j. Evidently, PSO's update is inherently parallel, since it is performed componentwise.After updating and evaluating the swarm, memory update takes place in two stages. In the first stage, the personal best position of each particle is updated as follows,pi(t+1)=xi(t+1),iffxi(t+1)<fpi(t),pi(t),otherwise,i∈I.The determination of new best positions is followed, in the second stage, by the update of all indices gi, i∈I, according to Eq. (16). This completes a PSO iteration. The procedure is repeated until a stopping criterion is satisfied, such as exceeding a prespecified number of function evaluations or reaching a target function value.Clerc and Kennedy [45] have extensively studied the stability of PSO. Their analysis offered significant mathematical evidence on its proper parameter settings. Based on their analysis, the parameter values,χ=0.729,c1=2.05,c2=2.05,have been shown to be a satisfactory starting choice, considered as the default parameter set of the constriction coefficient variant of PSO. Further information and alternative settings can be found in [45,47].PSO belongs among the most studied metaheuristics. In addition to [45,47], further theoretical analyses can be found in [48–50]. Its theoretical background, well-understood dynamic, as well as its frequently verified efficiency renders PSO a very appealing optimizer. Recently, it has been used with remarkable success in VNS [20,39,51–57].Although PSO was primarily designed to handle continuous variables, it has been successfully applied also on integer optimization problems [58–62]. This can be achieved by introducing integer-arithmetic based operators in PSO. However, in most cases, the resulting PSO variants barely resemble the original PSO dynamics. Alternatively, discrete values can be tackled by solving an extended version of the problem in the continuous space and rounding the candidate solutions to the nearest integers prior to their evaluation with the objective function. The latter procedure has minor effect on the algorithm. Also, it has been shown to work efficiently in various problems, offering motivation for selecting the latter approach in our study.In the mixed integer optimization problems of the present study, each particle should normally consist of integer and continuous components, corresponding to the discrete and continuous variables described in previous sections. Instead, we considered also the integer parameters to be continuous (retaining their bounds) and applied the presented PSO scheme. However, whenever a particle was evaluated with the objective function, its corresponding components were rounded to the nearest integers as follows:xij=⌊xij+0.5⌋,where ⌊.⌋ is the floor function. This scheme was successfully tested in previous works [20,39,51–57].A feature usually neglected in PSO implementations is that of maximum velocity. Specifically, whenever the velocities are computed by Eq. (17), they undergo a magnitude-restriction test as follows,vij(t+1)=vjmax,ifvij(t+1)>vjmax,−vjmax,ifvij(t+1)<−vjmax,vij(t+1),otherwise,∀i,j,t,wherevjmaxis a predefined positive value, possibly different for each j=1, 2…, n. Obviously, this procedure restricts the velocity components within the corresponding ranges[−vjmax,vjmax], preventing the particles from taking large steps that could lead to wide-range oscillations around the best positions or frequently escaping out of the search space. Naturally, this can have considerable impact on PSO's convergence speed. We can easily infer that large values ofvjmaxare more appropriate for search spaces with wide flat or low-curvature regions, while significantly smaller values may be required in steep functions with large number of minimizers, especially when they are closely concentrated.Typically, maximum velocity is determined as the maximum absolute distance allowed to be traveled by the particle in a single step at each component direction. For this purpose, it is usually defined as a fraction of the corresponding search space's range in the specific component direction. For example, if the search space is defined asX=[x1min,x1max]×⋯×[xnmin,xnmax], then the following restriction is commonly used,(19)vjmax=γjxjmax−xjmin,γj∈(0,1],j=1,2…,n.Available information on the form of the objective function may dictate larger or smaller values of the parameters γj. For example, the Lipschitz property can provide useful insight regarding the degree of variation of the objective function in the whole search space. However, in most cases such information is either unavailable or very laborious to be computed.In such cases, we can approximately estimate the Lipschitz constant by considering its modulus of continuity (MoC) δ>0, which is locally defined in a subset B⊂X of the search space as follows,|f(x)−f(y)|⩽δx−y,∀x,y∈B.Estimating the MoC around solutions obtained in preliminary experiments as well as on randomly selected points in the search space can partially reveal the local behavior of the objective function. This, in turn, can lead to more appropriate selection of the maximum velocity thresholds described above. The estimation can be easily conducted through Monte Carlo sampling within the corresponding region B.In our preliminary experiments, we observed that PSO performance in terms of convergence speed exhibited large deviations per optimization criterion for some cases. Thorough examination of the corresponding landscapes revealed the importance of proper velocity setting. Thus, we employed the procedure described above to obtain estimations of the maximum velocities for each optimization criterion based on Monte Carlo approximations of the MoC.Constrained optimization problems are usually tackled by splitting the initial problem into simpler subproblems than can be solved and used as the basis of an iterative process. The Active-Set (AS) is an iterative method that is used for solving a sequence of quadratic subproblems, guaranteeing the feasibility of the final solution [28,29]. The main mechanism is based on the solution of the Karush-Kuhn-Tucker (KKT) equations, which guarantee the optimality for a constrained optimization problem.Let us assume again the minimization problem as declared in Section 4.1,minx∈X⊂ℝnf(x),subject to m constraints (these constraints may be implicitly given as defining relations of the search space X),(20)Gi(x)=0,i=1,…,me,(21)Gi(x)≤0,i=me+1,…,m.The vector function G(x)=(G1(x), …, Gm(x))⊤ returns a vector of length m that includes the equality and inequality constraint values at x. The corresponding KKT equations are given by,(22)∇f(x)+∑i=1mλi∇Gi(x)=0(23)λiGi(x)=0,i=1,…,me,(24)λi≥0,i=me+1,…,m.Eq. (22) depicts the canceling process of the gradients between the objective function f(x) and the active constraints Gi(x) at x, through the use of the Lagrange multipliers λi, i=1, …, m. Lagrange multipliers are used in order to balance the deviations in magnitude of the objective function and constraint gradients. Due to the fact that only active constraints are included in the gradients canceling, non-active constraints are assigned λi=0, as it is stated implicitly by Eqs. (23) and (24).Thus, the AS method is based on the solution of the KKT equations and attempts to compute the Lagrange multipliers directly. It searches solutions in the feasible sets and if a minimizer is found during each iteration, followed by a decrease in the value of the objective function, the algorithm terminates after a user-defined stopping criterion. Such a criterion can be the maximum iteration number, the maximum number of function evaluations, the function tolerance, the tolerance of the optimal point x etc.In our problem, we used the robust implementation of the original Matlab® Optimization Toolbox. Further details on this implementation can be found in [63].Motivated by the benefits of both PSO and AS optimization algorithms, we combined their features introducing a hybrid PSO-AS approach, which is denoted as HPSOAS. This hybrid approach can be categorized as a memetic algorithm [30] and employs AS as local optimizer for further improving the findings of PSO.Algorithm 1HPSOASRequire: Initialize PSO algorithm.1:loop2:if (not stopping) then3:Update swarm and best positions.4:if (new overall best position is found) then5:Apply AS on the new best position.6:Make AS's solution the new overall best position.7:end if8:end if9:end loopSpecifically, when the overall best position of PSO changes, a local search procedure with AS is initiated from this point, in order to further improve it. The procedure is sketched in Algorithm 1. Although PSO and in many cases also AS were capable of successfully approximating the optimal solution in our experiments, the HPSOAS scheme was significantly more time-efficient, yet retaining the solutions’ quality. Its success lies on the fact that AS was rapidly improving the PSO's best findings, thereby providing better attractors (best positions) for the particles, while at the same time it surmounted the sensitivity of AS on the initial conditions (starting point).In the first part of this section, we describe the system's parameter configuration as well as the procedure for computing the expected video distortion, which is needed in Eq. (11) in order to estimate the parameters α and β. Also, the optimization algorithms’ parameters are given. In the second part of the section, we assess the two optimization criteria described in Section 3, on two real video sequences. In our experiments, we assessed the source-channel coding rate combinations, and power levels, for the two considered motion classes, as well as the received video quality for each video sequence, under the same experimental settings, i.e., for the same node distributions, bit rates and bandwidths. The experiments considered both the cases of neglecting and adding background and thermal noise.Based on the observation that high values of the parameter α (see Eq. (11)) imply high video sequence motion and vice versa, we assumed that the K nodes of the network are clustered into C=2 motion classes, namely a high-motion and a low-motion class. The “Foreman” video sequence was used to represent the class of nodes that detect high motion levels, while the “Akiyo” video sequence was used to represent the class of nodes that capture more stationary fields. The resolution of both video sequences was the quarter common intermediate format (QCIF). Thus, one set of URDC curves was needed per video sequence.The procedure for the computation of the expected video distortion can be concisely described as follows: for given BER, we determine the rate of the packet loss according to the real-time transport protocol (RTP). Then, packets are dropped from the video bit stream under investigation. We continue decoding the corrupted video sequence with the H.264/AVC video codec and, finally, the expected video distortion is obtained. Due to the existence of random channel errors in VSNs, the same procedure is repeated for 300 times and the expected video distortion is averaged over all these experiments to offer a more reliable estimation.After the computation of the expected video distortion, the parameters α and β of Eq. (11) are determined using least squares optimization from data obtained using a few BERs. Specifically, we considered the BER values 10−7, 10−6, and 10−5, while each of the “Foreman” and “Akiyo” video sequences is compressed at 32kbps, 48kbps, 64kbps, 72kbps, and 96kbps. Furthermore, the characteristics for both video sequences were obtained at a rate of 15 frames per second.The employed modulation scheme was the BPSK, while RCPC codes with mother code of rate 1/4 were used for channel coding [19]. Also, the considered target bit rate constraints were equal to 96kbps and 144kbps. Thus, the corresponding source-channel coding rate combinations for each motion class, resulting from the above bit rate constraints were,(a)Rtarget=96kbps that results in,(Rs,high,Rc,high),(Rs,low,Rc,low)∈{(32,1/3),(48,1/2),(64,2/3)}.Rtarget=144kbps that results in,(Rs,high,Rc,high),(Rs,low,Rc,low)∈{(48,1/3),(72,1/2),(96,2/3)}.For the power levels S, we assumed continuous values within the range S=[5.0, 15.0] (Watts), while for the bandwidth Wttwo different values were examined, namely 20MHz and 15MHz. The total number of nodes that composed our network was K=100.PSO was considered with its default parameter set defined in Section 4.1. Also, a swarm of N=40 particles was employed, under the ring topology of radius r=1. In our problem, each particle xiwas 4-dimensional, defined as,xi=(Shigh,Slow,Rs+c,high,Rs+c,low)⊤,i=1,2,…,40,containing the power level and the source-channel coding rate combination for each motion class (denoted as “high” and “low”, respectively). Furthermore, the discrete parameters, i.e., source and channel coding rate combinations, were represented in the particle with continuous values within the range R=[0.6, 3.4]. However, as explained in Section 4.1.1, they were rounded to the nearest integer for the particle's evaluation. Specifically, we assumed the following correspondences,Rtarget=96kbps1⟶(32,1/3)2⟶(48,1/2)3⟶(64,2/3),andRtarget=144kbps1⟶(48,1/3)2⟶(72,1/2)3⟶(96,2/3).Besides that, the maximum velocities were set based on the MoC estimation procedure described in Section 4.1.2. The corresponding values of γ in Eq. (19) for the 4 component directions in our problem were determined as follows,γMAD=(0.1,0.1,0.03,0.03)⊤,γMMD=(1,1,1,1)⊤.For each problem instance, PSO was executed 30 times for a maximum number of 1000 iterations and the best solution was recorded.Regarding the AS method, it was also applied on each problem instance 30 times from different initial conditions. However, AS was capable of providing only the power levels, since it works only on continuous search spaces. For the discrete source-channel coding rate combinations, we used exhaustive search among all possible pair values, i.e., 3 admissible values for each of the two motion classes, resulting in 3×3=9 cases. Thus, a single application of AS required 9 optimization runs, one for each discrete combination, and the final solution was selected as the best one among the obtained 9 solutions.Finally, identical experiments were conducted using the proposed hybrid algorithm HPSOAS. The parameter setting of HPSOAS was the same with that of PSO. HPSOAS exhibited similar performance with PSO in terms of solution quality for each problem instance. Note that all algorithms (PSO, AS, HPSOAS) were equipped with exactly the same total computational budget in terms of function evaluations, namely 40000 function evaluations, in order to achieve fair comparisons among them.All experiments were conducted on an Intel® Core™ 2 Quad CPU @ 2.50GHz with 4.00GB RAM, using the Matlab environment. For each problem instance, PSO and HPSOAS converged on the same solutions in all 30 experiments. Specifically, the obtained values of the objective functions (for the two optimization criteria) were identical up to 15–20 decimal digits, and therefore, they were considered to be essentially identical. However, this was not the case for AS, which produced inferior solutions in some cases due to its dependency on the initial conditions and the peculiarities of the objective function landscape. Nevertheless, in the rest problem instances it achieved the same solutions as the other two algorithms.Our first group of experiments was conducted under the assumption that thermal and background noise were neglected, namely N0=0W/Hz. The experiments were repeated in a second round, assuming that the aforementioned noise was significant. In this case, noise of magnitude N0=10−7W/Hz was added in our computations.In the case where thermal and background noise were neglected and the AWGN was introduced entirely from the interference among the nodes, PSO detected a multitude of solutions for the power levels of both motion classes, all of which had the same ratio Shigh/Slow, up to 4–5 decimal digits. All these solutions achieved the best objective values for both the MAD and MMD optimization criteria. This is a consequence of the fact that the ratio Ek/I0 in Eq. (7) is invariant under the multiplication of all power levels with the same constant. Therefore, estimation of the optimal ratio Shigh/Slow becomes the main goal rather than the determination of specific values for the power levels. In contrast to the power levels, the corresponding optimal source and channel coding rate combinations were unique in all solutions provided by PSO. Moreover, when thermal and background noise were added, PSO reported unique solutions for all transmission parameters.It is worth mentioning that the video quality for each motion class cl∈{1, 2}, was estimated by using the peak signal-to-noise ratio (PSNR), which is measured in decibel (dB),(25)PSNRcl=10log102552E[Ds+c,cl],with E[Ds+c,cl] representing the expected video distortion due to source and channel coding for the cl-th class of nodes.Tables 1–4report the obtained direction components of the solution vectors (transmission parameters) that correspond to the best objective values of both MAD and MMD, by all algorithms (the inferior solutions produced by AS in some cases are omitted), for the case where thermal and background noise are considered to be negligible, namely N0=0W/Hz. More specifically, the tables report the obtained source coding rates, channel coding rates, power levels and PSNR values, for both motion classes, optimization criteria and different node distributions and settings of the bit rate and bandwidth values. Since in this case we are interested in the optimal power level ratios, rather than the determination of the specific values of Shigh and Slow, we cite indicative Shigh and Slow values that correspond to the best objective value.Tables 5–8report the corresponding results under the assumption that thermal and background noise power spectral density are equal to N0=10−7W/Hz. The corresponding tables per combination of bit rate and bandwidth are summarized below,(a)Bit rate 96kbps and bandwidth 20MHz: Tables 1 and 5.Bit rate 96kbps and bandwidth 15MHz: Tables 2 and 6.Bit rate 144kbps and bandwidth 20MHz: Tables 3 and 7.Bit rate 144kbps and bandwidth 15MHz: Tables 4 and 8.The K=100 nodes of the network were assigned to the two motion classes in different proportions, called node distributions. Each line in the tables corresponds to a different node distribution, denoted as “Nhigh−Nlow”, whereNhigh,Nlow∈{10,30,50,70,90},Nhigh+Nlow=K=100,meaning that the corresponding classes consist of a number of Nhigh nodes capturing high-motion scenes and Nlow nodes capturing low-motion scenes, respectively. Moreover, the combination of source-channel coding rates, the power level and the PSNR of the high-motion class are represented as (Rs,high, Rc,high), Shigh, PSNRhigh, respectively, while (Rs,low, Rc,low), Slow, and PSNRlow, are the corresponding parameters for the low-motion class.A close inspection of the results, demonstrates that the MAD criterion works favorably for the low-motion class of nodes, equipping it with better image quality than the high-motion class. Concerning the MMD criterion, it is rather unbiased, offering identical PSNR values to both motion classes. These remarks are derived from all combinations of bit rate-bandwidth considered in our experiments.Moreover, the MMD criterion assigns higher PSNR values to high-motion class of nodes than the MAD criterion, in the corresponding cases. Indeed, the PSNR differences between the two criteria is inversely proportional to the cardinality of the high-motion class of nodes. Thus, the MMD criterion can be considered as the most appropriate choice in cases where we are interested in the amelioration of the high-motion scenes rather than improving the quality of the low-motion scenes. Surveillance applications are typical examples of such cases. On the contrary, the MAD criterion appears to be more suitable in cases where high video quality of low-motion scenes is desirable.Considering the noisy case, in general, no significant changes in PSNR values are observed after the addition of noise, except for a marginal reduction of no more than 0.01dB, which has imperceptible impact to the video quality. Similarly to the noiseless case, the high-motion class of nodes requires more power than the low-motion class. More specifically, the power levels of the high-motion class of nodes are not only higher than that of the low-motion class, but also they actually take their maximum possible value.Special attention shall be paid to the case of distributing 10 nodes in the high-motion class and 90 nodes in the low-motion class, which is reported in Table 7, for the MAD criterion. Despite the addition of noise, an increase of PSNR is observed for the low-motion class compared to the corresponding case in Table 3, for the same criterion. This is attributed to the fact that, under the influence of noise, the power level of the low-motion class is increased. Thus, the ratio Ek/I0 also increases as follows from Eq. (7).Additional information regarding the performance of the proposed schemes is graphically illustrated in figures. Specifically, Fig. 2(a) depicts the differences of the received PSNR between the MAD and MMD criteria for both the high- and low-motion class of nodes, for all node distributions and refers to the case of Rtarget=96kbps, Wt=20MHz, and N0=0W/Hz. The last column of the same figure shows the accumulated PSNR difference between MAD and MMD, also for all node distributions. This figure manifests that, cumulatively for all node distributions, the decrease in PSNR achieved by the MMD criterion for the low-motion class of nodes is considerably higher than the corresponding gain for the high-motion class of nodes. For the case of a bit rate of 96kbps and a bandwidth of 20MHz, the MMD increases the total PSNR for all members of the high-motion class by 6.0986dB, while the total PSNR for all members of the low-motion class decreases by 9.3798dB. Therefore, despite the fact that the MMD offers equal PSNR values to both motion classes, it is proved to be less fair than it was initially perceived, since it disfavors the low-motion class of nodes.An additional piece of information is provided by Fig. 2(b) that depicts the optimal ratios Shigh/Slow for the two optimization criteria when the number of nodes imaging high levels of motion increases, while that of the nodes imaging low levels of motion decreases. The specific figure refers to the case of Rtarget=96kbps, Wt=20MHz, and N0=0W/Hz and offers strong evidence that the MAD criterion requires much less power than the MMD. Spending less power for data transmission means that a stronger channel coding is used, capable of correcting a higher number of channel errors. From Eq. (6), it follows that the channel coding rate is inversely proportional to the data compression rate, highlighting the importance of considering the special characteristics of the video sequence (high- against low-motion), when determining the optimal power levels.Moreover, the power received by the CCU from the low-motion class of nodes is always less than that of the high-motion class for both optimization criteria, as it is confirmed in the tables for all the examined bit rate-bandwidth combinations. Hence, since the nodes that image high levels of motion need higher power levels than those in the low-motion class, it is reasonable that the ratio Ek/I0 of Eq. (7) exhibits further decrease when the cardinality of the high-motion class is larger than that of the low-motion class. Naturally, this incurs a reduction to the PSNR values for both motion classes.Actually, this fact explains the downward trend of the PSNR values illustrated in Figs. 3and 4, for both motion classes, for all examined bit rate and bandwidth combinations. Specifically, Fig. 3(a) includes the PSNR variations for the high-motion class of nodes, while Fig. 3(b) illustrates the same results for the low-motion class of nodes for the MAD criterion. Fig. 4 offers the corresponding information for the MMD criterion. Note that for the MMD case, the PSNR values for both motion classes were identical.Another issue that gained our interest was the efficiency of PSO against that of AS in terms of the number of iterations required for obtaining solutions of same quality with AS. Fig. 5offers this information. In particular, it illustrates the number of iterations required by PSO, averaged over the 30 independent experiments, to detect the same solution with AS for different levels of precision, namely 3, 4, 5, and 6 decimal digits. In each subfigure, both criteria are compared for all examined bit rate and bandwidth combinations. Fig. 5(a) and (b) present the aforementioned results for the node distribution “30–70”, while Fig. 5(c) and (d) for the node distribution “70–30”. Moreover, Fig. 5(a) and (c) refer to the noiseless case (N0=0W/Hz), and Fig. 5(b) and (d) to the noisy case (N0=10−7W/Hz).From the figures, we corroborate that PSO requires fewer iterations to achieve same quality solutions with the AS algorithm for lower precisions. As expected, increasing precision is accompanied by a higher number of iterations. Furthermore, it is obvious that the MMD criterion needs more function evaluations than MAD to achieve the optimal solution, while the addition of noise incurs an increase to the number of iterations for both optimization criteria. The aforementioned confirmations derive from all considered bit rate-bandwidth combinations and node distributions.Fig. 6presents the success ratio of AS in achieving the optimal solution, for both MAD and MMD criteria. Particularly, each of the cases as referred to this figure corresponds to the results of each corresponding table. For example, Case 3 corresponds to the results of Table 3, when Rtarget=144kbps, Wt=20MHz, and N0=0W/Hz. For this case, we observe that for the MAD it has a success ratio of 100% and 99.33% for MMD. For the MAD criterion, this means that each of the node distributions manages to achieve the same best solution in all 30 experiments. In contrast, the success ratio for MMD implies that one experiment out of 30, for a specific node distribution, failed to reach the best solution.Interpreting the AS efficiency for MAD and MMD, we infer that for the MAD criterion it is capable of detecting the best solution in most of the cases and exhibits better performance than for MMD. The superiority for MAD is clearer in noisy cases (Cases 5–8) where for MMD it presents success ratios of nearly 70%. Although it may seems high, we indicatively refer that for the case of Nhigh=90−Nlow=10 of Table 8, only 7/30 experiments reach the optimal solution. Therefore, this figure sheds light on the weakness of AS in detecting always the best solution, while it also demonstrates that its efficiency depends both on the initially supplied starting point as well as on the objective function to be optimized.Lastly, Table 9offers an intuition regarding the execution time required by each optimization algorithm averaged over 30 experiments per problem instance. Also, the total running times needed for the execution of all cases of node distributions for each specific bit rate and bandwidth combination, are presented. The same table includes results for both criteria and all cases of considered bit rate and bandwidth for both the noiseless and noisy cases. We shall note that AS was set to execute 40,000 function evaluations (as PSO and HPSOAS), with the function tolerance set to 10−15 and the tolerance of the candidate optimal solution to 10−15. We set the function and optimal point tolerances to such levels since we confirmed that the PSNR results provided by the AS were different from those of PSO even to the first decimal digit for some cases. At this point, we should recall that the AS method was run sequentially 9 times for each tested case in order to evaluate all possible combinations of bit rate and bandwidth. The execution times of this method are averaged over the number of successful experiments per case.An overall inspection of the results denotes that despite PSO is able to reach the optimal solutions efficiently in all 30 experiments of each considered case, as opposed to AS, it still needs more time to be executed compared to both AS and HPSOAS. Particularly, more time is needed as more nodes are assigned to the low-motion class of nodes. Moreover, it follows that MAD takes less time to reach the optimal point compared to MMD, while in the noisy case the MMD needs the double time to achieve the optimal solution compared to the noiseless case, using the PSO.Commenting on the AS execution times, it is clear from Table 9 that it requires less time compared to PSO, with two exceptions for the noiseless case of MMD criterion when the bit rate is set to 144kbps. This probably means that the randomly supplied starting points are far away from the optimal solution and that the shape of the objective function complicates further the finding of the solution. Additionally, we may think that the AS method follows the brute force approach testing all possible cases of source and channel pair values, increasing the overall problem's complexity. Therefore, this method is impractical to applications where problems with higher dimensions exist. For example, if we had made the assumption for a node clustering into 10 motion classes, the AS should be executed 310 times in order to determine the optimal source and channel coding rate combinations for each motion class, instead of the 32 times of our problem.Therefore, as it was also mentioned in Section 4.3 in order to surmount all weaknesses of both PSO and AS keeping their benefits at the same time, we resorted to the development of the HPSOAS optimization method. Table 9 demonstrates that the optimization of MMD criterion using the HPSOAS method requires significantly less time compared to both PSO and AS. Similarly, the MAD optimized under the use of the HPSOAS saves much execution time compared to PSO and the same holds also in the half cases compared to AS.Interesting enough are also the conclusions drawn by Fig. 7. The goal of the specific illustration is to give us an insight regarding the overall execution times for each of the MAD and MMD criteria, when we aggregate the total times for each considered bit rate and bandwidth combination, for both the noiseless and noisy cases. Evidently, it follows that the MMD takes longer to converge compared to the MAD, for all considered optimization methods. Moreover, for the MMD it is clear that HPSOAS needs about 1/6th and 1/4th of the corresponding time of PSO and AS, respectively, in order to converge to the optimal point. Further is the gain in time by adopting the HPSOAS to the MAD criterion instead of PSO. In this case, HPSOAS reaches the optimal solution in about 1/12th of PSO's time. Comparable execution times are required accumulatively by both AS and HPSOAS for the MAD criterion.Summarizing, HPSOAS keeps the following strong points that renders its use very appealing: (i) it is not sensitive to initial conditions, (ii) there is no need to execute an exhaustive search in order to determine the discrete parameters of this work, (iii) it can be applied to problems of higher dimensions without a tremendous effect on problem's complexity, and (iv) it takes much less time for its execution compared to other stochastic methods.

@&#CONCLUSIONS@&#
We studied the problem of optimal resource allocation among the nodes of a wireless DS-CDMA VSN. More specifically, we optimally allocated the source coding rates, channel coding rates, and power levels to all nodes, based on the detected amount of motion per node. For the source and channel coding rates, discrete values were considered, while for the power levels, we assumed continuous values within a predetermined range.The MAD criterion that minimizes the average network distortion and the MMD criterion that minimizes the maximum distortion among all nodes of the network, both aiming at achieving the highest possible video quality, were employed in the present study. In order to solve the underlying mixed integer optimization problems, the established PSO algorithm was employed, motivated by preliminary results in previous works. The performance of this algorithm was assessed in comparison with the performance of the AS method, justifying its capability in tackling such problems. Additionally, we developed a hybrid optimization method that is based on PSO, using the AS as a local optimizer. The experimental results using all optimization methods highlighted the superiority of HPSOAS over both AS and PSO, under various aspects.Having clustered the nodes into two motion classes based on the amount of the detected motion per node, extensive experimentation showed that the MAD criterion works favorably for the nodes that image low motion, since they are offered considerably higher PSNR values. On the other hand, the MMD assigns equal PSNR values to both motion classes. Nevertheless, it is not sufficiently clear how fair the MMD can be for the nodes that record low motion, taking into consideration the significant PSNR reduction observed in this case, for this class of nodes. Furthermore, our results confirmed that the CCU receives less power with the MAD than with the MMD, implying that MAD requires less power for data transmission.Experiments conducted under the presence of thermal and background noise, verified the conclusions derived for the noiseless case. The main impact of noise was a marginal reduction of the PSNR of both motion classes and optimization criteria, with only a minor exception. Also, the nodes that detected high levels of motion required considerably higher power levels than the nodes that detected low levels of motion, to accomplish data transmission.Summarizing, our investigation aligns with the general feeling that the most appropriate approach for tackling resource allocation problems in wireless VSNs, is rather problem-dependent. The application's special characteristics and requirements shall dictate the methodology to use, inhibiting the possibility of a panacea that would simultaneously favor all nodes.