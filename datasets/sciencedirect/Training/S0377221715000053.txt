@&#MAIN-TITLE@&#
A parallelised distributed implementation of a Branch and Fix Coordination algorithm

@&#HIGHLIGHTS@&#
Large-scale stochastic mixed-integer programming (SMIP) instances are hard to solve.Branch and Fix Coordination (BFC) algorithm can solve such SMIP instances.The decomposition appears to play an important role.Parallelizing BFC obtains solutions in instances than commercial solvers do not.

@&#KEYPHRASES@&#
Stochastic mixed-integer problems,Branch and fix coordination algorithm,Parallel programming,

@&#ABSTRACT@&#
Branch and Fix Coordination is an algorithm intended to solve large scale multi-stage stochastic mixed integer problems, based on the particular structure of such problems, so that they can be broken down into smaller subproblems. With this in mind, it is possible to use distributed computation techniques to solve the several subproblems in a parallel way, almost independently. To guarantee non-anticipativity in the global solution, the values of the integer variables in the subproblems are coordinated by a master thread. Scenario ‘clusters’ lend themselves particularly well to parallelisation, allowing us to solve some problems noticeably faster. Thanks to the decomposition into smaller subproblems, we can also attempt to solve otherwise intractable instances. In this work, we present details on the computational implementation of the Branch and Fix Coordination algorithm.

@&#INTRODUCTION@&#
Many infrastructure problems nowadays are solved using optimisation and equilibrium models (Li, Gabriel, Shim, and Azarm, 2011; Rømo et al., 2009). These problems often imply handling investments, which can be represented by yes/no decisions (“Should a pipeline be built or not?”) and modelled with binary variables. Mixed Integer Programming problems (MIPs), and specially those containing binary variables, are common in problems of transportation (Christiansen, Fagerholt, and Ronen, 2004), energy (Wallace and Fleten, 2003), real-state, etc. In these cases, strategic investment decisions influence a project’s development over long timespans. For example, expensive building projects induce costs and deliver returns for many years after the decision of building was made. Once built, under-utilisation impacts the economic prospects of the project.Dealing with these long-lasting investments in the real world, almost invariably, involves uncertainty in the parameters of the model we are trying to create. Prices, supplies and consumptions might differ significantly from estimates, unexpected events could make these estimates imprecise, or new legislation can turn once attractive investment into expensive ones (Alonso-Ayuso, Escudero, and Ortuño, 2003; Kall and Wallace, 1994).Uncertainty, therefore, further complicates already hard-to-solve MIPs with the introduction of additional variables/parameters: MIPs are complex because of the large number of combinatorial choices they imply; stochastic problems are complicated because of the amount of scenarios they involve if a lot of variability happens over long time spans. As a result, stochastic MIPs are likely to result in complex problems, even while working exclusively with linear constraints (SMILPs).In view of this, the Branch and Fix Coordination (BFC) algorithm was developed to tackle a certain class of SMILPs, namely, those in which both integer and continuous variables appear in (generally) every stage of the problem, all integer variables are binary, and no multi-scenario constraints appear (Alonso-Ayuso et al., 2003; Escudero, Garín, Merino, and Pérez, 2010b). BFC is based on the well-known Branch and Bound (B&B) algorithm, with the main difference that the search tree evaluates many subproblems at each step, and the decisions to branch, prune or bound are done taking all subproblems into consideration. The first version of the BFC was developed by the groups of Alonso-Ayuso et al. (2003) to take advantage of scenario-wise decomposition schemes for solving special cases of two-stage SMILPs (for more applications see also Escudero, Garín, Merino, and Pérez, 2007; 2009b; 2010a). The algorithm was generalised to multi-stage SMILPs with binary and continuous variables in any stage (Escudero, Garín, Merino, and Pérez, 2009a; 2009c; Escudero et al., 2010b). Further refinements such as the parallelisation of the algorithm or contemplating explicitly non-symmetric trees have proven to be successful in reducing running times (Aldasoro, Escudero, Merino, and Pérez, 2013; Escudero, Garín, Merino, and Pérez, 2012).In this paper, we present a particular implementation of the BFC routine, which uses parallel processing to solve many subproblems at the same time in a way that makes the process arguably faster and more efficient. The resulting application is able to solve problems with dimensions which are orders of magnitude larger than those reported in past papers, and also coordinating significantly more clusters and variables without apparent loss of efficiency. Moreover, this implementation improves over the existing ones with added flexibility (allowing for more branching options and data storage), as well as doing away with some bounding strategies in favour of looser but faster searches.The framework currently in place allows for a mostly seamless transition into solving fully-decomposed problems, which can then be stored in several files without the need to load the full problem anywhere in the solution process.The paper is organised as follows: first, we present some formulations useful for the description of the algorithm in Section 2. After that, in Section 3, we state the problems needed to be solved at several steps of this particular BFC implementation, which is itself described in Section 4, along with some details on the parallelisation we use. Later in Section 5, we show several problem instances, whose analysis suggests that BFC is competitive when compared to commercial MIP solvers.Consider the Deterministic Equivalent Model (DEM) of a stochastic mixed integer linear programming problem with only binary and continuous variables X, Y, and its matrix form, [AX + BY]. For most SMILPs, we can easily identify blocks in these matrices that represent constraints specific to a scenario tree node, or constraints that link one node in the tree to its parent nodes, children nodes, and so on. We can use this idea of blocks, and the constraints and variables they involve, to represent different variations of the problem by indexing sections of the constraint matrices which have special interest to us with a set of nodes which in turn corresponds to a row of blocks in the matrix.[AG1AG2⋮]X+[BG1BG2⋮]YSupposeGis the set of all nodes g of a (not necessarily symmetric) scenario tree, andG2the set of all subsets ofG(i.e. its power set). Further, letG1be the subset of one-element sets inG2,i.e.,G1:={m∈G2:|m|=1}; clearly, there is a one-to-one correspondence between the nodes inGand the sets inG1.At each node g, we have binary variables xg, iand continuous variables yg, j. We use the paired sets of indicesIg,Jgso thatXg={xg,i:i∈Ig}andYg={yg,j:j∈Jg},and in turnX={Xg:g∈G},andY={Yg:g∈G}.First, let us present a simplified formulation of a general stochastic problem in compact formulation. Here, we only differentiate between groups of constraints that (a) affect variables belonging to one node, or (b) affect in general all variables (for example, recourse constraints, among others). We then index each set of rows using either the setG∈G1corresponding to the node the set of rows involves, or the entire node setG. This gives AGand BG, for eachG∈G1∪{G},in which each G is a member of a set of sets. The DEM of this problem is(1a)min:f(X,Y)=∑g∈Gwg(agXg+bgYg)(1b)s.t.AGX+BGY≤CG,G∈G1∪{G};(1c)xg,i∈{0,1},i∈Ig,g∈G;(1d)yg,j∈R,j∈Jg,g∈G;with properly defined weights wgfor each node.In problem (1), all constraints which affect the variables of more than one node of the scenario tree appear asAGX+BGY≤CGin expression (1b). However, they do not necessarily need to be aggregated like this. By using this indexing idea, we can just as easily group nodes into relevant sets which correspond to scenarios, or to groups of scenarios, or to stages in the scenario tree, and use these sets to index parts of the constraint matrices. This helps us to write different formulations and variations of a SMILP DEM in a condensed notation.LetSibe the set of nodes g which belong to stage i (i.e.S1contains only the root node(s),S2contains the immediate children to the root nodes, and so on). If the tree has S stages, makeS={S1,⋯,SS}the family of all stage sets. Also, for the set of leaf nodesg∈SS,create a partitionL={Lc}so that all nodes in a givenLchave a common root. This makes it possible to define a cluster(Escudero, Garín, Merino, and Pérez, 2010a) as we use it in this work:Definition 1ClusterFor each elementLc∈L,a clusterCcis a set of nodes which contains all elements ofLcand their parent nodes up to the root. ThenC={Cc}is the set of all clusters in a scenario tree.With the familiesSandCthus defined, we can write constraints for all the nodes in one stage, or all the nodes of one scenario, or all the nodes in a given set of scenarios, and index them with sets regardless of the formulation chosen for the model.For example, if we consider the stochastic scenario tree in Fig. 1a, the set of nodes will beG={g1,⋯,g6},and consequentlyG1={{g1},⋯,{g6}}.The only possible cluster for this tree isC={{g1,g2,⋯,g6}},withS={{g1},{g2,g3},{g4,g5,g6}}. In this manner, the formulation would be equivalent to that in problem (1).On the other hand, if we define a different set of nodesG={g1,⋯,g9},such as the ones shown in Fig. 1b, withS={{g1,g2,g3},{g4,g5,g6},{g7,g8,g9}}andC={{g1,g4,g7},{g2,g5,g8},{g3,g6,g9}}being the only possible set of clusters, we can formulate the split-variable formulation of (1), MP1, once we defineAasA={{g1,g2,g3},{g4,g5}}as the set of all (non-trivial) non-anticipativity classes of equivalence. Notice that the weights wgshould be modified accordingly for the problems to be equivalent.(2a)MP1:min:f(X,Y)=∑g∈Gwg(agXg+bgYg)(2b)s.t.AGX+BGY≤CG,G∈C∪G1∪S;(2c)Xg=Xh,Yg=Yh,∀g,h∈An,An∈A;(2d)xg,i∈{0,1},i∈Ig,g∈G;(2e)yg,j∈R,j∈Jg,g∈G.Formulation MP1 is flexible enough to describe both the usual stochastic formulations, but also to describe formulations in between those, such as that shown in Fig. 1c. For large scenario trees, there are as many such possible mixed formulations (analogous to the splitting-compact representation used in Escudero et al., 2010a), as stages in the tree.The definitions of familiesL,Aand, more important to us,C,are closely linked to the selection of a break stage (effectively equivalent to that in Escudero et al., 2012 for most scenario trees).Definition 2Break stageFor a given DEM, we define stage s* as the break stage of the problem if s* is the earliest stage in which no node g in s* or any latter stage is subject to non-anticipativity constraints (NACs) in order to be equivalent to the compact formulation (e.g. a formulation with s* = 1 is equivalent to the compact formulation).A variable xg, i(yg, j), wherei∈Ig(j∈Jg), is said to be coordinated11This concept is related to the common variables defined in Escudero et al. (2010b). In some contexts, however, there might be common but non-coordinated variables, hence the distinction made here.ifg∈Ss,s<s*.Formulation MP1 represents the tree in Fig. 1c, with a break stage s* = 2, and consequentlyS={{g1,g2},{g3,g4},{g5, g6, g7}},C={{g1,g3,g5,g6},{g2, g4, g7}}, and the NAC classes asA={{g1,g2}}.The choice of the break stage determines which stages have explicit NACs (stages 1 through s* − 1) and which are handled implicitly using a compact formulation. Moreover, the selection of the break stage is important for the computational implementation of the BFC algorithm. It determines the number of subproblems we will solve, simultaneously, and the size thereof. The impact of this is well-discussed in Escudero et al. (2010b), where the optimal partition of the tree is analysed for a particular-serial-computational implementation of the algorithm. As we argue in the results section, the break stage and the resulting decomposition affect the quality of the bounds obtained and the effective speed at which we can solve subproblems (see also Escudero et al., 2010a for further comparisons). This sometimes makes a given problem dramatically faster to solve with a particular decomposition than with another one.While solving a problem using BFC, several versions and/or parts of said problem require solving in order to calculate candidate solutions and bounds. These include both individual clusters’ problems and various relaxed versions of the total DEM. For this section, as well as all experiments, we assume the problem models and instances are devoid of all multi-scenario constraints except for the NACs.The subproblems corresponding to each cluster are independent except for the existence of the NACs. Though variables from all clusters interact in the objective function, the assumption of linearity guarantees separability. In view of this, by relaxing all NACs in a particular (mixed) formulation, we can create|C|independent subproblems which can be solved as:(3a)CP1c:min:fc(X,Y)=∑g∈Ccwg(agXg+bgYg)(3b)s.t.AGX+BGY≤CG,G∈Gc1∪Cc;(3c)xg,i∈{0,1},i∈Ig,g∈Cc;(3d)yg,j∈R,j∈Jg,g∈Cc;where, to avoid ambiguity, we putGc1={G∈G1:G∩Cc≠∅}.It generally happens that, at a given point in the BFC solution process, some of the binary variables xg, iwill be fixed or even relaxed in one or more nodes belonging to one or more clusters. To express this, we define paired sets of indices for nodes and variables with some fixing,(Gf,Igf),withGf⊆GandIgf⊆Igfor someg∈Gf,and identically for pairs(Gr,Igr)for relaxed binary variables and(Gb,Igb)for non-fixed, non-relaxed binary variables. If we defineHf={(g,i):g∈Gf,i∈Igf}for the fixed variables (and analogouslyHr,Hbfor the relaxed and binary variables), then, for cluster c,22Because a B&B node can have all fixed, relaxed and binary variables, setsGr,GfandGbare not necessarily disjoint. However, since a given pair (g, i) represents one specific variable in the problem, which can only be in one of the three states, setsHf,Hr,Hbare mutually disjoint.the subproblem to be solved is:(4a)CP2c(Hf,Hr,Hb):min:fc(X,Y)=∑g∈Ccwg(agXg+bgYg)(4b)s.t.AGX+BGX≤CG,G∈Gc1∪Cc;(4c)xg,i=xg,if,(g,i)∈Hf,g∈Cc;(4d)xg,i∈{0,1},(g,i)∈Hb,g∈Cc;(4e)yg,j∈R,j∈Jg,g∈Cc;withxg,iffixed to either 0 or 1 parameters for all(g,i)∈Hf.When we solve the entire problem, binary variables must be treated as either integers, relaxed, or fixed to values provided by the clusters’ solutions or by the branching process. The corresponding model is thus:(5a)MP2(Hf,Hr,Hb):min:f(X,Y)=∑g∈Gwg(agXg+bgYg)(5b)s.t.AGX+BGY≤CG,G∈C∪G1;(5c)Xg=Xh,Yg=Yh,g,h∈As,As∈A;(5d)xg,i=xg,if,(g,i)∈Hf;(5e)xg,i∈{0,1},(g,i)∈Hb;(5f)xg,i∈[0,1],(g,i)∈Hr;(5g)yg,j∈R,j∈Jg,g∈G;withxg,iffixed to either 0 or 1 parameters for all(g,i)∈Hf.Every time a cluster problem CP2c(Hf,Hr,Hb)is solved, it delivers a solution for the variables Xg, Yg,g∈Cc. Unless the coordinated variables are equal for all clusters (either by chance or due to fixing), these values will likely not be optimal for the complete problem MP1 but rather will provide a lower bound which can be easily calculated by adding up the solutions of all the clusters:(6)f̲c=∑Cc∈Cfc(X*c,Y*c),where X*c, Y*care the optimal solutions for all the binary and continuous variables in CP2c(Hf,Hr,Hb).The bound(7)f̲LP=f(X*c,Y*c),is obtained by solving MP2(Hf,Hr,∅)with(Gf,If)determined by the progressive fixing of the binary variables according to the BFC process. All non-coordinated (hence non-fixed) binary variables are relaxed, as well as those coordinated binary variables which have not been fixed to a 0/1 value.For the BFC, the lower bound of a particular partition of the variables in(Hf,Hr,Hb)is then(8)f̲*=max(f̲c,f̲LP).However, if all coordinated binary variables Xghave been fixed to the same values for all nodes g in all clusters, the solutions X*cfrom all cluster subproblems CP2care feasible and non-anticipative for MP2. We then makeHfsuch thatGf:=G,Igf:=Igand fix all non-coordinated binary variables to the clusters’ solutions, and solve MP2 as an LP. If all continuous variables Y are feasible and non-anticipative too, we have a candidate solution for the original problem which can be used in the branch-and-bound part of the BFC algorithm described in the next section. If a feasible solution cannot be found, MP2 is solved as a MIP withGf:={g∈Ss|s<s*},Gr=∅andGb={g∈Ss|s≥s*}.One can think of the BFC algorithm as a branch-and-bound routine in which each node of the search tree contains the status of all variables in all subproblems, and decisions are made considering all the clusters at the same time. Some of the variables before stage s* are common for many clusters (i.e. those which are subject to NACs), so they should all branch coordinately: at the same moment and to the same values during the search.The goal of the algorithm is to maintain the non-anticipativity of these coordinated binary variables while providing feasible solutions to the non-coordinated variables; the non-anticipativity of the continuous variables before stage s* is also guaranteed whenever problem MP2 is solved.In every iteration, we will often (though not necessarily so) find ourselves doing as many calls to the solver as the number of clusters created from the original scenario tree, it is in this part where we implement parallel computing. Since the NACs will eventually hold due to the coordination part of the algorithm described in Step 3, we can solve independent subproblems for each cluster. However, some of the clusters’ solutions in a particular iteration would have been solved already because whichever variables which were newly fixed do not appear in those clusters, thus any previously calculated solutions remain valid.Notice that an early s* implies relatively larger subproblems CP2c, but fewer clusters and coordinated variables.For a more detailed explanation of the BFC algorithm, we refer to Alonso-Ayuso et al. (2003) and Escudero et al. (2010b, 2012). Our own implementation is summarised below. We assume the reader is familiar with the concepts used in a branch-and-bound routine (Wolsey, 1998).Let L be the list of open nodes (problems) in the B&B tree,f^the best solution to problem MP1, andfits best lower bound.Step 0(Initialization)f^:=+∞,f̲:=−∞,L:=∅.(Root node) Create the first B&B search node n0 in the problem list L, setting all coordinated variables as relaxed(Gf=∅,Gr=G),Igr=Igfor eachg∈Gr; and define the node lower boundf* as − ∞.Add this node to the list of unsolved B&B nodes, n0 → L.(Select a node from the list) If L = ∅, go to Step 13. Otherwise, choose the first node from the list as the current node.(Bound node) Solve the individual cluster subproblems CP2c(Hf,∅,Hb). For each cluster, buildHfusing the information of the current node to determine which coordinated binary variables are fixed to either 0 or 1. Assign the rest of the variables toHb.(Process bound) If any subproblem is infeasible, go to Step 12 (prune by infeasibility).Otherwise, computefcas shown in (6).Iff̲c≥f^,go to Step 12 (prune by cutoff).If not, update the lower bound for this search node asf* = max {f*,fc}.Then, if all binary variables satisfy the NACs, go to Step 8. Otherwise continue.(Select branching variable) Select a branching binary variable xg, iin the node, according to the selected branching order criterion (natural order, most fractional, etc.). Update the setsHf,Hrsuch thatGf=Gf∪{g},Igf=Igf∪{i}andIgr=Igr∖{i}. IfIgr=∅,thenGr=Gr∖{g}.(Branch) Add two new nodes to the list, identical to each other except for their value ofxg,if∈{0,1}. Update the priority value of each node according to the node selection criteria (best first, depth first, etc.)(Reorder) Reorder the list of nodes according to the priority criterion. Go to Step 2.(Check continuous NAC) Build a binary solution for the total problem given the solutions provided by subproblems CP2c. Solve this MP2(Hf,∅,∅),withHfcontaining all binary variables, to obtain a candidate solution to MP1.If MP2 is feasible, let f* be equal to its objective value. Otherwise, set f* as NaN (there is no candidate solution for this node at this point).If f* is better than the incumbentf^,updatef^:=min{f^,f*}. Prune the tree.(Bound node with LP model)Solve problem MP2(Hf,Hr,∅)withxg,ifdefined as those binary variables fixed by branching (i.e. the sets(Gf,Igf)∈Hfcontain some/all coordinated variables) to satisfy the NAC for the y variables to obtainfLP.If MP2 is infeasible, go to Step 12 (pruning by infeasibility).Update the lower bound of the node as in (8),f* = max (fc,fLP).(Check candidate solution) If f* = NaN, go to Step 11.If f* =f*, go to Step 12; else go to Step 5.(Fully coordinated node) Solve the MIP problem MP2(Hf,∅,Hb)withHfsuch that it contains all coordinated binary variables (xg,ifare given by the solutions of the cluster subproblems CP2c, which satisfy the NAC,) andHbcontains all the non-coordinated ones.If feasible, update the incumbent and prune the tree.If in this search node all coordinated variables have been fixed (i.e. no more branching can be done), go to Step 12, otherwise go to Step 5.(Close the node) Remove this node from the list. Go to Step 2.(End) Stop. The optimal solution value isf^.There are some particularities inherent to our BFC implementation. Most notable, there is one, and only one, subproblem per processing core, being solved in parallel each time a subproblem solution request is sent by the master thread. This means that loading a problem from file into memory is done only once, and the data is kept in the memory space allocated to that core. The only messages passed between cores are the fixed/relaxed states of the variables, optimal solution values, and the algorithm status reports; this keeps the message passing overhead and data input/output as light as possible.The master thread handles the list of open nodes and sends requests to the worker threads to solve the CP2csubproblems with a particular state of the common binary variables.When the master needs the solution of the node with highest priority (defined according to the node selection criteria, i.e. depth-first, best-first or breadth-first), it sends the information of said node to each worker so that they can determine which variation of their problem CP2cwill solve. Given that some coordinated variables are not common to all the clusters sometimes only a subset of the|C|clusters needs to actually be solved.The pseudo-code of the parallel algorithm is shown in Algorithm 1and Table 1briefly presents the main tasks done in each step of the algorithm, relating it to the BFC algorithm presented in the previous section.This algorithm is coded in C++, using the openMPI (Open MPI Project, 2013) implementation of the MPI protocol to pass the information between threads.As noted in the algorithm above, the subproblems are solved as MILPs, using the technology of the solver of choice to efficiently compute integer solutions which are then passed to the master thread. In turn, the master thread will mostly solve LPs, which arise from fixing each binary to a value determined either by coordination or by sub-problem solution; or by relaxing the non-fixed and the non-coordinated binary variables.The MILP versions of the complete problem to be solved in Step 11 of the algorithm are typically lighter than the problem on file. Indeed, because at this point we have fixed all coordinated variables, the number of ‘real’ binary variables in the clusters is smaller the higher the break stage s* is. As pointed out in the results section, many instances using the investment/decision paradigm, such as those labelled C##, would not reach Step 11 at all.When compared to other past and current BFC implementation efforts (Aldasoro et al., 2013; Escudero et al., 2010b; Escudero et al., 2012), we can say that our implementation differs in three key aspects:•We take on a problem directly, loading the total problem as-is into the master thread, while the research groups in the references use, among other advanced techniques, an Outer Parallelisation to fix some of the binary variables. This creates n independent problems roughly the size of the original one in a partial enumeration scheme. Along with other additional parallelisation schemes around the core BFC, this has reportedly good results. We, on the other hand, have focused our efforts in improving the performance of the core BFC itself, using distributed computing only to the extent it helps, or is needed by, the BFC.We rely on faster node processing over tightening of bounds; as opposed to evaluating a ‘large’ MIP to test the non-anticipativity of the continuous variables at certain points of the process. The full problem MP2 is solved only in Step 8 (when it is actually a LP due to binary fixing) or in Step 11 (when there is no other way to test feasibility of a coordinated node).Finally, we allow for the selection of the branching node and variable according to different strategies, instead of using a pre-determined branching order.This BFC implementation was designed with industry-sized problem instances in mind. This leads to the assumption that the scenario trees employed are large enough to be decomposed in as many clusters as needed to achieve computational efficiency. Since (in principle, and given good computational performance) all available processors can be used for a single cluster, the design of our application has been more focused on a lean and efficient BFC, instead of refining parallelised strategies for flexibility and maximum usage of resources.An ad hoc heuristic, while very valuable when one has partial integer solutions which ultimately will lead to poor or unfeasible leaf nodes, implies the total problem will be solved more often, which might cause performance problems if the machines used are not efficient enough. We prefer to avoid this at the expense of potentially poorer bounds.

@&#CONCLUSIONS@&#
