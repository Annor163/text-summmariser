@&#MAIN-TITLE@&#
Prediction of mortality after radical cystectomy for bladder cancer by machine learning techniques

@&#HIGHLIGHTS@&#
Machine learning methods are used to predict the mortality after radical cystectomy.Extreme learning machine (ELM) based algorithms outperform in speed and accuracy.ELM and regularized ELM can identify the predictors of mortality after the surgery.

@&#KEYPHRASES@&#
Bladder cancer,Radical cystectomy,Mortality,Prediction,Prognosis,Machine learning,

@&#ABSTRACT@&#
Bladder cancer is a common cancer in genitourinary malignancy. For muscle invasive bladder cancer, surgical removal of the bladder, i.e. radical cystectomy, is in general the definitive treatment which, unfortunately, carries significant morbidities and mortalities. Accurate prediction of the mortality of radical cystectomy is therefore needed. Statistical methods have conventionally been used for this purpose, despite the complex interactions of high-dimensional medical data. Machine learning has emerged as a promising technique for handling high-dimensional data, with increasing application in clinical decision support, e.g. cancer prediction and prognosis. Its ability to reveal the hidden nonlinear interactions and interpretable rules between dependent and independent variables is favorable for constructing models of effective generalization performance. In this paper, seven machine learning methods are utilized to predict the 5-year mortality of radical cystectomy, including back-propagation neural network (BPN), radial basis function (RBFN), extreme learning machine (ELM), regularized ELM (RELM), support vector machine (SVM), naive Bayes (NB) classifier and k-nearest neighbour (KNN), on a clinicopathological dataset of 117 patients of the urology unit of a hospital in Hong Kong. The experimental results indicate that RELM achieved the highest average prediction accuracy of 0.8 at a fast learning speed. The research findings demonstrate the potential of applying machine learning techniques to support clinical decision making.

@&#INTRODUCTION@&#
Bladder cancer is a common cancer in genitourinary tract [1], affecting mainly the elderly population. In Hong Kong, a survey reported in 2011 that bladder cancer constituted 1.4% of all new cases of cancers [2]. Among various types of bladder cancer, muscle invasive bladder cancer has poor prognosis, with high tendency of metastasis and mortality that necessitate prompt treatment [3]. The most effective treatment approach is the surgical excision of bladder and the surrounding lymphatic tissue, which is known as radical cystectomy. Radical cystectomy is a major surgery that has significant morbidity and mortality [2]. The early postoperative complication rate is between 25% and 57% [4,5], and the early mortality rate is around 3%. The rates are higher for elderly patients [6–8]. Efforts have been made to identify the risk factors in order to maximize the operative outcomes, particularly the long-term survival after surgery. In a retrospective review, the association between clinicopathological factors and mortality for 117 patients treated with radical cystectomy for bladder cancer was investigated from statistical inference perspective [2]. In this study, instead of statistical inference, seven machine learning methods – back-propagation neural network (BPN), radial basis function network (RBFN), extreme learning machine (ELM), regularized ELM (RELM), support vector machine (SVM), naive Bayes (NB) classifier and k-nearest neighbour (KNN) algorithm-are exploited as alternative approaches to predict the overall 5-year survival based on the same clinicopathological dataset of 117 patients treated with radical cystectomy.The indications for radical cystectomy include treatment failures in non-muscle invasive bladder cancer, and T2-T4aN0M0 muscle invasive bladder cancer [9,10]. Radical systectomy includes the excision of bladder, prostate (in case of men), urethra, part of distal ureter, and lymphatic tissue of pelvis. After the removal of urinary bladder, urinary diversion is performed to divert the urine produced from the kidneys outside of body. Urinary diversion can either be continence diversion by a neo-bladder or continence pouch, or urinary conduit and ureterocutaneostomy. Radical cystectomy and urinary diversion can be performed by conventional open surgery or minimally invasive surgery.In the fields of medicine, clinical data are essential for marking diagnosis, formulating treatment and predicting prognosis. Clinicians make use of the knowledge in different specialties to analyze the histological (cell-based), clinical (patient-based), and demographic (population-based) information [11], where statistical methods such as Cox regression, logistic regression and Kaplan–Meier estimator are conventional employed in the analyses. For example, Kaplan–Meier method and Cox proportional hazards model were used to evaluate the prognostic factors of recurrence, progression and disease mortality in patients with bladder cancer [12]. Logistic regression based on 12 variables was used to identify the predictors of overall 5-year survival of patients who had undergone radical cystectomy for bladder cancer [13]. However, with rapid development of health technologies and informatics, medical data of high dimensionality are made available in both volume and variety. The accuracy of outcome prediction depends heavily on effective information integration of data acquired from various sources, clinically or pathologically, making the conventional statistical analyses that rely on clinicians׳ knowledge and experience a difficult task. The weakness of statistical methods is more apparent when handling medical data with high variability, nonlinear interactions among the variables, and heterogeneous distributions. For example, regression model, a common statistical technique, often requires some explicit assumptions on the relationships among the data that may be practically invalid [13]. Hence, researchers have begun to investigate alternative techniques for clinical outcome prediction, where computational approaches are a main focus. In particular, machine learning has been introduced into the medical domain to overcome the problems with statistical methods and uncover the knowledge hidden in the complex clinical data.Machine learning is a field in computer science leveraging knowledge from artificial intelligence, optimization and statistics to develop algorithms based on the available data. The approach is to build a model by learning from experience (i.e. the existing data, or the known samples acquired) and use the model to make predictions for the new samples [14]. While the quality and size of the samples can affect the prediction performance, machine learning methods are able to handle large, noisy and complex datasets, rendering it a promising technique for broad application in various areas. They have been explored as a more powerful alternative to statistical methods for classifying patterns and making predictions using techniques such as unconventional optimization strategies, conditional probabilities or absolute conditionality [15].In medicine and healthcare, machine learning has been applied for personalized and predictive medicine [16], cancer diagnosis and detection [15], and for the study of prevention and treatment policy [11]. For bladder cancer, robust outcome predictions of patients undergoing radical cystectomy was achieved using artificial neural work (ANN) prediction model, with configuration optimized by genetic algorithm (GA) [17]. The system was user-friendly and had potential for widespread use for medical decision support. Histology type and bilharziasis datasets were employed to construct a model using ANN and radial basis function network to predict the survival of bladder cancer patients after diagnosis [18]. Besides, clinicopathological and molecular markers were also used to create an ANN model for predicting one-year survival of patients with muscle invasive bladder cancer [19].As there is never a best algorithm for all problems, it is necessary to test the performance of different algorithms on a specific problem and identify the optimal one [20]. In this paper, seven machine learning methods are investigated to evaluate their performance in predicting bladder cancer mortality after radical cystectomy for the purpose of prognosis. The seven methods – BPN, RBFN, ELM, RELM, SVM, NB classifier and KNN – were selected because they are representatives among the algorithms in their respective domains. Details of these methods will be provided in Section 3. The implementation of these methods included two major processes. In the training process, the learning methods made use of the training dataset, e.g. the supervised input-output pairs, to identify the relationship directly from the clinicopathological data and built the corresponding model. In the testing process, the classification ability of the model is evaluated using the testing dataset. The predicted outputs were compared with the actual outputs of the testing dataset to measure the performance in terms of accuracy, sensitivity, specificity and precision.The reminder of this paper is organized as follows. Section 2 describes the clinicopathological data adopted in the study. Section 3 briefly reviews the principles of the seven machine learning methods. In Section 4, the 10-fold cross validation strategy and performance indices used in the experiment are introduced. In Sections 6 and 7, the prediction results are presented and discussed. A conclusion is given in Section 7.The dataset employed in this study originated from a retrospective review on the 5-year survival of patients treated with radical cystectomy for bladder cancer [2]. The data were retrieved from computerized clinical records of 117 patients who had undergone radical cystectomy within the period from 2003 to 2011 in a urology unit in Hong Kong. The purpose in the retrospective study was to examine whether age, tumor stage and preoperative serum albumin level are independent predictors of survival after radical cystectomy. Ninety-nine of the patients were male. The mean age was 68 years old (SD=10 years). There was no loss of follow-up. The mean follow-up time was 31 months (SD=29 months). The 30-day mortality, 5-year cancer-specific mortality, other-cause mortality, and the overall mortality rates were 3%, 33%, 22% and 55% respectively. Open radical cystectomy was performed in 71 cases and laparoscopic/robotic-assisted radical cystectomy was conducted for the rest. 96 patients had ileal conduit and 21 patients had continent diversion. Other data includes hospital stay duration, preoperative serum albumin level, Charlson comorbidity index, tumor grade, tumor stage and pathological lymph node status. Further details about the dataset can be found in [2].For those attributes covering wide numerical range when compared with the others, i.e., age, preoperative serum albumin level and follow-up time, pre-processing was performed to normalize them into the range of [0, 1]. With the advice from physicians, irrelevant data were ignored for the study (e.g. date of operation, date of death) and the final dataset used in the study contained 10 attributes (input) and 1 class attribute (output), as shown inTable 1.In this section, the seven machine learning methods adopted in the study are briefly presented. The setting of the model parameters in the experiment is also discussed.ANN emulates the learning ability of biological neural networks where appropriate interconnections (i.e., nodes) are made between the neurons for information transmission and parallel processing at the neurons. BPN is the most prevailing algorithm developed to train the ANN [21]. In BPN, the multilayer perception (MLP) architecture is usually used for data classification and prediction. As shown inFig. 1, the architecture contains one input layer, one output layer, and one or more hidden layers. The nodes in the ANN are connected by links, each associated with a weight, to model the nonlinear relationship between the input and output layers.The network is trained using supervised learning with known input–output pairs. In the training process, data fed forward into the input layer pass through the nodes in the hidden layer via the interconnections to produce the predicted results at the nodes in the output layer. The value at each node is computed using a mathematical function, known as activation function [22]. The differences between the prediction and the actual outcome are back-propagated to optimize the network structure (i.e., to learn the nonlinear relationship). In this study, the value Hjof a hidden node j is expressed as(1)Hj=f(∑i=1nwijxi−bj)j=1,2,…,l,wherewijis the weight of the link connecting the input nodeiand the hidden nodej,xiis the value of the input nodei,bjis the bias,nis the number of input nodes,lis the number of hidden nodes, andfis the activation function. In this study,fis given by(2)f(x)=2(1+exp(−2x))−1.There was only one output node in the study, which is to predict 5-year survival after surgery. The valueOof the node in the output layer is given by(3)O=g(∑j=1lHjwj−b),wherewjis the weight of the link connecting the hidden nodejand the output node,bis the bias, andgis the activation function of the output node, which is given by(4)g(x)=x.If the predicted result is unacceptable, the error e between the computed output O and the target output Y, i.e.,(5)e=Y−Owill be back-propagated through the network, so that all the weights and the biases will be re-adjusted to minimize the error. The weight update rules are given by(6)wij=wij+ηHj(1−Hj)xiwije,i=1,2,...,n;j=1,2,...,l,and(7)wj=wj+ηHje,j=1,2,...,lwhereηis the learning rate. The bias update rules are(8)aj=aj+ηHj(1−Hj)wje,j=1,2,...,l,and(9)b=b+eThe BPN algorithm executes iteratively until an acceptable error is reached or the maximum iteration limit is met.In this study, the BPN had 10 input nodes and 1 output node, corresponding to attributes of the clinical data listed in Table 1. The number of hidden nodes was determined through repeated testing using the data to obtain the optimal number. While under-fitting of the data by the network model can be avoided by using more hidden nodes, the computational time and the model generalization capability are adversely affected. In addition, the learning rate was also varied in the experiment to identify the optimal value, and thus the corresponding weights and bias values of the resulting BPN. Otherwise, it would result in oscillations and instability [23] if the rate is too high, or leading to a slow training process if the rate is too low. The momentum of the learning process, a parameter to control the ability to get rid of suboptimal solutions during the learning process, is also set appropriately so that the network can converge to the optimal structure in a stable and swift manner.In the experiment, the number of hidden neurons, the momentum and the learning rate were varied by taking a value from the sets{3,5,7,9,11,13,15,17,19,21,23,25,27,29},{0.9,0.5,0.2,0}and{0.01,0.05,0.09}respectively in each trial. Different combinations were used to evaluate the performance of the BPN obtained, thereby identifying the optimal values and the best BPN structure.In addition to BPN, another algorithm for training ANN is also investigated in the study, namely, the RBFN [24]. The structure of RBFN adopted in this study was similar to that of the BPN, as shown in Fig. 1. In RBFN, radial basis function is used as the activation function for the hidden nodes. Gaussian function is typically adopted as the radial basis function since it allows for factorization and linear optimization in the formulation. In RBFN, each hidden node j is parameterized with the center vectorcj, and the widthσj[25]. When the Gaussian function is adopted, the output of the networkOis given by(10)O=∑j=1lwjexp(−12σ2‖x−cj‖2)wherex=(x1,x2,...,xn)Tis the input vector,cjis the center vector for the hidden nodej,‖x−cj‖is the Euclidean distance between the input vector and the center vector,wjis the weight of the connection between node j and the output node, andlis the number of hidden neurons. The widthσjof thejth hidden node, also called the spread, is defined as(11)σj=cmax2lj=1,2,...,lwherecmaxis the maximum of the Euclidean distances between the centers [26].It can be seen from Eq. (10) that the output of the RBF network is close to 1 if the distance between the input vector and the center vector is small. Geometrically, the hidden layer of the RBFN maps the input vector in the low dimensional space into the high dimensional space. In other words, the problem in the input space becomes linearly separable with a hyperplane in the high dimensional space.In the experiment, the spread of the RBFN was varied to identify the optimal value. An RBFN with large spread is prone to misclassification while a small spread has poor generalization capability [27]. The spread of the RBF network in the experiment was selected from 25 different values ranging from2e−12to2e12, at powers of 10, to compare the performance and find out the one yielding the highest accuracy.ELM is a fast learning method for single-hidden layer neural network [28,29]. A key feature of ELM is that the weights and the bias between the input and the hidden layers are randomly assigned, whereas the weights between the hidden and the output layers are analytically determined by utilizing Moore–Penrose (MP) generalized inverse operation of the hidden output matrices [30]. In this study, the algorithm used to implement the ELM is summarized as follows.Step 1: Randomly assign the input weightwijbetween the input and the hidden layers, and the biasbj. The nonlinear system is then transformed into a linear system and can be expressed as(12)Hβ=TwhereHis the hidden-layer output matrix,βis the matrix of output weights andTis the matrix of the desired output. Further details about the formulation can be found in [30].Step 2: Calculate the hidden layer output matrixH.Step 3: Calculate the output weights vectorβby obtaining the least-square solution of the linear system in Eq. (13).A variation of ELM, RELM, was also investigated in this study. RELM inherits the fast learning feature of ELM while its generalization performance is enhanced by using the least squares regression methods to identify the degree of relevance of the weight linking a hidden node to the output layer, where penalties are applied to the coefficient vectors [31]. In RELM, the regularization parameterγis introduced to improve the controllability [32]. In order to reduce the effect of noise, RELM introduces a weighting factorvito weigh the error between the output of RELM and the actual output of the ith input sample. Hence, the output weightβin Eq. (13) can be expressed as(14)β=(Iγ+HTD2H)+HTD2T,whereD=dialog(v1,v2,...vN)andγis the regularized factor. In the experiment, the number of hidden nodes in both the ELM and RELM was set experimentally by selecting a value from{20,22,24,26,28,30,32,34,36,38}.SVM is a typical kernel-based technique for supervised data classification. The basic principle is to create a hyperplane as the decision surface for classification, where the edge of the isolation between different categories of data is maximized. In the process, the input data vectors are first mapped to a high-dimensional space [33]. The SVM algorithm then searches for a hyperplane with the largest margin in order to achieve the best generalization ability.Fig. 2 gives an example with two linearly separable classes and the data points are denoted by crosses and triangles respectively. The points closest to the decision surface are the supporting vectors and the distance between support vectors and surface is the margin.Consider a given training datasetT={(x1,y1),...,(xN,yN)}∈(X×Y)withxi∈X=Rn,yi∈Y={1,−1}(i=1,2,...,N), where the training data matrixXhas two separable classes with the class labels −1 and +1 stored in the vectorY. Applying the Lagrangian multiplier with the kernel functionK(x,x′)and the regularization parameterC, the dual formulation of SVM is written as follows,(15)min12∑i=1N∑j=1NyiyjαiαjK(xi,xj)−∑j=1Nαjs.t.∑i=1Nyiαi=0,0≤αi≤C,i=1,...,Nwhere the optimal solutionα⁎=(α1⁎,...,αN⁎)Tand threshold valueb⁎can be obtained. Thus the decision function for the classification is given by(16)f(x)=sgn(∑i=1Nαi⁎yiK(x,xi)+b⁎).The summation in Eq. (16) is performed only on a small group of support vectors with non-zeroαi.In this study, a polynomial kernel function was used in the experiment, where the optimal value of the regularization parameter C is determined using the 10-fold cross-validation strategy (to be discussed in Section 4.1) to achieve a balance between classification accuracy (with a larger C) and generalizabity (with a smaller C, i.e. larger margin). The value of C in the experiment was chosen from{150,200,250}.The KNN algorithm was also investigated in this study for its simplicity in implementation [34]. In KNN, the training dataset is reserved and used to classify a new unclassified testing dataset. Classification is achieved by comparing the testing dataset with the groups in the training set to identify the most similar one based on a distance function [35]. In this study, the similarity between two neighboring data points x and y is measured by the Euclidean distance(17)∑i(xi−yi)2,wherex=(x1,x2,...,xn)andy=(y1,y2,...,yn)are vectors in n-dimensional space.The number of neighborskhas to be set appropriately to reduce the effect of noise and outliers on the classification (k too small) and to avoid the dominance of local behavior (k too large) [35]. In the study, the value ofkwas chosen experimentally with values in{10,12,15,18,20}to identify the one with the best performance.Lastly, the NB classifier was employed in the study by assuming that all the attributes were conditionally independent of the class labels [36,37]. In the training stage, the NB classifier estimated the parameters of the class priors and the probability distribution of the attributes by analyzing the training samples. In the testing stage, the method calculated the posterior probability of every sample belonging to each class. The NB classifier then selected the class with the largest posterior probability as the output of the testing samples.

@&#CONCLUSIONS@&#
