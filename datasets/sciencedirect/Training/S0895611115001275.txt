@&#MAIN-TITLE@&#
Telemedicine as a special case of machine translation

@&#HIGHLIGHTS@&#
We created Machine Translation system for medical text domain.We adapted it for PL-EN language pair.We improved translation quality by adaptation of additional training data and interpolating language models.Numbers of experiments on different methodologies were performed.The high quality translations were obtained in evaluation process.

@&#KEYPHRASES@&#


@&#ABSTRACT@&#
Machine translation is evolving quite rapidly in terms of quality. Nowadays, we have several machine translation systems available in the web, which provide reasonable translations. However, these systems are not perfect, and their quality may decrease in some specific domains. This paper examines the effects of different training methods when it comes to Polish–English Statistical Machine Translation system used for the medical data. Numerous elements of the EMEA parallel text corpora and not related OPUS Open Subtitles project were used as the ground for creation of phrase tables and different language models including the development, tuning and testing of these translation systems. The BLEU, NIST, METEOR, and TER metrics have been used in order to evaluate the results of various systems. Our experiments deal with the systems that include POS tagging, factored phrase models, hierarchical models, syntactic taggers, and other alignment methods. We also executed a deep analysis of Polish data as preparatory work before automatized data processing such as true casing or punctuation normalization phase. Normalized metrics was used to compare results. Scores lower than 15% mean that Machine Translation engine is unable to provide satisfying quality, scores greater than 30% mean that translations should be understandable without problems and scores over 50 reflect adequate translations. The average results of Polish to English translations scores for BLEU, NIST, METEOR, and TER were relatively high and ranged from 7058 to 8272. The lowest score was 6438. The average results ranges for English to Polish translations were little lower (6758–7897). The real-life implementations of presented high quality Machine Translation Systems are anticipated in general medical practice and telemedicine.

@&#INTRODUCTION@&#
Statistical Machine Translation (SMT) is the translation of the text by a computer, with no human involvement. SMT systems have no knowledge of language rules. Instead, they “learn” to translate by analyzing large amounts of data for each language pair. They can be trained in specific industries or disciplines using additional data relevant to the sector needed. Typically, SMT systems deliver fluent-sounding but less consistent translations.The machine translation is evolving quite rapidly in terms of quality. Currently, several machine translation systems are available on the web that provides reasonable translations. Developed systems are not perfect, and their quality may decrease in some specific domains. In addition to this, the scientific community is involved in machine translation. It must be pointed out that the scientific organizations, conferences, and events dedicate great effort to its improvement. One of the biggest advantages of machine translation is that most users do not require perfect translations [1]. Users may only be interested in roughly understanding a text simply to get an idea of what the text is about. However, other users may not be that flexible. For example, the correctness and beauty of writing in medicine may not be important, but the precision and adequacy in the translated message is crucial. In medical communication, a translation error between the patient and the physician, or an error in communication regarding treatment or a diagnosis may have serious consequences for a patient's health [2].Recently, due to the growing success of an interest in language technologies, machine translation has been applied to the field of medicine. For example, one study [3] analyzed the feasibility of post-editing machine translations of health-promotional English documents from local and national public health websites in the USA. It was assumed, a priori, that machine translation would not provide a high enough quality for the documents to be used as official versions. Despite that, language technologies are steadily increasing in quality. It should be expected that, in the not-too-distant future, machine translation will be capable of translating any text in any domain with the required quality.The medical data domain is, in our opinion, a very narrow, but relevant and promising field of research for language technologies. MT systems can be used for translation of medical records of any kind. Accessing and translating a foreign patient's medical history might even save their life. Preparation of direct speech-to-speech translation systems is also possible. The foreign patient's speech is recognized using an Automated Speech Recognition (ASR) system. After recognition, the speech is translated into another language and synthesized in real-time. For example, the EU-BRIDGE project aims at developing automatic transcription and translation technology that will permit the development of innovative multimedia captioning and translation services of audiovisual documents between European and non-European languages [http://www.eu-bridge.eu].Obtaining and providing medical information in comprehensive ways appears to be of crucial importance for both patients and physicians [4–7]. For example, as emphasized by Healthcare Technologies for the World Traveler (HTH) [8], a foreign patient may require an explanation and description of their diagnosis and comprehensive information about available treatment options. In several countries, many residents and immigrants communicate in languages other than the official one.According to Karliner et al. [9], it is necessary to analyze how human translators could enhance access to health care, including improvement of its quality [10]. Nevertheless, human translators experienced in telemedicine information are very often unavailable for both patients and medical professionals [11]. Although existing machine translation capacities are imperfect [11], machine translation must ensure the reduction of costs associated with medical translation. On the other hand, it is necessary to increase its availability and quality [12].Medical professionals, researchers, and patients require adequate access to the abundance of telemedicine information on the Internet [6,13]. This information can potentially improve our health and well-being. Sharing medical information could improve medical research, as well. English is the most dominant language used in medical science, but not the only one.Polish is considered to be the one of the most challenging West-Slavic languages, due to its complexity. It is a tough language for an SMT system. For example, Polish grammar, includes complicated rules and elements, including an immense vocabulary (thanks to its complex declension). Nearly free word order in sentences is also problematic. All of these are the main reasons for its challenging character. In addition, the Polish language includes 7 cases and 15 gender forms for both nouns and adjectives.As expected, these facts strongly influence the data and data structure used in statistical translation models. The lack of available and appropriate resources necessary for data input to SMT systems presents another problem. SMT systems give the best results for concrete and narrow text domains. The proper quality of the parallel data, including the required domains, has inadequate availability. On the other hand, Polish and English differ strongly in syntax. Above all, English is a positional language. This means that the syntactic order, which includes the word order of one sentence, has an invaluable significance, especially because of the limited inflection of words (for example, lack of declension endings). Sometimes, the position of the word in a sentence is the only indicator of the sentence meaning. As far as English sentences are concerned the subject comes before the predicate. Therefore, a sentence is structured in Subject–Verb–Object (SVO) word order. In contrast, Polish simply has no particular word order. Additionally, the word order itself has no decisive impact on a sentence's meaning. In Polish, one can express the same idea in many ways, which is simply not possible in English. For instance, the sentence “I have bought myself a new car.” can be expressed in Polish as “Kupiłem sobie nowy samochód”, or “Nowy samochód sobie kupiłem.”, or “Sobie kupiłem nowy samochód.”, or “Samochód nowy sobie kupiłem.” As one can see, changes in word order influence the complexity of the translation process.As a consequence, the development of SMT systems for the Polish language has been considerably slower in comparison to English and other languages. The primary goal of this research is to develop an SMT system for translation from Polish to English language and vice versa, with an emphasis on medical data. This paper has the following structure: Section 2 contains an introduction to the preparation of Polish data. Section 3 presents the English language issues. Section 4 describes the methods associated with translation evaluation. Section 5 presents the results. Sections 6 and 7 provide the summary of potential implications and opportunities for future work.The Polish data we included was a corpora derived from the European Medicines Agency (EMEA) parallel corpus. This corpus was created from biomedical PDF documents from the agency. It includes documents related to medical products and their translations into 22 official languages of the European Union. It contains roughly 1500 documents for most of the languages, but not all of them are available in every language [14]. It comprises around 80MB of data and 1044,764 sentences constructed from 11.67M words that were not tokenized. The data is pure text encoded in UTF-8. Additionally, the texts were separated into sentences (one per line) and structured in language pairs. We chose this corpus as most similar to medical texts (which we did not have access to unrestricted quantity) in terms of complexity and vocabulary.The vocabulary consisted of 148,170 unique Polish and 109,326 unique English tokens [15]. The disproportionate vocabulary size and number of tokens are also a true challenge, especially when it comes to translation from English to the Polish language.Before the use of the training translation model, preprocessing that included removal of long sentences (set to 80 tokens) had to be performed to limit the model size and computation time. Moses toolkit scripts [16] were used for this purpose. Moses is an open-source toolkit for statistical machine translation that supports linguistically-motivated factors, confusion network decoding, and efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.The preparation of the English data was far less complicated than that of the Polish data. We developed a tool to clean the English data by eliminating foreign words, strange symbols, etc. Compared to Polish, the English data included fewer errors drastically. However, some problems needed to be fixed. The most problematic were translations into languages other than English itself, including strange UTF-8 symbols, repetitions, and unfinished sentences. Such errors are typical when corpora are built using automatic tools.Human evaluations of machine translation outputs require considerable effort and are expensive. Human evaluations can take days or even weeks to finish. So, automatic metrics is needed to measure the translation quality derived from SMT systems. Different automated metrics is used to compare SMT translations and match the human translations. Among the most widely used SMT metrics are:-the Bilingual Evaluation Understudy (BLEU),the U.S. National Institute of Standards & Technology (NIST) metric;The Metric for Evaluation of Translation with Explicit Ordering (METEOR), andTranslation Error Rate (TER).SMT metrics was briefly described by Radziszewski [17]. BLEU is quick to use, is inexpensive to operate, is language independent, and correlates highly with human evaluation. It is the most widely-used automated method of determining the quality of machine translation. The metric scores of translation range from 0 to 1. It is frequently displayed as a percentage value. The closer to 1 (100%), the more the translation correlates to human translation. Put simply, the BLEU metric measures how many words overlap in a given translation when compared to a reference translation, giving higher scores to sequential words. Scores lower than 15% mean that the machine translation engine is unable to provide satisfactory quality, as reported by Lavie [19] and a commercial software manufacturer [18]. A high level of post-editing will be required to finalize output translations and reach publishable quality.A system score greater than 30% means that translations should be understandable without problems. Scores over 50% reflect good and fluent translations.The general approach for BLEU, as described in [19], is to attempt to match variable length phrases to reference translations. Weighted averages of the matches are then used to calculate the metric. The use of different weighting schemes leads to a family of BLEU metrics, such as standard BLEU, Multi-BLEU, and BLEU-C [20].The basic BLEU metric is expressed by equation [7]:BLEU=PBexp∑n=0Nwnlogpnwhere pnis an n-gram precision using n-grams up to length N and positive weightswnthat sum to one. The brevity penalty pBis calculated as:PB=1,c>re1−r/c,c≤rwhere c is the length of a candidate translation, and r is the effective reference corpus length [19].The standard BLEU metric calculates the matches between n-grams of the SMT and human translations, without considering the position of the words or phrases within the texts. In addition, the total count of each candidate SMT word is limited by the corresponding word count in each human reference translation. The BLEU metric avoids bias that would enable SMT systems to overuse high-confidence words to boost their score. BLEU applies this approach to texts sentence-by-sentence and then computes a score for the overall SMT output text. In doing this, the geometric mean of the individual scores is used, along with a penalty for excessive brevity in translation [19].On the other hand, the NIST metric tends to improve the BLEU metric by evaluating information in several ways. It uses the arithmetic mean instead of the geometric mean of the n-gram matches to emphasize the proper translation of rare words. The NIST metric also puts stronger importance on rare words. This metric has shown improvements compared to the BLEU metric. The NIST metric can output values between 0 and 15; the higher value, the better translation quality [21].The METEOR metric, introduced by the Language Technologies Institute of Carnegie Mellon University, is also designed to improve the BLEU metric. We applied it without synonyms and paraphrased matches for Polish. METEOR emphasizes recall by changing the BLEU brevity penalty. In addition, it takes into consideration higher order n-grams to favor matches in word order and uses arithmetic instead of geometric average values, as well. For multiple reference translations, METEOR measures the best score for word-to-word matching. Similar to BLEU, this metric also returns scores between 0 and 100. A detailed description of this metric was provided by Doddington [22].TER is the most recent SMT metric developed. This metric evaluates the minimum number of human corrections necessary for an SMT translation to successfully match a reference translation in terms of meaning and fluency. Necessary human corrections might include insertion, removal, and changing of words or phrases. In contrast to other metrics, a translation with a lower TER value is more similar to the reference translation. TER scores range from 0 to 100 [23].Several experiments have been conducted to evaluate different versions of SMT systems. The experiments included several steps, including corpora processing, tokenization, cleaning, factorization, lower casing, splitting, and final cleaning. Training data were evaluated, and a language model was developed. Tuning was performed for every experiment. Finally, the experiments were performed.Testing was performed using the Moses open source SMT toolkit, including the Experiment Management System (EMS) [24]. The SRI Language Modeling Toolkit (SRILM) [25] was used for 5-g language model training along with an interpolated version of Kneser–Ney discounting (interpolate – unk – in discount). We used the MGIZA++ tool for word and phrase alignment. KenLM [26] was applied to ensure the binarization of the language model. Lexical reordering was set to mid-bidirectional-fee. Reordering of the phrases’ probabilities was performed according to the lexical values of the phrases. It includes three different types of orientation, with an emphasis on sources and targeted phrases: monotone (M), swap (S), and discontinuous (D). The reordering of the bidirectional models includes the possible probabilities of the mutual positions of sourced counterparts in correlation with the actual and subsequent phrases. The probability distribution of foreign phrases is evaluated by “f,” and English phrases by “e” [2,27–29]. MGIZA++ is a multi-threaded version of the famous GIZA++ tool [25].A method of symmetrizing was developed to ensure appropriate word alignment. First, two-way alignments derived from GIZA++ were structured. As a result, only the points of alignments that appeared in both alignments were left. In the next phase, additional points of alignments that appear in their union were combined. The other steps contribute to the potential alignment points of unaligned and neighboring words. Neighboring can be positioned directly to left or right, top or bottom, including a diagonal (grow-dialog) position. During the final phase, the points of alignments between the words are combined (grow-dialog-final), where at least one is unaligned. If the grow-dialog-final method is applied, the point of alignment between the two unaligned words eventually occurs [30].Descriptive statistics, parametric, non-parametric tests, and inter-rater correlations were performed using the MedCalc Statistical Software version 15.2 (MedCalc Software bvba, Ostend, Belgium; http://www.medcalc.org; 2015) to find differences between the applied scores and between translation directions (Fig. 1).

@&#CONCLUSIONS@&#
Wu et al. [40] analyzed statistical machine translation output for six foreign language—English translation pairs (bi-directionally). They built a high-performing in-house system and evaluated its output for each translation pair on a large scale both with automated BLEU scores and human judgment. They also evaluated Google Translate's performance specifically within the biomedical domain. In their study automated BLEU scores did not achieve higher scores than 3624 for Polish to English translations. They found that Spanish to English and English to Spanish only achieved BLEU scores above 50. High-quality machine translation was noted for translating the biomedical titles, for languages (German, Spanish and French) with large training corpora accumulated in PubMed. The language training corpora influence the translation quality. They observed very low translation quality for languages with small training corpora.Pecina et al. [41] within the Khresmoi project investigated MT of user search queries in the context of cross-lingual information retrieval (IR) in the eHealth domain. Authors performed experiments and thoroughly evaluated on three language pairs: Czech–English, German–English, and French–English.They described that search query translation results were outstanding in their experiments. They noted 55% improvement on average. Google Translate and Microsoft Bing Translator in direct comparison carried out on all the language pairs. The baseline BLEU scores increased from 26.59 to 41.45 for Czech–English, from 23.03 to 40.82 for German–English, and from 32.67 to 40.82 for French–English. It must be noted that above experiments are not 100% comparable with our research because they were conducted on different data sets and for different languages. The text domain also puts a big impact on translation results. Our baseline system already had high scores.A couple of conclusions can be directly drawn from the results of the experiments conducted in this research. First of all, we were able to obtain relatively very high evaluation scores. Nonetheless, we anticipated such results because the statistical machine translation works best in very narrow and very specified text domain in which not only vocabulary is limited, but text are also semantically similar. This scenario was described by Wu et al. [42]. It was a little bit surprising that true casing and punctuation normalization lowered the scores by a significant factor. We believe that the texts have already been adequately cased and punctuated. In Experiment 02, we noticed that OSM lowered some metrics results. It often increases the quality of translations. Nevertheless, in the PL≥EN translation experiment the BLEU score improved just slightly, but, on the other hand, other metrics decreased. Similar results can be seen in the EN≥PL translation experiments. In this case, the BLEU score improved, but the other metrics decreased.The majority of other experiments produced the expected results. Almost all of them improved the score a little bit or at least confirmed our conclusions with each metric. Unfortunately, Experiment 12, which was structured in settings that ensured the best system score in IWSLT 2013 evaluation, did not improve the quality of this data as much as it previously did. The most probable reason is that the data applied in IWSLT did not derive from any specific text domain, while here we had to deal with a very narrow domain. The training, tuning, and adjustment parameters may need to be adjusted separately for every text domain. As for the cases in which improvements cannot be replicated, this should be done first. On the other hand, improvements achieved by training the hierarchically-based model surprised us. Compared to other experiments, Experiment 04 improved the BLEU score significantly. In addition, significant improvements can be noticed in both the PL≥EN and EN≥PL language translations, which most likely provide an excellent starting point for future research and experiments.It was decided to use that system for Experiments 13 and 14 with additional parallel data and Google's language model for Polish. The anticipated improvement of translation quality was achieved in both of these experiments. The translation quality in Experiment 13 is very close to what was anticipated, based on out-of-domain data. It was required to cover any out-of-domain translation scenarios or casual human dialogs. What is worth mentioning here is that, in our final experiments, we were able to obtain the almost equal quality of translation in both directions.Translation from EN to PL was much harder, which was proved by the experiments. As expected, the results of translation into Polish were not that good. Very likely, the reason for this is the complex structure of Polish grammar including the vast Polish vocabulary. It was one of the reasons why we used the Google language model.Our analysis led us to think that the translation results, where the BLEU measure is greater than 70, can be accepted as satisfactory only within the limits of a selected text domain. Such results should be possible to replicate with any language pair, but especially West-Slavic because of the linguistic and lexical similarities. Considering the specification of the BLEU metric, we assume that our translation systems perform with truly high quality, but there is still room for improvement. The differences of translation output (T) vs. original (O) texts are presented in Table 3. We can see that mistakes preserve sentence meaning and simply use synonyms or different sentence structure in many cases. The evaluation scores indicate that people could adequately comprehend the translation. and that they are good enough to help them in their work, but they are not sufficient to be used in such specialized areas as medicine yet (for example, in the case of patients in foreign hospitals). We have every reason to believe that an improvement in the BLEU score to a threshold higher than 80, or perhaps 85, could create systems that could be used for practical cases. It may be especially useful for the Polish language, which is one of the most complexes in terms of its structure, grammar, and spelling. Additionally, it must be emphasized that the experiments were conducted on texts obtained from the PDF documents shared by the European Medicines Agency. That is the reason the data is more complex and uses a more sophisticated vocabulary than casual human speech. Speech is typically less complicated and easier to process by SMT systems. It is here that we see another opportunity to increase output translation quality. Comparable results could be obtained for other language pairs as long as text domain is limited. Nonetheless, it would require adaptation of data and training parameter for each language independently.