@&#MAIN-TITLE@&#
An organ allocation system for liver transplantation based on ordinal regression

@&#HIGHLIGHTS@&#
The problem of constructing an organ allocation model via machine learning techniques is assessed.The combination of donor, recipient, and surgery characteristics helps the prediction of graft survival after transplantation.An ordinal classifier has been proved to perform well for the 4-category classification problem assessed.An extended supranational experimental design for liver transplantation allocation could be feasible considering more countries.

@&#KEYPHRASES@&#
Liver transplantation,Survival analysis,Machine learning,Support vector machines,Ordinal regression,Decision-making,

@&#ABSTRACT@&#
Liver transplantation is nowadays a widely-accepted treatment for patients who present a terminal liver disease. Nevertheless, transplantation is greatly hampered by the un-availability of suitable liver donors; several methods have been developed and applied to find a better system to prioritize recipients on the waiting list, although most of them only consider donor or recipient characteristics (but not both). This paper proposes a novel donor–recipient liver allocation system constructed to predict graft survival after transplantation by means of a dataset comprised of donor–recipient pairs from different centres (seven Spanish and one UK hospitals). The best model obtained is used in conjunction with the Model for End-stage Liver Disease score (MELD), one of the current assignation methodology most used globally. This problem is assessed using the ordinal regression learning paradigm due to the natural ordering in the classes of the problem, via a cascade binary decomposition methodology and the Support Vector Machine methodology. The methodology proposed has shown competitiveness in all the metrics selected, when compared to other machine learning techniques and efficiently complements the MELD score based on the principles of efficiency and equity. Finally, a simulation of the proposal is included, in order to visualize its performance in realistic situations. This simulation has shown that there are some determining factors in the characterization of the survival time after transplantation (concerning both donors and recipients) and that the joint use of these sets of information could be, in fact, more useful and beneficial for the survival principle. Nonetheless, the results obtained indicate the true complexity of the problem dealt within this study and the fact that other characteristics that have not been included in the dataset may be of importance for the characterization of the dependent variable (survival time after transplantation), thus starting a promising line of future work.

@&#INTRODUCTION@&#
During the last few decades, new trends in biomedicine have considered using some machine learning techniques as classification methods [1,2], which has worked well in a great number of problems and resulted in remarkable applications for science [3,4]. Liver transplantation is an accepted treatment for patients who present end-stage liver disease. However, transplantation is restricted by the lack of suitable liver donors; this imbalance between supply and demand resulting in significant waiting list death. In order to cope with this situation, several methods have been developed and applied to find a better system to prioritize recipients on the waiting list.The first attempt at developing a system was the Donor Risk Index (DRI) [5], aimed at establishing the quantitative risk associated with the transplant when considering donor information. Another widely validated methodology that is the cornerstone of current allocation policy, is the Model for End-stage Liver Disease (MELD) [6], which is based on the “sickest-first” principle, where the only aspect considered is information concerning the recipient. The use of expanded criteria donors (donors with extreme values of age, days in the intensive care unit (ICU), inotrope usage, body mass index (BMI) and cold ischemia time) results in an increased risk of recipient and/or graft losses compared to the risk associated with the use of livers from non-extended criteria donors [7]. These risks should be carefully analysed since the combination of several of these risk factors can result in graft loss [8]. Nevertheless, these methods can not be considered good predictors of graft failure after transplantation since they only take into consideration either characteristics of donors or of recipients (but not both), when there could actually be more complex factors involved in the situation (donor, recipient and transplant organ characteristics). In order to deal with this problem, Rana et al. [9] devised a scoring system (SOFT) that predicted recipient survival 3 months after liver transplantation, which is intended to complement MELD-predicted waiting list mortality rates by making use of both donor and recipient characteristics. P. Dutkowski et al. recently proposed a balance of risk (BAR) score [10] based on donor and recipient characteristics. A rule-based system was used to determine graft survival 1 year after the transplant [11]. The input of this rules-based system being the response of two artificial neural networks trained with donor, recipient and transplant organ characteristics.Fig. 1graphically represents the process of organ allocation (figure restructured from [12]). Generally, donors are assigned to the candidates under the greatest-risk according to the MELD score. This policy does not allow the liver transplant team to match the donor to the recipient according to principles of fairness and benefit. This could lead to a risk of unconscious gaming when trying to match marginal donors to urgent candidates.In the same vein, this paper considers a liver transplant dataset obtained from seven different Spanish hospitals and King's College Hospital in the UK and includes characteristics of donors, recipients and transplant organs, with the aim of developing and constructing a supranational system for predicting graft survival, by means of intelligent classification techniques. Although this problem has been tackled successfully before by means of a binary classification task [11], a significant contribution of this paper is that the classification problem is addressed using an ordinal regression point of view since the classes are ordered taking into account the time leading up to liver failure (in case of failure) providing therefore more information about the hypothetical graft failure. The classification problem could be also tackled by a multiclass classification problem but this approach will ignore the ordering information present in the output space. The classes involved in the dataset are: (1) failure of the graft within the first 15 days after transplantation, (2) failure between 15 days and 3 months, (3) failure between 3 months and 1 year, and (4) no failure presented. These intervals have been highlighted by experts as being the most pertinent in early graft loss. Several issues need to be taken into account in order to exploit the presence of this order structure. First of all, the learner (classifier, in this case) could benefit from this implicit ordering in order to construct more robust and fairer decision regions for the data, since the classification errors to be minimized vary from the ones considered in the nominal classification paradigm (the zero-one loss function). Secondly, with the final aim of evaluating the performance of those classifiers, different measures or metrics could be developed and used.In order to clarify the differences among these paradigms, the problem of classifying tumour cells given the labels: {normalcell, dysplasticcell, tumorcell, metastaticcell} could be considered. Clearly, an order among the categories can be appreciated, and there are also some misclassification errors that should be more penalized. For example, misclassifying a metastasiscell with a normalcell should be far more penalized than misclassifying it as a tumorcell. Since this is a common learning issue, several approaches to tackle this paradigm (known as ordinal regression or ordinal classification) have been proposed in the domain of pattern recognition and machine learning over the years, ever since the first work applying logistic regression dating back to 1980 [13]. This issue has generally been addressed by transforming ordinal scales into numeric values and solving the problem as one of standard regression or multinomial classification. However, there are several problems within this approach: on the one hand, the fact that, without a priori knowledge, the distance between different classes is unknown, thus the assumption of equidistant labels when performing standard regression may not hold; on the other hand, as nominal classification does not consider this order information, misclassification errors are treated equally. Nonetheless, other works have approached the paradigm considering the order information by means of threshold methods [14–16] which are based on the idea that some underlying real-valued outcomes exist (also called latent variable), although they are unobservable.However, there is still a major group of classification techniques specially designed for approaching ordinal regression which are based on the idea of decomposing the original problem into a set of binary classification tasks [17,18], or by formulating the original problem as one of extended binary classification [19,20]. Each subproblem can be solved in this case either by a single model or by a multiple set. The subproblems are defined in this case by a very natural methodology, considering whether a pattern x belongs to a class greater than a fixed k[21], and finally combining the binary predictions into a unique ordinal label. The idea of decomposing the target variable in simpler classification tasks has demonstrated to be very powerful in the context of ordinal regression.In this paper, due to the complexity of the classification problem, which presents an ordinal and highly unbalanced nature leading to difficulties in the classification of the three minority categories (note that from class 1 to 4 the number of patterns per class are respectively {76, 76, 62, 1223}), a binary decomposition method for ordinal regression known as OneVsPrevious is considered by using the well-known Support Vector Machine classifier (SVM) and two different approaches to combine the classifier outputs. Note that, although other base methodologies could be used for the decomposition method, such as artificial neural networks, the SVM paradigm has been chosen in order to provide a fair comparison in the experiments performed since most of the methods proposed for ordinal classification are based on SVMs [16,19,18]. Therefore, a SVM model is created for each subproblem by solving a global optimization problem seeking for the optimal separating hyperplane for the data and optimizing the parameters using a nested cross-validation over the parameters space. The methodology, which shows competitive results when compared to other ordinal and nominal approaches (based on SVM [16,19], artificial neural networks [29] and logistic regression [15]), is then used to develop a complete system of liver allocation, in conjunction with the MELD system known worldwide.The paper is organized as follows: Section 2 shows a description of the ordinal regression methodology used in this work; Section 3 thoroughly explains the constructed dataset and the experiments to be performed; Section 4 presents and analyses the results of the above-mentioned experiments. In Section 5, a simulation of the proposal is performed and, finally, Section 6 outlines some conclusions and future work.

@&#CONCLUSIONS@&#
Ordinal regression analysis and the Support Vector Machine paradigm have been used as machine learning techniques for predicting graft survival after liver transplantation taking into account donors and recipients characteristics and other operative factors concerning the transplant, through the construction of a dataset compound of donor–recipient pairs from Spanish and UK hospitals. More specifically, the classification model has been designed to deal with imbalanced and ordinal data to provide a fairer decision maker when allocating an organ to a recipient and the evaluation of the different classifiers has been accomplished by considering a set of four metrics designed for imbalanced and ordinal classification problems to avoid trivial solutions focused on improving overall error. The best model obtained from the whole set of methodologies tested was used in conjunction with the MELD score, which is the cornerstone of the current allocation policy globally. The experiments show that, although it is a really complex problem which may need more information in order to perform perfectly, the proposal is able to generalize well on unseen data, helps to avoid draws caused by the MELD score and does seem to work well in more realistic situations. The final rule-based system, which as said uses the MELD score and the best performing machine learning model, will consider the allocation of the organ to one of the first recipients in waiting list (these patients being ranked using the MELD score to estimate the patients severity) and the decision is made selecting the patient that presents a higher survival probability (note that we considered 4 different levels of time survival after transplantation).As future work, a sensitivity analysis of the best model can be developed in order to determine the most important variables for the end-point variable. Moreover, this study can be extended by considering other liver transplantation centres in the European Union to unify the procedure and create a more generic supranational organ allocation system. Finally, a different source of information concerning the transplant could be also used (post-transplant information) by reformulating the proposed algorithm to use the so-called privileged sources of information (note that this information can not be used directly in the model because it will not be available when deciding the matching).