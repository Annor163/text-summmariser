@&#MAIN-TITLE@&#
Accelerating wrapper-based feature selection with K-nearest-neighbor

@&#HIGHLIGHTS@&#
We propose to accelerate wrapper-based feature selection with a KNN classifier.We construct a classifier distance matrix to evaluate the quality of a feature.The proposed approach can apply to three types of wrapper-based feature selectors.Theoretical time complexity analysis proves the efficiency of the proposed approach.Experimental results demonstrate its effectiveness and efficiency.

@&#KEYPHRASES@&#
Gene selection,Microarray data,Wrapper,Filter,k-nearest-neighbor,

@&#ABSTRACT@&#
Wrapper-based feature subset selection (FSS) methods tend to obtain better classification accuracy than filter methods but are considerably more time-consuming, particularly for applications that have thousands of features, such as microarray data analysis. Accelerating this process without degrading its high accuracy would be of great value for gene expression analysis. In this study, we explored how to reduce the time complexity of wrapper-based FSS with an embedded K-Nearest-Neighbor (KNN) classifier. Instead of considering KNN as a black box, we proposed to construct a classifier distance matrix and incrementally update the matrix to accelerate the calculation of the relevance criteria in evaluating the quality of the candidate features. Extensive experiments on eight publicly available microarray datasets were first conducted to demonstrate the effectiveness of the wrapper methods with KNN for selecting informative features. To demonstrate the performance gain in terms of time cost reduction, we then conducted experiments on the eight microarray datasets with the embedded KNN classifiers and analyzed the theoretical time/space complexity. Both the experimental results and theoretical analysis demonstrated that the proposed approach markedly accelerates the wrapper-based feature selection process without degrading the high classification accuracy, and the space complexity analysis indicated that the additional space overhead is affordable in practice.

@&#INTRODUCTION@&#
The popularization and use of microarray technology in biomedical research and medicine facilitates the high-throughput measurement of many gene expression profiles simultaneously and enables their meaningful application in the diagnosis of cancers and tumor subtypes, the discovery of drug targets and the design of potentially effective drugs at the molecular level [1]. However, the intrinsic nature of microarray data with high dimensionality (as many as thousands of genes) and small sample sizes (as low as tens of samples) limits their powerful potential in practical use. In microarray data classification, the “curse of dimensionality” problem can lead to over-fitting, which can degrade the generalization ability of constructed classifiers in predicting unseen samples [2,3]. In addition, the feature space that is involved can have irrelevant and redundant features and often generates a classifier that has poor performance and weak robustness [4]. The available experimental evidence demonstrates that redundant features deteriorate the performance of the Naïve Bayes classifier and that instance-based learners are sensitive to irrelevant features [5]. One method to alleviate these problems is to remove irrelevant and redundant features from the original feature space using effective feature selection methods [6,7].Feature subset selection (FSS) for microarray data, which is also known as gene selection, is defined as the process of removing irrelevant and redundant features and the identification of a feature subset that contains the most discriminative information from the original feature space [8]. In addition to reducing the dimensionality of the original feature space, feature selection offers a multitude of advantages that are accompanied by a reduced number of features, such as enhancing the generalization ability of the classifiers, reducing the training time, improving the performance of the classifiers, facilitating data visualization and helping biologists identify the underlying biological mechanisms [9,10].Based on the framework that has been proposed by Dash and Liu [11], feature selection mainly consists of two components: a subset generation module and an evaluator module. The feature subset generation module exploits search strategies to generate candidate subsets, whereas the evaluator module measures the goodness of a subset. Depending on whether the evaluator is involved in the classifier, feature selection methods are typically classified into three groups: filter, wrapper and embedded [12]. Filter methods have lower computational complexity and better generalization ability. Because filter methods evaluate the quality of a feature or a subset of features by using only the intrinsic properties of the training samples, they are flexible in combination with a variety of classifiers. In contrast to filter methods, wrapper methods are specific to a given classifier and evaluate the quality of a candidate subset, and these methods tend to obtain better classification performance than the filter methods [13,14]. Embedded methods are special cases of wrapper methods that are characterized by a deeper interaction between the feature selection and the construction of the classifier. Feature subsets are generated when embedded methods are used to construct the classifier. Typical embedded methods include decision tree C4.5 [15] and SVM-RFE algorithms [8].Although wrapper methods achieve better classification accuracy, a main disadvantage is that they are far more time-consuming in actual use. For an experiment dataset with N features, wrapper methods must evaluate O(N2) candidate subsets when using the sequential forward selection scheme, and even incremental wrapper methods evaluate a sub-quadratic number of candidate subsets [16,17]. Such high time complexity would require a large amount of CPU time in the case of microarray data, which has thousands of genes [18]. To alleviate this problem and accelerate the process of feature selection, in this study, we investigated the wrapper and incremental wrapper methods with the K-Nearest-Neighbor (KNN) classifier embedded. Rather than considering the KNN classifier as a black box when evaluating the quality of a candidate feature, we constructed and maintained a classifier distance matrix to speed up the feature subset evaluation process and incrementally updated the matrix after adding a candidate feature into the selected feature subset. Because incrementally calculating the distance between any two instances projected over the selected features avoids a large amount of overlapping distance calculations, we expect a large reduction in the time cost. This work is a significant extension of our earlier paper by Wang et al. [19]. In particular, the main contributions of this paper are as follows: (1) we propose to accelerate wrapper-based feature selection by constructing a classifier distance matrix to store the distance between instances projected over the selected features. This helps us incrementally update the matrix when a new feature is selected, avoid constructing a new classifier from scratch, and greatly reduce massively repetitive calculations; (2) the proposed approach can apply to three types of feature selection methods, including wrapper methods with sequential forward/backward selection, incremental wrapper feature selection methods and incremental wrapper feature selection with replacement methods; (3) we test the effectiveness of wrapper-based feature selection method with KNN classifiers on eight benchmark microarray datasets, and compare it with the state-of-the-art feature selectors; (4) we analyze the theoretical time complexity of the proposed approach and experimentally validate its efficiency; (5) we finally analyze the space complexity of the proposed approach.The remainder of this paper is organized as follows. Section 2 briefly illustrates the classical KNN algorithm. In Section 3, we detail the procedure of wrapper-based sequential forward selection and incremental wrapper-based feature subset selection methods with the KNN classifier and describe an experiment to demonstrate their effectiveness for feature selection in comparison with that of the well-performing feature selector fast correlation-based filter (FCBF). Section 4 describes the improved wrapper method and incremental wrapper method in detail. In Section 5, we first experimentally compare the actual time cost on eight publicly available microarray datasets before and after acceleration, and this experiment is followed by theoretical time and space complexity analysis. The last section concludes the paper with a brief summary and discussion.In pattern recognition, K-Nearest-Neighbor (KNN) is a non-parametric learning algorithm that is used for classification and regression [19–21]. Because it is a typical type of instance-based or memory-based learning scheme, all of the computation of KNN is deferred until classification, and no explicit training step is required for constructing a KNN classifier. Therefore, KNN is a very simple but efficient algorithm that exhibits a time complexity of O(1) when training a KNN classifier and of O(mn+mlog2m) when classifying a new instance over a training set with m instances and n attributes, where O(mn) is the time complexity for calculating the distances between the new instance and each of the training instances. In addition, O(mlog2m) is the time complexity for sorting the distances when finding the k-nearest neighbors of the new instance [22].In KNN, various distance metrics are used to measure the distance between two instances according to the type of attribute. Given two instances x=(x1,…,xn) andx′=(x1′,…,xn′)from experimental samples, the distanced(xj;xj′)between two instances projected on an attribute xj(1⩽j⩽n) is calculated as follows. For a categorical variable,d(xj;xj′)=0ifxj′==xj, andd(xj;xj′)=1in other cases; for numerical attributes, the Euclidean and Manhattan distances are among the most commonly used metrics:d(xj;xj′)=(xj-xj′)2for the Euclidean distance andd(xj;xj′)=|xj-xj′|for the Manhattan distance. In terms of the Euclidean distance metric, the distance D(x;x′) between x and x′ can be recursively defined as(1)D(x;x′)2=D(x1,…,xn-1;x1′,…,xn-1′)2+d(xn;xn′)2.In the classification, to predict the class label of a new instance, KNN first finds its k closest neighbors from the training set according to the distance metric and then assigns the dominant label among the k neighbors to the new instance. If k=1, the label of a new instance is determined by its closest neighbor. Due to its implementation simplicity and classification effectiveness, KNN is commonly used as a standard classifier to evaluate and compare the performance of different feature selection algorithms [23–25] and is integrated into the feature selection framework to evaluate the quality of a candidate feature subset [26–29].Because the wrapper method integrates a classifier into the feature selection process to evaluate the quality of a feature or a subset of features, it tends to obtain a classifier with high classification accuracy. Obviously, enumerating all of the possible combinations of feature subsets and evaluating them one by one is the simplest approach and guarantees obtaining the globally optimal feature subset, while the computational complexity grows exponentially at O(2N) with N number of features. This approach is often unacceptable because it exhibits high time complexity in an actual application, particularly in the case of gene expression profiles, which have thousands of genes. To accelerate this process, researchers have proposed various feature subset search strategies to generate candidate feature subsets. Commonly used search methods include sequential forward selection (SFS), sequential backward selection (SBS), bidirectional search, sequential floating search, heuristic search, and random search [30]. Among these search strategies, SFS achieves a better tradeoff between the computational complexity and the quality of the obtained feature subset. Starting from an empty set, SFS first selects the feature that is most relevant to the target variable, as evaluated by a classifier and then searches for the next candidate feature that most contributes to the enhancement of the classification accuracy among the remaining features and continues with this process until there is no improvement in accuracy or there is no candidate feature left. Through adopting such a deterministic search strategy, the wrapper method evaluates only O((S+1)N) candidate feature subsets if S features are finally selected and O(N2) feature subsets in the worst case. Algorithm 1 presents a pseudo-code of the wrapper method with SFS.With the aim of further reducing the time cost and obtaining a final feature subset within linear time complexity, a hybrid feature selection method using a combination of filter and wrapper methods, which was denoted incremental wrapper subset selection (IWSS), has been proposed [31]. In contrast to the wrapper method with SFS for selecting the feature that most contributes to the enhancement of the classification accuracy within each run, IWSS first employs a filter method to obtain a sequence of ranked features according to their relevance to the target variable; starting from the first feature, IWSS then incrementally adds features from the sequence of ranked features to the selected subset in a wrapper manner. By integrating the filter and wrapper methods, IWSS not only achieves satisfactory results but also significantly reduces the time complexity to O(N) instead of O(N2) in SFS [31].Algorithm 1Wrapper-based Sequential Forward Selection (SFS)In the incremental wrapper subset selection without and with a replacement methods, i.e., IWSS and IWSSr, respectively, the goodness of a candidate feature is assessed by the function evaluate(classifier,Data↓Snew∪{C}), which trains and validates the classifier using a fivefold cross-validation over the dataset Data projected over Snew∪{C} (C is the target class) [31]. Rather than use the average accuracy of the fivefold cross-validation and conduct a t-test over the fivefold cross-validation results proposed previously [31], we adopted the following criteria [33]: (1) a fivefold cross-validation was employed to decide whether a new feature is added to the selected feature subset S and (2) the new feature f is included only if the average accuracy of the fivefold cross-validation overData↓S∪f∪{C}is better than that of the fivefold cross-validation overData↓S∪{C}and at least MinFoldersBetter (mf) of the five-folds works well. MinFoldersBetter (mf) is actually a counter for recording how many times the five classification accuracies obtained from the fivefold cross-validation overData↓S∪f∪{C}is better than the average accuracy of the fivefold cross-validation overData↓S∪{C}. This approach avoids the criticism of using a statistical test with a small sample size. For better control of noise and over-fitting in the feature selection, the recommended empirical values for mf are 2 or 3 [33]. For the wrapper-based SFS method, the criterion is that the new feature f is included only if the average accuracy of fivefold cross-validation overData↓S∪f∪{C}is better than that of fivefold cross-validation overData↓S∪{C}. In Algorithm 2, the returned items of the function evaluate(classifier,Data↓Snew∪{C}) include the average classification accuracy accnewof the fivefold cross-validation overData↓Snew∪{C}, and the number num indicates how many times the five classification accuracies obtained from the fivefold cross-validation are better than the previous average classification accuracy overData↓S∪{C}.In this section, we evaluate the quality of the selected feature subset obtained by the above-mentioned methods by comparing the classification accuracy over eight publicly available microarray datasets with high dimensionality and a small sample size, as shown in Table 1; the last column #SFR gives the ratio between the number of samples and the number of features.Algorithm 2Incremental Wrapper Subset Selection with Replacement (IWSSr)Central Nervous System (CNS) data: The task is to predict the patient outcomes for central nervous system embryonal tumors. This dataset contains 60 patient samples with 7129 genes in each sample, and of these samples, 21 are survivors, and 39 are failures [35].Prostate data: This dataset is composed of 50 non-tumor prostate samples and 52 prostate tumors with 12,600 genes [36]. The task is to identify the expression patterns that correlate with the distinction of prostate tumors from normal samples.Leukemia1 data: A collection of leukemia patient samples from the bone marrow and peripheral blood is used for distinguishing between acute myeloid leukemia (AML) and acute lymphoma leukemia (ALL) tissues. This dataset contains 72 samples with 7,129 genes: 25 samples of AML and 47 ALL tissues [1]. The classification task is to distinguish these two types of leukemia according to the gene expression profiles.Leukemia2 data: A collection of leukemia patient samples from bone marrow and peripheral blood for is used for distinguishing between acute myeloid leukemia (AML) and acute lymphoma leukemia (ALL) tissues. The data for the ALL tissues are further divided in terms of B cells and T cells. Leukemia2 consists of 72 samples with 5327 genes, and of these samples, 38 are of AML, nine are of ALL-B, and 25 are of ALL-T [1]. The task is to build a classification model to distinguish the three subtypes of leukemia.Diffuse Large-B-Cell Lymphoma (DLBCL) data: Diffuse large B-cell lymphomas (DLBCL) and follicular lymphomas (FL) are two B-cell lineage malignancies. There are 7129 genes with 58 DLBCL samples and 19 FL samples in the DLBCL data. The goal is to build a classification model to discriminate DLBCL from FL [38].Ovarian data: The goal of this experiment is to distinguish ovarian cancer from non-cancer using proteomic spectra data. Ovarian consists of 253 samples, including 162 ovarian cancers and 91 controls, for all 15,154 identities [39].Small Round Blue Cell Tumor (SRBCT) data: There are four different types of childhood tumors: Ewing’s family of tumors (EWS), neuroblastoma (NB), non-Hodgkin lymphoma Burkitt’s lymphoma (BL) and rhabdomyosarcoma (RMS). SRBCT consists of 83 samples with 2308 genes: 29 samples of EWS, 18 samples of NB, 11 samples of BL and 25 samples of RMS. The classification goal is to distinguish these four subtypes of tumors based on the gene expression profiles [37].For the purpose of this study, KNN was integrated into the wrapper procedure to evaluate the quality of a candidate feature subset and was also chosen as the classifier to evaluate the final obtained feature subset. In our study, we used ReliefF, which is a distance-based filter measure that has great power in selecting discriminative features and good stability toward the perturbation of the training set [5,40], to generate a ranked feature set from the original feature space. For each method, a 10-fold cross-validation was conducted, and in this process, one fold was used as the test set to evaluate the final selected feature subset while the remaining nine folds were used as the training set [41]. The training set was directed to the IWSS and IWSSr methods for feature selection using the relevance criteria presented in the above section. Specifically, feature selection was conducted on the training set to ensure an unbiased feature selection protocol [42], and the classifier was trained on the training set projected over the selected feature subset and evaluated on the test data projected over the selected features. To demonstrate the effectiveness of the KNN algorithm in wrapper-based feature selection, the commonly used 1-Nearest-Neighbor (1NN) and 3-Nearest-Neighbor (3NN) classifiers were employed to evaluate both the quality of the candidate subsets and the quality of the final selected feature subset. For the 1NN and 3NN classifiers, we used the Euclidean distance metric to calculate the distance between any two instances. In addition, the fast correlation based filter (FCBF) algorithm, a well-performing state-of-the-art feature subset selector [43,44], was used as a comparison with the proposed methods. To measure the quality of the feature subset selected by FCBF, 1NN and 3NN were also used as classifiers.Tables 2 and 3present the experimental results for the 1NN and 3NN classifiers, respectively, in terms of the average classification accuracy and the number of selected genes for FCBF, SFS, and IWSS with the IWSSr methods with mf={2, 3} as the superscript. For a comparison, the last two columns present the average accuracy over the original feature space and the number of features of each dataset. The best accuracy achieved by the four methods on each experimental dataset is shown in bold, and the last row “AVE.” presents the average accuracy and number of selected genes.As shown in Table 2, for the IWSS and IWSSr methods, the accuracy with all of the experimental datasets is improved markedly with a large reduction in the feature dimensionality; specifically, the accuracy reaches more than 95% for the SRBCT, Leukemia2 and DLBCL datasets with approximately 10.0 features selected, whereas the IWSS method even reaches 100% accuracy with an average of 9.4 features on the Ovarian dataset, and its average accuracy increased by 5.1% compared with that obtained with the same approach without feature selection. Although the SFS method does not achieve an accuracy as impressive as that obtained with IWSS and IWSSr, it greatly reduces the feature dimensions, potentially enhances the generalization ability of the KNN classifier, and reduces the time cost associated with constructing the classifier. Compared with FCBF, the IWSS and IWSSr methods achieved better accuracy on the majority of the experimental datasets and always obtained feature subsets with a smaller size. Specifically, IWSS2 obtained an 87.8% average accuracy with 10.0 features selected, and IWSS3 obtained an 88.5% average accuracy with 9.9 features selected compared with the 87.2% average accuracy and 44.8 features of the FCBF. Additionally, IWSSr2 obtained an 88.2% average accuracy with 5.6 features selected, and IWSSr3 obtained an 87.3% average accuracy with 4.9 features selected. The SFS method obtained feature subsets with a markedly smaller size, but its classification accuracy was not satisfactory and was worse than that of FCBF in our experiments.Similarly, as shown in Table 3, the IWSS and IWSSr methods improved the classification accuracy with approximately 10 features finally selected, which constituted a large reduction in the feature dimensions on all of the experimental datasets, and the average accuracy increased by 5.5% compared with that without feature selection. Compared with FCBF, the IWSS and IWSSr methods achieved better accuracy and obtained feature subsets with a smaller size on all of the experimental datasets. For example, IWSS2 achieved a 90.6% average accuracy with 10.0 features selected, and IWSSr2 achieved a 90.6% average accuracy with 5.6 features selected, in comparison to the 88.7% average accuracy with the 44.8 features of FCBF.Tables 2 and 3 show that these methods achieved improved classification accuracy with a significant reduction in the feature dimensionality and that the IWSS and IWSSr methods outperform the well-performing state-of-the-art feature selection algorithm FCBF in terms of classification accuracy and the size of the final selected feature subset, which demonstrates the effectiveness and superiority of the proposed KNN-wrapper-based feature subset selection method. However, their running time is not trivial. For example, in our study, IWSS2 had a cost of 367.9s (approximately 6min) on the Prostate dataset for the 1NN case and 441.3s (approximately 7.4min) for the 3NN case. Thus, in the next section, we explore the implementation details of these methods, particularly the classifier that is used inside, that are required to reduce the time complexity without degrading the quality of the selected feature subset.In the evaluation of the quality of a candidate feature subset, we previously considered the KNN to be a black box and disregarded the inside implementation details. In this approach, we construct a new KNN classifier from scratch each time when evaluating a new candidate feature subset. In the wrapper-based SFS and incremental wrapper feature selection methods, the final feature subsets are obtained incrementally by searching and evaluating the candidate features one by one. In contrast, there are classifiers that can be constructed incrementally along with the sequence of the selected features rather than constructed from the beginning, and the KNN algorithm is a typical case that can be constructed incrementally when a new feature is included in the selected feature subset, as discussed in the second section.Because there is no explicit training step for constructing a KNN classifier, all of the computation of the KNN is deferred until the classification, and the actual classification is conducted by comparing the distance between the test instance and all of the training instances and then choosing the k nearest neighbors to determine the class label of the test instance. Therefore, we could construct a distance matrix to maintain the distance between any two different instances in the experimental dataset projected over the selected feature subset. When evaluating a candidate feature, we can incrementally construct a new KNN classifier by adding the distance matrix on the candidate feature to the distance matrix over the selected features rather than calculate it over all of the features. To clarify this procedure, we will now introduce some notation and two definitions before illustrating the improved wrapper and incremental wrapper feature selection methods.We first introduce the following notation, which is used in the subsequent sections: Data is the experimental data with m instances, n features and one target variable C; F={F1, F2,…,Fn} is the original feature space of Data; R={R1, R2,…,Rn} is a ranked feature set of F that is obtained using a filter method; S={S1, S2,…,Ss} (1⩽s⩽n) is the current selected feature subset; and Fi, Ri, and Siare the ith features in F, R and S, respectively.Definition 1attribute distance matrixGiven a predictive attribute Fi, the attribute distance matrix of Ficonsists of the distance between any two different instances in the experimental dataset projected over feature Fi. This matrix is noted as D(Fi).Given the selected feature subset S, the element of a classifier distance matrix is the distance between any two different instances in the experimental dataset projected over the feature subset S. This element is noted as D.To provide an intuitive impression of the attribute distance matrix and the classifier distance matrix, we present their logical storage structure in Fig. 1. Each cell of the matrix stores the distance between any two instances, and each row or each column is a distance vector between an instance and the other instances. In actuality, because of the symmetry of the distance between two instances, we need to store only the upper or lower semi-triangular matrix to save on the physical storage space cost. For a numerical variable (as is the case of the gene expression variable), KNN with the Euclidean distance metric is often used. In our study, to incrementally update the distance between two instances, the squared Euclidean distance is stored in this matrix. Such an approach ensures that we can incrementally construct the KNN classifier along with the selection of features. When using this matrix to find the k closest instances of a test instance, we can use the square root (by taking the square root of each value in the matrix), or we can directly use the value in the matrix because distance is a non-negative and monotonically increasing metric along with the selection of features.The classifier distance matrix D not only acts as a fast KNN classifier to conduct the cross-validation for the experimental dataset projected over the selected features but also works together with the attribute distance matrix for the incremental construction of a new classifier when evaluating the next candidate feature. Based on the discussion above, we present the improved wrapper-based SFS method and the incremental wrapper method with the KNN classifier embedded.Compared with the classical wrapper-based SFS algorithm (refer to Algorithm 1), we evaluated the quality of a feature subset by performing a fivefold cross-validation on the classifier distance matrix rather than first calculating the distance between the test instance and all of the training instances and then conducting a fivefold cross-validation. When considering a candidate feature Fi, we first calculated the attribute distance matrix D(Fi), and we then obtained a candidate classifier distance matrix Dnewby adding D(Fi) to D and then performed a fivefold cross-validation on Dnewto evaluate the quality of the feature subset S∪Fi. For each run, we added the feature Fithat achieves the best accuracy to the selected subset S by replacing D with the corresponding Dnew, and we then continued by selecting the next feature. The stop criterion was that all of the features had been selected into S or that there was no increase in the classification accuracy when evaluating the remaining features. Algorithm 3 presents the details of the improved KNN-embedded wrapper-based SFS method.Algorithm 3KNN-embedded wrapper-based SFSFor the incremental wrapper methods, the candidate features are evaluated by a classifier that runs over the ranked feature set R. At the start of the algorithm, the first feature R1 is included into S, and the classifier distance matrix D is calculated on S. When evaluating a candidate feature Ri, KNN-embedded IWSS first calculates the attribute distance matrix D(Ri) and generates a candidate classifier distance matrix Dnewby adding D and D(Ri). If the result of a fivefold cross-validation on Dnewsatisfies the relevance criteria illustrated in the Relevance Criteria section, the candidate feature Riis included in S, and D is replaced by Dnew; otherwise, Riis disregarded, and D is maintained unchanged. The above procedure is repeated until no candidate feature is left in R.With the exception of the replacement operation in IWSSr, the procedure of IWSSr is not different from that of IWSS. In terms of the replacement operation in IWSSr, for a candidate feature Ri, we must evaluate the quality of the candidate feature subset {S1, S2,…,Sj−1, Sj+1,…,Ss, Ri}, which is obtained by swapping Sjand Riin the candidate classifier distance matrix Dnew. The result can be calculated with the following formula:(2)Dnew=D-D(Sj)+D(Ri).For the replacement operation, IWSSr evaluates the candidate subset that is obtained by swapping Sjand Riand records the swapping operation that achieves the best improved performance. In each run, IWSSr updates the selected feature subset with the addition or replacement operation or keeps it unchanged. Because the attribute distance matrix D(Sj) (Sj∈S) has been computed and stored in memory earlier when Sjwas evaluated, the proposed approach is expected to greatly accelerate the process. Algorithm 4 presents the pseudo-code of the KNN-embedded IWSSr method.Algorithm 4KNN-embedded IWSSr

@&#CONCLUSIONS@&#
In this study, we proposed an approach for accelerating wrapper-based feature subset selection methods with an embedded KNN classifier. The time cost in evaluating the quality of a candidate feature arises primarily from the inner fivefold cross-validation when using the KNN classifier as a black box. Considering this, we proposed the construction and dynamic maintenance of a classifier distance matrix (which consists of the distance between instances projected over the selected feature subset) rather than recalculation of the distance starting from scratch each time when a new feature is considered. This approach thus can greatly speed up the evaluation process and reduce the actual running time cost by avoiding massively repetitive calculations. Also, the proposed approach can apply to accelerating three types of feature selection methods, including wrapper methods with SFS, IWSS and IWSSr. Since the feature selection procedure and the criteria to include a candidate feature of the proposed approach are not different from the original approach, it is guaranteed that the proposed methods achieve the same feature subset as the original ones. To show the effectiveness of wrapper-based SFS, IWSS and IWSSr methods in selecting informative features, experiments were first conducted on eight publicly available microarray datasets. In comparison with the well-performing state-of-the-art feature selection method FCBF, the wrapper method with KNN outperforms FCBF in terms of classification accuracy and the size of the finally selected features. To demonstrate the performance gain in terms of time cost reduction, we then analyzed the theoretical time complexity and conducted an experimental study on the eight publicly available microarray datasets to show the actual time cost for both the black box case and the KNN-embedded case. The theoretical analysis and experimental results demonstrated the efficiency of the proposed approach in terms of running time without degrading the accuracy. In addition, a space complexity analysis showed that the additional space overhead is clearly affordable in practice when handling gene expression profiles.Notably, in our study, the squared Euclidean distance rather than the Euclidean distance is stored in the classifier distance matrix to save on the computational cost. Because distance is a non-negative metric, the squared Euclidean distance and Euclidean distance are equal for measuring the relative distance between the test instance and the training instances, which guarantees obtaining the same feature subset. If the Euclidean distance is stored in the classifier distance matrix, we would need to first square the distance and then add it to the attribute distance matrix to obtain a candidate classifier distance matrix for a distance comparison. Each time after selecting a new feature, we first calculate the square root of the distance and then store it in the classifier distance matrix. Obviously, the latter performs additional calculations and is more time-consuming compared with the former.Furthermore, compared with the case of considering KNN as a black box, the space complexity of our proposed method is O(m2n+m2) for SFS, O(mn+m2) for IWSS and O(m2n+m2) for IWSSr. Typically, if we maintain only a temporary attribute distance matrix rather than keeping the attribute distance matrices for all of the features, the space complexities of SFS and IWSSr are equal to that of IWSS, O(mn+m2). In handling gene expression profiles with high dimensionality (thousands of genes) and small sample sizes (as low as tens of samples), the space complexity of the proposed method is approximately equal to that of the black box case, i.e., O(mn+m), which indicates that the additional space cost of the proposed method is quite small and can be easily met by today’s computers for gene expression profile analysis. In handling data with ultra-large dimensionality and samples, the classifier distance matrix may not fit into the memory. Then, we can turn to the distributed computing paradigm, such as the MapReduce Framework, to divide the distance matrix into several small parts by row or column and store them on distributed hosts [45]. We would then use Map operations to calculate the attribute distance matrix and the candidate classifier distance matrix and use Reduce operations to decide whether to select a candidate feature and update these matrices in parallel [46]. In our future research, we plan to study other search strategies, such as sequential backward selection and sequential floating selection, as well as to explore other learning algorithms that have similar properties.