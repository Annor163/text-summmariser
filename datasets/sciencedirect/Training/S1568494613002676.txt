@&#MAIN-TITLE@&#
Linguistic description about circular structures of the Mars’ surface

@&#HIGHLIGHTS@&#
A methodology for linguistic description of the Mars’ surface is proposed.We contribute to the field of linguistic description of complex phenomena.Our approach allows generating a great variety of customizable linguistic reports.The experiments demonstrated the great expressiveness of the presented resources.

@&#KEYPHRASES@&#
Linguistic description of data,Computing with perceptions,Image description,Granular linguistic model of phenomena,

@&#ABSTRACT@&#
Satellites situated in the orbit of Mars have provided and continue providing thousand of images of the planet surface. Nevertheless the number of expert geologists analyzing these images is limited. Typically, these experts provide linguistic descriptions of their observations remarking the relevant features in the image and ignoring the irrelevant details for a given goal. In this paper, we apply our research in the field of Computational Theory of Perceptions to the challenge of developing computational systems able to generate linguistic reports comparable with the ones provided by human experts. We present a description of our contribution to solve this problem including last results of our research in this field. For example, we explore how to represent the multidimensional domain of computational perception values. We develop up the use of the relevance as an attribute of perceptions that allows us to generate reports that are automatically suited according to the user goals. We provide an application example as a demonstration of concept.

@&#INTRODUCTION@&#
Current technologies allow us to acquire and store big amounts of data about complex phenomena in many areas of science and technology. In order to be useful, these data must be interpreted and represented in an understandable way, by giving in each type of situation their relationship to data context and, in general, the information related to each specific phenomenon. Currently, these types of descriptions are reports that contain text and graphics produced by human experts. Nevertheless, the relation between the amount of data to analyze and the number of experts available is growing dramatically. This situation causes a strong demand for computational systems that can interpret and describe linguistically the large amount of information that is being generated in many areas.This paper was motivated by the existence of a huge database of thousands of images of the Mars’ surface produced by satellites and it is part of a collaboration project with the Spanish National Institute for Aerospace Technology (INTA). These images, which are usually analyzed by a small number of available experts on Martian geology, contain important information. For example, one of the most important research topics about the Mars planet is the seek of water, which can be developed by the analysis of its surface. In [1], authors use infrared to visible wavelength images of the Mars’ surface to obtain relevant geological information that may help in the location of water frost. In [2], authors analyze the measurements of elevations in order to extract a highly accurate global map of the topography of Mars that determines an upper limit of the present surface water inventory.In this work, we will use digital image processing techniques [3,4], in order to obtain automatically information about the presence of circular structures of the Mars’ surface and produce a linguistic description of it. There are several examples related to this work. In [5], authors use 3D image processing to automate the surface quality analysis of exterior car body panels. From the viewpoint of describing images in natural language (NL), there are recent works based on the use of Fuzzy Logic (FL) such as [6], where authors propose a hierarchical fuzzy segmentation of the image and a collection of linguistic features able to describe each region. In [7], authors explain how a fuzzy object-relational database management system can be employed to implement and integrate the different elements needed for the linguistic description of images, namely, ontology, concept representation and language generation.Our research is based on the Computational Theory of Perceptions (CTP) introduced by Zadeh [8–10]. In previous works in this line, we have developed computational systems able to generate linguistic descriptions of different types of phenomena. For example, we generated financial reports from data taken from the Spanish Securities Market Commission (CNMV) [11]. We have also worked in the field of activity recognition [12] and gait analysis [13], by providing automatically generated linguistic reports based on accelerometer data. And, we have also dealt with data related to the field of Intelligent Transportation Systems (ITS), where we generated assessing reports in truck driving simulators [14,15]. In [16], we generated linguistic reports about the traffic on roundabouts and also about traffic evolution in roads [17].In this paper, we face the challenge of creating human like reports that could help experts in Martian surface to analyze the huge database of available images.This paper is an extension of a previous work presented in [18]. Here, we present several contributions listed as follows:•An updated architecture of this type of computational applications, e.g., we introduce the multi-dimensionality of the domain of existence of a Computational Perception.We develop upon the concept of Relevance as a component of a Computational Perception that is used to choose the most suitable linguistic expressions to describe the current state of phenomena. We present a new version of Report Generator that is more flexible and more advanced than previous ones.We present an application that consists of creating linguistic reports about detected circular structures such as volcanoes or meteorite impacts. When these structures exist in the image, the report should provide information about their size and relative position according to certain criteria of the relevance of these circles for the final user.This paper is organized as follows: Section 2 describes the architecture of a computational system able to create linguistic descriptions of phenomena. Section 3 explains how to apply this architecture for describing the presence of circular structures such as volcanoes or meteorite impacts on the Mars’ surface. Section 4 shows the experimental results. And finally, Section 5 expounds some concluding remarks.A general approach including a general architecture for computational systems for linguistic description of data can be found in [19]. We face this challenge with a more specific architecture based on our research in the field of Computational Theory of Perceptions (CTP).CTP was introduced in Zadeh's seminal paper “From computing with numbers to computing with words – from manipulation of measurements to manipulation of perceptions” [8] and further developed in subsequent papers [9,10]. It grounds on the fact that human cognition is based on the role of perceptions, and the remarkable capability to granulate information in order to perform physical and mental tasks without any traditional measurements and computations.In Fig. 1, we identify two main stages in the text generation process, namely, off-line building process and on-line instantiation process.During the off-line building process, after analyzing the application domain and the user requirements, the designer collects a corpus of NL expressions that are typically used to describe the relevant features of the monitored phenomenon. The designer analyzes the particular meaning of each linguistic expression in specific situation types to build both, the Granular Linguistic Model of Phenomena (GLMP) and the Report Template.During the on-line instantiation process the computational system uses input data obtained by the Data Acquisition Module to generate suitable linguistic reports as result of two instantiation processes of these two generic data structures that are performed by the Validity Module and the Expression Module respectively.The Data acquisition (DAQ) module consists of sensors and measurement hardware-software resources. In our approach, the DAQ Module provides the data needed to feed up the instantiation process. It provides the interface with the application physical environment. This module could include either sensors or access to information stored in a database. In Section 3.1, we explain how the DAQ Module takes the information from satellite images and implements an image processing algorithm.The GLMP data structure is a useful paradigm for developing computational systems able to generate linguistic descriptions of data [13,17,18]. The main element of this structure is known as Computational Perception (CP), which is based on the concept of linguistic variable developed by Zadeh [20–22]. CPs are computational models of information units (granules) acquired by the designer about the phenomenon to be modeled, e.g., CPs correspond to perceptions of specific parts of the phenomenon at certain granularity degree.The GLMP is a network of Perception Mappings (PM), which are the elements used to create and aggregate or combine CPs. Each PM receives a set of input CPs and transmits upwards an output CP. We say that each output CP is explained by the PM using a set of input CPs. In this network, each CP covers specific aspects of the phenomenon with certain granularity degree. We call first order perception mappings (1PM) to those PMs which serve as input to the GLMP. We call first order computational perceptions (1CP) to the output of a 1PM. The PMs whose input are CPs are called 2PMs and their outputs are 2CPs. This classification is inspired on the definition of the three worlds by Popper, namely, world-1 of physical objects (phenomena), world-2 of the perceived objects (1CP) and world-3 of the mental objects built by using the objects in the world-2 (2CP) [23].Fig. 2shows an example of a GLMP. In this example, the phenomenon can be described at a very basic level in terms of three variables providing values z1, z2, and z3 respectively. In this GLMP, other two higher-level descriptions of the phenomenon are provided. These descriptions are given in the form of computational perceptions 2CP4 and 2CP5. The second order perception mappings (2PMs) 2PM4 and 2PM5 indicate that 2CP4 and 2CP5 can be explained in terms of 1CP1, 1CP2, and 1CP3. Finally, the top-order description of the phenomenon is provided, at the highest level of abstraction, by 2CP6, explained by 2PM6 in terms of 2CP4 and 2CP5. Notice that, by using this structure, one can provide not only a linguistic description of the phenomenon at a certain level, but an explanation in terms of linguistic expressions at a lower level. In the following subsections, we will see how, after being instantiated with a set of input data, the GLMP provides a structure of valid sentences that in medium size applications could include hundreds of sentences.A CP is a tuple with three components (A, W, R) described as follows:Ais a multidimensional matrix of linguistic expressions (words or sentences in NL) that represents the whole linguistic domain of CP, i.e., all its possible linguistic values. In the application context, each element of A describes the value of CP in each situation of the phenomenon with specific granularity degree. During the off-line stage, these values are defined by the designer extracting suitable sentences from the linguistic corpus of the application domain. For example, consider the sentences which describe a CP that were generated with the template:“The image contains {zero ∣ one ∣ two ∣ three ∣ four ∣ various ∣ many} {small ∣ medium ∣ big} circles” Here, because the linguistic template contains two variables, we will organize the domain of A as a bidimensional matrix:A=a11a12a13a21a22a23a31a32a33a41a42a43a51a52a53a61a62a63a71a72a73Note that the number of dimensions and size of A depends on each application domain. In this case, rows refer to number of circles (7 rows: zero, one, two, …, many) and columns to the size of circles (3 columns: small, medium, big).is a multidimensional matrix of validity degrees whose elements are in the interval [0, 1] assigned to each element of A in a specific context. The validity degree represents the precision of each sentence when describing the specific input data. During the on-line stage, these validity values are assigned dynamically reflecting the current state of the monitored phenomenon. For example, provided the NL expressions in A, the matrix of validity degrees will be also a bidimensional matrix:W=w11w12w13w21w22w23w31w32w33w41w42w43w51w52w53w61w62w63w71w72w73is a multidimensional matrix of relevancy degrees whose elements are also in the interval [0, 1] assigned to each element of A in a specific context. The value of relevance depends on the application. During the off-line stage, these values are assigned by the designer depending on the specific application requirements. The designer assigns values to R indicating the relative importance of the described situation types. During the report generation phase, the values of R will be used as complementary data to select the most suitable sentences to describe the current state of the phenomenon. For example, provided the NL expressions in A, the matrix of relevancy degrees will be also a bidimensional matrix:R=000.2000.6000.7000.8000.9001001Here, the designer has defined that, the bigger and numerous the circles, the more relevant is the linguistic expression; while the small and medium circles are not relevant at all.A PM is a tuple (U, y, g, T) where:Uis a set of input CPs. As explained before, we call first order perception mappings (1PMs) when U are valuesz∈ℝprovided either by sensors or obtained from a database.is the output CP, y=(Ay, Wy, Ry).is an aggregation function that calculates the vector of validity degrees Wy. It is a fuzzy aggregation of the validity degrees of the input CPs. In FL, many different types of aggregation functions can be developed. For example g, could be implemented using a set of fuzzy rules or, in the case of 1PMs, g is built using a set of membership functions as follows:Wy=w11w12…w1nw21w22…w2n⋮⋮⋮⋮wm1wm2…wmny=μa11(z)μa12(z)…μa1n(z)μa21(z)μa22(z)…μa2n(z)⋮⋮⋮⋮μam1(z)μam2(z)…μamny(z)where Wyis the matrix of validity degrees associated to the matrix of linguistic expressions Ay, andμaij(z)is the membership degree of the input data to each membership function.is a text generation algorithm that allows generating the sentences in Ay. For example, in simple cases, T is a linguistic template, e.g., “The circle is {small ∣ medium ∣ big}”.This processing module calculates the validity degrees of each CP during the on-line process by instantiating in the GLMP structure the most suitable linguistic expressions to represent the input data, i.e., this module provides as output a collection of linguistic clauses together with associated validity degrees.The Report Template is designed during the off-line process. Here, we develop upon the concept of Fuzzy Tree of Choices (FTC), which is a mechanism to represent part of the constraints imposed to the linguistic report. It consists of a directed graph including choices and the linguistic expressions to be linked together. This FTC has a generic version that will be instantiated for each input data (see Fig. 7).Provided a set of valid linguistic clauses, this processing module is in charge of combining this information to generate the most relevant linguistic report by choosing and connecting the adequate ones. This is done during the on-line process by instantiating the Report template.The Expression Module is used to analyze every combination of branches in the FTC until it finds out the sequence that accumulates the highest validity degree. More specifically, we use Eq. (1) to calculate the product of the validity degrees of all linguistic expressions belonging to each possible sequence.(1)wsequence=∏∀i∈sequencewiOnce we obtain the sequence with the highest validity degree, the relevance of each expression is used to define the order in which the information is going to be expressed in the final report. In the same way, it determines which information is not relevant to be included in such report. So, the FTC is pruned and reordered, resulting in an instantiated report depending on the analyzed image.In this section, we describe in detail a computational application for producing linguistic descriptions of the Mars’ surface.This module is in charge of recognizing circles in the image. The recognition of patterns is an open issue of research in the field of automatic image processing. Here, in order to extract information about the presence of circular structures, we have applied classical filtering techniques to remove the background, techniques of edge detection, and the generalized Hough transform which is particularly suitable for detecting the presence of circles [4,24].In the first task, known as pre-processing, the image is converted to grayscale and everything that is not interesting in the image is “deleted”. This is a procedure that slightly transforms the image to eliminate any noise, imperfections, shine, etc. Then, we detect the edges in the image that can be defined as transitions between two significantly different levels of gray intensity. This provides valuable information on the borders of objects that can be used for image segmentation.We can find some interesting edge detection and segmentation methods based on Soft Computing techniques. For example, an ant-inspired algorithm was used in [25] for the detection of image edge features while a simulated annealing was used in [26] to edge-based segmentation of grayscale images. There are also recent works that use fuzzy techniques for the same purpose such as [27], where authors used fuzzy c-means clustering with weighted image patch for image segmentation; in [28], a color image segmentation was practiced using fuzzy clustering techniques and competitive neural networks; and, finally, authors in [29], used an adaptive region based color texture segmentation using a fuzzified distance metric. Here, in order to solve the faced problems in this prototype, we have used a classical version of the Sobel operator [4] as boundary filter which consists of two arrays whose size is 5×5 pixels.The search for circles in the image is tackled by means of the Hough Transform [30], which was initially concerned with the identification of lines in the image, but later it has been extended to identify arbitrary shapes such as circles or ellipses. To get good results with this approach, the pre-processing stage is essential, since the procedure is mainly based on the color jump produced at the edge of either meteorite impacts or volcano craters. The great difficulty of this procedure lies in the analysis of those images in which, by its nature, the land has many irregularities. This fact accumulates inaccuracies in the analysis and, therefore, we must assume a margin of error in the obtained results. Moreover, since we do not know in advance the size and shapes of the relevant objects, we have to work with a generalized Hough transform that enables the detection of objects whose shapes and dimensions are, initially, unknown. As any other pattern recognition algorithm, the Hough transform introduces some uncertainty by committing several false positives and false negatives mistakes. We have taken advantage of this feature to emulate the behavior of a human observer that could make similar mistakes and therefore to generate linguistic descriptions containing such type of imprecision.For each image, the DAQ Module provides the number of detected circles in the image, together with their x and y centroid coordinates, and their corresponding radius (in pixels). As an example of the performance of this module, Fig. 3 shows the circles detected for a specific image.Fig. 4shows the designed GLMP. We have defined three 1CPs which describe the size and position (in the vertical and horizontal axis respectively) of each circle and five 2CPs which describe the quantity of circles and their position at different levels of detail. The 2PMs that appear in this GLMP can be classified in those ones that aggregate (Σ) the information from the same subordinate CP and the ones which combine (Π) information from different subordinate CPs. In the following subsections, we thoroughly describe each PM that appears in the GLMP.The elements of this 1PM can be described as follows:Uis the numerical value (z1∈ℝ) of the x coordinate of the centroid of a circle.is the output CP (1CPX), that has the following set of possible sentencesaXi, with i ∈ {1, 2, 3}: “A circle in the {left ∣ center ∣ right} part”, e.g.,aX1= “A circle in the left part”. Since this CP works only as input data and it is not used in the linguistic reports, the relevance valuesrXiare set to zero ∀i.is the function that calculates the validity degrees of the output CP. These values are obtained by means of uniformly distributed trapezoidal membership functions (MFs) forming a strong fuzzy partition (SFP) [31]. Here, the three linguistic labels associated to each NL expression are trapezoidal MFs distributed over one and two thirds of the image widthw. We have heuristically decided its vertexes as follows: {left→(−∞,−∞,7·w/24,9·w/24),center→(7·w/24,9·w/24,15·w/24,17·w/24),right→(15·w/24,17·w/24,∞,∞)} in such a way that these vertexes are placed in one and two thirds of the image width plus or minus an additional value corresponding tow/24. In Fig. 5, can be seen a graphical representation of these linguistic labels for a specific image.This 1PM is similar to 1PMXand can be described as follows:Uis the numerical value (z2∈ℝ) of the y coordinate of the centroid of a circle.is the output CP (1CPY), it has the following set of possible sentencesaYi, with i ∈ {1, 2, 3}: “A circle in the {top ∣ center ∣ bottom} part”. Since this CP also works only as input data and it is not used in the linguistic reports, the relevance valuesrYiare zero ∀i.is the output function. It consists of three trapezoidal MFs that were distributed over one and two thirds of the image height h similarly to the position of the circle in the x coordinate, resulting in the following vertexes: {top→(−∞, −∞, 7·h/24, 9·h/24), center→(7·h/24, 9·h/24, 15·h/24, 17·h/24), bottom→(15·h/24, 17·h/24, ∞, ∞)}. In Fig. 5, can be seen a graphical representation of these linguistic labels for a specific image.This 1PM can be described as follows:Uis the numerical value (z3∈ℝ) of the radius of the circle in pixels.is the output CP (1CPS), it has the following set of possible sentencesaSi, with i∈{1, 2, 3}: “A{small∣medium∣big}circle″. Since this CP also works only as input data and it is not used in the linguistic reports, the relevance valuesrSiare zero ∀i.is the output function. It consists of three trapezoidal linguistic labels heuristically defined by their vertexes as follow: {small (S) (−∞, −∞, s/16, s/8), medium (M) (s/16, s/8, 2·s/3, 3⋯/2), big (B) (2·s/3, 3·s/2, ∞, ∞)}, where s is the minimum between the width (w) and height (h) of the figure divided by six, as is shown in Eq. (2):(2)s=min(w,h)6.This 2PM aggregates (Σ) the information from the 1CPXsubordinate CP. It has the following elements:Uis the input CP: 1CPX.is the output CP (2CPSX), it has the following set of possible sentencesaSXij, with i ∈ {1, 2, …, 7} and j ∈ {1, 2, 3}:aSX1j→“The image does not contain any circle in the {left ∣ center ∣ right} part”aSX2j→“The image contains one circle in the {left ∣ center ∣ right} part”aSX3j→“The image contains two circles in the {left ∣ center ∣ right} part”aSX4j→“The image contains three circles in the {left ∣ center ∣ right} part”aSX5j→“The image contains four circles in the {left ∣ center ∣ right} part”aSX6j→“The image contains various circles in the {left ∣ center ∣ right} part”aSX7j→“The image contains many circles in the {left ∣ center ∣ right} part”is the output function, which is based on the α-cuts based method proposed in [32]. For each part of the image that appears in the set of sentences (left, center, or right) determined by the index j, we calculate the percentage of circles contained at each α-level (Nαj) by means of Eq. (3), with α∈A={0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}:(3)Nαj=1K∑k=1KFα(wX1j[k])where Fα(z) is defined by Eq. (4), andwX1j[k]is the validity degree of 1CPX, which is calculated for each circle (k) until the total number of circles (K) detected in the image.(4)Fα(z)=1ifz≥α0ifz<αThen, we calculate the membership degree of each Nαjto each element of the set of linguistic quantifiers: {Q1, …, Q7} = {Zero, One, Two, Three, Four, Various, Many}, e.g.,μQ4(Nαj)=Three(Nαj). The shapes of these linguistic labels are determined by the total number of circles K as can be seen in Fig. 6.The last step is to calculate the average value of the membership degrees obtained for each α-level using Eq. (5). The number of elements in the set A is the resolution degree, i.e., here,A=10.(5)wSXij=1A∑∀α∈AμQi(Nαj)This final value contains the relevant information about the amount of circles attending to the x coordinate (left, center or right) of their centroid. For example, the validity degree of the sentence “The image contains three circles in the center part” (wSX42) will be determined by Eq. (6):(6)wSX42=1A∑∀α∈AThree(Nα2)This 2PM aggregates (Σ) the information from the 1CPYsubordinate CP in the same way as 2PMSX. It has the following elements:Uis the input CP: 1CPY.is the output CP (2CPSY), it has the following set of possible sentencesaSYij, with i ∈ {1, 2, …, 7} and j ∈ {1, 2, 3}:aSY1j→“The image does not contain any circle in the {top ∣ center ∣ bottom} part”aSY2j→“The image contains one circle in the {top ∣ center ∣ bottom} part”aSY3j→“The image contains two circles in the {top ∣ center ∣ bottom} part”aSY4j→“The image contains three circles in the {top ∣ center ∣ bottom} part”aSY5j→“The image contains four circles in the {top ∣ center ∣ bottom} part”aSY6j→“The image contains various circles in the {top ∣ center ∣ bottom} part”aSY7j→“The image contains many circles in the {top ∣ center ∣ bottom} part”is the output function, which calculates the validity degrees (wSYij) in the same way as the validity degrees of 2CPSX(wSXij), e.g., the validity degree of the sentence “The image contains four circles in the top part” (wSY51) will be determined by Eq. (7):(7)wSY51=1A∑∀α∈AFour1K∑k=1KFα(wY11[k])This 2PM aggregates (Σ) the information from the 1CPSsubordinate CP in the same way as 2PMSXand 2PMSY. It has the following elements:Uis the input CP: 1CPS.is the output CP (2CPSS), it has the following set of possible sentencesaSSij, with i ∈ {1, 2, …, 7} and j ∈ {1, 2, 3}:aSS1j→“The image does not contain any {small ∣ medium ∣ big} circle”aSS2j→“The image contains one {small ∣ medium ∣ big} circle”aSS3j→“The image contains two {small ∣ medium ∣ big} circles”aSS4j→“The image contains three {small ∣ medium ∣ big} circles”aSS5j→“The image contains four {small ∣ medium ∣ big} circles”aSS6j→“The image contains various {small ∣ medium ∣ big} circles”aSS7j→“The image contains many {small ∣ medium ∣ big} circles”RSS=0010.40.60.80.40.60.80.40.60.80.40.60.80.80.910.80.91is the output function, which calculates the validity degrees (wSSij) in the same way as the validity degrees of 2CPSXand 2CPSY, e.g., the validity degree of the sentence “The image contains many big circles” (wSS73) will be determined by Eq. (8):(8)wSS73=1A∑∀α∈AMany1K∑k=1KFα(wS13[k])This 2PM combines (Π) the information from 1CPYand 1CPXsubordinate CPs. It has the following elements:Uare the input CPs: {1CPY, 1CPX}is the output CP (2CPYX), it has the following set of possible sentencesaYXi, with i∈ {1, 2, …, 9}: “A circle in the {top left ∣ top center ∣ top right ∣ center left ∣ center ∣ center rigth ∣ bottom left ∣ bottom center ∣ bottom right} part”. We consider that this information is not relevant in the final report, but it is available if the designer decides to show it in the future. Therefore, the relevance valuesrYXiare set to zero ∀i.is the output function, which calculates the matrix of validity degrees (WYXi), using the product for the intersection, as follows:WYX=wY1·wX1wY1·wX2wY1·wX3wY2·wX1…wY3·wX3This 2PM combines (Π) the information from 2CPYXand 1CPSsubordinate CPs. It has the following elements:Uare the input CPs: {1CPYX, 1CPS}is the output CP (2CPSP), it has the following set of possible sentencesaSPij, with i ∈ {1, 2, 3} and j ∈ {1, …, 9}:aSP1j→“A small circle in the {top left ∣ top center ∣ top right ∣ center left ∣ center ∣ center right ∣ bottom left ∣ bottom center ∣ bottom right} part”aSP2j→“A medium circle in the {top left ∣ top center ∣ top right ∣ center left ∣ center ∣ center right ∣ bottom left ∣ bottom center ∣ bottom right} part”aSP3j→“A big circle in the {top left ∣ top center ∣ top right ∣ center left ∣ center ∣ center right ∣ bottom left ∣ bottom center ∣ bottom right} part”RSP=0.40.40.40.40.40.40.40.40.40.70.70.70.70.70.70.70.70.7111111111is the output function, which calculates the validity degrees (wSPij) based on the validity degrees of the subordinate CPs, using for the intersection the product of the two matrices of validity degrees, as follows:WSP=wS1wS2wS3·wYX1wYX2wYX3wYX4wYX5wYX6wYX7wYX8wYX9In this application, we developed the FTC that can be seen in Fig. 7. It contains four choices (queries) made over 2CPSSlisted as follows:q1→Does the image contain circles?How many big circles does the image contain?How many medium circles does the image contain?How many small circles does the image contain?As we explained in Section 2.5, the relevancy matrix of each CP (R) feed the Expression Module to instantiate the FTC. Therefore, each report will be different depending on the analyzed image. In this application example, the elements of the relevancy matrix of all CPs are zero, except for 2CPSS(RSS) and 2CPSP(RSP). This is because the designer only wants to include in the report the information corresponding to these two CPs.The 2CPSSinforms about the number of circles in the image. RSSspecifies the relevancy values of each possible sentence to be included in the final report. Figs. 8 and 9show two examples of reports automatically generated by the Expression Module for two different images (Images 1 and 5 of the experimentation).The 2CPSSof the first example, Fig. 8, gave as result that the image contains four small circles, many medium circles and one big circle, as the sequence with highest validity degree calculated by Eq. (1). Attending to the relevancy matrix RSS, the relevancy degrees corresponding to this result are 0.4, 0.9 and 0.8, respectively. These results allow the Expression Module to instantiate the corresponding report.As part of the report template, we have introduced a relevancy threshold (TSS) associated with the expression of CPSS. It indicates the minimum relevancy degree of sentences to be included in the report. In this first example, with TSSfixed to 0.5, the final report will include information about medium and big circles, but not about small ones. Therefore, the resulting sentence from this example would be “The image contains many medium circles and one big circle”.The 2CPSSof the second example, Fig. 9, gave as result that the analyzed image contains various small circles, three medium circles and one big circle. As we can see in this figure, the report created by the Expression Module in this case is totally different from the previous one. If we look at RSS, the relevancy degrees corresponding to this result are equal to 0.8, 0.6 and 0.8, respectively. Since the relevance of having various small circles is the same as having one big circle, we established that the presence of big circles has preference in the final report. With TSS=0.5, the final report will include the following sentence “The image contains one big circle, various small circles and three medium circles”. Note that, the quantity of information provided and the order in which this information has been expressed has completely changed.Once the relevant number of circles is described, the designer would like to include their positions in the image. This information is provided by the 2CPSP. The relevancy degrees of this CP are specified in RSP. In this case, we also define a relevancy threshold (TSP) that allows the Expression Module to know the locations of which circles have to be included in the final report. Following with the examples showed in Figs. 8 and 9, with TSPfixed to 0.8, the final report of the first example will add that “the big circle is in the center of the image” and the final report of the second example will add that “the big circle is in the center left part of the image”. In both cases, the information about the location of small and medium circles has been omitted (their relevancy degrees are lower than TSP).In the experimental phase, we have worked with a total of 10 different images (Fig. 10).The relevancy thresholds were fixed to TSS=0.5 and TSP=0.8 as an example of application. These parameters can be modified by the expert depending on the level of detail that she/he requires in the final report. We obtained the following linguistic descriptions for each image.•Image 1 → “The image contains many medium circles and one big circle. The big circle is in the center of the image”Image 2 → “The image does not contain big circles but it contains one medium circle”Image 3 → “The image contains one big circle in the center”Image 4 → “The image contains two big circles and two medium circles. The two big circles are in the bottom center and in the top center parts of the image”Image 5 → “The image contains one big circle, various small circles and three medium circles. The big circle is in the center left part of the image”Image 6 → “The image contains various medium circles and one big circle. The big circle is in the bottom center part of the image”Image 7 → “The image contains one big circle and two medium circles. The big circle is in the center right part of the image”Image 8 → “The image contains two big circles. The two big circles are in the center of the image”Image 9 → “The image contains one big circle and three medium circles. The big circle is in the center right part of the image”Image 10 → “The image contains one big circle and three medium circles. The big circle is in the top center part of the image”Assessing the performance of a system which aims to describe data using NL is a challenging task. The meaning of NL sentences is determined by its context of use, including the personal experience of the writer and the reader [33]. In order to contribute to solve this problem, we have used a straightforward strategy: we have used the computer to create a set of sentences to cover a simple application domain.In order to asses the matching of the obtained descriptions with the test images, we have done a survey counting with the help of a set of people. The test consisted of showing them five different descriptions generated by the application for each of the ten images. These five different descriptions were chosen among those sequences which got the best validity degrees, including, of course, the best one which is considered the actual output of our system. Since we are trying to model the behavior of a non-expert observer, we asked a total of 22 different people without knowledge about Martian geology to choose the best among these five descriptions.In Table 1, we show, for each image, the average percentage of agreement of these people with the computational application (System agreement), i.e., the percentage of times that the choice of the people is the same as the description obtained by our application. Also, we show the average percentage of agreement of these people among them (People agreement), i.e., the average percentage of times that the choice of each person is the same as the description obtained by the rest of the people. The global average values for all the images are also represented at the bottom.The system agreement gets an average for all the images of 59.5% which is indeed greater than the average people agreement (45.3%). The highest values correspond to the images number 4, 8, and 3, which have a low number of circles easy to identify. Moreover, the system agreement is greater than the people agreement for all of the images except the number 9, which means that each person agrees, in average, more with our application than with the descriptions chosen by the rest of the people. This fact can be explained focusing in Fig. 3, where the four recognized circles of the image number 9 are represented. The description made by the application based on these four circles is: “The image contains one big circle and three medium circles. The big circle is in the center right part of the image”. However, a human observer who tends to include the big circle in the center right part of the image, tends also to include the circle at the top left corner that is not recognized by the DAQ Module. Therefore, not only is the description obtained by our report generator extremely dependent on the results of the DAQ Module, but also on the subjectivity of the people.To proof the effectiveness of relevance and relevancy thresholds, we have modified the parameters TSSand TSP, checking the evolution of the sentences obtained in final reports. Then, the results of this experimental process applied to three of the ten analyzed images are explained as follows:•Image 2:Original: “The image does not contain big circles but it contains one medium circle.”Increasing TSSfrom 0.5 to 0.7: “The image does not contain big circles.”Decreasing TSPfrom 0.8 to 0.7: “The image does not contain big circles but it contains one medium circle. The medium circle is in the center of the image.”Image 4:Original: “The image contains two big circles and two medium circles. The two big circles are in the bottom center and in the top center parts of the image.”Increasing TSSfrom 0.5 to 0.7: “The image contains two big circles. The two big circles are in the bottom center and in the top center parts of the image.”Decreasing TSPfrom 0.8 to 0.7: “The image contains two big circles and two medium circles. The two big circles are in the bottom center and in the top center parts of the image. The two medium circles are in the bottom left part of the image.”Image 9:Original: “The image contains one big circle and three medium circles. The big circle is in the center right part of the image.”Increasing TSSfrom 0.5 to 0.7: “The image contains one big circle. The big circle is in the top center part of the image.”Decreasing TSPfrom 0.8 to 0.7: “The image contains one big circle and three medium circles. The big circle is in the center right part of the image. The three medium circles are in the top left and center left parts of the image.”As we can see in the previous results, when a relevancy threshold is increased, the Expression Module omits more information, including only the most relevant information. On the other hand, when a relevancy threshold is decreased, the Expression Module includes more information, specifying details less relevant for the expert.This paper present our last contributions to the field of generating linguistic descriptions of data:•We have explored the multidimensionality of the domain of existence of a CP.We have developed upon the use of the relevance to choose the most suitable linguistic expressions.We have presented a new version of Report Generator based on the concept of Fuzzy Tree of Choices.We have described a computational application able to create customizable linguistic reports about detected circular structures such as volcanoes or meteorite impacts.In human generated linguistic reports, the use of NL depends on the application context and specifically of the writer experience and intentions. Here, the chosen linguistic expressions should capture the subjectivity of the human beings participating in the process of designing the computational system, namely, the expert on Martian geology that will provide the functional requirements and the designer that will try to implement that functionality.Of course, this approach can also be applied to different fields. The difficulties of each application will depend on the complexity of the data and of the complexity of the desired linguistic reports.In future works in this application, we could improve the DAQ Module in order to recognize different geological structures and we will develop new reports to provide more complex descriptions of the Mars’ surface.

@&#CONCLUSIONS@&#
