@&#MAIN-TITLE@&#
On the convergence of ICA algorithms with weighted orthogonal constraint

@&#HIGHLIGHTS@&#
The symmetric Minimum Distance weighted Unitary Mapping of nonsingular square matrix is obtained.A characterization of the stationary points corresponding to weighted orthogonal constrained ICA algorithms using symmetric Minimum Distance weighted Unitary Mapping is obtained.The monotonic convergence property of the weighted orthogonal constrained fixed point ICA algorithms using symmetric Minimum Distance weighted Unitary Mapping for convex contrast function is proved, which is further extended to nonconvex contrast function case.

@&#KEYPHRASES@&#
Independent component analysis,Weighted orthogonal constraint,Symmetric MDWUM,Convergence analysis,

@&#ABSTRACT@&#
A kind of weighted orthogonal constrained independent component analysis (ICA) algorithms with weighted orthogonalization for achieving this constraint is proposed recently. It has been proved in the literature that weighted orthogonal constrained ICA algorithms keep the equivariance property and have much better convergence speed, separation ability and steady state misadjustment, but the convergence is not yet analyzed in the published literature. The goal of this paper is to fill this gap. Firstly, a characterization of the stationary points corresponding to these algorithms using symmetric Minimum Distance Weighted Unitary Mapping (MDWUM) for achieving the weighted orthogonalization is obtained. Secondly, the monotonic convergence of the weighted orthogonal constrained fixed point ICA algorithms using symmetric MDWUM for convex contrast function is proved, which is further extended to nonconvex contrast functions case by adding a weighted orthogonal constraint term onto the contrast function. Together with the boundedness of contrast function, the convergence of fixed point ICA algorithms with weighted orthogonal constraint using symmetric MDWUM is implied. Simulation experiments results show that the adaptive ICA algorithms using symmetric MDWUM are better in terms of accuracy than the ones with pre-whitening, and the fixed-point ICA algorithms using symmetric MDWUM converge monotonically.

@&#INTRODUCTION@&#
ICA consists of recovering statistically independent but otherwise unobserved source signals from their linear mixtures without prior knowledge of the source signals and the mixing coefficients. As a hot topic in the field of signal processing, ICA has been widely applied in seismic exploration, mobile communication, speech processing, array signal processing and biomedical engineering, etc. [1].The algorithms in ICA are derived from searching methods aiming to maximize a given contrast function that reflects a measure based on independence or non-Gaussianity of the source signals. Whitening is the prerequisite of ICA, however, there is no equivariance property in the ICA algorithms based on pre-whitened data. To solve this problem, by transforming the pre-whitening into weighted orthogonal constraint, a new definition of contrast function is proposed in [2], various weighted orthogonal constrained adaptive ICA algorithms and a weighted orthogonal constrained Fast ICA algorithm are proposed in [2–5].In the past, a major result about the convergence of the one-unit ICA algorithms was provided in [6], where it is shown that the one-unit Fast ICA algorithms have cubic convergence for the special choice of kurtosis contrast function. In [7], Regalia and Kofidis have shown the monotonic convergence of the one-unit fixed point ICA algorithms for a larger set of contrast functions. In [8], Hao and Huper used a dynamical systems framework for the local convergence analysis of Fast ICA. For the convergence analysis of the symmetric ICA algorithms, Oja proposed a convergence analysis for the symmetric Fast ICA algorithm based on a kurtosis contrast function [9], which also appeared in [10]. Furthermore in [11], both one-unit and symmetric ICA algorithms are shown to approach the Crammer–Rao lower bound for certain scenarios. Recently, a comprehensive result on the convergence of ICA algorithms with symmetric orthogonalization was reported in [12].The goal of this article is to contribute to the convergence analysis of the recently proposed ICA algorithms with weighted orthogonal constraint. Firstly, we introduce the weighted orthogonal constraint and the symmetric MDWUM for achieving this constraint. Secondly, a convenient characterization of stationary points of both gradient ascent and fixed point ICA algorithms with this constraint using symmetric MDWUM are provided. Then, we analyze the monotonic convergence of weighted orthogonal constrained fixed point ICA algorithm using symmetric MDWUM based on convex contrast functions, and extend it to nonconvex contrast functions case. Finally, we prove the relevant analysis results by simulation experiments.In the ICA model, the observed signals are the output of a group of sensors, what each sensor received is a mixture of multiple source signals. n source signals issued byst=[s1(t),…,sn(t)]Tare received by m sensors, then observed signalsxt=[x1(t),…,xm(t)]Tare generated according to(1)xt=Ast,t=1,2,…where A is mixing matrix. Without loss of generality, we assume that the source signals have zero mean and unit variance, and at most, one of them is Gaussian distributed. Solving ICA problem is to find a matrix B called separating matrix, which makes the matrix transformation(2)yt=Bxt,t=1,2,…be an estimation of the source signals, whereyt=[y1(t),…,yn(t)]Tare called separated signals [1].Except for the observed signals and the statistically independent assumption of the source signals, nothing can be used in solving ICA problem. Therefore, a contrast function based on independence or non-Gaussianity of the source signals is first designed, then the maximum point of it yields the optimal separating matrix. Generally, two kinds of optimizing updates are used in ICA algorithms.Gradient ascent update:(3)Bk+1=Bk+ηk∇BJ(Bk)Fixed-point update:(4)Bk+1=∇BJ(Bk)whereJ(B)is contrast function, η is learning step-size and∇BJ(B)is the gradient of contrast function with respect to B[1].As the pre-processing of ICA, pre-whitening makes ICA somewhat easier. The pre-whitened observed signals are given by(5)vt=Vxt,t=1,2,…wherevt=[v1(t),…,vn(t)]Thave unit variance, V is pre-whitening matrix. In fact, the separating matrix B can be factorized into the product of a new separating matrix W and pre-whitening matrix, at this time, the separating outputs (2) can be written as(6)yt=Bxt=WVxt=Wvt,t=1,2,…In addition, we know that the source signals have unit variance from the basic assumption of ICA model, so the separated signals should also have unit variance, hence(7)E{ytytT}=E{WvtvtTWT}=WE{vtvtT}WT=WWT=Iwhich illustrates that, the new separating matrix W is orthogonal when observed signals are pre-whitened [1,13].Equivariance property is a significant property of adaptive ICA algorithms, which means the evolution law of the global transfer matrixCk=BkAis independent of the mixing matrix [3,14]. However, because of the accumulated error in pre-whitening stage, the ICA algorithms based on pre-whitened data often have no equivariance property. As a result, when the condition number of mixing matrix is bad or some source signals are weak, the separated results may be very poor. To overcome the above weakness, supposed the pre-whitening matrix is obtained, and instead of multiplying from left to observed signalsxtto get the pre-whitened observed signalsvt, we multiply pre-whitening matrix from right to W, then the separating matrixB=WVshould satisfy(8)BRxBT=WVE{xtxtT}VTWT=WE{VxtxtTVT}WT=WE{vtvtT}WT=WWT=IwhereRxis the covariance matrix of observed signals. In other words, the separating matrix B is weighted orthogonal when observed signals are not pre-whitened. Based on this, a new definition of contrast is proposed by transforming the pre-whitening into the condition (8) which is called weighted orthogonal constraint.Definition 1(See [3].) An alternative contrast is defined as a multivariate mappingJ(⋅)from the set of probability distribution ofyt=Bxtto the real number setR, so thatJ(xt,B)obtains its maximum (minimum) if and only ifB=PDA−1for any t, where B is ann×mfull rank matrix satisfyingBRxBT=I, P is a permutation matrix and D is a nonsingular diagonal matrix.The new contrast can be formulated as the following optimization model(9){maxJ(xt,B)∀ts.t.BRxBT=I,B∈Rn×mwortwhereRn×mwortdenotes the set composed byn×mweighted orthogonal matrices. In this case, the shortcoming that the accumulated error in the estimation of pre-whitening matrix degenerates the equivariance property of the ICA algorithms based on pre-whitened data will be evaded.For this constrained optimization problem, we can use projection method to solve it [13,15–17]. That is to say, we solve the above optimization problem with an unconstrained learning first, which might be gradient ascent update or fixed-point update, then after each iteration step, the updated B is projected weighted orthogonally onto the constraint set so that it satisfies the constraint. To sum up, the ICA algorithms with weighted orthogonal constraint based on projection method are composed of the following two steps [1,3]:Step 1: an unconstrained ICA learning based on classic contrast (3) or (4), ignoring the weighted orthogonal constraint.Step 2: the enforcement of the weighted orthogonal constraint (8), namely, to make the updated B be weighted orthogonal.In this section, the symmetric MDWUM is introduced first, then two important Lemmas about symmetric MDWUM are proposed and one Lemma about generalized eigne-decomposition of matrix pencil is introduced. For simplicity, from now on, we assume thatm=n.According to the above ICA algorithms with weighted orthogonal constraint based on projection method, for achieving this constraint, we need to do weighted orthogonalization to B after the traditional update. Generally, two methods are used [12,18,19], one is Gram–Schmidt procedure based on QR factorization, which is applied to the updated B to convert its columns to weighted orthogonal vectors, the other is symmetric MDWUM, in which case, the argument is mapped to the closest weighted orthogonal matrix in both Frobenius norm and induced-2-norm sense. In other words, given Q a nonsingular square matrix, then symmetric MDWUM maps the Q toMWUMDWUM(Q): the closest weighted orthogonal matrix to Q, namely,MWUMDWUM(Q)satisfies(10)‖MWUMDWUM(Q)−Q‖F2=minB∈Rn×nwort‖B−Q‖F2=minB∈Rn×nwortTr((B−Q)(B−Q)T)=minB∈Rn×nwortTr(Rx−1−BQT−QBT+QQT)=−2minB∈Rn×nwortTr(BQT)+cwhere subscript “WU” means the mapped matrix is a “weighted unitary” matrix. Suppose the matrix Q has the singular value decomposition (SVD)Q=UQΣQVQT, whereΣQis a diagonal matrix consisted of the square root of eigenvalues ofQQT,UQandVQare corresponding unitary matrix, later, in Theorem 3, we will obtain(11)MWUMDWUM(Q)=UQVQTRx−1/2In fact, we can rewrite(12)Q=UQΣQVQT=UQΣQUQT︸UQVQTRx−1/2︸Rx1/2=ΠBRx1/2where Π is positive definite, B is weighed orthogonal, namely,BRx1/2is orthogonal:BRx1/2(BRx1/2)T=BRxBT=I.Before the convergence analysis of this new ICA algorithm, we first propose the following two Lemmas about symmetric MDWUM and introduce one lemma about generalized eigen-decomposition of matrix pencil that will be used latter.Lemma 1For a full rank square matrixQand weighted orthogonal matrixB,MWUMDWUM(BRx1/2Q)=BRx1/2MWUMDWUM(Q).When B is weighted orthogonal,BRx1/2is orthogonal, so this Lemma is clearly established according to (11).Lemma 2For a full rank square matrixQ,MWUMDWUM(Q)=Rx−1/2if and only ifQis a positive-definite matrix.Together with (11), according to eigenvalue decomposition and SVD, this Lemma is easy to be proved.Lemma 3(See[20].) Suppose matrix pencil(A,B)is regular, then1.all the generalized eigenvalue of matrix pencil(A,B)is real, i.e.,λi∈R,i=1,…,n.there exit ann×nmatrixC, such thatCBCT=In,CACT=diag(λ1,…,λn).This Lemma shows that the eigenvalue problem of regular matrix pencil is similar to one of Hermitian matrix of this matrix pencil.On the convergence of ICA algorithm, we should analyze “which points” the algorithm converges to and “how” it converges. In this section, we will first obtain a general characterization of the stationary points of ICA algorithms with weighted orthogonal constraint using symmetric MDWUM.A separating matrix is referred as the stationary point of ICA algorithm if the algorithm iterations starting from this point will remains at the same point, and once the algorithm reaches to one of these stationary points, it becomes “trapped” at that point. Based on this, we propose the following two Theorems, which provides the stationary point characterization of ICA algorithms with weighted orthogonal constraint using symmetric MDWUM based on gradient ascent update and fixed-point update respectively. Here, we assume that the gradient of the contrast function with respect to B exists and is of full rank over the constraint.Theorem 1For the ICA algorithms with weighted orthogonal constraint using symmetric MDWUM and gradient ascent update, where the step-size is sufficiently small such thatBin(3)is a full rank matrix, a weighted orthogonal matrixB⁎is the stationary point of the above algorithm if and only if there exists a matrixSsatisfying(13)∇BJ(B⁎)=B⁎Rx1/2SandS=STA weighted orthogonal matrixB⁎is the stationary point of this ICA algorithm if and only ifB⁎is mapped back to itself after gradient ascent update and symmetric MDWUM, which can be mathematically written as(14)B⁎=MWUMDWUM(B⁎+ηk∇BJ(B⁎))=B⁎Rx1/2MWUMDWUM(Rx−1/2+ηkRx1/2B⁎T∇BJ(B⁎))where last equalities follow fromB⁎RxB⁎T=Iand Lemma 1. As a result,B⁎to be stationary point can be rewritten as(15)MWUMDWUM(Rx−1/2+ηkRx1/2B⁎T∇BJ(B⁎))=Rx−1/2Due to step-size is enough small, the argument of aboveMWUMDWUMis a full rank matrix. Therefore, according to Lemma 2,B⁎is a stationary point if and only ifRx−1/2+ηkRx1/2B⁎T∇BJ(B⁎)is a positive-definite matrix, which is further equivalent to the condition thatRx1/2B⁎T∇BJ(B⁎)is a symmetric matrix. In other words, there exists a matrix S satisfying(16)Rx1/2B⁎T∇BJ(B⁎)=SandS=ST⟹∇BJ(B⁎)=B⁎Rx1/2S□Here, ifRx1/2B⁎T∇BJ(B⁎)is positive-definite matrix, thenB⁎is a stationary point for any nonnegative step-size. IfRx1/2B⁎T∇BJ(B⁎)is a symmetric matrix with negative eigenvalues, thenB⁎is a stationary point for the non-negative step-size that should satisfy [12](17)ηk<1/|λmin(Rx1/2B⁎T∇BJ(B⁎))|∀k⩾0Theorem 2For the ICA algorithms with weighted orthogonal constraint using symmetric MDWUM and fixed-point update(4), a weighted orthogonal matrixB⁎is the stationary point if and only if there exists a matrixSsatisfying(13).A weighted orthogonal matrixB⁎is the stationary point of this ICA algorithm if and only ifB⁎is mapped back to itself after fixed-point update and symmetric MDWUM, which can be mathematically written as(18)B⁎=MWUMDWUM(∇BJ(B⁎))=B⁎Rx1/2MWUMDWUM(Rx1/2B⁎T∇BJ(B⁎))where last equalities follow fromB⁎RxB⁎T=Iand Lemma 1. As a result,B⁎to be stationary point can be rewritten as(19)MWUMDWUM(Rx1/2B⁎T∇BJ(B⁎))=Rx−1/2Then, according to Lemma 2 and the full rank assumption on the gradient of contrast function with respect to B,B⁎is a stationary point if and only ifRx1/2B⁎T∇BJ(B⁎)is a positive-definite matrix, which is also symmetric. In other words, there exists a matrix S satisfying (16). □For the ICA algorithms with weighted orthogonal constraint using symmetric MDWUM, the convergence analysis is obstructed by the complication caused by symmetric MDWUM. In this section, we will analyze “how” this fixed-point ICA algorithm converges. First, we will concentrate on the monotonic convergence analysis based on the convex contrast functions. Then, extend it to the nonconvex contrast functions case.Based on Theorems 1 and 2, we first propose the following Theorem which will give the solution of (10) and be useful in the proof of monotonic convergence.Theorem 3Given a full rank matrixQ, the unique global maximum of the problem(20){maxTr(BQT)s.t.BRxBT=Iis given byB⁎=MWUMDWUM(Q).Based on Theorems 1 and 2, a local optimumB⁎of this problem should satisfy(21)Q=B⁎Rx1/2SandS=STAssuming Q has a SVDQ=UQΣQVQT, for the local optimum points of the above problem, we can get(22)QTQ=ST(B⁎Rx1/2)TB⁎Rx1/2S=STS=VQΣQΣQVQT=(VQΛVQT)TVQΛVQTHere,(B⁎Rx1/2)TB⁎Rx1/2=B⁎Rx1/2(B⁎Rx1/2)T=B⁎RxB⁎T=I, this is becauseB⁎Rx1/2is an orthogonal square matrix, andΛ=FΣQ, in which F is a diagonal matrix with 1's, and/or −1's on the diagonal. So we can conclude that S is a matrix with(23)S=VQΛVQTandS=STThen, according to (21), we can get that a local optimum point of the above problem should satisfy(24)B⁎=QS−1Rx−1/2=UQF−1VQ−1Rx−1/2Tr(B⁎QT)=Tr(UQF−1VQ−1Rx−1/2VQΣQUQT)Among all local optimum points specified by (24), the value ofTr(B⁎QT)at the global optimum point achieved forF=Iis strictly greater than its values at other local optimum obtained forF≠I. Then the global optimum pointB⁎for the above problem is obtained by projecting Q to the set of weighted orthogonal matrices using symmetric MDWUM (11). □Based on Theorem 3, for convex contrast functions in the ICA algorithms, we can propose the following Theorem about monotonic convergence of the ICA algorithms with weighted orthogonal constraint using symmetric MDWUM and fixed-point update.Theorem 4An ICA algorithm with weighted orthogonal constraint using symmetric MDWUM corresponding to the optimization setting in fixed-point update(4)where J is a smooth convex contrast function (over a convex set containing the set of weighted orthogonal matrices), which is bounded on the set of weighted orthogonal matrices, is monotonically convergent to one of the stationary points defined byTheorem 2.For given J which is a smooth convex function of B (over a convex set containing the set of weighted orthogonal matrices), there is(25)J(Bk+1)⩾J(Bk)+Tr(∇BJ(Bk)T(Bk+1−Bk))With fixed-point update and symmetric MDWUM, for any givenBkwhich is not a stationary point of the problem (9)(26){Bk≠MWUMDWUM(∇BJ(Bk))Bk+1=MWUMDWUM(∇BJ(Bk))According to Theorem 3, these guarantee that(27)Tr(∇BJ(Bk)TBk+1)>Tr(∇BJ(Bk)TBk)So combining (25), for anyBk,Bk+1pair in (4), we have(28)J(Bk+1)>J(Bk)Therefore, for convex contrast functions in ICA algorithm, this algorithm is monotonically convergent. □For nonconvex contrast functions in ICA algorithms with weighted orthogonal constraint using symmetric MDWUM and fixed-point update, we need to modify the update expression from the convex case onto the nonconvex case, such that the resulting algorithm is guaranteed to be monotonically convergent.When the Hessian matrix∂∂b(∂J∂b)T(orHJ(B)) ofJ(B)to vectorized coordinatesb=[B:,1TB:,2T⋯B:,mT]Tis not positive-semidefinite, we can introduce an auxiliary contrast function [12,21](29)P(B)=J(B)+γTr(BRxBT−I)over the domainD={B|BRxBT⪯I}, which meansI−BRxBTis a positive-semidefinite matrix. Then, the gradient ofP(B)with respect to B can be written as(30)∇BP(B)=∇BJ(B)+γBRxThe Hessian matrix∂∂b(∂P∂b)TofP(B)to vectorized coordinates b is given by(31)HP(B)=HJ(B)+γRx̲whereγRx̲denotesm2×m2Hessian matrix of the second term in (29).Rx̲=[Rx00⋯00Rx0⋯0⋅⋅⋅⋅⋅00⋯0Rx]It is easy to see thatRx̲is positive matrix. The first termHJ(B)is a symmetric matrix and the second termγRx̲is a positive matrix, so the matrix pencil(HJ(B),γRx̲)is a regular matrix pencil. According to Lemma 3, there exists a nonsingular matrix C satisfyingCHJ(B)CT=diag(λ1,λ2,…,λm2), whereλi,i=1,2,…,m2denotes the eigenvalues ofHJ(B), andCγRx̲CT=I. These yield(32)CHP(B)CT=diag(λ1,λ2,…,λm2)+γIfrom which, it is easy to see that the Hessian matrixHP(B)is positive if γ satisfies [12](33)γ⩾−infB∈Dλmin(HJ(B))whereHJ(B)has negative eigenvalues overDdue to the fact that J is nonconvex. At this time,HP(B)is positive-semidefinite overD, thenP(B)would be convex overD. Therefore, due to Theorem 4, the corresponding new ICA algorithm is monotonically convergent. Regardless of symmetric MDWUM, the fixed-point update expression corresponding to this new contrast function can be written as(34)Bk+1=∇BP(Bk)=γBkRx+∇BJ(Bk)To sum up, together with the boundedness of contrast functions, the monotonic convergence of the fixed-point ICA algorithms with weighted orthogonal constraint using symmetric MDWUM is implied.The weighted orthogonality in this paper is the extension of the orthogonality (the weighted matrix is extended from unit matrix I to covariance matrix of the mixtureRx), so, the conclusions in this paper are the extension of the conclusion in Ref. [12]. What is more, the ICA algorithms without pre-whitening using symmetric MDWUM have the equivariance property, which makes that corresponding ICA algorithms work more reliable, so the relevant convergence analysis is necessary. In addition, like in extension of monotonic convergence to nonconvex contrast functions case, our proof is much more complicated than the corresponding proof in [12], meanwhile, we will use simulation experiments to support our analysis.In this section, we will provide 3 groups of experiments to illustrate the convergence, stability and accuracy of the ICA algorithms with weighted orthogonal constraint using symmetric MDWUM, and prove the monotonic convergence of this new ICA algorithm based on fixed-point update. In experiments, we use 4 observation sensors, the following signals and some speech signals [2–5,22–24]:s1(t):Square wave signalsign(cos(2π×155t))High frequency sinusoidal signalsin(2π×800t)Phase modulation signalsin(2π×300t+6cos(2π×60t))Uniformly distributed noise signal in[−1,1].The following 3 groups of experiments are used to simulate the adaptive and the batch gradient descent ICA algorithms and fixed-point ICA algorithm respectively using adaptive natural gradient ICA (NG-ICA) algorithm, batch NG-ICA algorithm and fast fixed-point ICA (Fast ICA) algorithm.In experiment 1, we select the above 4 signals:s1(t),s2(t),s3(t),s4(t)as the source signals. The weighted orthogonal constrained adaptive NG-ICA algorithm using symmetric MDWUM, the adaptive NG-ICA algorithm based on pre-whitened data and the traditional adaptive NG-ICA algorithm are executed simultaneously. In three algorithms, the step-sizes all take value 0.01. The convergence stability performance curves of different algorithms are given in Fig. 1.In experiment 2, we select a speech signal and the above 3 signals:s2(t),s3(t),s4(t)as the source signals, whose waveforms are plotted in Fig. 2. The weighted orthogonal constrained batch NG-ICA algorithm using symmetric MDWUM and the batch NG-ICA algorithm based on pre-whitened data are executed simultaneously. In two algorithms, the step-sizes all take value 0.02. Similarly, the separated signals waveform achieved by batch NG-ICA algorithms using symmetric MDWUM and the convergence stability performance curves of different algorithms are given in Figs. 3 and 4.In experiment 3, we select 3 speech signals and the above noise signals4(t)as the source signals, whose waveform are plotted in Fig. 5. The convergence stability performance of the weighted orthogonal constrained Fast ICA algorithm using symmetric MDWUM and the Fast ICA algorithm based on pre-whitened data are compared in Fig. 7. Meanwhile, the separated signals waveform achieved by Fast ICA algorithm using symmetric MDWUM are given in Fig. 6. Here, the update rule of the weighted orthogonal constrained Fast ICA algorithm is:(37)bi=(E{xtg(bixt)}−E{g′(bixt)}E{xt(bixt)})BkTBkwherebi,i=1,…,nis the i-th row ofBk,g(y)=[g1(y1),g2(y2),…,gn(yn)]Tare some increasing odd functions, called activation functions, for supergaussian source signals, we usually setgi(yi)=2tanh(yi),i=1,…,n, for subgaussian source signals, we usually setgi(yi)=yi3,i=1,…,n. In addition, for proving the monotonic convergence of weighted orthogonal constrained Fast ICA algorithm using symmetric MDWUM, the kurtosis contrast functions values of the weighted orthogonal constrained Fast ICA algorithm and the Fast ICA algorithm based on pre-whitened data are directly compared in Fig. 8.It is needed to be emphasized that, in all experiments, we must do symmetric MDWUM followed by the basic update forBkin the every iteration of weighted orthogonal constrained ICA algorithms.In simulation, the mixing matrix is generated randomly in each run and its element is subject to the uniform distribution in[−1,1]; the sampling period of source signals isTs=0.0001s. The convergence stability performance curves and the kurtosis contrast functions curves are all plotted according to the mean values of 200 run results, and the kurtosis contrast functions values are the mean of absolute values of kurtosis of all separated signals.By Figs. 1, 4 and 7, we can see that crosstalk error of ICA algorithms with pre-whitening are convergent gradually, so are the ICA algorithms with symmetric MDWUM, but the values of latter in the adaptive NG-ICA algorithm are smaller when the algorithm is convergent, which shows that the new adaptive NG-ICA algorithm with weighted orthogonal constraint using symmetric MDWUM is more accuracy. Meanwhile, although we do not do pre-whitening on observed signals in weighted orthogonal constrained ICA algorithms, but the convergence speed and stability of the batch algorithms are basically same to the ones based on pre-whitened data. By Figs. 2, 3, 5 and 6, we can see that using batch NG-ICA and Fast ICA algorithms with symmetric MDWUM to separate speech signals is effective. In addition, by Fig. 8, we can also see that the Fast ICA algorithm with symmetric MDWUM is monotonic convergent, the mean values of kurtosis contrast functions of separated signals increases monotonic to be invariant, whose track is the same to the Fast ICA algorithm with pre-whitening.

@&#CONCLUSIONS@&#
