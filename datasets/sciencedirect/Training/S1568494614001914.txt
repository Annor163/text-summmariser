@&#MAIN-TITLE@&#
Reliability analysis of geostructures based on metaheuristic optimization

@&#HIGHLIGHTS@&#
Critical assessment of seven metaheuristic optimization algorithms.Combined algorithm of metaheuristic optimization with first order reliability method for solving reliability problems.Four real world geotechnical problems are considered, among whichthe SCALERE damin centre-north of Italy.

@&#KEYPHRASES@&#
Metaheuristics,Reliability analysis,Neural networks,Monte Carlo simulation,Approximate reliability methods,

@&#ABSTRACT@&#
The objective of this work is to incorporate metaheuristic optimization into the framework of the reliability analysis of geostructures in conjunction with innovative tools for treating computational intensive problems of real-world geostructural systems. For the purposes of this study two types of random variables are considered: those which influence demand and those that affect capacity. The Monte Carlo simulation method is considered as the most reliable method for estimating exceedance probabilities or other statistical quantities albeit with excessive, in many cases, computational cost, while first or second order reliability methods (FORM, SORM) constitute alternative approaches. In this study, in order to propose an efficient methodology for performing reliability analysis of geostructures we assess and compare seven metaheuristic optimization algorithms incorporated into FORM for solving reliability analysis problems, while one neural network approximation of the limit-state function is also examined. The results suggest that the seven approaches examined offer applicable as well as fast solutions but with varying qualitative characteristics.

@&#INTRODUCTION@&#
Heuristic and metaheuristic algorithms are nature-inspired or bio-inspired search methods as they have been developed based on the successful behaviour of biological systems by learning from nature. Nature has been solving various problems over many years based on the principle that the best and robust solutions endure according to the norm of the survival of the fittest. Similarly, heuristic algorithms use learning and adaptation to solve problems. Modern metaheuristic algorithms are almost guaranteed to an efficient performance for a wide range of combinatorial optimization problems. Roughly speaking, modern metaheuristic algorithms for engineering optimization and not only include genetic algorithms (GA) [1], simulated annealing [2], particle swarm optimization (PSO) [3], ant colony algorithm [4], artificial bee colony (ABC) algorithm [5], harmony search (HS) [6], firefly algorithm [7], and many others.Reliability analysis can be performed either with simulation methods (like Monte Carlo simulation method), or by means of approximation ones. First and second order reliability methods (FORM, SORM), belonging to the second category, require prior knowledge of mean and variance of the random variables along with a differentiable failure function. On the other hand, although the major advantage of Monte Carlo simulation (MCS) method is that accurate solutions are obtained for almost every problem, yet in many cases it requires excessive computational cost. Variance reduction techniques, like directional simulation, importance sampling, antithetic variates or adaptive sampling, have been proposed, among others, in order to reduce the computational effort of MCS. Variance reduction techniques still require significant number of the system response evaluations to estimate failure probabilities of the order less than 10−3[8]. Other simulation methods, such as line sampling [8] and subset simulation [9], were proved to be very efficient computationally reducing the sample size; however, their performance and ergodicity are sensitive to the values of parameters which are not known a priori [10].In the past, the problems of reliability analysis and risk assessment of geotechnical problems was studied extensively. Indicatively, Koutsourelakis et al. [11] dealt with non-linear stochastic dynamic analysis of a soil-structure interacting system in an effort to determine the system's risk of damage due to liquefaction under a wide range of earthquake intensities. Srivastava and Babu [12] considered spatially variable soil parameters in reliability-based geotechnical deign. Zevgolis and Bourdeau [13] formulated a probabilistic model in order to assess the reliability of the external stability of reinforced soil structures, considering geotechnical and material uncertainty. Abbaszadeh et al. [14] evaluated the reliability index of the Sungun copper mine slope stability based on the Taylor series, Rosenblueth point estimate and MCS methods. Soubra and Mao [15] presented a probabilistic analysis at the ultimate limit state of a shallow strip footing resting on a (c, φ) soil when subjected to an inclined load.The application of a computationally efficient MCS method to complex systems, requires an approximate knowledge of the limit-state function g(x). Such approximations contribute to the reduction of the excessive number of system response evaluations. However, complex reliability problems are characterized by limit-state functions of implicit nature. Furthermore, an explicit expression of either the entire limit-state function g(x) or of its limit-state surface g(x)=0 in the space of the random variables x is required for the implementation of FORM or SORM. The response surface (RS) method is used to approximate the required limit-state function; however, it was found that the performance of RS method depends on the experimental points used to derive the approximate limit-state function [16,17]. As an alternative, several studies have been presented in the past where the limit-state function is estimated by means of neural networks (NN) [18,19].In this study, innovative and efficient procedures for performing reliability analysis are implemented for assessing real-world geostructural systems. We assess seven metaheuristic optimization (MO) algorithms incorporated into FORM for solving reliability analysis problems, resulting into the so-called MO-FORM approach. In particular, we critically assess and compare the efficiency and robustness of the following metaheuristic optimization algorithms: genetic algorithms, particle swarm optimization, differential evolution, harmony search, covariance matrix adaptation (CMA) evolution strategy, elitist covariance matrix adaptation (ECMA) and artificial bee colony, when implemented for solving large-scale reliability analysis problems of geostructures. In addition, randomness and uncertainty are taken into consideration with two methodologies for reducing the computational cost. These methodologies are based on the Monte Carlo and first order reliability methods and exploit the neural network approximation of the limit state function. The efficiency of NN predictions are highly influenced by the number of random variables; the larger the worst [20,21]. Thus, the benefits from this work are twofold: first, evaluating the effectiveness of metaheuristic optimization approaches we offer evidence regarding the implementation of these algorithms in practice and, second, we examine an alternative for solving large scale reliability analysis problems of geostructures implementing the MO-FORM approach.The rest of the paper is organized as follows: the formulation of the reliability analysis problem along with the methods implemented for the solution is described in solving the reliability analysis problem section. Metaheuristic algorithms section discusses the general framework of metaheuristic optimization while a short description of the seven algorithms employed in this study is provided. Subsequently, the neural networks are described together with their implementation for the replacement of the polynomial representation of the response surface function. Finally, four large-scale test examples are adopted in order to examine the efficiency of the proposed methodology implemented for solving real-world geostructural reliability problems.The advancements in reliability theory during the last 20 years and the development of more accurate quantifications of uncertainties associated with system loads and resistances have stimulated the interest in probabilistic treatment of systems [22]. The reliability of a system or its probability of failure constitute important factors during the design procedure, since those characterize the system's capability to successfully accomplish its design requirements. Reliability analysis leads to safety measures that a design engineer has to take into account due to uncertainties. First and second order reliability methods, that have been developed to perform reliability, although they lead to elegant formulations, they require prior knowledge of the means and variances of component random variables and the definition of a differentiable limit-state function. On the other hand the Monte Carlo simulation method is not restricted by form and knowledge of the limit-state function but is characterized by high computational cost.The MCS method is applied in stochastic analysis when an analytical expression of the limit-state function is not available. This is mainly the case in problems of complex nature with a large number of random variables, where all other stochastic analysis methods are not applicable. In stochastic analysis problems, the violation probability of the behavioural constraints can be written as:(1)pviol=∫g(x)≥0fx(x)dxwhere fx(x) denotes the joint probability function of the random variables, the limit-state function g(x)<0 designates the safe region whilexis the vector of the m random variables. Considering that MCS is based on the theory of large numbers (N∞), an unbiased estimator of the violation probability is given by:(2)pviol=1N∞∑j=1N∞I(xj)wherexjis the jth vector of random variables, I(xj) is an indicator if jth simulation violates or not the limit state function, and it is defined as:(3)I(xj)=1ifg(xj)≥00ifg(xj)<0In order to estimate pviolan adequate number Nsimof random samples is generated using specific uniform probability density function of vectorx. The value of the limit state function is computed for each random samplexjand the estimation of pviolis given in terms of sample mean by:(4)pviol≅NHNsimwhere NHis the number of simulations where the limit state function is violated while Nsimis the total number.The basic MCS is simple to use and has the capability of handling practically every possible case regardless of its complexity. However, due to large sample size required by MCS the computational demand becomes excessive. To reduce the computational effort more elaborate simulation methods, called variance reduction techniques, have been developed, their efficiency though is limited for larger probability values. Moreover, despite the improvements achieved on the efficiency of computational methods for treating geostructural analysis problems of large scale systems, they still require disproportional computational effort for reliability analysis of realistic problems.The main objective of the first order reliability method is to calculate the reliability index β. The Hasofer-Lind reliability index β[23] is calculated through a minimization process, and the probability of violation is approximated by the following expression:(5)pviol=Φ(−β)where Φ is the standard normal cumulative distribution function. This equation is precise when the failure criterion is linear and all random variables have normal distributions. Given a vector of basic variablesx, a failure surface ∂ω on which the failure criterion g(x)=0 is satisfied and a safe region denoted by g(x)>0, the vector of the reduced variables z is defined as follows:(6)z≡Sx−1⋅(x−μx)whereSx, is a diagonal matrix of the standard deviations andμxis the vector of mean values. Then the Hasofer-Lind reliability index β is defined as:(7)β≡minz∈∂ωzT⋅C−1⋅zThe point on the failure surface g(x)=0, where its transformation to thezspace satisfies Eq. (7) is called design or most probable point and will be denoted aszDandCis the correlation matrix. The design point zDis located on the limit-state surface g(x)=0 and in the standard normal space has minimum distance from the origin. First or second-order methods require an explicit analytical expression of either the entire limit-state function g(x) or of its limit-state surface g(x)=0 in the space of random variablesx. This is due to the fact that these methods require not only knowledge of the functions's analytical expression but also of its gradient in the vicinity of the limit-state surface. Therefore, the limit-state function is usually approximated by means of the RS method, in the case of unknown expression.Seven metaheuristic optimization algorithms are tested in this paper that appear to be very promising as they have been implemented in various challenging problems with success.GA method is probably the best-known evolutionary algorithm, receiving substantial attention. The first attempt to use evolutionary algorithms took place in the sixties by a team of biologists [24] and was focused in developing a computer software that would simulate the evolution process of the nature. However, the model implemented in GA refers to that introduced and studied by Holland and co-workers [25]. In the basic genetic algorithm each population member is a binary or real valued string, which is sometimes referred to as genotype or chromosome.The three main steps of basic GA implemented in the current study are: (i) Initialization that corresponds to the generation of the initial population memberssi,g=1 (i=1,….,NP), where NP is the population size, which in our implementation are encoded as binary strings. After creating the initial population, each member of the population is evaluated by computing the corresponding objective and constraint functions. (ii) Selection operator is applied to the current population in order to create an intermediate one. In the first generation the initial population is considered as the intermediate one, while in the next generations this population is created by the application of the selection operator. (iii) Generation (Crossover & Mutation). In order to create the next generation crossover and mutation operators are applied to the intermediate population. Crossover operator forms new chromosomes by combining parts of each of the two parental chromosomes (pcis the crossover probability). Many types of crossover have been proposed, in the present study the uniform crossover operator is adopted. Mutation is a reproduction operator that forms one new chromosome by making (usually small, with mutation probability pm) alterations to the values of the genes in a copy of a single parent chromosome and serves to recover lost alleles. For binary encoding each gene may have an allele of 1 or 0. The purpose of mutation operator is to maintain diversity within the population and inhibit premature convergence. The process of moving from the current population to the next population constitutes one generation in the evolution process of a GA model. If the termination criterion is satisfied the procedure stops otherwise returns to the selection step. The flowchart of the GA method implemented in the current study is presented in Fig. 1.In particle swarm optimization, multiple candidate solutions coexist and collaborate [26]. Each solution is designated as particle, having velocity and position into the multidimensional space while a population of particles is called a swarm (NP is the swarm size). Each particle “flies” into the problem search space seeking for the optimal position. During the iterations of the algorithm each particle adjusts its position and velocity based on its own experience, built by memorizing the best position encountered, as well as that of neighbouring particles. PSO algorithm combines local search (by means of self-experience) and global search (through neighbouring experience), aiming to balance exploration with exploitation. For each particle velocity and position are updated as follows:(8)vj,t+1=w⋅vj,t+c1⋅r1⋅sPb,j−sj,t+c2⋅r2⋅sGb−sj,t(9)sj,t+1=sj,t+vj,t+1wherevj,tis the velocity vector of particle j at iteration t,sj,tis the position vector of particle j at iteration t, vectorsPb,jis the personal best ever position of particle j, and vectorsGb is the global best location found by the swarm. The acceleration coefficients c1 and c2 indicate the degree of confidence in the best solution found by individual particle (c1 – cognitive parameter) and by the swarm (c2 – social parameter), respectively, while r1 and r2 are two random vectors uniformly distributed in the interval [0,1]; a schematic representation of PSO is given in Fig. 2.Storn and Price [27] proposed a floating point evolutionary algorithm for global optimization and named it differential evolution (DE). DE is a direct search method which utilizes a population of NP parameter vectorssi,g(i=1,…,NP) for each generation g. Several variants of DE have been proposed so far [28], but the two most widely used are the following ones, while schematically they are presented in Fig. 3.Scheme DE1. In the first variant, a donor vector vi,g+1 is generated first according to:(10)vi,g+1=sr1,g+F⋅(sr2,g−sr3,g)This step is equivalent to the mutation operator step of genetic algorithms or evolution strategies. Integers r1, r2 and r3 are chosen randomly in the interval [1,NP] while i≠r1, r2 and r3. F is a real constant value, called mutation factor, which controls the amplification of differential variation(sr2,g−sr3,g). In the next step crossover operator is applied in order to generate the trial vectorui,g+1 which is defined from the elements ofsi,gand those of the donor vectorvi,g+1 with probability CR as follows:(11)uj,i,g+1=vj,i,g+1ifrandj,i≤CRorj=Irandsj,i,gifrandj,i>CRorj≠Irandi=1,2,…,NPandj=1,2,…,nwhere randj,i∼U[0, 1], Irandis a random integer defined in [1,2,…,n] that ensures thatvi,g+1≠si,g. The last step of the generation procedure is the implementation of the selection operator where vector si,g, is compared to trial vectorui,g+1:(12)si,g+1=ui,g+1iff(ui,g+1)≤f(si,g)si,gotherwisei=1,2,…,NPScheme DE2. In the second variant the donor vector vi,g+1 is generated first according to:(13)vi,g+1=si,g+K⋅(sbest,g−si,g)+F⋅(sr2,g−sr3,g)where an additional control variable K is used. The generation of the trial vectorui,g+1 as well as the decision process are the same with those of DE1. In our work DE2 has been implemented since it was found to perform better that the first one [29].The harmony search algorithm was inspired by the improvisation process of Jazz musicians [6]. According to the analogy between improvisation and optimization, each musician (saxophonist, bassist, guitarist, etc.) corresponds to one decision variable; each musical instrument's pitch range corresponds to decision variable's value range. Musical harmony corresponds to the solution vector, and audience's aesthetics corresponds to the objective function. HS algorithm consists of the following steps: parameter initialization, harmony memory initialization, new harmony improvisation and harmony memory update (see Fig. 4).In the first step, the optimization problem is formulated selecting the n decision variables along with the range of each decision variable i(siL≤si≤siU,i=1,2,…,n). Algorithmic parameters are also defined in this step: HMS is the harmony memory size that corresponds to the number of simultaneous solution vectors stored in harmony memory, HMCR is the harmony memory considering rate and PAR is the pitch adjusting rate.In the second step, harmony memory (HM) is initialized with HMS randomly generated solution vectors defining musician's harmony memory matrix:(14)HM=s11s21s31...sn1s12s22s32...sn2............s1HMSs2HMSs3HMS...snHMSIn the third step, new harmony vectors are improvised following three rules: memory consideration, random selection and pitch adjustment. According to the random selection, the value of decision variable si is chosen randomly from pitches stored inHM=[si1,si2,⋯,siHMS]with probability HMCR (0≤HMCR≤1) or according to the memory consideration is chosen randomly with a probability (1-HMCR) within its value range, similar to a musician that plays any pitch within the instrument's pitch range:(15)si=si∈[si1,si2,⋯,siHMS]withprobabilityHMCRsiL≤si≤siUwithprobability(1−HMCR)After the value siis randomly chosen, it can further be adjusted into neighbouring values, with probability HMCR·PAR (0≤PAR≤1) while the original pitch obtained in HM consideration is preserved with a probability HMCR·(1−PAR):(16)si=si(k+m)withprobabilityHMCR⋅PARsiwithprobabilityHMCR⋅(1−PAR)If the new generated harmony vector is better than the worst harmony vector of HM, with reference to the objective function value, the worst harmony is replaced by the new harmony vector.The covariance matrix adaptation algorithm, proposed by Hansen and Ostermeier [30] is based on a completely derandomized self-adaptation scheme following the subsequent rules. First, the covariance matrix of the mutation distribution is changed in order to increase the probability of reproducing the selected mutation step. Second, the rate of change is adjusted according to the number of strategy parameters to be adapted. Third, the expectation of the covariance matrix is stationary under random selection. The adaptation mechanism is inherently independent of the given coordinate system. The transition from generation g to g+1 is described by the following steps (shown schematically in Fig. 5):Create λ new offsprings as follows:(17)skg+1∼Nm(g),σ(g)2C(g)∼m(g)+σ(g)N0,C(g)wheresk(g+1)∈ℜnis the design vector of the kth offspring in generation g+1 (k=1, …, λ), N(m(g), C(g)) are normally distributed random numbers where m(g)∈ℜnis the mean value vector and C(g) is the covariance matrix whileσg∈ℜ+is the global step size. In order to perform a generation step, the new mean value vector m(g+1), global step size σ(g+1) and covariance matrix C(g+1) have to be defined first.After operating selection scheme (μ, λ) over λ, where μ is the number of parents and λ is the number of offsprings, the new mean value vector m(g+1) is calculated according to the following expression:(18)m(g+1)=∑i=1μwisi:λ(g+1)wheresi:λ(g+1)is the ith best offspring and wiare weight coefficients defined as follows:(19)wi=ln(μ+1)−lni∑j=1μ(ln(μ+1)−lnj),∑i=1μwi=1,wi>0,i=1,…,μThe new global step size is defined according to the following expression:(20)σ(g+1)=σ(g)expcσdσpσ(g+1)EN0,I−1where dσand cσconstants are defined as follows:(21)cσ=μeff+2n+μeff+3dσ=1+2max0,μeff−1n+1−1+cσ,μeff=∑i=1μwi2−1EN0,Iis the expected value of the Euclidean norm of a normally distributed vector, andpσ(g+1)is the conjugate evolution path(pσ(0)=0), that is given by:(22)pσ(g+1)=1−cσpσ(g)+cσ2−cσμeffC(g)−1/2m(g+1)−m(g)σ(g)where the matrixCg−1/2is given by:(23)Cg−1/2=BgDg−1BgTwhere the columns ofBgare an orthogonal basis of the eigenvectors ofCgand the diagonal elements ofDgare the square roots of the corresponding positive eigenvalues.The new covariance matrixCg+1is defined using the following expression:(24)Cg+1=1−ccovCg+ccovμcovpcg+1pcg+1T+ccov1−1μcov∑i=1μwiOPsi:λg+1−mgσgOP symbolises the outer product of a vector with itself andpcg∈ℜnis the evolution path(pc(0)=0)which is given by equation:(25)pcg+1=1−ccpcg+Hσg+1cc2−ccμeffmg+1−mgσgwhere:(26)cc=44+n,μcov=μeffccov=1μcov2n+22+1−1μcovmin1,2μeff−1n+22+μeffThe elitist CMA evolution strategy is a combination of the well-known (1+λ)-selection scheme of evolution strategies [31], with covariance matrix adaptation [32]. The original update rule for the covariance matrix can be reasonably applied to the (1+λ)-selection scheme too. While, the cumulative step size adaptation (path length control) of (μ/μ, λ)-CMA is replaced by a success rule based step size control. Every individual a of ECMA is composed of five components:(27)a=s,p¯succ,σ,pc,Cwheresis the design vector,p¯succis a parameter that states the success rate during the evolution process, σ is the step size, pcis the evolution path and C is the covariance matrix of the mutation strengths. Contrary to CMA algorithm, each individual has its own step size σ, evolution path Pcand covariance matrix C. A pseudo code of ECMA algorithm is given in Fig. 6(a), where in line #1 a new parentaparentgis generated. In lines #4–6, λ new offsprings are generated from the parent vectoraparentg. The new offsprings are sampled according to Eq. (17), with variable m(g) being replaced by the parent vectorsparent(g). After λ new offsprings are sampled, parent's step size is updated by means of UpdateStepSize subroutine (see Fig. 6(b)). The arguments of the subroutine are the parentaparentgand success rateλsuccg+1/λ, whereλsuccg+1is the number of offsprings having better fitness function than the parent. The step size update is based on the 1/5 success rule. When the ratioλsuccg+1/λis larger than 1/5, step size increases otherwise step size decreases. If parent vector has worst fitness value than the best offspring, this becomes the parent of next generation (see lines #8–9). If the inequality of line #8 is satisfied, then the covariance matrix of new parent is updated by means of UpdateCovariance subroutine (see Fig. 6(c)). The arguments of the subroutine are the current parent and the step change:(28)sparentg+1−sparentgσparentgThe update of evolution path and covariance matrix depends on the success rate:(29)p¯succ=λsuccλIf the success rate is below a given threshold value pthreshthen the step size is taken into account and the evolution path and covariance matrix are updated (see lines #2–3 of Fig. 6(c)). If the success rate is above the given threshold pthreshthe step change is not taken into account and the evolution path and covariance matrix are updated (see lines #5–6). The pre-calculated values of the strategic parameters are given by the following expressions:(30)d=1+n2λ(31)psucctarget=15+(λ/2)(32)cp=psucctargetλ2+psucctargetλwhere d is a damping parameter used in subroutine UpdateStepSize, n is the number of design variables, cpis a normalization constant of thep¯succparameter used in subroutine UpdateStepSize,psucctargetis a constant that depends on the number of offsprings λ.Artificial bee colony [5] is a population based algorithm where the position of a food source represents a possible solution to the optimization problem and the nectar amount of a food source corresponds to the quality (fitness) of the associated solution. In particular, a population of bees composes a colony of artificial bees (NP is the bees’ population size) that consists of three groups: employed bees, onlookers and scouts. Onlookers watch the dances of the employed bees and choose the food sources depending on the dances. While the employed bees go to the food source and return to hive and dance in this area. The employed bee, whose food source has been abandoned, becomes scout and starts to search for finding a new food source. The number of the employed bees and onlookers is equal to the number of the solutions in the population.The main steps of the ABC algorithm are: (i) initially a randomly distributed population (food source position for each employed bee) is generated. (ii) Employed bees determine a food source position within the neighbourhood in their memory. The new solution to the optimization problem corresponding to a new food source position, for the cycle t+1, is generated according to the following expression:(33)si,jt+1=si,jt+ϕi,j(si,jt−sk,jt)where k∈{1, 2, …, NP} with k≠i while j∈{1, 2, …, n} are randomly chosen indices, φi,jis a random number between [–1,1]. Parameter j is a randomly selected index and is generated with uniform distribution into the discrete set {1,2,…,n} during the iterations of the algorithm. (iii) After all employed bees complete the search process; they share the new position information of the sources with the onlookers on the dance area. The preference of a food source by an onlooker bee depends on nectar amountf(sit)of the food source. An onlooker bee chooses a food source depending on the probability value associated with that food source, pi, defined by the following formula:(34)pi=f(si(t))∑j=1NPf(sj(t))wheref(sit)is the fitness value of the solution i evaluated by its employed bee, which is proportional to the nectar amount of the food source in the position i. In this way, the employed bees exchange their information with onlookers. Similar to the employed bees, onlookers determine a neighbour of the chosen food source and check its nectar amount (Eq. (33)). Provided that its nectar amount is better than that of the previous one, bee memorizes the new position and forgets the old one. (iv) The abandoned food sources are determined and they are replaced with the new food sources discovered by the scouts. A schematic representation of the ABC algorithm is given in Fig. 7.Artificial neural networks are biologically inspired since their components, known as artificial neurons, simulate the elementary functions of biological neurons. NN are organized based on brain's anatomy and exhibit a surprising number of brain's characteristics such as: learning from experience, generalizing from previous examples and abstracting essential characteristics from sets of inputs containing irrelevant data. The use of hidden layers and nonlinear activation functions enhance the ability of NN to “learn” nonlinear relationships between sets of input–output data.A feed-forward NN attempts to create the desired mapping between inputs and target outputs of the training set, that is composed by m input-target pairs D=[xm, tm]. A Neural Network architecture A consists of specific number of layers, number of units in each layer and form of activation function. For a given set of weight parameterswis assigned to the network connections, a specific mapping y(xm; w, A) between input xmand output y vectors is defined. The quality of this mapping is measured using the following error function:(35)ED(D|w,A)=∑m12y(xm;w,A)−tm2Learning algorithms try to define weight parameters w aiming to minimize the error function ED value. Iterative minimization algorithms are therefore used to determine optimized weight parameters w. Operator O is applied for the solution of the minimization problem, resulting to the following iterative formula:(36)w(t+1)=O(w(t))=w(t)+Δw(t)Most of the numerical minimization methods are based on the above expression, while to initiate the algorithm a starting vector of weight parameters w(0) is required. The varying part of the algorithm Δw(t) can be further decomposed into:(37)Δw(t)=atd(t)where d(t) specifies the direction of search and atis the corresponding step size.Learning algorithms are classified into local or global algorithms. Global algorithms make use of the knowledge about the state of entire network, such as direction of overall weight update vector. In contrast, local adaptation strategies are based on specific information of weight values such as temporal behaviour of weights’ partial derivative. The local approach is better related to the NN concept of distributed processing, where computations are performed independently. Furthermore, for many applications local strategies achieve faster and more reliable predictions than global techniques (see for example in [33]). In the widely used back-propagation global learning algorithm the gradient descent algorithm is used, while in this study the Resilient backpropagation learning algorithm, abbreviated as Rprop [34], is adopted. Rprop is a local algorithm, based on an adaptive version of the Manhattan-learning rule that has been proved efficient in the past [35].RS method was originally proposed by Box [36] as a statistical tool, to find the operating conditions of a chemical process at which some response was optimized. In order to reduce computational effort while maintaining an acceptable accuracy, two important issues should be considered when applying RS method to failure probability: (i) Definition of experimental points used for approximating the limit-state function, (ii) Analytical expression of response surface function [16], usually a quadratic RS function is used:(38)g¯(x)=a+∑i=1nbixi+∑i=1ncixi2defined in the n-dimensional random variable space where constants a, biand ciare determined by evaluating g(x) at specified experimental points, while xi, i=1,…,n are the random variables.RS method was found to be significantly influenced by the polynomial representation and the way experimental points are generated. In an effort to improve robustness of the approximation procedure, the polynomial representation of RS is replaced by an approximation derived by neural networks. Hurtado [17] confirmed with statistical learning concepts the sensitivity of RS approximation based on polynomial expressions compared to neural network approximations. The global capabilities of neural network approximator were examined [37,38] in the framework of FORM for simple analytic limit-state functions or academic structural reliability problems. The local approximation capabilities of neural networks in conjunction with reliability analysis methods were examined in a simple portal frame by Deng et al. [39]. In this work, neural networks are implemented as approximator of the limit-state function both for MCS and FORM. In the first step a number of samples are generated through the Latin hypercube sampling (LHS) technique [40]. This sampling method can generate a variable number of samples well distributed over the entire range of interest. The main advantage of LHS technique is that it produces well distributed samples of almost any sample size of experimental points. In the next step sample points are used to define the NN approximation of the limit state function according to the following equation:(39)g¯(x)=f∑i=1kwH,ij⋅f∑i=1nwO,ij⋅xkwhere k is the number of hidden layer neurons, n is the number of inputs for NN that is equal to the number of random variables, wH and wO are the weight parameters of hidden and output layers, respectively, and f(x) is the sigmoid transfer function:(40)f(y)=11+exp(−γy)The coefficient γ defines the slope of the sigmoid function. By the time NN is trained and weight parameters are defined, MCS or FORM is performed in order to define the reliability index β.Limiting uncertainties and increasing safety, during the design of structural or geostructural systems, is an important issue to be considered. Probabilistic analysis of structures or geostructures, which is implemented to define the probability that system meets some specified demands for a specified time period under specified environmental conditions and is used as a measure to evaluate the reliability of the system. The performance function of structural or geostructural systems must be determined to describe the systems’ behaviour and to identify the relationship between the basic parameters of the systems. It should be noted that in case of earthquake prone environment uncertainties related to seismic demand and structure's or geostructure's capacity are strongly coupled.The main scope of the present study is to compare the MCS based reliability analysis procedure in terms of computational efficiency and robustness with the MO-FORM method presented in this study. In the case of MO-FORM method an optimization problem is solved that is formulated as follows:(41)minz∈FzT⋅C−1⋅zsubjecttog(z)=0where z is a vector of reduced variables and is defined according to Eq. (6),Cis the correlation matrix and g(z) is the limit state function, while F is the feasible part of the design space. To handle equality constraints in MO, they are transformed into inequality constraints as follows [41]:g(z)−ε≤0, where ɛ is the tolerance allowed (corresponding to a small value). For comparative reasons the method adopted for handling the constraints is the same for all seven metaheuristic optimization algorithms. In particular, the simple yet effective, multiple linear segment penalty function [42] is used in this study for handling constraints. According to this technique if no violation of the constraints is detected, then no penalty is imposed on the objective function, otherwise, a relative to the maximum degree of constraints’ violation penalty is applied to the objective function.The performance of the metaheuristics is influenced by the selection of their control parameters; in order to identify the best combination of these parameters for each metaheuristic algorithm, 30 combinations of the parameters were generated by means of LHS, while for each combination 100 runs were performed in order to calculate mean and coefficient of variation (COV) with reference to the optimized objective function value [43]. This numerical investigation was performed for the first and second test examples (liquefaction and concrete dam problems) which are easy to handle and the resulting parameters are used for all four problems. The algorithmic parameters are usually set empirically, in the current study those that were identified for each algorithm are based on the ranges successfully used in previous studies by the authors [42,44]: (i) For GA, population size (NP) defined in the range of [50,200], mutation probability (pm) defined in the range of [0,0.05] and crossover probability (pc) defined in the range of [0,1]. (ii) For PSO, number of particles NP defined in the range of [50,200], inertia weight (w) defined in the range of [–1,1], while cognitive parameter (c1) and social parameter (c2) both defined in the range of [–5,5]. (iii) For DE, population size (NP) defined in the range of [50,200], probability (CR), constant (F) and control variable (K) all defined in the range of [0,1]. (iv) For HS, harmony memory size (HMS) defined in the range of [50,200], while harmony memory consideration rate (HMCR) and pitch adjusting rate (PAR) both defined in the range of [0,1]. (v) For CMA, number of parents (μ) and number of offsprings (λ) both defined in the range of [50,200]. (vi) For ECMA, number of offsprings (λ) defined in the range of [50,200]. (vii) For ABC, colony size (NP) defined in the range of [50,200].For all seven algorithms the initial population is generated randomly with the use of LHS in the range of design space for each test example examined, while for the implementation of all seven algorithms, except of GA method, the real valued representation of the design vectors is adopted. For comparison purposes, the termination criterion for the sensitivity analysis performed is the same for all seven metaheuristic optimization algorithms. In particular, the termination criterion used is the number of function evaluations that was set to 200,000 [45], while for the implementation of MO-FORM procedure in order to be compared with MCS, the algorithm terminates if for the last 10 generations/iterations no improvement is achieved.Soil liquefaction is a type of ground failure associated with earthquakes. This type of failure takes place when the active stress of soil exceeds soil resistance as a result of an increase in pore water pressure during earthquake vibration. During liquefaction, the behaviour of soil is similar to the behaviour of fluids. Soil liquefaction can cause major damages into structures and infrastructures such as buildings, bridges, dams etc. To date, several methods have been developed for evaluating liquefaction phenomena, most of which, however, take into account a limited number of parameters. In fact, analysis of failure due to liquefaction is affected by many seismic and soil-geotechnical parameters.First and second test examples were employed mainly for the calibration of the seven algorithms due to its simplicity to handle. In this test example the probability of liquefaction is studied and for this purpose the expression derived by Sen and Akyol using NN [46] is used for defining if liquefaction occurs or not:(42)LI=−5.13⋅SSSR7.54.39+2.29⋅lnrd1.6+1+9.91⋅D501.31⋅SSSR7.51.40−P1⋅lnD506.38+1−0.06⋅lnqc2.62+1⋅rd5.11−P2⋅lnD507.74+1⋅GWT4.48−0.88where the parameters that are involved into the above expression are considered as random variables. The description of these parameters is provided in Table 1along with the type of random distribution and its characteristics, while P1 and P2 are defined as follows:(43a)P1=0.0ifσvoσ′vo≤0.8388.97ifσvoσ′vo>0.838(43b)P2=0.0ifGWTz≤0.5558.97ifGWTz>0.555The reliability problem is defined as follows:(44)PLI=P(LI>0.50)The calculation of the probability that liquefaction occurs (PLI) can be achieved either with MCS method (Eq. (2)) or by means of FORM method which is equivalent with solving the optimization problem of Eq. (7). In the latter case the optimization problem is formulated as follows:(45)minz∈FzT⋅C-1⋅zsubjecttoLI(z)−0.50−a⋅0.50≤0whereg(z)=LI(z)−0.50=0(limitstatefunction)where z is a vector of the reduced variables defined according to Eq. (6) andCis the correlation matrix, in this problem is equal to the unit matrixI(i.e. there is no correlation between the random variables), while parameter a was set equal to 0.01, that defines the feasible part of design space F. Lower and upper bounds of the decision variables of the problem described in Eq. (45) are given in Table 1 and the decision variables of the problem are continuous defined in these bounds. For the solution of the problem of Eq. (45) seven MO methods are considered and they are compared with the reference probability obtained by means of the MCS method that is equal to 45.9%.Tables 2–8present the sensitivity of the seven algorithms with respect to the parameters varied. This sensitivity analysis is carried out in order to examine the influence of the seven MO with reference to their parameters and thus proving their robustness. In order to identify the best combination of the parameters for each metaheuristic algorithm, 30 combinations of the parameters are generated by means of LHS, while for each combination 100 optimization runs are performed in order to calculate mean and coefficient of variation of the optimized objective function value. The resulting optimization runs performed for defining the parameters of the seven metaheuristics are equal to 7 metaheuristics×30 combinations×100 runs=21,000 optimization runs.In these seven tables mean and coefficient of variation of the optimized objective function values are depicted for each combination of algorithmic parameters; the lower COV values are the better is. This is because low COV means that the algorithm is not influenced by the independent runs. In these tables the best combination (with respect to COV value) is denoted with black bold letters. As it can be seen in Table 2 corresponding to GA, COV of the optimized objective function value for the different parameters of the algorithms varies from 3.8% to 26.7%, while the mean value is 12.5% and the mean value of the probability achieved is 45.6%. In the case of PSO, as shown in Table 3, COV varies from 4.5% to 31.9%, while the mean value is 19.4% and the mean value of the probability achieved is 45.8%. In the case of DE, as shown in Table 4, COV varies from 0.12% to 26.7%, while the mean value is 11.1% and the mean value of the probability achieved is 45.4%. In the case of HS, as shown in Table 5, COV varies from 4.1% to 27.7%, while the mean value is 16.8% and the mean value of the probability achieved is 45.0%. In the case of CMA, as shown in Table 6, COV varies from 7.1% to 9.9%, while the mean value is 8.3% and the mean value of the probability achieved is 44.8%. In the case of ECMA, as shown in Table 7, COV varies from 1.6% to 13.3%, while the mean value is 6.0% and the mean value of the probability achieved is 42.0%. Finally, in the case of ABC, as shown in Table 8, COV varies from 3.8% to 21.6%, while the mean value is 11.8% and the mean value of the probability achieved is 45.5%. The best performance with reference to the mean optimized objective function value is achieved by PSO, varying only by 0.28% with reference to the value achieved by the MCS method; while the worst one is achieved by ECMA varying by 8.6%. On the other hand the best performance with reference to the influence of the parameters of the algorithms is ECMA with mean value of the COV 6.0% and the worst one is PSO with 31.9%. DE, GA and ABC can be considered as the compromise best algorithms, for this test example, since the mean optimized objective function value varies by 0.68% to 1.11% and COV 11.1% to 12.5%.A large concrete arch dam has been considered as the second test example used to illustrate the feasibility of MO-FORM. The objective is to calculate the probability of failure of the dam, computed for given water level and for predefined damage level of the dam. The probability of failure is estimated using either MCS or MO-FORM reliability analysis procedures in conjunction with the continuum strong discontinuity approach (CSDA) [47,48], while neural networks are also used for predicting the peak load of the dam. The uncertain properties (modelled as random variables) considered for the dam are the Young's modulus, Poisson's ratio, tensile strength and specific fracture energy of concrete. All random variables are assumed to be Gaussian. This problem corresponds to the benchmark test presented in the 7th International Benchmark Workshop [49], which was reproduced using CSDA. The SCALERE dam is an arch dam located in the centre-north of Italy and was constructed in 1910–1911. The foundation rock consists of stratified Eocene sandstones dipping upstream. The dam has not been provided with contraction joints, while the crest elevation is at 830.5m a.s.l., maximum height 34.0m and crest length 158.0m.The use of neural networks was motivated by the approximate concepts inherent in reliability analysis and time consuming repeated analyses required for either MCS or MO-FORM. The Rprop algorithm is implemented for training NN utilizing available information generated from selected CSDA analyses. The trained NN is then used in the context of reliability analysis procedure to compute the peak load capacity of the concrete dam achieved for different sets of basic random variables leading to close prediction of the probability of failure. This way it is made possible to obtain rigorous estimates of the probability of failure for the concrete dam.The geometry of the concrete dam has been taken from [50]. Only a small part of the foundation was modelled because the analysis is addressed to the dam structural collapse. Nevertheless, in order to recover the actual structural response, appropriate boundary conditions have been considered, the finite element mesh is displayed in Fig. 8. The kinematical restrictions that were used, are the following: the lateral and bottom surfaces are clamped (surface A, B and C in Fig. 8(b)), while the other ones are free. The loading action to trace the structural response consists of the so called increasing fictitious water density strategy: the hydrostatic pressure, acting on the up-stream wall of the dam for the full reservoir case, is multiplied by a loading factor (starting from zero) while the self-weight of the dam is being kept constant. The maximum attained load factor would be that determining the safety factor for the most unfavourable action (the hydrostatic pressure). Certainly, since the hydrostatic pressure is proportional to the water density, this is completely equivalent to apply the load factor to that water density resulting in a “fictitious” water density. This is not the most common scenario adopted in dam design, but it is considered more acceptable from the structural failure point of view. Eventually, the resulting pressure for every load factor could be translated into an equivalent height of the water above the dam crest (increasing overflow strategy).The adopted mechanical properties are the following: both materials (rock and concrete) have been modelled by an isotropic continuum damage model [51]. The analysis assumes uncertainty for the concrete modelling and deterministic for the rock foundation (for reasons of comparison with the previous work of the authors [52]); since it is evaluated the “structural collapse” rather than the “structure-foundation collapse”. The rock foundation is modelled providing appropriate boundary conditions. The mechanical parameters of rock and concrete are summarised in Tables 9 and 10, respectively. Mean values and standard deviations for the four random variables taken into account are given in Table 10.The principal hypotheses considered are the following: (i) Two materials are modelled: rock and concrete. (ii) No interface elements between rock foundation and concrete are used. (iii) Rock was assumed as deterministic. (iv) The four concrete material properties (Young's modulus, Poisson's ratio, tensile strength and fracture energy) are defined taking into account randomness using probability density functions. (v) A unique numerical scenario was adopted. (vi) Linear geometric analyses are performed. (vii) Hydraulic fracture phenomena was included. (viii) A multi-crack procedure was considered, that allows for multiples failure surfaces.Numerical results were obtained, in terms of the load factor, for a number of sets of basic random material properties. The iso-displacement maps (Fig. 9(a)) and the geometry in the deformed configuration (Fig. 9(b)) display the typical failure mechanism. It is formed by the conjunction of two primary macro cracks propagating across the concrete bulk, as shown in Fig. 10(a) and (b). The set of CDSA analysis results obtained from [52] are used for NN training. In particular a fully connected NN is used, where each node in a layer is connected to all nodes of previous and next layers. The NN configuration adopted in this study has four input nodes (Young modulus, Poisson's ratio, tensile strength and fracture energy of the concrete), one hidden layer with 20 hidden nodes and one output node (critical load factor λCL).Once the NN based MCS and MO-FORM are established, close predictions of the critical load factor (λCL) for different combination of the random variables are obtained, thus the NN response surface can be used in order to calculate the probability failure of the dam for a given level of external loading. From [52] it was obtained the probability of failure Pf=5×10−7% achieved with MCS after 108 simulations. The structural collapse damage level is considered where it is assumed to correspond to the 100% of the peak load and the critical load factor is less than 3.0. The reliability problem is defined as follows:(46)Pf=P(λCL<3.0)The calculation of the probability of failure can be performed either with the MCS method (Eq. (2)) or by means of the FORM method which is equivalent with solving the optimization problem of Eq. (7). In the latter case the optimization problem is formulated as follows:(47)minz∈FzT⋅C-1⋅zsubjecttoλCL(z)−3.0−a⋅3.0≤0whereg(z)=λCL(z)−3.0=0(limitstatefunction)where z is a vector of reduced variables defined according to Eq. (6) andCis the correlation matrix, that is equal to the unit matrixI(since, there is no correlation between the random variables), while parameter a was set equal to 0.01, that defines the feasible part of design space F. Lower and upper bounds of the continuous decision variables of the problem of Eq. (47) are given in Table 10.A similar sensitivity analysis of the seven algorithms to that performed for the previous test example is also presented in Tables 2–8 for the concrete dam test example. In particular, in Table 2 corresponding to GA, COV of the optimized objective function value for the different parameters of the algorithms varies from 4.8% to 52.8%, while the mean value is 16.9% and the mean value of the probability achieved is Pf=3.7×10−7%. In the case of PSO, as shown in Table 3, COV varies from 16.5% to 29.4%, while the mean value is 23.4% and the mean value of the probability achieved is Pf=4.7×10−7%. In the case of DE, as shown in Table 4, COV varies from 0.01% to 20.9%, while the mean value is 5.9% and the mean value of the probability achieved is Pf=4.6×10−7%. In the case of HS, as shown in Table 5, COV varies from 7.5% to 19.7%, while the mean value is 14.8% and the mean value of the probability achieved is Pf=6.9×10−7%. In the case of CMA, as shown in Table 6, COV varies from 8.8% to 11.8%, while the mean value is 10.2% and the mean value of the probability achieved is Pf=2.1×10−7%. In the case of ECMA, as shown in Table 7, COV varies from 0.0% to 27.5%, while the mean value is 4.7% and the mean value of the probability achieved is Pf=6.5×10−7%. Finally, in the case of ABC, as shown in Table 8, COV varies from 3.4% to 37.9%, while the mean value is 12.2% and the mean value of the probability achieved is Pf=5.3×10−7%. The best performance with reference to the mean optimized objective function value is achieved by ABC, varying only by 5.21% with reference to the value achieved by the MCS method; while the worst one is achieved by CMA varying by 57.5%. On the other hand the best performance with reference to the influence of the parameters of the algorithms is ECMA with mean value of the COV 4.7% and the worst one is PSO with 23.36%. DE and ABC can be considered as the compromise best algorithms, for this test example, since the mean optimized objective function value varies by 5.21% to 9.1% and COV 5.9% to 12.2%.In the third test example, probabilistic slope stability analysis of the embankment shown in Fig. 11is performed, using the Bishop's method. For the needs of pseudostatic slope stability analyses both pseudostatic horizontal acceleration (PHA) and pseudostatic vertical acceleration (PVA) are taken into account. In accordance to seismic design codes [53], vertical acceleration was set equal to PVA=±0.50×PHA, to account for the vertical pseudostatic inertial force. In the current study, pseudostatic analyses were conducted in which the required slope stability analyses were performed using a computer code developed by Zania et al. [54] that utilizes the simplified Bishop's method for dry conditions and assume circular failure surfaces within the body of the embankment (see Fig. 12). In addition, it is capable of randomly modifying mechanical and geometric characteristics of the model, as well as the pseudostatic horizontal acceleration values required by probabilistic analyses performed via both MCS and FORM methods.Uncertainty on capacity is taken into consideration through the soil mechanical properties and more specifically the unit weight (γ), friction angle (φ) and cohesion (c), as well as the embankment's geometry. The geometry of the simple trapezoid embankment is determined using three parameters: height, slope width and deck width (see Fig. 11). The normal distribution is used for the basic parameters encountered in pseudostatic slope stability analyses (i.e., geometry, unit weight, cohesion and friction angle), while lognormal distribution is used for seismic demand (PHA). The mean values and coefficient of variation values for the soil mechanical properties and the geometry are given in Table 11. Cohesion depicts increased scattering, thus larger COV value is used, than the other mechanical properties of the embankment.The reliability problem is formulated as to calculate the probability that the safety factor (SF) is lower than 2.0, this value corresponds to the very high safety margin limit state according to the characterization of the limit states given in Table 12. The reliability problem is defined as follows:(48)PLS=P(SF<2.0)The calculation of the limit state probability (PLS) that SF is lower than 2.0, can be performed either with the MCS method (Eq. (2)) or by means of the FORM method which is equivalent with solving the optimization problem of Eq. (7). In the latter case the optimization problem is formulated as follows:(49)minz∈FzT⋅C-1⋅zsubjecttoSF(z)−2.0−a⋅2.0≤0whereg(z)=SF(z)−2.0=0(limitstatefunction)where z is a vector of the reduced variables defined according to Eq. (6) andCis the correlation matrix, that is equal to the unit matrixI(since, no correlation between the random variables is considered), while parameter a was set equal to 0.01, that defines the feasible part of design space F. Lower and upper bounds of the continuous decision variables of the problem of Eq. (49) are given in Table 11. In order to assess the MO-FORM variants, the reference basis was defined first using MCS method (107 simulations) resulting into PLS=6.63×10−5%.Fig. 13depicts the efficiency of the MO-FORM method compared to MCS with reference to the estimated probability. For the implementation of the seven MO, the parameters selected are those corresponding to the combination (shown in Tables 2–8) achieving the best compromise performance for both test examples (liquefaction and concrete dam) examined, while the termination criterion used is that the algorithm terminates if for the last 10 generations/iterations no improvement is achieved. PSO, DE and HS resulted in very close values; the variation with reference to the results obtained by MCS method is lower than 4% while ABC, CMA and ECMA varied up to 13%, on the other hand GA vary by 25%. Fig. 14depict the number of function evaluations required for all seven methods, as it can be seen all optimization methods incorporated into FORM method required considerably less evaluations than MCS (three orders of magnitude less function evaluations), while DE and ECMA require the least function evaluations.For the purpose of the fourth test example a real-world pile-group design is considered for a particular soil type and for a given axial load corresponding to the weight of the superstructure. In particular, the 31-storey building of the Hyde Park Cavalry Barracks in London founded on clay (see Fig. 15) [55], is considered. The building is of 90m height and its weight is equal to 228 MN. It is estimated that at the end of the construction 60% (0.60·228MN=136.80MN) of the building load is carried by the piles and 40% by the raft. In this study various sources of uncertainty are considered: on the ground motion excitation which influences the level of seismic demand and on the modelling and material properties which affects the structural capacity. The characteristics of the random variables are provided in Table 13, and the reliability problem is defined as follows:(50)PEXC=P(df>2.0cm)where dfis the horizontal displacement of the foundation at the superstructure level and the objective is to define the probability of exceeding the limit state of 2.0cm for the case of 10% in 50 years earthquake hazard level.For clay soil conditions an elastic–plastic material exhibiting plasticity in the deviatoric stress–strain response only is employed. The volumetric stress–strain response is linear-elastic and is independent of the deviatoric response. This material law can simulate monotonic or cyclic response of materials whose shear behaviour is insensitive to the confinement change, such as organic soils or clay under undrained loading conditions. During the application of gravity loads, material behaviour is linear-elastic. In the subsequent earthquake (horizontal) loading phase, the stress–strain response is considered elastic–plastic. Plasticity is formulated based on the multi-surface (nested surfaces) concept, with an associative flow rule, while the yield surfaces are of the Von Mises type. Nonlinear static or dynamic analysis needs a detailed simulation of the pile foundation in the regions where inelastic deformations are expected to develop. In order to consider the inelastic behaviour of the piles either the plastic-hinge or the fibre approach can be adopted. The plastic hinge approach has limitations in terms of accuracy particularly in cyclic loading and therefore the fibre beam-column elements are preferred [56]. According to the fibre approach, each structural element is divided into integration sections restrained to beam kinematics, and each one is discretized with fibres having specific material properties. Every fibre in the section can be assigned to different material properties, e.g. concrete, structural steel, or reinforcing bar material properties (see Fig. 16). The sections are located at the Gaussian integration points of the elements. The nonlinear static analyses required for the calculation of the response quantity required are performed using the OpenSees [57] platform, where a bilinear material model with pure kinematic hardening is adopted for the steel reinforcement of the piles. For the simulation of the concrete the modified Kent and Park [58] model, as extended by Scott et al. in [59], is employed. This model was chosen because it allows for an accurate prediction of the demand for flexure-dominated RC members despite its relatively simple formulation. The transient behaviour of reinforcing bars was simulated with the Menegotto-Pinto model [60]. Spring elements are implemented for modelling the interaction between piles and the surrounding soil, in order to simulate the soil-pile interface. Without the use of springs the soil and pile elements move together when subjected to any loading or ground motion. With the use of these springs a more realistic simulation is achieved [61] and the relative displacements between the soil and each pile can be simulated. Tz springs were used for the vertical components of the pile interface and Pysprings for the horizontal components (see Fig. 17). All nodes of the ground base are fully constrained in both x (horizontal) and y (vertical) directions, while the side boundaries are constrained in the x (horizontal) direction.The calculation of the probability (PEXC) of exceeding the limit state of 2.0cm, can be performed either with the MCS method (Eq. (2)) or by means of the FORM method which is equivalent with solving the optimization problem of Eq. (7). In the latter case the optimization problem is formulated as follows:(51)minz∈FzT⋅C−1⋅zsubjecttodf(z)−0.02−a⋅0.02≤0whereg(z)=df(z)−0.02=0(limitstatefunction)where z is a vector of the reduced variables defined according to Eq. (6) andCis the correlation matrix, that is equal to the unit matrixI(since, no correlation between the random variables is implemented), while parameter a was set equal to 0.01, that defines the feasible part of design space F. Lower and upper bounds of the continuous decision variables of the problem of Eq. (51) are given in Table 13. In order to assess the MO-FORM variants, the reference basis was defined first using MCS method (106 simulations) resulting into PLS=3.27×10−4%.Fig. 18depicts the efficiency of the MO-FORM method compared to MCS with reference to the estimated probability. Similar to the previous test example, for the implementation of the seven MO, the parameters selected are those corresponding to the combination (shown in Tables 2–8) achieving the best compromise performance for both test examples (liquefaction and concrete dam) examined, while the termination criterion used is that the algorithm terminates if for the last 10 generations/iterations no improvement is achieved. PSO, DE, HS, CMA and ABC resulted in very close values; the variation with reference to the results obtained by MCS is lower than 7%, while GA and ECMA vary up to 35%. Fig. 19depict the number of function evaluations required for all seven methods, as it can be seen all optimization methods incorporated into FORM method required considerably less evaluations than MCS (two orders of magnitude less function evaluations), while DE and ECMA require the least function evaluations.In order to assess the performance of the metaheuristic based formulation discussed herein, four real-world test examples were considered. In these four real-world geostructural engineering test examples it was shown how metaheuristic optimization algorithms can be incorporated into the reliability analysis procedure in order to calculate performance probabilities using the Hasofer–Lind reliability index. As it was shown, the proposed MO-FORM method requires orders of magnitude less function evaluations than the MCS method, achieving also very good estimations of the associated probabilities.Through the sensitivity analysis performed the algorithmic parameters of the seven MO methods have been tuned for two test problems using different dimensionalities (4 and 10 design variables), while the dimensionalities of the rest two test examples was 7 and 5, respectively. Both test problems have been used in the framework of MO-FORM in order to yield algorithmic parameters that should work well in general for this type of problems. Therefore, it can be said that if you need to tackle with problems with 5 to 10 design variables and you can perform 30,000 function evaluations and more the parameters identified herein is a good choice. On all of the test problems examined herein the results are close to the optimal values achieved by MCS with small variances. Ending, it can be said that between the seven algorithms we have found that DE and ABC outperformed relatively the rest algorithms and the following algorithmic parameters: (i) for DE, NP=89, CR=0.872, F=0.296 and K=0.438, while for (ii) ABC, NP=129, are suggested to be used.

@&#CONCLUSIONS@&#
In this work we present a metaheuristic optimization based approach for solving reliability analysis problems of real-world geotechnical structures. In particular, four test examples are considered, the problem of calculating the probability of liquefaction, the problem of calculating the probability that the SCALERE dam will reach the structural collapse limit state, the problem of calculating the probability that the safety factor of an embankment is below a certain value and the problem of calculating the probability that the horizontal displacement of a pilled foundation will exceed a specific value. For this purpose we assessed and compared seven metaheuristic optimization algorithms for solving the above mentioned reliability analysis problems in a combined framework with the first order reliability method. In particular, the genetic algorithms, differential evolution, harmony search, particle swarm optimization, covariance matrix adaptation evolution strategy, elitist covariance matrix adaptation and artificial bee colony algorithms are employed. The combined framework proposed herein is compared with the Monte Carlo simulation method in terms of computational efficiency and robustness. Close estimation of the probabilities derived with the Monte Carlo simulation method are achieved by most of the seven algorithms (variance by less than 7%).In particular, the combined methods outperformed the implementation of the Monte Carlo simulation method by two or more orders of magnitude in terms of computation time depending on the required number of MCS samples, proving the efficiency of the proposed method. Above all, differential evolution, harmony search, covariance matrix adaptation evolution strategy and elitist covariance matrix adaptation algorithms required less function evaluations compared to the other three algorithms. On the other hand, close estimation of the probabilities is achieved by most of the seven algorithms, variance by less than 7% with reference to those obtained by the Monte Carlo simulation method.