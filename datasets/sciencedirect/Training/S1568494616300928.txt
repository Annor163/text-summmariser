@&#MAIN-TITLE@&#
Application of Legendre Neural Network for solving ordinary differential equations

@&#HIGHLIGHTS@&#
Numerical solution of ordinary differential equations using Legendre polynomial based Functional Link Artificial Neural Network (FLANN).It is a single layer neural network, so number of parameters is less than MLP and the hidden layer is eliminated by expanding the input pattern by Legendre polynomials.Unsupervised back propagation algorithm is used here.Obtained results are compared with the existing methods, plots and tables to show the powerfulness of the methodology.

@&#KEYPHRASES@&#
Differential equations,Feed forward neural network model,Back propagation algorithm,Functional Link Artificial Neural Network,Legendre polynomial,

@&#ABSTRACT@&#
In this paper, a new method based on single layer Legendre Neural Network (LeNN) model has been developed to solve initial and boundary value problems. In the proposed approach a Legendre polynomial based Functional Link Artificial Neural Network (FLANN) is developed. Nonlinear singular initial value problem (IVP), boundary value problem (BVP) and system of coupled ordinary differential equations are solved by the proposed approach to show the reliability of the method. The hidden layer is eliminated by expanding the input pattern using Legendre polynomials. Error back propagation algorithm is used for updating the network parameters (weights). Results obtained are compared with the existing methods and are found to be in good agreement.

@&#INTRODUCTION@&#
It is well known that the differential equations are back bone of physical systems. Many problems in engineering, mathematics, physics, economics etc. may be modeled by ordinary or partial differential equations [1–4]. In most cases analytical solutions of differential equations may not be obtained easily. So various numerical methods such as Runge–Kutta [5], predictor–corrector [6], finite difference, finite element [7], etc. have been developed to solve these equations. These numerical methods require the discretization of domain into the number of finite domains/points where the functions are approximated locally.Recently, various machine intelligence methods in particular Artificial Neural Networks (ANN) are being used to solve initial and boundary value problems. The approximate solutions by ANN have many advantages. The trial solutions of ANN involve a single independent variable regardless of the dimension of the problem. The approximate solutions are continuous over all the domain of integration. Moreover, other numerical methods are usually iterative in nature, where we fix the step size before initiating the computation. After the solution is obtained, if we want to know the solution in between steps then again the procedure is to be repeated from initial stage. ANN may be one of the reliefs where we may overcome this repetition of iterations. Also we may use it as a black box to get numerical results at any arbitrary point in the domain.In 1990, Lee and Kang [8] introduced a method to solve first order ordinary differential equation using Hopfield neural network models. Solution of linear and nonlinear ordinary differential equations using linear B1 splines as basis function in feed forward neural network model has been approached by Meade and Fernandez [9,10]. Liu and Jammes [11] proposed a hybrid numerical method based on both neural network and optimization techniques to solve higher order ordinary differential equations. Lagaris et al. [12] used multi layer perceptron in their network architecture to solve both ordinary and partial differential equations. Malek and Beidokhti [13] solved higher order ordinary differential equations using artificial neural networks and optimization technique. An unsupervised version of kernel least mean square algorithm for solving first and second order ordinary differential equations has been developed by Yazdi et al. [14]. Selvaraju and Samant [15] proposed new algorithms based on neural network for solving matrix Riccati differential equations. Evolutionary algorithm with neural network training has been proposed by Aarts and Van der Veer [16] for solving partial differential equation and initial value problems. Another method for solving mixed boundary value problems on irregular domains have been implemented by Hoda and Nagla [17]. Shirvany et al. [18] used multilayer perceptron and radial basis function (RBF) neural networks with a new unsupervised training method to obtain numerical solution of non linear Schrodinger equation. Mcfall and Mahan [19] introduced an artificial neural network method for solution of mixed boundary value problems with irregular domain. A multi-quadric radial basis function neural network has been used to solve linear differential equations (ordinary and elliptic partial differential equations) by Mai-Duy and Tran-Cong [20]. Recently, Mall and Chakraverty [21,22] proposed regression based neural network model for solving lower as well as higher order ordinary differential equations. Also neural network method has been used by Ibraheem and Khalaf [23] to get the solution of boundary value problems. In another approach, Parisi et al. [24] steady-state heat transfer problem has been solved by using artificial neural network. Raja and Ahmad [25] implemented the solution of boundary value problems of one dimensional Bratu type equations using neural network.A single layer Functional Link Artificial Neural Network (FLANN) model is introduced by Pao and Philips [26]. In FLANN, number of network parameters and number of iterations for training are less than that of multi layer perceptron (MLP) structure. In FLANN the hidden layer is replaced by a functional expansion block for enhancement of the input patterns using orthogonal polynomials such as Chebyshev, Legendre, etc. So single layer FLNN model is computationally efficient and having higher convergence speed. Chebyshev polynomial based Functional Link Artificial Neural Network has extensively applied to nonlinear dynamic system identification [27,28], digital communication [29], channel equalization [30], function approximation [31], etc.

@&#CONCLUSIONS@&#
