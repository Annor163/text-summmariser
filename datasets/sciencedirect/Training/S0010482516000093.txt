@&#MAIN-TITLE@&#
Prediction of time dependent survival in HF patients after VAD implantation using pre- and post-operative data

@&#HIGHLIGHTS@&#
A methodology for the prediction of time dependent survival in patients after LVAD implantation.Data mining techniques applied.Pre-implantation data are exploited in order to estimate their predictive power.Prediction accuracy enhancement with the integration of post-implantation data.An optimal subset of features highly correlated with patient survival is identified.

@&#KEYPHRASES@&#
Heart failure disease,Ventricular assist device,Survival prediction,Data mining,Feature selection,Classification,

@&#ABSTRACT@&#
Heart failure is one of the most common diseases worldwide. In recent years, Ventricular Assist Devices (VADs) have become a valuable option for patients with advanced HF. Although it has been shown that VADs improve patient survival rates, several complications persist during left VAD (LVAD) support. The stratification scores currently employed are mostly generic, i.e. not specifically built for LVAD patients, and are based on pre-implantation patient data. In this work we apply data mining approaches for the prediction of time dependent survival in patients after LVAD implantation. Moreover, the predictions acquired with the use of pre-implantation data are enriched by employing post-implantation data, i.e. follow-up data. Different clinical scenarios have been depicted and the subsequent conditions are tested in order to identify the optimal set of pre- and post-implant features, as well as the most suitable algorithms for feature selection and prediction. The proposed approach is applied to a real dataset of 71 patients, reporting an accuracy of 84.5%, sensitivity of 87% and specificity of 82%. Based on the reported results, expert cardio-surgeons can be supported in planning the treatment of VAD patients.

@&#INTRODUCTION@&#
Heart failure (HF) is one of the most common diseases worldwide, with a prevalence of over 5.8 million in the USA, and over 23 million worldwide. Projections show that the prevalence of HF will increase by 46% from 2012 to 2030, resulting in more than 8 million people in US [1,2]. Up to some years ago, heart transplantation was the conventional treatment for end-stage heart failure. Each year approximately 5000 transplantations take place worldwide, although it is estimated that up to 50,000 people are candidates for transplantation [3]. The need for alternative treatment has led to the development of implantable Ventricular Assist Devices (VAD) which has become a valuable option for patients with advanced HF. The progress in VAD technology over the last years has moved VAD usage from limited “bridge-to-transplant” (BTT) to durable lifelong support, i.e. a “destination-therapy” (DT), while occasionally VADs are used as a “bridge-to-(myocardial)-recovery”. Although it has been shown that VADs improve patient׳s survival rates (see e.g. [4,5]) several complications persist during left VAD (LVAD) support, which are based on: (i) the pre-existing effects of advanced heart failure, (ii) the requirement of extensive surgery to implant the device and (iii) the effects of VAD in compromised patients [6]. Sepsis, right heart failure and multiorgan failure are the most common causes of death [7]. Current estimates calculate the number of end-stage HF patients potentially in need for LVAD support from 30,000 to 100,000 in the US [8] or up to 200,000 worldwide [9]. Therefore the need for careful patient selection of candidates is of paramount importance. As we are moving from BTR to DT approaches though, what seems to be lacking is not only a reliable pre-operative stratification score, but also a tool in order to assess the status of the patient after the implantation.The most widely employed by clinicians stratification scores are the Heart Failure Survival Score (HFSS) [10] and the Seattle Heart Failure Model (SHFM) [11]. Both of them have not been specifically built for LVAD patients and can estimate survival rates of HF patients, without considering the LVAD implantation, based on hemodynamic instability and other features of HF. Still, Schaffer et al. [12] report that SHFM works better than other scores for mortality prediction before and after VAD implantation. Complementary to the above, the Model for End-Stage Liver Disease (MELD) score [13] is also used identifying candidate patients at high risk of developing other diseases (multi-organ failures) when undergoing LVAD implantation.The first score specifically built for LVAD patients was the Lietz-Miller Destination Therapy Risk Score (DTRS) [14] which is derived from a patient cohort of 222 patients receiving a first generation LVAD treatment who were followed up until death and discriminates risk groups at 1-year survival. Through multivariate analysis, the DTRS estimates risk scores for 90-day in-hospital mortality based on 9 pre-operative patient variables and is considered one of the major attempts for risk stratification of LVAD patients. Still, it has received some criticism on the synthesis of the dataset, the representation of different patient groups including the ones with co-morbidities.As DTRS was shown to provide modest discrimination for destination therapy patients with continuous flows devices, logistic regression was recently applied by Cowger et al. [8] for the calculation of a Heartmate II relevant risk score estimating 90-day mortality risk based on five pre-operative variables. Wang et al. [15] introduce machine learning techniques for patient classification into two groups (alive/dead). Three different machine learning algorithms are utilized for risk prognosis of candidate VAD patients, namely: Decision Trees, Support Vector Machines, and Bayesian Tree-Augmented Network. Among the three models developed for predicting 90-day survival, Decision Trees outperform the other two approaches with predictive accuracy 61.2%. The authors compare the performance of their model with the survival index of Lietz et al. [14] and note that the proposed model performs better in identifying high-risk patients. Machine learning techniques and more specifically Bayesian Networks have been recently applied to a large study employing data from the Interagency Registry for Mechanically Assisted Circulatory Support (INTERMACS) database. A risk stratification system called CHRiSS (Cardiac Health Risk Stratification System) has been proposed in order to further predict patient mortality at specific endpoints (namely Day 30, Day 90, Month, 6, Year 1 and Year 2) [9]. Approximately two hundred pre-implant clinical features have been examined for building the five distinct Bayesian Network models. Machine learning techniques have been also recently applied in [16] where 25 pre-operative variables are initially employed. Different classifiers are tested, namely Naïve Bayes, k-Nearest Neighbor, Decision trees, Random forests, Multilayer perceptron neural networks and Support Vector Machines; with Decision Trees finally selected as they provide the best results.All the above studies are limited to pre-operative data. As LVAD now becomes Destination Therapy, a series of data (reflecting patient hemodynamic status and organ functioning) is generated after the implantation. Therefore they can further enhance initial predictions, by providing input on the post-implantation status. In the present study, we integrate a multitude of clinical data related to HF disease, aiming to estimate the survival probabilities of patients who are receiving VAD therapy. The aim of this work is to estimate the predictive power of the pre-operative data and identify an optimal subset of factors that are highly correlated with patient survival. Based on these results, we are able to investigate whether and how the integration of additional information, namely post-operative patient data, along with the utilization of data mining methods can enhance the prediction accuracy. Such an improvement in prediction can be proven quite helpful towards determining the most proper design of a treatment plan.In the sections that follow, we layout our study which is divided into three main parts. In the first part, namely Section 2, we describe our dataset by presenting the features extracted for the candidate patients and we introduce the methods utilized for predicting the survival probabilities. In Section 3, we present the performance of the employed methods along with the calculated metrics for several experimental settings. Finally, in the last part, we discuss the results of the proposed analysis and suggest possible extensions of the current study.The dataset considered in the current study has been provided by the Katholieke Universiteit of Leuven, Belgium and contains data from 71 patients treated with second generation VADs. Patients were followed for an up to 12 month period after implantation unless they died. The follow up period refers to specific time points, i.e. 1 month, 2 months, 3 months, 6 months, 9 months and 1 year. Moreover, patients that had a heart transplant, or at their latest measurement were alive or had a VAD explantation are considered as survived. Both above mentioned cases, i.e. VAD explanation/Heart Transplant, are considered to be characterized by a stable patient condition and relatively good progress. Out of the 71 patients, 51 survived the 12 month period. In total 39 pre-operative features and 9 post-operative ones for each time slice were recorded for each patient. The 39 pre-operative features are shown inTable 1, with their mean (or median) and their standard deviation. The related features are in accordance with medical literature on pre-operative risk factors for VAD patients [17].Related to the post-operative data, specific features capturing multi-organ functioning have been monitored during the follow-up visits (Table 2). Both sources of data are exploited aiming to predict survival probabilities of patients supported with VAD therapy.

@&#CONCLUSIONS@&#
A methodology for the prediction of survival time of patients suffering from end stage HF and receiving LVAD therapy has been presented. Our approach involves feature selection and prediction algorithms aiming to identify the optimal ones and their parameters for the accurate prediction of patient survival. The method has been applied to 4 different datasets. Both pre- and post-operative clinical data are considered in the current study. The obtained results show that data mining algorithms is a prominent way for addressing the specific time dependent survival probability estimation problem. Moreover, they prove the usefulness of the post-operative data in this prediction. Overall, the proposed methodology and results can be proven quite helpful for clinicians towards the design of an appropriate treatment protocol. Still, further testing in the everyday clinical setting will fully reveal the clinical usefulness and potential of the proposed methodology.We do not wish to declare any conflict of interest.