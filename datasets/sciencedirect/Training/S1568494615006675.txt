@&#MAIN-TITLE@&#
Optimization of type-2 fuzzy weights in backpropagation learning for neural networks using GAs and PSO

@&#HIGHLIGHTS@&#
Optimization of type-2 fuzzy inference systems using GAs and PSO are presented.Optimized type-2 fuzzy systems are used to estimate the type-2 fuzzy weights.Simulation results and a comparative study are presented to illustrate the method.Bio-inspired optimization of the type-2 fuzzy systems is viable for this problem.

@&#KEYPHRASES@&#
Type-2 fuzzy logic,Neural networks,Time series prediction,Genetic algorithm,Particle swarm optimization,

@&#ABSTRACT@&#
In this paper the optimization of type-2 fuzzy inference systems using genetic algorithms (GAs) and particle swarm optimization (PSO) is presented. The optimized type-2 fuzzy inference systems are used to estimate the type-2 fuzzy weights of backpropagation neural networks. Simulation results and a comparative study among neural networks with type-2 fuzzy weights without optimization of the type-2 fuzzy inference systems, neural networks with optimized type-2 fuzzy weights using genetic algorithms, and neural networks with optimized type-2 fuzzy weights using particle swarm optimization are presented to illustrate the advantages of the bio-inspired methods. The comparative study is based on a benchmark case of prediction, which is the Mackey-Glass time series (for τ=17) problem.

@&#INTRODUCTION@&#
In this paper we propose the optimization of type-2 fuzzy inference systems with genetic algorithms and particle swarm optimization for achieving fuzzy weight adaptation in neural networks [3,11,21,25]. The optimized type-2 fuzzy inference systems are used to estimate the type-2 fuzzy weights for a neural network using the backpropagation learning algorithm [12,15,22].This work is focused on managing the weights of neural networks using type-2 fuzzy inference systems optimized with bio-inspired methods. This is important due to the fact that the weights affect the performance of the learning process of the neural network [2,6,16,27,28], and the use of type-2 fuzzy weights can be important in the training phase for managing uncertainty in real world data [7,17,24,59].The neural networks with optimized type-2 fuzzy weights are applied in the prediction of the Mackey-Glass time series [5,9,18,55,74]. In this case, the goal is obtaining the minimum prediction error for the time series data. This is similar to the work by Ahmadi et al. that obtains the Prediction of asphaltene precipitation using a neural network optimized by the imperialist competitive algorithm, and other similar works [4,8,13,19,23].This paper is focused on the comparison of neural networks with type-2 fuzzy weights (without optimization) with respect to neural networks with optimized type-2 fuzzy weights. This is important because the weights affect the performance of the learning process of the neural network and therefore are critical in obtaining better results. The same architecture and learning algorithm are used for the neural models for achieving a fair comparison. In addition, noise is also used in the real test data to analyze the performance of the models.The main contribution of the paper is the proposed optimization for the type-2 fuzzy weight adjustment in the backpropagation learning of neural networks for providing them with the ability to manage uncertainty in real data. The main idea of the proposed optimization is that enhancing the backpropagation method with type-2 fuzzy logic enables better management of uncertainty in the learning process and with this improved results can be achieved [10,20,29]. The neural networks with type-2 fuzzy weights used in this paper are different than the adaptive neuro-fuzzy inference system (ANFIS) models, because ANFIS uses the neural network architecture for obtaining the characteristics of the fuzzy systems, and performs the operations based on the calculations of the fuzzy systems; this is different to the proposed method in this paper that uses the type-2 fuzzy systems to update the weights used in the neural network for the training (learning) process.The approach used in this paper is different than other ones that can be found in the literature, where weight adaptation is achieved using a momentum variable and adaptive learning rate [63,68], or with triangular or trapezoidal fuzzy numbers for the weights [49,50]. In this paper the proposed method works with type-2 fuzzy weights, which is the main difference with respect to the others methods, as this is not found in related works.The next section presents a background of different weight management strategies and proposed modifications of the backpropagation algorithm in neural networks. Section 3 explains the proposed method and the problem description. Section 4 describes the monolithic neural network architecture and the neural network architecture with type-1 fuzzy weights. Section 5 describes the neural network with type-2 fuzzy weights proposed in this paper. Section 6 presents the simulation results for the proposed methods. Finally, in Section 7 the conclusions are presented.This section presents a brief summary of basic concepts and related work relevant to this paper.The concept of a type-2 fuzzy set was introduced by Zadeh (1975) as an extension of the concept of an ordinary fuzzy set (henceforth called a “type-1 fuzzy set”). A type-2 fuzzy set is characterized by a fuzzy membership function, i.e., the membership degree for each element of this set is a fuzzy set in [0,1], unlike a type-1 set where the membership degree is a crisp number in [0,1] [33,83]. Such sets can be used in situations where there is uncertainty about the membership degrees themselves, e.g., uncertainty in the shape of the membership function or in some of its parameters [60]. Consider the transition from ordinary sets to fuzzy sets. When we cannot determine the membership of an element in a set as 0 or 1, we use fuzzy sets of type-1 [51,65,80]. Similarly, when the situation is so fuzzy that we have trouble determining the membership degree even as a crisp number in [0,1], then we use fuzzy sets of type-2 [32,35,37,47,71,73].The genetic algorithm is a searching procedure using methods that simulate the natural genetic theory, like the processes of selection, mutation and recombination. The individuals used in this algorithm can be binary, integer or real to represent the possible solutions to the problem of study. The mutation and recombination are used to modify the individuals in search of obtaining better individuals. The selection process consists in using objective functions that determine the fitness for all individuals and then to choose an individual based on the fitness in different ways, like the random or elitist methods [42].Particle swarm optimization is an algorithm which simulates the behavior of birds flocking or fish schooling looking for food. Each particle represents a solution to the optimization problem and each one has a position and a velocity that defines the search direction in the search space. The particle adjusts its velocity and position according to the best experiences found so far, which are called the pbest found by the particle itself and the global best, called gbest, found by all its neighbors [54,75,76].The architecture of a neural network is formed by connecting multiple elementary processors, with this forming an adaptive system that has a particular algorithm to adjust their weights (free parameters) to achieve the performance requirements of the problem based on representative samples [38,41,57,70].The artificial neuron consists of several components, which are the inputs, weights, the summation, and finally the activation function f. The input values are multiplied by the weights [67], obtaining the following equation:(1)y=f∑i=1nxiwijIn the area of neural networks, the backpropagation algorithm [57,67] and methods to accelerate the convergence of this algorithm are of relevance for this paper [63,68].In the adjustment or managing of weights of the neural network [62,81,82], we considered as related work the method of Gedeon [45] using a discrete selection approach; the method of Kamarthi and Pittner [52], using extrapolation for the new weights; the proposed method by Ishibuchi et al. [49,50], using trapezoidal fuzzy numbers or triangular fuzzy numbers; the computation of the new lower and upper limits of the weights developed by Feuring [43]; and the proposed approach of type-2 neuro-fuzzy models by Castro et al. [34].There are also recent works on type-2 fuzzy logic that have been developed for time series prediction, like that of Castro et al. [36], and other researchers [1,53,69].In the recent literature, bio-inspired algorithms have proven to be good tools for optimization in different areas of research and have shown to find optimal results in solving real world problems [78]. There are many works on different proposed bio-inspired algorithms, but only works dealing with genetic algorithms and particle swarm optimization are the most important and relevant for this paper to be considered here as related work [26,85]: for example Valdez et al. [79], performed a hybridization of the GA and PSO algorithms based on the concept of fuzzy logic. In Melin et al. [61], they used PSO for designing fuzzy classification systems, another case is that by Cervantes and Castillo [39], that used a GA for optimization of membership functions, and Hidalgo et al. [48], which designed type-2 fuzzy inference systems using GAs.The proposed approach in this paper has the aim of optimizing the type-2 fuzzy inference systems, used to obtain the type-2 fuzzy weights in the neural network, with genetic algorithms (GAs) and particle swarm optimization (PSO). This optimization allows generalizing the backpropagation algorithm to give the neural network the robustness to handle data with uncertainty. For the type-2 fuzzy sets, it will be necessary to automatically vary the footprint of uncertainty (FOU) of the membership functions using an optimization method for the corresponding particular applications [66,77,84,86].The process of obtaining the weights in the connections for each neuron is performed using type-2 fuzzy inference systems, considering the possible modification in the way we work internally in the neuron and the adjustment of the weights given in this way (Fig. 1) [30].The optimization with genetic algorithms and particle swarm optimization is applied to the design of the type-2 fuzzy inference systems used in each layer for obtaining the type-2 fuzzy weights and finally to obtain the output of the neural network (Fig. 1) [40,51].In Fig. 2, a diagram of the proposed optimization of the neural network with type-2 fuzzy weights using GA and PSO is presented. In this diagram, the Mackey-Glass time series is the input to the neural network; afterwards, we optimize the membership functions of the type-2 fuzzy inference systems (FIST2) used to obtain the weights between the input layer and the hidden layer (called type-2 fuzzy weights); subsequently, we perform calculations with these weights with the neurons at the hidden layer; later, we optimize the membership functions of the type-2 fuzzy inference systems used to obtain the weights between the hidden layer and the output layer; and finally, we perform calculations with these weights in the neurons at the output layer.A neural network architecture with type-2 fuzzy weights is used to deal with the test data of the Mackey-Glass time series. We consider 30 neurons in the hidden layer and 1 neuron in the output layer; the type-2 fuzzy inference systems used to obtain the type-2 fuzzy weights are optimized with a GA and PSO (see Fig. 3) [46,64,72].The mathematical design of the neural network with type-2 fuzzy weights and the adaptation of type-2 fuzzy weights based on the backpropagation algorithm are described in detail in a previous paper [44], but are presented below in a summarized way.The mathematical design of the neural network with type-2 fuzzy weights is as follows:Layer 0: Input.(2)x=[x1,x2,…,xn]Layer 1: Interval type-2 fuzzy weights for the hidden layer [31,34,58].(3)w˜=[w_,w¯]where(4)w¯=∑k=1Lf¯k⋅wlk+∑k=L+1Mf_k⋅wlk∑k=1Lf¯k+∑k=L+1Mf_k(5)w_=∑k=1Rf_k⋅wrk+∑k=R+1Mf¯k⋅wrk∑k=1Rf_k+∑k=R+1Mf¯kwhere L and R are the switch points [35].Layer 2: Hidden neurons with interval type-2 fuzzy weights.(6)Net=∑i=1nxiw˜ιLayer 3: Output neurons with interval type-2 fuzzy weights.(7)Out=∑i=1nyiw˜ιLayer 4: Obtain a single output of the neural network.The experiments are performed in time-series prediction, specifically for the Mackey-Glass time series (for τ=17) that behaves chaotically and it is a well known benchmark problem considered in many studies, which equation is defined as follows:(8)dx(t)dt=0.2x(−t−τ)1+x10(t−τ)−0.1x(t)We used 800 data points of the Mackey-Glass time series: the first 500 data points are used for training the three neural networks and 300 data points are considered to validate the model [14].We used the gradient descent backpropagation algorithm with adaptive learning rate for the experiments. The initial weights for the training in the neural network with type-2 fuzzy weights are set randomly in the first epoch.We consider first the neural network with type-2 fuzzy weights without optimization and later we optimize the type-2 fuzzy inference systems for performing a comparison between them.For the neural network with type-2 fuzzy weights without optimization, we used 2 similar type-2 fuzzy systems to obtain the type-2 fuzzy weights in the hidden and output layers for the neural network.The first type-2 fuzzy system consists of two inputs: the current weight in the actual epoch and the change of the weight for the next epoch, and one output: the new weight for the next epoch (see Fig. 4).The “current weight” input has two triangular membership functions with a range of −1 to 1. The “change of the weight” input has two triangular membership functions with a range of −0.003 to 0.003. The output of the “new weight” has two triangular membership functions with a range of −1 to 1 (see Fig. 5).We defined six rules for the type-2 fuzzy inference system of the hidden layer, corresponding to the four combinations of the two triangular membership functions and we have also added two rules for the case when the change of weight is null (see Fig. 6).The second type-2 fuzzy system consists of two inputs: the current weight in the actual epoch and the change of the weight for the next epoch, and one output: the new weight for the next epoch (see Fig. 7).The “current weight” input has two triangular membership functions with a range of −1 to 1. The “change of the weight” input has two triangular membership functions with a range of −0.01 to 0.01. The output of the “new weight” has two triangular membership functions with a range of −1 to 1 (see Fig. 8).We used six rules for the type-2 fuzzy inference system of the output layer, corresponding to the four combinations of two membership functions and we also added two rules for the case when the change of weight is null (see Fig. 9).For the optimization of the type-2 fuzzy inference systems that are used in the neural network with type-2 fuzzy weights, we performed the optimization with genetic algorithms (GAs) and particle swarm optimization (PSO).The optimization process is not applied to the architecture of the neural network only to the membership functions of the type-2 fuzzy inference systems.The parameters of the GA applied in the optimization of the type-2 fuzzy inference systems used to obtain the type-2 fuzzy weights in the neural network are shown in Table 1[56].We used a population of 100 individuals per each generation, and each individual consists of 60 genes of real type (see Fig. 10). We used the ranking method to assign fitness values to all the individuals.In this case 10 genes are used to generate the membership functions of the inputs and outputs of the type-2 fuzzy inference systems; the first point of the first membership function of the input or output is established using the initial value of the range (ri) of the input or output, and the last point of the second membership function is the final value of the range (rf). The first gen of the chromosome is the second point of the first membership function and the second gen of the chromosome is the third point of the first membership function and so on.We used the stochastic universal sampling method for individual selection and applying the mutation and crossover operators. Single point crossover is used for the recombination of the genes, using 80 percent for the crossover rate and 40 percent for mutation rate.We optimized the two type-2 fuzzy inference systems that calculate the type-2 fuzzy weights, but we only optimized the membership functions of the two inputs and the output, and we maintain the fuzzy system structure of two inputs and one output and the six rules used in the non-optimized type-2 fuzzy inference systems.The membership functions obtained for the two inputs and one output for the type-2 fuzzy inference system in the connections between the neuron of the input layer and the neurons for the hidden layer are shown in Fig. 11.The membership functions obtained for the two inputs and one output for the type-2 fuzzy inference system in the connections between the neurons of the hidden layer and the neurons for the output layer are shown in Fig. 12.The parameters of the PSO used in the optimization of the type-2 fuzzy inference systems used to calculate the type-2 fuzzy weights in the neural network are shown in Table 2.We used a swarm of 100 particles per iteration, and each particle consists of 60 dimensions of real type (same structure used for the GA, see Fig. 10). We used the inertia weight with value of 0.1, the constriction coefficient with value of 1, the variables R1 and R2 are random in the range between 0 and 1, the variable C1 handles values with a linear decrease from 2 to 0.5, and the variable C2 handles values with a linear increase from 0.5 to 2.We optimized the two type-2 fuzzy inference systems that are used to calculate the type-2 fuzzy weights in the same way as with the genetic algorithm, in other words, we only optimized the membership functions of the two inputs and the output, and we maintain the structure of two inputs and one output and the six rules.The membership functions obtained for the two inputs and one output for the type-2 fuzzy inference system in the connections between the neuron of the input layer and the neurons for the hidden layer are shown in Fig. 13.The membership functions obtained for the two inputs and one output of the type-2 fuzzy inference system in the connections between the neuron of the hidden layer and the neurons for the output layer are shown in Fig. 14.The results for the experiments of the neural network with type-2 fuzzy weights are shown in Table 3and Fig. 15. In this case all the parameters of the neural network and the two type-2 fuzzy inference systems are established empirically. The best prediction error is of 0.0560, and the average error is of 0.0829.We are presenting 10 experiments in Table 3 as an illustration, but the average error was calculated considering 30 experiments with the same parameters and conditions.The training parameters of the feed-forward backpropagation neural network are as follows: the number of epochs for the training network is of 100, the network desired goal error is of 0.0000001, the training algorithm is the gradient descent backpropagation with momentum and learning rate, and the activation function is the sigmoid logarithm in the hidden neurons and a linear function in the output neuron.In Fig. 15, we show a graph of the real data of the Mackey-Glass time series against the prediction data of the best training for the neural network with type-2 fuzzy weights.The equation used to obtain the prediction error (pe) is the sum of the differences of the real data (r) and the prediction data (p) divided by total number of data points of the Mackey-Glass time series (N) used in this experiment for testing (in this case 297 points):(9)pe=∑i=0n(ri−pi)NThe obtained results for the experiments with the neural network with type-2 fuzzy weights optimized with a genetic algorithm (GA) are shown in Table 4and Fig. 16. The best prediction error is of 0.0431, and the average error is of 0.0486.We presented 10 experiments of the optimization in Table 4, but the average error is calculated considering 30 experiments with the same parameters and conditions. The number of epochs for the network to test each chromosome of the population in each generation is of 100 and the network desired goal error is of 0.0000001.In Fig. 16, we show a graph of the real data of the Mackey-Glass time series against the prediction data of the best training for the neural network with type-2 fuzzy weights (based on type-2 fuzzy inference systems optimized with a genetic algorithm).The obtained results for the experiments with the neural network with type-2 fuzzy weights optimized with particle swarm optimization (PSO) are shown in Table 5and Fig. 17. The best prediction error is of 0.0456, and the average error is of 0.0527.We are presenting 10 experiments of the optimization in Table 5, but the average error is calculated considering 30 experiments with the same parameters and conditions. The number of epochs of the network for testing each particle of the swarm is of 100 and the network desired goal error is established at 0.0000001.In Fig. 17, we show a graph of the real data of the Mackey-Glass time series against the prediction data of the best training for the neural network with type-2 fuzzy weights (type-2 fuzzy inference systems optimized with particle swarm optimization).In Table 6, we are presenting the comparison among the neural network with type-2 fuzzy weights (NNT2FW), the neural network with type-2 fuzzy weights optimized with the GA (NNT2FWGA) and the neural network with type-2 fuzzy weights optimized with the PSO (NNT2FWPSO).We also show in Table 6, the processing times of the NNT2FWGA and NNT2FWGA and we can notice that these are higher than the NNT2FW. However, the times of the NNT2FWGA and NNT2FWGA are for the complete execution of the algorithm and not only for the evaluation of Mackey-Glass time series data with the neural network with the type-2 fuzzy weights using the type-2 fuzzy inference systems optimized with these approaches.We also performed a t-student statistical test comparing the NNT2FW with the NNT2FWGA and NNT2FWPSO and also comparing NNT2FWGA with NNT2FWPSO. All statistical tests are performed with a 95% of confidence and in the tests and 30 experiments are considered.We show in Table 7the parameters for the comparison of the NNT2FW with NNT2FWGA. We performed the Hypothesis testing with H0: NNT2FW=NNT2FWGA and for the alternative hypothesis H1: NNT2FW>NNT2FWGA. The results obtained with the statistical test are of 0.03434 in the estimated difference, 0.03076 in the lower limit of the difference, t value of 16.19, P value of 0.0001 and 37 degrees of freedom. The results show that there is sufficient statistical evidence to say that the NNT2FWGA is better than the NNT2FW.We show in Table 8the parameters for the comparison of the NNT2FW with NNT2FWPSO. We performed the Hypothesis testing with H0: NNT2FW=NNT2FWPSO and for the alternative hypothesis H1: NNT2FW>NNT2FWPSO. The results obtained with the statistical test are of 0.03019 in the estimated difference, 0.02639 in the lower limit of the difference, t value of 13.35, P value of 0.0001 and 45 degrees of freedom. The results show that there is sufficient statistical evidence to say that the NNT2FWPSO is better than the NNT2FW.We show in Table 9the parameters for the comparison of the NNT2FWGA with NNT2FWPSO. We performed the Hypothesis testing with H0: NNT2FWGA=NNT2FWPSO and for the alternative hypothesis H1: NNT2FWGA<NNT2FWPSO. The results obtained with the statistic test are of −0.00414 in the estimated difference, −0.00187 in the lower limit of the difference, t value of −3.06, P value of 0.002 and 52 degrees of freedom. The results show that there is sufficient statistical evidence to say the NNT2FWGA is better than the NNT2FWPSO.We also performed an experiment applying noise in the range (0.1–1) to the test data to observe the behavior of the neural network with non-optimized type-2 fuzzy weights, and the type-2 fuzzy weights optimized with the GA and PSO. The obtained results for the experiments are shown in Table 10.From the tables in the previous section, we can notice that the optimization of the type-2 fuzzy inference systems (used to obtain the type-2 fuzzy weights of the neural network) with the genetic algorithm (NNT2FW2GA) and particle swarm optimization (NNT2FWPSO) shows better performance than the type-2 fuzzy inference system without optimization (NNT2FW) in the prediction of the Mackey-Glass time series. This is because in all results with noise and without noise, and in the statistical tests, the NNT2FW2GA and NNT2FWPSO have better results than the NNT2FW. That is the NNT2FW2GA and NNT2FWPSO obtained improved type-2 fuzzy inference systems with respect to NNT2FW. We also find that NNT2FWPSO has better performance than NNT2FW2GA when there is noise present (Table 10). On the other hand, there is statistical evidence that NNT2FW2GA is better when there is no noise. This shows the advantage of the optimization with GA and PSO over the non-optimized type-2 fuzzy inference system.

@&#CONCLUSIONS@&#
In the experiments, we observe that by optimizing (with genetic algorithms and particle swarm optimization) the type-2 fuzzy inference systems that produce the type-2 fuzzy weights for the neural network we can achieve better results than the non-optimized neural networks, for the Mackey-Glass time series. This conclusion is based on the corresponding prediction errors of 0.0560 for the non-optimized type-2 fuzzy inference system, 0.0431 for the optimized with the GA and 0.0456 for the optimized with the PSO, and the average errors (30 experiments) of 0.0486, 0.077 and 0.0527, respectively.Bio-inspired optimization algorithms, like genetic algorithms and particle swarm optimization, were used in this paper to perform the search for the optimal type-2 fuzzy inference systems of the neural network allowing us to obtain better results than the non-optimized versions. The neural network with type-2 fuzzy weights optimized with the GA shows better behavior in the plots of the real data against the prediction data (see Figs. 15–17) than the neural network with non-optimized type-2 fuzzy weights and the one optimized with PSO. This conclusion is based on the fact that the prediction error obtained with the NNT2FWGA (0.0431) is smaller than the results of the NNT2FW and NNT2FWPSO, 0.0560 and 0.0456, respectively.Applying different levels of noise we observe that the neural network with type-2 fuzzy weights optimized with PSO has better behavior and tolerance to noise than the neural network with non-optimized type-2 fuzzy weights. In addition it is also better than the neural network with optimized type-2 fuzzy weights with GA. This conclusion was found by observing that the neural network with optimized type-2 fuzzy weights with PSO presents lower prediction errors than the other methods.The results obtained in these experiments show that the neural network with optimized type-2 fuzzy weights with the bio-inspired algorithm obtained better results, without noise and with noise, than the neural network with non-optimized type-2 fuzzy weights, in the prediction of the Mackey-Glass time series.The proposed optimization of the type-2 fuzzy inference systems used to obtain the type-2 fuzzy weights in the neural network has higher robustness and achieves better results than the neural network without optimization. Besides, the type-2 fuzzy weights provide the neural network with less susceptibility to the occurrence of a significant increase in the prediction error when noise is applied to the real data.