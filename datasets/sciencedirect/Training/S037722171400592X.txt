@&#MAIN-TITLE@&#
The impact of mismeasurement in performance benchmarking: A Monte Carlo comparison of SFA and DEA with different multi-period budgeting strategies

@&#HIGHLIGHTS@&#
We report the impact of frontier estimation models on performance-informed budgeting.Selection of a proper benchmark model can lead to substantial gain in performance.A peanut-butter strategy performs better in the presence of high measurement error.The use of frontier estimation models is still more beneficial than a random ranking.SFA outperforms DEA but the result is reversed as a measurement error increases.

@&#KEYPHRASES@&#
Efficiency estimation,Stochastic frontier analysis,Data envelopment analysis,Performance-based budgeting,

@&#ABSTRACT@&#
Performance-based budgeting has received increasing attention from public and for-profit organizations in an effort to achieve a fair and balanced allocation of funds among their individual producers or operating units for overall system optimization. Although existing frontier estimation models can be used to measure and rank the performance of each producer, few studies have addressed how the mismeasurement by frontier estimation models affects the budget allocation and system performance. There is therefore a need for analysis of the accuracy of performance assessments in performance-based budgeting. This paper reports the results of a Monte Carlo analysis in which measurement errors are introduced and the system throughput in various experimental scenarios is compared. Each scenario assumes a different multi-period budgeting strategy and production frontier estimation model; the frontier estimation models considered are stochastic frontier analysis (SFA) and data envelopment analysis (DEA). The main results are as follows: (1) the selection of a proper budgeting strategy and benchmark model can lead to substantial improvement in the system throughput; (2) a “peanut butter” strategy outperforms a discriminative strategy in the presence of relatively high measurement errors, but a discriminative strategy is preferred for small measurement errors; (3) frontier estimation models outperform models with randomly-generated ranks even in cases with relatively high measurement errors; (4) SFA outperforms DEA for small measurement errors, but DEA becomes increasingly favorable relative to SFA as the measurement errors increase.

@&#INTRODUCTION@&#
Performance assessment of production and services is becoming an increasingly important managerial activity as public and for-profit organizations turn to performance-based budgeting in an effort to optimize the allocation of funds across their individual producers. The performance-based budgeting technique is clearly applicable and adaptable to many practical situations. One example is the allocation of an energy conservation program budget and assignment of appropriate energy quotas to the individual plants in a global automotive manufacturing company. The common practice in setting energy quotas is to apply the peanut butter approach,1It is frequently used as a business term where it can refer to the efforts to apply the same tactics to all aspects or parts of a business. As a person makes a peanut butter sandwich by spreading the butter thin evenly on the bread, a business or a government may want to spread something (tactics, money, or tax break) evenly across all areas.1in which the limited total budget is simply divided among individual plants in proportional to their production sizes along with an equal percentage of energy reduction quota (e.g., a 4% cut in the energy cost per production unit from the previous year), without considering the different energy saving potentials of the various plants. For example, certain plants may already have implemented aggressive energy saving programs and reached a point of diminishing returns, while other plants barely met (or even failed to meet) their allocated reduction quotas in previous years, carrying the remaining energy saving potential over to the next year. In such cases, the peanut butter approach may not serve to optimize the overall system performance or lead to efficient use of a limited budget. It would be far more desirable to allocate relatively low energy saving targets and low budgets to best-practice plants while allocating aggressive energy saving targets to inefficient plants along with high budgets to encourage major changes in their energy conservation practices.The effectiveness of performance-based budgeting depends on the accuracy of the performance assessment in distinguishing between best-practice and inefficient producers. There are two major approaches to frontier estimation – stochastic frontier analysis (SFA) and data envelopment analysis (DEA). The consensus in the performance benchmarking literature is that DEA is preferable in applications in which the frontier model cannot be expressed in algebraic form or does not have a known inefficiency distribution. The SFA method is preferable when certain classical assumptions are satisfied regarding the composite error terms, including the contributions from the inefficiency distribution and measurement errors. It has also been claimed that DEA has a comparative advantage in cases involving relatively small measurement errors due to the conceptual treatment of the errors, while the complementary SFA method has the advantage when the measurement errors are relatively high. A more thorough assessment of the two frontier estimation models may help managers responsible for performance-based budgeting to make more informed decisions regarding the most appropriate method for their particular circumstances.Several studies have addressed the comparative advantages of stochastic versus deterministic frontier estimation. Banker, Gadh, and Gorr (1993) and Banker, Charnes, Cooper, and Maindiratta (1987) compared the efficiency estimation accuracy of corrected ordinary least squares (COLS) and DEA using Monte Carlo methods. The results indicated that DEA outperformed COLS in most cases and that COLS failed to distinguish between the measurement error and inefficiency. However, both frontier models failed as the measurement errors became large for all of the experimental scenarios considered. These results contradicted the traditional view favoring the use of stochastic frontier models. The DEA method has been criticized previously for its neglect of measurement errors; Greene (1993) even suggested that econometricians have abandoned the deterministic frontier model because it does not consider measurement errors. Gong and Sickles (1992) demonstrated the superiority of the SFA approach using Monte Carlo analysis; however, their results were criticized because of their assumption that the efficiency of the firm remains constant over time. In reality, the efficiency of a firm varies over time due to a variety of exogenous and endogenous factors. Ruggiero (1999) conducted another Monte Carlo analysis in which previous comparative studies were extended to include more general experimental scenarios and discovered that the deterministic frontier model which ignores the impact of measurement error is not as limited as the main criticism against the deterministic models stated negatively, but rather outperformed the stochastic frontier analysis model from the average rank correlation perspective.Mixed results have therefore been obtained concerning the comparative advantages of stochastic versus deterministic frontier estimation. Nonetheless, one consistent finding is that DEA remains attractive as a frontier model, especially when the measurement errors become large. The traditional criticism of DEA based solely on the conceptual treatment of the errors should therefore be reconsidered. Another consistent finding is that as the measurement errors increase, the accuracy of the performance measurement decreases in both models. However, very little comparative research has been performed to date on how mismeasurement by frontier estimation models impacts the capital budget allocation and degree of system optimization.Several studies in the DEA literature have addressed resource allocation based on efficiency analysis using variants of the DEA method. The goal of these studies is to balance the desires of two management layers, a central management authority and a set of operating units, by allocating the available resources in an optimal fashion. The balance is achieved by adjusting the input and output in such a way that the efficiency of each operating unit is maintained (the desire of the operating units) while the total output of units is maximized (the desire of the central management). Korhonen and Syrjanen (2004) developed a formal interactive approach based on DEA and multiple-objective linear programming to identify the optimal allocation plan. In this approach, the units are assumed to be capable of modifying their production within a specified production possibility set. Yan, Wei, and Hao (2002) extended the “inverse” DEA method by introducing preference cone constraints to allow decision makers to incorporate their preferences into the resource allocation algorithm. Li and Cui (2008) investigated an “efficient-effective-equality” resource allocation framework consisting of a DEA-based method leveraging many existing resource allocation algorithms. However, these DEA-based resource allocation approaches assume that sector-level decision making units are able to modify their production plants in a timely manner following instructions from the central management. In practice, this sort of rapid production plan modification is only possible in service firms such as supermarket chains, banks, universities, hospitals and tourist agencies. In the manufacturing industry, for instance, plants generally require a long time to adjust to new production plans, and the time required for a particular unit can vary depending on its operating conditions.DEA has many opportunities and challenges under the multi-criteria environment. Mehdiabadi, Rohani, and Amirabdollahiyan (2013) proposed a new approach to combine DEA and Order Preference by Similarity to Ideal Solution (TOPSIS) to rank various industries which is also a multiple criteria decision making problem. Das, Sarkar, and Ray (2013) extended the proposed approach by Mehdiabadi et al. (2013) into fuzzy AHP–DEA-TOPSIS methodology which is applicable to any multiple criteria decision making problem due to its generic nature. Makui and Momeni (2012) considered similarities between multi-criteria decision making and DEA and tried to interpret decision makers preferences in UTA-STAR method using the common set of weights (CSW) in DEA.The purpose of this paper is to perform a Monte Carlo analysis of different frontier estimation models combined with different multi-period budgeting strategies and to provide a set of decision rules for selecting the budgeting strategy and benchmark model that are most appropriate for a specified set of circumstances. Artificial measurement errors are included in the analysis.The remainder of the paper is organized as follows. In Section 2, a replication study is performed to ensure the reliability of previous comparative study results on stochastic versus deterministic frontier estimation. Section 3 describes the experimental design and introduces the budgeting strategies, scenario generation methods and time-varying efficiency model used in this paper. The results of the experiments are presented in Section 4. An analytical proof is provided for the fact that a peanut butter strategy outperforms a discriminative strategy in the presence of large measurement errors, while the discriminative strategy is preferred when the measurement errors are small. Section 5 concludes with a summary of the findings of this study and suggestions for future research directions.Previous findings from related studies indicate that DEA and SFA have comparative advantages in the cases of small and large measurement errors, respectively. However, the accuracy of both frontier models decreases as the measurement error increases. A replication study is performed in this section to assure the reliability of these findings and to raise concerns regarding the use of frontier estimation models for a performance-based budgeting system in the presence of measurement errors.Assume that a large organization includes multiple individual producers and desires a systematic method for measuring and comparing the performance of the various producers in the organization, including cases in which the inputs and outputs of the individual producers have different scales. The organization may use the performance information to identify opportunities for greater efficiency and to increase the overall system throughput and competitiveness. For simplicity, it can be assumed that each producer employs the same production process and therefore has the same production function. In this study, the Cobb-Douglas function for the n-th producer in the organization is assumed to have one output y, and two inputs,x1andx2, assuming constant returns of scale although our mathematical analysis is independent of the returns of scale:(1)yn=2x1n0.4x2n0.6.This functional form is in accordance with the models of Aigner and Chu (1968) and Ruggiero (1999). The inputs were generated randomly from a uniform distribution on the interval from 5 to 15. To express the difference between the actual production and frontier more realistically, the termsunandvnare introduced into the translog form of Eq. (1), whereunrepresents the half-normally distributed [|N(0,σu2)|] inefficiency andvndenotes the normally distributed [N(0,σv2)] measurement error (including the data collection/reporting error and all other effects that are not accounted for in the analysis). The stochastic production frontier extending Eq. (1) to be used throughout this paper is then as follows:(2)lnyn=ln2+0.4lnx1n+0.6lnx2n-un+vn.Eq. (2) gives the actual production of producer n, while Eq. (1) represents the best practice for product n. To distinguishynin Eq. (1) fromynin Eq. (2),ynin Eq. (1) will henceforth be denoted byyˆn. In Eq. (2), a firm-specific estimate of the Farrell efficiency is given bye-un. For example, whenunis assumed to be half-normally distributed according to|N(0,0.252)|, the mean inefficiency is0.2≈0.252/πand the efficiency of a firm with the mean inefficiency becomes82%(≈e-0.2). The study in this section considered six inefficiency distributions in which the value ofσu2was varied between0.1,0.15,0.2,0.25,0.3, and0.35. Seven measurement error distributions were considered withσv2taking values of0.05,0.1,0.15,0.2,0.25,0.3, and0.35. With six inefficiency distributions and seven measurement error distributions, a total of 42 experimental combinations were generated. In this short replication study, the sample size was restricted to 100. The “Benchmarking” package in R (Bogetoft & Otto, 2010) is used to implement the SFA and DEA.The results of the Monte Carlo analysis are shown in Tables 1–6. This study uses two different performance metrics to evaluate the performance of SFA and DEA. The Spearman rank correlation is used by following Gong and Sickles (1992) and Ruggiero (1999) and also, MAD (mean absolute deviations) is used by following Banker et al. (1993). Therefore, Table 1 reports the average rank correlations between the true and SFA-estimated efficiencies. Tables 2 and 3 report the average rank correlations between the true and DEA-estimated efficiencies and between the SFA and DEA estimates, respectively. Similarly, Table 4 reports the MAD of true minus SFA-estimated efficiencies. Tables 5 and 6 report the MAD of true minus DEA-estimated efficiencies and SFA minus DEA-estimated efficiencies, respectively. All results are based on 100 replications. Three interesting results were obtained in this brief replication study.First, SFA outperforms DEA regardless of the values adopted for the inefficiency and measurement errors (see Tables 1 and 2) when the rank correlation is used. Note that the higher correlation means the higher accuracy in the tables. Meanwhile, when MAD is used, the results show that the MAD of true minus SFA is smaller than that of DEA in the presence of small measurement errors (σv⩽0.15), but the result is reversed for large measurement errors (σv⩾0.35) (see Tables 4 and 5). In the middle range of measurement errors (0.2⩽σv≤0.3), the MAD-based performances are mixed with two methods competing neck and neck. Since the smaller MAD is the more accurate, the results implies that SFA-estimated efficiency score is closer to the true efficiency than DEA in case that small measurement errors are present, but the result is reversed with large measurement errors. From this observations, it is found that the efficiency scores of SFA and DEA are sensitive to the change of magnitude of measurement error, leading to the plausibility of incurring severe mismeasurement. In the light of comparison with Banker et al. (1993), this results are consistent with theirs when the measurement errors is large. It also should be noted that since what they compared with DEA is not SFA but COLS2COLS (corrected ordinary least squares) produces consistent estimates based on the second and third moments of the OLS residuals; the biased intercept of the model is “corrected” based on the expected value of the composite error.2and furthermore SFA is considered more advanced than COLS in terms of separating noise from inefficiency, their results suggesting that DEA outperforms COLS make sense.Second, the SFA and DEA estimates are highly correlated (staying around 0.7) in terms of their rank order regardless of the inefficiency and random error variation (see Table 3). This high correlation between the SFA and DEA estimates implies that the ranks in SFA and DEA are likely to be consistent and less fluctuate for varying measurement error, carrying both good and bad news for modelers. The good news is the degree of consistency between the two models, demonstrating the feasibility and robustness of the model estimations (Agrell & Bogetoft, 2007). The bad news is that it is difficult to use two complementary models to detect problems such as outlier presence or dominance in DEA or type-II error occurrence in SFA. In detail, when the DEA frontier estimate is biased high because of outlier data lying beyond the true frontier, the DEA method erroneously extends the estimated frontier outward. If the SFA method can distinguish between the inefficiency and noise with sufficient accuracy, then this method can be used in a complementary fashion to detect the DEA outlier problem. Similarly, DEA can be used in a complementary manner to detect the type-II error in SFA when the SFA frontier line reduces to a standard linear regression line. On the contrary, the MAD of SFA minus DEA efficiency scores increases from 0.035 to values greater than 0.2 as the measurement errors increases (see Table 6). The increasing MAD of SFA minus DEA efficiency scores means that although the accuracy in terms of efficiency scores deteriorates in both SFA and DEA, the deterioration of SFA is much faster than DEA because SFA is more sensitive to the change.Third, the accuracy of the performance rank order in both models deteriorates significantly as the measurement errors increase. This third finding raises questions regarding the use of frontier estimation models in the presence of measurement errors, especially when a performance-based budgeting system is designed to allocate budgets to individual producers based on their results (e.g., performance ranks, efficiency scores). The impact of mismeasurement on future system performance and throughput potentials is also uncertain. In subsequent sections, the impact of mismeasurement is investigated using Monte Carlo analysis, and decision rules for the selection of a proper set of budgeting strategy and benchmark model in various scenarios are explored, with the goal of minimizing the loss in future system throughput potential.

@&#CONCLUSIONS@&#
In this paper, a Monte Carlo analysis was performed to compare the overall system throughput in various experimental scenarios generated by pairing various multi-period budgeting strategies with two production frontier estimation models, SFA and DEA. Previously published studies only compared the accuracy of SFA and DEA efficiency estimation. This paper extended these studies by considering how mismeasurement by frontier estimation models impacts the capital budget allocation and final system performance. Decision rules are also provided for the selection of the most appropriate budgeting strategy and benchmark model for a particular set of circumstances.One key conclusion from the analysis is that the selection of a proper budgeting strategy and benchmark model can lead to a substantial improvement in the system throughput. Three results that may be relevant for managers who are interested in performance-based budgeting are as follows: (1) a potential-weighted strategy performs better than a peanut butter strategy in improving the overall system throughput when the performance measurement errors are relatively small, but the result is reversed as the measurement error increases; (2) the use of frontier estimation models is beneficial even in cases with relatively high measurement errors compared to using randomly-generated ranks; and (3) SFA outperforms DEA regardless of the budgeting strategy for small measurement errors, but the result is reversed as the measurement errors increase.Future studies should expand the scope of this work by further investigating optimal budget allocation strategies that may improve the overall system throughput much faster than the peanut butter or potential-weighted strategies proposed in this paper. As shown in Section 4,c+=c1+,c2+,…,cN+, the optimal budget for the entire period, should be chosen to lie between two vectors determined by the peanut butter and potential-weighted strategies, in such a way thatc0n+cn+/yˆnis the same for all n. However, the challenge is thatyˆnvaries over time for realistic environments due to constantly-evolving technology and, more importantly, the dynamics of competition between producers. In addition, the inefficiency and measurement error distributions are uncertain in real-world scenarios. As a result, the problem is more complicated compared to the case in whichyˆnis constant over time. Future studies should therefore extend the experiments in this paper to include the optimization of budget allocation by balancing the trade-off between peanut butter and potential-weighted strategies by evolving the current rank-order based bucketing method in the presence of time-varying best-practice frontier lines.In addition, the authors admit that the assumption of using an identical measurement error distribution and an inefficiency distribution over time is not realistic in industry because it is conceivable that the distributions of measurement error and inefficiency may change dynamically over time. The dynamics behind time-varying inefficiency and measurement error distributions needs to be taken into account and accordingly, there is a need to develop a new dynamic SFA model to capture the dynamics. The authors are interested to extend the research to address this issue as part of future works.