@&#MAIN-TITLE@&#
Time series forecasting with a neuro-fuzzy modeling scheme

@&#HIGHLIGHTS@&#
Proposing a local modeling strategy, consisting of local context finding and lags selection, for time series prediction.Developing algorithms to extract training patterns from historical data for constructing forecasting models.Developing procedures to construct neuro-fuzzy based forecasting models.Demonstrating by experiments on real-world datasets the effectiveness of the proposed approach.

@&#KEYPHRASES@&#
Time series prediction,Neuro-fuzzy modeling,TSK fuzzy rules,Learning algorithm,Training patterns,

@&#ABSTRACT@&#
Time series forecasting concerns the prediction of future values based on the observations previously taken at equally spaced time points. Statistical methods have been extensively applied in the forecasting community for the past decades. Recently, machine learning techniques have drawn attention and useful forecasting systems based on these techniques have been developed. In this paper, we propose an approach based on neuro-fuzzy modeling for time series prediction. Given a predicting sequence, the local context of the sequence is located in the series of the observed data. Proper lags of relevant variables are selected and training patterns are extracted. Based on the extracted training patterns, a set of TSK fuzzy rules are constructed and the parameters involved in the rules are refined by a hybrid learning algorithm. The refined fuzzy rules are then used for prediction. Our approach has several advantages. It can produce adaptive forecasting models. It works for univariate and multivariate prediction. It also works for one-step as well as multi-step prediction. Several experiments are conducted to demonstrate the effectiveness of the proposed approach.

@&#INTRODUCTION@&#
Time series prediction is concerned about the forecasting of future values based on a time series of the previously observed data. It has played an important role in the decision making process in a variety of fields such as finance, power supply, and medical care. One example is to use the previously collected data to predict the stock exchange indices or the closing stock prices [1–9]. Another example is to predict the electricity demand to avoid producing extra electric power [10–13]. If forecasting is done for one time step ahead into the future, it is called single-step or one-step prediction. Forecasting can also be done for two or more time steps ahead. In this case, it is called multi-step prediction [14–17].Two approaches have been adopted for constructing time series forecasting models. The global modeling approach constructs a model which is independent of the target to be forecasted. For time series prediction, the conditions of the environment may vary as time goes on. A global model is not adaptive and thus accuracy suffers. A local model constructed by the local modeling approach [19,18] is dependent on the target to be forecasted and therefore is adaptive. Local models are usually characterized by using a small number of the neighbors in the proximity of the predicting sequence. Another issue in time series prediction is to determine the lags to be involved in the model. The lags have a big influence on the forecasting accuracy. For example, in steel making engineering [20,21], the furnace temperature will change after two to eight hours from the time when the materials are applied into the furnace. This indicates there is a time lag of two to eight hours for the temperature change. Furthermore, the lags may vary as time goes on and adjusting them is required [22]. Two strategies, direct [23] and iterative [24], have been traditionally adopted for constructing multi-step time series forecasting models. The difference between them lies on the incorporation of the forecasts of previous steps in the prediction of the current step. These strategies have their respective pros and cons. Due to accumulated errors, an iterative forecasting model may suffer from low prediction accuracy. On the other hand, a direct forecasting model can only acquire the estimated value of the specified step. However, all the estimated values up to the specified steps can be acquired by applying an iterative model. It should be noted that several research efforts have been paid to some new modeling strategies for multi-step time series forecasting. For example, a smart and adaptive modeling strategy was proposed in [17], which employs a PSO based heuristic to create flexible divides with varying sizes of prediction horizons under the multiple-input several multiple-output (MISMO) strategy.Recently, machine learning techniques have drawn attention and useful forecasting systems based on these techniques have been developed [25]. The multilayer perceptron, often simply called neural network, is a popular network architecture in use for time series prediction [26–31]. Neural network encounters the local minimum problem during the learning process and the number of nodes in the hidden layer is difficult to decide. Also, it is hard to provide a comprehensible expression for humans to understand or investigate. The k-nearest neighbor regression method is a nonparametric method which bases its prediction on the k nearest neighbors of the target to be forecasted [18,32]. However, it lacks the capability of adaptation and the distance measure adopted may affect the prediction performance. Fuzzy theory is incorporated for prediction in stock market [2,3,5,6,33,7,8,34]. However, membership functions need to be determined which is often a challenging task. Also, no learning is offered by fuzzy theory. Support vector regression provides high accuracy in time series prediction [35,12,9,13,16]. It is free from local minimum. However, the computational burden is heavy due to solving quadratic programming problems. Also, choosing kernels and hyperparameters may not be an easy task. To overcome these difficulties, the least squares form of support vector regression was used [18] and a multiple-kernel framework was adopted [36]. Neuro-fuzzy modeling is a hybrid approach, which takes advantage of both fuzzy theory and neural network learning techniques, for modeling complex relationships between inputs and outputs [5,37,38,32]. However, deriving an appropriate set of fuzzy rules automatically and refining the associated parameters efficiently are two of the challenges involved.In this paper, we propose an approach based on neuro-fuzzy modeling for time series prediction. Given a predicting sequence, the local context of the sequence is located in a series of the observed data. A distance measure taking the trend of the data into account is adopted. Proper lags of relevant variables for prediction are selected and training patterns are extracted. Based on the extracted training patterns, a set of TSK fuzzy rules are constructed automatically by an incremental clustering algorithm. The parameters involved in the fuzzy rules are then refined by an efficient hybrid learning algorithm which incorporates a least squares estimator and the gradient descent method. The refined fuzzy rules constitute the predicting model and can then be used for prediction. Both direct and iterative forecasting models are developed. Our approach has several advantages. It can produce adaptive forecasting models. It works for univariate and multivariate prediction. It also works for one-step as well as multi-step prediction. The neuro-fuzzy modeling scheme is adopted since it incorporates the ideas of fuzzy theory and neural networks, offering good properties such as non-linear learning capability, quick convergence, and high accuracy. Furthermore, the rules obtained are comprehensible to human beings.The rest of this paper is organized as follows. Section 2 describes the problem to be solved. Section 3 gives a brief description of the adopted neuro-fuzzy modeling technique. Our proposed approach is outlined in Section 4. Section 5 describes the process of extracting training patterns from a series of the observed data. Section 6 describes the process of deriving a forecasting model from the training patterns. A small example is given in Section 7. Experimental results are presented in Section 8. Finally, concluding remarks are given in Section 9.Consider a series of real-valued observations [39]:(1)X0,Y0,X1,Y1,…,Xt,Yttaken at equally spaced time points t0, t0+Δt, t0+2Δt, … for some process P, where Yidenotes the value of the output variable (or dependent variable) observed at the time point t0+iΔt and Xidenotes the values of m additional variables (or independent variables), m≥0, observed at the time point t0+iΔt. Time series prediction is to estimate the value of Y at some future time t+s, i.e., Yt+s, by(2)Yˆt+s=G(Xt−q,Yt−q,…,Xt−1,Yt−1,Xt,Yt)where s≥1 is called the horizon of prediction, G is the predicting function or model, Yt−iis the ith lag of Yt, Xt−iis the ith lag of Xt, and q is the lag-span of the prediction. For s=1, it is called one-step prediction. For s>1, it is called multi-step prediction. Also, if m=0, it is univariate prediction; otherwise, it is multivariate prediction. For convenience,(3)Q=〈Xt−q,Yt−q,…,Xt−1,Yt−1,Xt,Yt〉is called the predicting sequence for predicting Yt+s.The prediction of Yt+scan be regarded as a function approximation task. Two strategies are usually adopted to construct forecasting models [23,24]:•Direct. Train on Xt−q, Yt−q, …, Xt−1, Yt−1, Xt, Ytto predict Yt+sdirectly, for any s≥1.Iterative. Train to predict Yt+1 only, but iterate to get Yt+sfor any s>1.Neuro-fuzzy modeling concerns deriving a model for an unknown system based on a set of training input–output data observed for the system. The derived model consists of a set of fuzzy rules which, given a certain input, can be used to predict the system output through fuzzy inference. In this paper, we adopt the neuro-fuzzy modeling technique proposed in [40]. Two stages, creation of fuzzy rules and refinement of fuzzy rules, are involved in the modeling process. In the first stage, a set of TSK fuzzy rules are generated from given training patterns. In the second stage, the parameters involved in the fuzzy rules are refined by a hybrid learning algorithm. The refined fuzzy rules constitute the predicting model for the unknown system and can then be used to predict the output of the system.Suppose the system we’d like to model has n input variables, denoted as x=〈x1, x2, …, xn〉, and one output variable, y. A brief description of the neuro-fuzzy modeling technique is given below.Assume that we are given a setTof N training patterns(pv,qv),1≤v≤N, wherepv=〈p1v,p2v,…,pnv〉andqvdenote the n input values and the desired output value, respectively, associated with thevth pattern. By applying an incremental clustering algorithm [38], the patterns inTare grouped into J clusters C1, C2, …, CJ. Each training pattern is assigned to only one cluster. Each cluster Cj, 1≤j≤J, is characterized by Gj(x) and hj, where Gj(x) is a distribution with mean mj=〈m1j, m2j, …, mnj〉 and deviationσj=〈σ1j, σ2j, …, σnj〉, and hjis the height of Cj. Note that mj,σj, and hjof Cjare computed as follows:•mijand σijare the average and deviation, respectively, of the ith coordinate of the patterns contained in Gj, 1≤i≤n.hjis the average of the desired output values of the patterns contained in Gj.When the J clusters are obtained, each cluster Cjis converted to a TSK-type fuzzy rule Rjof the following form [41]:(4)IFx1ISμ1j(x1)AND…ANDxnISμnj(xn)THENyISfj(x)=b0j+b1jx1+⋯+bnjxnwhere μij(xi) is assumed to be a Gaussian function, i.e.,(5)μij=exp−xi−mijσij2,which denotes the membership function of the fuzzy set associated with xi, and b0jis set to hjand all b1j, b2j, …, bnjare set to 0. For convenience, mijand σijare called antecedent parameters and b0j, b1j, …, bnjare called consequent parameters. These parameters will be adjusted later in the second stage. As a result, we have a rule baseR={R1,R2,…,RJ}consisting of J fuzzy rules.The process of creating fuzzy rules from the given training setTis summarized below.procedure create-fuzzy-rules(T)Group the training patterns inTinto J clusters;Convert the clusters to J fuzzy rules in the form of Eq. (4);endprocedureThe obtained rule setRcan be used to provide a system output for any given input through an interpolation of all the rules. For an input x=〈x1, x2, …, xn〉, the system output y can be computed by centroid defuzzification as follows:(6)y=∑j=1Jαj(x)×(b0j+b1jx1+⋯+bnjxn)∑k=1Jαk(x)=∑j=1Jαj(x)∑k=1Jαk(x)×(b0j+b1jx1+⋯+bnjxn)where αj(x) denotes the degree to which x matches Rj:(7)αj(x)=μ1j(x1)×μ2j(x2)×⋯×μnj(xn)which is called the firing strength of Rjfor x.To improve the performance of the fuzzy rules created in the previous stage, a hybrid learning algorithm which incorporates a least squares estimator and the gradient descent method is used to tune the antecedent and consequent parameters involved in each rule inR. Assume that we are given a setT′of N′ training patterns(pv′,qv′),1≤v≤N′. As described earlier,pv′=〈p1v′,…,pnv′〉andqv′denote the n input values and the desired output value, respectively, associated with thevth pattern. Note thatT′is not necessarily to be the setTused in the first stage.According to Eq. (6), the system outputyv′for any training inputpv′,1≤v≤N′, is(8)yv′=∑j=1Javj×(b0j+b1jp1v′+⋯+bnjpnv′)whereavj=αj(pv′)∑k=1Jαk(pv′). Let the mean square error E of all the N′ training patterns be defined as(9)E=1N′∑v=1N′(qv′−yv′)2.The parameters are tuned as follows. First, all the antecedent parameters are kept fixed and the consequent parameters are refined. By requiring E to be as small as possible, optimal values for the consequent parameters bij, 0≤i≤n, 1≤j≤J, can be obtained by finding the least squares solution to the following equation:(10)AB=DwhereD=q1′q2′…qN′′T,A=a11a11p11′…a11pn1′…a1Ja1Jp11′…a1Jpn1′a21a21p12′…a21pn2′…a2Ja2Jp12′…a2Jpn2′⋮⋮⋮⋮⋮⋮⋮⋮⋮aN′1aN′1p1N′′…aN′1pnN′′…aN′JaN′Jp1N′′…aN′JpnN′′,B=b01b11…bn1…b0Jb1J…bnJT.Next, all the consequent parameters are kept fixed and antecedent parameters are refined. The following gradient descent learning rules(11)mijnew=mijold−η1∂E∂mij,σijnew=σijold−η2∂E∂σijare applied to refine the antecedent parameters mijand σij, 1≤i≤n, 1≤j≤J, where∂E∂mij=2N′∑v=1N′[yv′−qv′]avj[fj(pv′)−yv′][piv′−mij]σij2,∂E∂σij=2N′∑v=1N′[yv′−qv′]avj[fj(pv′)−yv′][piv′−mij]2σij3.Note that η1 and η2 are predefined learning rates.For convenience, one epoch of learning for tuning the parameters is summarized below.procedure one-epoch-learning(T′)Keep the antecedent parameters as fixed, and refine the consequent parameters by Eq. (10);Keep the consequent parameters as fixed, and refine the antecedent parameters by Eq. (11);endprocedureIf necessary, the one-epoch-learning procedure can be applied iteratively until the required estimation accuracy is achieved.We give a brief complexity analysis of the adopted neuro-fuzzy modeling technique. Suppose there are J clusters obtained. The complexity of creating these J clusters and corresponding J fuzzy rules in the first stage is O(JNn). In the second stage, a complexity of O(JNn) is required to obtain the matrix A of Eq. (10). The least squares estimation for solving Eq. (10) takes a complexity of O(N(J(n+1))2). Therefore, the complexity of refining the consequent parameters is O(JNn)+O(N(J(n+1))2) in one epoch. The complexity of refining the antecedent parameters is O(JNn) in one epoch. Let λ be the number of epoches required for refining the fuzzy rules. The total complexity of the modeling technique is O(JNn)+λ(O(JNn)+O(N(J(n+1))2)+O(JNn)) which can be simplified to O(λNJ2n2).We propose a time series prediction approach based on the neuro-fuzzy modeling technique presented in the previous section. Without loss of generality, we consider Y and only one additional variable X in Eq. (1). Therefore, Eq. (1) becomes(12)X0,Y0,X1,Y1,…,Xt,Ytand our goal is to predict Yt+s, s≥1, based on this series of the observed data. Extension to the cases with more than one additional variable is trivial. As described in Section 2, we would like to predict Yt+sfrom Xt−q, Yt−q, …, Xt−1, Yt−1, Xt, Yt, and(13)Q=〈Xt−q,Yt−q,…,Xt−1,Yt−1,Xt,Yt〉is the predicting sequence for predicting Yt+s.Our approach consists of two phases, training and prediction. In the training phase, a forecasting model is constructed. Two steps, training patterns extraction and model derivation, are performed in this phase. In the training patterns extraction step, training patterns are extracted from the series of the observed data. Then in the model derivation step, the neuro-fuzzy modeling technique is applied to develop a forecasting model based on the extracted training patterns. In the prediction phase, the value of Yt+sis predicted by using the derived forecasting model. Both direct and iterative forecasting models are developed. Details are presented below.In the training phase, a suitable set of training patterns for Q are extracted from the series of the observed data. Firstly, we’d like to locate a local context consisting of certain sequences which are most similar to Q. Secondly, proper lags to be involved in training are selected. Then training patterns are extracted out.Intuitively, we learn from the observed data how•Yq+scan be predicted from X0, Y0, X1, Y1, …, Xq, Yq;Yq+1+scan be predicted from X1, Y1, X2, Y2, …, Xq+1, Yq+1;…;Ytcan be predicted from Xt−s−q, Yt−s−q, Xt−s+1−q, Yt−s+1−q, …, Xt−s, Yt−s.We define the most appropriate context for constructing a forecasting model for Yt+sto be the k nearest neighbors of Q inW, where k is a positive integer specified by the user. We adopt a measure, which is similar to that in [18], to compute the distance between two sequences. Let A1 and A2 be two sequences:A1=〈Xa−q,Ya−q,Xa+1−q,Ya+1−q,…,Xa−1,Ya−1,Xa,Ya〉,A2=〈Xb−q,Yb−q,Xb+1−q,Yb+1−q,…,Xb−1,Yb−1,Xb,Yb〉and F1 and F2 be the differential sequences of A1 and A2, respectively, defined in Table 1. Note that the differential sequence reveals the trend, rising or falling conditions, of the original sequence. A negative entry in the differential sequence indicates a falling occurrence at the underlying point in the original sequence while a positive entry indicates a rising occurrence. Let NE(A1, A2) be the normalized Euclidean distance between A1 and A2, and NE(F1, F2) be the normalized Euclidean distance between F1 and F2. Any measure taking both into account can be used to compute the distance between A1 and A2. For simplicity, we adopt the following measure:(16)NH(A1,A2)=NE(A1,A2)+NE(F1,F2)2to locate the k nearest neighbors of Q. We call NH(A1, A2) the hybrid distance between A1 and A2.We identify the k nearest neighbors of Q as follows. We calculate the hybrid distance between Q and every neighbor inW. As a result, we have z hybrid distances. The k sequences with the k shortest hybrid distances are taken to be the k nearest neighbors of Q. Let S1, S2, …, Skbe these k nearest neighbors with end indices t1, t2, …, tk, respectively. Then we have(17)S1=〈Xt1−q,Yt1−q,Xt1+1−q,Yt1+1−q,…,Xt1,Yt1〉;S2=〈Xt2−q,Yt2−q,Xt2+1−q,Yt2+1−q,…,Xt2,Yt2〉;…=…;Sk=〈Xtk−q,Ytk−q,Xtk+1−q,Ytk+1−q,…,Xtk,Ytk〉.Note that q≤ti≤t−s for any i, 1≤i≤k. We call the setS={S1,S2,…,Sk}the local context of Q.Next, we select certain lags out of Xt−q, Yt−q, Xt+1−q, Yt+1−q, …, Xt−1, Yt−1, Xt, and Ytfor training. One simple idea is to use them all and the number of lags is 2q+2 in this case. However, this is not necessarily a good idea. Usually, we do not know in advance how much the response time of the underlying system is. To be worse, the response time may vary along the way. If q is not large enough, we might miss some influencing inputs to the system. Therefore, q is usually set sufficiently large. Adopting all the lags may lead to the underfitting problem and the prediction accuracy can be poor. To avoid this, we only select a subset from the 2q+2 candidate lags.We adopt the concept of mutual information [42] for lags selection. Let the mutual information between two vectors U and V be denoted as MI(U, V). Intuitively, mutual information measures the information that U and V share. That is, it measures how much knowing one of these vectors reduces uncertainty about the other. If MI(U, V) is large, there is likely some strong connection between U and V. The adoption of mutual information, rather than others, e.g., correlation [43], is mainly due to its capability of measuring non-linear relationship between involved vectors.The s-step ahead outputs responding to the k nearest neighbors in Eq. (17) areYt1+s,Yt2+s,…,Ytk+s, respectively. Consequently, the resulting s-step input–output pairs are(18)(S1,Yt1+s),(S2,Yt2+s),…,(Sk,Ytk+s).From these pairs, we form 2q+3 vectors:(19)Z1,Z2,…,Z2q−1,Z2q,Z2q+1,Z2q+2,Hwhere(20)Z2i+1=Xt1+i−qXt2+i−q…Xtk+i−qT,0≤i≤q;Z2i+2=Yt1+i−qYt2+i−q…Ytk+i−qT,0≤i≤q;H=Yt1+sYt2+s…Ytk+sT.We use a greedy approach to find a desired number of lags. Let d be the number of lags to be found. Like feature selection in pattern recognition [44], there are several strategies for selecting the lags, such as exhaustive search, forward selection, and backward elimination. The exhaustive search is complete, but is computationally inefficient and thus is not practically applicable. Both forward selection and backward elimination are greedy and perform equally well [22]. However, forward selection is less demanding in memory consumption and thus is adopted here. Firstly, we calculate MI(Zi, H), 1≤i≤2q+2. LetMI(Zd1,H)be the largest, indicating the most significant connection exists betweenZd1and H. Therefore,Zd1is selected. Next, we calculateMI({Zd1,Zi},H), 1≤i≤2q+2 and i≠d1. LetMI({Zd1,Zd2},H)be the largest. Therefore,Zd2is also selected. Then we calculateMI({Zd1,Zd2,Zi},H), 1≤i≤2q+2 and i≠d1 and i≠d2. This goes on until d lags are found. The lags obtained are used to determine the training patterns to be extracted. For example, suppose d=3 and Z2q−1, Z2q+1, and Z2q+2 are selected. By referring to Eqs. (18) and (19), we extract the following k training patterns:(21)(〈Xtj−1,Xtj,Ytj〉,Ytj+s),1≤j≤k.These training patterns are to be used in training a direct forecasting model in the model derivation step. Accordingly, the input for forecasting Yt+sis 〈Xt−1, Xt, Yt〉.For s=1, the iterative forecasting model is totally identical to the direct forecasting model. For the case of s>1, one-step prediction is applied s times to get the estimate of Yt+s. To do this, we need to decompose the s-step prediction into s one-step predictions. As mentioned in Section 2, only Y lags are allowed in this case. Let the k nearest neighbors be those shown in Eq. (17) but without the X lags. Every sequence Sj, 1≤j≤k, is decomposed into s sequences:(22)Sj,0=〈Ytj−q,Ytj+1−q,…,Ytj〉;Sj,1=〈Ytj+1−q,Ytj+2−q,…,Ytj+1〉;…=…;Sj,s−1=〈Ytj+s−1−q,Ytj+s−q,…,Ytj+s−1〉.Therefore, we have the following ks one-step input–output pairs:(23)(S1,0,yt1+1),(S1,1,yt1+2),…,(S1,s−1,yt1+s),…,(Sk,0,ytk+1),(Sk,1,ytk+2),…,(Sk,s−1,ytk+s).From these input–output pairs, we form q+2 vectors:(24)Z1,Z2,…,Zq,Zq+1,Hwhere(25)Zi+1=Yt1+i−qYt1+i+1−q…Yt1+i+s−1−q…Ytk+i−qYtk+i+1−q…Ytk+i+s−1−qT,0≤i≤q;H=Yt1+1Yt1+2…Yt1+s…Ytk+1Ytk+2…Ytk+sT.Again, we find d desired lags by forward selection as described in Section 5.2.1. SupposeZr1+1,Zr2+1,…,Zrd+1are selected, with r1<r2<…<rd. By referring to Eqs. (23) and (24), we extract the following ks training patterns:(26)(〈Ytj+i+r1−q,Ytj+i+r2−q,…,Ytj+i+rd−q〉,Ytj+i+1),0≤i≤s−1,1≤j≤k.Note that each training pattern consists of an input sequence and the corresponding desired one-step output. For example, suppose d=3 and Zq−2, Zq, and Zq+1 are selected. Wee get the following ks training patterns:(27)(〈Ytj+i−3,Ytj+i−1,Ytj+i〉,Ytj+i+1),0≤i≤s−1,1≤j≤k.These training patterns are to be used in training an iterative forecasting model in the model derivation step. Accordingly, the input for forecasting Yt+sis 〈Yt−3, Yt−1, Yt〉.After we obtain the training patterns, we can proceed to derive direct and iterative forecasting models.The general architecture of a direct forecasting model is shown in Fig. 1(a). The k training patterns obtained in Section 5.2.1 are used to create and train the model. LetTbe the set of these k training patterns, i.e., N=k. Firstly, the procedure create-fuzzy-rules(T) is called to create a set of J TSK-type fuzzy rules. Secondly, the procedure one-epoch-learning(T) is called iteratively to refine the parameters involved in the fuzzy rules. Note that both procedures use the same set of training patterns. The trained fuzzy rules constitute the desired forecasting model. The direct model derivation process can be summarized below.procedure direct-forecasting-model-derivation(T)Call create-fuzzy-rules(T) to create J fuzzy rules;repeatCall one-epoch-learning(T) to refine the parameters;Calculate training error EL;untilELis acceptably small;endprocedureThe training error ELused in the above algorithm is calculated as(28)EL=1k∑j=1k(Ytj+s−Oj)2where Ojis the system output forYtj+s. The derived forecasting model can then be used to predict Yt+s.Consider the example at the end of Section 5.2.1. We use the training patterns shown in Eq. (21) to create and train the fuzzy rules. By applying the input 〈Xt−1, Xt, Yt〉 to the trained rules, we will getYˆt+sas shown in Fig. 1(b).For one-step prediction, i.e., s=1, the iterative forecasting model is totally identical to the direct forecasting model. For s>1, the general architecture of an iterative forecasting model is shown in Fig. 2(a). Note that there is a feedback from the output to the input of the model. The trained fuzzy rules basically form a one-step predictor. The outputs of the previous iterations may serve as part of the input of the current iteration. To getYˆt+s, s iterations are required. Consider the example at the end of Section 5.2.2. The iterative forecasting model for Yt+sis shown in Fig. 2(b) with the input 〈Yt−3, Yt−1, Yt〉. Let s=3. Three iterations are performed to obtainYˆt+3. In the first iteration, the outputYˆt+1is produced and pulled back, and the input becomes〈Yt−2,Yt,Yˆt+1〉. In the second iteration, the outputYˆt+2is produced and pulled back, and the input becomes〈Yt−1,Yˆt+1,Yˆt+2〉. In the third iteration, the outputYˆt+3is produced and we are done. Of course, a delay is required for the feedback and a kind of switching mechanism are needed to provide the required input in Fig. 2. For simplicity, they are not shown in this figure.The ks training patterns of Eq. (26) are used to create and train the fuzzy rules in Fig. 2(a). For convenience, these training patterns are grouped into s sets,T1,T2, …,Ts, as follows:T1={(〈Ytj+r1−q,Ytj+r2−q,…,Ytj+rd−q〉,Ytj+1)∣1≤j≤k};T1={(〈Ytj+r1+1−q,Ytj+r2+1−q,…,Ytj+rd+1−q〉,Ytj+2)∣1≤j≤k};…=…;Ts={(〈Ytj+s+r1−1−q,Ytj+s+r2−1−q,…,Ytj+s+rd−1−q〉,Ytj+s)∣1≤j≤k}.LetT=T1∪T2∪…∪Ts. Firstly, the procedure create-fuzzy-rules(T) is called to create a set of J TSK-type fuzzy rules. Secondly,T1,T2, …, andTsare applied successively and iteratively to refine the parameters until the learning error is acceptably small. The trained fuzzy rules constitute the desired forecasting model. The iterative model derivation process can be summarized below.procedure iterative-forecasting-model-derivation(T1,T2, …,Ts)Call create-fuzzy-rules(T1∪T2∪…∪Ts) to create J fuzzy rules;repeatfori from 1 to sCall one-epoch-learning(Ti) to refine the parameters;endfor;Calculate training error EL;untilELis acceptably small;endprocedureThe training error ELused in the above algorithm is identical to that shown in Eq. (28). Note thatT1,T2,…,andTsare applied in order. Since Yt+1, Yt+2, …, Yt+sare estimated in succession, it is natural and advantageous that the model is also trained in this order. The derived forecasting model can then be used to predict Yt+s.We give an example to illustrate how training patterns are extracted for the derivation of a direct forecasting model. Suppose we have a series of observations X0, Y0, X1, Y1, …, X19, Y19, as shown in Table 2. We want to forecast the value of Y21 based on the given data. For this case, we have t=19 and s=2. Let q=1, k=6, and d=2. According to Eq. (13), Q isQ=〈X18,Y18,X19,Y19〉=〈1.140,1.066,1.174,1.114〉and according to Eq. (14), the neighbor setWcontains the following 17 neighbors of Q:W1=〈X0,Y0,X1,Y1〉=〈1.118,1.073,1.116,1.062〉;W2=〈X1,Y1,X2,Y2〉=〈1.116,1.062,1.005,0.963〉;…=…;W16=〈X15,Y15,X16,Y16〉=〈1.161,1.097,1.065,0.990〉;W17=〈X16,Y16,X17,Y17〉=〈1.065,0.990,0.944,0.870〉.The training patterns for forecasting Y21 are extracted as described below.•Finding Local Context. We calculate the hybrid distance between Q and Wi, i=1, 2, …, 17, by Eq. (16). The distances are shown in Table 2. For example, the hybrid distance between Q and W17 is 0.9207, shown at the row with time index 17. The hybrid distance between Q and W16 is 0.6371, shown at the row with time index 16. The 6 nearest neighbors of Q are W1, W5, W8, W12, W13, and W14, with t1=1, t2=5, t3=8, t4=12, t5=13, and t6=14, respectively, for Eq. (17).Lags selection. According to Eq. (19), we setZ1=X0X4X7X11X12X13T;Z2=Y0Y4Y7Y11Y12Y13T;Z3=X1X5X8X12X13X14T;Z4=Y1Y5Y8Y12Y13Y14T;H=Y3Y7Y10Y14Y15Y16T.Firstly, we compute MI(Z1, H)=0.2500, MI(Z2, H)=0.2083, MI(Z3, H)=0.2083, and MI(Z4, H)=0.4583. Note that MI(Z4, H) is the largest, so Z4 is selected. Next, we compute MI({Z4, Z1}, H)=0.1667, MI({Z4, Z2}, H)=0.1250, and MI({Z4, Z3}, H)=0.2917. MI({Z4, Z3}, H) is the largest, so Z3 is also selected. Since d is 2, we end up with Z3 and Z4. Z3 and Z4 correspond to the last two elements in each nearest neighbor. Therefore, we get the following 6 training patterns:–(〈X1, Y1〉, Y3);(〈X5, Y5〉, Y7);(〈X8, Y8〉, Y10);(〈X12, Y12〉, Y14);(〈X13, Y13〉, Y15);(〈X14, Y14〉, Y16).

@&#CONCLUSIONS@&#
