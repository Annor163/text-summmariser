@&#MAIN-TITLE@&#
Proximal point algorithms for nonsmooth convex optimization with fixed point constraints

@&#HIGHLIGHTS@&#
We consider a nonsmooth convex optimization problem with fixed point constraints.We propose the Halpern-type incremental proximal algorithm for solving the problem.We propose the Mann-type incremental proximal algorithm for solving the problem.We present their convergence analyses for a diminishing step size.We give numerical examples to support the convergence analyses.

@&#KEYPHRASES@&#
Fixed point,Halpern algorithm,Incremental subgradient method,Krasnosel’skiĭ–Mann algorithm,Proximal point algorithm,

@&#ABSTRACT@&#
The problem of minimizing the sum of nonsmooth, convex objective functions defined on a real Hilbert space over the intersection of fixed point sets of nonexpansive mappings, onto which the projections cannot be efficiently computed, is considered. The use of proximal point algorithms that use the proximity operators of the objective functions and incremental optimization techniques is proposed for solving the problem. With the focus on fixed point approximation techniques, two algorithms are devised for solving the problem. One blends an incremental subgradient method, which is a useful algorithm for nonsmooth convex optimization, with a Halpern-type fixed point iteration algorithm. The other is based on an incremental subgradient method and the Krasnosel’skiĭ–Mann fixed point algorithm. It is shown that any weak sequential cluster point of the sequence generated by the Halpern-type algorithm belongs to the solution set of the problem and that there exists a weak sequential cluster point of the sequence generated by the Krasnosel’skiĭ–Mann-type algorithm, which also belongs to the solution set. Numerical comparisons of the two proposed algorithms with existing subgradient methods for concrete nonsmooth convex optimization show that the proposed algorithms achieve faster convergence.

@&#INTRODUCTION@&#
Convex optimization theory is a powerful tool for solving many practical problems in operational research (see, e.g., Ben-Israel, Levin, Levin, and Rozin, 2008; Plateau and Rios-Solis, 2010 and references therein). In particular, it has been widely used to solve practical convex minimization problems over complicated constraints, e.g., convex optimization problems with a fixed point constraint (Combettes, 2003; Iiduka, 2013; Iiduka & Hishinuma, 2014; Maingé, 2014; Yamada, 2001; Yao, Cho, & Liou, 2011) and with a variational inequality constraint (Facchinei, Pang, Scutari, & Lampariello, 2014; Maingé, 2010).Consider the following convex optimization problem: given a convex objective functionf:H→Rand a nonexpansive mapping T: H → H,(1)minimizef(x)subjecttox∈Fix(T),where H is a real Hilbert space and Fix(T) stands for the fixed point set of T. Problem (1) enables consideration of optimization problems with complicated constraint sets (Combettes and Bondon, 1999, Section 1), (Neto and De Pierro, 2010, Subsection 3.2), (Yamada, 2001, Section 4) onto which metric projections cannot be easily calculated. Several algorithms (e.g., Combettes, 2003; Iiduka, 2013; Iiduka & Hishinuma, 2014; Yamada, 2001) have been proposed for solving problem (1) when f is smooth and convex, which includes practical problems such as signal recovery (Combettes, 2003), beamforming (Slavakis & Yamada, 2007), and network resource allocation (Iiduka, 2013; Iiduka & Hishinuma, 2014).Here, problem (1) is considered for when f is convex but not always smooth. One objective is to devise optimization algorithms for nonsmooth convex optimization problem (1), which cannot be solved using conventional algorithms for smooth convex optimization (Combettes, 2003; Iiduka, 2013; Iiduka & Hishinuma, 2014; Yamada, 2001). There are significant problems with problem (1) when f is a general nonsmooth convex function (e.g., the L1-norm). They include the problem of minimizing the total variation of a signal over a convex set, Tykhonov-like problems with L1-norms (Combettes and Pesquet, 2007, 1. Introduction), the classifier ensemble problem with sparsity and diversity learning (Yin, Huang, Hao, Iqbal, and Wang, 2014a, Subsection 2.2.3), (Yin, Huang, Yang, Hao, and Wang, 2014b, Subsection 3.2.4), which is expressed as L1-norm minimization, and the minimal antenna-subset selection problem (Yamada, Yukawa, and Yamagishi, 2011, Subsection 17.4). Another objective is to solve problem (1) including the above real-world problems by using incremental optimization techniques. If the explicit forms of f and T in problem (1) are unknowable, algorithms making the best use of their mapping information cannot be applied to the problem. To enable us to consider such a case, a networked system with a finite number of users is assumed, and each user i is assumed to have its own private convex, nonsmooth objective function f(i) and nonexpansive mapping T(i). The main objective is to devise optimization algorithms that enable each user to find an optimal solution to problem (1) with(2)f:=∑i=1If(i)andFix(T):=⋂i=1IFix(T(i)),whereI∈Nis the number of users, without using the private information of other users.There have been many reports on incremental and parallel optimization algorithms. Parallel proximal algorithms (Bauschke and Combettes, 2011, Proposition 27.8), which use the proximity operators of nonsmooth, convex functions, are useful for minimizing the sum of nonsmooth, convex functions over the whole space. Incremental subgradient methods (Nedić & Bertsekas, 2001; Neto & De Pierro, 2010; Solodov & Zavriev, 1998) and projected multi-agent algorithms (Nedić, Olshevsky, Ozdaglar, & Tsitsiklis, 2009; Nedić & Ozdaglar, 2009, 2010) can minimize the sum of nonsmooth, convex functions for certain constraint sets by using the subgradients of the nonsmooth, convex functions instead of the proximity operators. The incremental subgradient algorithm (Neto & De Pierro, 2010) and the asynchronous proximal algorithm (Pesquet & Repetti, 2015) can work on nonsmooth convex optimization over sublevel sets of convex functions onto which the projections cannot be easily calculated. The incremental and parallel gradient algorithms (Iiduka, 2013; Iiduka & Hishinuma, 2014) can work on smooth convex optimization over fixed point sets of nonexpansive mappings. The incremental and parallel algorithms (Iiduka, 2015, 2016, 2015) use the subgradients of nonsmooth convex functions and can optimize the sum of the nonsmooth convex functions over fixed point sets of nonexpansive mappings. To the best of our knowledge, there have been no reports on incremental proximal point algorithms for nonsmooth convex optimization with fixed point constraints.Ideas from three useful types of algorithms, (I) proximal point algorithms, (II) incremental subgradient algorithms, and (III) fixed point algorithms, are used to achieve the main objective.(I) The well-known proximal point algorithms (see, e.g., Bauschke and Combettes, 2011, Chapter 27; Lions and Mercier, 1979; Martinet, 1970; Rockafellar, 1976 and references therein) for nonsmooth convex optimization use the proximity operators (Bauschke and Combettes, 2011, Definition 12.23), (Moreau, 1962) of convex functions. Here, it is assumed that user i can use the proximity operator of f(i), which is defined for all x ∈ H byProxf(i)(x)∈argminy∈H[f(i)(y)+12∥x−y∥2].(II) Incremental subgradient algorithms (Nedić & Bertsekas, 2001; Neto & De Pierro, 2010; Solodov & Zavriev, 1998) are useful algorithms for nonsmooth convex optimization. An iteration n of the algorithm is defined as follows: givenxn(0)∈H,(3)xn(i):=xn(i)(xn(i−1),f(i),T(i))(i=1,2,…,I),xn+1:=xn(I)=:xn+1(0).Under the assumption that user i can communicate with neighbor user(i−1),user i can implement algorithm (3) by using only its own private mappings f(i), T(i) and informationxn(i−1)transmitted from the neighbor user.(III) There are many fixed point algorithms (Berinde, 2007) for solving fixed point problems. Here, the focus is on using the Halpern fixed point algorithm (Halpern, 1967; Wittmann, 1992) and the Krasnosel’skiĭ–Mann fixed point algorithm (Krasnosel’skiĭ, 1955; Mann, 1953) to search for a fixed point of a nonexpansive mapping T. The former is defined as follows: for eachn∈N,xn+1:=αnx0+(1−αn)T(xn). The latter is defined asxn+1:=αnxn+(1−αn)T(xn),where x0 ∈ H and(αn)n∈N⊂[0,1]. When user i has x(i) ∈ H, f(i), and T(i) and informationxn(i−1)transmitted from user(i−1),user i can compute(4)xn(i):=αnx(i)+(1−αn)T(i)(yn(i)(xn(i−1),f(i))),which is based on the Halpern fixed point algorithm, or(5)xn(i):=αnxn(i−1)+(1−αn)T(i)(yn(i)(xn(i−1),f(i))),which is based on the Krasnosel’skiĭ–Mann fixed point algorithm, whereyn(i)is a point depending on onlyxn(i−1)and f(i). From (I),yn(i)can be defined using the value of the proximity operator of f(i) atxn(i−1); i.e.,(6)yn(i):=Proxf(i)(xn(i−1)).Two incremental proximal point algorithms are proposed for solving problem (1) with f and T defined by (2). One is based on the proximal point algorithm (6), the incremental subgradient method (3), and the Halpern fixed point algorithm (4). The other uses the ideas of the proximal point algorithm (6), the incremental subgradient method (3), and the Krasnosel’skiĭ–Mann fixed point algorithm (5).Here, let us explicitly compare the two proposed algorithms with the existing algorithms (Iiduka, 2015, 2016, 2015). The proposed and existing algorithms can be applied to problem (1) with (2). The convergence analyses in Iiduka (2015, 2016, 2015) showed that there exists a weak sequential cluster point of the sequence generated by one of the existing algorithms that belongs to the solution set of problem (1) with (2). However, these results are not strong enough. This is because knowing the existence of one optimal cluster point cannot help users to identify an optimal solution when multiple cluster points are observed. In contrast to the results in Iiduka (2015, 2016, 2015), one of the proposed algorithms (Algorithm 3.1) satisfies a gratifying convergence property such that any weak sequential cluster point of the sequence generated by the proposed algorithm belongs to the solution set of problem (1) with (2) under certain assumptions (Theorem 3.1). This result is attributed to the framework of the algorithm being based on the Halpern fixed point algorithm (4) as compared with the existing algorithms, which are based on the Krasnosel’skiĭ–Mann fixed point algorithm (5). Since the other proposed algorithm (Algorithm 4.1) is based on the Krasnosel’skiĭ–Mann fixed point algorithm (5), it is not guaranteed that any weak sequential cluster point of the sequence in Algorithm 4.1 belongs to the solution set (Theorem 4.1). However, Algorithm 4.1 can work when the step sizes are constant, which are the most tractable choice of step size sequences, in contrast to Algorithm 3.1, which uses diminishing step size sequences.One contribution of this paper is analysis of the proposed algorithms’ convergence. It is shown that, under certain assumptions, any weak sequential cluster point of the sequence generated by the Halpern-type algorithm belongs to the solution set of the problem and that there exists a weak sequential cluster point of the sequence generated by the Krasnosel’skiĭ–Mann-type algorithm, which also belongs to the solution set. Another contribution of this paper is provision of examples showing that the proposed algorithms perform better than subgradient-type algorithms. In this paper, concrete nonsmooth convex optimization problems are discussed, and the two proposed algorithms are numerically compared with the existing subgradient methods to evaluate their effectiveness.This paper is organized as follows. Section 2 gives the mathematical preliminaries. Section 3 presents the incremental proximal point algorithm based on the Halpern fixed point algorithm and analyzes its convergence. Section 4 presents the incremental proximal point algorithm based on the Krasnosel’skiĭ–Mann fixed point algorithm and analyzes its convergence. Section 5 describes concrete nonsmooth convex optimization problems and numerically compares the behaviors of the two proposed algorithms with those of the existing algorithms. Section 6 concludes the paper with a brief summary and mentions future directions for improving the proposed algorithms.Let H be a real Hilbert space with inner product ⟨·, ·⟩ and its induced norm ‖·‖, letRbe the set of all real numbers, and letNbe the set of all positive integers including zero. LetFix(T):={x∈H:T(x)=x}be the fixed point set of a mapping T: H → H. Let dom(f) ≔ {x ∈ H: f(x) < ∞} be the domain of a functionf:H→(−∞,∞]. The identity mapping on H is denoted by Id. Let(xn)n∈Nbe a sequence in H. A point x ∈ H is said to be a weak sequential cluster point of(xn)n∈N(Bauschke and Combettes, 2011, Subchapters 1.7 and 2.5) if(xn)n∈Npossesses a subsequence that weakly converges to x ∈ H.A mapping T: H → H is said to be nonexpansive (Bauschke and Combettes, 2011, Definition 4.1(ii)) if∥T(x)−T(y)∥≤∥x−y∥(x, y ∈ H). T is said to be firmly nonexpansive (Bauschke and Combettes, 2011, Definition 4.1(i)) if∥T(x)−T(y)∥2+∥(Id−T)(x)−(Id−T)(y)∥2≤∥x−y∥2(x, y ∈ H). The metric projection PConto a nonempty, closed convex subset C of H is firmly nonexpansive withFix(PC)=C(Bauschke and Combettes, 2011, Proposition 4.8, (4.8)).Letf:H→(−∞,∞]be proper, lower semicontinuous, and convex. Then, the proximity operator of f (Bauschke and Combettes, 2011, Definition 12.23), (Moreau, 1962), denoted by Proxf, maps every x ∈ H to the unique minimizer off+(1/2)∥x−·∥2; i.e.,Proxf(x)=argminy∈H[f(y)+12∥x−y∥2](x∈H).The uniqueness and existence of Proxf(x) are guaranteed for all x ∈ H (Bauschke and Combettes, 2011, Definition 12.23), (Minty, 1965). The subdifferential of f is the set-valued operator∂f:H→2H:x↦{u∈H:f(y)≥f(x)+〈y−x,u〉(y∈H)}.Proposition 2.1Bauschke and Combettes, 2011, Propositions 12.26, 12.27, 12.28, and 16.14Letf:H→(−∞,∞]be proper, lower semicontinuous, and convex. Then, the following hold:(i)Let x, p ∈ H.p=Proxf(x)if and only ifx−p∈∂f(p)(i.e.,〈y−p,x−p〉+f(p)≤f(y)for all y ∈ H).Proxfis firmly nonexpansive withFix(Proxf)=argminx∈Hf(x).If f is continuous at x ∈ dom(f), ∂f(x) is nonempty. Moreover, δ > 0 exists such that ∂f(B(x; δ)) is bounded, where B(x; δ) stands for a closed ball with center x and radius δ.Consider a networked system consisting of I users, where user i (i∈I:={1,2,…,I}) is assumed to have its own private mappings11The explicit forms of T(i) and f(i) are user i’s private information; i.e., other users cannot get the explicit forms of T(i) and f(i).T(i) and f(i). The following problem is discussed.Problem 2.1Assume that(A1)T(i): H → H (i∈I) is firmly nonexpansive with⋂i∈IFix(T(i))≠∅;f(i):H→R(i∈I) is continuous and convex withdom(f(i))=HandProxf(i)can be efficiently computed.22Tables 10.1 and 10.2 in Combettes and Pesquet (2011) present important examples of convex functions for which proximity operators can be easily computed within a finite number of arithmetic operations.Then,minimizef(x):=∑i∈If(i)(x)subjecttox∈X:=⋂i∈IFix(T(i)).The existence of a solution to Problem 2.1 is guaranteed when at least one of Fix(T(i)) is bounded (Zeidler, 1985, Theorem 25.C). Under the assumptions in the main theorems (Theorems 3.1 and 4.1), the existence of a solution to Problem 2.1 is guaranteed (see Lemmas 3.3, 4.2(iv), and 4.3(iv)). If at least one of f(i) is strictly convex, the uniqueness of the solution to Problem 2.1 is also guaranteed (Zeidler, 1985, Corollary 25.15).The following propositions will be used to prove the main theorems in this paper.Proposition 2.2Berinde, 2007, Lemma 1.2Assume that(an)n∈N⊂[0,∞)satisfiesan+1≤(1−αn)an+αnβn(n∈N),where(αn)n∈N⊂(0,1]and(βn)n∈N⊂Rwith∑n=1∞αn=∞andlim supn→∞βn≤0. Then,limn→∞an=0.Suppose that(xn)n∈N⊂Hweakly converges tox^∈Handx¯≠x^. Then,lim infn→∞∥xn−x^∥<lim infn→∞∥xn−x¯∥.Whenf:H→Ris convex, f is weakly lower semicontinuous if and only if f is lower semicontinuous.This section presents the following algorithm for solving Problem 2.1 using the Halpern algorithm (Halpern, 1967; Wittmann, 1992) for finding a fixed point of a nonexpansive mapping.Algorithm 3.1Step 0.User i (i∈I) chooses x(i) ∈ H arbitrarily and sets(αn)n∈N⊂(0,1]and(γn)n∈N⊂(0,∞). User I sets x0 ∈ H arbitrarily and transmitsx0(0):=x0∈Hto user 1.User i (i∈I) computesxn(i)∈Hcyclically usingxn(i):=αnx(i)+(1−αn)T(i)(Proxγnf(i)(xn(i−1)))(i=1,2,…,I).User I definesxn+1∈Husingxn+1:=xn(I)and transmitsxn+1(0):=xn+1to user 1. The value of n is then set ton+1,and the processing returns to Step 1.The stopping criterions of Algorithm 3.1 are given by, for example,∑i∈I∥xn−T(i)(xn)∥<ϵ1and|f(xn−1)−f(xn)|<ϵ2,where ϵi> 0(i=1,2)is small enough. However, in general, such stopping criterions cannot be included in Algorithm 3.1 because none of the users can use all xn, all T(i), and all f(i). If there exists an operator who manages the networked system and communicates with all users, the operator can verify whether the stopping criterions of Algorithm 3.1 are satisfied. The numerical section provides the number of iterations and elapsed time such that Algorithm 3.1 satisfies|f(xn−1)−f(xn)|<10−3(see Section 5 for details).All users participating in the network are assumed to have the following information before the algorithm is executed.Assumption 3.1User i (i∈I) uses(αn)n∈N⊂(0,1]and(γn)n∈N⊂(0,∞),which converge to 0 and satisfy the following conditions:33Examples of(γn)n∈Nand(αn)n∈Nareγn:=1/(n+1)aandαn:=1/(n+1)b(a∈(0,1/2),b∈(a,1−a),a+b<1).(C1)∑n=0∞αn=∞,(C2)limn→∞1αn+1|1γn+1−1γn|=0,(C3)limn→∞1γn+1|1−αnαn+1|=0,(C4)limn→∞1αn+1|γn+1−γn|γn+12=0,(C5)limn→∞αnγn=0.Step 1 in Algorithm 3.1 is a search for the fixed point of T(i), which is based on the Halpern algorithm (Halpern, 1967; Wittmann, 1992) defined by x0 ∈ H andxn+1=αnx0+(1−αn)T(i)(xn)(n∈N). The algorithm withlimn→∞αn=0and (C1) strongly converges to the minimizer of∥·−x0∥2over Fix(T(i)) (Halpern, 1967; Wittmann, 1992). Moreover, sincexn(i)in Step 1 uses the proximity operatorProxγnf(i),it can be seen intuitively that, for alli∈I,(xn(i))n∈Nin Step 1 converges to not only a fixed point of T(i) but also a minimizer of f(i). Furthermore, Steps 1 and 2 in Algorithm 3.1 lead to the finding thatxn+1=xn(I)(xn(I−1))=xn(I)(xn(I−1)(xn(I−2)))=⋯=xn(I)(xn(I−1),xn(I−2),…,xn(1)); i.e.,xn+1has all the information ofxn(i)(i∈I)needed to optimize f(i) over Fix(T(i)). Hence, it can be seen that(xn)n∈Napproximates a minimizer of∑i∈If(i)over⋂i∈IFix(T(i)). See Subsection 3.1 for the proof for the convergence property of(xn)n∈Nin Algorithm 3.1.This convergence result depends on the following assumption.Assumption 3.2The sequence(yn(i):=Proxγnf(i)(xn(i−1)))n∈N(i∈I) generated by Algorithm 3.1 is bounded.Assume that, for alli∈I,argminx∈Hf(i)(x)(=Fix(Proxf(i)))≠∅and Fix(T(i)) is bounded. Then, user i can choose in advance a bounded, closed convex set X(i) (e.g., X(i) is a closed ball with a large enough radius) satisfying X(i) ⊃ Fix(T(i)). Accordingly, user i can compute, for example,(7)xn(i):=PX(i)[αnx(i)+(1−αn)T(i)(yn(i))]instead ofxn(i)in Algorithm 3.1. Since X(i)(i∈I)is bounded,(xn(i))n∈N(i∈I)is bounded. Moreover, since Proposition 2.1(ii) ensures that∥yn(i)−x∥≤∥xn(i−1)−x∥(i∈I,x∈Fix(Proxf(i))), the boundedness of(xn(i))n∈N(i∈I) guarantees that(yn(i))n∈N(i∈I) is bounded. Hence, it can be assumed that(xn(i))n∈N(i∈I)in Algorithm 3.1 is as in (7) in place of Assumption 3.2.Next, a convergence analysis of Algorithm 3.1 is presented.Theorem 3.1Under Assumptions (A1), (A2),3.1, and3.2, any weak sequential cluster point of(xn(i))n∈N(i∈I) generated by Algorithm3.1belongs to the solution set of Problem2.1.An application example of Algorithm 3.1 is as follows. Let X(i) ⊂ H(i∈I)be bounded, closed, and convex (see (7)), letCk(i)⊂H(i∈I,k∈K(i):={1,2,…,K(i)})be a closed convex set onto which the projection can be easily calculated, and let(wk(i))k∈K(i)⊂(0,1)(i∈I)satisfy∑k∈K(i)wk(i)=1. Here let us define(8)g(i)(x):=12∑k∈K(i)wk(i)(miny∈Ck(i)∥x−y∥)2(x∈H),(9)T¯(i):=PX(i)(∑k∈K(i)wk(i)PCk(i)),andT(i):=12(Id+T¯(i))(i∈I).The function g(i)(i∈I)defined by (8) stands for the mean square value of the distances from x ∈ H toCk(i)s. Accordingly, we can express a subset of X(i) with the elements closest toCk(i)s in terms of the mean square norm by(10)Cg(i):={x∈X(i):g(i)(x)=miny∈X(i)g(i)(y)}(i∈I).TheCg(i)is referred to as the generalized convex feasible set (Combettes and Bondon, 1999, Section 1, Framework 2), (Yamada, 2001, Definition 4.1). The conditionCg(i)≠∅(i∈I)holds from the boundedness of X(i) (Yamada, 2001, Remark 4.3(a)). Even ifX(i)∩⋂k∈K(i)Ck(i)=∅,Cg(i)is well-defined. In particular,Cg(i)=X(i)∩⋂k∈K(i)Ck(i)holds whenX(i)∩⋂k∈K(i)Ck(i)≠∅. Furthermore,T¯(i)(i∈I)is nonexpansive withFix(T¯(i))=Cg(i)(Yamada, 2001, Proposition 4.2); i.e., T(i)(i∈I)defined by (9) is firmly nonexpansive andFix(T(i))=Fix(T¯(i))=Cg(i)(i∈I).Therefore, Theorem 3.1 leads to the following.Corollary 3.1Let T(i)(i∈I) be a mapping defined by(9), let f(i)(i∈I) satisfy (A2), and let(xn(i))n∈N(i∈I) be the sequence generated by(7), where(αn)n∈Nand(γn)n∈Nsatisfy Assumption3.1. Then, any weak sequential cluster point of(xn(i))n∈N(i∈I) belongs to the solution set of Problem2.1withX=⋂i∈ICg(i).Section 5 applies the proposed algorithms to the problem of minimizing f over⋂i∈ICg(i)and compares the behaviors of the proposed algorithms with the existing ones.First, the following lemma is proven.Lemma 3.1Suppose that Assumptions (A1), (A2), and3.2hold and(xn(i))n∈Nand(yn(i))n∈N(i∈I) are the sequences generated byAlgorithm 3.1. Then,(T(i)(yn(i)))n∈Nand(xn(i))n∈N(i∈I) are bounded.Assumption (A1) guarantees that∥T(i)(yn(i))−x∥≤∥yn(i)−x∥(i∈I,n∈N,x∈X),which, together with Assumption 3.2, implies that(T(i)(yn(i)))n∈N(i∈I)is bounded. The definition ofxn(i)(i∈I,n∈N)and the boundedness of(T(i)(yn(i)))n∈Nlead to the boundedness of(xn(i))n∈N(i∈I).□Next, the following lemma is considered.Lemma 3.2Suppose that Assumptions (A1), (A2),3.1, and3.2are satisfied. Then, the following hold:(i)limn→∞∥xn+1−xn∥/γn=0;limn→∞∥yn(i)−T(i)(yn(i))∥=0andlimn→∞∥xn(i−1)−yn(i)∥=0(i∈I);limn→∞∥xn−yn(i)∥=0andlimn→∞∥xn−T(i)(xn)∥=0(i∈I).(i)The definition ofxn(i)(i∈I,n∈N)and (A1) imply that, for alli∈Iand for all n ≥ 1,∥xn(i)−xn−1(i)∥=∥(1−αn)(T(i)(yn(i))−T(i)(yn−1(i)))+(αn−αn−1)(x(i)−T(i)(yn−1(i)))∥≤(1−αn)∥T(i)(yn(i))−T(i)(yn−1(i))∥+|αn−αn−1|∥x(i)−T(i)(yn−1(i))∥≤(1−αn)∥yn(i)−yn−1(i)∥+M1|αn−αn−1|,whereM1:=maxi∈I(sup{∥x(i)−T(i)(yn(i))∥:n∈N})and M1 < ∞ holds from Lemma 3.1. Given definitiony¯n(i):=Proxγn+1f(i)(xn(i−1))(i∈I,n∈N),Proposition 2.1(ii) ensures that, for alli∈Iand for all n ≥ 1,∥yn(i)−yn−1(i)∥≤∥Proxγnf(i)(xn(i−1))−Proxγnf(i)(xn−1(i−1))∥+∥y¯n−1(i)−yn−1(i)∥≤∥xn(i−1)−xn−1(i−1)∥+∥y¯n−1(i)−yn−1(i)∥.Proposition 2.1(i) means thatyn−1(i):=Proxγn−1f(i)(xn−1(i−1))andy¯n−1(i):=Proxγnf(i)(xn−1(i−1))satisfy(xn−1(i−1)−yn−1(i))/γn−1∈∂f(i)(yn−1(i))and(xn−1(i−1)−y¯n−1(i))/γn∈∂f(i)(y¯n−1(i)). Accordingly, the monotonicity of ∂f(i) guarantees that, for alli∈Iand for all n ≥ 1,〈yn−1(i)−y¯n−1(i),xn−1(i−1)−yn−1(i)γn−1−xn−1(i−1)−y¯n−1(i)γn〉≥0.Hence,1γn−1γn{〈yn−1(i)−y¯n−1(i),(γn−γn−1)xn−1(i−1)〉+〈yn−1(i)−y¯n−1(i),−γn(yn−1(i)−y¯n−1(i))〉+〈yn−1(i)−y¯n−1(i),(γn−1−γn)y¯n−1(i)〉}≥0,which, together with the triangle inequality, means that∥yn−1(i)−y¯n−1(i)∥2≤|γn−γn−1|γn(∥xn−1(i−1)∥+∥y¯n−1(i)∥)∥yn−1(i)−y¯n−1(i)∥≤M2|γn−γn−1|γn∥yn−1(i)−y¯n−1(i)∥,whereM2:=maxi∈I(sup{∥xn(i−1)∥+∥y¯n(i)∥:n∈N})and M2 < ∞ holds from Lemma 3.1, Assumption 3.2, and Proposition 2.1(ii). Thus, for alli∈Iand for all n ≥ 1,∥yn−1(i)−y¯n−1(i)∥≤M2|γn−γn−1|γn.Therefore, for alli∈Iand for all n ≥ 1,(11)∥xn(i)−xn−1(i)∥≤(1−αn)∥xn(i−1)−xn−1(i−1)∥+M2|γn−γn−1|γn+M1|αn−αn−1|,which implies that, for all n ≥ 1,∥xn+1−xn∥≤(1−αn)∥xn−xn−1∥+IM1|αn−αn−1|+IM2|γn−γn−1|γn.Hence, for all n ≥ 1,∥xn+1−xn∥γn≤(1−αn)∥xn−xn−1∥γn−1+(1−αn){∥xn−xn−1∥γn−∥xn−xn−1∥γn−1}+IM1|αn−αn−1|γn+IM2|γn−γn−1|γn2≤(1−αn)∥xn−xn−1∥γn−1+IM1|αn−αn−1|γn+IM2|γn−γn−1|γn2+M3|1γn−1γn−1|,whereM3:=sup{∥xn+1−xn∥:n∈N}<∞. This leads to the finding that∥xn+1−xn∥γn≤(1−αn)∥xn−xn−1∥γn−1+αnXn(n≥1),whereXn:=IM11αn|αn−αn−1|γn+IM21αn|γn−γn−1|γn2+M31αn|1γn−1γn−1|(n≥1).Proposition 2.2 and (C1), (C2), (C3), and (C4) ensure that(12)limn→∞∥xn+1−xn∥γn=0.Eq. (12) andlimn→∞γn=0imply thatlimn→∞∥xn+1−xn∥=0.The convexity of ‖ · ‖2 and (A1) guarantee that, for all x ∈ X, for alln∈N,and for alli∈I,∥xn(i)−x∥2≤αn∥x(i)−x∥2+(1−αn)∥T(i)(yn(i))−T(i)(x)∥2≤αn∥x(i)−x∥2+∥yn(i)−x∥2−(1−αn)∥yn(i)−T(i)(yn(i))∥2.Proposition 2.1(i) andyn(i):=Proxγnf(i)(xn(i−1))(i∈I,n∈N)mean that, for all x ∈ X, for alln∈N,and for alli∈I,〈x−yn(i),xn(i−1)−yn(i)〉≤γn(f(i)(x)−f(i)(yn(i))).Moreover, from〈x,y〉=(1/2)(∥x∥2+∥y∥2−∥x−y∥2)(x, y ∈ H),〈x−yn(i),xn(i−1)−yn(i)〉=12(∥x−yn(i)∥2+∥xn(i−1)−yn(i)∥2−∥x−xn(i−1)∥2)for all x ∈ X, for alli∈I,and for alln∈N. Hence, for all x ∈ X, for alln∈N,and for alli∈I,(13)∥yn(i)−x∥2≤∥xn(i−1)−x∥2−∥xn(i−1)−yn(i)∥2+2γn(f(i)(x)−f(i)(yn(i))).Accordingly, settingM4:=maxi∈I∥x(i)−x∥2(x ∈ X) leads to(14)∥xn(i)−x∥2≤M4αn−(1−αn)∥yn(i)−T(i)(yn(i))∥2+∥xn(i−1)−x∥2−∥xn(i−1)−yn(i)∥2+2γn(f(i)(x)−f(i)(yn(i))).Since Proposition 2.1(iii) and (A2) ensure the existence of z(i) ∈ ∂f(i)(x) and the boundedness of ∂f(i)(x)(x∈X,i∈I),the definition of ∂f(i) and Assumption 3.2 imply that there exists M5 < ∞ such that2(f(i)(x)−f(i)(yn(i)))≤2〈x−yn(i),z(i)〉≤2∥x−yn(i)∥∥z(i)∥≤M5(x∈X,i∈I,n∈N). Hence, for all x ∈ X and for alln∈N,∥xn+1−x∥2≤IM4αn−(1−αn)∑i∈I∥yn(i)−T(i)(yn(i))∥2+∥xn−x∥2+IM5γn−∑i∈I∥xn(i−1)−yn(i)∥2.Since Lemma 3.1 means the existence of M6 < ∞ such that, for all x ∈ X and for alln∈N,∥xn−x∥2−∥xn+1−x∥2=(∥xn−x∥−∥xn+1−x∥)(∥xn−x∥+∥xn+1−x∥)≤M6∥xn+1−xn∥,we have that, for all x ∈ X and for alln∈N,(1−αn)∑i∈I∥yn(i)−T(i)(yn(i))∥2≤IM4αn+IM5γn+M6∥xn+1−xn∥,∑i∈I∥xn(i−1)−yn(i)∥2≤IM4αn+IM5γn+M6∥xn+1−xn∥,which, together withlimn→∞∥xn+1−xn∥=0andlimn→∞αn=limn→∞γn=0,implies that(15)limn→∞∥yn(i)−T(i)(yn(i))∥=0andlimn→∞∥xn(i−1)−yn(i)∥=0(i∈I).From∥xn(i)−T(i)(yn(i))∥=αn∥x(i)−T(i)(yn(i))∥(i∈I,n∈N)andlimn→∞αn=0,limn→∞∥xn(i)−T(i)(yn(i))∥=0(i∈I). Since, for alli∈Iand for alln∈N,∥xn−xn(i−1)∥≤∑j=1i−1(∥xn(j−1)−yn(j)∥+∥yn(j)−T(j)(yn(j))∥+∥T(j)(yn(j))−xn(j)∥),(15) andlimn→∞∥xn(i)−T(i)(yn(i))∥=0(i∈I)guarantee thatlimn→∞∥xn−xn(i−1)∥=0(i∈I). From∥yn(i)−xn∥≤∥yn(i)−xn(i−1)∥+∥xn(i−1)−xn∥(i∈I,n∈N),(15) implies that(16)limn→∞∥xn−yn(i)∥=0(i∈I).Moreover, since∥xn−T(i)(xn)∥≤∥xn−yn(i)∥+∥yn(i)−T(i)(yn(i))∥+∥T(i)(yn(i))−T(i)(xn)∥(i∈I,n∈N),(A1), (15), and (16) ensure that(17)limn→∞∥xn−T(i)(xn)∥=0(i∈I).This proves Lemma 3.2.□Lemmas 3.1 and 3.2 lead to the following lemma.Lemma 3.3Suppose that the assumptions inLemma 3.2hold. Then, the following hold:(i)lim supn→∞f(xn)≤f(x)for all x ∈ X;There exists a weak sequential cluster point of(xn)n∈Nthat belongs to the solution set X⋆ of Problem 2.1;Any weak sequential cluster point of(xn(i))n∈N(i∈I) is in X⋆.(i)Inequality (14) guarantees that, for all x ∈ X, for alln∈N,and for alli∈I,∥xn(i)−x∥2≤∥xn(i−1)−x∥2+2γn(f(i)(x)−f(i)(yn(i)))+M4αn,which, together withxn+1=xn(I)=xn+1(0)(n∈N)andf:=∑i∈If(i),implies that∥xn+1−x∥2≤∥xn−x∥2+2γn∑i∈I(f(i)(x)−f(i)(yn(i)))+IM4αn=∥xn−x∥2+IM4αn+2γn(f(x)−f(xn)+∑i∈I[f(i)(xn)−f(i)(yn(i))]).Since Lemma 3.1 means that M6 < ∞ exists such that∥xn−x∥2−∥xn+1−x∥2≤M6∥xn+1−xn∥(x∈X,n∈N),for all x ∈ X and for alln∈N,2(f(xn)−f(x))≤M6∥xn−xn+1∥γn+IM4αnγn+2∑i∈I[f(i)(xn)−f(i)(yn(i))].Moreover, the definition of ∂f(i)(i∈I),(A2), Lemma 3.1, and Proposition 2.1(iii) lead to the existence of M7 < ∞ such that, for alli∈Iand for alln∈N,f(i)(xn)−f(i)(yn(i))≤M7∥xn−yn(i)∥,which, together with (16), implies thatlim supn→∞[f(i)(xn)−f(i)(yn(i))]≤0(i∈I). Hence, (12) and (C5) ensure that2lim supn→∞(f(xn)−f(x))≤2∑i∈Ilim supn→∞[f(i)(xn)−f(i)(yn(i))]≤0.Therefore,lim supn→∞f(xn)≤f(x)(x ∈ X).Lemma 3.1 guarantees the existence of a weak sequential cluster point of(xn)n∈N. Let x* ∈ H be an arbitrary weak sequential cluster point of(xn)n∈N. Then, there exists(xnk)k∈N(⊂(xn)n∈N)such that(xnk)k∈Nweakly converges to x*. Here,i∈Iis arbitrarily fixed, and x* ∉ Fix(T(i)) is assumed. Then, Proposition 2.3, Lemma 3.2(iii), and (A1) produce a contradiction:lim infk→∞∥xnk−x*∥<lim infk→∞∥xnk−T(i)(x*)∥=lim infk→∞∥xnk−T(i)(xnk)+T(i)(xnk)−T(i)(x*)∥=lim infk→∞∥T(i)(xnk)−T(i)(x*)∥≤lim infk→∞∥xnk−x*∥.Therefore, x* ∈ Fix(T(i))(i∈I); i.e., x* ∈ X. Moreover, (A2), the weak convergence of(xnk)k∈Nto x* ∈ X, and Proposition 2.4 imply thatf(x*)≤lim infk→∞f(xnk). Accordingly, Lemma 3.3(i) guarantees that, for all x ∈ X,f(x*)≤lim infk→∞f(xnk)≤lim supk→∞f(xnk)≤lim supn→∞f(xn)≤f(x);i.e.,x*∈X☆.Lemma 3.3(ii) means that any weak sequential cluster point of(xn)n∈Nis in X⋆. From Lemma 3.1,limn→∞∥xn−xn(i−1)∥=0(i∈I),andxn+1=xn(I)(n∈N),any weak sequential cluster point of(xn(i))n∈N(i∈I)is in X⋆. This completes the proof.□The following algorithm using the Krasnosel’skiĭ–Mann algorithm (Krasnosel’skiĭ, 1955; Mann, 1953) is presented.Algorithm 4.1Step 0.User i (i∈I) sets(αn)n∈N⊂(0,1]and(γn)n∈N⊂(0,∞). User I sets x0 ∈ H arbitrarily and transmitsx0(0):=x0∈Hto user 1.User i (i∈I) computesxn(i)∈Hcyclically usingxn(i):=αnxn(i−1)+(1−αn)T(i)(Proxγnf(i)(xn(i−1)))(i=1,2,…,I).User I definesxn+1∈Husingxn+1:=xn(I)and transmitsxn+1(0):=xn+1to user 1. The value of n is then set ton+1,and the processing returns to Step 1.Two assumptions are made here.Assumption 4.1User i (i∈I) uses(αn)n∈N⊂(0,1]and(γn)n∈N⊂(0,∞)satisfying the following conditions:44Examples of(γn)n∈Nand(αn)n∈Nareγn:=1/(n+1)aand αn≔ t (a ∈ (0, 1], t ∈ (0, 1)).(C6)0<lim infn→∞αn≤lim supn→∞αn<1,(C7)limn→∞γn=0,(C8)∑n=0∞γn=∞.The sequence(yn(i):=Proxγnf(i)(xn(i−1)))n∈N(i∈I) generated by Algorithm 4.1 is bounded.Step 1 in Algorithm 4.1 is a search for the fixed point of T(i), which is based on the Krasnosel’skiĭ–Mann algorithm (Krasnosel’skiĭ, 1955; Mann, 1953) defined by x0 ∈ H andxn+1=αnxn+(1−αn)T(i)(xn)(n∈N). It is guaranteed that the algorithm with (C6) weakly converges to a fixed point of T(i) (Krasnosel’skiĭ, 1955; Mann, 1953). Accordingly, from the use of the proximity operatorProxγnf(i),it can be seen intuitively that(xn(i))in Step 1 approximates a fixed point of T(i) as well as a minimizer of f(i). From the incremental steps in Steps 1 and 2 (see also the discussion of Algorithm 3.1), it can be seen that Algorithm 4.1 optimizes∑i∈If(i)over⋂i∈IFix(T(i)). The mathematical proof for the convergence property of(xn)n∈Nin Algorithm 4.1 is given in Subsection 4.1.Next, a convergence analysis of Algorithm 4.1 is presented.Theorem 4.1Under Assumptions (A1), (A2),4.1, and4.2, there exists a weak sequential cluster point of(xn(i))n∈N(i∈I) generated by Algorithm4.1which belongs to the solution set of Problem2.1.The discussion in Section 3 leads to the following.Corollary 4.1Let T(i)(i∈I) be a mapping defined by(9), let f(i)(i∈I) satisfy (A2), and let(xn(i))n∈N(i∈I) be the sequence generated by(7)when x(i)is replaced byxn(i−1),where(αn)n∈Nand(γn)n∈Nsatisfy Assumption4.1. Then, there exists a weak sequential cluster point of(xn(i))n∈N(i∈I) which belongs to the solution set of Problem2.1withX=⋂i∈ICg(i).The proof starts with the following lemma.Lemma 4.1The sequence(xn)n∈Ngenerated by Algorithm4.1satisfies that, for all x ∈ X and for alln∈N,∥xn+1−x∥2≤∥xn−x∥2−(1−αn)×∑i∈I{∥xn(i−1)−yn(i)∥2+∥yn(i)−T(i)(yn(i))∥2}+2(1−αn)γn∑i∈I[f(i)(x)−f(i)(yn(i))].The definition ofxn(i)(i∈I,n∈N)and the convexity of ‖ · ‖2 guarantee that, for all x ∈ X, for alln∈N,and for alli∈I,∥xn(i)−x∥2≤αn∥xn(i−1)−x∥2+(1−αn)∥T(i)(yn(i))−x∥2,which, together with (A1), implies that∥xn(i)−x∥2≤αn∥xn(i−1)−x∥2+(1−αn)∥yn(i)−x∥2−(1−αn)∥yn(i)−T(i)(yn(i))∥2.Moreover, (13) means that, for all x ∈ X, for alln∈N,and for alli∈I,∥xn(i)−x∥2≤αn∥xn(i−1)−x∥2+(1−αn){∥xn(i−1)−x∥2−∥xn(i−1)−yn(i)∥2+2γn(f(i)(x)−f(i)(yn(i)))}−(1−αn)∥yn(i)−T(i)(yn(i))∥2=∥xn(i−1)−x∥2−(1−αn)∥xn(i−1)−yn(i)∥2+2(1−αn)γn(f(i)(x)−f(i)(yn(i)))−(1−αn)∥yn(i)−T(i)(yn(i))∥2.Summing this inequality over all i completes the proof of Lemma 4.1.□The following lemma indicates that Theorem 4.1 holds when(xn)n∈Nin Algorithm 4.1 is Fejér monotone with respect to X⋆ (Bauschke and Combettes, 2011, Chapter 5).Lemma 4.2Suppose that Assumptions (A1), (A2),4.1, and4.2hold and there existsn0∈Nsuch that∥xn+1−x☆∥≤∥xn−x☆∥for all x⋆ ∈ X⋆and for all n ≥ n0. Then, the following hold:(i)limn→∞∥xn(i−1)−yn(i)∥=0andlimn→∞∥yn(i)−T(i)(yn(i))∥=0(i∈I);limn→∞∥xn−yn(i)∥=0andlimn→∞∥xn−T(i)(xn)∥=0(i∈I);lim infn→∞f(xn)≤f(x)(x ∈ X);There exists(xnl(i))l∈N⊂(xn(i))n∈N(i∈I) which weakly converges to x* ∈ X⋆.(i)The definition of ∂f(i) ensures that, for all x ∈ X, for alln∈N,and for alli∈I,f(i)(x)−f(i)(yn(i))≤〈x−yn(i),z(i)〉≤N1,where z(i) ∈ ∂f(i)(x)(i∈I),N1:=maxi∈I(sup|{〈yn(i)−x,z(i)〉|:n∈N}),and N1 < ∞ is satisfied from Assumption 4.2. Accordingly, Lemma 4.1 guarantees that, for all x⋆ ∈ X⋆ and for alln∈N,(18)(1−αn)∑i∈I∥xn(i−1)−yn(i)∥2≤∥xn−x☆∥2−∥xn+1−x☆∥2+2IN1(1−αn)γn,(1−αn)∑i∈I∥yn(i)−T(i)(yn(i))∥2≤∥xn−x☆∥2−∥xn+1−x☆∥2+2IN1(1−αn)γn,which, together with (C6), (C7), and the existence oflimn→∞∥xn−x☆∥(by∥xn+1−x☆∥≤∥xn−x☆∥(x⋆ ∈ X⋆, n ≥ n0)), means thatlimn→∞∥xn(i−1)−yn(i)∥=0andlimn→∞∥yn(i)−T(i)(yn(i))∥=0(i∈I).From∥xn(i)−xn(i−1)∥≤∥T(i)(yn(i))−xn(i−1)∥≤∥T(i)(yn(i))−yn(i)∥+∥yn(i)−xn(i−1)∥,Lemma 4.2(i) leads tolimn→∞∥xn(i)−xn(i−1)∥=0(i∈I). Since∥T(i)(yn(i))−xn(i)∥≤∥T(i)(yn(i))−yn(i)∥+∥yn(i)−xn(i−1)∥+∥xn(i−1)−xn(i)∥,Lemma 4.2(i) implies thatlimn→∞∥T(i)(yn(i))−xn(i)∥=0(i∈I). Thus, a discussion similar to the one for obtaining (16) and (17) leads tolimn→∞∥xn−xn(i−1)∥=0,limn→∞∥xn−yn(i)∥=0,andlimn→∞∥xn−T(i)(xn)∥=0(i∈I).Fromf:=∑i∈If(i),the definition of ∂f(i), (A2), and Proposition 2.1(iii), there exists N2 < ∞ such that, for all x ∈ X and for alln∈N,∑i∈I[f(i)(x)−f(i)(yn(i))]=f(x)−f(xn)+∑i∈I[f(i)(xn)−f(i)(yn(i))]≤f(x)−f(xn)+N2∑i∈I∥xn−yn(i)∥.Accordingly, Lemma 4.1 implies that, for all x ∈ X and for alln∈N,(19)2(1−αn)γn(f(xn)−f(x)−N2∑i∈I∥xn−yn(i)∥)≤∥xn−x∥2−∥xn+1−x∥2.Summing up (19) fromn=0to infinity leads to∑n=0∞γn(1−αn)(f(xn)−f(x)−N2∑i∈I∥xn−yn(i)∥)≤∥x0−x∥<∞.It is next shown thatlim infn→∞(1−αn)(f(xn)−f(x)−N2∑i∈I∥xn−yn(i)∥)≤0(x ∈ X). If this assertion does not hold, there existx∈X,m0∈N, and γ > 0 such that(1−αn)(f(xn)−f(x)−N2∑i∈I∥xn−yn(i)∥)≥γfor all n ≥ m0. Accordingly, (C8) ensures that∞=γ∑n=m0∞γn≤∑n=m0∞γn(1−αn)×(f(xn)−f(x)−N2∑i∈I∥xn−yn(i)∥)<∞,which is a contradiction. Hence, (C6) and Lemma 4.2(ii) imply that there exists α ∈ (0, 1) such that, for all x ∈ X,(1−α)lim infn→∞(f(xn)−f(x))≤lim infn→∞(1−αn)(f(xn)−f(x))≤N2lim supn→∞(1−αn)∑i∈I∥xn−yn(i)∥=0.Therefore,lim infn→∞f(xn)≤f(x)(x ∈ X).Lemma 4.2(iii) ensures the existence of a subsequence(xnl)l∈Nof(xn)n∈Nsuch that, for all x ∈ X,liml→∞f(xnl)=lim infn→∞f(xn)≤f(x).The boundedness of(xnl)l∈Nguarantees that there exists(xnlm)m∈N⊂(xnl)l∈Nthat weakly converges to x*. The same discussion as in the proof of Lemma 3.3(ii) leads to x* ∈ X. Since Proposition 2.4 implies thatf(x*)≤lim infm→∞f(xnlm),f(x*)≤lim infm→∞f(xnlm)=liml→∞f(xnl)≤f(x)(x∈X),i.e.,x*∈X☆.Consider another subsequence(xnlk)k∈N⊂(xnl)l∈Nthat weakly converges to x*. From the above discussion, x* ∈ X⋆. Here, assume that x* ≠ x*. Then, the existence oflimn→∞∥xn−x☆∥(x⋆ ∈ X⋆) and Proposition 2.3 lead to a contradiction:limn→∞∥xn−x*∥=limm→∞∥xnlm−x*∥<limm→∞∥xnlm−x*∥=limn→∞∥xn−x*∥=limk→∞∥xnlk−x*∥<limk→∞∥xnlk−x*∥=limn→∞∥xn−x*∥.Therefore, any subsequence of(xnl)l∈Nconverges weakly to x* ∈ X⋆. This means that(xnl)l∈Nweakly converges to x* ∈ X⋆. Fromlimn→∞∥xn−xn(i−1)∥=0(i∈I),(xnl(i))n∈N(i∈I)weakly converges to x* ∈ X⋆.This completes the proof.□Next it is proven that Theorem 4.1 holds when(xn)n∈Nin Algorithm 4.1 is not Fejér monotone with respect to X⋆.Lemma 4.3Suppose that Assumptions (A1), (A2),4.1, and4.2hold and there existx0☆∈X☆and(xnj)j∈N⊂(xn)n∈Nsuch that∥xnj−x0☆∥<∥xnj+1−x0☆∥for allj∈N. Then, the following hold:(i)limj→∞∥xnj(i−1)−ynj(i)∥=0andlimj→∞∥ynj(i)−T(i)(ynj(i))∥=0(i∈I);limj→∞∥xnj−ynj(i)∥=0andlimj→∞∥xnj−T(i)(xnj)∥=0(i∈I);lim supj→∞f(xnj)≤f(x0☆);There exists a weak sequential cluster point of(xn)n∈Nwhich is in X⋆.(i)A discussion similar to the one for obtaining (18) and∥xnj−x0☆∥<∥xnj+1−x0☆∥(j∈N)ensure that, for allj∈N,(1−αnj)∑i∈I∥xnj(i−1)−ynj(i)∥2<2IN1(1−αnj)γnj,(1−αnj)∑i∈I∥ynj(i)−T(i)(ynj(i))∥2<2IN1(1−αnj)γnj,which, together with (C6) and (C7), implies thatlimj→∞∥xnj(i−1)−ynj(i)∥=0andlimj→∞∥ynj(i)−T(i)(ynj(i))∥=0(i∈I).The same reasoning as in the proofs of Lemmas 4.2(ii) and 4.3(i) lead tolimj→∞∥xnj−xnj(i−1)∥=0,limj→∞∥xnj−ynj(i)∥=0,andlimj→∞∥xnj−T(i)(xnj)∥=0(i∈I). Assumption 4.2 andlimj→∞∥xnj−ynj(i)∥=0imply the boundedness of(xnj)j∈N.A discussion similar to the one for obtaining (19) means that, for allj∈N,f(xnj)−f(x0☆)<N2∑i∈I∥xnj−ynj(i)∥,which, together with Lemma 4.3(ii), means thatlim supj→∞(f(xnj)−f(x0☆))≤N2∑i∈Ilimj→∞∥xnj−ynj(i)∥=0.Thus,lim supj→∞f(xnj)≤f(x0☆).The boundedness of(xnj)j∈Nimplies that there exists(xnjk)k∈N⊂(xnj)j∈Nsuch that(xnjk)k∈Nweakly converges to x⋆. The same discussion as in the proof of Lemma 3.3(ii) leads to x⋆ ∈ X. Moreover, (A2) and Proposition 2.4 imply thatf(x☆)≤lim infk→∞f(xnjk). Accordingly, Lemma 4.3(iii) guarantees thatf(x☆)≤lim infk→∞f(xnjk)≤lim supk→∞f(xnjk)≤lim supj→∞f(xnj)≤f(x0☆).That is, x⋆ ∈ X⋆. Fromlimj→∞∥xnj−xnj(i−1)∥=0(i∈I),(xnjk(i))k∈N(i∈I)weakly converges to x⋆ ∈ X⋆.This completes the proof.□Consider the following problem with nonsmooth, convex objective functions (Combettes and Pesquet, 2007, Example 28) (see also Corollaries 3.1 and 4.1).Problem 5.1Assume that user i (i∈I:={1,2,…,I}) has its own private parametersωj(i)>0,aj(i)∈R,dk(i)∈R,andck(i)∈RNwithck(i)≠0,wherej∈N:={1,2,…,N}andk∈K:={1,2,…,K}. Definef(i):RN→RandCk(i)⊂RN(i∈I,k∈K) usingf(i)(x):=∑j∈Nωj(i)|xj−aj(i)|(x∈RN)andCk(i):={x∈RN:〈ck(i),x〉≤dk(i)}.Then,minimize∑i∈If(i)(x)subjecttox∈⋂i∈ICg(i),whereCg(i)(i∈I) is the generalized convex feasible set defined by (8) and (10) whenwk(i):=1/KandX(i)=C:={x∈RN:∥x∥≤1}(i∈I,k∈K).Here,T(i):RN→RN(i∈I)is defined by (9) withX(i)=Candwk(i):=1/K(k∈K). Accordingly, T(i)(i∈I)is firmly nonexpansive withFix(T(i))=Cg(i)(see Section 3). Hence, it is evident that Problem 5.1 is an example of Problem 2.1.The experimental evaluations of the two proposed algorithms were done using a 27-inch iMac with a 3.2 GigaHertz Intel Core i5 processor and 24GigaByte 1600 MegaHertz DDR3 memory. The algorithms were written in Java 1.8.0_60-b27 with N ≔ 100, I ≔ 10, and K ≔ 3.The values ofωj(i)∈(0,1],aj(i)∈[−3,3],dk(i)∈[0,1],ck(i)with∥ck(i)∥=1,and x(i) were randomly generated using org.apache.commons.math3.random.MersenneTwister. Algorithm 3.1 was used with (7) when X(i) ≔ C, and(αn)n∈Nand(γn)n∈Nwere defined by55Numerical results in Iiduka (2015, 2013) indicate that the existing fixed point algorithms with small step sizes (e.g.,γn:=10−2/(n+1)a,10−3/(n+1)a) have faster convergence. Hence, the experiment described in this section used the step sizes in (20).(20)γn:=10−3(n+1)a(a=14,18)andαn:=10−3(n+1)b(b=12,34)while Algorithm 4.1 was used with (7) when x(i) was replaced byxn(i−1),X(i) ≔ C,αn:=t=1/2,and(γn)n∈Nwas as given in (20).The incremental subgradient method (ISM) (Iiduka, 2016) and parallel subgradient method (PSM) (Iiduka, 2015) were used for comparison. ISM can be obtained by replacingProxγnf(i)(xn(i−1))in Algorithm 4.1 withxn(i−1)−γngn(i),wheregn(i)∈∂f(i)(xn(i−1)). The sequence generated by PSM is defined byxn+1:=(1/I)∑i∈Ixn(i),wherexn(i):=txn+(1−t)T(i)(xn−γngn(i))andgn(i)∈∂f(i)(xn). It is evident that Algorithms 3.1 and 4.1 use the proximity operators of f(i)s while ISM and PSM use the subgradients of f(i)s. To see how the choice of the order of the indices inI:={1,2,…,I}affects the convergence rate of Algorithms 3.1 and 4.1, we compared Algorithms 3.1 and 4.1 when (Case 1)xn(i)(i∈I)is calculated in the order of1,2,…,Iand when (Case 2)xn(i)(i∈I)is calculated in randomly shuffled order. We found that the performances of Algorithms 3.1 and 4.1 in Case 1 were almost the same as those in Case 2. Only the results for Case 1 are given due to lack of space.One hundred samplings, each starting from a different randomly chosen initial point, were performed, and the results were averaged. Two performance measures were used. For eachn∈N,Fn:=1100∑s=1100∑i∈If(i)(xn(s))andDn:=1100∑s=1100∑i∈I∥xn(s)−T(i)(xn(s))∥,where(xn(s))n∈Nis the sequence generated from initial point x(s)(s=1,2,…,100)for each of the four algorithms. The value of Dnrepresents the mean value of the sums of the distances between xn(s) and T(i)(xn(s)). Hence, if(Dn)n∈Nconverges to 0,(xn)n∈Nconverges to some point in⋂i∈IFix(T(i))=⋂i∈ICg(i).Let us first consider Problem 5.1 when the intersection of C and⋂i∈I⋂k∈KCk(i)is nonempty.Table 1shows the number of iterations n and elapsed time when the algorithms (Algorithms 3.1 and 4.1, ISM, and PSM) satisfied|Fn−1−Fn|<10−3and|Dn−1−Dn|<10−6. As shown, the(Fn)n∈Ngenerated by the incremental algorithms (Algorithm 3.1(ii), Algorithm 4.1(ii), and ISM(ii)) using (20) witha=1/8andb=3/4converged faster than those (Algorithm 3.1(i), Algorithm 4.1(i), and ISM(i)) usinga=1/4andb=1/2. Slowly diminishing step sizes such asγn=10−3/(n+1)1/8apparently affect the fast convergence of the algorithms. The number of iterations when PSM satisfied|Fn−1−Fn|<10−3was more than 2000, and PSM converged slowly compared with the incremental algorithms. The(Dn)n∈Ngenerated by all of the algorithms converged to 0; i.e., the algorithms converged to a point in the constrained set in Problem 5.1. Algorithm 4.1(i) and ISM(i) performed better than Algorithm 3.1(i) and PSM(i), and Algorithm 3.1(ii), Algorithm 4.1(ii), and ISM(ii) had almost the same performance and converged faster than PSM(ii).Next, let us consider Problem 5.1 when the intersection of C and⋂i∈I⋂k∈KCk(i)is empty. Here, we assume that all users have the same T(i) to satisfy⋂i∈ICg(i)=⋂i∈IFix(T(i))≠∅. Accordingly, we consider the problem of minimizing∑i∈If(i)overCg(i)≠∅,whereC∩⋂k∈KCk(i)=∅.Table 2shows the results for Algorithms 3.1 and 4.1, ISM, and PSM. Although Algorithm 3.1(ii), Algorithm 4.1(ii), and ISM(ii) needed more iterations to satisfy|Fn−1−Fn|<10−3than Algorithm 3.1(i), Algorithm 4.1(i), and ISM(i), Algorithm 3.1(ii), Algorithm 4.1(ii), and ISM(ii) better optimized∑i∈If(i)than Algorithm 3.1(i), Algorithm 4.1(i), and ISM(i). PSM converged slowly compared with the incremental algorithms, as also seen in Table 1. All the algorithms converged to a point inCg(i)in the early stages and, in particular, Algorithm 3.1(ii) (F1419 ≈ 737), which is based on the Halpern fixed point algorithm, performed better than the algorithms based on the Krasnosel’skiĭ–Mann fixed point algorithm. This is because the Halpern fixed point algorithm can minimize a certain convex function over the fixed point set of a nonexpansive mapping while the Krasnosel’skiĭ–Mann fixed point algorithm can only find a fixed point. Since Problem 5.1 is to minimize a convex function over the fixed point set of a nonexpansive mapping, Algorithm 3.1(ii) based on the Halpern algorithm is better suited for Problem 5.1 than the algorithms based on the Krasnosel’skiĭ–Mann algorithm.The problem of minimizing the sum of all users’ nonsmooth, convex objective functions over the intersection of all users’ fixed point sets in a Hilbert space was discussed, and two incremental proximal point algorithms were presented for solving the problem. One combines an incremental subgradient method with the Halpern fixed point algorithm, and the other is based on the Krasnosel’skiĭ–Mann fixed point algorithm. Convergence analysis showed that, under certain assumptions, any weak sequential cluster point of the sequence generated by the Halpern-type algorithm is guaranteed to belong to the solution set of the problem and that there exists a weak sequential cluster point of the sequence generated by the Krasnosel’skiĭ–Mann-type algorithm, which also belongs to the solution set. Numerical evaluations using concrete, nonsmooth, convex optimization problems showed the efficiency of the two algorithms.Although nonsmooth, convex optimization with fixed point constraints in a Hilbert space was discussed, the numerically tested problems were defined in a finite-dimensional space. Future work includes generating numerical results that have special features of an infinite-dimensional space.Since the bundle method (Hiriart-Urruty and Lemaréchal, 1996, Chapter XIV) is one of the most efficient methods for solving the problem of minimizing a general nonsmooth function, it would be of great interest to investigate whether bundle-type algorithms are well suited for nonsmooth (nonconvex) optimization with fixed point constraints. The first step would be to devise bundle-type algorithms for nonsmooth convex optimization over fixed point sets on the basis of previously reported results for the bundle method.

@&#CONCLUSIONS@&#
