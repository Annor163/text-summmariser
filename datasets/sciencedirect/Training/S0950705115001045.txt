@&#MAIN-TITLE@&#
Optimization of gear blank preforms based on a new R-GPLVM model utilizing GA-ELM

@&#HIGHLIGHTS@&#
A novel R-GPLVM is proposed to screen out critical dimensions of the preform.A newly GA-ELM framework seamlessly integrated with R-GPLVM is proposed.Discussions demonstrate that Gaussian kernel function has the higher accuracy.The relevant parameters of ELM are optimized with the improved performance.Engineering applications and FEM validate the feasibility of the proposed method.

@&#KEYPHRASES@&#
Preform optimization,R-GPLVM,ELM,Gear blank,GA,

@&#ABSTRACT@&#
The determination of the key dimensions of gear blank preforms with complicated geometries is a highly nonlinear optimization task. To determine critical design dimensions, we propose a novel and efficient dimensionality reduction (DR) model that adapts Gaussian process regression (GPR) to construct a topological constraint between the design latent variables (LVs) and the regression space. This procedure is termed the regression-constrained Gaussian process latent variables model (R-GPLVM), which overcomes GPLVM’s drawback of ignoring the regression constrains. To determine the appropriate sub-manifolds of the high-dimensional sample space, we combine the maximum a posteriori method with the scaled conjugate gradient (SCG) algorithm. This procedure can estimate the coordinates of preform samples in the space of LVs. Numerical experiments reveal that the R-GPLVM outperforms the pure GPR in various dimensional spaces, when the proper hyper-parameters and kernel functions are solved for. Results using an extreme learning model (ELM) obtain a better prediction precision than the back propagation method (BP), when the dimensions are reduced to seven and a Gaussian kernel function is adopted. After the seven key variables are screened out, the ELM model will be constructed with realistic inputs and obtains improved prediction accuracy. However, since the ELM has a problem with validity of the prediction, a genetic algorithm (GA) is exploited to optimize the connection parameters between each network layer to improve the reliability and generalization. In terms of prediction accuracy for testing datasets, GA has a better performance compared to the differential evolution (DE) approach, which motivates the choice to use the genetic algorithm-extreme learning model (GA-ELM). Moreover, GA-ELM is employed to measure the aforementioned DR using engineering criteria. In the end, to obtain the optimal geometry, a parallel selection method of multi-objective optimization is proposed to obtain the Pareto-optimal solution, while the maximum finisher forming force (MFFF) and the maximum finisher die stress (MFDS) are both minimized. Comparative analysis with other numerical models including finite element model (FEM) simulation is conducted using the GA optimized preform. Results show that the values of MFFF and MFDS predicted by GA-ELM and R-GPLVM agree well with the experimental results, which validates the feasibility of our proposed methods.

@&#INTRODUCTION@&#
Flashless forging of gears has been widely used because of its high production efficiency, good product performance and low material waste compared to machining [1,2]. Presently, methods that rapidly forge the gear are first blanked, and then the teeth are machined to obtain the final part. This combines the rapidity of flashless die forging with the high accuracy of machining [3–6]. Due to the complexity of the gear structure, gears are formed mostly by multi-station forging processes to effectively reduce the forming force and die stress, aiming to increase the deformation uniformity and improve the die life. In the past ten years, finite element model (FEM) methods, together with sensitivity analysis and reverse trace simulation have been employed to optimize the design of preforms [7–13]. However, due to the complexities of the calculation, the time-consuming nature of these methods, and the limitations to the accuracy of the boundary conditions assumed, these technologies are rarely applied to complex preforms. Meanwhile, FEM has certain limitations because its assumptions for the calculation are based on a simplified model [14].The goal of this proposal is to predict the finished forming force for an optimized preform. However, the prediction models for the preforms are nonlinear, and the traditional empirical approaches and the FEM method cannot always address the difficulties involved in complex preforms. A machine-learning model (MLM) can deal with these nonlinear problems that pose difficulties in mathematical modeling. For example, BP neural networks (BPNN) are well suited to address these problems, and they have been used in the prediction of formation loads of metal [15–18]. There also have been cases in which BPNN has been employed to optimize preforms [19,20]. Nevertheless, BPNN learning algorithms have their own shortcomings that have become the main impediments to wider adoption, namely slow training speed and inadequate generalization performance [21]. Extreme learning machine (ELM) algorithms based on single hidden layer feed-forward neural networks (SLFN) overcome these disadvantages, and thus are used in a wide range of applications in engineering [22–26]. However, there are two problems in the training of the ELM network model. Firstly, compared to other gear blanks, there are more design variables in the preform and relatively less training data can be obtained from the experiment. This makes it necessary to conduct dimensionality reduction (DR) on the input variables to remove the redundant information of the input data so that the regression analysis subsequently can obtain a more accurate prediction model. Secondly, there is inevitably a mix of some unwanted and unoptimized data within the selected data. This arises from the inherent randomness in the weights between the input and hidden layers, as well as the bias of the hidden layer neurons. This randomness may sometimes lead to degeneracies among the columns of the output matrix of the hidden layer [27], rendering the model used to train output weights unsolvable. This limits the predictability of ELM. Meanwhile, Huang [28], who proposed ELM originally, indicates that since evolutionary algorithms (EAs) are widely used as a global searching method for optimization, the hybrids of EA and analytical methods should be promising for network training. Huang proposed a modified differential evolution (DE) to globally search for the optimal input weights and hidden biases, while the Moore–Penrose generalized inverse is used to analytically calculate the output weights. Although many versions of DE have been determined to be effective approaches to optimize parameters, widespread adoption is limited due to susceptibility to premature and over-fitting, which limit its effectiveness of one-to-one parameters adjustment in practice. Genetic algorithm (GA) overcomes these disadvantages, and also has the advantage of amenability to parallel adjustment [29,30]. Thus, it is worthwhile to explore the usage of GA as another practical global searching method for the optimization for the weights and biases.The key to solving DR problems is to find the low-dimensional (sub-manifold) that is embedded in the higher-dimensional space. Linear mapping is adopted in many DR methods such as singular value decomposition (SVD) [31], factor analysis (FA) and principal component analysis (PCA) [32]. Since many realistic sample data are nonlinearly distributed, the linearity hypothesis usually results in poor model performance. As a result, nonlinear DR methods have become popular recently, including Gaussian process latent variables models (GPLVM) [33], local linear embedding (LLE) [34], kernel principle component analysis (KPCA) [35] and ISO metric mapping (ISOMAP). Among them, GPLVM is a very prominent nonlinear mapping model. It overcomes the limitation of the linear DR method but is still able to find the low dimensional manifold hidden in the observational data, even when the observation sample data is relatively small. GPLVM is very suited to handle the high-dimensional data of small samples encountered in the gear blank preform optimization problems considered in this paper. However, the first proposed GPLVM only takes the observation sample space itself into consideration, regardless of the regression constraint corresponding to that space. This limits the utility in many practical situations. Therefore, in this paper, a regression-constrained Gaussian process latent variables model (R-GPLVM) is proposed. Firstly, the relationship between the latent variables (LVs) and the observational data is established using GPLVM, and then GPR is exploited as the topology of constraint to accomplish the DR of the higher-dimensional observational data. Finally, a follow-up genetic algorithm-extreme learning model (GA-ELM) is employed to measure the aforementioned DR using the engineering criteria.As far as the generalization and stability, Bartlett’s theory about the generalization performance of feed-forward learning networks has pointed out that smaller connection weight norms leads to smaller output errors of the network training, resulting in better generalization performance [36]. Many researchers use GA, DE and other evolutional algorithms to promote generalization performance, but there are rarely reports on the optimization of ELM, especially using the approach of GA, except for Dr. Huang and Refs. [37,38]. In Refs. [37,38], the authors proposed an artificial bee colony algorithm (ABC) and particle swarm optimization (PSO) to tune weights and biases of ELM to improve the generalization performance. However, most of the datasets used in their experiments are based upon huge benchmark data, which may generate unbearable time cost if GA optimization is adopted. As we have indicated that GPLVM is very suitable for handling the high-dimensional data of small samples, the time cost of using GA can be alleviated in our experiments. Owing to the unique advantage of global optimization of GA [39–41], we propose a tuned GA to improve the effectiveness, generalization and stability of ELM prediction models of the preform, even though the approach has a relatively higher time cost. Moreover, unlike other approaches of single fitness functions, we use two fitness functions to optimize the connection parameters between each ELM network layer, namely the output weights of hidden layer neurons and the root mean square error (RMSE). In cases where the RMSE of the whole training set is used as a fitness function, network over-fitting will be easily encountered. To mitigate this, training and validation datasets are used without repetition. Moreover, since the minimum norm of connection weights of the training dataset is obtained by solving the least square solution, the individual training errors of each training dataset are similar. Thus, in this paper, the RMSE of the testing dataset, rather than that of the whole training dataset, is chosen as a fitness function to improve the efficiency of the model.Until now, there are few reports on the GA-ELM method for regression issues. This is mainly because of the insufferable time cost for the huge benchmark data with high-dimensionality. In literature [42], the authors presented a revised genetic algorithm based on ELM. They indicated that by utilizing the exceptional function estimation and continuously retraining the ELM based evolution operators, the global search and convergence capabilities of GA could be improved. Different from literature [42], to obtain a reliable convergent solution, we adjusted the selective strategy of genetic operators, i.e., besides the root mean square error (RMSE), the norm of the output weights of the hidden layer was introduced as the convergent criteria. When combining the ELM with the adjusted GA, the calculation results demonstrated a better convergent performance. It should be emphasized that since a compact network leads to faster response of the trained networks, we proposed a new R-GPLVM method to compact the network with reduced dimensions. This method can accelerate the response of GA since the amount of data has become smaller after dimensions reduction. In literature [43], the researchers explored a novel feature selection method for classification issues based on a special GA using restricted search in conjunction with ELM. Unlike the feature selection algorithm in literature [43], we adopted GA method to tune weights and biases of ELM to improve the generalization performance for regression problems. Also in literature [44], the authors proposed a new model termed the genetic algorithm-extreme learning machine-particle swarm optimization (GA-ELM-PSO) classifier for magnetic resonance imaging (MRI) images. They use ELM for classification, with its performance optimized by the PSO and GA method for feature selection. They also use GA to reduce the high dimensional features needed for classification. Differed from these, we use a new R-GPLVM method to conduct dimensionality reduction (feature selection) for regression issues, and we have analyzed that the R-GPLVM is very suitable for small samples with higher-dimensional data. Lastly, in literature [45], the authors carry out a statistical study of the micro-genetic algorithm (μ-GA) ELM algorithm. They analyzed the performance of theμ-GA ELM only in small 1-dimensional problems with small population and very few generations. By contrast, our method can cope with not only 1-dimensional problems, but also high-dimensions issues with large population and generations. In summary, the major differences between our method and other GA-ELM methods are as followed: (1) Most of the previous GA methods including traditional evolutionary ELM are based on classification issues, few reports are associated with regression problems. Meanwhile, our proposed GA-ELM method can be used to cope with regression issues. (2) The previous GA-ELM procedures need more and more convergence iterations, runtime and convergence time, with the growth of the dimension. By comparison, in our scheme, a newly dimensionality reduction method R-GPLVM is proposed to decrease the amount of data, to accelerate the response of GA optimization.The optimization of the preform design model is the ultimate goal of this work, minimizing the maximum finisher forming force (MFFF) and maximum finisher die stress (MFDS) when the finisher cavity is just filled. These GA multi-objective optimization methods already have some engineering applications [46–49]. In this paper, a parallel selection method is exploited to solve the multi-objective optimization based on the ELM network model. That is, two prediction models of MFFF and MFDS calculated from GA-ELM network (DR optimized) serve as sub-goal functions. All the individuals of the genetic population are equally divided among the two sub-goal functions. Each subgroup has a sub-goal function with the selection operation conducted independently. Some individuals with high fitness are selected in each sub-goal function, and then these subgroups are merged into a larger group. The division, parallel selection and merging functions are conducted repeatedly. Finally, the Pareto-optimal solutions of the two goal optimization functions are solved.The rest of this paper is organized as follows. In Section 2, the modeling of R-GPLVM and the optimization of the hyper-parameters are mentioned, with the latent variables analyzed using a probability analysis and the scaled conjugate gradient (SCG) method [50]. In Section 3, a new GA-ELM framework seamlessly integrated with R-GPLVM is proposed, and the corresponding theories and building of the ELM model of the preform are introduced, with connection parameters between each ELM network layer optimized by GA. In Section 4, the prediction accuracy of GA-ELM, differential evolution-extreme learning machine (DE-ELM), ELM and BP, and the results before and after DR of R-GPLVM are compared. The SCG optimizations of the hyper-parameters are illustrated in comparison with the CG method, and the prediction accuracies using different kernel functions in various dimensional spaces are discussed. The multi-objective parallel selection method via GA is employed to obtain a Pareto-optimal solution. The optimized preform is applied into the actual production with comparison to the FEM simulation. In Section 5, conclusions are presented.The nonlinear regression model with Gaussian processes can be defined as follows:(1)Gi=fxi+σ2IHere,σ2Iis the noise term, the functionfxiis a Gaussian process defined as:(2)fxi∼GP0,kxi,x′As can be seen from the above formula, the relation betweenX=x1,x2,…,xNTandG=g1,g2,…,gNTcan be defined with the Gaussian process:(3)G∼N0,K+σ2IAccording to Eq. (3), we can obtain the log-likelihood function of GPR:(4)lnpG|X=-12GT(K+σ2I)-1G-12GTK+σ2I-12ln2πHere,Kis the covariance function matrix, namely, the kernel function. Since the model parameters include the kernel function and noise terms, the training of the model is mainly determined by the kernel function.The latent variables model (LVM) is established based on the following assumptions. (1) The errors and individual differences can be filtered out as much as possible in the observation samples and LVs, i.e., the samples are independent and identically distributed (IID). (2) There exits an internal relation between the high-dimensional observation data and low-dimensional LVs. As LVM is established based on a probabilistic framework, it has the ability to estimate the probability distribution from small samples, and it is more conducive to estimate the high-dimensional data of small samples. A commonly used method is the kernel function, and different kernel functions will have different LVMs. It can be assumed that the mappingfdin each dimension is IID with the Gaussian process as follows:(5)p(f)=∏d=1Dpfd=∏d=1DNfd|0,kThe likelihood of the observation data with d-dimensions can be expressed as:(6)py:,d|X,ξ1=∫py:,d|xn,fd,ξ1pfddfd=Ny:,d|0,kHerey:,ddenotes thedth dimension for all samples. As the dimensions are independent from one another, the likelihood of the observed data can be represented with the product of the likelihood of each spatial dimension:(7)pY|X,ξ1=∏d=1Dy:,d|X,ξ1=∏d=1D12πN/2|K|1/2exp-12y:,dTK-1y:,dEq. (7) can be expressed as:(8)pY|X,ξ1=∏d=1D12πN/2|K|1/2exp-12trK-1YYTHere,Kis the kernel function matrix. In this paper, the Gaussian radial basis kernel function is exploited and can be formalized as follows:(9)kxi,xj=1π3/2exp-12xi-xjTxi-xjHere,kxi,xjis theijth element in the kernel function matrix, which will be discussed in Section 4.4.Assuming thatY=y1,y2,…,yNTrepresents the observation sample dataset, each sampleyiis a D-dimension feature vectoryi∈RD. DefiningG=g1,g2,…,gNTas the regression matrix corresponding to the samplesY, its dimension can be determined from the dimension of the output matrix with a 1-dimensional (or M-dimensional) regression value. In our test cases, the matrixGis a 2-dimensional regression value, and LVs can be expressed byX=x1,x2,…,xNT,xi∈RD. In order to effectively utilize the regression of observation samples, the relationship between the observation data spaceYand the regression value spaceGis built through the LVs space, as shown in Fig. 1. Firstly, the LVs can be initialized using the singular value decomposition (SVD) method, and subsequently the relationship betweenXandYis established via GPLVM. Meanwhile, the relationship betweenXandGis constructed using a GPR model. Thus, we can constrain the previous DR process using the regression values. In addition, the regression values of the testing datasets can be obtained with the model. Finally, the previous DR process and the regression results are evaluated by using GA-ELM model introduced afterward. R-GPLVM can be expressed by Eq. (10):(10)Y=fYX,ξ1+λYG=fGX,ξ2+λGHere, the functionfYrepresents the mapping relation from the low-dimensional latent spaceXto the high-dimensional spaceYandξ1represents the hyper-parameter of DR mapping. The functionfGrepresents the regression mapping fromXtoGand the hyper-parameter of mapping can be expressed byξ2. The parametersλYandλGrespectively represent the noise with a Gaussian distribution assumed. In the LV model, the observation data (Y,G)are independent from each dimension, namely,(11)py,g|x=py|xpg|xThe joint likelihood of the observed data can thus be given as:(12)pY,G|X=∏n=1Npyn,gn|xn=∏n=1Npyn|xnpgn|xn=pY|XpG|XBayes’ theorem relates the probabilities as:(13)pX|Y,G=pY,G|Xp(X)pY,GBy combining Eq. (12) with Eq. (13), Eq. (13) can be rewritten as:(14)pX|Y,G=pY|XpG|Xp(X)pY,GWe take the logarithm on both sides of Eq. (14).(15)lnpX|Y,G=lnpY|X+lnpG|X+lnp(X)-lnpY,GThe first term in the right-hand side of Eq. (15) is the log-likelihood of GPLVM.(16)Fξ1=lnpY|X,ξ1=-DN2ln2π-D2lnKξ1-12tr(Kξ1-1YYT)The second term in the right-hand side of Eq. (15) is the log-likelihood of GPR.(17)Fξ2=lnpG|X,ξ2=-12GT(Kξ2+σ2I)-1G-12GTKξ2+σ2I-12ln2πHere,Kξ1=Kξ1x,x′is the GPLVM kernel function, andKξ2=Kξ2x,x′is the GPR kernel function. As for Eq. (15), sincelnpY,Gdoes not directly influenceX, maximizing the logarithm of the posterior ofpY|Xis equivalent to maximizing the joint log-likelihood of GPLVM, GPR, and the prior probabilityp(X). The objection function of R-GPLVM can then be expressed with the following fonctionelle:(18)X,|ξ1,ξ2=argmaxX,ξ1,ξ2lnpY|X+lnpG|X+lnp(X)The maximization ofXin Eq. (18) is equal toargminX-Fξ1-Fξ2, according to Eqs. (11)–(17), and the modeling process is as same as the process of training the hyper-parameterξ1,ξ2and determining LVsX. Since R-GPLVM is established based upon a Gaussian process, the probability distribution of the data can be obtained. In addition, the posterior probability and coordinates of LVsXin the low-dimensional space can be calculated and determined further using Bayes’ theorem. In this paper, the posterior probability of the observation dataYis maximized with the SCG method, with the optimized LVsXand the hyper-parameterξ1andξ2output.The derivative ofXin Eq. (15) is:(19)∂lnpX|Y,G∂X=∂lnpY|X∂X+∂lnpG|X∂X+∂lnp(X)∂X-∂lnpY,G∂XSince the full probabilityp(Y,G)for the derivative ofXis zero, Eq. (19) is as same as the following based on the chain rule:(20)∂lnpX|Y,G∂X=∂lnpY|X∂Kξ1·∂Kξ1∂X+∂lnpG|X∂Kξ2·∂Kξ2∂XHere,(21)∂lnpY|X∂Kξ1=Kξ1-1YYTKξ1-1-DKξ1-1,∂lnpG|X∂Kξ2=Kξ2-1GGTKξ2-1-MKξ2-1The derivatives∂kξ1∂Xand∂Kξ2∂Xwill have different values according to the selection of the kernel function, and the gradient of the hyper-parametersξ1,ξ2of the log-likelihood can be obtained as follows:(22)∂lnpX|Y,G∂ξ1=∂lnpY|X∂Kξ1·∂Kξ1∂ξ1,∂lnpX|Y,G∂ξ2=∂lnpG|X∂Kξ2·∂Kξ2∂ξ2In summary, by using an SCG iterative method, the posterior probability based on Bayes’ theorem is maximized to determine the optimized hyper-parametersξ1andξ2. In addition, the LVs X will be determined by the same process. If the initial values and accuracy are given, the training process will be terminated when the iterations of the hyper-parameters converge. Algorithms to optimize the LVs and hyper-parameters using the SCG method contain the following steps:Input: The high-dimensional space matrix isY∈RN×D, the GPR regression constraint matrix isG∈RN×M, and the number of iterations isV.Initialization: The initial latent variablesX∈RN×dare obtained via SVD, with initial hyper-parametersξ1=[1,1,1], andξ2=[1,1,1].Pseudo-code:Loop: (m=1toV)Step 1: Calculate GPLVM, GPR kernelKξ1(v-1)=K(X(v-1),ξ1(v-1)),Kξ2(v-1)=K(X(v-1),ξ2(v-1)).Step 2: Calculate log-likelihoodFξ1(v-1)=-DN2ln2π-D2lnKξ1(v-1)-12trKξ1(v-1)-1YYT.Step 3: Calculate log-likelihoodFξ2(v-1)=-12GT(Kξ2(v-1)+σ2I)-1G-12GTKξ2(v-1)+σ2I-12ln2π.Step 4: Optimizeξ1(v),ξ2(v)=argminξ1,ξ2-Fξ1(v-1)-Fξ2(v-1).UpdateKξ1(v-1)=K(X(v-1),ξ1(v)),Kξ2(v-1)=K(X(v-1),ξ2(v)).Step 5: Re-calculate log-likelihoodFξ1(v-1)=-DN2ln2π-D2lnKξ1(v-1)-12trKξ1(v-1)-1YYT.Step 6: Re-calculate log-likelihoodFξ2(v-1)=-12GT(Kξ2(v-1)+σ2I)-1G-12GTKξ2(v-1)+σ2I-12ln2π.Step 7: OptimizeX(v)=argminX-Fξ1(v-1)-Fξ2(v-1).Convergence criterion: ifError(v)=∑i=1NXi(v)-Xi(v-1)2⩽εthen R-GPLVM converges.End Loop overV.Output: the hyper-plane parametersξ1,ξ2and latent variablesX.Furthermore, the DR of the high-dimension testing datasetYtestis conducted using the LVsXand the hyper-parametersξ1andξ2obtained from the aforementioned training of the model. Subsequently the LVsXtestis determined. Finally, the regression output valuegtestcan be obtained using GPR.A diagram of the finisher is shown in Fig. 2. The flashless die forging of the gear blank is conducted by the die and equipment, as shown in Fig. 3. In Fig. 4, the multi-station forging process can be described as upsetting→preforming→finishing→punching. The equipment used is Hauteur’s APM70 high-speed hot upsetting automated production lines from Switzerland. Since the loss of metal burrs from forging is small with high dimensional accuracy and the utilization of materials is high, the forging process is stable and reliable with obvious economic advantages.In this work, the primary focus is the influence of key dimensional parameters of the preform on the formation of the finisher forging, while the parameters of the forging process are given. It is of great importance to obtain a preform model, and the parameterized preform model can be shown in Fig. 5. Since the preform is complex with many dimensions, parameters marked with red signals in Fig. 5 are selected as the most important, according to previous engineering experience and the data analysis afterward. Various design models of the preform are proposed varying the dimensional parameters until the approximate equation for the volume of the finisher achieves a tolerance less than 10%, when the shape of the finisher is known. The experimental dataset has 96 samples, of which the first 76 groups of MFFF and MFDS are obtained from FEM, with the statistics listed in Table 1. The last 20 groups are used for designing the preform die and punch. Pressure sensors are equipped to measure the MFFF, and then high temperature strain gauges are employed to measure the die strain and stress, with the results shown in Table 2. The primary optimization of the gear blank preform determines whether the MFFF and MFDS are both minimum when the die cavity is first filled. The purpose of this work is to quantify the aforementioned process to get the optimized dimension of the preform to provide the basis for the design of the finisher die, with samples observed as much as possible.The ELM model can be classically expressed as:Hβ=T′, whereT′represents the transposition of the output matrixTof the network andHdenotes the output matrix of the hidden layer. Without loss of generality, the number of neurons of the input and output layers can be set to benandmrespectively. Moreover,βjkdenotes the weight between thejth neuron of the hidden layer and thekth neuron of the output layer, whilewijdenotes the weight between theith neuron of the input layer and thejth neuron of the hidden layer. This assumes that the input matrix of the training dataset can be expressed asXn×QwithQsamples, and the output matrix can be expressed asYm×Q, with an activation function of the hidden layer neuron represented asg(x). Based on this, the output matrixTof ELM can be illustrated as follows:(23)T=t1,t2,…tQm×Q,tj=t1jt2j⋮tmjm×1=∑i=1Lβilgωixj+bi∑i=1Lβi2gωixj+bi⋮∑i=1Lβimgωixj+bi,j=1,2,…,QHere,wi=ωi1,ωi2,…ωinandxj=x1j,x2j,…xnjT,Lis the number of the neurons of the hidden layer andb=b1,b2,…,bLL×1Tis the threshold of the hidden layer neurons.For any givenwandb, if the number of the hidden layer neurons is equal to the number of the training dataset samples, the SLFN model can be approximated using the training samples with zero error, namely,(24)∑j=1Qtj-yj=0,yj=y1j,y1j,…,ynjT,j=1,2,…,QSince the weightwand thresholdbare randomly generated before training, we only need to calculate the number of the hidden layer neurons and the activation function, andβcan then be determined. However, in order to reduce the number of operations, the number of the hidden layer neuronsKis usually taken to be smaller thanQwhen the number of training samples is relatively large. Moreover, the connection weightsβbetween the hidden layer and the output layer can be achieved by a least squares solution of the following equation(25)minβHβ-T′→βˆ=H+T′Here,H+is the Moore–Penrose generalized inverse of the output matrixH.The seven screened out dimensions are used as input parameters. These are the maximum diameter (d1), the slope of the starting diameter (d2), the bore diameter (d3), the convex diameter (d4), the bore depth (h1), the convex height (h2), and the slope value (a1). Meanwhile, the MFFF (fp)and MFDS (fm)are used as output parameters. The ELM model can be represented as:(26)Ffp,fm=ψd1,d2,d3,d4,h1,h2,a1In this work, the ELM model of the preform can be constructed with a typical SLFN structure, as shown in Fig. 6. As mentioned in Section 1, since the values ofwandbare both assigned randomly, the validity of the ELM prediction will be reduced and consequently the generalization of the model will be effected. As a result, the connection weights and biases of the neurons should be optimized and adjusted effectively. The optimization and adjustment of the parameters of GA-ELM is seamlessly integrated with R-GPLVM, mainly including the following three frameworks, as shown in Fig. 7.Part_1 R-GPVLM: The topology between LVs and the observation samples can be established using GPLVM, with the DR of observation samples performed. GPR is exploited as a constraint to restrict the DR of the high-dimensional observation data. GA-ELM regressions are used to evaluate the previous DR and regression, while the workflow can be seen in the R-GPLVM module of Fig. 7.Part_2 ELM: The dimensions after DR are employed as the inputs of ELM, with thewandbrandomly assigned to [−1,1], and the output matrixHand connection weightβwill be calculated using the least square method. Based on this procedure, the output of the ELM network will be verified with the test samples. Thewandbcould be optimized using GA if the output accuracy cannot meet the requirements. Subsequently, the DR will be terminated when the accuracy meets the engineering requirements, otherwise the course of DR (in Part_1) will continue, which is the same as the optimization ofwandb.Part_3 GA: The optimization for the connection parameters of the ELM network layers includes four sections.(1)It assumes that the ELM input layer hasnneurons and the hidden layer haslneurons with a real number encoding adopted. Each individual of the population to be selected consists of two parts: the connection weight and the neuron bias of the hidden layer. The length of each individual is (n×l+1). All the values ofwijandbjcan be assigned [−1,1], and the size of the initial population is set to 96 according to the individual numbers of the samples.As for each individual, the output matrixHof the ELM hidden layer and the Moore Penrose generalized inverseH+are calculated using the training set. The connection weightβcan be calculated according to the formulaβˆ=H+T′.In this work, in order to improve the efficiency, the prediction RMSE of the testing set is chosen as the fitness function instead of the whole training set, namely(27)f(·)=RMSE=SSE/n=Tˆ-Trn=1n∑i=1nyiˆ-yi2Here,nis the number of the testing sample dataset,Tˆ=y1ˆ,yˆ2,…,yˆnis the prediction value of the testing dataset, andTr=y1,y2,…yndenotes the true value. The RMSE decreases with the sum of the squared error (SSE), which indicates that the model fitting is better and the data prediction is more successful.After the calculations of the fitness of all individuals in the populations are completed, it is necessary to conduct the GA operations of selection, mutation, and crossover for individuals in the populations. The termination condition is when the fitness function reaches a desired tolerance. The detailed algorithm procedure can be shown in the GA module of Fig. 7. The vector of individuals with larger fitness values in populations is preserved by the selection operation. Their mutated variable vectors are compared to the original ones, and are passed to the next generation. In this work, in order to further improve the generalization performance of the network, the strategy of the genetic selection is adjusted, i.e., the norms of the output weights of the hidden layer are added to the new ones except for the RMSE. After that, the crossover and mutation are conducted to generate the optimized sub-population. If the results are not satisfied, it returns to Section 2, and the optimal individual will be exported until the terminal conditions of RMSE and the weight norm‖β‖are met. In the end, all the optimizations are finished, and the aforementioned seven key dimensions of the preform will be determined.

@&#CONCLUSIONS@&#
In this paper, a new GA-ELM framework was integrated with R-GPLVM. In this framework, the ELM model was constructed including the dimensions, MFFF and MFDS. This has an advantage over traditional FEM optimizations, which have certain limitations when dealing with the complex nonlinear modeling of the preform. However, there are two problems to overcome in the training of the ELM network model. First, as the preform model has more input variables and relatively fewer data compared to conventional ones, we proposed a novel DR model defined as R-GPLVM. In this model, the weakness of GPLVM was overcome using the correlative regression constraint information of the sample observation space, and was observed to be very suitable for small samples with higher-dimensional data. Experiments of regression illustrated that R-GPLVM outweighs GPR in various dimensions. Second, due to the random assignments of the connection weights between the input layer and hidden layer, and the neurons bias of the hidden layer, the predictive validity of ELM will be decreased. In our proposed solution, technical issues such as insufficient generalization performance and unsatisfactory stability of the regression model were overcome. To address this, the connection parameters of the ELM network in each layer were optimized using GA and by adjusting their strategy for the fitness function. As another important optimization approach, DE was also employed to tune the parameters of ELM, and the results demonstrated that GA-ELM has higher stability and more accurate predictions compared to the BP, DE-ELM and ELM models. Furthermore, results of GA-ELM optimization were used to evaluate the previous DR and regression processes. Since the establishment of the kernel function has important effects on the model precision, comparisons of different kernel functions are also conducted, which indicate that the Gaussian kernel function has higher prediction accuracy. In the training of R-GPLVM, theξ1andξ2hyper-parameters were optimized via an SCG algorithm, with the LVs determined. Comparative analysis with CG also shows that SCG has a faster convergence rate.Since the optimization of the design model of the preform is the ultimate goal, parallel selection methods were adopted to obtain the optimized preform in this work, when the predictive MFFF and MFDS are both minimized. By comparing the results of FEM simulation (using the optimized preform) with that applied to the actual production, the feasibility of our proposed method was verified.