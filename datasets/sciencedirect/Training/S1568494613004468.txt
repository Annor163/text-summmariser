@&#MAIN-TITLE@&#
A beginner's guide to tuning methods

@&#HIGHLIGHTS@&#
We study the similarities and differences of well-known tuning methods.We empirically compare their results when tuning numerical parameter of a standard genetic algorithm.We establish guidelines for beginner's users of tuning algorithms.

@&#KEYPHRASES@&#
Metaheuristics,Evolutionary algorithms,Tuning methods,Parameter setting problem,

@&#ABSTRACT@&#
Metaheuristic methods have been demonstrated to be efficient tools to solve hard optimization problems. Most metaheuristics define a set of parameters that must be tuned. A good setup of that parameter values can lead to take advantage of the metaheuristic capabilities to solve the problem at hand. Tuning strategies are step by step methods based on multiple runs of the metaheuristic algorithm. In this study we compare four automated tuning methods: F-Race, Revac, ParamILS and SPO. We evaluate the performance of each method using a standard genetic algorithm for continuous function optimization. We discuss about the requirements of each method, the resources used and quality of solutions found in different scenarios. Finally we establish some guidelines that can help to choose the more appropriate tuning procedure.

@&#INTRODUCTION@&#
Performance of metaheuristic methods is strongly related to the definition of proper components and a suitable setting of parameter values. In this sense, we distinguish between two sets of parameters:•Categorical parameters: They are procedures or functions of the algorithm which can be implemented in different ways. For example, a categorical parameter can be the selection method to use in an evolutionary algorithm. In this case the choice could be made among: roulette wheel, tournament or ranking.Numerical parameters: They are real or integer values. Examples of numeric parameters are population size and transformation operators rates in evolutionary algorithms.Parameter setting problem is an important problem when we use metaheuristics. The selection of a suitable set of values for the parameters can lead to a very good performing version of the metaheuristic at hand. The main difficulties related to the parameter setting problem can be summarized as:•Time consuming task: Parameter setting problem is a time consuming process which usually requires many runs of the metaheuristic with different problem instances and seeds.Best parameter values set depends on problem at hand: It is reasonable to set the same parameter values for all instances of a problem? Different instances of the same problem can strongly vary in terms of size and topology of their search spaces.Parameters are interrelated: Usually, the behavior of the metaheuristics is related to unknown and complex non-linear interactions among the parameters.It is important to note that the stochastic nature of metaheuristics is a key factor to consider when the parameters are being set. Different executions of a metaheuristic using the same parameter values but different seeds can lead to different performances of the metaheuristic algorithm.In [11] the difference between tuning and control parameter setting methods is defined. Main features of parameter tuning methods can be summarized as: (1) Those processes are performed before the run of the tuned algorithm and (2) those processes search for parameter values, which will remain fixed during the run of the algorithm. Moreover, tuning processes usually involve multiple runs of the algorithm in order to analyze its performance with different parameter values and considering its stochastic nature. This makes tuning processes high consuming time tasks. On the other hand, control methods are processes performed during the execution of the algorithm that allow it to change the parameter values during the run. Control methods are included in the original algorithm possibly modifying its original design and increasing its execution time.The first approach to perform this process can be defined as a Brute-force approach. It consists in determining the best parameter values estimating the performance of each possible parameter configuration (a full factorial set) by means of a sufficiently large number of executions and a sufficiently large set of problem instances. In literature, different tuning methods have been proposed. Tuning methods can be classified as:•Hand-made tuning: In this case the metaheuristic algorithm's designer studies the performance of the algorithm in a set of problem instances using appropriate parameter values for the algorithm (according to his own criteria). These parameter values are iteratively modified in the way of achieving improvements in the performance of the algorithm. Hand-made tuning is the typical approach to parameter tuning in the oldest research works in metaheuristic area [11].Tuning by analogy: In this case, the idea is to follow guidelines that recognized authors have established according to the studies they have performed. Some well known parameter settings were proposed by De Jong [10] and Grefenstette [12]. Common guidelines for evolutionary algorithms are to use a mutation probability of 1/L, where L is the length of the chromosome and a crossover probability of 0.6 [10].Experimental design based tuning: We consider as experimental based tuning, those studies based on experimental design to set the parameter values. In [9], the authors perform a statistical exploratory analysis to model the effect and relations between the crossover and mutation probabilities on a standard genetic algorithm.Racing methods can also be considered in this category. Racing methods search for good configurations, starting by an initial set of parameter configurations and, iteratively, discarding the worst performing according to statistical tests [7,6,8].Sequential Parameter Optimization method [2,15,5] corresponds to a special case of the well known sequential model-based optimization methods like the efficient global optimization (EGO) algorithm [13] and the sequential kriging optimization procedure [17]. SPO has been specially adapted to deal with stochastic nature response surfaces. SPO method is a three steps method. First, it performs an experimental analysis of a set of design points (parameter configurations); second, a stochastic process model is used for estimating the algorithm performance; and, finally, it determines new promising design points according to estimations of the model.Search based tuning: One of the first approaches to search based tuning methods is the meta-level genetic algorithm (meta-GA) proposed in [12]. It is itself a standard genetic algorithm as defined in [10]. Parameter values of meta-GA were tuned by analogy according to the De Jong guidelines described in [10].Revac algorithm [21] is an evolutionary algorithm whose operators are used to estimate parameter's distributions. Revac works with a population of parameter calibrations, at each step, it generates a new calibration using mutation and crossover operations. Revac operators are able to search in the whole range of values previously defined for each operator.ParamILS is an iterated local search strategy [14,16]. It starts with a default parameter configuration and iteratively tries to improve its performance by searching in its neighborhood for better parameter calibrations. The Focused ParamILS version implements a discrimination method able to perform robust comparisons between parameter configurations.Hybrid tuning: In [1] the authors propose Calibra method, this method combines experimental design and local search concepts. Calibra uses the Design of Experiments to focus the search promising areas of the search space and local search to vary the parameter values found at each step on the promising areas. Calibra method is able to tune less than five parameters, because it uses the Taguchi's L9(34) design.Tuning methods are strategies designed to automatically search for the best configuration for parameter values in metaheuristics methods. Given a metaheuristic with k parameters, tuning methods search for a parameter configuration c*={p1, …, pk} that leads the best performance of the tuned algorithm.F-Race was proposed in [7] based on the general idea of racing methods. Racing methods are based on brute-force approach. The idea is to provide a better allocation of resources among candidate parameter configurations, reducing the computational resources allocated to poor configurations. F-Race is an iterative process, at each step it evaluates a set of candidate configurations in a new problem instance or seed and eliminates from the set those calibrations that show a statistical worse performance. Fig. 1shows 4 steps of a F-Race process. In the first step, calibrations have not been evaluated, so they are considered to have the same performance. In race i, calibrations white and gray outperform calibration black. At step i+1, calibrations white and gray are clearly better than black calibration and, hence, black calibration is eliminated as it can be seen in step i+2. The idea of discarding poor parameter configurations is to perform more evaluations of the promising configurations on more instances and, hence, obtain a more reliable estimation of their performance.F-Race is a specific racing method specially adapted to tune stochastic search algorithms. It uses Friedman two-way analysis of variance by ranks to compare the performance among the entire set of candidate parameter configurations. This is a non-parametric test based on ranking; thus, hypotheses on the distribution of the observations are not required. F-Race method implements a block design, which considers as sources of variation the different problem instances. The null hypothesis of this test is formulated as: “all possible rankings of the candidates within each block are equally likely”. This hypothesis is contrasted using the Friedman test which considers the statistic shown in Eq. (1).(1)T=(n−1)∑j=1n(Rj−((m(n+1))/2))2∑l=1m∑j=1nRij2−(mn(n+1)2)/4where m is the step of the race with n remaining candidates parameter configurations, Rljis the rank of parameter configuration cjwithin block l andRj=∑l=1mRlj. The statistic T is χ distributed with n−1 degrees of freedom. If the observed T exceeds the (1−α) quartile of such a distribution, the null hypothesis is refused at the approximate level α. In this case, the hypothesis that at least one candidate tends to yield a better performance than at least one other is accepted.A general description of F-Race is shown in Algorithm 1. The procedure starts with a set C of parameter configurations. F-Race requires the definition of a discrete set of values for each parameter to tune. In the first version of F-Race, this initial set includes all possible combinations of discrete sets of parameter values (full factorial set).At the beginning of the process, F-Race performs r runs for each parameter configuration in C to obtain enough information before any elimination. The information is stored in the array of costs of each parameter configuration (Costr(cj)).At each step, F-Race selects randomly an instance to go forward to the performance comparisons among candidate parameter configurations in C. F-Race evaluates the tuned algorithm on the instance and appends this information to the array of costs of each parameter configuration. When all the arrays of costs are computed, F-Race performs the Friedman test according to Eq. (1). If the null hypothesis is rejected, it can be interpreted as at least one candidate configuration tends to show a better performance than at least other configuration. In this case pairwise comparisons between parameter configurations and the best ranked parameter configuration are performed. The parameter configurations with a statistical lower performance are eliminated from the set of candidates configurations C. The block design used by F-Race allows the normalization of the performances observed for each instance and the use of this statistic. A significance level α should be set for the tests. F-Race process stops either when there is only one parameter configuration remaining, or when some predefined amount of executions is reached.Algorithm 1F-Race algorithm.Procedure F-Racegenerate set C of candidate configurationsrace←1while notterminationcriterion() doselect randomly instance i from set of instances Iforeachcj∈Cdoappend the cost of executing cjin i to Cost(cj)endforforeachcj∈Cdoforeach instance l executed doRlj←ranking of configuration cjin block lendforRj←sum of ranks over all instances of configuration cjendforif race>rthenperform the Friedman test according to Eq. (1)if null hypothesis associated to T is rejected thencbest←configuration in C with minimum Rforeachcj∈C\cbestdoperform a pairwise testif null hypothesis is rejected thenC←C\cjendforendifendifrace←race+1end whilereturn remaining set CF-Race method defines three parameters: the amount of races without elimination of candidate configurations (r), the confidence level of hypothesis tests (α) and the maximum number of executions of the tuned algorithm (budget). These parameters and their typical values are shown in Table 1. F-Race also requires the determination of the discrete levels for each parameter to tune. The amount of levels of all parameters will determine the size of the initial set of candidate parameter configurations.The Relevance Estimation and Value Calibration of evolutionary algorithms method was proposed in [21]. Revac is defined as an estimation of distribution algorithm [22]. It works with a set of parameter configurations, a population. Fig. 2shows the representation of Revac population. Each row represents a parameter configuration and each row has k elements, corresponding to the k parameters to tune. Each column represents the distribution of values of one parameter in population, each column has M elements corresponding to the size of the set of configurations.For each parameter, Revac starts the search process with an uniform distribution of values within a given range as it can be seen in Fig. 3. At each step, Revac reduces the range of values of each parameter by using specially designed transformation operators (see Fig. 3).In Revac, the relevance of each parameter is determined according to the entropy measurement [21]. A parameter that shows a low entropy is considered very useful for the performance of the algorithm, because the best performance was obtained when the value of the parameter was fixed at a small possible set of values. Analogously, a parameter that shows a high entropy value will be classified as a not important one for the performance of the algorithm.Revac procedure is shown in Algorithm 2. The algorithm starts with a random population of M parameter configurations.Each parameter configuration is then evaluated. Revac does not define a specialized mechanism to evaluate a parameter configuration quality among a set of instances. Evaluation process considers the execution of the algorithm in the problem instance using just one random seed. Replication is not required as shown in [20].At each iteration, only one new parameter configuration is created and the child configuration is created through a multi-parent crossover and a mutation transformation. The multi-parent crossover considers the best N<M parameter configurations with uniform scanning. Mutation operator calculates, for each parameter, a mutation interval. For this purpose, all different parameter values present in the population are ordered. The upper and lower interval bounds correspond respectively to the Hth lowest neighbor of the value of parameter i in the child and the Hth highest neighbor of the value of the parameter i in the child. Then a random value is selected from the mutation interval and assigned to the child configuration.Revac process ends after 1000 parameter configuration executions.Algorithm 2Revac procedure.Procedure RevacCp←generate M random parameter configurationsevaluate each parameter configuration∈Cpwhile notmax_execsdocchild←uniform crossover of N best parameter configurationscchild←mutate cchildon interval 2·Hevaluate configuration cchildreplace the oldest parameter configuration∈Cpwith cchildcalculate entropy of each parameter∈Cpend whilereturn range of values and entropy of each parameter∈CpRevac method defines three parameters: the size of the population of parameter configurations (M), the size of the crossover and mutation operators (N and H, respectively) and the maximum number of executions of the tuned algorithm. These parameters and their values are shown in Table 2.The ParamILS method was proposed in [14]. It works as an iterated local search algorithm and was inspired by the hand-made tuning process. ParamILS starts with a default parameter configuration and iteratively improves its performance searching in its neighborhood. The neighborhood is defined as the change of the value of just one parameter. Fig. 4outlines the process performed at each iteration. Comparison between parameter configurations are based on both the performance of the parameter configurations and the number of seeds used to calculate their fitness.ParamILS process is shown in Algorithm 3. The algorithm starts with a default parameter configuration, usually based on the experience of the user. Then, the method performs R tries searching for a new configuration with a better quality than the default parameter configuration. This new configuration found follows a local search procedure.At each iteration, ParamILS performs s random perturbations to the configuration at hand, then a local search process (IterativeFirstImprovement(c, N)) is performed using the best configuration obtained and then it compares its performance to the best parameter configuration found so far. There is also a restart probability (prestart), which allows the algorithm to escape from local optimum.Algorithm 3ParamILS.Procedure ParamILSc0←default parameter vectorfori←1 to Rdoc←random parameter vectorif better(c, c0) thenc0←cendforcils←IterativeFirstImprovement(c0,N)while notterminationcriterion() doc←cilsfori←1 to sdoc←random parameter vector inN(c)c←IterativeFirstImprovement(c,N)if better(c, cils) thencils←cifprestartthencils←random parameter vectorendwhilereturn overall best cThe IterativeFirstImprovement(c, N) process used by ParamILS is a procedure which searches randomly in the neighborhood of a parameter configuration for a better one. This procedure is shown in Algorithm 4.Algorithm 4ParamILS IterativeFirstImprovement procedure.ProcedureIterativeFirstImprovement(c,N)repeatc′←cforeachc″∈N(c′)in randomized order doif better(c″, c′) thenc←c″breakuntilc′=creturncAlternative ParamILS algorithm versions are: BasicILS and FocusedILS. These versions differ in the better(c, c′) procedure they implement. BasicILS considers a fixed amount N of seeds to compare the performance of configurations c and c′ while the better(c, c′) procedure implemented by FocusedILS works defining the dominance concept. In this case, it is possible to say that c dominates c′, if and only if, the average performance of the algorithm using c for solving n seeds is better than the performance obtained using c′ configuration for solving n′ seeds, with n>n′.ParamILS method defines four parameters; these parameters and their values are shown in Table 3. The use of ParamILS requires the definition of the interval levels for each parameter and an initial parameter configuration.The Sequential Parameter Optimization method is based on the combination of Design of Experiments (DOE) and Design and Analysis of Computer Experiments (DACE), coupled with specially designed techniques to tackle randomness of stochastic nature algorithms. The combination of DOE and DACE has been extensively used and the first approaches to use them in stochastic algorithms were performed in [4,3]. The term sequential parameter optimization was introduced in [2].The method can be defined as a search heuristic to optimize the performance of stochastic algorithms. SPO is a method that iteratively performs an experimental analysis of a set of design points (parameter configurations), then it estimates the performance of the algorithm to tune by means of a stochastic process model and finally determines new design points.SPO is shown in Algorithm 5. It starts constructing a set of initial design points through a Latin Hypercube Sampling (LHS) design over the range of parameter values defined. The LHS design is used because it provides a good coverage of the design space. To determine n design points, the range of each parameter must be divided into n equally sized intervals. Then one parameter value is randomly chosen for each range defined in order to obtain n possible values.Due to the stochastic nature of the search algorithms considered here, performance for each design point is evaluated by means of several repeats. The best design points from the previous iteration are included in the set of points of the current iteration and reevaluated, thereby increasing its number of repeats. SPO enforces fair comparison of the current best design points; the new generated design points are executed as many times as the best design point has been executed.The set of design points is used to estimate the algorithm performance by means of a stochastic process model defined by Eq. (2).(2)Y(x)=∑j=1kβj·fj(x)+Z(x),where Z(·) is a random process with mean zero and covarianceV(u,v)=σ2R(c,u,v)and using the correlation shown in Eq. (3).(3)R(c,u,v)=∏j=1dexp(−cj·(uj−vj)2).The set of design points of the next iterations includes the best points found so far and a set of expected good designs, whereby expectation is based on the model created in previous step. For this purpose, a new set of (cp) candidate points are also created using the LHS design. This set is typically much larger than the initial set of design points. The generalized expected improvement criterion is determined computing the candidates model value. This criterion takes into account that we are uncertain about unknown design points. Thus, it estimates the probability of a candidate of being better than the known best so far, by taking the modeling error into account. Therefore, the points with the highest probability of being better than the current best design point are selected as candidates for the next iteration.Algorithm 5Sequential Parameter Optimization procedure.Procedure Sequential Parameter OptimizationC←n LHS designed configurationswhile notterminationcriterion() dofor eachi∈Cdorun cisamples timesCost(ci)←average performance of samplesconstruct modelMaccording to Cost(C) using (2) and (3)Cold←select r·n best configurations from CCcandidates←cp LHS designed configurationsCnew←select (1−r)·n best promising configurationsfrom Ccandidatesaccording to modelMC←Cold+Cnewupdatesamplesizeendwhilereturn overall best c∈CTable 4shows the four parameters defined by SPO and their typical values. The method requires the definition of a range of values for each parameter to tune.

@&#CONCLUSIONS@&#
In this section we summarize the main global conclusions about our comparison of these parameter tuning methods.First, it is clear that in terms of performance it is difficult to determine if there exists a tuning method able to find always the best performing parameter calibration. Hence, we briefly enumerate the main features of tuning methods which can determine their usability in different tuning scenarios.•About tuning by analogy: It is clear that to use a tuning method instead of using a set of recommended parameter values is a good choice. For all the functions, there is at least one parameter configuration that performs better than the default and the set of random parameter configurations.About the use of tuning methods: It is clear that the use of a blind random search approach to find a good parameter configuration has been a good option for this algorithm. The average performance obtained by its calibrations is for all functions comparable to the performance obtained by tuning methods. Moreover, in most cases, the average performance of the calibrations obtained by the blind random search is significantly better compared to the average performance obtained by the default and random configurations.It is important to note that the only result that can be obtained from the blind random search process is a calibration that performs well with one seed, whereas the result obtained by ParamILS is a configuration that is statistically better than all others studied considering a significant set of seeds. It is also important to note that tuning process performed by the blind random search strategy is, on average, more than twice time consuming compared to the process performed by ParamILS or SPO for getting equivalent quality results. Table 13shows the average tuning time spent by tuning methods in each scenario.Parameters nature: It is important to note that Revac is not able to search categorical parameters, because its transformation process is done on a continuous search space for the parameters. The categorical parameters have categorical values; therefore, there are not numerical relations among their values, as is required by this methods. F-Race, ParamILS and SPO are able to tune numeric and categorical parameters. Firstly, both require the definition of discrete set of values for the parameters, which in some cases can be restrictive.Budget of the process: In our experiments, we considered a fixed budget of 1000 SGA executions, because this is the budget established by Revac for its tuning processes. As can be seen in experimental results presented, the amount of executions required for each tuning process depends on the problem instance to tune and the tuning method that is being used. In general, it is not possible to determine this value before the process; hence to keep this value fixed is not a good option.From a practical point of view, methods like ParamILS and SPO have the advantage that at each step of the tuning process they are able to report a solution; hence, the process can be stopped at any point of the search. After the first M SGA executions of Revac and r*size(C) SGA executions of F-Race, they could also be stopped and they will be able to deliver results.Stochastic nature: Revac, ParamILS and SPO are clearly stochastic nature methods; hence, we have run each method 5 times for each function to tune. From the results we can note that, in general, all are able to find similar results in different executions, but there are some cases, where the results of some methods clearly depend on the seed used. The most clear examples are the results of F-Race in Scenario 1. Even when F-Race is not a stochastic nature method, its evaluations depend on the seeds selected to execute the algorithm to tune.Input requirements: The definition of discrete sets of values for F-Race is a hard task, because their sizes are strongly related to the time the tuning process will take. ParamILS also requires the definition of discrete sets of values for parameters, but the size of these sets has shown not being determinant on the quality of the tuning process. On the other hand, Revac and SPO just require the definition of ranges of values to search.Parameter interactions: F-Race, ParamILS and SPO work with the notion of configuration more than just parameter values, i.e. they put more attention in the interaction between parameter values compared to the process performed by Revac where each parameter range of values is iteratively reduced.Output expected: Related to the output of each method, we define three kinds of users. The first one is a user who wants to compare its new metaheuristic with respect to other metaheuristics methods. In this case, the user is interested in finding one parameter calibration able to obtain the best performance of the metaheuristic method. The second kind of user wants to explore and analyze the capabilities of his metaheuristic. Finally, the third kind of user is who tries to quickly find a good configuration for the problem.For the first user ParamILS and SPO can be good options to tune his/her algorithm. The search performed by Revac, on the other hand, is oriented to find good ranges of values for each parameter. The problem with this approach can appear when more than one range gives a good performance of the algorithm to tune. In these cases, Revac can converge to a wide range of values which can include not good parameter values.For the second kind of user, the recommendation is to spend more time using F-Race. The final set of configurations delivered by F-Race is an important source of information which can be used to detect not effective or redundant components [19]. This study has proved that F-Race is useful to assist the design process.For the last kind of user, the recommendation is to use ParamILS which performs a strongly intensified search on the parameter search space and can quickly find good performing parameter configurations able to solve the problem at hand. Performance of ParamILS calibrations shown always a very good performance in almost all cases studied.Time: In terms of time, it is important to note that the quality criterion of the SGA tuned in this study is the amount of evaluations to the optimum. Therefore, in our case, a good parameter calibration determines, always, a low time consuming SGA execution. Table 13 shows a summary of the tuning time (in minutes) measured in Section 3. Here we can note that ParamILS, Revac and SPO take, on average, half the time required by F-Race and BRS to tune the same scenario. This is due to the fact that ParamILS and SPO work with a small set of configurations and, usually, can quickly improve their quality. In this case, Revac performance can be helped by a set of good quality configurations which determine its efficient behavior even when it works with a large population. On the other hand, F-Race is forced to test its entire large set of initial configurations. Note that these results show that ParamILS and SPO methods are tuning process strongly focused on finding good parameter calibrations.Table 14shows the average number of SGA executions required by each tuning method to reach its final parameter calibration. Here we can observe that F-Race is the most expensive method, this due to the execution of the initial number of runs without elimination of calibrations. We remark that F-Race is not able to deliver a result until it performs its initial phase of testing runs. Revac looks like the second more runs-consuming method. This condition is mainly due to the size of the final range of parameter values. Here, we arbitrarily decided that Revac process can be considered finished when all the parameter ranges have been reduced to their 5%. However, some users can consider that a wide range of values is sufficiently good. SPO and ParamILS look like the more efficient tuning methods using just a 40–50% of the total budget assigned.Blind random search method is very competitive considering the number of SGA executions it required to find its best parameter configuration. However, it is important to emphasize that the time required for the SGA executions is always higher than the time required by the other tuning processes.Stop criterion: From the methods used, the only method able to stop the search process by itself is F-Race. It is able to detect only one configuration left in the set of candidate configurations and then stop the process. Some possible criteria to consider can be related to the convergence of the tuning process and/or the percentage of reduction of the initial range of values.Moreover, it is important to remark that tuning processes performed by ParamILS, SPO and even Revac can be stopped at almost any point of the search and they will be able to deliver a good quality parameter calibration.Training and testing sets: From the results of the three scenarios the difference that can produce the selection of the training sets in the final results of the tuning methods is not clear. In our experiments we considered the multimodality of instances as a feature to design the training and testing sets, but apparently this is not a feature that differentiates the instances for the SGA. According to bibliography, diverse training set should be considered while tuning parameter for an algorithm in order to avoid over-fitting effects. A good idea can be to incorporate boostrap methods in tuning methods to reduce these problems.Table 15summarizes the main features of the tuning methods discussed previously.