@&#MAIN-TITLE@&#
Reliable scalable symbolic computation: The design of SymGridPar2

@&#HIGHLIGHTS@&#
This paper presents the design and initial evaluation of SymGridPar2.SymGridPar2 is designed to provide scalability and fault tolerance.Scalability using layering and by allowing the programmer to control task placement.We report the scalability and efficiency, including weak scaling to about 32k cores.We present a fault tolerant work stealing protocol and measure supervision overheads.

@&#KEYPHRASES@&#
Parallel functional programming,Locality control,Fault tolerance,

@&#ABSTRACT@&#
Symbolic computation is an important area of both Mathematics and Computer Science, with many large computations that would benefit from parallel execution. Symbolic computations are, however, challenging to parallelise as they have complex data and control structures, and both dynamic and highly irregular parallelism. The SymGridPar framework (SGP) has been developed to address these challenges on small-scale parallel architectures. However the multicore revolution means that the number of cores and the number of failures are growing exponentially, and that the communication topology is becoming increasingly complex. Hence an improved parallel symbolic computation framework is required.This paper presents the design and initial evaluation of SymGridPar2 (SGP2), a successor to SymGridPar that is designed to provide scalability onto 105 cores, and hence also provide fault tolerance. We present the SGP2 design goals, principles and architecture. We describe how scalability is achieved using layering and by allowing the programmer to control task placement. We outline how fault tolerance is provided by supervising remote computations, and outline higher-level fault tolerance abstractions.We describe the SGP2 implementation status and development plans. We report the scalability and efficiency, including weak scaling to about 32,000 cores, and investigate the overheads of tolerating faults for simple symbolic computations.

@&#INTRODUCTION@&#
Symbolic computation has underpinned key advances in Mathematics and Computer Science, for example in number theory, cryptography, and coding theory. Many symbolic problems are large, and the algorithms often exhibit a high degree of parallelism. However, parallelising symbolic computations poses challenges, as symbolic algorithms tend to employ complex data and control structures. Moreover, the parallelism is often both dynamically generated and highly irregular, e.g. the number and sizes of subtasks may vary by several orders of magnitude. The SCIEnce project developed SymGridPar [23] as a standard framework for executing symbolic computations on small-scale parallel architectures (Section 2). SymGridPar uses OpenMath [32] as a lingua franca for communicating mathematical data structures, and dynamic load management for handling dynamic and irregular parallelism.SymGridPar (SGP) is not, however, designed for parallel architectures with large numbers of cores. The multicore revolution is driving the number of cores along an exponential curve, but interconnection technology does not scale that fast. Hence many anticipate that processor architectures will have ever deeper memory hierarchies, with memory access latencies varying by several orders of magnitude. The expectation is similar for large scale computing systems, where an increasing number of cores will lead to deeper interconnection networks, with relatively high communication latency between distant cores. Related to the exponential growth in the number of cores is a predicted exponential growth in core failures, as core reliability will remain constant, at best. These trends exacerbate the challenges of exploiting large scale architectures because they require the programmer to pay attention to locality and to guard against failures.This paper presents the design and initial evaluation of SymGridPar2 (SGP2), a successor to SymGridPar that is designed to scale onto 105 cores by providing the programmer with high-level abstractions for locality control and fault tolerance. SGP2 is being developed as part of the UK EPSRC HPC-GAP project, which aims to scale the GAP computer algebra system to large scale clusters and HPC architectures.The remainder of the paper is organised as follows. Section 2 surveys related work on parallel symbolic computation. Section 3 presents the SGP2 design goals, principles and architecture. A key implementation design decision is to coordinate the parallel computations in HdpH, a scalable fault tolerant domain specific language (Section 3.2).We describe how scalability is achieved using layering and by allowing the programmer to control task placement on a distance-based abstraction of the communication topology of large architectures (Section 4). We outline how fault tolerance is provided by supervising remote computations. A fault tolerance API is presented, and we sketch higher-level fault tolerance abstractions, namely supervised skeletons (Section 5).SGP2 is still under development, and we outline the current implementation and give preliminary scalability and fault tolerance results. Specifically, we investigate the scalability and efficiency of a layered task placement strategy on approximately 32,000 cores of an HPC architecture (Section 6.2), and we evaluate the overheads of a fault tolerant skeleton on a Beowulf cluster, both in the presence and absence of faults (Section 6.3).This paper is an extension of a paper [27] presented at the 2013 ACM Symposium on Applied Computing (SAC×³13). It goes beyond the conference version in the following ways. It provides more details on topology-aware work stealing in Section 4 and a more extensive discussion of SGP2 fault tolerance in Section 5, including a new fault tolerance work stealing protocol. Section 6 also presents new experimental results, including a demonstration of weak scaling of an SGP2 prototype up to 32k cores.

@&#CONCLUSIONS@&#
