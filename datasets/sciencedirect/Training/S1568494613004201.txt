@&#MAIN-TITLE@&#
Study on multi-center fuzzy C-means algorithm based on transitive closure and spectral clustering

@&#HIGHLIGHTS@&#
The multi-center initialization method can get rid of the problems that the FCM algorithm is sensitive to the initial prototypes, and it can handle non-traditional curved clusters.The similarity matrix generated by the Floyd algorithm is notably block symmetric, which ensure the effective extraction of spectral features.The problem of clustering samples is transformed to a problem of merging subclusters, the computational load is low, and has strong robustness.

@&#KEYPHRASES@&#
Fuzzy C-means algorithm,Multi-center,Lattice similarity,Spectral clustering,

@&#ABSTRACT@&#
Fuzzy C-means (FCM) clustering has been widely used successfully in many real-world applications. However, the FCM algorithm is sensitive to the initial prototypes, and it cannot handle non-traditional curved clusters. In this paper, a multi-center fuzzy C-means algorithm based on transitive closure and spectral clustering (MFCM-TCSC) is provided. In this algorithm, the initial guesses of the locations of the cluster centers or the membership values are not necessary. Multi-centers are adopted to represent the non-spherical shape of clusters. Thus, the clustering algorithm with multi-center clusters can handle non-traditional curved clusters. The novel algorithm contains three phases. First, the dataset is partitioned into some subclusters by FCM algorithm with multi-centers. Then, the subclusters are merged by spectral clustering. Finally, based on these two clustering results, the final results are obtained. When merging subclusters, we adopt the lattice similarity method as the distance between two subclusters, which has explicit form when we use the fuzzy membership values of subclusters as the features. Experimental results on two artificial datasets, UCI dataset and real image segmentation show that the proposed method outperforms traditional FCM algorithm and spectral clustering obviously in efficiency and robustness.

@&#INTRODUCTION@&#
Clustering plays an important role in pattern recognition, image processing, and computer vision [1]. With a clustering technique, a collection of objects or feature vectors is partitioned into clusters. In the past few decades, many clustering algorithms have been developed, which mainly contain hierarchical clustering (such as Single Link and Complete Link), partitional clustering (such as k-means, fuzzy C-means, Gaussian Mixture and Density Estimation) and spectral clustering. Moreover, in the last few years, fuzzy C-means (FCM) clustering and spectral clustering algorithms are research focus [2,3].Unlike the hard clustering techniques (each object is assigned to one and only one cluster), fuzzy C-means clustering allows an object to belong to a cluster with a grade of membership. Moreover, when there is not enough information about the structure of the data, fuzzy C-means clustering algorithm can handle this uncertainty better, and has been widely applied to the data clustering area. However, there are still some open problems in FCM algorithm [4–6]. (1) The number of clusters needs to be known in advance. But we have to cluster objects without knowing the number of clusters in a lot of cases. (2) It can only recognize spherical clusters, but not irregular or elongated shaped clusters. (3) It is easy to fall into local optimum in the iterative process.Spectral clustering algorithm (SC) transforms the original dataset into a new one in a lower-dimensional eigenspace by utilizing eigenvalues and eigenvectors of a similarity matrix derived from the dataset. Then the traditional clustering algorithms can be performed on this new dataset to obtain the final clustering result. In spite of these successes in spectral clustering algorithm, there are still some problems urgently needing to be solved, which are as follows [7–9]: (1) The construction of similarity matrix. Up to now, there is not a method of wide application or high validity. (2) There is not a theory which can solve the selection (including how many and what kind of eigenvectors are needed as spectral features) and the usage of the eigenvectors. (3) The high load of computation makes the spectral clustering act poorly in large-scaled learning.To solve the problem of partitional clustering algorithm that one center of each class in the irregular shaped datasets, such as non-convex and elongated shaped classes, cannot represent the class well in clustering, the multi-center clustering algorithm [10] proposed by Chaudhuri and Chaudhuri. Based on the multi-center clustering algorithm, to solve the problems of FCM algorithm, an unsupervised fuzzy clustering with multi-centers algorithm [11] is provided by Tao. In the multi-center clustering algorithm, the most widely used method in merging subclusters is DBSCAN algorithm [12] based on density (proposed by Martin Ester). The DBSCAN algorithm considers that a class contains the samples which has more than a specified threshold number of samples in a specified neighborhood. The class is constructed by finding a maximum set of the connected samples which satisfy the density (samples in the neighborhood) condition. That is, DBSCAN expands the class in a point-wise manner. Although DBSCAN can find the classes with arbitrary shape, a priori information (specified neighborhood and density threshold) is necessary, and the errors are high when the samples’ distribution density is unbalanced.To achieve a successful clustering without the subclusters-merging difficulty by the multi-center fuzzy C-means clustering algorithm, the spectral clustering algorithm is introduced in this paper. Spectral clustering algorithm has problems which are the construction of similarity matrix and the high load of computation. In order to solve these problems, we choose the fuzzy membership values of subclusters as the features in the lattice similarity method to construct the similarity matrix. At the same time, the Folyd algorithm developed from the Zadeh's operations is used to modify the similarity, generate the transitive closure of subclusters’ fuzzy relations, which is notably block symmetric. The similarity matrix among subclusters instead of the similarity matrix among all the samples, the novel method reduces the computational cost brought by spectral clustering.In order to handle the non-spherical datasets classification, in this paper, we design a novel fuzzy clustering method, named multi-center fuzzy C-means algorithm based on transitive closure and spectral clustering (MFCM-TCSC). Large classes or non-spherical shaped datasets are first divided into lots of subclusters by using the FCM algorithm. The fuzzy membership values, which represent the degrees that samples belongs to different subclusters, are used as the features to compute the similarity among subclusters via the lattice similarity method. The Folyd algorithm developed from the Zadeh's operations is used to modify the similarity, generate the transitive closure of subclusters’ fuzzy relations. The transitive closure is regarded as the similarity matrix to generate the Laplacian matrix used in spectral decomposition. The eigenvectors corresponding to the second minimal eigenvalues and the third minimal eigenvalues are chosen to be the spectral features. At last, the FCM algorithm is applied again to cluster the spectral features, thereby, to merge the subclusters. The main highlights in the novel algorithm: (1) the multi-center initialization method can get rid of the problems that the FCM algorithm is sensitive to the initial prototypes, and it can handle non-traditional curved clusters. (2) The similarity matrix generated by the Floyd algorithm is notably block symmetric, which ensure the effective extraction of spectral features. (3) The problem of clustering samples is transformed to a problem of merging subclusters, the computational load is low, and has strong robustness.The remainder of this paper is organized as follows. The FCM algorithm and spectral clustering are given in Section 2. Section 3 shows the multi-center FCM algorithm based on transitive closure and spectral clustering. Experimental results, the analysis of the robustness and the complexity of our method are described in Section 4. Finally, we present our conclusions and discuss future work in Section 5.Let the sample set be X={x1, x2, …, xN}⊂Rq×N, where N is the number of elements in the sample set, and q is the dimension of the feature space. Divide the sample set X into c classes, then the fuzzy partition matrix of N samples belonging to c classes isU=[uij]c×N, where uij(1≤i≤c, 1≤j≤N) is the fuzzy membership degree of the jth sample xjbelongs to the ith class, and uijshould satisfy the following two constraints:(1)∑i=1cuij=1,1≤j≤N(2)0≤uij≤1,1≤i≤c,1≤j≤NBezdek [14] defines a general objective of FCM clustering algorithm as:(3)minJm(X,U,P)=∑i=1c∑j=1Nuijmdij2where P=(p1, p2, …, pc) in Eq. (3) is a q×c matrix; pj(j=1, 2, …, c)∈Rqis the center of the jth class; (dij)2=||xj−pi||M=(xj−pi)TM(xj−pi) shows the general expression of the distance between xjand pi, and M is the feature weighting matrix; m is the fuzzy index which controls the fuzzy degree of the fuzzy partition matrix U.By using the Lagrange multipliers, we can solve for the fuzzy partition matrix U. The fuzzy membership degree uijand the center pican be updated by the expectation–maximization (E–M) algorithm:E-step:(4)uij=1∑k=1c(dij2/dkj2)(1/(m−1))forj=1,2,…,Nandi=1,2,…,cM-step:(5)pi=∑j=1Nuijm⋅xj∑j=1Nuijmfori=1,2,…,cThe E–M algorithm recursively proceeds until a convergence condition is satisfied. The FCM algorithm is an iterative process to minimize the objective function Jm(X, U, P).Spectral clustering concerns the optimization of graph cut problem in spectral graph theory. In spectral clustering, a neighborhood graph on the data points is first constructed based on some criteria, such as the fully connected graph or the K-nearest neighbor graph. Then, a weighted affinity matrix S∈ℜN×Nis defined, whose (i, j) element sijreflects the similarity between xiand xj. The measuring of the similarity of samples is an open problem in the spectral clustering. There is not a method of wide application or high validity. Gaussian kernel function is adopted in most references to measure the similarity of samples. Specifically(6)sij=exp(−||xi−xj||2/2σ2)i≠j0i=jwhere ||·|| denotes the Euclidean norm, σ is the Gaussian kernel parameter. Many examples show that this function is sensitive to σ, but the artificial selection of σ leads to some certain limitations. Ref. [13] gets the σ automatically by repeating NJW algorithm. It eliminates the artificial factors, but the computation time, on the other hand, increases. Currently, the generation of the similarity matrix in most references relies on knowledge in different fields. There is not a widely used rule.Given a neighborhood graph with affinity matrix S, taking the bipartitioning of a graph as an example, A and B are two disjoint subsets of graph G, subject to A∩B=ϕ and A∪B=V, a simple but efficient clustering criterion is the normalized cut (Ncut) [7] which is defined as(7)Ncut(A,B)=assoc(A,B)assoc(A,V)+assoc(A,B)assoc(B,V)where assoc(A, B)=∑i∈A,j∈BSij. In other words, Ncut criterion computes the cut cost as a fraction of the total edge connections to all the nodes and achieves a better balance in the cardinality A and B. Thus the aim is to minimize Ncut. Following some algebraic formulations, it turns out [8] that minimizing normalized cut can be equivalently recast as(8)minTr(HT(D−S)H)A1,…,Acs.t.HTDH=Iwhere A1, …, Acis a partition of X (A1∪⋯∪Ac=X, Ai∩Aj=ϕ, i≠j and Ai≠ϕ, i=1, …, c), c is the number of classes, D is an N×N diagonal matrix with Dii=∑jsij, I is the identity matrix, H∈ℜN×cis a specific discrete matrix, and Tr denotes the trace of a matrix.Unfortunately, solving the above discrete optimization problem is NP-hard. To make it tractable, an efficient relaxation is adopted so as to solve a real-valued problem instead of a discrete-valued one. This is done by computing the first k generalized eigenvectors z1, …, zk, corresponding to the k smallest eigenvalues, of the generalized eigenvalue problem(9)(D−S)z=λDzwhere λ is the eigenvalue and z is the corresponding eigenvector. Furthermore, D−S is called as the normalized Laplacian matrix which is symmetric positive semi-definite.Finally, fuzzy C-means method is performed on the row vectors of Z=[z1, …, zk]∈ℜN×kto obtain the clusters.Many datasets are non-convex shaped in practical clustering problems. As is showed in Fig. 1(a), red stars and green stars represent samples belonging to different classes. The direct application of the FCM algorithm cannot achieve good results, because the FCM algorithm is only suitable in spherical clusters, it cannot recognize irregular or elongated shaped clusters, and it is sensitive to the initial centers of clusters. For the purpose of solving these problems, we propose the multi-center FCM algorithm based on transitive closure and spectral clustering (MFCM-TCSC). This method divides irregular classes into lots of subclusters (multi-cluster), transforms the classification problem into a problem of merging subclusters. There is no need to consider the initial number of clusters, whether the distribution of datasets is spherical, nor the local optimum problem in clustering.In order to recognize the irregular classes, one class is represented by multiple subclusters. That is, partitioning the irregular class into some subclusters (multi-clusters) at first, hence the classification problem can be converted into a subcluster merging problem. As is showed in Fig. 1(a), the dataset is made up of two classes. When the multi-center FCM algorithm is adopted, the dataset is clustered into 6 subclusters. The first class is partitioned into 4 subclusters, and the second is partitioned into 2 subclusters. Merge those subclusters and get the results showed in Fig. 1(c).In the merging the subclusters process, two criteria are acquired from analyzing the results in Fig. 1: (1) the subclusters of the same class should satisfy the similarity and nearness relations (such as the subclusters 1 and 2 in Fig. 1(a)); (2) the similarity can be transitive between the subclusters of the same class (in Fig. 1(a), the similarity between subcluster 1 and subcluster 4 can be obtained by taking the maximal similarity value through the path: subcluster 1→subcluster 2→subcluster 3→subcluster 4).In this paper, the number of subclusters isN, which is the upper limit of the number of cluster based on the Literature [5,14,15]. After the large classes or non-spherical datasets are divided into lots of subclusters by using the FCM algorithm, then merge the subclusters under those two criteria.After the multi-center FCM algorithm clustering, the centers and the fuzzy membership values of subclusters will be derived. The representation of the features of subclusters can be studied in two aspects: the centers of subclusters and the information of fuzzy membership matrix. (1) Replace subclusters with features of centers. That is, the features of the center pi(i=1, 2, …, c)∈Rqas the features of the ith subcluster. It is the sparsification of samples, which can reduce the computational load of the similarity measure and the spectral clustering. But the position of a center is related closely to the quantity of samples in this cluster which the center represents. If one class is partitioned sparsely and another densely, the clustering results may be wrong when the features of centers are used as the features of subclusters. (2) Regard the values of the fuzzy membership matrix as the features of subclusters. That is, the values of ui.(i=1, 2, …, c), which is the fuzzy membership values of all samples belongs to the ith subcluster (the ith row of the fuzzy membership matrix U), as the features of the ith subcluster. For the reason that the fuzzy membership matrix is a fuzzy matrix, the features compose a fuzzy sequence, so that the similarity among subclusters can be measured with the similarity measure of the fuzzy sets.In this paper, the values of the fuzzy membership matrixU=[uij]c×Nare used as the features of subclusters:(10)U=[uij]c×N=u11u12⋯u1Nu21u22⋯u2N⋮⋮⋮⋮uc1uc2⋯ucN→the features of subcluster1→the features of subcluster2⋮→the features of subclustercwhere uij(1≤i≤c, 1≤j≤N) is the fuzzy membership values of the jth sample xjbelongs to the ith subcluster.According to the Criterion 1 of the merge subclusters showed in Section 3.1, we want to find a similarity measure which can show the similarity and nearness relations among subclusters. When the values of the fuzzy membership matrix are used as the features of subclusters, plenty of experiments demonstrate that the lattice similarity method will be better while both subcluster i (the feature is ui.) and subcluster j (the feature is uj.) have the elements belonging to them and unbelonging to them. In this paper, the lattice similarity is used as the similarity measure among subclusters. The formula of lattice similarity is:(11)rij=12[(ui.∘uj.)+(1−ui.⊗uj.)]where rijis the similarity degree between subclusters i and j,ui.∘uj.=∨k=1N{uik∧ujk}is the inner product of ui. and uj.,ui.⊗uj.=∧k=1N{uik∨ujk}is the exterior product of them. The formula of lattice similarity listed above is given due to its definition. Actually, the lattice similarity does not satisfy the reflexivity. In this paper, we set the elements on its diagonal zero.Get the similarity of all the subclusters by the similarity measure based on the lattice similarity, build the relationship graph of the similarity which is showed in Fig. 1(b). In this figure, there are six subclusters, and we can know that there are more than one similarity path from the ith subcluster to the jth subcluster from the graph.According to the Criterion 2 of the merge subclusters showed in Section 3.1, we want to construct a similarity measure which can be transitive among the subclusters of the same class. To solve this problem, we need to modify the original lattice similarity. We define that the real similarity of two subclusters is the optimum similarity relation values in the relationship graph. Obviously, to find the real similarity of two subclusters is an optimum path problem, and it embodies the transitivity theory.Floyd algorithm is a classical method to get the shortest path. The method of Floyd algorithm is introduced as follows. Insert vertexvkas an intermediate (all vertices in the path except for the starting vertex and the terminating vertex) between any two vertices and compare the known shortest distance fromvitovjwith the possible distance fromvitovjwhich comes out while the existence of the inserted vertexvk. Displace the original value with the smaller one to get the new weighted adjacency matrix. Repeat it, when all the vertices have been in turn inserted between and as the intermediate, the new weighted adjacency matrix can tell every shortest distance from one vertex to another. It is the distance matrix of graph G.In this paper, Floyd algorithm is adopted to find the real similarity among subclusters. That is, apply max and min operators of Zadeh's operations between two subclusters to get the optimum similarity between them. For example, the real similarity between subclusters 1 and 2 in Fig. 1(b) can be formulated as(12)r12*=max{r12,min(r13,r32),…,min(r13,r34,r42),…,min(r13,r34,r45,r56,r62),…}We can know it from formula (12) that the essence of the computation progress of Floyd algorithm by using max and min operators to get the real similarity is finding the maximum among all the possible paths as the similarity of subclusters and choosing the minimum in a path as the weight of itself.When we use Floyd algorithm to find the optimum similarity (real similarity), we first consider the condition of inserting only one vertex. The condition of inserting multiple vertices is actually a computational composition of inserting only one vertex. The detailed computational process of finding the real similarity is as follows.To subclusters x and z, Y is the set of all the subclusters, R1 is the original similarity between x and Y, R2 is the original similarity between z and Y. If an optional element of Y is set as the intermediate vertex, the real similarity of x and z is R1∘R2. The formula of the real similarity is(13)(R1∘R2)(x,z)Δ̲̲∨y∈Y(R1(x,y)∧R2(y,z))If the number of subclusters is n, the optimum similarity matrix Ropt of inserting one intermediate vertex can be derived from original similarity matrix R. LetR=(rij)n×n,Ropt=(bij)n×n, then Ropt is(14)Ropt=R∘R=(bij)n×nwherebij=∨k=1n(rik∧rkj).The condition of inserting multiple vertices is actually a computational composition of inserting only one vertex, which can be computed with iteration on the fundamental of the former condition. When the similarity matrix converges and does not change any more, it is the real similarity matrix. Let R=rn×n, it is the original similarity matrix. Similar to getting the fuzzy equivalent matrix in transitive closure, there is a minimal natural number h(h≤n) satisfying t(R)=Rh. To all the natural numbers l lager than h, there is Rl=Rh, and t(R) is the real similarity matrix. In this paper, the square method is adopted to get the real similarity matrix t(R). Square one by one from original similarity matrix R:(15)R→R2→R4→⋯→R2i→⋯.The first time Rh∘Rh=Rh, Rhis the real similarity matrix t(R). After i times squares, we can get the real similarity matrixt(R)=R2iof an n×n matrix R. It satisfiesR2i≤Rn, 2i≤n. The complexity of this method is O([log2n]+1).For example, we can get the real similarity matrix (transitive closure) among subclusters in Fig. 1(b) by our method, which are given in Fig. 2.Under the guidance of the two criteria of merging subclusters which showed in Section 3.1, we adopt Floyd algorithm to get the transitive closure of subclusters, which is of similarity and nearness relations, also transitivity. In this paper, we define that the real similarity matrix of the subclusters is the transitive closure of the subclusters.When the transitive closure got from the original similarity of subclusters, the key is to find an algorithm to merge those subclusters by using the transitive closure. Among so many algorithms, spectral clustering algorithm is a method based on the similarity matrix of samples to cluster, so we use spectral clustering to merge those subclusters. In our method, the transitive closure as the similarity matrix is used in spectral clustering to merge those subclusters, which can gets rid of the selection of parameters in generating the similarity matrix. Furthermore, the transitive closure can reflect the similarity among subclusters more objectively than the matrix got from the Gaussian kernel function. In spectral clustering, Merge subclusters by using FCM algorithm again upon the proper spectral features. In the graph Laplacians theory, the eigenvectors corresponding to the k smallest eigenvalues contains the main classification information. In this paper, we selects the eigenvectors corresponding to the second minimal eigenvalues (issues in connection with curve segmentation) and the third minimal eigenvalues (issues in connection with surface segmentation) as the spectral features based on the Literature [7,8].The novel algorithm contains three phases. First, the data set is partitioned into some subclusters with spherical distribution by using the FCM algorithm with multi-centers, and the fuzzy membership values of subclusters are used as the features to construct the similarity matrix among subclusters via the lattice similarity method. Then, the Folyd algorithm developed from the Zadeh's operations is used to modify the similarity, generate the transitive closure of subclusters’ fuzzy relations, and the subclusters obtained in the first phase are merged by spectral clustering. Finally, based on these two clustering results, the final results are obtained. The main steps of the proposed method are given as follows:Algorithm 1MFCM-TCSCInput: Given the number of subclustersN, the number of classes c, the distance function ||·||, the termination criterion ζ.Output: The fuzzy membership matrix U, the cluster centers P of the classes and the partition of the dataset.1.Perform the FCM algorithm with multi-centers to get the fuzzy membership matrix and cluster centers of the subclusters. The number of subclusters isN.Regard the values of the fuzzy membership matrix as the features of subclusters. Get the similarity matrices of the subclusters R via the lattice similarity by using formulas (11).Adopt Floyd algorithm in formula (15) to modify the original similarity matrix R into transitive closure t(R).Regard the transitive closure t(R) as the similarity matrix S which is used in spectral decomposition. Build the Laplacian matrix according to formula (9). Decompose the Laplacian matrix to get the spectral features.Select spectral features. Generally, the eigenvectors corresponding to the second minimal eigenvalues and the third minimal eigenvalues are chosen to be the spectral features.Cluster spectral features by using the FCM algorithm to get the results of merging the subclusters to implement the clustering of the dataset.In this section, we verify the effectiveness of the MFCM-TCSC algorithm. We first apply it to two non-convex clustering problems on artificial datasets. The results are compared with fuzzy C-means algorithm (FCM) [6], spectral clustering algorithm (SC) [8] and the multi-center FCM algorithm based on the original lattice similarity matrix (MFCM-OLS). In order to show the performance visually, the relationship graphs of the similarity matrix based on the lattice similarity method and the transitive closure of subclusters are shown. In all the algorithms, the desired number of clusters was set in advance.In the second experiment, we solve seven sub-datasets of the UCI datasets [16] classification problems using our method. The results are compared with those of the fuzzy C-means algorithm (FCM), a modified K-means algorithm using the manifold-distance-based dissimilarity measure (DSKM) [17], the genetic-algorithm-based clustering technique (GAC) proposed by Maulik and Bandyopadhyay [18], DBSCAN algorithm based on density proposed by Martin Ester [12], and expectation-maximization clustering based on Gaussian mixture models (EM) [19]. Then we make IRIS dataset classification problems by our method as the typical analysis. In all the algorithms, the desired number of clusters was set in advance.In the third experiment, we solve the segmentation problems of road image by using our method, and compared with the gray-level histogram, FCM, and the EM method.In the first two experiments, because of the true partitioning is known, we use normalized mutual information (NMI) as the clustering validation measure. NMI is an external clustering validation metric that estimates the quality of the clustering with respect to the given true labels of the datasets: it measures how closely the clustering algorithm could reconstruct the underlying label distribution in the data [20]. If C is the random variable denoting the cluster assignments of the instances and Y is the random variable denoting the underlying class labels on the instances, then the NMI measure is defined as:(16)NMI(C,Y)=I(C;Y)(H(C)+H(Y))/2where I(C;Y)=H(Y)−H(Y|C) is the mutual information between the random variables C and Y, H(C) is the Shannon entropy of C, and H(Y|C) is the conditional entropy of Y given C. NMI effectively measures the amount of statistical information shared by the random variables representing the cluster assignments and the user-labeled class assignments of the instances. The range of NMI values is 0 to 1. In general, the larger the NMI value is, the better the clustering quality is. NMI is better than other external clustering validation measures such as purity and entropy, since it does not necessarily increase when the number of clusters increases [21].We implemented the MFCM-TCSC algorithm in Matlab 7.0 and conducted our experiments on a PC with PIV 2.33G CPU and 1.95G main memory running under the Microsoft Windows 7 operating system. For each test dataset, we repeated experiments for 20 runs. The parameter settings in the UCI datasets experimental study, for GAC, the maximum iterative number is set to 100, the population size is 20, the crossover probability is 0.8, and the mutation probability is 0.1. For FCM, DSKM and EM, the maximum iterative number is set to 100, and the stop threshold is 10−4. Also, the final result is the average of the results from the 20 runs.In the last, the noise immunity and computational time of our method is compared with spectral clustering algorithm, so that the superiority of robustness and computational complexity of our method can be distinctly seen.The two-moon pattern show in Fig. 3(a) is a typical sample set in studies on non-convex clustering algorithm. Two intertwining moons are in this pattern, and there are 400 samples in each of them. If we use the FCM algorithm to cluster, we will get the clustering result given in Fig. 3(b). The result is not good. Fig. 3(c) shows the spectral clustering results of two-moon datasets when the Gaussian kernel parameter σ=0.6. Note that if σ gets a no-proper value, the clustering results can be bad. In this experiment, the spectral clustering results can get the optimum, when σ is from 0.45 to 0.7.When we adopt the FCM algorithm with multi-centers to divide the sample set into 28 subclusters (based on800≈28), the result is shown in Fig. 3(d). The adoption of the lattice similarity can get the original similarity relationship graph of the subclusters, as is shown in Fig. 3(e). Then get the relationship graph of the transitive closure among all the subclusters as is shown in Fig. 3(g) by using Floyd algorithm. In Fig. 3(g) we can see that the relationship graph is notable block symmetric so that the clustering information is more outstanding.According to the graph spectral partitioning theory, the eigenvector corresponding to the second minimal eigenvalue contains the graph partitioning information. In this experiment, the eigenvectors corresponding to the second minimal eigenvalues are chosen to be the spectral features. Fig. 3(f) and (h) corresponds to the spectral features of using original similarity matrix and transitive closure in spectral decomposition. In Fig. 3(f) and (h), the red and black dots represent subclusters’ feature of two different classes; the information on the x-coordinate represents the subclusters’ order, the values on the y-coordinate are the spectral features of the subclusters.As is shown in Fig. 3(f), we can know that the result is not good if we use the spectral features got from the original similarity matrix. The result by spectral clustering which based on the transitive closure is showed in Fig. 3(h). Apply the FCM algorithm again, and those subclusters can be merged correctly based on the spectral features, which are derived from the transitive closure. It illustrates that the transitive closure can reflect the similarity and nearness relations among subclusters in a better way.In order to demonstrate the generalization of our algorithm, we now discuss the case of three-dimensional dataset. In Fig. 4(a), there is a dataset of three twining 3D rings, the red, blue, and black dots represent three different classes. In this dataset, the class 1 contains 500 samples, the class 2 contains 900 samples and the class 3 contains 800 samples.The application of the FCM algorithm divided the dataset into three clusters. The result is poor, as is showed in Fig. 4(b). If the Gaussian kernel parameter σ=1.8, the spectral clustering results is shown in Fig. 4(c). The result is very good. In this experiment, the spectral clustering results can get the optimum, when σ is from 1.75 to 2.4. Fig. 4(d) shows the results of dividing the sample sets into 47 subclusters (based on2200≈47) by FCM algorithm with multi-centers. In Fig. 4(d), the class 1 is partitioned into 18 subclusters, the class 2 is partitioned into 16 subclusters and the class 3 is partitioned into 13 subclusters. The relationship graphs of the original lattice similarity and the transitive closure are shown in Fig. 4(e) and (f).The spectral features by the original lattice similarity matrix and the transitive closure are shown in Table 1. In Table 1, if the original similarity matrix, which is generated by the lattice similarity method, as the Laplacian matrix in spectral decomposition, the spectral features can hardly distinguish the subclusters. Fig. 4(f) shows that the transitive closure modified from the original similarity matrix is notably block symmetric. Its corresponding spectral features showed in Table 1 are good. Apply the FCM algorithm again, subclusters can be merged correctly based on those spectral features. In this experiment, the computational time of spectral clustering is 430.34s, and the computational time of our method is 6.59s. So our method outperforms spectral clustering obviously in terms of computational time.The results are compared with FCM, spectral clustering and MFCM-OLS, and the average results and the standard deviation of the NMI values in 20 runs are shown in Table 2.In order to validate the performance of the MFCM-TCSC algorithm, we select seven lower-dimensional sub-datasets of the UCI datasets, named IRIS, breast, zoo, wine, Pimaindians, balance and page, to study a range of different interesting data properties. The results are compared with FCM, DSKM, GAC, DBSCAN and EM. The average results and the standard deviation of the NMI values in 20 runs are shown in Table 3.From Table 3, we can see clearly that the MFCM-TCSC did best on six out of the seven problems, while FCM did best only on the IRIS dataset. DBSCAN, EM and GAC only obtained the desired clustering for the two spheroid datasets, IRIS and Page. This is because the structure of the other five datasets does not satisfy convex distribution. On the other hand, the MFCM-TCSC can successfully recognize these complex clusters, and DSKM also obtained the true clustering on four problems, iris, zoo, balance and page, which indicates that the manifold distance metrics and the multi-center clustering are very suitable to measure complicated clustering structures. In comparisons between the MFCM-TCSC and DSKM, the MFCM-TCSC obtained the true clustering on wine and breast. Furthermore, for the Pimaindians problems, the MFCM-TCSC did a little better than DSKM in the clustering correct ratio. The main drawback of DSKM is that it has to recalculate the geometrical center of each cluster with the K-means algorithm after cluster assignment, which reduces its ability to reflect global consistency. The MFCM-TCSC avoids this drawback by the multi-centers of each class representatives.From Table 3, we can see clearly that the MFCM-TCSC can get rid of the problems that the FCM algorithm is sensitive to the initial prototypes, and it can handle non-traditional curved clusters.In order to further illustrate our method, we make IRIS dataset classification problems by our method as the typical analysis. IRIS dataset consists of information on 150 iris flowers, 50 each from one of three iris species: Setosa (class 1), Versicolour (class 2), and Virginica (class 3). Each flower is characterized by four numeric attributes, denoted as X1, X2, X3 and X4. By projecting the data onto two dimensional plane, showed in Fig. 5(a) and (b) (the red, blue, and black dots represent three different classes), we can see that there are two large clusters: one consisting of samples of class 1 and the other consisting of samples from class 2 and class 3. The second cluster can further be divided into two subclusters, one composed of samples from class 2 and the other from class 3.The subclusters constructed by the MFCM-TCSC algorithm which has been projected onto X1, X2 and X3, X4 attributes respectively are shown in Fig. 5(a) and (b) (the class 1 is partitioned into subclusters 1st–4th, the class 2 is partitioned into subclusters 5th–8th and the class 3 is partitioned into subclusters 9th–12th; ‘*’, ‘+’, ‘.’, ‘×’ represent the different subcluster). In the MFCM-TCSC, the number of subclusters is 12, because of150≈12. Then we show the spectral features of all subclusters in Table 4. From the Table 4, we can see these three classes. After merging those 12 subclusters using FCM, we get the final clustering result in Fig. 5(c). From the Table 4 and Fig. 5(c), we can see the resulting clusters are fitting the class label very well, although the samples of subcluster 9 are overlapped samples. There is some overlap between two clusters note that our method might have difficulty in clustering highly overlapped clusters, and the percentage of classification error is because the clustering algorithm is not optimized for classification accuracy but rather for clustering soundness.In this experiment, we discuss the real road segmentation by our method. In Fig. 6(a), there is a picture of a real road, which is made up of street, trees and skies. Grey level values of a pixel and its 8-connected neighborhoods are chosen as the features to partition.Fig. 6(b) shows the gray-level histogram of the real road. Fig. 6(c) shows the segmentation result by the gray-level histogram. The gray-level values used to partition are 70 and 135. Black color represents trees, gray is the street, and white is the sky in Fig. 6(c). We can see it from the result that a large part of the street is partitioned as part of the sky, and some trees are partitioned as the street. Fig. 6(d) shows the clustering result by the FCM algorithm. Green color represents trees, black represents the street, and white is the sky in Fig. 6(d). Though the segmentation result is better than the former one, but some part of the street is partitioned as part of the sky. Fig. 6(e) shows the Gaussian mixture models of the road, when the Gaussian number is three. Fig. 6(f) shows the segmentation result by the EM clustering based on Gaussian mixture models. We can see it from the result that the street and the sky are mixed.In Fig. 6(g), the picture is divided into 7 subclusters given by the FCM algorithm with multi-centers, and the different color represents different subcluster. In this experiment, to illustrate our method briefly, we set the number of subcluster to 7. The transitive closure got from the lattice similarity is regarded as the similarity matrix to generate the Laplacian matrix used in spectral decomposition. The eigenvectors corresponding to the second minimal eigenvalues are chosen to be the spectral features. The eigenvectors corresponding to the second minimal eigenvalues and their corresponding colors in Fig. 6(g) are: −0.1858 (green), −0.3724 (blue), 0.4682 (light blue), 0.4682 (red), −0.3724 (pink), −0.3586 (black) and 0.3481 (yellow). Hence, the area in green is a cluster alone, those in blue, pink and black are merged, the left ones in light blue, red, and yellow are merged together. Fig. 6(h) shows the segmentation result given by merging the 7 subclusters into 3 classes, and the green color represents trees, black represents the street, white represents the sky. In spite that a few part of the sky is partitioned as the street, the result is good on the whole.In this experiment, noises are added into the two-moon pattern to illustrate the robustness of our method. Fig. 7(a) shows that some bridging noises connecting the two moons. In Fig. 7(a), red dots and black ones represent samples in two classes, blue dots are noise points. To solve this problem, Wang compares results of different algorithms [22]. In order to show the robustness of our method, 10 bridging noises are added into the two-moons pattern (in Fig. 7(a), there are 400 sample points in each class and 10 noise points). Fig. 7(b) shows the classification result by the FCM algorithm, and the clustering result is poor. Fig. 7(c) shows the classification result by spectral clustering. The Gaussian kernel parameter σ is 0.6, which is the optimum value when there is no noise. We can see it in Fig. 7(c) that two classes can hardly be divided apart, if the Gaussian kernel parameter is 0.6. Experiments show that if the value of Gaussian kernel parameter changes a little, clustering results will be very bad. In other words, the value should be set again if the datasets change a little. For this reason, spectral clustering algorithm is highly sensitive to noise. Fig. 7(d) shows the result of dividing the datasets into 28 subclusters (based on800≈28) by the FCM algorithm with multi-centers. The spectral features by the transitive closure modified from the original lattice similarity matrix are shown in Fig. 7(e). The meanings of dots in Fig. 7(e) are the same with those in Fig. 3(h). After merging those 28 subclusters using FCM, we get the final clustering result in Fig. 7(f). The results show that our method can divide two classes apart well, and has strong robustness.Furthermore, we find that the validity of our method is related to the density of noise points. When the proportion of noise points density to original samples density reaches a certain value, our method will fail. In the two-moon pattern dataset, when the proportion of noise points density to original samples density reaches 50%, our method will fail.In this experiment, we study the computational complexity of our method. In our method, dataset is divided into multiple subclusters by using the FCM algorithm with multi-centers, and the problem of clustering samples is converted to a problem of merging subclusters, the computational load is low.The time complexity of the FCM is O(N·q·c2·i), where N is the number of samples, c is the number of cluster, q is the feature dimension, and i is the number of iteration. The time complexity of transitive closure is O([log2N]+1). The time complexity of the spectral clustering algorithm based on original dataset is O(N3). So our method's time complexity is O(n3)+O(N·q·n2·i)+O([log2n]+1), where n is the number of subclusters. In our method, n is approximate toN, the empirical optimum number of cluster. The larger the N, the larger the n. Then the average number of samples in subcluster is more, so that the accelerating effect of our method gets better.In order to compare the computational times between our method and spectral clustering algorithm, define the proportion of the time of running spectral clustering to the time of running our method as the accelerating rate. This value shows the relative speed of our method to spectral clustering algorithm. Table 5gives the computational times of spectral clustering algorithm and our method as well as the accelerating rate. The values in Table 5 are the average values got after 50 independent runs on each problem and the experimental parameters are the same as Table 2. The accelerating rate in Table 5 shows that when the quantity of samples is small, the computational times are close. With the increase of samples, the time of running spectral clustering algorithm goes up rapidly, while the time of running our method just rises a little bit. When the number of samples gets 800, the time of running our method is much less than the time of running spectral clustering algorithm, and the accelerating rate is 22.6.

@&#CONCLUSIONS@&#
In this paper, a new unsupervised fuzzy clustering algorithm is presented. Since multi-centers are used to represent classes, the nonspherical classes can be correctly detected. Moreover, the proposed clustering algorithm can handle non-traditional, curved classes. The experiment shows that our method can get rid of the problems that the FCM algorithm is sensitive to the initial prototypes and the similarity matrix of spectral clustering algorithm is hard to generate. Compared to spectral clustering algorithm, our method greatly improves the noise immunity and computational speed. However, our method might have difficulty in clustering highly overlapped clusters and noised clusters.There is still much further work for us to be done. In our experiment, the number of subcluster isN, lack of theoretical support, how to set the initial number of subclusters is our next major research issue. Moreover, how to generate a widely used, high validated similarity measure method of the subclusters is another important direction of our study. In this paper, regarding the values of the fuzzy membership as the features of subclusters may cause redundancy features. Especially, if samples distribute unbalanced, samples in dense area contribute a lot to the similarity measure among subclusters. So the representation of the features of subclusters is an issue in the further work. There is not a theory which can solve the selection (including how many and what kind of eigenvectors are needed as spectral features) and the usage of the eigenvectors. This problem also is further work for us to be done.