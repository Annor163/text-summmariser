@&#MAIN-TITLE@&#
Input-based adaptive randomized test case prioritization: A local beam search approach

@&#HIGHLIGHTS@&#
We extend ART with the search-based algorithm for test case prioritization.Our techniques are as effective as the best search-based TCP technique.Our techniques are more efficient than genetic and greedy, but not ART.It is the first experiment to evaluate input-based search-based TCP techniques.

@&#KEYPHRASES@&#
Regression testing,Adaptive test case prioritization,Randomized algorithm,

@&#ABSTRACT@&#
Test case prioritization assigns the execution priorities of the test cases in a given test suite. Many existing test case prioritization techniques assume the full-fledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in real-world software development projects. This paper proposes a novel family of input-based local-beam-search adaptive-randomized techniques. They make adaptive tree-based randomized explorations with a randomized candidate test set strategy to even out the search space explorations among the branches of the exploration trees constructed by the test inputs in the test suite. We report a validation experiment on a suite of four medium-size benchmarks. The results show that our techniques achieve either higher APFD values than or the same mean APFD values as the existing code-coverage-based greedy or search-based prioritization techniques, including Genetic, Greedy and ART, in both our controlled experiment and case study. Our techniques are also significantly more efficient than the Genetic and Greedy, but are less efficient than ART.

@&#INTRODUCTION@&#
Regression testing (Yoo and Harman, 2012) is a widely-practiced activity in real-world software development projects (Onoma et al., 1998), in which a better testing infrastructure has a potential to recover the economic loss resulting from software failures by one third (Tassey, 2002). During a session of a regression test, a changed program P is executed over a regression test suite T. Many companies executed the whole test suite to ensure the quality of their software (Onoma et al., 1998). Moreover, each nightly build of many open-source software projects such as MySQL (MySQL, 2013) and FireFox (FireFox, 2013) always apply the whole test suite to verify the version built.If the time spent to complete the execution of a program over an individual test case is non-trivial, the time cost to execute the whole test suite T may be large (Jiang et al., 2011). For instance, profiling an execution trace of a C/C++ program at the memory access level using a pintool may easily incur tens to one hundred fold of slowdown (Luk et al., 2005). On the other hand, programmers may want to know the test results as early as possible at low cost.Test case prioritization (Elbaum et al., 2002; Wong et al., 1997) is a safe aspect of regression testing. In essence, test case prioritization reorders the test cases in a test suite T and does not discard any test case in T for execution toward a chosen testing goal (denoted by G).A vast majority of existing test case prioritization research studies (Yoo and Harman, 2012) propose to collect data from the executions of a previous version (denoted by Q) of P over a test suite Told to guide the prioritization on T. For ease of presentation, we denote the set of program execution traces of Q over Told by Q(Told) and that of P over T by P(T).Numerous types of such data (such as the fault history (Kim and Porter, 2002), the change history (Elbaum et al., 2004), and the execution profiles (Elbaum et al., 2002)) obtained from these executions of Q have been empirically evaluated in diverse contexts with respect to the differences in their effects on regression testing results towards the selected goal G of regression testing techniques. For instance, a vast majority of empirical studies on test case prioritization validate on how quickly the permutations of T generated by such test case prioritization techniques detect faults in P by assuming that T = Told. A recent trend is to replace the rate of fault detection by the rate of program element coverage (Li et al., 2007) or to incorporate the results of change impact analysis (Li et al., 2013) in their problem or solution formulations. Still, the essential assumption of inferring T based on Told remains unchanged.In this paper, we propose a new family of novel input-based randomized local beam search (LBS) techniques for test case prioritization. This family of techniques targets to be applied in the general (more practical) scenario, where T may be different from Told and Q may be different from P without taking any approximation (i.e., not assuming either T inferable from Told or Q and P similar). Because both Told and Q are irrelevant to this family of techniques, these LBS techniques can be applied in both regression testing and functional testing. In this way, practitioners need not care about whether a testing technique is applicable to functional testing scenarios only or to regression testing scenarios only, or both.Given a test suite T, each LBS technique starts with a set of k partial solutions, each being a sequence of single test case taken from T. For each partial solution S, it randomly selects a number of test cases from T \ S to construct a candidate set C and evaluates each extended partial solution S^{t}, where t ∈ C, according to a chosen distance measure. It marks the overall best k extended partial and randomized solutions as the current set X of partial solutions. It then goes into the next iteration to extend each partial solution in the current set X in the same manner until all test cases in T have been included in each partial solution X. It addresses the search space exploration cost problem by controlling the number of branches in the exploration tree in each round to a small number. Suppose that at each round, both the size of the candidate set and the number of branches to be explored by an LBS technique are k, and the number of distance comparisons between test cases in each node of the tree being explored is capped to be m, and then there are at most mk2|T| comparisons.We have validated our LBS techniques on four medium-sized UNIX utility programs in a controlled experiment setting to evaluate their overall performance. We have further performed a case study on the comparison of our LBS search algorithm to the algorithms of Greedy (Elbaum et al., 2002), ART (Jiang et al., 2009), and Genetic (Li et al., 2007) by encoding test cases using input information and using the even-spread of test cases as the guidance heuristic to determine whether the performance of our techniques is merely attributed to the LBS algorithm. In both validations, we measured their effectiveness in terms of the average rate of fault detection (i.e., APFD (Elbaum et al., 2002)) and the time spent in generating a resultant prioritized test suite.The empirical results from both the controlled experiment and the case study show that LBS achieves either higher mean APFD values than or similar mean APFD values as Greedy, ART, and GA. LBS also is significantly more efficient than GA but less efficient than ART at the 5% significance level. The result further shows that the effectiveness of LBS is not much affected by different parameter values needed to initialize the LBS algorithm. In the case study, we have the following observations: (1) the effectiveness of the studied LBS techniques was mainly contributed by our LBS algorithm, and (2) the use of input information for test case encoding and our heuristic also contributed to the significant reduction of the test case prioritization cost.This work significantly extends its preliminary version (Jiang and Chan, 2013): (1) It generalizes the family of LBS techniques by presenting five more new techniques and evaluates the family against more existing techniques for benchmarking. (2) It reports a new experiment that investigates the impact of candidate set size and beam width on the effectiveness of the family. (3) It presents a new case study on comparing this family with several adapted classical search-based test case prioritization algorithms (Greedy, ART, and Genetic).The main contribution of the paper together with its preliminary version (Jiang and Chan, 2013) is twofold. (1) This paper is the first work that presents a family of novel input-based randomized test case prioritization techniques. (2) It presents the first series of experiments to validate input-based search-based test case prioritization techniques.We organize the rest of paper as follows: we review the preliminaries of this work in Section 2. Then, we describe our family of LBS techniques with their design rationales in Section 3. After that, we present validation experiments in Section 4 and Section 5. Section 6 discusses other issues relevant to our LBS techniques. Finally, we present the related work followed by the conclusion of the paper in Section 7 and Section 8, respectively.Elbaum et al. (2002) described the test case prioritization problem as follows:Given: T, a test suite; PT, the set of permutations of T; g, a goal function from PT to real numbers.Problem: To find T′∈  PT such that ∀T″ ∈  PT, g(T′) ≥ g(T″).In this problem formulation, PT represents a set of all possible prioritizations (orderings) of T and g is a goal function that calculates an award value for that ordering.For a test suite containing N test cases, the size |PT| is N!, which is intractable if N is large. In practice, the set PT in the universal quantification under the above problem formation is replaced by an enumerated subset of PT.Moreover, a goal g, such as the rate of code coverage (Li et al., 2007), can be measurable before the execution of P over the prioritized test suite T′. Such a goal can be used by a search-based optimization technique for test case prioritization.There are however other types of goals, such as the rate of fault detection (Elbaum et al., 2002), which cannot be measured directly before the execution of the test cases. A recent attempt is to use a heuristic (e.g., code coverage, even spreading of test cases) to make an approximation. Our techniques try to spread the test cases in T as evenly as possible within the input domain in each iteration using a randomized local beam search approach.In this section, we revisit the assumptions made in the typical test case prioritization research work.In general, T may be different from Told and P may be different from Q. The dataset or execution profile of Q(Told) is also unlikely to be the same as these of Q(T), P(Told), or P(T). Moreover, if either a test case reduction/selection technique (Do et al., 2008; Yoo and Harman, 2012) or an impact analysis technique (Li et al., 2013) has been applied on Told to construct a proper subset Told’of Told and the test cases in Told\Told’have not been executed by Q, we have Q(Told’) ⊂ Q(Told). In this connection, if a test case prioritization technique relies on Q(Told’) in prioritizing test cases in T, it is a threat.We further categorize our observations on the limitations due to such assumptions into five classes:First, assuming the historic data of Told always available is restrictive. For instances, the execution profile data are seldom maintained in the repositories of real-world software development projects such as MySQL (MySQL, 2013), Eclipse (Eclipse, 2013), and FireFox (FireFox, 2013). In many projects, such as numerous Android applications (e.g., Foursquared (Foursquared, 2012)) available in Google Code (Google Code, 2013), their bug repositories only keep few bug reports.One way to alleviate this problem is to run T on an older version Q. However, the correctness criteria (e.g., the assertion statements in JUnit test cases) may require manual determination. Both the executions of the test cases and the determination of their correctness lead to non-trivial overheads.Collecting code coverage data requires profiling program executions, which may be impractical in some industrial environments. For instance, in safety-critical software like avionics control (where enumerators cannot support the execution of the whole test suite), there are many pieces of timing-related interrupt handling code. The profiling overhead may lead to unintended timeout exceptions.Second, there are newly defined test cases in T (i.e., test cases in T\Told). Assuming that Q is able to execute each of such test cases is unrealistic as well. The program P is a changed version of Q. It is quite likely that some new test cases (for new features) have been added to the regression test suite. Nonetheless, it is unlikely that such a test case has been run over the older version Q of the program.Third, there are revisions of test cases that are common to both T and Told. Testers need to map between these test cases, which in general is a many-to-many mapping.Fourth, the implementation gap between P and Q can be non-trivial. Take GNU flex (Flex, 2013) as an example. More than 12% (1731 lines) of the source code of flex has been changed from version 2.5.1 to version 2.5.2, and we found that many of the test cases applicable to both versions have changed their coverage statistics between the two versions.Last, in cloud computing, web services merely expose their interface information and keep their implementation (including source code) inaccessible. Conducting third party testing (e.g., through an independent service certification agency) or in-field regression testing is challenging.The former two classes are due to the unavailability of a prior execution data. The next two classes make the execution data extracted from Q(Told’) or Q(Told) unsound for the regression testing on P. The fifth class motivates us further to exploit input information to prioritize test cases.As such, suppose that a controlled experiment on a test case prioritization technique M only evaluates the extent of M affected in the scenarios of either T = Told ∧ Q = P or its close approximation (i.e., T ≈ Told ∧ T ⊆ Told ∧ Q ≈ P, for some similarity judgment criterion ≈). Such a controlled experiment incurs significant threats to internal and external validities because the crucial element of realism in the study has been significantly compromised.To circumvent the limitations associated with using the execution profile data extracted from Q(Told), a prioritization technique may use the static information (data) associated with the program P or the test suite T to guide the prioritization process. Such static information can be either static program specification or information gained from static analysis. Precise program specifications of P are rarely available in real-world projects. Besides, using such information requires an accurate mapping from individual test cases to such information, which may not be sound.Another way to alleviate the problem is to apply static analysis on P. Since static analysis does not involve any actual execution of the program, they often only provide conservative over-approximations of all possible execution traces of each test case. Nonetheless, if both |T| and P are large in scale or involve concurrent components, static analysis techniques often fail to scale up to identify precise test case coverage information. Applying static analysis techniques to regression testing however still has a clear merit because P(T) is only conservatively approximated by its own static version instead of approximated by Q or Q(Told). If we sacrifice precision for efficiency, test case prioritization may be misled by the false positive issues in the approximated coverage data.Impact analysis (Lahiri et al., 2010; Li et al., 2013; Rovegard et al., 2008) has a potential to precisely identify the change in between P and Q. Such impact analyses may be safe under certain conditions (Rothermel and Harrold, 1997). Adding a change impact analysis to figure out the difference between P and Q is still unable to eliminate the discrepancy between their corresponding execution profiles.The test suite T alone contains much information. Suppose that the details (including the comments) of T are well-maintained. In this case, linguistic data from each test case (e.g., comment, string literal) in T can be extracted to guide the prioritization process that maximizes the average distance from the already-prioritized test cases (Thomas et al., 2014). The distribution of real world coordinates of points-of-interest in the expected outputs and inputs of test cases in T have been proposed to prioritize test cases in T (Zhai et al., 2014). It does not rely on the mapping information between the test case and program version. However, such techniques are inapplicable to prioritize non-coordinate based test cases.Random ordering (Arcuri et al., 2012; Elbaum et al., 2002) is a strategy that can perfectly be applied in the general scenario, but is usually ineffective.Adapting existing code-coverage-based techniques to use test input information requires a new formulation of what to be covered and the notion of coverage equivalence. These techniques also require evaluations to validate their effectiveness. This paper contributes to all of these aspects.Manual approaches to test case prioritization are flexible but tedious to repeat precisely. Some test cases may be selected and run first based on their associated risks, which are often a managerial aspect of test case prioritization. For brevity, we do not discuss them further in this paper.This section revisits the test case prioritization techniques evaluated in our controlled experiment and case study.The Greedy algorithms studied in our experiment include the total statement technique and the additional statement technique (Elbaum et al., 2002). The total statement (total-st) test case prioritization technique sorts test cases in descending order of the total number of statements covered by each test case. In the case of a tie, it selects the involved test cases randomly. The additional statement (addtl-st) prioritization technique selects, in turn, the next test case that covers the maximum number of statements not yet covered in the previous round. When no remaining test case can improve statement coverage, the technique will reset all the statements to “not yet covered” and reapply addtl-st on the remaining test cases. When more than one test case covers the same number of statements not yet covered, it selects one of them randomly.The 2-Optimal algorithm (2-Opti) (Lin, 1965; Skiena, 1998) is another typical greedy algorithm for test case prioritization. Similar to the additional greedy algorithm, it iteratively selects test case(s) to maximally cover those not yet covered statements in the previous round. Different from additional greedy, it selects two best test cases maximizing additional coverage rather than one in each round. When the complete coverage has been achieved, 2-Opti resets the coverage.We evaluated the steepest ascent hill climbing algorithm (Hill) (Li et al., 2007) for test case prioritization. The algorithm first randomly constructs a test suite permutation to make its current state. Then it evaluates neighbors of the current state. A neighbor of the current state is another test suite permutation constructed by exchanging the position of the first test case with any test case in the current permutation. It then moves to the neighbor with largest increase in fitness from the current state (no move if no neighbor has a larger fitness value). Then, the above movement is repeated until there is no state with higher fitness value to go to. The test suite permutation of the final state is returned as the prioritization result. In our experiment, we follow the algorithm presented in (Li et al., 2007) to implement the technique.Genetic Algorithm (GA) (Holland, 1975) is a class of adaptive search techniques based on the processes of natural genetic selection. It first randomly generates a set of permutation as the initial population. Then individual permutation of the initial population is evaluated. Finally, pairs of selected individuals are combined and mutated to generate new permutations as the new population. The process repeats until the maximally defined number of iteration is reached.The implementation of the GA can vary a lot, depending on the choice of test suite encoding, fitness function, crossover strategy, mutation strategy, population size, and termination condition. In the evaluation of this work, we follow the implementation consideration presented in (Li et al., 2007). Specifically, we use the position of test cases in the permutation as the encoding. The fitness function is the Baker's linear ranking algorithm. The crossover strategy is to divide the parent encoding into two parts at a random position, and then exchange the two parts of two parent encodings to generate two offspring encoding. The mutation strategy used is to exchange two positions randomly within the sequence. Finally, the population size, the termination condition (i.e., the number of generations), the crossover probability, and the mutation probability are set as 100, 100, 0.8, and 0.1, respectively.In this section, we present our family of input-based test case prioritization techniques.On one hand, our techniques use the relative lightweight input information instead of code-coverage information for test case prioritization. They should be more efficient. On the other hand, because our generalized local beam search strategy keeps a beam width of k to search for the best partial solutions instead of merely one possibility at any one round during the search process, it incurs additional time cost.Thus, unlike the greedy techniques (Elbaum et al., 2002) that select the successors among all not yet covered test cases, our LBS techniques are designed to save time by randomly sampling a candidate set of c successors from all possible successors for each state as Jiang et al. Jiang et al. (2011) did. Moreover, it only selects the best k (where k ≤ c) successors of each state for further exploration. To simplify our presentation, we use k to refer to both the beam width and the number of successors expanded for each state. In general, it can be configured as two independent parameters. Because the total number of successors (i.e., the number of remaining test cases to be prioritized) s must decrease while the subsequence of test case (i.e., a state of a local beam search) is being constructed, we use the minimum between s and c (denoted by min(s,c)) as the size of candidate set. Note that this strategy has not been developed by previous work (Carlson et al., 2011; Jiang et al., 2011). Similarly, we use min(s,k) as the number of successors selected for each state (i.e., the number of branches at each expansion step).Our techniques have also included a new design on the candidate set usage to make it more “diverse”. Specifically, each of our techniques prevents each unselected successor (i.e., test case) in a previous candidate set from including into the current candidate set until all test cases have entered the candidate set at least once. Then, it will ‘reset’ this inclusion label so that each remaining test case can be selected and included in a candidate set.Our insight on the above candidate set design is that the distribution of test cases in the inputs among the available regression test suites may not be even enough. Thus, test cases from a denser region will have higher chances to be selected into a candidate set than test cases from a sparser region. To the best of our knowledge, no adaptive random testing techniques for test case generation and test case prioritization have the above characteristics.We aim to allocate the regression test cases across the space rendered by a regression test suite as evenly as possible. We select from a successor pool those successors who are farthest away from those already prioritized test cases in each search step. Chen et al. (2004) has shown that this simple strategy only ensures the test cases to be far away from each other, but cannot ensure the input domain to have the same density of test cases.In statistics, Discrepancy measures whether different regions of a domain have equal density of sample points. The smaller the Discrepancy value, the more evenly spread are the set of test cases. Thus in our techniques, we use Discrepancy to select the final best k successors from the successor pool in each round. In this way, the selected successors can be both faraway from each other and distributed within the input domain with equal density.Discrepancy in Chen et al. (2007) is defined as follows: given domain D and D1, D2, …, Dmdonate m rectangular sub-domains of D whose location and size are randomly defined; and given E represents the set of all test cases and Eiare the sets of test cases whose input is within domain Diwhere 1≤i≤m. A further generalization of discrepancy for a sequence is feasible and we leave it as a future work.Discrepancy(E)=max1≤i≤m||Ei||E|−|Di||D||Table 1shows our LBS algorithm entitled prioritize. It accepts a set of unordered test cases T and generates a sequence of prioritized test cases P. It first randomly selects k test cases from T as the initial k subsequences and puts them into an ordered set S (line 4). Then it enters a loop to select one more test case in each round until all the test cases have been prioritized (lines 5 to 22).In each round of iteration, the algorithm tries to find the best k successors by selecting one more test case (lines 6 to 21). It first determines ci, which stands for the size of the candidate set for Si(line 7), and randomly selects citest cases that have not been selected yet to form the candidate set C (line 8).After that, it calculates the distance matrix D between test cases in the candidate set and the already-prioritized test cases (lines 9–12). Then it sorts the candidate test cases in descending order of their distances from the set of prioritized test cases as defined by function f2 (lines 13–16). Next, it selects the first kicandidate test cases, appends each of them to Sito form kibest successors, and puts them into the successor's pool A (lines 17–19). When all the successors of S have been selected, it selects the best k successors from the pool A as the new S in ascending order of their discrepancy values (line 21). Finally, if all the test cases have been prioritized, it selects a sequence of test cases with the smallest discrepancy value from S (lines 23 and 24).In this algorithm, the function f1determines the distance between the inputs of two test cases, which is best determined by the input structure and semantics of the application under test. For example, when the applications under test are numerical applications, we can use the Euclidean distance to measure the inputs. When testing command line applications like sed, we can use the string edit distance (Gusfield, 1997) to measure their distance.In the algorithm, the function f2measures the distance between a candidate test case Cnand the set of already prioritized test cases Si. In this work, we extend its conference version to propose the family of generalized LBS techniques by extending the definition of f2, which we will discuss in the following section.f2(D,Si,Cn)={min0≤m≤|Si|dmn(1)(seeChenetal.[5]),LBS100avg0≤m≤|Si|dmn(2)(seeCiupaetal.[9]),LBS010max0≤m≤|Si|dmn(3)(seeJiangetat.[16]),LBS001random(min0≤m≤|Si|dmn,avg0≤m≤|Si|dmn)(4),LBS110random(min0≤m≤|Si|dmn,max0≤m≤|Si|dmn)(5),LBS101random(avg0≤m≤|Si|dmn,max0≤m≤|Si|dmn)(6),LBS011random(min0≤m≤|Si|dmn,avg0≤m≤|Si|dmn,max0≤m≤|Si|dmn)(7),LBS111The definition of test set distance is shown in the equation below. As mentioned in the last section, we generalize the LBS techniques by extending the definition of test set distance f2in the conference version. In the conference version, f2is defined as the minimum (Eq. (1)) or average (Eq. (2)) or maximum (Eq. (3)) distances between the candidate test case and the already selected test cases across all beam steps. However, fixing the test set distance to one definition across all beam steps may also limit the evenly spread of the test cases. The most flexible test set distance definition is to randomly select one of the three test set distances in each beam step.Suppose that we use the encoding 〈min, avg, max〉 to represent the test set distance in a beam step, where “min”, “avg”, or “max” is a Boolean variable. A true value (i.e., 1) means the corresponding test set distance can be selected in the beam step. Similarly, a false value (i.e., 0) means that the corresponding test set distance cannot be selected in the beam step. Thus, the strategy of randomly selecting one of the three test set distances in a beam step can be encoded as 〈1,1,1〉 (which forms the technique LBS111 using Eq. (7)). Similarly, randomly selecting one test set distance among “avg” and “max” only is encoded as 〈0,1,1〉 (which forms the technique LBS011 using Eq. (6)). The other LBS techniques shown in the equations can be interpreted similarly. So, in general, there are seven LBS techniques: LBS100, LBS010, LBS001, LBS110, LBS101, LBS011, and LBS111.The most closely related work of LBS is the adaptive random test case prioritization (ART) (Jiang et al., 2009). We recall that adaptive random testing (Chen et al., 2004) is a test case generation technique that makes use of the input information, spreading test cases as evenly as possible across the input domain. It aims at using a fewer amount of test cases to expose the first failure (in terms of F-measure) compared to the pure random testing (RT). If the extra time cost of adaptive random testing for test case generation is neither negligible (Arcuri and Briand, 2011; Chen et al., 2004) nor controllable, then RT may be able to reveal the first failure of each fault faster (in terms of the total amount of time spent so far) than adaptive random testing for test case generation (Chen et al., 2004).Hence, some of the key issues in formulating a successful class of ART techniques include (1) controlling the extra time cost and (2) retaining a high effectiveness in a real-world development setting. The work of Jiang et al. (Jiang et al., 2011) contributes to extend adaptive random testing for test case generation to the domain of test case prioritization for regression testing, where the test cases have been generated and the size of a test suite is often limited. Both factors help to control the extra cost of ART mentioned above. Their techniques have shown to be effective. The prioritization costs of their techniques are significantly lower than that of the additional series of test case prioritization techniques. However, shared with other code coverage-based techniques, these techniques require execution profiling, which incurs additional prioritization costs.LBS can be regarded as a novel class of ART (Jiang et al., 2011). Our paper contributes to lower the cost of ART for test case prioritization to prioritize test cases based on the information in the test inputs of the test cases, and still LBS is highly effective. Usually, the amount of data from execution profile of a test case is significantly more than the amount of data in the input of the same test case. Hence, it is more challenging for LBS to effectively use the more limited amount of data for effective test case prioritization. We make this claim by our controlled experiment to be presented in Section 4. For instance, in the test case generation domain, adaptive random testing is observed to be, on average, 11% more effective than RT. As we are going to present in the evaluation section of this paper, the LBS techniques consistently achieve higher than 90% in terms of APFD, and random ordering varies from 58 to 81%.To the best of our knowledge, this work has ironed out the first set of effective input-based ART for test case prioritization. We are also unaware of existing works that have applied other search-based algorithms to input-based test case prioritization.Moreover, ART only constructs one subsequence of test cases at any time, whereas, LBS makes a tree-based search over the input space rendered by each regression test suite. Intuitively, while all other factors equal, LBS is likely to be more effective but less efficient than ART in exploring the solution space.In this section, we report a controlled experiment evaluating the effectiveness and efficiency of the LBS techniques.We study three critical research questions as follows:RQ1: Is the family of input-based LBS techniques effective?RQ2: Is the family of input-based LBS techniques efficient?RQ3: Will the size of the candidate set and beam width impact on the effectiveness of input-based LBS techniques?The answer to RQ1 clarifies whether the input-based LBS techniques can increase the rate of fault detection in regression testing. Furthermore, it enables us to examine whether LBS techniques using an atomic test set distance (i.e., LBS100, LBS010, and LBS001) share the same effectiveness with the LBS techniques using a composite of two test set distance measures (i.e., LBS110, LBS011, and LBS101) or more (LBS111). The answer to RQ1 will also tell us the relative effectiveness of LBS with respect to classical code-coverage-based search-based techniques for test case prioritization.The answer to RQ2 validates whether the prioritization cost of LBS techniques is low. It also reveals whether LBS techniques using composite test set distances may be different significantly from LBS techniques using atomic test set distance in terms of efficiency. The study will also tell us about the relative efficiency of LBS techniques as compared with classical code-coverage-based search-based algorithms.The answer to RQ3 explores the design space and guides the testers on the direction on selecting a more appropriate candidate set size and beam width.In this section, we present the setup of our experiment.Our platform to conduct our experiment was a Dell PowerEdge M520 server running Ubuntu Linux. The server was configured with a Xeon E5-2400 (16 cores) processor with 12GB physical memory.For Subject Programs and Test Suites, we used a suite of four real-world UNIX benchmarks (downloaded from http://sir.unl.edu) in our evaluation. Table 2shows their descriptive statistics. Following (Elbaum et al., 2002), we used all the faulty versions whose faults can be revealed by more than 0% and less than 20% of the test cases.For the number of runs per test case prioritization technique, according to (Arcuri and Briand, 2011), 1000 runs per subject are sufficient to evaluate a randomized technique on the subject. Following (Elbaum et al., 2002), we generated 1000 test suites iteratively from the test pool such that each test suite can cover all branches in the subject at least once. To reduce the huge computation cost in the experiment, we randomly selected 50 suites from all the available 1000 test suites for each of the UNIX programs. Since both the LBS techniques and random were nondeterministic due to the impact of random seed, we set the number of repeated trials of each technique over each test suite to be 50.Each of our subject programs accepts command line and file as inputs. We extracted the command line and file contents as strings, and use the edit distance (Gusfield, 1997) as function f1.The first independent variable we studied was the test case prioritization (TCP) technique. We compared the family of LBS test case prioritization techniques with random and several classical search-based (total statement, additional statement, 2-Optimal, Hill Climbing, and Genetic Algorithm) techniques for test case prioritization as shown in Table 3. We selected them because these are representative search-based techniques for test case prioritization and they are evaluated in the experiment reported by Li et al. (2007). Our implementation of total statement and additional statement follows strictly according to the algorithms described in Elbaum et al. (2002). The same implementations for the greedy algorithms have been used to report the findings presented in Jiang et al. (2009).We did not compare with those black-box input-based test case prioritization techniques such as Ledru et al. (2011) and Thomas et al. (2014) because of two reasons. The first reason was that the empirical studies in those work focused on the comparison of different test case distances while ignoring the comparison of test case prioritization effectiveness. In our controlled experiment, we aimed to compare our LBS techniques with classical search-based techniques directly. The second reason is that we aimed at systematically exploring the factor of test set distance, beam width, and candidate set size, which already made the scale of the experiment huge.The second independent variable was test set distance. Specifically, we aimed to compare whether using a fixed distance measure (LBS100, LBS010, and LBS001) shared the same effectiveness and efficiency as using a mixed distance measure (o LBS110, LBS011, and LBS101). We also examined a follow-up question to study whether using a composite distance measure increases the effectiveness further (i.e., LBS111 versus others).We had also evaluated all combinations of the candidate set sizec and beam widthk systematically where c ∈{10, 20, 30, 40, 50} and k ∈{3, 4, 5, …, 20}. We aim at examining whether the choice of specific value combinations of c and k may affect the same technique (i.e., holding the value of other independent variables the same) significantly. For these 90 combinations of k and c, we performed 225,000 prioritizations for each LBS technique on each subject, resulting in 6.3-million permutations of test suite in total to study RQ1 and RQ3.For RQ2, we randomly selected x%, where x ∈{10, 20, 30, 40, 50, 60 70, 80, 90, 100} of the whole test pool of each subject as a test suite. We applied each such test suite to each technique and measure the time cost (see the next section). We repeated this procedure 1000 times.We usedAPFDto measure the rate of fault detection (Elbaum et al., 2002). APFD is the weighted average of the percentage of faults detected over the life of the suite. It is widely used in previous regression testing experiments: let T be a test suite containing n test cases and let F be a set of m faults revealed by T. Let TFibe the first test case in the prioritized test suite T’ of T that reveals fault i. The APFD value for T’ is given by the equation:APFD=1−TF1+TF2+⋯+TFmnm+12nFinally, to measure the efficiency of different test case prioritization techniques, we used the time cost for test case prioritization (in seconds).

@&#CONCLUSIONS@&#
