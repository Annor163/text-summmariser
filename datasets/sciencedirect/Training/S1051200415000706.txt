@&#MAIN-TITLE@&#
An unconstrained optimization approach to empirical mode decomposition

@&#HIGHLIGHTS@&#
A new simple unconstrained optimization-based approach to EMD is introduced.The new method provides an analytical and easily implemented closed solution.Unlike other proposals, computational cost of the present one is similar to EMD's.The number of parameters to be tuned has been reduced to only one.The use of explicit spline interpolations is avoided.

@&#KEYPHRASES@&#
Empirical mode decomposition (EMD),Convex optimization,Time frequency,Data-driven methods,

@&#ABSTRACT@&#
Empirical mode decomposition (EMD) is an adaptive (data-driven) method to decompose non-linear and non-stationary signals into AM-FM components. Despite its well-known usefulness, one of the major EMD drawbacks is its lack of mathematical foundation, being defined as an algorithm output. In this paper we present an alternative formulation for the EMD method, based on unconstrained optimization. Unlike previous optimization-based efforts, our approach is simple, with an analytic solution, and its algorithm can be easily implemented. By making no explicit use of envelopes to find the local mean, possible inherent problems of the original EMD formulation (such as the under- and overshoot) are avoided. Classical EMD experiments with artificial signals overlapped in both time and frequency are revisited, and comparisons with other optimization-based approaches to EMD are made, showing advantages for our proposal both in recovering known components and computational times. A voice signal is decomposed by our method evidencing some advantages in comparison with traditional EMD and noise-assisted versions. The new method here introduced catches most flavors of the original EMD but with a more solid mathematical framework, which could lead to explore analytical properties of this technique.

@&#INTRODUCTION@&#
Empirical Mode Decomposition (EMD) [1] is an adaptive method introduced to analyze non-linear and non-stationary signals. It consists in a local and fully data-driven separation of a signal in fast and slow oscillations. At the end of the decomposition, the original signal can be expressed as a sum of amplitude and frequency modulated (AM–FM) functions called intrinsic mode functions (IMFs) plus a final trend either monotonic or constant. However, EMD experiences some problems, such as the presence of oscillations of very disparate amplitude in a mode, or the presence of very similar oscillations in different modes, named as mode mixing (an interesting strategy to alleviate noise-related mode mixing can be found in [2]). Besides this issue, one of its major drawbacks is the lack of mathematical framework, being defined as an algorithm output.Several efforts have been made in order to provide some mathematical foundations for EMD. Deléchelle et al. [3] estimated the envelopes by solving a parabolic differential equation. Xu et al. [4] modified the envelope definition to obtain one with a simple analytical expression, by which the variations of the extrema in the iterative procedure are investigated in detail to reveal the nature of the sifting process. Hawley et al. [5] replaced the cubic spline interpolations for trigonometric interpolations when estimating the envelopes. Thanks to that, they derive some interesting properties and convergence guarantees, although the results significantly differ from those of classical EMD. Daubechies et al. [6,7] compared EMD with wavelet theory by using a special case of reassignment called synchrosqueezing.A different approach, based on optimization theory, has recently aroused the interest of the EMD scientific community. B. Huang and Kunoth [8] replaced the explicit interpolation through extrema for solving an optimization problem to estimate the envelopes. However, they keep an envelope-related approach. No explicit envelope is estimated by Oberlin et al. in [9]. They search for a local mean in a specific B-spline space subject to some symmetry constraints on the amplitude of the modes. In a similar way, Pustelnik et al. [10,11] minimized the difference between a signal and its local mean plus mode subject to smoothness, symmetry and quasiorthogonality requirements, based on a multicomponent variational analysis.In this paper, we propose a new approach for EMD based on optimization, with the goal of mimic EMD. Unlike those above mentioned, here we present an unconstrained optimization problem, with a unique analytic solution. This results in the following benefits:•This approach may help to better understand some properties of EMD.The explicit computation of envelopes to find the local mean is not needed, in contrast to algorithm-based EMD.The proposed method provides an analytical and easily implemented closed solution, unlike previous optimization-based efforts that need iterative algorithms to solve the optimization problem.The use of explicit spline interpolations is avoided.The number of parameters to be tuned has been reduced to only one, in contrast to other optimization-based proposals where several parameters are needed.The computational cost is similar to the cost of EMD. On the contrary, other optimization-based methods are tens of times slower than EMD.The paper is organized as follows. We recall the basic principles of EMD in Section 2. In Section 3 we present our new unconstrained optimization approach to EMD. Experiments and results with both artificial and real signals are introduced and discussed in Section 5. Conclusions are presented in Section 6.The main idea on EMD is to iteratively subtract the local mean from a signal (or residue) to obtain the zero local mean AM–FM components called intrinsic mode functions or simply modes. From this perspective, the slow oscillation is considered the local mean (trend) while the mode is the fast one. If x is the signal to be decomposed, the EMD algorithm can be summarized as follows [1]:1.Setk=0and find all extrema ofr0=x.Interpolate between minima (maxima) ofrkto obtain the lower (upper) envelopeemin(emax).Compute the mean envelopem=(emin+emax)/2.Compute the IMF candidatedk+1=rk−m.Isdk+1an IMF?•Yes. Savedk+1, compute the residuerk+1=x−∑i=1k+1di,k=k+1, and treatrkas input data in step 2.No. Treatdk+1as input data in step 2.Continue until the final residuerKsatisfies some predefined stopping criterion.At the end, the signal x can be expressed as(1)x=∑i=1Kdk+rK,where each modedkadmits well-behaved Hilbert transforms. The refinement process carried out to ensure that the modedkis actually an IMF is the so-called sifting process. Further details can be found in [1].The symmetry of the modes' envelopes resides on the IMF definition. To be considered an IMF, a function must fulfill two conditions: (i) the number of extrema and zero crossings are equal or differ at most by one; and (ii) the mean between the upper and lower envelope is zero for all the signal duration.Some of the main characteristics of the EMD are its multiscale and local nature. The local scale is defined as the interval between successive extrema. The number of extrema of the modes decreases as k increases. Although one may give “spectral” interpretation of the modes, it must be emphasized that this applies only locally. The automatic selection of the local highest frequency content cannot be achieved by a predetermined subband filtering; it rather corresponds to an adaptive (data-dependent) time-variant filtering [12]. When decomposing fractional Gaussian noise (fGn), EMD acts on average as a dyadic filter bank [12,13].Notice that, in the original EMD algorithm, the local mean is defined as the mean of the envelopes, which are obtained by interpolating through local extrema (usually with cubic splines). Therefore, from the second mode onwards, all of them are sums of splines. We must get rid of the envelopes, so we propose here a different approach to obtain the local mean. Previous efforts have focused their attention on the smoothness of the local mean [14] (even restricting their search to a spline subspace [9]). In those approaches, the IMF-likeness of the modes is not considered on the objective function but in the form of inequality constraints, where the corresponding bounds have to be set. However, the IMF conditions are the heart of EMD and the sifting process is carried out until the mode is close enough to an IMF, so the original signal is the sum of IMFs plus a final trend. For this reason, our proposal focuses on the IMF-likeness of the modes.Let us return to the IMF definition in Section 2. It is clear that condition (ii) cannot be satisfied without fulfilling condition (i), so it is enough to pursuit condition (ii). We consider this issue in a similar fashion to Oberlin et al. [9] and Pustelnik et al. [10]. Lettk[l],1≤l≤L, with L the number of local extrema, be the locations of the local extrema of the signal (or residue) under study. If we consider these points as estimations of the local extrema locations of the k-th mode (dk), for2≤l≤L−1, we can define the inner product(2)ptk[l]kdk=dk(tk[l])+dk(tk[l+1])Δl−+dk(tk[l−1])Δl+Δl++Δl−,whereΔl+=tk[l+1]−tk[l],Δl−=tk[l]−tk[l−1],dkis a column vector andptk[l]kis thetk[l]-th row of a matrixPk. Then, the only non-zero elements of thetk[l]-th row ofPkare(3a)Pk(tk[l],tk[l])=1,(3b)Pk(tk[l],tk[l−1])=Δl+Δl++Δl−,(3c)Pk(tk[l],tk[l+1])=Δl−Δl++Δl−.It should be emphasized the fact that matrixPkhas as many rows as the length ofdk. Rows not involved in (2) are zeros, because there are no local extrema at that positions. The goal of (2) is to compare the signal (or residue) at each extrema with the corresponding linear interpolation between its two adjacent extrema. Smaller values of (2) would mean thatdklocally (aroundtk[l]) better satisfies the IMF condition (ii). The minimization of||Pkdk||2would contribute, at least globally, to the fulfillment of the IMF conditions. As it was pointed out by Pustelnik et al., matrix P is a “...linear operator which models the penalization imposed on d at each locationtk[l]”[10]. It must be noticed that, in this approach, the IMF conditions are not evaluated over the whole time span of the signal but only on its local extrema.Given a signal x, we can define our quest for a first mode by searching for a local mean (or approximation) a which minimizes||P1(x−a)||2. A solution to this problem isx=a, of course undesirable. However, this solution is not unique since P may be singular. Because of that, we must regularize our problem. To favor smooth solutions, we choose a second-order Tikhonov regularization [15]. With this in mind, we can formulate the following optimization problem:(P)mina⁡||P(x−a)||2+λ||Da||2︸f(a),with its unique solution at(4)∇af=−2PTP(x−a)+2λDTDa=0⇒⇒PTPa+λDTDa=PTPxa⁎=(PTP+λDTD)−1PTPx,where x is the signal, a the estimated local mean, D the second-order difference matrix,||⋅||is the Euclidean norm andλ>0the regularization parameter used to control the trade-off between the smoothness of a and the IMF-likeness of the moded=x−a.Using problem (P) and its closed solution given by (4) we can formulate the following algorithm:In the above algorithm we perform only one “sifting” iteration (steps 5 to 7 for a fixed k) for each mode extraction. However, it is possible to use more iterations.MatrixPkmakes this method adaptive (data-driven) and local, with local extrema as characteristic points. A comment regarding the regularization parameter must be done. As λ tends to infinity, the approximation a becomes affine. However, according to our studies, the here presented method behaves robust to the choice of λ. In order to study its influence, in our first experiment, in Section 5.1, we will explore different values for this parameter. Thereafter, despite being in very different situations, we will always use the same valueλ=1. Objective methods to choose the regularization parameter, such as the L-curve method or Morozov's discrepancy principle could be employed, but such options involve an expensive computational cost.Regularization process is crucial for the problem to be solvable. Since matrix P (we drop the index k for the sake of simplicity) may be not invertible, as it occurs in most cases, it is essential to add the penalization term to the optimization problem. Because of that, a unique solution is achieved since symmetric matrixPTP+λDTDis positive definite:(6)yT(PTP+λDTD)y=yTPTPy+λyTDTDy=(Py)TPy+λ(Dy)TDy>0,∀y≠0,due toN(P)∩N(D)={0}:Dy=0only for linear functions y, for which P is null (no local extrema) and it would make no sense to even formulate problem (P). Therefore it is only necessary for λ to be positive. However, a very small value of λ (i.e.,λ→0) will lead to a large condition number, and results might be inaccurate.In order to efficiently solve the matrix inversion involved in (5c), we must explore the particular structure of matrixM1. For a signal of length N, matrixDTDhas5N−6nonzero elements and matrixPTPhas 5L entries which are nonzero, but L of those elements are on the main diagonal. Then, the nonzero elements of matrixPTP+λDTDare(7)5N−6+4L≤9N−6<9N,hence the proportion of nonzero elements is bounded by9/N, which means that this matrix is sparse. Another important feature for this matrix is being band diagonal, with a matrix bandwidth ofbw=maxl⁡(t[l]−t[l+2])[16]. Sparsity of matrices P and D is exploited in the implementation and a significant amount of computational time is saved.Let us take a deeper look at (5). Since symmetric matrixM2=(Pk)TPkhas non-zero columns only on the extrema locations (tk[l]-th columns), the productΦ=M1−1M2has exactly L non-zero columns. Because of that, we can write(8)ak=Φak−1=∑l=1Lϕtk[l]ak−1(tk[l]),whereϕtk[l]stands for thetk[l]-th column of Φ andak−1(tk[l])is the signal (or residue) evaluated at its local extrema. Therefore, the local meanakis in the subspace created by the non-zero columns of Φ, and the values of the signal at its local extrema are the coefficients. This “sparse” representation ofakis consistent with the original formulation for EMD, where the local mean only depends on the locations of the local extrema and the values of the signal at those points.Every method that uses local extrema of the signal to estimate those of the extracted component may need some form of sifting. If some of the mode extrema are hidden (they do not appear in the composed signal), these can be revealed after one round of sifting [17,18]. Then, the proto-IMF would have more extrema than the original signal, and matrix P (constructed from the extrema of the original signal) would not be able to model the penalization on it. A new matrix P would need to be constructed, and a new trend extraction would be performed. In our particular case, sifting may also help the method to work properly.Two different strategies for stopping the sifting process (steps 5 to 7 for a fixed k) will be used in this work. Such as in traditional EMD, one may use a fixed number of sifting iterations [19,20]. However, it may be also interesting to use a variable number of iterations, especially for real signals. Then, an objective stopping criterion is needed. Since no envelope is computed, it is not possible to apply a local/global criterion, like the one proposed in [21], to check IMF condition (ii). Nevertheless, an energy-based criterion could be used to decide whether or not the modedkhas zero mean. In those cases, following [20], we use(9)z=||aˆk(i+1)||2||dk(i)||2,whereaˆk(i+1)stands for the local mean computed from the i-th candidate for the k-th modedk(i)(the proto-IMF after i iterations of sifting), with the recommended value ofz=10−4[20]. A Matlab toolbox for the proposed method is available on our website (www.bioingenieria.edu.ar/grupos/ldnlys).We will call our method UOA-EMD in what follows.

@&#CONCLUSIONS@&#
We have proposed an alternative formulation for EMD. Our approach is based on an unconstrained problem, with a unique analytic solution and only one parameter is needed (λ) in the problem statement. An additional benefit for the user, is that the value of λ does not need to be perfectly fit because the results can be refined by sifting, althoughλ=1seems to be a reasonable choice for most cases. Also, a key element such as matrix P is straightforward to construct. For these reasons our algorithm is simpler than previous approaches. Additionally, the results here presented can be easily reproduced, since analytic solution of the optimization problem has been derived.The construction of the modes makes our method complete because their sum retrieves the original signal, in contrast to [11] where the reconstruction is not guaranteed. Computational cost is similar to EMD's. For some simulations, the cost was in favor of our method. We must remark that the cost of our proposal is at least two orders of magnitude lower than those of other optimization-based methods (Pustelink et al. and Oberlin et al.).By making no explicit use of envelopes to find the local mean, the method introduced in this paper avoids possible problems present in the original formulation of EMD, such as overshoot (undershoot), i.e. global maximum (minimum) of the upper (lower) envelope greater (lesser) than global maximum (minima) of the signal [14]. As in traditional EMD, one must avoid to use an extremely large number of sifting iterations, in order to not oversmooth the modes' envelopes.Summarizing, the new method here presented catches most flavors of the original EMD, with a more solid mathematical framework, which could lead to shed light on analytical properties of EMD. It also works in a wide range of applications where other optimization approaches fail. Additionally, its application to real and simulated signals yields comparable or even better results than traditional EMD.