@&#MAIN-TITLE@&#
Inexact subgradient methods for quasi-convex optimization problems

@&#HIGHLIGHTS@&#
An inexact subgradient algorithm is proposed for the quasi-convex programming.We establish the convergence property, finite convergence and efficiency estimates.The Hölder condition is assumed instead of usc when the constraint set X is compact.The generalized weak sharp minima is introduced and utilized when X is noncompact.We apply the proposed algorithm to solve the Cobb–Douglas production efficiency problem.

@&#KEYPHRASES@&#
Subgradient method,Quasi-convex optimization,Noise,Weak sharp minima,

@&#ABSTRACT@&#
In this paper, we consider a generic inexact subgradient algorithm to solve a nondifferentiable quasi-convex constrained optimization problem. The inexactness stems from computation errors and noise, which come from practical considerations and applications. Assuming that the computational errors and noise are deterministic and bounded, we study the effect of the inexactness on the subgradient method when the constraint set is compact or the objective function has a set of generalized weak sharp minima. In both cases, using the constant and diminishing stepsize rules, we describe convergence results in both objective values and iterates, and finite convergence to approximate optimality. We also investigate efficiency estimates of iterates and apply the inexact subgradient algorithm to solve the Cobb–Douglas production efficiency problem. The numerical results verify our theoretical analysis and show the high efficiency of our proposed algorithm, especially for the large-scale problems.

@&#INTRODUCTION@&#
Subgradient methods are popular and practical techniques used to minimize a nondifferentiable convex function. Subgradient methods originated with the works of Polyak (1967) and Ermoliev (1966) and were further developed by Shor, Kiwiel, and Ruszczyński (1985). In the last 40years, many properties of subgradient methods have been discovered, generalizations and extensions have been proposed, and various applications have been found (see Auslender & Teboulle, 2004; Bertsekas, Nedić, & Ozdaglar, 2003; Hiriart-Urruty & Lemaréchal, 1996; Larsson, Patriksson, & Strömberg, 1996; Nedić & Bertsekas, 2001; Nesterov, 2009; Patriksson, 2008; Shor et al., 1985 and references therein). Nowadays, the subgradient method still remains an important tool for nonsmooth and stochastic optimization problems, special for large-scale problems, due to its simple formulation and low storage requirement.Motivated by practical reasons, approximate subgradient methods (also called ∊-subgradient methods) are widely studied in Auslender and Teboulle (2004), D’Antonio and Frangioni (2009), Kiwiel (2004), Larsson, Patriksson, and Strömberg (2003), Shor et al. (1985). Kiwiel (2004) proposed a unified convergence framework for approximate subgradient methods. The author presented convergence in objective values and convergence to a neighborhood of the optimal solution set, using both the diminishing and nonvanishing stepsize rules. Larsson et al. (2003) proposed and analyzed conditional ∊-subgradient methods to solve convex optimization problems and convex–concave saddle-point problems. Improving conditional subgradient methods, D’Antonio and Frangioni (2009) combined the deflection and the conditional subgradient technique into one iterative process, and investigated the unified convergence analysis for the deflected conditional approximate subgradient methods, using both the Polyak-type and diminishing stepsize rules. Furthermore, Auslender and Teboulle (2004) proposed and developed an interior ∊-subgradient method for convex constrained optimization problems over polyhedral sets, in particularR+n, via replacing the Euclidean distance function by a logarithmic-quadratic distance-like function.Recently, Nedić and Bertsekas (2010) investigated the effect of noise on subgradient methods for convex optimization problems. Their work was motivated by the distributed optimization in networks where the data is quantized before being transmitted between nodes (see Kashyap, Basar, & Srikant, 2007; Rabbat & Nowak, 2005 and references therein). When the constraint set is compact or the objective function has a set of weak sharp minima, the authors established convergence properties to the optimal value within some tolerance, which is expressed in terms of errors and noise, under the bounded subgradient assumption.Quasi-convex optimization problems can be found in important applications in various areas, such as economics, engineering, management science and various applied sciences (see Avriel, Diewert, Schaible, & Zang, 1988; Crouzeix, Martinez-Legaz, & Volle, 1998; Hadjisavvas, Komlósi, & Schaible, 2005 and references therein). The study of using subgradient methods to solve quasi-convex optimization problems has been limited. Using the diminishing stepsize rule, Kiwiel (2001) studied convergence properties and efficiency estimates of the exact subgradient method for solving a quasi-convex optimization problem under the assumption that the objective function is upper semi-continuous. On the other hand, modified dual subgradient algorithms were investigated in Gasimov (2002) and Burachik, Gasimov, Ismayilova, and Kaya (2006) for solving a general nonconvex optimization problem with equality constraints by virtue of a sharp augmented Lagrangian.Motivated by practical and theoretical reasons, in this paper, we focus on an inexact subgradient algorithm for solving the following quasi-convex optimization problem:(1.1)minf(x)s.t.x∈X,wheref:Rn→Ris a quasi-convex function and the constraint set X is nonempty, closed and convex. We denote the optimal solution set and the optimal value respectively byX∗andf∗, and we assume thatX∗is nonempty and compact.Inspired by the idea in Nedić and Bertsekas (2010) and references therein, we investigate the influence of inexact terms, including both computation errors and noise, on the inexact subgradient algorithm. The computation errors, which give rise to the ∊-subgradient, is inevitable in computing process. On the other hand, the noise may come from practical considerations and applications, and is manifested in inexact computation of subgradients. Considering a generic inexact subgradient algorithm for the quasi-convex optimization problem (1.1) and assuming that the computational errors and noise are deterministic and bounded, we establish convergence properties in both objective values and iterates within some tolerance given explicitly in terms of errors and noise. We also describe the finite convergence behavior to approximate optimality and efficiency estimates of iterates.The quasi-convex function is more difficult to deal with, as the epigraph of a convex function is convex; while only the sublevel set of a quasi-convex function is convex. Lacking the convexity assumed in Nedić and Bertsekas (2010), the main technical challenges are defining a suitable subdifferential of a quasi-convex function, establishing the proper basic inequality, which is a key tool needed in this area of study, and applying the convexity of the sublevel set instead of that of the epigraph of a convex function, when analyzing the inexact subgradient method algorithm for the quasi-convex optimization problem. To meet these challenges, we adopt the closure of Greenberg–Pierskalla subdifferential as the quasi-convex subdifferential, introduce the Hölder condition to relate the quasi-convex subgradient with objective function values and establish the basic inequality, which is only a local property though, and then obtain the convergence property in objective values and finite convergence under the Hölder condition, instead of the upper semi-continuity of the objective function used in Kiwiel (2001). Another contribution is to describe the convergence property in iterates, which are absent in Nedić and Bertsekas (2010), by virtue of convexity of a sublevel set. When X is noncompact, we need to assume an additional generalized weak sharp minima condition. This condition extends the concept of weak sharp minima in Nedić and Bertsekas (2010) and is presented by usingdist(x,X∗), the distance of the decision variable x toX∗.We also investigate the quantification of the influence of errors and noise by using both the constant and diminishing stepsize rules, while only the diminishing stepsize rule is considered in studying convergence properties and efficiency estimates of an exact subgradient method in Kiwiel (2001).We further consider the fractional programming as an application of the quasi-convex model, describe the Cobb–Douglas production efficiency problem as an example, and perform some numerical experiments on this problem via applying the inexact subgradient method. The numerical results verify our theoretical analysis and show that the quasi-subgradient type method is highly efficient for the production efficiency problem, even when the problem is large-scale.This paper is organized as follows. In Section 2, we present the notations used in this paper, the quasi-subdifferential theory and the inexact subgradient algorithm. In Section 3, we establish convergence properties in both objective values and iterates, and finite convergence behavior of our algorithm when the constraint set X is compact. Section 4 presents the convergence behavior when f has a set of generalized weak sharp minima over noncompact X, and Section 5 gives the efficiency estimates. Finally in Section 6, we apply our algorithm to the Cobb–Douglas production efficiency problem, and demonstrate the numerical results.We consider the n-dimensional Euclidean spaceRn. We view vector as a column vector, and denote by〈x,y〉the inner product of two vectorsx,y∈Rn. We use‖x‖to denote the standard Euclidean norm,‖x‖=〈x,x〉. Forx∈Rnandδ∈R+,B(x,δ)denotes the closed ball of radiusδcentered at x and specially B denotes the unit closed ball at the origin. For a setZ⊆Rn, we denote the closure of Z byclZ. We also writedist(x,Z)to denote the Euclidean distance of a vector x from the set Z, i.e.,dist(x,Z)=infz∈Z‖x-z‖.A functionf:Rn→Ris said to be quasi-convex if for allx,y∈Rnandα∈[0,1], the following inequality holdsf((1-α)x+αy)⩽max{f(x),f(y)}.f is said to be upper semi-continuous (usc) onRniff(x)=limsupy→xf(y)for allx∈Rn. For eachα∈R, we denote the (strict) sublevel sets of f bySf,α={x∈Rn:f(x)<α},Sf(x)=Sf,f(x),S‾f,α={x∈Rn:f(x)⩽α},S‾f(x)=S‾f,f(x).It is well-known that f is quasi-convex if and only ifSf,α(S‾f,α)is convex for allα∈R, and that f is usc onRnif and only ifSf,αis open for allα∈R.There are many different types of subdifferential, such as Clarke–Rockafellar subdifferential, Dini subdifferential, Fréchet subdifferential (see Aussel, Corvellec, & Lassonde, 1995 and references therein) and so on. They are the same for convex functions, but different for nonconvex functions. Here we introduce the Greenberg–Pierskalla subdifferential, defined by Greenberg and Pierskalla (1973), as follows.Definition 2.1(see Greenberg & Pierskalla, 1973)The z-quasi-conjugate of f is a functionfz∗:Rn→R∪{+∞}, defined byfz∗(x)=z-inf{f(y):〈x,y〉⩾z}.It is recalled in Greenberg and Pierskalla (1973, Theorem 1) that the z-quasi-conjugate function provides a lower bound for the corresponding convex conjugate function, and indeed, the convex conjugate function is the supremum of the z-quasi-conjugate over z.Definition 2.2(see Greenberg & Pierskalla, 1973)A Greenberg–Pierskalla subgradient of f at x is a vectorg∈Rnsuch that(2.1)f(x)+f〈g,x〉∗(g)=〈g,x〉.The set of Greenberg–Pierskalla subgradients of f at x is called the Greenberg–Pierskalla subdifferential of f at x and is denoted by∂∗f(x).The following proposition gives an equivalent formula and some important properties of the Greenberg–Pierskalla subdifferential.Proposition 2.1(Greenberg & Pierskalla, 1973, Theorem 6)The following statements are true:(i)∂∗f(x)={g:〈g,y-x〉<0,∀y∈Sf(x)},∂∗f(x)is a convex cone,0∈∂∗f(x)if and only ifx∈argminf.Unfortunately, different from traditional subdifferentials, the Greenberg–Pierskalla subdifferential of f is not a closed set. In order to overcome this shortcoming, in this paper, we define the following closed set, which contains the closure of∂∗f(x), instead as the quasi-subdifferential, and use it in the inexact subgradient method.Definition 2.3Letf:Rn→Rbe a quasi-convex function. The quasi-subdifferential of f at x is defined by(2.2)∂¯∗f(x)={g:〈g,y-x〉⩽0,∀y∈Sf(x)}.When f is convex, the quasi-subdifferential coincides with the convex cone hull of the convex subdifferential (i.e.,∂¯∗f(x)=cone(∂f(x)); see Hiriart-Urruty and Lemaréchal, (1996), Chapter VI, Theorem 1.3.5), and the inexact subgradient method (2.4) is reduced to a normalized version of inexact subgradient method in Nedić and Bertsekas (2010). When f is quasi-convex, the existence and relationship between the Greenberg–Pierskalla subdifferential and the quasi-subdifferential are described in the following lemma.Lemma 2.1If f is quasi-convex onRn, then∂¯∗f(x)⧹{0}≠∅. In addition, if f is usc onRn, then∂∗f(x)≠∅, and∂¯∗f(x)coincides with the closure of∂∗f(x), i.e.,∂¯∗f(x)=∂∗f(x)∪{0}.IfSf(x)=∅, then∂¯∗f(x)=Rnand the conclusions hold automatically. Now supposeSf(x)≠∅, since the convex sets{x}andSf(x)are disjoint, it follows from Bertsekas et al. (2003, Proposition 2.4.5) that there exists a proper hyperplane separation, i.e., there exists a vectorg≠0such thatsupy∈Sf(x)〈g,y〉⩽〈g,x〉andinfy∈Sf(x)〈g,y〉<〈g,x〉.Thus, the vector g is a nonzero vector in∂¯∗f(x). For the second conclusion, see Kiwiel (2001, Lemma 3).□The above lemma shows that the existence of nonzero quasi-subgradient only requires the quasi-convexity. Therefore, throughout this paper, we assume that the objective function is quasi-convex. In particular, we do not assume the upper semi-continuity of the objective function as in Kiwiel (2001), unless otherwise specified.Motivated by practical reasons, relaxing (2.2) byf(x)+f〈g,x〉∗(g)⩽〈g,x〉+∊, we define the ∊-quasi-subdifferential as follows.Definition 2.4Letf:Rn→Rbe a quasi-convex function. The ∊-quasi-subdifferential of f at x is defined by(2.3)∂¯∊∗f(x)={g:〈g,y-x〉⩽0,∀y∈Sf,f(x)-∊}.In this paper, we introduce a generic inexact subgradient method, which we also call the approximate quasi-subgradient method, to solve the quasi-convex optimization problem (1.1) as follows.Approximate quasi-subgradient methodSelect a stepsize sequence{vk}, an error sequence{∊k}and a noise sequence{rk}. Start with an initial pointx0∈X, and generate a sequence{xk}⊆Xvia the iteration(2.4)xk+1=PX(xk-vkg̃k),wherePX(·)denotes the Euclidean projection operator onto X and the iterative directiong̃kis an approximate quasi-subgradient of the following form(2.5)g̃k≔gk/‖gk‖+rk,whererkis a noise vector andgk∈∂¯∊k∗f(xk)is an arbitrary nonzero∊k-quasi-subgradient of f atxk.Let us first consider the following example, which says that ∊-quasi-subdifferential does not coincide with quasi-subdifferential with noise.Example 2.1Consider the quasi-convex functionf(x,y)≔x2+y2,x⩾0,y2,x<0.Its strict sublevel setSf(0,1)=Sf,1is illustrated in Fig. 1, thus it is easy to see∂¯∗f(0,1)=cone{(0,1)}. Let the noise vectorr=(-δ,0)withδ>0. Then its quasi-subdifferential with noise and ∊-quasi-subdifferential are respectively given by∂¯∗f(0,1)+r={(-δ,λ):λ∈R+},and∂¯∊∗f(0,1)=cone{(0,1),(∊,1-∊)},∊<1,R2,∊⩾1.It is obvious that(-δ,1)∉∂¯∊∗f(0,1)for allδ>0when∊<1. Thus, from this example, we see that the quasi-subdifferential with noise cannot be represented by the ∊-quasi-subdifferential.It is well-known that the stepsize rule is critical in subgradient methods. In this paper, we investigate convergence properties of the approximate quasi-subgradient method using the following stepsize rules.(a)Constant stepsize rule. The stepsizevkis fixed to be a positive scalar v.Diminishing stepsize rule. The stepsizevksatisfies(2.6)vk>0,limk→∞vk=0,∑k=0∞vk=+∞.In this section, we investigate convergence properties of the approximate quasi-subgradient method when the constraint set X is compact. Throughout this section, the following three assumptions are made.Assumption 1The constraint set X is compact.f satisfies the Hölder condition of orderp>0with modulusμ>0onRn, that is,(3.1)f(x)-f∗⩽μdist(x,X∗)p,∀x∈Rn.The noise and errors are bounded, i.e., there exist someR,∊≥0such that‖rk‖⩽R,∀k⩾0andlimsupk→∞∊k=∊.Since the constraint set X is compact, all iterates are bounded. Therefore, there exists somed>0(such as the diameter of X) such that‖xk-x‖⩽dfor allx∈Xandk⩾0. Moreover, under the bounded noise assumption, it follows from (2.5) that approximate quasi-subgradients are uniformly bounded, i.e.,‖g̃k‖⩽1+Rfor allk⩾0.The Hölder condition of order p is used to describe some properties of quasi-subgradients in Konnov (1994). Here, we use this condition to investigate convergence properties of the approximate quasi-subgradient method. It is worth noting that the Hölder condition of order 1 is equivalent to the bounded subgradient assumption, assumed in Nedić and Bertsekas (2010), whenever f is convex.We now give the basic inequality and the convergence property in objective values using both the constant and diminishing stepsize rules. We start with the basic inequality, which shows a significant property of a subgradient iteration.Lemma 3.1Let Assumptions1 and 3hold and the sequence{xk}be generated by the approximate quasi-subgradient method. Then for allx∈X, we have(3.2)‖xk+1-x‖2⩽‖xk-x‖2-2vk〈gk/‖gk‖,xk-x〉-Rd-12vk(1+R)2,∀k.By (2.4) and (2.5) and the nonexpansive property of projection operator, for allx∈X, we have the following basic inequality(3.3)‖xk+1-x‖2⩽‖xk-vkg̃k-x‖2=‖xk-x‖2-2vk〈gk/‖gk‖+rk,xk-x〉+vk2‖gk/‖gk‖+rk‖2⩽‖xk-x‖2-2vk〈gk/‖gk‖,xk-x〉-Rd-12vk(1+R)2,where the last inequality follows from the compactness of X and boundedness of noise and errors.□The main difficulty in the study of the approximate quasi-subgradient method comes from the difference between the basic inequality (3.2) for our proposed quasi-convex subgradient method and that of convex subgradient method (cf. Nedić & Bertsekas, 2010). This difference originates from definitions and properties of subgradients: the convex subgradient directly connects with objective values and shares a global property of the objective function, while the quasi-convex subgradient is a normal direction to its sublevel set and is not directly associated with the objective function. Here, we utilize the Hölder condition to relate the quasi-convex subgradient with objective function values, which is only a local property.Lemma 3.2Kiwiel, 2001, Lemma 6IfB(x¯,r¯)⊂clSf,f(xk)-∊kfor somex¯∈Rnandr¯⩾0, then〈gk/‖gk‖,xk-x¯〉⩾r¯.IfAssumption 2holds andf(xk)>f∗+μr¯p+∊kholds for somer¯⩾0, then〈gk/‖gk‖,xk-x∗〉⩾r¯for allx∗∈X∗.Givenx∗∈X∗, by the Hölder condition of order p and the hypotheses of this lemma, for allx∈B(x∗,r¯), we havef(x)-f∗⩽μdist(x,X∗)p⩽μr¯p<f(xk)-f∗-∊k,which impliesB(x∗,r¯)⊂Sf,f(xk)-∊k. Hence, the conclusion follows from Lemma 3.2.□Let Assumptions 1–3hold. Then, for a sequence{xk}generated by the approximate quasi-subgradient method with the constant stepsize rule, we haveliminfk→∞f(xk)⩽f∗+μRd+v2(1+R)2p+∊.We prove by contradiction, assuming thatliminfk→∞f(xk)>f∗+μRd+v2(1+R)2p+∊,that is, there exists someδ>0and positive integerk0such that(3.4)f(xk)>f∗+μRd+v2(1+R)2+δp+∊k,∀k⩾k0.It follows from Lemma 3.3 that for allx∗∈X∗andk⩾k0there holds〈gk/‖gk‖,xk-x∗〉⩾Rd+v2(1+R)2+δ.Therefore, by using the basic inequality (3.2) withvk≡vandx=x∗, we obtain‖xk+1-x∗‖2⩽‖xk-x∗‖2-2vRd+v2(1+R)2+δ-Rd-v2(1+R)2=‖xk-x∗‖2-2vδ⩽⋯⩽‖xk0-x∗‖2-2(k-k0+1)vδ,which yields a contradiction for sufficiently large k. The proof is complete.□In Assumption 2, we assume that f satisfies the Hölder condition on the whole spaceRn. Actually, this assumption is essential for the convergence result in Theorem 3.1. Relaxing it by the assumption that f satisfies the Hölder condition on the constraint set X cannot ensure the validity of Theorem 3.1 even if f is continuous onRn, as shown by the following example.Example 3.1Consider the objective functionf(u,v)≔M|v|,u⩽0,u+M|v|,u>0,withM=100and the constraint setX={(u,v):-1⩽u⩽1,v=0}. Obviously, the optimal value of (1.1) isf∗=0and the optimal solution set isX∗={(u,v):-1⩽u⩽0,v=0}. It is easy to check that f is continuous and quasi-convex onR2and satisfies the Hölder condition (cf. (3.1)) on X withμ=p=1.Starting fromx0=(1,0), we use the approximate quasi-subgradient method (cf. (2.4) and (2.5)) to solve this problem. Specially, we choose the quasi-subgradientg=(1/1+M2,M/1+M2)∈∂¯∗f(x0), the noise vectorr=(-1/1+M2,0)and the constant stepsize rulev=1/2, then we havex1=PXx0-v(g+r)=PX(1,0)-v0,M/1+M2=(1,0)=x0.Hence, a fixed sequence is generated andlimk→∞f(xk)=f(x0)=1. However,R=0.01,∊=0,d=2,v=1/2and then the total errorμRd+v2(1+R)2p+∊<1/2. Therefore, Theorem 3.1 fails for this problem.Using the diminishing stepsize rule, the error term involving the stepsize v in Theorem 3.1 vanishes and the following theorem is obtained.Theorem 3.2Let Assumptions1–3hold. Then, for a sequence{xk}generated by the approximate quasi-subgradient method with the diminishing stepsize rule, we haveliminfk→∞f(xk)⩽f∗+μ(Rd)p+∊.The proof uses properties of the diminishing stepsize rule (cf. (2.6)) and a line of analysis similar to that of Theorem 3.1. We omit the details.□Theorems 3.1 and 3.2 show convergence to the optimal value within some tolerance given in terms of errors and noise by using the constant and diminishing stepsize rules, respectively. In Theorem 3.2, the total errorc≔μ(Rd)p+∊, which is a similar formula as in Nedić and Bertsekas (2010), has an additive form, including the noise level R and the error level ∊. By contrast, in Theorem 3.1, the total error additionally includes a term related to the constant stepsize v.When the noise vanishes (R=0), the approximate quasi-subgradient method is reduced to the ∊-quasi-subgradient method. In such a situation, the term〈rk,xk-x〉vanishes in the corresponding basic inequality, and Lemma 3.1 holds (whereR=0) without the compactness hypothesis of X. Therefore, when the noise vanishes, the convergence result holds regardless of the compactness hypothesis of X. Furthermore, when the error level is precise (∊=0), we obtain the convergence result of the exact quasi-subgradient method, which is the main result in Kiwiel (2001), where upper semi-continuity of f is assumed. Here, we have obtained the result as in Kiwiel (2001) without the usc assumption, but using the Hölder condition of order p instead. The following two examples show that the Hölder condition and upper semi-continuity are independent of each other.Example 3.2The function satisfies the Hölder condition but is not uscConsider the objective functionf(x)≔0,x⩽0,x2,0<x⩽1,2,x>1,with the constraint setX={x∈R:0⩽x⩽10}. Obviously, the optimal value of problem (1.1) isf∗=0and the optimal solution set isX∗={0}. It is easy to verify that f is quasi-convex and satisfies the Hölder condition of order 2 with modulus 1 onR. However, f is not usc atx=1. Thus, this example shows that the Hölder condition does not imply upper semi-continuity.Thus, from Kiwiel (2001), we cannot obtain convergence of the exact quasi-subgradient method (cf. (14)–(15) in Kiwiel (2001)) for this example. However, the sequence generated by the exact quasi-subgradient method converges toX∗. Indeed, for anyx∈X⧹X∗, the strict sublevel setSf(x)is the line segment[0,min{1,x})and the quasi-subdifferential∂¯∗f(x)=R+. Thus,xk+1=PX(xk-vkgk/‖gk‖)=max{xk-vk,0},and the sequence{xk}converges to the origin, the optimal solution, by properties of the diminishing stepsize rule. This iterative result coincides with the result in Theorem 3.2 (by settingR=0and∊=0).Consider the objective functionf(x)=ex,and the constraint setR+. Obviously, the optimal value of problem (1.1) isf∗=1and the optimal solution set isX∗={0}. It is easy to check that f is continuous and quasi-convex (since it is monotone) onR. However, by the Taylor expansionex=∑n=0∞xnn!, we claim that f does not satisfy the Hölder condition onRfor any positive scalars p and μ. Indeed, given positive scalars p and μ, whenx⩾explog(μ⌈p+1⌉!)⌈p+1⌉-p, whereexp(·)and⌈p⌉denote the exponential function and the largest integer not greater than p respectively, we havef(x)-f∗=ex-1>x⌈p+1⌉⌈p+1⌉!+x00!-1=x⌈p+1⌉⌈p+1⌉!⩾μxp,which contradicts with (3.1). Thus, this example shows that upper semi-continuity does not imply the Hölder condition.Although, from Kiwiel (2001), we obtain the convergence property of the exact quasi-subgradient method for this example. However, the convergence result of the approximate quasi-subgradient method (see Theorem 3.2) fails for this example. Indeed, given positive scalars p and μ, we consider the constraint setX=x∈R:0⩽x⩽explog(μ⌈p+1⌉!)⌈p+1⌉-p, noiserk≡-1and errors∊k≡0. For anyx∈X⧹X∗, the strict sublevel setSf(x)is the line segment[0,x)and the quasi-subdifferential∂¯∗f(x)=R+. Thus, starting fromx0=explog(μ⌈p+1⌉!)⌈p+1⌉-p, we havex1=PXx0-v0(g0/‖g0‖+r0)=x0.Hence, the approximate quasi-subgradient method (cf. (2.4) and (2.5)) generates a fixed sequence andlimk→∞f(xk)=f(x0)=ex0. However, whenR=1,∊=0andd=explog(μ⌈p+1⌉!)⌈p+1⌉-p, the total error, given in Theorem 3.2,μ(Rd)p+∊=μdp=d⌈p+1⌉/⌈p+1⌉!<ed=ex0, where the inequality follows from the Taylor expansion. Therefore, Theorem 3.2 fails for this example.From the above two examples, we observe that the Hölder condition of order p describes some property of the objective function, which is essentially different from the upper semi-continuity, and it can be used to investigate convergence properties of the approximate quasi-subgradient method. Hence, using the mild assumptions, we have established convergence properties of the approximate quasi-subgradient method from a new perspective, which is different from that in Kiwiel (2001).The optimal solution setX∗has a nonempty interior in many interesting applications, such as surrogate relaxation of discrete programming problems (see Dyer, 1980). Here, we demonstrate finite convergence behavior to the approximate optimal solution set of problem (1.1) under the assumption that the optimal solution setX∗has a nonempty interior.Theorem 3.3Let Assumptions1–3hold,intX∗≠∅and the diminishing stepsize rule be chosen. Thenf(xk)⩽f∗+μ(Rd)p+∊for some k.By contradiction, we assume thatf(xk)>f∗+μ(Rd)p+∊for allk∈N. SinceintX∗≠∅, we setB(x¯,δ¯)⊂X∗for someδ¯>0. Then for allx∈B(x¯,Rd+23δ¯), we have(3.5)f(x)-f∗⩽μdist(x,X∗)p⩽μRd-13δ¯p=μ(Rd)p-δ′<f(xk)-f∗-∊-δ′,whereδ′is a scalar in13μpδ¯(Rd-13δ¯)p-1,13μpδ¯(Rd)p-1satisfying the mean value theorem. Furthermore, sincelimsupk→∞∊k=∊, there exists some positive integerk0such that∊k⩽∊+δ′for allk⩾k0. Therefore, (3.5) impliesf(x)<f(xk)-∊kand thenBx¯,Rd+23δ¯⊂Sf,f(xk)-∊kfor allk⩾k0. Hence, it follows from Lemma 3.2 that(3.6)〈gk/‖gk‖,xk-x¯〉⩾Rd+23δ¯.However, summing the basic inequalities (3.2) withx=x¯fori=k0,⋯,k, we obtain(3.7)mini=k0,…,kgi‖gi‖,xi-x¯⩽∑kokvigi‖gi‖,xi-x¯∑kokvi⩽‖xk0-x¯‖22∑k0kvi+Rd+∑k0kvi22∑k0kvi(1+R)2.By the property of the diminishing stepsize rule (cf. (2.6)), it follows from Kiwiel (2004, Lemma 2.1) thatlimk→∞(∑i=k0kvi2/∑i=k0kvi)=0, and thus the right hand side of (3.7) tends to Rd as k tends to infinity. Hence we arrive at a contradiction with (3.6). The proof is complete.□Under the same assumption of Theorem 3.3, we now describe a related result for the nonvanishing stepsize rule.Theorem 3.4Let Assumptions1–3hold. IfB(x¯,δ¯)⊂X∗for someδ¯>0and there exists some0<κ<1andk0∈Nsuch thatvk∈κ2δ¯(1+R)2,κδ¯(1+R)2for allk⩾k0, thenf(xk)⩽f∗+μ(Rd)p+∊for some k.By contradiction, supposef(xk)>f∗+μ(Rd)p+∊for allk∈N. As in the proof of Theorem 3.3 and (3.7), we haveRd+23δ¯⩽mini=k0,…,k〈gi/‖gi‖,xi-x¯〉⩽‖xk0-x¯‖22∑k0kvi+Rd+∑k0kvi22∑k0kvi(1+R)2⩽‖xk0-x¯‖22κ2δ¯(k-k0+1)(1+R)2+Rd+δ¯/2,whose last right hand side tends toRd+δ¯/2as k tends to infinity. The contradiction happens.□We have shown the convergence property in objective values in Section 3.1, and in this subsection we consider the convergence property in iterates. In Nedić and Bertsekas (2010), where noise in subgradient methods for convex optimization was considered, Nedić and Bertsekas did not give convergence property in iterates. In fact, convergence of{xk}is quite difficult to obtain. Kiwiel (2004) described the convergence of{xk}generated by ∊-subgradient method for convex optimization. Although Kiwiel did not consider the effect of noise, his work is really helpful for our research. Following the framework of Kiwiel (2004), we give the convergence of{xk}generated by the approximate quasi-subgradient method using the diminishing stepsize rule. Besides the extension to the approximate quasi-subgradient method, another improvement of our work is to maintain the convergence property without the lower semi-continuous and coercive condition assumptions used in Kiwiel (2004), instead we use the usc assumption.First, let us show a useful property of a convergent sequence, which converges in objective values as well. This result requires the additional usc assumption.Lemma 3.4Suppose f is usc onRn,α>0, and the sequence{xk}converges tox¯withlimk→∞f(xk)⩽f∗+α. Thendist(x¯,S‾f,f∗+α)=0.Observe thatSf,f∗+βis open and convex (as f is usc and quasi-convex) for allβ>f∗+αand that⋂β>f∗+αSf,β⊃Sf,f∗+α, which is nonempty (asαis positive and f is usc). Since further{Sf,β}is decreasing asβ↓f∗+α, by Rockafellar and Wets (1998, Exercise 4.3(b)), we have(3.8)limβ↓f∗+αSf,β=⋂β>f∗+αclSf,β=cl⋂β>f∗+αSf,β=clS‾f,f∗+α,where the second equality follows from Rockafellar (1970, Theorem 6.5). Finally, by Rockafellar and Wets (1998, Corollary 4.7) and (3.8), we arrive at thatdist(x¯,S‾f,f∗+α)=dist(x¯,cl(S‾f,f∗+α))=limβ↓f∗+αdist(x¯,Sf,β)=0,wheredist(x¯,Sf,β)=0for allβ>f∗+α, sincelimk→∞xk=x¯andlimk→∞f(xk)⩽f∗+α<β.□Next, we describe the convergence of{xk}to some approximate optimal solution set by using the diminishing stepsize rule.Theorem 3.5Let Assumptions1–3hold, the total errorc≔μ(Rd)p+∊>0, f be usc onRnand the diminishing stepsize rule be chosen. Then the following statements are true:(i)liminfk→∞dist(xk,S‾f,f∗+c∩X)=0.limk→∞dist(xk,X∗+ρ(c)B)=0, whereρ(c)is defined byρ(c)≔max{dist(x,X∗):x∈S‾f,f∗+c∩X}.First, observe thatX∗⊂S‾f,f∗+c∩X⊂X∗+ρ(c)B. Furthermore, the nonemptiness ofX∗and the compactness of X imply thatS‾f,f∗+c∩Xis nonempty and bounded.(i)Theorem 3.2 gives thatliminfk→∞f(xk)⩽f∗+c. The compactness of X then implies that there exists some subsequence{xki}that converges to somex¯∈Xwithlimi→∞f(xki)⩽f∗+c. Thus, the conclusion follows from Lemma 3.4.Givenσ>0, defineV2σ≔X∗+ρ(c)B+2σB,and(3.9)eσ≔inff(x):x∈X,dist(x,S‾f,f∗+c∩X)⩾σ-(f∗+c).We first claim thateσ>0. Indeed, ifeσ=0, then there exists sequence{zi}, in{x:x∈X,dist(x,S‾f,f∗+c∩X)⩾σ}, converges to somez¯∈Xwithlimi→∞f(zi)=f∗+c. It follows from Lemma 3.4 thatdist(z¯,S‾f,f∗+c)=0. Moreover, sincez¯∈X,dist(z¯,S‾f,f∗+c∩X)=0, which is impossible asσ>0.For such positiveeσ, there exists someδ>0such that(3.10)μ(Rd+δ)p⩽μ(Rd)p+eσ/2.Since the stepsizevkdiminishes, there existskδ∈Nsuch that(3.11)vk⩽δ/(1+R)2,∀k⩾kδ.Sincelimsupk→∞∊k=∊andlimk→∞‖xk+1-xk‖=0(sincevkdiminishes), there exists somekσ⩾kδsuch that(3.12)∊k<∊+eσ/2,and(3.13)‖xk+1-xk‖⩽σ,for allk⩾kσ. Sinceliminfk→∞dist(xk,S‾f,f∗+c∩X)=0(cf. (i)), there exists somekσ′⩾kσ⩾kδsuch thatxkσ′∈(S‾f,f∗+c∩X)+σB⊂X∗+ρ(c)B+σB⊂V2σ,that is,xkσ′∈V2σ.Next, we claim thatxk∈V2σfor allk⩾kσ′. Proving by induction, we assume thatxk∈V2σfor somek⩾kσ′and consider the following two cases.Case 1. Ifdist(xk,S‾f,f∗+c∩X)⩽σ, from (3.13), we havexk+1∈{xk}+σB⊂(S‾f,f∗+c∩X+σB)+σB⊂X∗+ρ(c)B+2σB=V2σ.Case 2. Supposedist(xk,S‾f,f∗+c∩X)>σ, from (3.9), we havef(xk)⩾eσ+f∗+c=f∗+(μ(Rd)p+eσ/2)+(∊+eσ/2)>f∗+μ(Rd+δ)p+∊k,∀k⩾kσ′,where the second inequality follows from (3.10) and (3.12). Hence, from Lemmas 3.1 and 3.3, we have‖xk+1-x∗‖2⩽‖xk-x∗‖2-2vkδ-vk2(1+R)2⩽‖xk-x∗‖2,where the second inequality follows from (3.11). Thus,xk∈V2σimpliesxk+1∈V2σ. Therefore, by induction,xk∈V2σ, and hence,dist(xk,X∗+ρ(c)B)⩽2σfor allk⩾kσ′. Sinceσ>0is arbitrary, thendist(xk,X∗+ρ(c)B)vanishes as k tends to infinity.□In this section, we consider the other case when X is noncompact. Considering the similar case, Nedić and Bertsekas (2010) assumed that the objective function f has a set of weak sharp minima and the ∊-subgradients are uniformly bounded on X (see Nedić & Bertsekas, 2010, Assumptions 3.1–3.2). The function f is said to have a set of weak sharp minima over X (see Burke & Ferris, 1993) if for some scalarη>0there holdsf(x)-f∗⩾ηdist(x,X∗),∀x∈X.A natural extension to generalize the weak sharp minima is the weak sharp minima of order q (see Bonnans & Ioffe, 1995; Studniarski & Ward, 1999), that is, there exist some scalarsη,q>0such that(4.1)f(x)-f∗⩾ηdist(x,X∗)q,∀x∈X.However, ifp>q, contradiction between (3.1) and (4.1) arises asdist(x,X∗)tends to zero. Also, ifp<q, contradiction arises again asdist(x,X∗)tends to infinity. In order to avoid the contradiction, we weaken the assumption (4.1) as the generalized weak sharp minima, in which the constant q is replaced by a positive functiong(t).Furthermore, in what follows we consider a noise sequence{rk}whose norm bound R is lower than(η/μ)1/p, which we refer to as a low level noise sequence (see Nedić & Bertsekas, 2010). In particular, we introduce the following assumptions.Assumption 4The function f satisfies the generalized weak sharp minima condition over X, that is, there exist some scalarsη>0,q⩾pand a functiong:R+→R+, satisfyingg(·)⩾p,supt⩾0g(t)=qandlimt→∞g(t)=p, such that(4.2)f(x)-f∗⩾ηdist(x,X∗)g(dist(x,X∗)),∀x∈X,where p is the order of Hölder condition used in Assumption 2.{rk}is a low level noise sequence (i.e.,R<(η/μ)1/p).Wheng(t)≡p, Assumption 4 is reduced to weak sharp minima of order p, whose sufficient and necessary conditions have been described by Studniarski and Ward (1999) and Bonnans and Ioffe (1995) for specifiedp=2. Furthermore, ifp=1, it is reduced to the well-known weak sharp minima introduced by Burke and Ferris (1993). Note that, to arrive at the corresponding convergence results, Assumptions 2–5 with specifiedp=q=1were used in Nedić and Bertsekas (2010).Wheng(t)≔g(0),0⩽t⩽1,p,t>1,whereg(0)>p, Assumption 4 is reduced tof(x)-f∗⩾min{ηdist(x,X∗)g(0),ηdist(x,X∗)p},which is equivalent to that f has Höldrian level sets over X (see Pang, 1997). Another interesting example of Assumption 4 isg(t)=p+1/t.Before we go on, we introduce an auxiliary functionHv,θxand investigate some properties of the maximum solution ofHv,θx(z)⩾0over X, which are useful in the study of convergence properties in objective values and iterates when X is noncompact in next two subsections.Definition 4.1Letμand p be scalars given in Assumption 2, R and ∊ be scalars given in Assumption 3, and the function g be described in Assumption 4. For eachv⩾0,θ⩾0andx∈X, we define an auxiliary functionHv,θx:R+→Rby(4.3)Hv,θx(z)≔μv2(1+R)2+Rzη1/g(dist(x,X∗))p+∊+θ-z,We denote byzv,θ∗to be the maximum solution of the inequalityHv,θx(z)⩾0for somex∈X, defined by(4.4)zv,θ∗≔sup{z:Hv,θx(z)⩾0for somex∈X}.Let Assumptions4 and 5hold. Then the following statements hold:(i)zv,θ∗is finite for allv⩾0andθ⩾0.limθ→0+zv,θ∗=zv,0∗for allv⩾0.limv→0+zv,θ∗=z0,θ∗for allθ⩾0.(i)By the assumptions thatR<(η/μ)1/pandq⩾p, we havelimz→∞μRη1/qz1/q-1/pp<1,which is equivalent tolimz→∞μzv2(1+R)2+Rzη1/qp+∊+θz<1,∀v⩾0,θ⩾0.This implieslimz→∞Gv,θq(z)<0. Hence,zv,θq<+∞for allv⩾0andθ⩾0, sinceGv,θq(·)is continuous. Similarly, we can prove thatzv,θp<+∞for allv⩾0andθ⩾0. Thus, by using (4.6), we arrive at thatzv,θ∗is finite for allv⩾0andθ⩾0.SinceGv1,θ1q(·)⩽Gv2,θ2q(·)for allv1⩽v2andθ1⩽θ2, thenzv1,θ1q⩽zv2,θ2q. This monotonicity immediately implieslimθ→0+zv,θq⩾zv,0q.Next, we prove the reverse inequality. By the definition ofzv,θq, for givenv⩾0and each positive integer n, there exists someznsatisfyingzn>zv,1/nq-1/nandGv,1/nq(zn)⩾0. Together with the monotonicity ofzv,θq, we have-1<zn⩽zv,1/nq⩽zv,1q, where the last term is finite by (i). So the sequence{zn}is bounded and has cluster points. Thus, for each of its cluster pointsz¯, taking a subsequence of{zn}if necessary, we havelimn→∞Gv,1/nq(zn)=limn→∞μv2(1+R)2+Rznη1/qp+∊+1n-zn=μv2(1+R)2+Rz¯η1/qp+∊-z¯=Gv,0q(z¯),which is nonnegative, since{Gv,1/nq(zn)}are all nonnegative. Then, by the definition ofzv,θq, we havezv,0q⩾z¯⩾limθ→0+zv,θq, where the second inequality holds due tozn>zv,1/nq-1/n. Therefore, we arrive atlimθ→0+zv,θq=zv,0q.Similarly, we can prove thatlimθ→0+zv,θp=zv,0p. Thus, from (4.6), we arrive atlimθ→0+zv,θ∗=zv,0∗for allv⩾0.The proof is similar to that of (ii).□Similar to Section 3.1, we obtain the following basic inequality.Lemma 4.2Let{xk}be the sequence generated by the approximate quasi-subgradient method. Then for allx∈X, we have‖xk+1-x‖2⩽‖xk-x‖2-2vk〈gk/‖gk‖,xk-x〉-R‖xk-x‖-12vk(1+R)2,∀k.Before we discuss the convergence in objective values which is the main result of this subsection, we consider the following two lemmas which show the boundedness of{xk}, generated by the approximated quasi-subgradient method using both types of stepsize rules. This interesting property is new in the literature.Lemma 4.3Let Assumptions2–5hold and{xk}be generated by the approximate quasi-subgradient method with the constant stepsize rule. Then{xk}is bounded.Sincelimsupk→∞∊k=∊, for anyθ>0, there exists some positive integerk0such that(4.7)∊k<∊+θ,∀k⩾k0.Define the maximum solution oftg(t)⩽zv,θ∗/ηby(4.8)T≔supt∈R+:tg(t)⩽zv,θ∗/η,which is finite, sincezv,θ∗is finite (cf. Lemma 4.1(i)) andlimt→∞tg(t)=+∞(cf. Assumption 4). Next, we claim that the following inequality holds for alli⩾k0:(4.9)dist(xi,X∗)⩽max{dist(xk0,X∗),T+v(1+R)}.It is obvious that (4.9) holds ifi=k0. Proving by induction, we assume that (4.9) holds for somei=k(⩾k0). We consider the following two cases.Case 1. Iff(xk)⩽f∗+μv2(1+R)2+Rf(xk)-f∗η1/g(dist(xk,X∗))p+∊k, by (4.7), we haveμv2(1+R)2+Rf(xk)-f∗η1/g(dist(xk,X∗))p+∊+θ-(f(xk)-f∗)⩾0,that is,Hv,θxk(f(xk)-f∗)⩾0. Hence, by (4.4), we obtainf(xk)-f∗⩽zv,θ∗and thendist(xk,X∗)g(dist(xk,X∗))⩽zv,θ∗/η,which follows from (4.2). Thus, from (4.8), we arrive atdist(xk,X∗)⩽T, and thus relations (2.4) and (2.5) implydist(xk+1,X∗)⩽dist(xk,X∗)+vk‖gk/‖gk‖+rk‖<T+v(1+R).That is (4.9) holds fori=k+1.Case 2. Supposef(xk)>f∗+μv2(1+R)2+Rf(xk)-f∗η1/g(dist(xk,X∗))p+∊k, then it follows from Lemma 3.3 that〈gk/‖gk‖,xk-x∗〉⩾v2(1+R)2+Rf(xk)-f∗η1/g(dist(xk,X∗))⩾v2(1+R)2+Rdist(xk,X∗),where the second inequality follows from (4.2). Hence, applying Lemma 4.2 withvk=vandx∗=PX∗(xk), we obtaindist(xk+1,X∗)2⩽‖xk+1-x∗‖2⩽‖xk-x∗‖2-2vv2(1+R)2+Rdist(xk,X∗)-R‖xk-x∗‖-v2(1+R)2=dist(xk,X∗)2.Hence, (4.9) holds fori=k+1.Therefore, by induction, (4.9) holds for alli⩾k0. Since the right hand side of (4.9) is finite andX∗is compact, then{xk}is bounded. □When using the diminishing stepsize rule, we can also achieve the boundedness of the generated sequence as follows. The proof is omitted.Lemma 4.4Let Assumptions2–5hold and{xk}be generated by the approximate quasi-subgradient method with the diminishing stepsize rule. Then{xk}is bounded.From Lemmas 4.3 and 4.4, one can see that{xk}is bounded, and hence,{f(xk)}is bounded from above due to the Hölder condition (cf. (3.1)), by using both types of stepsize rules. We denote by M the upper bound of{f(xk)}in what follows. Next, we first present the convergence property of the approximate quasi-subgradient method by using the constant stepsize rule.Theorem 4.1Let Assumptions2–5hold and{xk}be generated by the approximate quasi-subgradient method with the constant stepsize rule. Then,zv,0∗is finite andliminfk→∞f(xk)⩽f∗+zv,0∗.The finiteness ofzv,0∗has been proved in Lemma 4.1(i). To prove the convergence property, we first show thatliminfk→∞f(xk)<f∗+zv,θ∗for allθ>0by contradiction, that is, assume the following inequality holds for someθ>0,liminfk→∞f(xk)⩾f∗+zv,θ∗.Thus, there exists someδ∈(0,min{θ/2,zv,θ∗})and positive integerk0such that(4.10)f(xk)>f∗+zv,θ∗-δ,and(4.11)∊k<∊+θ/2,for allk⩾k0, where (4.11) holds due tolimsupk→∞∊k=∊.By (4.4) and (4.10), we obtainf(xk)-f∗+δ>sup{z:Hv,θxk(z)⩾0}and thenHv,θxk(f(xk)-f∗+δ)<0, that is,f(xk)>f∗+μv2(1+R)2+Rf(xk)-f∗+δη1/g(dist(xk,X∗))p+∊+θ-δ>f∗+μv2(1+R)2+Rf(xk)-f∗+δη1/g(dist(xk,X∗))p+∊k⩾f∗+μv2(1+R)2+Rf(xk)-f∗η1/g(dist(xk,X∗))+δ′p+∊k,∀k⩾k0,where the second inequality follows from (4.11) and0<δ<θ/2, and the third inequality follows from the Taylor expansion withδ′=minδηqzv,θ∗η1/q-1,δηpM-f∗η1/p-1>0(recall that M is the upper bound of{f(xk)}). Therefore, by Lemmas 3.3 and 4.2, we obtain〈gk/‖gk‖,xk-x∗〉⩾v2(1+R)2+Rdist(xk,X∗)+δ′,∀k⩾k0,and thus,dist(xk+1,X∗)2⩽dist(xk,X∗)2-2vδ′⩽⋯⩽dist(x0,X∗)2-2(k-k0+1)vδ′,which yields a contradiction for sufficiently large k. Thus, we haveliminfk→∞f(xk)⩽f∗+zv,θ∗,∀θ>0.Taking the limit asθ→0, by Lemma 4.1, we arrive at the conclusion.□We now give some explicit expressions for the total error in approachingf∗in Theorem 4.1 for specific cases of p andg(t). By solving (4.5) and (4.6), we have the following corollaries where the total errors are given in explicit expressions.Corollary 4.1Let Assumptions2–5hold withg(t)≡pandp=1. Then, for a sequence{xk}generated by the approximate quasi-subgradient method with the constant stepsize rule, we haveliminfk→∞f(xk)⩽f∗+12μv(1+R)2+∊ηη-Rμ.By assumptions,g(t)≡pandp=q=1, we haveGv,0p(z)=Gv,0q(z)=μv2(1+R)2+Rzη+∊-zandzv,0p=zv,0q.It is clear thatGv,0p(z)is linear and decreasing due toR<η/μ. Thus, by (4.5),zv,0pis just the solution ofGv,0p(z)=0. Then, by (4.6), we havezv,0∗=zv,0p=12μv(1+R)2+∊ηη-Rμ. Hence, by Theorem 4.1, we arrive at the conclusion. □Similar to Corollary 4.1, we obtain explicit expressions for the total error wheng(t)≡pandp=2. The proof is straightforward, and thus, omitted.Corollary 4.2Let Assumptions2–5hold withg(t)≡pandp=2. Then, for a sequence{xk}generated by the approximate quasi-subgradient method with the constant stepsize rule, we haveliminfk→∞f(xk)⩽f∗+ημvR(1+R)2+ημv2(1+R)4+4∊(η-μR2)2(η-μR2)2.Using the diminishing stepsize rule, the total error tends toz0,0∗asvkdiminishes and the following theorem is obtained.Theorem 4.2Let Assumptions2–5hold and{xk}be generated by the approximate quasi-subgradient method with the diminishing stepsize rule. Thenz0,0∗is finite andliminfk→∞f(xk)⩽f∗+z0,0∗.So far, we have established the convergence property in objective values for approximate quasi-subgradient method and extended the corresponding results in Nedić and Bertsekas (2010) in Theorems 4.1 and 4.2 in the presence of generalized weak sharp minima. Specifyingg(t)≡pandp=1, the generalized weak sharp minima is reduced to the weak sharp minima used in Nedić and Bertsekas (2010), and the obtained total errors (cf. Corollary 4.1) have similar formulae to that of Nedić and Bertsekas (2010, Propositions 3.1 & 3.2).In this subsection, by the virtual of the auxiliary functionHv,θxand its maximum solutionzv,θ∗, we describe the finite convergence behavior and convergence of{xk}of the approximate quasi-subgradient method when the constraint set is noncompact. The line of analysis is similar to preceding sections, and thus, we omit the details.Theorem 4.3Let Assumptions2–5hold,intX∗≠∅and the diminishing stepsize rule be chosen. Thenf(xk)⩽f∗+z0,0∗for some k.Let Assumptions2–5hold. IfB(x¯,δ¯)⊂X∗withδ¯>0and there exists some0<κ<1andk0∈Nsuch thatvk∈κ2δ¯(1+R)2,κδ¯(1+R)2for allk⩾k0, thenf(xk)⩽f∗+z0,0∗for some k.Let Assumptions2–5hold withz0,0∗>0(cf.(4.4)), f be usc onRnand the diminishing stepsize rule be chosen. Then the following statements are true:(i)liminfk→∞dist(xk,S‾f,f∗+z0,0∗∩X)=0.limk→∞dist(xk,X∗+ρ(z0,0∗)B)=0, whereρ(z0,0∗)is defined byρ(z0,0∗)≔maxdist(x,X∗):x∈S‾f,f∗+z0,0∗∩X.In this section, under the bounded assumption (see Assumptions 1 and 3), we discuss the efficiency estimates of the approximate quasi-subgradient method. In order to quantify the efficiency, we introduce some concepts as in Kiwiel (2001).The inradius of a set Z denotes the radius of the largest ball contained in Z, defined by(5.1)ṙ(Z)≔sup{r>0:B(x,r)⊂Zforsomex∈Z}.For anyγ∈(0,1), theγ-solution set of problem (1.1) is defined by(5.2)Xγ∗≔{x∈X:ṙ(Sf(x))<γṙ(X)}.It follows from (5.2) that x is anγ-solution of problem (1.1) ifx∈XandSf(x)does not contain a ball with radiusγṙ(X). Thus, the significance of inradius is to estimate the efficiency of algorithms, inasmuch as x is anγ-solution ifṙ(Sf(x))<γṙ(X). The criterion is that the quality of iterate improves if the inradius of its strict sublevel set decreases.At iterationk≥1, the record valuef∊,krecdenotes the best approximate value found so far, and is defined by(5.3)f∊,krec≔minj=1,…,kf(xj)-∊j.Letṙkdenote the inradius of the record strict sublevel set, defined byṙk≔ṙ(Sf,f∊,krec),which is nonincreasing in k.In view of application considerations, we would like our algorithm to reach theγ-solution set as fast as possible. Since the quality of the record value/point improves if the inradiusṙkdecreases (cf. Kiwiel, 2001, Lemma 13), we would likeṙkto decrease as fast as possible. For this purpose, we now give an upper bound ofṙkthat depends on the stepsize rule.Lemma 5.1Let Assumptions1 and 3hold. For a sequence{xk}generated by the approximate quasi-subgradient method, we have(5.4)ṙk⩽Rd+d2+(1+R)2∑j=ikvj22∑j=ikvj,fori=1,⋯,k.Supposeṙk>0. For anyδ<ṙk, it follows from (5.1) that there exists somex¯such thatB(x¯,δ)⊂Sf,f∊,krec. Then for eachj=1,⋯,k, from (5.3), we haveB(x¯,δ)⊂Sf,f(xj)-∊j. Hence, it follows from Lemma 3.2 that〈gj/‖gj‖,xj-x¯〉⩾δ,forj=1,⋯,k.Therefore, from Lemma 3.1, we have‖xj+1-x¯‖2⩽‖xj-x¯‖2-2vjδ+2vjRd+vj2(1+R)2.Summing these inequalities overj=i,⋯,k, we arrive atδ⩽Rd+d2+(1+R)2∑j=ikvj22∑j=ikvj,fori=1,⋯,k.Sinceδ<ṙkis arbitrary, we arrive at the conclusion.□In the sense of guaranteeing that the record values/points becomeγ-solutions as fast as possible, the best stepsize may be found by minimizing the upper bound ofṙkin (5.4). In the following, we offer the best choice on the constant stepsize rule and estimate the rate of efficiency by using the diminishing stepsize rule.Theorem 5.1Let Assumptions1 and 3hold. For a sequence{xk}generated by the approximate quasi-subgradient method, the following statements hold:(i)If a constant stepsize v is chosen, thenṙk⩽d22kv+Rd+v2(1+R)2.The best constant stepsize isvi=d(1+R)kandṙk⩽d(1+R)k+Rd.If the diminishing stepsize is chosen asvi=a/i, thenṙk⩽Rd+ck-1/2withc=d2+a2(1+ln2)(1+R)2a(4-22).More general, ifvkis chosen as the diminishing stepsize rule, thenlimk→∞ṙk⩽Rd.(i)It is (5.4) specifyingi=1andvi≡v.Minimizing the upper bound ofṙkin (i) with respect to v, we obtain the best constant stepsizev=d(1+R)kand the corresponding upper bound on the inradius.It follows from Nesterov (1989, p. 157) that∑j=ikj-1⩽1+ln2and∑j=ikj-1/2⩾(2-2)k1/2,fori=k2.Using (5.4), we obtainṙk⩽Rd+d2+a2(1+ln2)(1+R)2a(4-22)k1/2=Rd+ck-1/2.Furthermore, the property of the diminishing stepsize rule implieslimk→∞∑j=ikvj2/∑j=ikvj=0(cf. Kiwiel, 2004, Lemma 2.1), and thus (5.4) implieslimk→∞ṙk⩽Rd.□Fractional programming is widely used in the modeling of practical problems arising in various areas, such as economics, information theory, management science and applied physics. In fractional programming problems, the objective is to optimize certain indicator (efficiency), characterized by a ratio of technical and economical terms, subject to the constraint imposed on the availability of goods. Examples of such situations are financial and corporate planning (debt/equity ratio), production planning (inventory/sales, output/employee), health care and hospital planning (cost/patient, nurse/patient ratio), etc. For details, one can refer to Avriel et al. (1988), Crouzeix et al. (1998), Hadjisavvas et al. (2005), Stancu-Minasian (1997) and references therein.We consider the Cobb–Douglas production efficiency problem introduced by Bradley and Frey (1974). The problem is briefly described as follows. Consider a set of projectsi=1,…,mand a collection of production factorsj=1,…,n, the total profit value assigned to these projects is given by the following Cobb–Douglas production functionProfit=a0∏j=1nxjaj,where∑j=1naj=1,where the variablesxjdesignate the production factors. The Cobb–Douglas production function represents the relationship between the input variable specifying the production factors and the output variables specifying the results of the production activities. The total cost is a linear function of the levels of investment in these projects, denoted byCost=∑j=1ncjxj+c0.The production efficiency problem is to maximize the profit/cost ratio, which is an efficiency indicator, i.e., the ratio between what is obtained and the expenditure, subject to a variety of constraints on funding levels. Hence, the Cobb–Douglas production efficiency model is stated as(6.1)maxf(x)≔a0∏j=1nxjaj∑j=1ncjxj+c0s.t.∑j=1nbijxj⩾pi,i=1,…,m,x⩾0,wherepirepresents the profit that must be obtained at project i andbijrepresents the contribution of the production factor j to project i to realize the profitpi. According to the circumstance of the Cobb–Douglas production efficiency problem, all parameters on profit (aj) and cost (cj) are all positive. From Stancu-Minasian (1997, Theorems 2.3.3 & 2.5.1), it is clear that (6.1) is a quasi-concave maximization problem.We conduct all numerical experiments in a personal laptop (Intel Core i7, 2.00GHz, 8.00GB of RAM) using MATLAB R2009a. In the numerical experiments, the parameters of the problem (6.1) are randomly chosen from different intervals,aj,bij∈[0,1],a0,c0,cj∈[0,10],andpi∈[0,n/2].The diminishing stepsize rule is chosen asvk=v/(1+0.1k),where v is always chosen between [2,5], while the constant stepsize is selected between [0.2,0.5]. The larger the problem size, the larger the stepsize.We first show the performance (in both optimal value and CPU time) of the approximate quasi-subgradient algorithm using the diminishing stepsize rule for different dimensions. The computation results are displayed in Table 1. In this table, QSM (resp. AQSM-R, AQSM-∊) denotes the exact quasi-subgradient method (resp. the approximate quasi-subgradient method with noise only, the approximate quasi-subgradient method with error only), the columns of Projects and Factors represent the numbers of projects and production factors of the problem (6.1) respectively,foptand CPU time denote the obtained optimal value and the CPU time (seconds) cost to reachfoptby each algorithm, respectively.From the results in Table 1, we can see that the quasi-subgradient type methods are highly efficient for the Cobb–Douglas production efficiency problem, even when the problem is large-scale. In the presence of persistent noise (R=1) or error (∊=1), there are some tolerances from the optimal value of the QSM, which is consistent with the theoretical analysis in the preceding section. We can also note that the AQSM-∊ achieves the better optimal value than the AQSM-R.The second experiment is performed to study the sensitivity analysis on noise and error, by using both the constant and diminishing stepsize rules. In this experiment, we fix the problem size 100×100, generate the noise and error series on [0,10], respectively. We characterize the performance by the relative error of optima value(f∗-fopt)/f∗, wheref∗is the optimal value obtained by the QSM.The numerical results, plotted in Fig. 2, are consistent with the theoretical analysis in Section 3. Although the constraint set of the problem (6.1) may be noncompact, the optimal solution and the iterates are always placed in some bounded area. Recall that Theorems 3.1 and 3.2 provide tolerances away from the optimal value of the formsμRd+v2(1+R)2p+∊andμ(Rd)p+∊,respectively by using the constant and diminishing stepsize rules, wherep<1asaj<1in the problem (6.1). In absence of the error ∊, the curves (plotted by ∘) of AQSM-R basically fit the exponential form of tolerance. When the noise R vanishes, the curves (plotted by □) of AQSM-∊ verify the linear dependence of tolerance on ∊.We further analyze the sensitivity behavior on noise and error simultaneously. The results are plotted in Fig. 3, where the left one is for the diminishing stepsize rule and the right one is for the constant stepsize rule. These results are also consistent with the theoretical analysis in Theorems 3.1 and 3.2.We also test the global convergence property of the QSM by randomly selecting initial starting points. We adopt the same diminishing stepsize rule as the one used in Table 1, that isvk=3/(1+0.1k), and start from several different initial points, either feasible or infeasible. As long as the iteration number is taken large enough, the sequence of the function values always converges to the same value. Also, the QSM starting from feasible points significantly outperforms when starting from infeasible points.

@&#CONCLUSIONS@&#
