@&#MAIN-TITLE@&#
Application of reinforcement learning for security enhancement in cognitive radio networks

@&#HIGHLIGHTS@&#
Cognitive radio leverages on reinforcement learning (RL) to enhance network security.There is lack of reviews on the application of RL to based security schemes.We cover the challenges, characteristics, performance enhancements, and others.

@&#KEYPHRASES@&#
Reinforcement learning,Trust,Reputation,Security,Cognitive radio networks,

@&#ABSTRACT@&#
Cognitive radio network (CRN) enables unlicensed users (or secondary users, SUs) to sense for and opportunistically operate in underutilized licensed channels, which are owned by the licensed users (or primary users, PUs). Cognitive radio network (CRN) has been regarded as the next-generation wireless network centered on the application of artificial intelligence, which helps the SUs to learn about, as well as to adaptively and dynamically reconfigure its operating parameters, including the sensing and transmission channels, for network performance enhancement. This motivates the use of artificial intelligence to enhance security schemes for CRNs. Provisioning security in CRNs is challenging since existing techniques, such as entity authentication, are not feasible in the dynamic environment that CRN presents since they require pre-registration. In addition these techniques cannot prevent an authenticated node from acting maliciously. In this article, we advocate the use of reinforcement learning (RL) to achieve optimal or near-optimal solutions for security enhancement through the detection of various malicious nodes and their attacks in CRNs. RL, which is an artificial intelligence technique, has the ability to learn new attacks and to detect previously learned ones. RL has been perceived as a promising approach to enhance the overall security aspect of CRNs. RL, which has been applied to address the dynamic aspect of security schemes in other wireless networks, such as wireless sensor networks and wireless mesh networks can be leveraged to design security schemes in CRNs. We believe that these RL solutions will complement and enhance existing security solutions applied to CRN To the best of our knowledge, this is the first survey article that focuses on the use of RL-based techniques for security enhancement in CRNs.

@&#INTRODUCTION@&#
Cognitive radio (CR) [1,2] is the next-generation wireless communication system that promises to address the artificial spectrum scarcity issue resulting from the traditional static spectrum allocation policy through dynamic spectrum access. With dynamic spectrum access, unlicensed users, or secondary users (SUs), can opportunistically exploit underutilized spectrum owned by the licensed users or primary users (PUs). Hence, CRs can improve the overall spectrum utilization by improving bandwidth availability at SUs. To achieve these functions, artificial intelligence (AI) techniques have been adopted in CR so that the SUs can sense, learn, and adapt to the dynamic network conditions, in which the PUs’ and malicious users’ activities appear and reappear. Cognitive radio networks (CRNs) can operate in both centralized and distributed settings: a centralized CRN consists of a SU base station (or access point) that communicates with SU nodes while a distributed network consists of SU nodes that communicate with each other in an ad-hoc manner.CRNs rely on cooperation for much of their functionality. While such a reliance on cooperative algorithms can make CRNs more efficient, this also opens CRNs up to numerous security vulnerabilities. One of the important requirements of CRNs is that SUs must minimize harmful interference to PUs. This requires SUs to collaborate amongst themselves to perform channel sensing and make accurate final decision on the availability of a channel. However, such collaboration among SUs may pose a security challenge to the SUs’ trustworthiness. For instance, in collaborative channel sensing, the legitimate (or honest) SUs depend highly on the dynamic allocation of a common control channel (CCC), which is used for the exchange of control messages during normal operations. However, the collaborating SUs may be malicious, and they may intentionally provide false sensing outcomes to interfere with the PUs or the other SUs, as well as to launch jamming attacks on the CCC, which adversely impacts performance and causes transmissions to come to a halt [3]. Hence, such SUs need to be detected and ignored in collaboration. The aforementioned discussion highlights that CRNs are susceptible to various attacks, such as channel jamming, eavesdropping or packets alteration. Further details on CRNs vulnerabilities, attacks and security threats can be found in detailed survey articles on this topic [4–6].The main security challenge in a CRN is that it operates in a dynamic set of licensed and unlicensed channels (in contrast to traditional wireless networks that typically operate with a fixed set of limited channels). In addition, the dynamic nature of the activities of PUs and malicious nodes requires SUs to change their operating channels from time to time: hence, longer-term knowledge is necessary so that SUs do not oscillate or constantly switch their actions within a short period of time. With this inherent characteristic, a mechanism to manage and learn from the ever-changing environment is needed to tackle the security challenge.Reinforcement learning (RL) is an artificial intelligence (AI) approach that helps a decision maker (or agent) to learn the optimal action through repeated interaction with the operating environment [7]. RL is an unsupervised and intelligent approach that enables an agent to observe and learn about the static or dynamic operating environment in the absence of guidance, feedback or the expected response from supervisors (or external critics), and subsequently make decisions on action selection in order to achieve optimal or near-optimal system performance. RL has been adopted in the literature [8–16] because it does not require prior knowledge of channel availability and it is highly adaptive to the dynamicity of channels characteristics. In addition, it enables decision makers (or agents) to learn and subsequently achieve near-optimal or optimal solutions in the dynamic environment that may be complex and large-scale in nature [7,8,16,17]. RL has been applied as an alternative to the traditional policy-based approach for system performance enhancement. The policy-based approach requires an agent to follow a set of static and predefined rules on action selection, and it has been traditionally applied to a wide range of application schemes in wireless networks [18].Several AI approaches, such as RL, artificial neural networks, rule-based system and game-based approaches, have been applied in CRNs to address the security challenge. A comparison of the strengths and limitations of these AI-based security approaches is presented in [19]. In this article, we will focus on RL-based security enhancements in CRNs.In the following, we will present some advantages of the application of RL to security enhancement in CRNs:(a)RL enables SUs to learn from experience without using an accurate model of the operating environment, and even without using any model, allowing the nodes to adapt to their dynamic and uncertain operating environment [20–22]. Through adaptation, RL is useful in the identification of SUs’ behavior, such as honest SUs that turn malicious [23]. In practice, an issue is that, obtaining the model is a complex procedure that requires high amount of processing and storage capabilities. The issue of obtaining an appropriate model is faced by many AI schemes, such as game theory (which has been applied to address jamming attacks [24–26]) and belief propagation (which has been applied to address primary user emulation attacks [27]). Using RL helps to solve this issue since RL can be applied in a model-free manner.RL enables SUs to make decisions based on a series of actions made, with the notion of maximizing long-term reward, which is more efficient. Using RL helps to solve the performance efficiency issue associated with the game-based approaches, particularly one-shot or repetitive games (e.g., potential game and matrix game), that have been applied to address jamming and primary user emulation attacks [24,28,29].RL enables SUs to explore new operating environment and exploit the knowledge gained so far. An issue is that, SUs may converge to a sub-optimal joint action when one of these conditions happens: actions with severe negative rewards exist or multiple high performance actions exist [30]. Using RL helps to solve such issue found in game-based approaches since RL has the flexibility to fine tune its policy as time progresses.In spite of the advantages being offered by RL, the application of RL to security enhancement has been limited as compared to other application schemes in wireless networks [31] such as routing, scheduling and topology management. Other machine learning algorithms, such as artificial neural networks, support vector machines and K-means have been widely discussed in [32] to tackle CR problems such as malicious SUs reporting inaccurate sensing outcomes in spectrum sensing. While there have been separate surveys on the security aspects of CRNs [4,6,17] and the application of RL to CRNs [33], there is lack of a comprehensive survey on the intersection of these two topics. This motivates our study of RL-based security enhancement schemes in CRNs in which we specifically focus on the application of RL as an effective tool to address security issues and provide security enhancement in CRNs. However, due to its novelty, we have also discussed RL models and algorithms applied in wireless sensor networks (WSNs) and wireless mesh networks (WMNs) in order to leverage to their RL approaches in CRNs.For clarity, the acronyms used throughout this article are summarized in Table 1; and the generic and specific notations applied in RL models (see Section 4), are presented in Tables 2 and 3. Note that, the generic notations cover the essential representations of RL, while the specific notations cover the additional representations to improve RL.The rest of the article is organized as follows. Section 2 presents an overview of RL and its representations. Section 3 presents a taxonomy of RL for security enhancement in CRNs with a minor focus on other kinds of wireless networks, such as WSNs and WMNs. Section 4 presents a survey on the application of RL to security enhancement schemes in CRNs with a minor focus on other kinds of wireless networks, such as WSNs and WMNs. Section 5 provides a discussion on RL performance and complexities. Section 6 presents design consideration for RL models applied to security enhancement schemes. Section 7 discusses open issues. Finally, we provide conclusions in Section 8.This section presents the generic model and algorithm, as well as the flowchart, of a popular RL approach, namely Q-learning. This section serves as a foundation for more advanced RL models and algorithms applied to security enhancement schemes presented in Section 4.Q-learning is an on-line algorithm in RL [7,34]. On-line learning enables an agent to learn in an interactive manner with the operating environment as the agent operates. Fig. 1shows an abstract view of RL, which can embedded in an agent (or a SU). RL consists of three main elements, namely state, action and reward. The state, which is observed from the operating environment, represents the factors that affect the way in which an agent makes a decision (e.g., the availability of a spectrum in an anti-jamming scheme). The state may not be represented in some RL models as the changes to the environment do not affect an agent's action selection, and hence, such model is called stateless. The reward represents the performance metrics to be maximized (e.g., detection rate), or minimized (e.g., probabilities of false positive and false negative). The action represents an action taken by an agent in order to maximize its reward (e.g., the choice of control and data channels to transmit messages in an anti-jamming scheme). For instance, with respect to a security enhancement scheme that applies reputation, statesj∈K,τi∈Srepresents the reputation value of a neighbor node j∈J, where J represents the set of neighbor nodes of node i[35,36], actionak∈K,ti∈Arepresents the selection of a neighbor node k by node i to forward packets toward destination [35,36], and rewardrm∈M,τi(ak∈K,τi)=+1represents a constant value to be rewarded to all nodes in a route after an episode eτ, which is the time required to establish a route. This means that a selected node for a route will only receive its reward after a route has been established [35,36]. Note that, the reward may be received at every episode eτ+1, which is comprised of a number of time instants, or at every time instant τ. As an example for the latter case, a node i chooses sub-channel k at time τ and receives rewardrk,τ+1i(aτi)=(S/N), which represents a signal-to-noise-ratio (SNR) value, at time τ+1 [37]. In the rest of this subsection, to enhance the readability, the notations are simplified by considering a single SU in the operating environment. For instance, we write state sτto represent statesj∈J,τialthough both refer to the state.Q-learning estimates the Q-values of state-action pairs Qτ(sτ, aτ). For each state-action pair, an agent observes its short-term reward rτ+1(sτ+1, aτ), and learns its future reward as time progresses. The short-term reward rτ+1(sτ+1, aτ) is received at time τ+1 after an agent has taken the action aτat time τ. The future rewardγmaxa∈AQτ(sτ+1,a)represents the cumulative rewards received by an agent at time τ+1, τ+2, ⋯ [7]. The short-term reward is called the “delayed reward” in RL terminology while the future reward is called the “discounted reward”. The Q-value Qτ(sτ, aτ) is updated as follows:(1)Qτ+1(sτ,aτ)←(1−α)Qτ(sτ,aτ)+α[rτ+1(sτ+1,aτ)+γmaxa∈AQτ(sτ+1,a)]where the learning rate 0≤α≤1 determines the extent to which the newly acquired knowledge overrides the previously learnt Q-value Qτ(sτ, aτ); and the discount factor 0≤γ≤1 emphasizes on the importance of future rewards. The higher the learning rate α, the greater the current learnt Q-value overrides its old value, and this speeds up the learning process which may lead to faster convergence; however, this may destabilize the learning process causing the agent fail to converge. On the other hand, the lower the learning rate α, the smoother the learning process albeit at the potential cost of convergence taking longer. If α=1, an agent considers the most current Q-value only. The higher the discount factor γ, the greater the agent relies on the discounted reward, which is the maximum Q-value of the next state-action pair. If γ=1, an agent considers the same weightage for both delayed and discounted rewards. If γ=0, the agent only considers maximizing the short-term delayed reward, and in this case, it is called a myopic approach.The state-action pairs and their respective delayed and discounted rewards are represented by Q-values, which are kept in a two-dimensional |S|×|A| Q-table. The action taken is based either on an optimal policy π* (using “exploitation” in the RL terminology) or a random policy (called “exploration”). When an agent selects an appropriate action for a particular state, the agent receives positive delayed reward, and so the respective Q-value increases, and vice versa. Hence, in order to maximize the cumulative rewardVπ*(sτ)(or the value function) over a period of time, an agent learns to take the optimal or near-optimal policy π* (or a series of actions), which has the optimal Q-value given a particular state as follows:(2)Vπ*(sτ)=maxa∈AQτ(sτ,a)Hence, the optimal policy is(3)π*(sτ)=argmaxa∈AQτ(sτ,a)Both exploration and exploitation complement each other. To explore, an agent takes random actions. While this may result in lesser accumulation of reward in the short term, it may lead to the discovery of an action that yields better accumulated rewards in the future. There are two popular exploration policies, namely ɛ-greedy and softmax action selection approaches. The ɛ-greedy policy chooses exploration actions with a small probability ɛ and exploitation actions with probability 1−ɛ. The softmax policy, on the other hand, chooses exploitation actions with probabilityeQt(a)/T/∑b=1AeQt(b)/T, where Qτ(s, a) represents the knowledge (or Q-value) of state s and action a at time τ, and T represents temperature. Higher T increases exploration rate while lower T increases exploitation rate, and so the temperature is reduced as time goes by. To exploit, an agent chooses an action that has the highest Q-value using Eq. (3) to provide the highest possible accumulated reward. To achieve an optimal or near-optimal performance, a balanced trade-off between exploration and exploitation is required. Hence, RL is suitable for most problems that require an agent to learn and re-learn from an uncertain or changing operating environment in order to achieve a given goal.As seen from Algorithm 1 and Fig. 1(a), at time τ an agent observes the current state sτ, chooses and executes an action aτfor the current state sτ. We consider the ɛ-greedy approach in this case. At time τ+1, an agent receives delayed reward rτ+1(sτ+1, aτ), which is the consequence of its action aτat time τ (see Fig. 1(b)). The agent also observes the state sτ+1 to identify its discounted rewardγmaxa∈AQτ(sτ+1,a). The Q-value Qτ(sτ, aτ) for state sτand action aτis updated using the delayed and discounted rewards rτ+1(sτ+1) is updated using Equation (1). The iteration repeats until the Q-values converge.Algorithm 1Q-Learning algorithm.Fig. 2represents Algorithm 1 as a flowchart with the respective step in the algorithm shown in the figure. The flowchart has been prepared for ease of comparison with the flowcharts for various RL algorithms presented in Section 4.In this section, we discuss the security vulnerabilities associated with CRNs and application schemes, with a minor focus on WSNs and WMNs. Our discussion covers the application and security enhancement schemes, the types of attacks as well as the challenges, characteristics, and performance metrics associated with the different security enhancement schemes. To provide further insights on this topic, and to make our article comprehensive in order to leverage on additional RL schemes, we also discuss RL models and algorithms applied to security enhancement schemes in WSNs and WMNs. A taxonomy of RL for wireless security enhancement is shown in Fig. 3, and it is discussed in the rest of this section.Generally speaking, there are two types of application schemes with security vulnerabilities in which RL has been applied to address in CRNs as follows:P.1Channel sensing. CR uses channel sensing technique, such as energy detection, to sense for white spaces. Generally speaking, exploration enables SUs to detect the white spaces at the earliest possible time, while exploitation enables SUs to efficiently access and utilize the channel(s) [38].Channel access. CR enables SUs to opportunistically access underutilized channels without causing unacceptable interference to the PUs’ activities. While this mechanism enables SUs to maximize the usage of the underutilized channels, it poses some security vulnerabilities whereby the malicious SUs can do likewise with the intention to jam the channels so as to deprive honest SUs from accessing the channels [39].In this subsection, we describe RL-based applications in WSNs and WMNs in which the RL models and algorithms applied to these networks can be leveraged to design security schemes in CRNs.Wireless mesh network (WMN) is a wireless network where each node can communicate directly with one or more neighbor nodes in the absence of fixed network infrastructure [40]. The nodes are made up of mesh routers and mesh clients that help to forward packets in multiple hops. A WMN is self-organized and self-configured in nature, in which the nodes can automatically establish and maintain connectivity among themselves throughout a distributed and dynamic network. RL has been applied to address the security vulnerabilities for the routing application in WMNs as explained below:P.3Routing. In WMNs, nodes rely on forwarding by their neighboring nodes for packet delivery through multi-hop routing/forwarding. A WMN is vulnerable to attacks as some nodes in the route may be malicious. As such, it is vital to ensure that nodes in a route are non-malicious. In previous work, RL has been applied to select a next-hop neighbor node [36].Wireless sensor network (WSN) comprises autonomous sensor nodes that collaboratively monitor the surrounding environment. The nodes send their sensing outcomes to a sink node that performs data fusion and make final decision on sensing outcomes. Sensor nodes are mostly battery powered and energy constrained. In addition, the limited computational capability and memory of sensor nodes, along with its energy-constrained nature, means that there is a high probability of node failure [41,42]. Thus, even though the sensor nodes are mostly stationary, the topology of WSNs can be dynamic due to node failure. In WSNs, RL has been applied to address the following application scheme with potential security vulnerabilities:P.4Data sensing/reporting. In WSNs, a sensor node must ensure the accuracy of the final decision on sensing outcomes. This requires both the sensor nodes and their respective upstream sensor nodes leading to sink nodes to be trustworthy. In previous work, RL has been applied to select honest upstream nodes [43].There are six distinct types of attacks that RL has been applied to address:A.1Byzantine attack. In this attack, malicious nodes appear to be honest nodes [23] and use their privilege to disrupt the communication of other nodes in the network without consideration of its own resource consumption [44]. As the malicious nodes appear to be honest, it can generate some mistrust amongst honest nodes.Unintentional attack. In this attack, non-malicious nodes unintentionally launch attacks (e.g., generating inaccurate sensing outcomes and making incorrect decisions) due to manipulation from malicious nodes, technological limitation (e.g., hardware and software errors), or due to environmental factors (e.g., sensors located in shadowing zone) [38]. As these attacks are unintentional, some measures need to be incorporated in the detection mechanism to ensure that these non-malicious nodes are not misdetected as malicious [45].Random attack. In this attack, malicious nodes launch attacks at random. For instance, malicious nodes introduce errors in the sensing outcomes [43], or jam the channels randomly [39]. Such attacks affect the honest nodes so that they are unable to predict the malicious nodes’ next course of actions. As these attacks are hard to predict, the honest nodes would need to learn and re-learn the next possible course of actions to be taken by the malicious nodes.Bias attack. In this attack, malicious nodes launch attacks systematically and intentionally in a collaborative manner. For instance, the malicious nodes may either send inaccurate sensing outcomes [43], or jam the channels in order to reduce channel access opportunities of honest nodes [39]. As these attacks are biased, the damaging effects to honest nodes may be higher than that of other attacks, such as random attacks.Jamming attack. In this attack, malicious nodes keep the network busy by constantly sending packets on a particular channel, or intentionally cause high interference to disrupt the network in order to reduce the signal-to-noise ratio significantly, and subsequently prevent efficient utilization of the channels [36]. A malicious SU may choose to launch an attack on the CCC to inflict maximum damage since many important operations, such as the exchange of control messages, take place on the CCC. These control messages may include sensing outcomes, routing information and coordination information of channel access [46].Intelligent attack. In this attack, malicious nodes maximize their attack performance by leveraging on AI techniques. For instance, in Sybil attack [45], which enables nodes to have multiple identities, the malicious nodes make use of RL to learn the optimal number of false sensing outcomes to subvert collaborative spectrum sensing. This is to prevent their malicious intentions being detected [47].There are five challenges associated with security enhancement schemes as follows:C.1Dynamicity of nodes’ behavior. Nodes may change from honest to malicious as time progresses, and vice versa [38]. RL has been applied to provide a dynamic and intelligent mechanism to monitor the nodes’ behavior at all times.Dynamicity of network topology. Nodes may be added or removed from wireless networks in the absence of a centralized base station. With such flexibility, it opens up security vulnerabilities as the malicious nodes could choose to leave the network either before they have been detected as malicious or otherwise [43]. RL has been applied to provide a network with honest nodes for collaboration, and hence, it improves the efficiency of the network.Dynamicity of attack strategies. Malicious nodes’ strategies may change dynamically and it is a challenge to keep track of the way (e.g., frequency) they attack [39]. RL has been applied to provide a dynamic and intelligent mechanism to learn malicious nodes’ strategies as time progresses.Dynamicity of channel access. During a channel switch, a SU chooses the next operating channel that provides at least, if not better, channel quality and bandwidth leading to network performance enhancement. However, malicious SUs may launch jamming attacks A(5) on those better channels, which are likely to be chosen, making the channel switch ineffective. RL has been applied to provide a better channel transition [48].Allocation of data and control channels. Optimal allocation of data and control channels maximizes CRN performance. The allocation is vulnerable to attacks as malicious nodes may choose to jam the control channel to achieve maximum effects. RL has been applied to learn the strategy for optimal channel allocation among the honest SUs [49].The two characteristics associated with security enhancement schemes are as follows:H.1Security enhancement scheme model: centralized H(1.1) or distributed H(1.2). There are two types of models, namely centralized and distributed models. Centralized model is normally embedded in a centralized entity, such as a base station or a decision fusion node in centralized networks; while distributed models are normally embedded in distributed entities, such as the hosts in distributed networks. In centralized model, a central node, such as a base station and a fusion center, may have the knowledge, such as sensing outcomes, of most of its hosts; while in distributed model, every node may have the knowledge of its neighbor nodes only.Availability of local RL-based information from neighboring nodes: available H(2.1) or unavailable H(2.2). The local RL-based information (i.e., state, selected action and received reward) from neighboring nodes may be available or unavailable to a node. The availability H(2.1) of the information enables nodes to make decisions on action selection without jeopardizing network performance of neighboring nodes; however, message exchanges may be necessary, and so it incurs control overhead. The unavailability H(2.2) of the information requires nodes to make decisions on action selection independently.The RL approach has been applied in two security enhancement schemes as follows:S.1Trust and reputation management (TRM). Cooperation enables nodes to achieve, through collective efforts, a common or network-wide objective. During collaboration, the collaborating nodes exchange information and use the collective information to make final decisions on the action selection. While authentication, authorization and access control may detect the malicious nodes or manipulated information, attackers may still exist and manipulate the decision [45,50]. Hence, it is necessary to detect malicious nodes and anomalous events, which deviate from an expected range of system behavior. In general, a threshold characterizes the expected range of system behavior. For instance, the operating parameter should be smaller than the threshold. TRM calculates the reputation value of each node, and subsequently uses the value to identify malicious users among its collaborating nodes.Higher reputation values indicate higher degree of legitimacy (or smaller deviations from the thresholds). RL has been applied in TRM to identify malicious nodes and address the challenge of dynamicity in the malicious nodes’ behavior C(1) [38].Anti-jamming. Due to the intrinsic nature of CRN that allows SUs to switch from one channel to another in order to maximize the use of the underutilized channels, this characteristic has opened up some security vulnerabilities. The malicious SUs may intentionally jam the white spaces by occupying them through constant transmission of signals in the white spaces [51] or by producing high interference to starve honest SUs and prevent them from transmitting [52]. Although malicious nodes may target other channels, an intelligent malicious SU node may prefer to jam control channels as it can cause denial of service in the networks. RL has been shown to be effective in tackling jamming [39] due to its ability to learn from the operating environment and adapt quickly to changes.The RL approach has been shown to achieve the following four performance enhancements.E.1Lower probability of false positive. False positive/alarm is triggered when honest nodes and their respective activities are incorrectly identified as malicious in nature. RL can help reduce the false positives/alarms [38,53].Higher detection rate. Detection rate is the accuracy of identifying malicious nodes. It can be measured in terms of the number of iterations or cycles required to detect the malicious nodes. RL can help improve the detection rate [43,53].Lower probability of missed detection. Missed detection happens when malicious nodes and their respective activities are incorrectly identified as honest nodes. With RL, the probability of missed detection is reduced [38,53].Higher utilization gain. Utilization gain can be measured by network performance in terms of data throughput, packet loss or delay. Higher utilization gain indicates higher data throughput, lower packet loss and lower delay. Lower utilization gain may be triggered by attacks, such as jamming activities caused by malicious SUs. RL can help improve the utilization gain [39,48,49].To further expound in Fig. 3, we present the insights on the role of RL in the aspects of applications, types of attacks, challenges, characteristics and security enhancement scheme in Table 4.This section presents RL models with respect to their applications, algorithms used, types of challenges and attacks for CRNs (see Sections 4.1–4.4). As a comprehensive survey article, we have included RL-based applications in WSNs (see Section 4.5.1) and WMNs (see Section 4.5.2), which are limited in the literature, in order to provide further insight on the leveraging of such RL models and algorithms to CRNs. To do this, we describe how to address the core challenges of CRNs (such as channel dynamicity) while providing support to other minor challenges in CRNs, such as energy conservation while leveraging the RL models to CRNs. Design considerations and a guideline for the application of RL to CRN is provided in Section 6.1. Table 5summarizes the RL models, which have been applied to security schemes in the literature. Table 5 also compares various aspects of RL models in the aspects of strength, attacks and challenges. The complexity of various RL algorithms has also been incorporated into the respective tables (Algorithms 2–7). For consistency purpose, we standardize the various notations used in the respective articles as shown in Tables 2 and 3.Vučević et al. [38] propose a RL model with suitability value for TRM S(1) applied to channel sensing P(1) in order to identify honest nodes for collaboration in CRNs. In this model, each node selects its neighbor nodes to collaborate by evaluating their suitability values, which are derived from the outcome of the final decisions. The suitability value is calculated using the softmax approach (see Section 2), and subsequently it is used to update the Q-function. Higher suitability value indicates higher probability that the node is chosen to collaborate. The purpose of the proposed RL model is to enable a node to monitor its neighbor nodes’ behavior as time progresses in order to identify honest nodes in the presence of Byzantine A(1) and unintentional A(2) attacks. Hence, the RL model addresses the challenge of the dynamicity of malicious nodes’ behavior C(1). The proposed RL model is a distributed model H(1.2), and it is embedded in each node. Each node makes decision independently without RL information exchange with neighbor nodes H(2.2). The proposed RL model has been shown to enhance the reliability of cooperation and specifically, it minimizes the probabilities of false alarm E(1) and missed detection E(3).Table 6shows the proposed RL model at node i for the TRM scheme in channel sensing. The state is not represented and so it is a stateless model. This means that the changes of the operating environment do not affect the node's action selection.Algorithm 2 presents the RL algorithm for the scheme at node i; while Fig. 4presents the flowchart of the algorithm. The flowchart is prepared for ease of comparison with the traditional RL algorithm presented in Section 2, as well as among the RL algorithms presented in this section. In this RL model with suitability value, the suitability value0≤ϕk,τi≤1increases with Q-value, and hence, the updated Q-valueQτ+1i(ak,τi)decreases with higher number of false alarmsrm,τ+1i(ak,τi)as shown in Eq. (4) in Algorithm 2. The table also shows the computational and storage complexities of the RL algorithm for a single node i. Since the RL is executed in a distributed network with N SU nodes, the network-wide computational complexity is O(N(N−1)|A|) and the network-wide storage complexity is ≤(N(N−1)|A|). Please refer to Section 5.2 for the assumptions made and notations used in complexity analysis.Algorithm 2RL algorithm for channel sensing embedded in each node i[38] and its complexity.Wang et al. [39] propose a RL model with minimax Q-learning for anti-jamming S(2) applied to channel access P(2) in order to maximize channel utilization in CRNs. This model learns the malicious nodes’ actions or strategies in a zero-sum game [54] in which the malicious nodes’ gains increase with decreasing gains from the honest nodes. This model updates the Q-value using a state value that is derived from the optimal policies by considering the worst scenario in which the malicious nodes adopt the best possible policies. The proposed RL model aims to enable SUs to maximize channel utilization in the presence of jamming attacks A(5). The malicious SUs may continually change their jamming strategies causing the honest SUs to also dynamically and strategically change their channel access policies in order to avoid the jammed channels. The malicious SU's objective is to optimize the effects of the attacks on SUs with limited jamming effort. Hence, the RL model addresses the challenge of the dynamicity of attack strategies C(3). The proposed RL model is a distributed model H(1.2), and it is embedded in each node. Each node makes decision independently without RL information exchange with neighbor nodes H(2.2). The proposed RL model has been shown to increase SU spectrum utilization gain E(4), which is defined as a function of throughput, packet loss and delay.Table 7shows the proposed RL model at node i for the anti-jamming scheme in channel access. As for the malicious node h (which is not shown in Table 7), the actionak∈K,τhrepresents malicious node h jamming spectrum k using previously un-attacked channels(ak,D1,τh)or jamming previously jammed channels (ak,D2,τh). Algorithm 3 presents the RL algorithm for the scheme at node i; while Fig. 5presents the flowchart of the algorithm. The table also shows the computational and storage complexities of the RL algorithm for a single node i. Since the RL is executed in a distributed network with N SU nodes, the network-wide computational complexity is O(N(N−1)(|A|)) and the network-wide storage complexity is ≤(N(N−1)|S||A|).Algorithm 3RL algorithm for channel access embedded in each node i[39] and its complexity.Wu et al. [48] propose a RL model with decreasing learning rate for anti-jamming S(2) applied to channel access P(2) in order to maximize channel utilization in CRNs. This model derives the learning rate, which is the reciprocal of the number of updates for the Q-values of state-action pairs. Hence, the learning rate in this model reduces as time progresses. The purpose of the proposed RL model is to enable the SUs to maximize their long-term rewards in the presence of jamming attacks A(5) that may be launched at random, or with certain collaborative strategy, to attempt to jam the channels. The malicious SUs collaboratively launch attacks in order to minimize the honest SU's utilization gain. This may cause the honest SUs to dynamically and strategically change their operating channels in order to avoid the jammed channels. The malicious SU's objective is to maximize the adverse effects of the attacks on SUs. Hence, the RL model addresses the challenges of the dynamicity of attack strategies C(3) and the dynamicity of channel access C(4). The proposed RL model is a distributed model H(1.2), and it is embedded in each node. Each node makes decision independently without RL information exchange with its neighbor nodes H(2.2). The proposed RL model has been shown to increase spectrum utilization gain E(4).Table 8shows the proposed RL model at node i for the anti-jamming scheme in channel access. In contrast to the traditional reward representation, a different reward function is applied to calculate the delayed reward according to the different kinds of states observed and actions taken.Algorithm 4 presents the RL algorithm for the scheme at node i; while Fig. 6presents the flowchart of the algorithm. The table also shows the computational and storage complexities of the RL algorithm for a single node i. Since the RL is executed in a distributed network with N SU nodes, the network-wide computational complexity is O(N(N−1)|A|) and the network-wide storage complexity is ≤(N(N−1)|S||A|).Algorithm 4RL algorithm for channel access embedded in each node i[48] and its complexity.Lo et al. [49] propose a RL model with policy hill-climbing (PHC) and win-or-learn-fast (WoLF) for anti-jamming S(2) applied to channel access P(2) in order to maximize the CCC utilization in CRNs. This RL model aims to approximate the gradient ascent approach, which is a variant of the gradient descent approach [49], to adjust the step size of policy updates according to PUs’ activities and malicious SUs strategies. Hence, when the PU's activity is low, WoLF increases the policy step size to ensure faster learning in order to avoid being attacked by the malicious SUs; and when the PU's activity is high (indicating low CCC availability), WoLF reduces the policy step size to delay malicious SUs’ strategies [55] in order to ensure convergence to a greedy strategy. Next, the step size is used by PHC to update the policy using the step size given by WOLF. This RL model is multi-agent in nature because the honest SUs collaborate amongst themselves through message exchange (i.e., the common control information) using PHC and WoLF to achieve the maximum reward. The purpose of the proposed RL model is to enable the SUs to find an optimal control channel allocation strategy in the presence of jamming attacks A(5). Hence, the RL model addresses the challenge of the data and control channel allocation C(5). The proposed RL model is a distributed model H(1.2), and it is embedded in each node. Each node makes decision independently without RL information exchange with neighbor nodes H(2.2). The proposed RL model has been shown to increase CCC utilization gain E(4). Table 9shows the proposed RL model at node i for the anti-jamming scheme in channel sensing.Algorithm 5 presents the RL algorithm for the scheme at node i; while Fig. 7presents the flowchart of the algorithm. Note that, δ is the step size for policy update where it brings a step closer to the optimal policy when the action taken maximizes the Q-value. The table also shows the computational and storage complexities of the RL algorithm. The table also shows the computational and storage complexities of the RL algorithm for a single node i. Since the RL is executed in a distributed network with N SU nodes, the network-wide computational complexity is O(N(N−1)|A|) and the network-wide storage complexity is ≤N(N−1)|S||A|). Note that, the complexity of PHC and WoLF are dependent on the algorithms themselves and hence, they are not considered in the complexity of this RL algorithm.Algorithm 5RL algorithm for channel sensing embedded in each node i[49] and its complexity.Mistry et al. [43] propose a RL model with discount factor γ=0 for TRM S(1) applied to data reporting P(4) in order to manage the reputation values of decision fusion centers in WSNs (see Section 3.1.1.2). This model makes use of the most current reputation values of the upstream nodes for collaboration. The reputation value is derived from the number of accurate and inaccurate final decisions made by the upstream node within a time window τ=t. Since the discount factor γ=0, the Q-value is updated using delayed reward only. The purpose of the proposed RL model is to enable a node to monitor its upstream nodes’ behavior as time progresses in order to identify honest upstream nodes, which play the role as decision fusion centers to aggregate sensing outcomes from downstream nodes, in the presence of random attacks A(3) and bias attacks A(4). Note that, with discount factor γ=0, the future or discounted rewards are not considered and so only next-hop upstream nodes are considered in this model. RL has been applied by downstream nodes to detect malicious next-hop upstream nodes. Hence, the RL model addresses the challenges of the dynamicity of malicious nodes’ behavior C(1) and the dynamicity of network topology C(2). The proposed RL model is a centralized model H(1.1), and it is embedded in each potential downstream node. Each node makes decision independently without RL information exchange with neighbor nodes H(2.2). The proposed RL model has been shown to be efficient and accurate in updating and calculating the upstream nodes’ Q-value, and hence, it increases the detection rate of malicious nodes E(2).Table 10shows the proposed RL model at node i for the TRM scheme in data reporting. The state is not represented and so it is a stateless model. This means that the changes of the operating environment do not affect the SU's action selection. Algorithm 6 presents the RL algorithm for the scheme at node i; while Fig. 8presents the flowchart of the algorithm. The table also shows the computational and storage complexities of the RL algorithm for a single node i. The RL is executed in a centralized network. The network-wide computational complexity is O(N|A|) and the network-wide storage complexity is ≤(N|A|).Algorithm 6RL algorithm for data reporting embedded in each downstream node i[43] and its complexity.Similar myopic approach has been applied to [53] for TRM S(1) in data reporting P(4) in order to detect malicious nodes in WSNs (see Section 3.1.1.2). The purpose of the proposed RL model is to calculate the nodes’ reputation values in order to identify honest nodes for collaboration in the presence of random attacks A(3). Hence, the RL model addresses the challenges of the dynamicity of malicious nodes’ behavior C(1) and the dynamicity of network topology C(2). The proposed RL model is a centralized model H(1.1), and it is embedded in a decision fusion node. Each node makes decision independently without RL information exchange with neighbor nodes H(2.2). The proposed RL model has been shown to enhance the reliability of cooperation, specifically, to minimize probabilities of false alarm E(1) and missed detection E(3), as well as to maximize the detection rate of malicious nodes E(2).Table 11shows the proposed RL model at decision fusion node i for the TRM scheme in data reporting. The state is not represented and so it is a stateless model. This means that the changes of the operating environment do not affect the agent's action selection.Maneenil et al. [36] propose a RL model with episodic rewards for TRM S(1) applied to routing P(3) in order to identify honest nodes for collaboration in WMNs (see Section 3.1.1.1). This model updates the Q-value with delayed reward only after an episode is completed. The purpose of the proposed RL model is to monitor next-hop neighboring node's behavior as time goes by in order to identify honest nodes in the presence of Byzantine attacks A(1). The identified honest nodes help to forward packets toward the destination node; while malicious nodes may discard the packets. The proposed RL model addresses the challenge of the dynamicity of the malicious nodes’ behavior C(1). The proposed RL model is a distributed model H(1.2), and it is embedded in each node. Each node makes decision independently H(2.2). The proposed RL model has been shown to increase spectrum utilization gain E(4).Table 12shows the proposed RL model at node i for the TRM scheme in routing. Note that, an episode etis required to establish a route. This means a selected node for a route will only receive its reward after a route has been established. Therefore, an honest node has higher Q-value because it has been regularly chosen to forward packets. Algorithm 7 presents the RL algorithm for the scheme at a single node i; while Fig. 9presents the flowchart of the algorithm. Since the RL is executed in a distributed network with N SU nodes, the network-wide computational complexity is O(N(N−1)(|A|) and the network-wide storage complexity is ≤(N(N−1)|S||A|).Algorithm 7RL algorithm for routing embedded in each node i[36] and its complexity.The effectiveness and usability of RL can be measured in terms of its performance enhancements and its complexity. Two scenarios, namely centralized and distributed CRNs, are presented for analysis. Section 5.1 tabulates the performance of various RL models in terms of different performance metrics, such as false positive, detection rate, missed detection and utilization gain. Section 5.2 discusses RL complexity in terms of computational and storage overhead complexities. Table 15 provides a summary of the RL complexities. The breakdown of the analysis can be found in Sections 4.1–4.5 where the RL models and algorithms are presented in details.Table 13provides a summary of the performance enhancements brought about by RL approaches in CRNs and other wireless networks. The performance metrics E(1)–E(4) have been previously discussed in detail in Section 3.6.RL approaches incur computational and storage costs in terms of the time taken to calculate Q-values for the SUs, and the memory requirement needed to store Q-values. This section aims to discuss the complexity analysis of a general RL approach, which has been applied in security context.In RL algorithms, SUs calculate the Q-values per time step. The values are subsequently used to detect malicious SUs. The following two types of general network models are considered:•In a centralized network, the upstream node serves as the decision fusion center. It calculates the Q-value of each of the downstream nodes based on their actions taken. There are N nodes randomly distributed in the network.In a distributed network, all SUs observe their neighbors’ action, as well as calculate and update their neighbors’ Q-values. There are N SUs randomly distributed in the network with each SU having at most N−1 SU neighbors.For simplicity, henceforth, the nodes and SUs are referred as SUs.In this article, we analyze RL algorithms with respect to computational and storage complexities associated with observing the state, taking an appropriate action and receiving a delayed reward in each learning cycle. The complexity analysis conducted in this section is inspired by similar investigation performed in [56].This section aims to investigate a general RL model with respect to computational and storage overhead complexities for the entire centralized and distributed networks, respectively. Table 14describes the parameters used in the complexity analysis while Table 15provides a summary of computational and storage overhead complexities for centralized and distributed networks. The breakdown of these complexities can be found in Algorithms 2–7.We define the following terms:•Computational complexity is the maximum number of times the RL algorithm is being executed in order to calculate the Q-values for all SUs in a network. The following calculation of complexities is for a simple and generic RL algorithm. In a centralized network, upon receiving actions from N SUs, the SU (fusion center) calculates and updates their respective Q-values, so the computational complexity is O(N|A|). In a distributed network, each SU receives at most N–1 actions, so the computational overhead is O((N−1)|A|) at each SU; hence, with N SUs in the network, the computational complexity is O(N(N−1)|A|).Storage overhead complexity is the amount of memory needed for the storage of the values during the course of a RL algorithm execution. Suppose, each SU maintains a table that keeps track of the Q-values. In a centralized network, each SU with non-stateless RL model, has |S||A| Q-values, so the storage overhead complexity is ≤N(|S||A|). In a distributed network, each SU with non-stateless RL model, has |S||A| Q-values, so the storage overhead is ≤(N−1)(|S||A|) at each SU; hence, with N SUs in the network, the storage overhead complexity is ≤N(N−1)(|S||A|).This section presents guidelines and design considerations for application of RL to security enhancement in CRNs.When considering application of RL for security enhancement in CRNs, a problem or open issue at hand needs to be identified and well understood. This includes the objectives and purposes, as well as the problem statement and research questions applicable to the problem. Subsequently, the following questions need to be answered. We present guidelines for the application of RL to CRNs in the light of a sample case study [39] that we will refer to throughout this subsection. In [39], RL with minimax Q-learning for anti-jamming is applied to channel access in order to maximize channel utilization in CRNs. Next, we define the state, action and reward for the anti-jamming scheme as follows:(a)Defining state. What are the decision making factors that an agent observe from the operating environment? For instance, in [39], the objectives are to counter jamming attacks in order to maximize spectrum utilization. Therefore, the agent represents the states with the presence of PU in spectrum j, node i gain (i.e., throughput), the number of jammed control channels in spectrum j, and the number of jammed data channels in spectrum j. Upon observing the state, the agent makes decision on its action based on the state.Defining action. What are the possible actions that an agent can take to maximize its rewards? For instance, in [39], with respect to the objective of avoiding jammers, the agent must choose the available control and data channels to transmit. It is expected that, by choosing an action in an intelligent manner, the agent chooses an unjammed channel. This allows the agent to receive higher rewards.Defining reward. What is the expected delayed reward received (or performance enhancement enjoyed) by an agent after it has taken an action in the state? For instance, in [39], the delayed reward is the spectrum gain when the agent selects a channel that is unjammed.Choosing an algorithm. What are the objectives of the algorithm? The main objective is to help SUs to learn the dynamic attack strategies from malicious SUs, who tend to optimize their attacks. Therefore, SUs must learn to take optimal actions in the presence of worst case attacks from malicious SUs. Hence, minimax Q-learning algorithm is chosen. The rest of the RL algorithms shown in Table 5 can be selected based on the main objective of the security scheme.This section presents some considerations that can be taken into account when designing a RL model for security enhancement in CRNs.In CRNs, where PUs’ activities can be dynamic and unpredictable, effective application of Q-learning algorithm to channel sensing and channel access requires some security considerations to be met with regards to defining states and actions, and assigning reward values. In some operating environment where only one state exists, the agent independently selects and performs an action, and receives a reward. The agents then updates its policy based on this reward, and the next iteration starts. Such an environment is static, i.e., no state transition occurs as can be seen in [38,43]. In [38,43], the SUs do not consider the state of the operating environment such as PU existence. On the other hand, RL allows the operating environment to be expressed in a comprehensive yet condensed format to represent the real world environment as seen in [39] in order to learn the hostile environment. Additionally, in channel sensing, instead of a fixed reward value, the value may be assigned according to the current scenario or activity in order to avoid or detect malicious SUs. For instance, the rewards in RL models [38,46] are derived from the number of channels accurately sensed and the number of valid CCCs selected, respectively. This reflects the currency of the given reward. Such reward variable can also be seen in RL models [39,48].The RL models discussed in this article are single-agent RL (SARL) except for [49], which is a multi-agent RL (MARL) approach. Previous works [57–60] have shown that SARL performance in partially observable, non-Markovian and multi-agent systems can be unsatisfactory. For instance, policy-gradient methods [59,60] have been shown to outperform RL, where the policy-gradient approach has been shown to be more efficient in partially observable environments since it searches directly for optimal policies in the policy space. While some work has been done in MARL environment [61,62], such as designing learning policies for CRNs, some aspect of security can be taken into consideration, such as incorporating TRM into the MARL environment to provide an additional layer of detection for malicious SUs.In an operating environment where attackers’ number and strategies are dynamic, learning rate in RL models can be adjusted according to the current scenario. An appropriate choice of learning rate will enable the SU to learn accurately from the operating environment. For instance, in [39], the learning rate decreases as the SU has learnt enough to exploit the operating environment, while in [46], the RL algorithm makes use of WoLF to adjust the learning rate based on the PU's activity.In an operating environment where the future reward accumulated by an agent is considered as important, the discount factor may be used for the purpose [17]. Higher value of the discount factor indicates stronger emphasis on maximizing the long-term reward. The RL parameter can be adjusted accordingly depending on the operating scenario emphasis. For instance, in [43,53], the short-term reward (γ=0) is used to detect malicious nodes, while in [48], the long term reward (γ=0.95) is used.State space explosion occurs when the RL algorithm is applied to a large-scale operating environment (which has an increased number of states and the size of each states). Such increase can incur an exponential growth in the learning time due to slower convergence and increased computational problems in terms of memory and speed. This is mainly due to the size of the Q-tables which increases growth exponentially. Such growth can have an adverse effect on RL algorithm's performance, as it may not be able to detect malicious SU in an efficient manner. Hence, a RL algorithm of this capacity and capability may need to find an alternative approach. In [63], batch RL is used to solve the state space explosion problem. The RL batch uses algorithms such as Fitted Q-Iteration [64] and Least-Squares Policy Iteration [65] to store state-action-reward tuples and process them in batches.This section discusses some open issues associated with the application of RL to security enhancement scheme that can be pursued.Yen et al. [66] show that, in a dynamic operating environment, it is possible for a Q-learning agent to be bounded in a small area of state space due to exploitation, and this may result in its inability to detect attackers’ behavior, such as honest nodes that turn malicious C(1). While it may be desirable to explore in a dynamic operating environment in order to increase the agent's flexibility to adapt to the changing environment, pure exploration may degrade the agent's learning capability [67]. Hence, the main challenge in exploration and exploitation is to find a balanced trade-off at the shortest possible time in order to achieve the maximum reward [68], such as higher detection rate E(2) and higher utilization gain E(4); and subsequently to incorporate the mechanism into RL algorithms. Traditional learning policies such as ɛ-greedy, Boltzmann exploration (softmax), simulated annealing and probability matching could also be studied and used for comparison. Note that, in such an investigation, it is also important to find the convergence results of the mechanism.While RL model has been shown to be effective in minimizing the probability of false positive [38] E(1), it has been noted in [43] that an increase in learning rate α value may increase false positives. For instance, if α=1, the agent considers only the most current Q-value, which may not be optimal or accurate especially when the agent is exploring the state space. The increase in false positives is detrimental to RL as it falsely reports attacks or malicious nodes when there are none, leading to inaccurate decisions. On the other hand, a decrease in learning rate α value increases the missed detections on malicious nodes. For instance, if α=0, the agent relies on the previous or old Q-values only, which again may not be accurate especially in a dynamic operating environment, where nodes’ behavior may vary as time progresses, such as honest nodes that turn malicious C(1). Hence, it is important to find an acceptable value for α so that false positive and missed detection rate can be at their lowest optimal in order to provide a more accurate and timely solution. Even-Dar et al. [69] show a relationship between the learning rate and the convergence rate, and their work can be further investigated to find the lower and upper bounds of these values in Q-learning algorithms.RL algorithms are expected to detect malicious nodes in wireless networks in the shortest possible time (or with the highest possible convergence rate) in most sizes of state spaces. An efficient RL algorithm needs to adopt a suitable policy that reflects the current status of the operating environment, where an honest node may turn malicious, and vice versa C(1). The update of a policy can be performed at every time instant (called immediate policy) or at the end of each epoch, which consists of a number of time instants (called epoch policy). Higher convergence rate indicates lower number of time instants and epochs needed to achieve the optimal policy. Using the epoch policy [70], the efficiency of RL algorithms may be influenced by the policy update frequency. However, using the immediate policy may also incur higher learning time and this may decrease the convergence rate. Hence, further work could be carried out to explore suitable policies for RL algorithms in various operating environment settings in order to improve convergence rate.While the majority of current works consider small state space only, the state space in a real-world environment may be large and dynamic in nature C(2) and C(4). For instance, multi-agent reinforcement learning (MARL) [40] faces the curse of dimensionality problem, which results in the exponential growth of the state-action pairs, when the number of agents increases. As a result, the computational complexity of the RL algorithm, which is the number of times the algorithm needs to be executed, increases exponentially too [40,71]. This may cause the algorithm to perform poorly leading to a longer time required to detect malicious nodes in the network. Investigation could be carried out to incorporate feature selection method, which is a preprocessing step in RL to remove the unimportant features in the state space [55,72]. This step reduces the dimension of data and it may improve the speed of detection of malicious nodes.Given the intrinsic nature of RL where the delayed reward is received at the end of the next epoch time in the epoch policy, it is worth studying the duration of each time epoch in which the attackers may leverage a longer epoch time to dynamically change their behavior C(1) and attack strategies C(3). In the epoch policy, the update is only done at the end of each epoch to minimize the computational complexity [70]. However, longer duration of each epoch may inadvertently open up opportunities for the attackers to improve their attack strategies. Further work can be carried out to study the implication and various duration of each epoch, and the maximum allowable epoch time in order to reduce the number of attacks.An important component of a RL-based security enhancement scheme is the construction of the reward function. By appropriately defining the reward function, a RL scheme can help increase the detection rate of malicious nodes while reducing the probabilities of false alarm and missed detection. In [73], an investigation was conducted on when, what and how much to reward in RL. When determines the moment which may be the end of each epoch, subtask or other interval of task, what is the objective function such as duration and accuracy, and how much determines the magnitude of a reward. Similar study can be carried out to investigate the feasibility of assigning a reward value based on the consequence or impact of the attacks. In [74,75], the authors constructed their reward functions based on the characteristic of the states. Further work can also be done to assign rewards based on the severity of attacks. For instance, when an agent experienced an intelligent attack A(6), it should receive much lesser reward than that of unintentional attacks A(2).In Ezirim et al. [47], the attackers launch intelligent attacks A(6) by leveraging RL to maximize the potency of attack. For instance, in CRNs, attackers launch RL-based Sybil attacks (a form of denial-of-service attacks) to learn the optimal number of false sensing outcomes to be sent to a fusion center without being detected as malicious. Such attacks may affect the accuracy of the final decisions on sensing outcomes, which may result in higher rate of false positives. Further work could be carried out to counter such intelligent attacks by using RL to predict the imminent attacks in the operating environment based on the SUs’ activities.A promising approach to detect malicious nodes in CRNs is to get the neighboring nodes to collaborate. As shown in [76], cooperative agents in multi-agent-based RL approach can significantly improve the performance of a joint task, and such cooperation has been shown to speed up the learning process and subsequently converge sooner as compared to independent agents [77]. Further work could be carried out to measure the effectiveness of applying cooperative agents in CRNs. In addition to measuring the speed of the detection of malicious nodes in a dynamic operating environment, performance metrics, such as higher detection rate E(2) could also be analyzed to ensure that the mechanism offers optimum speed of detection.

@&#CONCLUSIONS@&#
In this article, we presented an extensive review on the use of reinforcement learning (RL) to achieve security enhancement in cognitive radio networks (CRNs), as well as other wireless networks. RL is an unsupervised and intelligent approach that enables an agent to observe and learn about the static or dynamic operating environment in the absence of guidance, feedback or the expected response from supervisors, and subsequently make decisions on action selection in order to achieve optimal or near-optimal system performance. RL-based security enhancement schemes in CRNs are capable of learning new security attacks and to detect previously learned ones. RL-based security enhancement schemes have been successfully applied in a number of diverse problems, such as channel sensing, channel access, routing and data sensing/reporting. This article presents the performance enhancements of security enhancement schemes achieved by RL: lower probability of false positive and missed detection, higher detection rate, and higher utilization gain. Various RL models, such as RL model with suitability value, RL model with minimax Q-learning, and RL model with policy hill-climbing and win-or-learn-fast have been studied. This article also presents a complexity analysis of these RL models, and discusses a number of open issues associated with RL, such as balancing trade-off between exploration and exploitation, determining the learning rate value, and ameliorating the curse of dimensionality.