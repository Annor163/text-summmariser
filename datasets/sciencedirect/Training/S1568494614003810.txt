@&#MAIN-TITLE@&#
Rule inducing by fuzzy lattice reasoning classifier based on metric distances (FLRC-MD)

@&#HIGHLIGHTS@&#
In this work, we develop a fuzzy lattice reasoning classifier using various metric distances.As a consequence, the new algorithm named FLRC-MD shows better classification results and more generalization.Also, the proposed method generates fewer induced rules.A comprehensive experimental results is performed to assess the effectiveness of the proposed model, results obtained confirm the effectiveness of the proposed method.

@&#KEYPHRASES@&#
Classifier design,Rule inducing,Lattice theory,Distance metric,FLR,

@&#ABSTRACT@&#
Recently much more attention has been paid to the applications of lattice theory in different fields. Fuzzy lattice reasoning (FLR) was described lately as a lattice data domain extension of fuzzy-ARTMAP based on a lattice inclusion measure function. In this work, we develop a fuzzy lattice reasoning classifier using various distance metrics. As a consequence, the new algorithm named FLRC-MD shows better classification results and more generalization and it will lead to generate fewer induced rules. To assess the effectiveness of the proposed model, twenty benchmark data sets are tested. The results are compared favorably with those from a number of state-of-the-art machine learning techniques published in the literature. Results obtained confirm the effectiveness of the proposed method.

@&#INTRODUCTION@&#
A lattice [26] is a kind of algebraic structure that is an abstract model of the real world. Recently much more attention has been paid to the applications of lattice theory in different fields including neural networks [16]. Neural networks whose computation is based on lattice algebra are called morphological neural networks. Lattices are popular in mathematical morphology including image processing applications [13,20]. Moreover, algebraic lattices have been used for modeling associative memories [12]. One way and bidirectional lattice associative memories [8] have been proposed to overcome capacity limitations [4,5]. Furthermore, lattices are used implicitly in some neural networks such as fuzzy-ART and min-max [10,11]. A practical advantage of lattice theory is the ability to model uncertain information that may be comparable or incomparable. More specifically, the lattice theory provides instruments for granular computing, where uncertainty/ambiguity is accommodated in partially/lattice-ordered information granules [6,35–37,40,41].The term of a fuzzy lattice was proposed by Nanda in 1989 on the basis of the concept of a fuzzy partial order relation [19]. Several authors have employed a notion of ‘fuzzy lattice’ in mathematics with emphasis on algebraic properties of lattice ideals [21,24]. Furthermore, the notion of fuzzy concept lattice has been studied in [22,25,27].Fuzzy lattices have also been used in an array of computational intelligence applications regarding clustering, classification and regression [43–46]. More specifically, Kaburlasos et al. [14,18,42] proposed a fundamentally new and inherently hierarchical approach in neurocomputing named Fuzzy Lattice Neurocomputing (FLN). Moreover, Fuzzy Lattice Reasoning (FLR) classifier was introduced for inducing descriptive, decision-making knowledge (rules) in a mathematical data domain including the space RNand it has been successfully applied to a variety of problems such as ambient ozone estimation [16] as well as air quality assessment [15]. Sussner and Esmi [6] have introduced the morphological perceptron with a fusion of fuzzy lattice reasoning for competitive learning.In this work, we introduce a new algorithm named FLRC-MD which is an extension of fuzzy lattice reasoning classifier. Decision-making in FLRC-MD is based on a distance metric instead of an inclusion measure. As a consequence FLRC-MD shows better classification accuracy, less training time and more generalization than FLR and it will cause to generate fewer induced rules. The performance of the FLRC-MD algorithm is demonstrated by examples on various datasets including several benchmarks.The layout of this paper is as follows. In Section 2 the mathematical background on lattices and metric functions is reviewed. Section 3 explains our proposed model. Section 4 provides experimental results that demonstrate the performance of FLRC-MD. Finally, Section 5 summarizes the results of this work.A lattice (L,≤) is a partially ordered set, such that any two of its elementsa,b∈Lhave a greatest lower bounda∧b=inf{a,b}and a least upper bounda∨b=sup{a,b}.The lattice operations ∧ and ∨ are also called meet and join, respectively. A lattice (L,≤) is called complete when each of its subsets has a least upper bound and a greatest lower bound in L [26]. A non-void complete lattice has a least element and a greatest element denoted by O and I, respectively. The inverse ≤ of an order relation ≥ is itself an order relation. The order ≥ is called the dual order of ≤ symbolically ≤∂. A lattice (L,≤) can be the Cartesian product of N constituent latticesL1,…,LNi.e.L=L1×…LN.The lattice operations meet and join of the product lattice are defined as below:(1)a∧b=(a1,…,aN)∧(b1,…,bN)=(a1∧b1,…,aN∧bN)(2)a∨b=(a1,…,aN)∨(b1,…,bN)=(a1∨b1,…,aN∨bN)A valuation in a lattice L is a real-valued functionv:L→Rwhich satisfiesv(a)+v(b)=v(a∨b)+v(a∧b),a,b∈L. A valuation is called monotone iffa≤bin L impliesv(a)≤v(b)and positive iffa<bimpliesv(a)<v(b).The goal of positive valuation functionvis to deal with lattice elements. Note that choosing a suitable valuation function is problem dependent [16,38,39].Consider the set R of real numbers. It turns out that(R¯=R∪{−∞,+∞},≤)under the inequality relation ≤ betweena,b∈Ris a complete lattice with the least element −∞ and the greatest element +∞[17].For lattice (L,≤) we define the set of (closed) intervals asτ(L)={[a,b]|a,b∈Landa≤b}. We remark that(τ(L),≤)is a lattice with the ordering relation, lattice join and meet defined as below [17]:(3)[a,b]≤[c,d]≡c≤aandb≤d(4)[a,b]∨[c,d]=[a∧c,b∨d](5)[a,b]∧[c,d]=[a∨c,b∧d]Augmenting a least (empty) interval, denoted by [I,O], to(τ(L),≤)leads to a complete lattice(τO(L),≤)=(τ(L)∪{[I,O]},≤).Note that in case ofL=R¯the lattice(τO(L),≤)is equal to conventional intervals (sets) inR¯.. Our particular interest is the complete lattice(τO([0,1]),≤)with upper bound [0,1] and lower bound [1,0] which is a sublattice of(τO(R¯),≤).Based on the positive valuation functionvof lattice(L,≤)and an isomorphic functionθ:(L,≤∂)→(L,≤)a valuation functionvτOin(τO(R¯),≤)is defined as:(6)vτO([a,b])=v(θ(a))+v(b)In some applications, we may want to numerically express the differences of two objects by means of the distance of the corresponding sets. It follows that if two sets A and B have a large similarity, then they will have a small distance. A formal definition of distance metric is given in [23,58]. Many machine learning techniques such as nearest neighbor classifier, radial base function and c-means clustering are very sensitive on the choice of a proper distance metric used to compute distances between different examples. The way of selecting an appropriate metric for evaluating classification accuracy [59] is an old but still lively discussion. Several distance metrics, such as the Manhattan distance, the Euclidean distance and the Vector Cosine Angle distance have been proposed in the literature for determining similarity between feature vectors [60]. Euclidean distance and Manhattan distance are mainly applied to content-based image retrieval applications for measuring similarities between two images and in document retrieval systems, cosine based metrics are typically employed to identify similarity between a pair of documents. Over the past few years, there has been significant interest on distance metric learning. In this approach, Instead of choosing the metric manually, it is automatically learned from data [61,62]. In this section we introduce some metric functions capable of dealing with intervals of real numbers [1,2,3,7,16,31]. Note that in practical applications when we are faced with uncertain information, a neighborhood of values as defined by an interval can be fed to the system to compensate for uncertainty of measurements. We remark that a single real numbera∈Rcorresponds to the trivial interval [a,a]. For two N-dimensional hypercubesA=[a1,b1]×…×[aN,bN]andB=[c1,d1]×…×[cN,dN]the following distances between two intervals A and B are defined:•The normalized Euclidean distance:(7)d1(A,B)=12N∑i=1N[(ai−ci)2+(bi−di)2.The normalized Hamming distance:(8)d2(A,B)=12N∑i=1N(|ai−ci|+|bi−di|)A useful metric distance functiond:L×L→R0+in a lattice is defined as(9)d3(A,B)=vτO(A∨B)−vτO(A∧B)=∑i=1N[v(θ(ai∧ci)−v(θ(ai∨ci))+v(bi∨di)−v(bi∧di)]Modified De Carvalho distance(10)d4(A,B)=1N∑i=1NL([ai∧ci,bi∨di])−L([ai∨ci,bi∧di])+γ(2L([ai∨ci,bi∧di])−L([ai,bi])−L([ci,di]))L([ai∧ci,bi∨di])+αwhereL([a,b])denotes the length of the interval [a,b], i.e.L([a,b])=b−aifb≥a0else.In this work the FLRC-MD algorithm based on distance metricsdi,i=1,…,4is denoted by FLRC-MDii=1,…,4.We remark that d1,d2 are straightforward generalizations of distances used in the classical set theory, d3[26] is defined from a positive valuation in a lattice L and a so-called isomorphic function θ and the definition of d4, which is based on the join and meet operators, has been modified by adding a new parameter α. This parameter not only prevent divide by zero error but also the performance of FLRC-MD4 can be optimized by choosing different values of α. The parameter γ controls the effects of the inner-side nearness and the outer-side nearness between two intervals A and B. In our experiments bothγ,α∈(0,1]..The performance of the aforementioned distances is illustrated in the following example:Example: Let H1 and H2 be two hyperboxes and let P be a trivial hyperbox (point) shown in Fig. 1. Using metricsdi,i=1,...,4,The aim is to classify point P in one of the hyperboxes by calculating the corresponding distance between two hyperboxes H1, H2 and the point P as follows:d1(P,H1)=(0.3)2+(0.1)2+(0.1)2+(0.3)24≈0.2230d1(P,H2)=(0.1)2+(0.2)2+(0.2)2+(0.4)24≈0.2250d2(P,H1)=0.3+0.1+0.1+0.34=0.20d2(P,H2)=0.1+0.2+0.2+0.44=0.2250d3(P,H1)=d3([0.5,0.5],[0.2,0.4])+d3([0.2.0.2],[0.3,0.5])==[(0.8−0.5)+(0.5−0.4)]+[(0.8−0.7)+(0.5−0.2)]=0.8d3(P,H2)=d3([0.5,0.5],[0.6,0.7])+d3([0.2.0.2],[0.4,0.6])==[(0.5−0.4)+(0.7−0.5)]+[(0.8−0.6)+(0.6−0.2)]=0.9d4(P,H1)=((0.3−0.2γ)/(0.3+α)+(0.3−0.2γ)/(0.3+α))2d4(P,H2)=((0.2−0.1γ)/(0.2+α)+(0.4−0.2γ)/(0.4+α))2For different values of γ and α we have:γ=α=0.5⇒d4(p,H1)=0.250,d4(P,H2)≈0.2738γ=0.2,α=0.8⇒d4(p,H1)≈0.2364,d4(P,H2)=0.240γ=0.1,α=0.9⇒d4(p,H1)≈0.2333,d4(P,H2)≈0.2325γ=0.9,α=0.1⇒d4(p,H1)=0.30,d4(P,H2)≈0.4034According to the computation by the above metrics the distance between point P and the hyperbox H2 is larger than the distance between point P and the hyperbox H1. Thus, The point is classified in Hyperbox H1 as expected intuitively from Fig. 1 by inspection. The above example has demonstrated that all metrics produce common sense results just in the case of d4 it turns out that the outcome is counter-intuitive if the value of the parameter α is far larger than the value of γ.The main goal of a fuzzy inference system employing fuzzy if-then rules is to model human decision making and reasoning processes. Through the use of linguistic labels and membership functions, a fuzzy if-then rule can easily capture the spirit of a “rule of thumb” used by humans. The advantages of reasoning and knowledge representation with rules include modularity, uniformity and naturalness [48,50]. Fuzzy if-then rules have been widely used in both control and modeling. Models that employ the Takagi-Sugeno [51–57] type rules have been shown to be able to accurately represent complex behavior with only a few rules [49].This section presents a classifier called FLRC-MD for rule extracting from the input data based on lattice theory. It should be mentioned that an input datum to the network is as(ai→Ck)where Ckis the class label of the datum aiand it can be interpreted as a rule “if aithen Ck” FLRC-MD bases both its learning and generalization on the computation of hyperboxes in the space RNi.e. a rule induced by FLRC-MD corresponds to an N-dimensional hypercube. Suppose a rule baseRB={(a1→C1),…,(ac→Cc)}is given. RB can be empty at first. Decision-making in FLRC-MD is based on a distance metric instead of an inclusion measure. Once an input datum(a0→C0)is presented to the network, distance between input and each stored rules in RB will be calculated as{d(a0,a1),…,d(a0,ac)}.. The FLRC-MD will choose the rule withargmin{d(a0,ai)i∈{1,…,c}}as the winner. If the winner rule aJand input datum a0 have the same class label and the size of(a0∨aJ),, denoted by Z, is less than a user defined threshold then the winner rule will be updated. Note that the size of an interval [a,b] is computed asZ[a,b]=v(b)−v(a)Otherwise this process is repeated; if no more rules are left then the input datum(a0→C0)will be a new member of RB.FLRC-MD can cope with both points and intervals. Moreover, stable learning is carried out both incrementally and fast in a single pass through the training data. Algorithm for training is described in the following:FLRC-MD training algorithmS0. The first input(a0→C0)is memorized. At an instant, there are c KnownclassesC1,…,Ccmemorized in the memory, initially c=0.S1. Present the next input(ai→Ci),i=1,…,mto the initially “set” family of rules.S2. If no rules are “set” thenStore input(ai→Ci),c=c+1,Go to S1.ElseCompute the distancesd(ai,ak),k=1,…,cof the “set” rules.S3. Competition among the “set” rules:Winner is rule(aJ→CJ)such thatJ=argmin{d(ai,ak)k∈{1,…,c}}.S4. The Assimilation Condition:BothZ(ai∨aJ)≤ρandCi=CJ.S5. If the Assimilation Condition is satisfied thenReplace aJbyai∨aJ..Else“reset” the winner(aJ→CJ),Go to S2.Note that ρ is the threshold size which specifies the maximum size of a hyperbox to be learned. The decision boundaries which can be formed by FLRC-MD is illustrated in the following example.Example: The spiral problem is a nontrivial benchmark for testing the performance of a learning algorithm [28]. The training set points forms two 2D spirals. The points of the two spirals obey the following equations. Forn=1,...,15,(xn,yn)=(rnsinαn+0.5,rncosαn+0.5)wherern=0.4((105−n)/104),αn=(π(n−1))/16.If the points(xn,yn),n=1,…,15belongs to the first spiral then the(1−xn,1−yn),n=1,…,15points belongs to the second spiral. Here due to lack of space we have used 30 points which have been selected randomly. The decision surface and induced rules generated by FLRC-MD are depicted in Fig. 2.In this section, we evaluate the classification performance and the computational effort of the FLRC-MD model in a series of experiments on 20 well-known datasets, where All the benchmark data sets can be obtained either from the Carnegie Mellon University's neural-net benchmark collection [29] or from the University of California Irvine repository of machine learning database [9]. For some of the benchmarks training and testing set are given explicitly. In some other data sets when the data are not given separated into a training set and a testing set, we have divided all the data into a training set consisting of 70 percent of patterns which have been selected randomly and a testing set consisting of the remaining patterns. In order to have a fast simulation for large data sets including Hand recognition, Chess, Magic and Nursery we have used 10 percent of data for whole training set. Furthermore, this amount of data was enough to train the networks. Just for Lung and Balloon benchmarks we have used the same training and testing set due to small amount of data.This data was used to illustrate the power of the optimal discriminant plane even in ill-posed settings. The data set includes 32 56-dimensional vectors which describes 3 types of pathological lung cancers.This data consist of 32 samples distributed in two classes. Each sample is a 4-dimensional vector. The aim is to predict if a balloon is inflated or not.These data are the results of chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. One hundred seventy-eight data vectors are given distributed by 59, 71, and 48 in three wine types. The aim was the correct classification to one of the three wine types.The aim was the correct identification of the type of radar returns in one of two classes from a vector of 34 attributes. The classes are “good” radar returns which show evidence of some type of structure in the ionosphere, and “bad” radar returns which do not.This data set was selected because its familiarity to the pattern recognition community allows a measure of relative performance. The Iris data consist of 150 4-dimensional patterns in three separate classes, 50 for each class. The aim was the correct classification to one of the three iris types.The data are 9-dimensional vectors that specify various chemical elements in two types of glass, these are float processed and non-float processed window glasses. Eighty seven vectors are given for float processed and 76 vectors are given for non-float processed window glasses. The aim was the correct identification of a glass in one of two classes.This data set is generated from the “breast cancer Wisconsin-Diagnostic” data set. It encompasses the symptoms by which a judgment about classification of a tumor into two classes benign or malignant is made. A benign tumor is one that is not likely to cause a death. But a malignant tumor has advanced a lot and gets uncontrollable and is likely to lead to death. This data set includes 569 instances. Each instance of data set uses 30 attributes out of 32.The Soybean data set consist of 683 35-dimensional patterns in 19 separate classes. There are 35 categorical attributes, some nominal and some ordered.The data set was used to demonstrate the RFMTC marketing model. 748 samples at random from the donor database of Blood Transfusion Service Center in Hsin-Chu City in Taiwan have been selected. Each sample is a 4-dimensional vector. The aim was to predict whether a donor donated blood in March 2007.The Sonar benchmark was employed from Carnegie Mellon University's (CMU) collection of neural net benchmarks. The data are 60-dimensional vectors that correspond to sonar signals bounded off either a metal cylinder or a cylindrical rock. 104 vectors are provided for training and another 104 for testing. The aim was the correct classification of sonar testing signals to one of the two classes: “mine” or “Rock”.This database contains 5 numeric-valued attributes distributed in 3 classes. Training and testing sets including respectively 132 and 28 instances are given explicitly.The data set describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images. Each of the patients is classified into two categories: normal and abnormal. The database of 267 SPECT image sets (patients) was processed to extract features that summarize the original SPECT images. As a result, 44 continuous feature pattern was created for each patient. The pattern was further processed to obtain 22 binary feature patterns. SPECT is a good data set for testing ML algorithms; it is spitted in training and testing sets including respectively 187 and 80 instances that are described by 23 binary attributesThe image segmentation data set was donated by the Vision Group, University of Massachussets, and is included in the Machine Learning Repository of the University of the California, Irvine. This data set consists of 210 samples for training and 2100 samples for testing. The data have 19 continues attributes. Each sample is described by 9 continuous attributes and corresponds to a 3×3 region that was randomly drown from an outdoor image. The images were hand segmented to create a classification for every pixel. The goal is to distinguish between seven different classes.The MONK's problem is derived from a domain in which each training example is represented by six discrete-valued attributes. The problem involves learning a binary function defined over this domain, from a sample of training examples of this function. The training set contains 124 vectors, while the testing set consists of 432 vectors.Ripley's synthetic dataset consists of data samples from two classes. Each sample has two features. The data are divided into a training set and a test set consisting of 250 and 1000 samples, respectively, with the same number of samples belonging to each of the two classes.This data set was taken from the Carnegie Mellon University's collection of neural net benchmarks. It consists of ten-dimensional vectors of the linear predictive coefficients with regard to eleven steady-state vowels of British English. The training set contains 528 vectors, while the testing set consists of 462 vectors. The aim is the correct classification of the testing data to their corresponding classes, these are the eleven steady-state vowels.This dataset consists of 1593 handwritten digits from around 80 persons were scanned, stretched in a rectangular box 16×16 in a gray scale of 256 values. Then each pixel of each image was scaled into a boolean value using a fixed threshold. Each person wrote on a paper all the digits from 0 to 9, twice. The commitment was to write the digit the first time in the normal way and the second time in a fast way.The format for instances in this database is a sequence of 37 attribute values. Each instance is a board-description for this chess endgame. The first 36 attributes describe the board. The last (37th) attribute represents the endgame positions: “win” or “no-win”, i.e. in 1669 of the positions White can win and in 1527 of the positions White cannot win.The data are generated by a Monte Carlo program to simulate registration of high energy gamma particles in a ground-based atmospheric Cherenkov gamma telescope using the imaging technique. There are 19020 10-dimensional vectors distributed in 2 classes.Nursery Database was derived from a hierarchical decision model originally developed to rank applications for nursery schools. The Nursery Database contains examples with the structural information removed. Because of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods. The data set includes 12960 8-dimensional vectors distributed in 5 classes.We remark that in our experiments non-numeric attributes are converted to a stream of 0's and 1's. Hence, in some cases it might cause to dimension increasing.Table 1shows briefly the characteristics of 20 selected benchmark data sets selected from UCI and CMU databases.

@&#CONCLUSIONS@&#
