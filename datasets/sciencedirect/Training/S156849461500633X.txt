@&#MAIN-TITLE@&#
Using metaheuristic algorithms for parameter estimation in generalized Mallows models

@&#HIGHLIGHTS@&#
We deal with the problem of parameter estimation in Generalized Mallows model (GMM).We deal with 22 real datasets, all of them but one created by the authors.We have designed two experiments varying the maximum evaluations allowed.Obtained results significantly improve the previous competing approaches.

@&#KEYPHRASES@&#
Generalized Mallows model,Parameter estimation,Metaheuristic algorithms,Search methods,

@&#ABSTRACT@&#
This paper deals with the problem of parameter estimation in the generalized Mallows model (GMM) by using both local and global search metaheuristic (MH) algorithms. The task we undertake is to learn parameters for defining the GMM from a dataset of complete rankings/permutations. Several approaches can be found in the literature, some of which are based on greedy search and branch and bound search. The greedy approach has the disadvantage of usually becoming trapped in local optima, while the branch and bound approach, basically A* search, usually comes down to approximate search because of memory requirements, losing in this way its guaranteed optimality. Here, we carry out a comparative study of several MH algorithms (iterated local search (ILS) methods, variable neighborhood search (VNS) methods, genetic algorithms (GAs) and estimation of distribution algorithms (EDAs)) and a tailored algorithm A* to address parameter estimation in GMMs. We use 22 real datasets of different complexity, all but one of which were created by the authors by preprocessing real raw data. We provide a complete analysis of the experiments in terms of accuracy, number of iterations and CPU time requirements.

@&#INTRODUCTION@&#
Although the problem of dealing with permutations is a long-studied topic, there has been remarkable interest in this field from the statistics and machine learning community in the last two decades. One reason for this is the increasing presence of problems whose basic data are rankings, which are represented as permutations (e.g. recommender systems, combinatorial optimization, preferences, etc.).Of the different ways of dealing with permutations/rankings in a compact manner, probabilistic models have received more attention than others [14]. Undoubtedly, the most famous probabilistic model for permutations is the Mallows model (MM) [32], introduced by C.L. Mallows in the late fifties. This model has recently experienced a noteworthy revival, and has been applied to problems of different nature: combinatorial optimization [5], supervised classification [7], preference modeling and collaborative filtering [47], content modeling [6], etc.The Mallows distribution is a distance-based ranking model which has a certain resemblance to the Gaussian distribution. It also belongs to the exponential family and is specified by two parameters: a permutation π0 which can be seen as the consensus ranking (the mode), and a spread parameter θ which determines how concentrated the different rankings are around π0, that is, how flat or peaked the distribution is. Perhaps the most famous extension of the Mallows model is the generalized Mallows model (GMM) [13]. In the GMM, again we have the mode or consensus permutation, π0, but instead of a single spread parameter θ, GMM requires n−1 spread parameters{θj}j=1n−1, where each θjaffects the jth position of the permutation. When an appropriate distance is used, the GMM can be seen as a multistage ranking model[15].In this paper, our aim is to estimate the parameters required in the GMM from a dataset of permutations/rankings by using a variety of widely tried and tested search-based metaheuristic (MH) methods. This task has been previously approached by using greedy heuristics and branch-and-bound (A*) algorithms [2,33,35]. However, the results achieved for GMM parameters are far from being as good as the ones obtained when learning the parameters for the Mallows model. The main problem lies in the fact that in the Mallows model the estimation of the parameters can be performed independently, first the consensus permutation (which simply comes down to the well-known Kemeny ranking problem [27]), and then the spread value, while in the GMM constructive algorithms need to estimate the corresponding θjsimultaneously with the consensus permutation that is being built step by step.In this study we propose to approach the problem by using both local and global search metaheuristic algorithms. This type of method searches for the entire consensus permutation all at once, and not constructively item by item as the Borda and A* algorithms do.As mentioned above, the main difference between GMM and MM is that now the problem does not come down to the Kemeny ranking problem, because the quality of a candidate permutation does not depend only on itself but also on the estimated θjvalues. Therefore, we need to interleave the estimation of candidate permutations with the estimation of their corresponding set of θjvalues, which, given a permutation, can be efficiently carried out by using a numerical algorithm for convex optimization [33]. As the inference engine for guiding the search we propose to apply the following MHs methods:•Iterated local search (ILS) methods [31], which are multi-start local search algorithms that try to escape from a current local optimum by perturbing it, and use the resulting configuration to seed a new hill climbing iteration. Regarding the hill climbing stage, we have considered two different neighborhoods.Variable neighborhood search (VNS) methods [21], which are also local search algorithms which alternate different neighborhoods to guide the search.Genetic algorithms (GAs). When addressing optimization problems by using global search, evolutionary algorithms [37] and, in particular, GAs [20] have proven to perform successfully in permutation-based problems [1].Estimation of distribution algorithms (EDAs)[29]. These are a novel family of evolutionary computation algorithms based on modeling a population by using probabilistic models. Here we have considered two specific algorithms to deal with permutations: a node-based histogram EDA (NHM) and an edge-based histogram EDA (EHM).Thus, we have carried out a comparative study of the selected MHs algorithms (ILS, VNS, GAs and EDAs) and the branch-and-bound (A*) algorithm, which was specifically designed to cope with the target problem by Ali and Meila [2,33,35]. We use 22 real datasets of different complexity, all but one of which were created by the authors by preprocessing real raw data (see Section 5.1).As the experiments reported here demonstrate, the proposed MHs algorithms obtain better results (statistically significant) than the competing approaches considered. Obviously, MH algorithms need far more time than the Borda algorithm. However, given a dataset, the learning task is usually performed only once and so can be run off-line. On-line inference tasks will benefit from repeatedly using a good (fast) model. The use of MHs is widespread when the goal is to obtain accurate models at the expense of time consumption [10,17].Our paper is organized as follows. In Section 2 we provide preliminaries and basic notations regarding rankings and the GMM. In Section 3 we review related/competing approaches to dealing with the problem of parameter estimation in GMM. Section 4 is devoted to describing the proposed MH algorithms. In Section 5 we describe our experimental study, detailing the datasets, methodology and results. We have designed two kinds of experiments by varying the number of evaluations that an algorithm is allowed to carry out, in order to test different features of the MH algorithms. Finally, in Section 6 we present our conclusions.In this section we introduce the notation as well as some background on the problem under study.Suppose we have to rank n items labeled 1, 2, …, n. Then, any permutation π of these items represents a ranking. The space of all the possible rankings forms the symmetric groupSn:Definition 1(Symmetric group)[12] The symmetric groupSnis the group whose elements are the permutations of the n symbols {1, 2, …, n}, and whose group operation is the composition of such permutations, which are treated as bijective functions from the set of symbols to itself. Since there exists n! different permutations of size n, the order of the symmetric group is n!.We will write π=(x1x2, …, xn) to indicate that x1 is ranked as the first item, x2 as the second one, and so on. We will also denote by π(j) the jth element of π.To compare two given rankings or permutations, distances are used as the common tool. Although different distance measures are available in the literature, the Kendall tau distance is usually considered for the definition of Mallows and generalized Mallows distributions (see Sections 2.1 and 2.2).Definition 2(Kendall distance [28]) The Kendall distance d(π, σ) between two rankingsπ,σ∈Snis defined as the total number of item pairs over which they disagree. There is disagreement over an item pair (i, j), 1≤i<j≤n, if the relative order of i and j is different in π and σ. Hence(1)d(π,σ)=∑i≺πj1[j≺σi],where 1[·] is the indicator function and i≺πj means that i precedes j in the permutation π.More precisely,d(π,σ)=|{(i,j):i<j,(σ(i)<σ(j)∧π(i)>π(j))∨(σ(i)>σ(j)∧π(i)<π(j))}|Given two permutations π, σ∈Sn, Vj(π, σ), j=1, …, n−1, is defined as [33]:(2)Vj(π,σ)=∑i=j+1n1[π(i)≺σπ(j)]Thus, V1 stands for the number of elements ranked in π after π(1) and ranked in σ before π(1); V2 corresponds to the number of elements ranked in π after π(2) and ranked in σ before π(2), and so forth (see [13] for more details).Note that Vjtakes values in {0, …, n−j}. Moreover, it is possible to determine π from σ and the n−1 integer values of V1(π, σ), V2(π, σ), …, Vn−1(π, σ) [35].Using (1) and (2), the Kendall distance d(π, σ) can be expressed in terms of the functions Vj(π, σ) asd(π,σ)=∑j=1n−1Vj(π,σ).When a set of permutations Π={π1, π2, …, πN} is considered, a precedence matrixQ=Qiji,j=1:nis useful in order to compute the consensus permutation, that is, the Kemeny ranking of Π [27]. Specifically, Q is computed as(3)Qij=1N∑k=1N1[i≺πkj].In other words, Qijrepresents the fraction of times that item i is ranked before item j across all the N rankings.Moreover, givenσ∈Snwe defineV¯j(Π,σ)as the average of the Vj(πi, σ)'s, that is,V¯j(Π,σ)=∑i=1NVj(πi,σ)N.Observe also (see [33]) that(4)V¯j(Π,σ)=∑i=j+1nQσ(i)σ(j).The Mallows model (MM) [32] is a distance-based probability distribution over permutation spaces which belongs to the exponential family. Given a distance over permutations, it can be defined by two parameters: the central permutation π0, and the spread parameter θ.Definition 3(Mallows model [32]) The Mallows model is the probability distribution that satisfies, for all rankingsπ∈Sn,P(π)=e−θ·d(π,π0)ψ(θ),where the ranking π0 and θ≥0 are the model parameters, and ψ(θ) is a normalization constant (see [13]).The parameter θ of the MM quantifies the concentration of the distribution around its peak π0. For θ>0, the probability of π0 is the one with the highest value and the probability of the other n!−1 permutations decreases with the distance from π0 and the spread parameter θ. For θ=0 the uniform distribution is obtained. Due to this behaviour, the Mallows distribution on the space of permutations is considered analogous to the Gaussian distribution on the space of real numbers.From now on, we assume that the Kendall distance is considered.The generalized Mallows model (GMM) was introduced by Fligner and Verducci in [13] as an extension to the MM [32].The parameters which define this probability distribution are n−1 positive real numbers θ1, θ2, …, θn−1, called dispersion parameters, and the consensus or modal ranking π0. The parameters θjquantify the disorder of the samples at each position of the rankings, and can be interpreted by viewing the ranking process as a sequence of independent stages. Thus, for the first position each judge selects, from all the items, the one that he considers the best (ranked first). The relative entropy at this position corresponds to θ1. In the second stage, each judge selects, from the n−1 remaining items, the one he considers the best (ranked second), and so forth (see [13] for more details).The problem of estimating these parameters differs from the estimation in the Mallows model, because it requires lower bounding a real-valued function of several discrete variables. It is not purely a combinatorial optimization task, as the estimation of the parameter π0 in the Mallows model was. In what follows, we will denote byπˆ0andθˆjthe estimated parameters for π0 and θj.Definition 4(Generalized Mallows model [13]) The generalized Mallows model proposed in [13] defines the following probability distribution over all rankingπ∈Sn:(5)P(π)=e−∑j=1n−1θj·Vj(π,π0)ψ(θ1,θ2,…,θn−1),where ψ(θ1, θ2, …, θn−1) is a normalization constant.Note that when θ1=θ2=⋯=θn−1, the GMM (5) reduces to the MM (3).Given a set of independent samplesΠ={π1,π2,…,πN},πi∈Sn, we wish to model them by using the GMM. To do this, our aim is to determine the parametersπˆ0,θˆ1,…,θˆn−1from Π such that the loglikelihood of the data (Π) is maximized with respect to the model obtained. More precisely, the data loglikelihood is expressed in terms of the GMM parameters as(6)logl(Π;π0,θ1,θ2,…,θn−1)=−N∑j=1n−1θjV¯j(Π,π0)+logψj(θj),whereψj(θj)=∑q=0n−je−qθj=1−e−(n−j+1)θj1−e−θj(see e.g. [35] for details).Maximizing the likelihood to estimate θj, j=1, …, n−1, when the modal ranking is known, becomes simply a matter of solving the implicit equation in one variable obtained by taking the partial derivative in (6) with respect to θj.

@&#CONCLUSIONS@&#
In this paper we have presented a comparative study of MH algorithms and a tailored one (A*) designed to address parameter estimation (i.e. model learning) in GMM.The results obtained by the A* algorithm are significantly worse than those obtained by the MHs. Regarding the metaheuristic algorithms, the statistical study shows that the local search algorithms which use the insert neighborhood (NI) and the GA stand out from the rest with respect to the accuracy (loglikelihood) of the model learnt. The difference among them arises when we reduce the number of fitness evaluations allowed. In fact, if a large number of evaluations is available, then the systematic exploration of the complexNIneighborhood is feasible and ILS(I) turns out to be the best method. On the other hand, when fewer evaluations are available, the GA and the VNS algorithms behave better. In this last case, the factor which makes VNS work better than ILS(I) is that VNS first uses the neighborhoodNX. This allows VNS to arrive at promising solutions faster, and so the number of iterations (and consequently the number of fitness evaluations) is smaller than when usingNIfirst.