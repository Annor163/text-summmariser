@&#MAIN-TITLE@&#
A factorization approach to evaluate open-response assignments in MOOCs using preference learning on peer assessments

@&#HIGHLIGHTS@&#
We approach the problem of assessing open-text questions in MOOCs by peer-assessment.Our method avoids the intrinsic subjectivity of the numeric grades given by graders.Experiments where made with real-world data collected from 3 Universities in Spain.Our method performs well when comparing discrepancies among instructors’ grades.

@&#KEYPHRASES@&#
Peer grading,Factorization,Preference learning,Ordinal and cardinal approaches,MOOCs,

@&#ABSTRACT@&#
Evaluating open-response assignments in Massive Open Online Courses is a difficult task because of the huge number of students involved. Peer grading is an effective method to address this problem. There are two basic approaches in the literature: cardinal and ordinal. The first case uses grades assigned by student-graders to a set of assignments of other colleagues. In the ordinal approach, the raw materials used by grading systems are the relative orders that graders appreciate in the assignments that they evaluate. In this paper we present a factorization method that seeks a trade-off between cardinal and ordinal approaches. The algorithm learns from preference judgments to avoid the subjectivity of the numeric grades. But in addition to preferences expressed by student-graders, we include other preferences: those induced from assignments with significantly different average grades. The paper includes a report of the results obtained using this approach in a real world dataset collected in 3 Universities of Spain, A Coruña, Pablo de Olavide at Sevilla, and Oviedo at Gijón. Additionally, we studied the sensitivity of the method with respect to the number of assignments graded by each student. Our method achieves similar or better scores than staff instructors when we measure the discrepancies with other instructor’s grades.

@&#INTRODUCTION@&#
Massive Open Online Courses (MOOCs) have attracted thousands of students from many parts of the world. These courses make university lectures available online and most of the times are free or have a very low cost. It is possible to find MOOCs of almost any kind of subject; even for highly experimental subjects, where students can make real experiments with simulated materials (microscopes, etc.) or can take real data from remote controlled devices [1]. The success of MOOCs is based on the fact that several providers are spin-off from the most reputable Universities. This is the case, for instance, of Academic Earth, Coursera, edX, Khan Academy or Novoed.The evaluation is an important part of the teaching process, and has to be addressed in order to make MOOCs useful to provide a feedback to students and to guarantee the quality of the titles given to graduates. The challenge is to evaluate a very large number of assignments that cannot be automatically evaluated in all cases. In some cases, the assignments include open-response (open-ended) questions whose evaluation requires human intervention.Since it is unpractical that an instructor evaluates all the assignments, or even a set of Teaching Assistants (TA), researchers have been exploring the possibilities of using methods previously employed in Journals or Conferences: peer grading or peer assessment[2–7]. The students of the course are asked to evaluate a small set of anonymized assignments submitted by other students. Additionally, these student-graders receive a set of detailed instructions (called rubric) in order to uniform the assessment. However, students typically have no experience in this task and then effective peer grading must deal with the effects of inconsistent subjective evaluation.Roughly speaking there are two kinds of methods to use peer grading in practice: cardinal and ordinal approaches. In cardinal peer grading, each grader returns a cardinal-valued assessment for each assignment. Grades are then numbers or categorical labels with a straightforward numerical semantics. This type of grades is really very useful since they transmit an accurate assessment. Graders classify the assignments in an ordered scale that it is hopefully assumed universally. The final grade given to an assignment is usually determined as the average (or median) of the corresponding peer-grades [3] given by the evaluators.However, the cardinal approach has severe shortcomings. Firstly, we have to expect that assessments will be affected by some graders’ bias that would deviate them with respect to the ideal ground truth. The presumed universality of the semantics of cardinal grades is not so general. Some students tend to give very high grades, while others (probably with different academic backgrounds) are less generous with their assessments. A way to overcome this problem is to get many grades for each assignment; then, the correct grade could be approximated averaging all available grades. Moreover, assuming that each assignment was graded by a big amount of students, it has been reported [4] that averages are more consistently accurate with respect to the rubric than the staff grades. We have confirmed this fact in the experiments reported at the end of the paper. But unfortunately, we only can obtain a reduced number of assessments for each assignment.In addition to the existence of different scales, there is a second shortcoming that has to be addressed in cardinal approaches: the batch effect. It has been observed [8–10] that an item tends to receive a higher score when it is evaluated in a batch of worse items than when it is evaluated in a group of better items. Fortunately, despite the graders’ biases, the ranking entailed by their assessments is coherent with the ground truth. In other words, the grades can be unreliable but the order is, in general, correctly assessed [11,8,9,2].The basic axiom of Ordinal peer grading methods is that the essential knowledge provided by inexpert graders is the ordering of their bunch of assignments. Notice that these orderings indicate a relative but not absolute quality assessment. Thus, ordinal approaches are committed to take advantage of the robustness of ordering against the alleged accuracy of cardinal grades.In this paper we present an approach that tries to combine the strengths of both views. The student-graders are asked to give cardinal grades. The whole set of evaluations of each grader is considered as a relative ordering; from them we obtain a set of pairs of preferences (also called preference judgments or comparisons). In this way we avoid ties: we only consider pairs of assignments with different grades. But in addition to student-graders we include a new grader, we call it the Gaussian grader. This grader compares assignments with significantly different average grades.We would like to emphasize the role of the artificial Gaussian grader. On the one hand, its preferences include somehow a calibration in the pure ordinal approach. The idea is that some assignments are clearly better than others. Then their comparison has to be present in the learning task, even if the assignments involved have not had any common grader.On the other hand, the comparisons induced by the Gaussian grader make harder the possibility of having islands in the grading network [4]. Notice that any ordinal grading algorithm needs to have each assignment connected to the rest through a chain of comparisons, otherwise the grades of each connected component are unrelated with the others.Finally, with the set of preference pairs of all graders we learn an utility function that returns higher values to better assignments. This function can be learned using preference learning methods like those presented by Herbrich et al. [12] or by Joachims [10]. In this paper, as it was done in the work by Díez et al. [7], we use a factorization method [13] to learn the utility function. This function estimates the consensus ordering of the assignments computing a ranking that can be easily transformed into a grading function for each assignment.An additional advantage of this approach is that it is straightforward to include additional features of the assignments or students involved in the evaluation process. Both assignments and graders are represented by feature vectors. If no other information is available, these features are just a binary codification of their identity. But the representation can be enriched with any information about the characteristics of the assignment, or previous academic history of the students. About this issue we have to be aware of ethic considerations, for instance not including features like gender or ethnicity; see [4]. But additional features may be crucial for the success in some cases; see [14,15].Other approaches perform preference learning by taking into account the degree of preference of one item over another, as the work of Wang and Fan [16]. This method starts from preference matrices given by users, where they express their degrees of preference with respect to a pair of products. Thus, it is, to some extent, a cardinal approach in contrast to ours, which is mainly an ordinal approach that takes into account cardinal differences only when they are statistically significant. Despite the drawbacks of cardinal approaches discussed previously, this method is also hindered by a common characteristic of the peer assessing tasks: each grader evaluates only a few assignments so the preference matrices given by the graders will be very sparse. The consensus matrix, computed as a weighted sum of the preference matrices, will be nearly zero and the underlying ranking will hardly be better than any other chosen at random.In the rest of the paper we review some related work, and then we make a formal presentation of our approach. The paper is closed with the report of the results obtained with two real word dataset obtained from a common assignment for Computer Science students of 3 Universities of Spain: A Coruña, Pablo de Olavide at Sevilla, and Oviedo at Gijón. Each dataset has 1327 grades given by 160 students to 175 assignments.The experiments compare the performance of the factorization method proposed here with a baseline that simply averages the grades given to each assignment. The comparison is established against the grades given by the staff instructors of each university, that assessed the whole set of assignments, not only those of their own Universities. Additionally, the experimental section presents a study of the sensitivity of the method with respect to the number of assignments that graders received to evaluate. We observe that the scores improve with this number, and that our method achieves similar or better scores than staff instructors when we measure the discrepancies with other instructor’s grades.

@&#CONCLUSIONS@&#
We have presented a factorization method to implement peer assessment. Our approach seeks a trade-off between cardinal and ordinal approaches. The algorithm presented in Section 3 learns from preference judgments to avoid the subjectivity of numeric grades. But in addition to preference judgments expressed by student-graders, we included other preferences: those induced from assignments with significantly different average grades.The algorithm presented uses a maximum margin approach solved using a SGD optimizer. It is fast and can be easily scalable to a large number of students.In addition, our method lacks some constraints present in other state-of-the-art approaches, such as the need of students’ self-grading or previous grading by instructors. In fact, the last constrain makes those approaches inapplicable to a very large number of assignments, which is one of the main characteristics found in MOOCs.The paper includes a report of the results obtained using this approach in a real world dataset collected in 3 Universities of Spain, see Section 5 for details. We compare the ranking obtained by our method with those given by a baseline and the staff instructors. The baseline was the average of the scores of student-graders. We found that when the assignments are not too hard for the students and they thoroughly understand the rubric, the performance of our models is better than the baseline and comparable with the rankings of the professional graders. In other case, the baseline and the models are similar and still can be comparable to those of the instructors.We also checked that the number of assignments given to each student-grader is important. In all cases the performance improves as this number increases.