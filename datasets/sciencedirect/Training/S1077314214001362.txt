@&#MAIN-TITLE@&#
Tracking object poses in the context of robust body pose estimates

@&#HIGHLIGHTS@&#
We track 3D object pose (translation+rotation) during human-object interactions.To do so we learn 3D spatial relationships between objects and different body parts.These mappings enable our tracker to automatically initialise and re-initialise.It can also select the best parts from which to make predictions about object-pose.Results are better than when predicting only from hands (common in the literature).

@&#KEYPHRASES@&#
Human-object interaction,Object localisation,Object tracking,Depth data,RGB-D,

@&#ABSTRACT@&#
This work focuses on tracking objects being used by humans. These objects are often small, fast moving and heavily occluded by the user. Attempting to recover their 3D position and orientation over time is a challenging research problem. To make progress we appeal to the fact that these objects are often used in a consistent way. The body poses of different people using the same object tend to have similarities, and, when considered relative to those body poses, so do the respective object poses. Our intuition is that, in the context of recent advances in body-pose tracking from RGB-D data, robust object-pose tracking during human-object interactions should also be possible. We propose a combined generative and discriminative tracking framework able to follow gradual changes in object-pose over time but also able to re-initialise object-pose upon recognising distinctive body-poses. The framework is able to predict object-pose relative to a set of independent coordinate systems, each one centred upon a different part of the body. We conduct a quantitative investigation into which body parts serve as the best predictors of object-pose over the course of different interactions. We find that while object-translation should be predicted from nearby body parts, object-rotation can be more robustly predicted by using a much wider range of body parts. Our main contribution is to provide the first object-tracking system able to estimate 3D translation and orientation from RGB-D observations of human-object interactions. By tracking precise changes in object-pose, our method opens up the possibility of more detailed computational reasoning about human-object interactions and their outcomes. For example, in assistive living systems that go beyond just recognising the actions and objects involved in everyday tasks such as sweeping or drinking, to reasoning that a person has “missed sweeping under the chair” or “not drunk enough water today”.

@&#INTRODUCTION@&#
This paper attempts to track the 3D pose of objects being used by humans. Although similar efforts have been made in the past, they have usually focused on the localisation of objects using bounding boxes (e.g. [1,2]). Here we try to recover the full 3D translation and orientation of objects over time, our ultimate goal being to automate deeper reasoning about human-object interactions and their outcomes. Tracking 3D object pose is challenging because the objects are often small relative to the person using them, fast-moving, and heavily occluded. However, we observe that humans are often able to estimate both the object class and 3D pose from the corresponding body pose alone, see for example Fig. 1a showing a person using a fully occluded mobile phone. From this observation we deduce that both the body-poses and relative object-poses seen in many human-object interactions feature reasonably high levels of consistency. Therefore, if body-poses can be estimated robustly our intuition is that object-poses can be usefully predicted from them. We examine this claim in this work.If good body-pose estimates are available then, as a simple first step, we might try to locate the corresponding object-pose based on the position of the participant’s hand. For example, this presumption is used as a first step in object localisation by [1,3,4]. However, where interactions are with larger objects (e.g. brooms) or are more complex (e.g. two-handed: Fig. 1b; or involving the transfer of objects between body parts: Fig. 1c) we believe that a more sophisticated framework is necessary. In particular, while the position of the dominant hand may sometimes serve as a good predictor for object translation, we anticipate that it may not always be the best (or the only good) predictor of object orientation.In order to test our ideas we set about the task of learning the 3D spatial and rotational relationships between body parts and objects during human-object interactions. This is in contrast to previous studies which have learned 2D spatial relationships between human-object centroids [2,5,6] or part-object centroids [7]. Additionally, where other work has attempted to learn aggregated models across the duration of the interaction (e.g. “hats are always on top of heads” [5]), a key aim of our approach is to determine when different body parts offer good predictions of object-pose. For example, the hands while picking up and putting on a hat, but the head once wearing the hat.In order to study the relationship between body and object, we learn a large number of body-pose→object-pose mappings from labelled training data. We have found that, in general, this mapping does not remain one-to-one during human-object interactions. That is, there are times during the interaction where nearby poses in body space map to very different poses in object space. This means that the problem is not suited to a pure discriminative approach where we infer each object-pose solely from the current body-pose. On the other hand, a pure generative approach where we gradually “update and test” the pose of the object between consecutive images is also likely to fail due to a lack of good image features (the object is often small, motion blurred and partially occluded).Instead, we propose that the problem of object tracking during human-object interactions is best addressed in a combined generative+discriminative (G+D) tracking framework (e.g. [8–10]). The idea is that the body→object pose mappings can be used in a discriminative strand, able to initialise tracking and to re-initialise at points in the interaction where the mapping is (near) one-to-one. However, for periods where the mapping is multivalued, we can rely on a second generative strand to gradually update the object-pose between frames and test against available image evidence. We bring about this combination using a particle-based Bayesian approach that extends our earlier work [11] on the importance sampling framework [12].Our wider research goal is to automate deeper reasoning about human-object interactions by computer vision systems. Previous works have studied human-object interactions in order to improve reasoning about objects (e.g. “is that a jug?” [1]), about human actions (e.g. “is he pouring water from the jug?” [6]) and about scenes (e.g. “where did he leave the jug?” [13]). By pursuing a more detailed description of object-pose during interactions we hope to pave the way for deeper reasoning about outcomes, such as “did he pour all of the water out of the jug?”, “has the floor by the table been thoroughly swept?”, or “what is she taking a photograph of?”. A critical ingredient in this type of reasoning is an accurate 3D description of the changing object-pose over time, and this is the specific aim of this work. Future applications could include vision-based assisted living systems for the elderly, able to reason about the upkeep of the home (e.g., cleanliness, consumption of foodstuffs) by observing human-object interactions in detail.We make the following contributions:•We present an approach that gives full 3D estimates of object-pose (translation and orientation) during human-object interactions. To the best of our knowledge this is the first paper to provide this level of detail from a single sensor ([14] do so with multiple, synchronised video cameras and static backgrounds).The approach is able to automatically initialise itself at the first frame, track subsequent object-pose changes with a generative particle set, and perform “soft” re-initialisations through the introduction of discriminative particles in variable numbers.These new predictions about object-pose can be made relative to any part of the body and we introduce methods for selecting the best parts for predicting: (i) translation; and (ii) rotation, given the current point in the interaction.By using a large dataset of labelled human-object interactions we are able to demonstrate quantitatively the value of the approach over the use of the hand (e.g. [1]) or randomly chosen body parts (e.g. [11]) for prediction. We also show the importance of the combined G+D scheme over a purely generative approach.

@&#CONCLUSIONS@&#
We have presented a system that is, to the best of our knowledge, the first able to track full 3D object-poses (translation and orientation) from RGB-D observations of human-object interactions. Our method allows independent predictions about object-pose to be made from each of the different parts of the body. We use these predictions to drive a combined generative and discriminative particle-based object-pose tracker. During tracking, the system constantly looks for opportunities to re-initialise particles based on the nature of the mapping between the body- and object-pose spaces. Where re-initialisation is possible, the best body parts from which to make predictions are selected automatically. We have found the optimal choice often proves to be different between the object’s rotational and translational components. Quantitative evaluation on a large dataset has enabled us to demonstrate robustly the importance of discriminative re-initialisation versus pure generative tracking, and the value of careful part predictor selection over random choice, or the use of the hands (as is common in the literature). In constructing the proposed approach we have also resisted making assumptions about de-cluttered, or static backgrounds, or about the visibility of particular classes of object in depth data; all of which are difficult to guarantee in real-world scenarios. By recovering precise changes in object-pose, the presented methods open up the possibility for more detailed computational reasoning about human-object interactions and their outcomes.