@&#MAIN-TITLE@&#
Low intrusive coupling of implicit and explicit time integration schemes for structural dynamics: Application to low energy impacts on composite structures

@&#HIGHLIGHTS@&#
Non-intrusive GC implementation.Industrial code-coupling.Simulation of low energy impact on laminated stiffened panel.Python interface to finite element codes.

@&#KEYPHRASES@&#
Implicit dynamics,Explicit dynamics,Code coupling,GC method,Composite materials,

@&#ABSTRACT@&#
Simulation of low energy impacts on composite structures is a key feature in aeronautics. Unfortunately it involves very expensive numerical simulations: on the one side, the structures of interest have large dimensions and need fine volumic meshes (at least locally) in order to properly capture damage. On the other side, explicit simulations are commonly used to lead this kind of simulations (Lopes et al., 2009 [1]; Bouvet, 2009 [2]), which results in very small time steps to ensure the CFL condition (Courant et al., 1967 [3]). Implicit algorithms are actually more difficult to use in this situation because of the lack of smoothness of the solution that can lead to prohibitive number of time steps or even to non-convergence of Newton-like iterative processes. It is also observed that non-smooth phenomena are localized in space and time (near the impacted zone). It may therefore be advantageous to adopt a multiscale space/time approach by splitting the structure into several substructures with their own space/time discretization and their own integration scheme. The purpose of this decomposition is to take advantage of the specificities of both algorithms families: explicit scheme focuses on non-smooth areas while smoother parts (actually linear in this work) of the solutions are computed with larger time steps with an implicit scheme. We propose here an implementation of the Gravouil–Combescure method (GC) (Combescure and Gravouil, 2002 [4]) by the mean of low intrusive coupling between the implicit finite element analysis (FEA) code Zset/Zébulon (Z-set official website, 2013 [5]) and the explicit FEA code Europlexus (Europlexus official website, 2013 [6]). Simulations of low energy impacts on composite stiffened panels are presented. It is shown on this application that large time step ratios can be reached, thus saving computation time.

@&#INTRODUCTION@&#
Low energy impacts can be very harmful for composite laminates used in the aerospace industry. They can actually cause significant damages (matrix cracking, delamination, fiber failure, etc.) inside the composite or on the side opposite to the impact. However, the residual print left on the impacted side can be almost undetectable to the naked eye. Induced damages can therefore lead to early failure of the structure while they can be unnoticed during a visual inspection, this is related to the concept of BVID (Barely Visible Impact Damage). Controlling such situations is essential for manufacturers. Numerical simulations of this phenomenon could be really helpful to orient and to rationalize test campaigns by the use of virtual testing as well as to understand scale effects. Various researches are led in the scientific community to simulate these impacts which is actually very difficult to carry out at industrial level. Implicit solvers can be used to deal with this type of problem with satisfactory results [7–9]. However non-smooth sources like contact, softening damage laws or cohesive zone models are often introduced in the models which can make convergence of implicit solvers difficult to achieve. Using explicit solvers to simulate low-energy impacts is an alternative way to handle this difficulty despite the non-dominance of high frequency terms or wave propagation in this context [1,2,10,11]. Explicit solvers are indeed more suitable to solve non-smooth problems. However, stability requires the use of very small time steps which linearly decrease with the characteristic length of the smallest mesh element. Moreover, very fine meshes are usually required (at least locally) to capture the non-linear phenomena occurring during an impact. This thus leads to a very large number of increments which can be prohibitive. Note however that these non-linear phenomena occur on a very localized area around the impact point.Adopting a space/time multiscale strategy thus appears to be advantageous to solve this kind of multiscale problems. This can be performed through domain decomposition where each subdomain owns its time discretization. The purpose of this decomposition is to focus on numerical computation where non-linear phenomena appear [12]. Explicit resolution in the area close to the impact is required because of the lack of smoothness of the solution. However, on the complementary area where the solution is smoother, implicit integration is appropriate. Larger time steps can then be used to save CPU time. The present work is based on the GC method [4] and aims at coupling semi-industrial finite elements codes (FEA) Zset/Zébulon11Zset/Zébulonis an implicit FEA code developed by Mines ParisTech, Onera and NW Numerics & Modeling, http://zset-software.com/and Europlexus.22Europlexus is an explicit FEA code developed by Commissariat à l׳énergie atomique (CEA) and the Joint Research Centre (JRC) in Ispra, Italy.Fig. 1illustrates the computation strategy. It shows one section of an impacted plate and a typical mesh. This mesh is divided into two domains: an impacted domain (center) which is processed by the explicit code Europlexus with a fine time step and a complementary domain which is processed by the implicit code Zset/Zébulon with larger time steps. In addition, low intrusivity is a key feature of the implementation when industrial applications are aimed [13,14]. In the present work this goal has been achieved through Python/NumPy high level scripting so that no coding was required into the native programming languages of Zset/Zébulon and Europlexus to implement the coupling algorithm itself.We describe in this section the algorithmic framework of this study. Space and time discretization of structural dynamics problems are firstly presented. Both implicit and explicit algorithms are expressed in terms of velocity unknowns. This unusual form is useful to handle both implicit and explicit schemes equations within the GC domain decomposition framework (see Section 2.3) which requires continuity of interfacial velocity. Gravouil and Combescure [4] indeed showed that imposing interfacial velocity continuity is mandatory to ensure stability when coupling arbitrary Newmark schemes with different time steps. Domain decomposition method written under this constraint is then presented for two domains. Extension of domain decomposition with different time steps within each sub-domain through the GC method is finally described.We consider here the finite element discretization of the principle of virtual power which leads to the semi-discretized equilibrium system of Eqs. (1) for an undamped structure [15]:(1){∀t∈[t0,tf],Mu¨(t)+fint(u(t))=fext(t)u(t0)=u0,u̇(t0)=u̇0whereMis the symmetric definite-positive consistent mass matrix,uis the nodal displacements vector,fintis the internal forces vector (fint(t)=Ku(t)for linear elasticity withKthe stiffness matrix) andfextis the external forces vector. Single and double superposed dots over a quantity denote respectively its first and second time derivatives. The initial displacements and velocities vectors are respectively denoted asu0andu̇0. t0 and tfdenote respectively the beginning and the end of the time domain of interest.The system (1) is then discretized in time to be solved numerically. A lot of time integrators can be found in the literature, see for instance [16–19]. However, due to GC method constraints, the present work is restricted to Newmark schemes [20]. A Newmark temporal integrator is defined by two parameters γ and β and involves relations (2) among displacement, velocity and acceleration vectors from time tnto timetn+1=tn+Δt.Δtis the time step,n∈〚0,nstep−1〛andnstep∈N⁎is the number of time step:(2){un+1=unp+βΔt2u¨n+1u̇n+1=u̇np+γΔtu¨n+1with displacement predictorsunpand velocity predictoru̇npgiven by the following relations:(3){unp=un+Δtu̇n+(1−2β)Δt22u¨nu̇np=u̇n+(1−γ)Δtu¨npwhere subscripted quantitiesxnare the approximations ofx(tn)at time tn.Introducing Newmark relations (2) in Eq. (1) leads to the following system of equations as long as γ is not equal to zero which never happens in practice for stability reasons [15]:(4)u¨0=M−1(f0ext−fint(u0))∀n∈〚0,nstep−1〛,{1γΔtM(u̇n+1−u̇np)+fint(un+1)=fn+1extun+1=unp+βΔtγ(u̇n+1−u̇np)u¨n+1=1γΔt(u̇n+1−u̇np)From this point, we focus on a step that consists in solving system (4) from time tnto timetn+1. The choice of the parameter β in the Newmark relations can lead to implicit or explicit systems in the mathematical point of view. Note however that due to CPU cost and programmation constraints, implementations of implicit and explicit algorithms are in practice very different. This is the reason why code coupling is of major importance in this work (see Section 3).When β is set to 0 the system (4) can become explicit ifMis replaced by the lumped mass matrix notedMlump[21]. Non-linearities are also naturally handled without requiring any iterative process. The explicit system is obtained as follows:(5){A1u̇n+1=b1un+1=unpu¨n+1=2Δt(u̇n+1−u̇np)Note that in explicit finite element codes, γ is generally set to 1/2.A1is the explicit positive definite diagonal operator defined by(6)A1=2ΔtMlumpand(7)b1=fn+1ext−fint(unp)+A1u̇npAlthough there is no system to solve, an explicit algorithm remains stable as soon as the Courant–Friedrich–Levy (CFL) condition is ensured [3]. In composite laminates damage simulations, at least one element in the ply thickness is required, thus leading to critical time steps of the order of1×10−8s. This time step is then really small in comparison to the time range of the study (up to10×10−3s). A lot of time steps are therefore required that is why one may want to use unconditionally stable implicit algorithms with larger time step when it is possible (i.e. where and when local solution is smooth or mainly governed by low frequencies) to save CPU time.In this study, the implicit solver will be applied only to linear problems. Under this assumption, the system (4) can then be expressed as follows:(8){A2u̇n+1=b2un+1=unp+βΔtγ(u̇n+1−u̇np)u¨n+1=1γΔt(u̇n+1−u̇np)withA2being the implicit positive definite symmetrical operator which is defined by(9)A2=1γΔt(M+βΔt2K)where(10)b2=fn+1ext+A2u̇np−KpunNote that the first equation of system (8) could also be expressed in terms of acceleration or displacement unknowns by rearranging the system (8).Different multiscale space/time approaches can be found in the literature [4,22–28]. These can be viewed as extensions of dual Domain Decomposition (DD) methods, mostly used in parallel computing [29,30]. DD methods consist in spatially splitting a structure into several subdomains and search solution on each subdomain as independently as possible (an example of structure splits into two subdomains named Ω1 and Ω2 is shown in dotted box in Fig. 2). The two main key points in DD methods are (i) determining the boundary conditions that must be applied on the subdomain interfaces and (ii) solve the interface problem in an efficient way. The interfacial boundary condition should indeed ensure both kinematics continuities and equilibrium at the interface.For the sake of simplification we consider in this section only two subdomains (Ω1 and Ω2). Applying a dual DD technique under the constraints to satisfy velocity continuity and equilibrium at the interface [31], the first equation of system (4) leads to system (11) for compatible meshes at the interface [32,29]. At this stage we may note that from the discrete point of view, imposing the continuity of the displacement, velocity or acceleration is not equivalent [4]:(11)[A10−C1T0A2−C2T−C1−C20][u̇1u̇2λ]=[b1b20]Ad,u̇dandbdare the quantities previously introduced in Section 2.1.1 which are relative to the subdomain Ωdd∈〚1,2〛.CdTis the transposed interface operator (trace operator), it is a signed booleannd×nd∣Γmatrix where ndis the number of degree of freedom (DOF) of the subdomain Ωd andnd∣Γis the number of DOF on the interface.λis the unknown vector of interfacial forces (the size of this vector isnd∣Γ). System (11) can then be solved with the following three steps:1.Free problems: Free interface velocity vectors of subdomains 1 and 2 respectively denoted asu̇1∣Γfree=C1u̇1freeandu̇2∣Γfree=C2u̇2freeare computed by solving (12) and projecting free velocities obtained (u̇xfree,x∈〚1,2〛)on the interface with trace operator:(12){A1u̇1free=b1A2u̇2free=b2System of Eqs. (12) can be solved in parallel. Note also that it can be solved in terms of displacements by rearranging Newmark relations (2).Interface problem: Interface problem is solved to determine interface forces vectorλ. This problem is obtained by condensing (11) on the third line which leads to the following equation:(13)[C1A1−1C1T+C2A2−1C2T]︸sumofSchurcomplementsλ=−(u̇1∣Γfree+u̇2∣Γfree)︸velocitiesobtainedfromfreeproblems(step1)System (13) can then be solved directly after building the interface operator (based on domains Schur complements) or be solved through iterative solvers which do not require explicit computation of the interface operator [32,30]. In fully implicit DD analysis, iterative solution is often advantageous.Linked problems: Solving the linked problem (14),(14){A1u̇1=b1+C1TλA2u̇2=b2+C2TλThis problem is similar to step 1 but interface nodal forces are now taken into account. This step can also be performed in parallel.The additional feature of multiscale time methods in comparison to a single time-scale DD method is that each subdomain may have its own time stepping.LetΔtbe the finest time step associated with the explicit subdomain Ω1 and letΔTbe the largest time step associated with the implicit domain Ω2. m is the ratio between these two time steps:m=ΔT/Δt,m∈N⁎. The interface problem has to be written in order to satisfy the kinematic continuities and the balance equations in time. The GC method proposes to ensure both the velocity continuity and the interface equilibrium on the fine time scale. As mentioned above, imposing the continuity of the displacement, velocity or acceleration is not equivalent from a discrete point of view. The velocity is the kinematic quantity that has to be chosen in order to ensure the stability of the coupling method as shown in [4].Free interface velocity field of the coarse time scale is however not known at each fine time step, so it is linearly interpolated [4] as shown in Fig. 3. Note that high order interpolation is also possible [33]. The interface problem is then slightly modified in comparison to a dual DD method. In fact it can be shown in [31] that the interface problem (13) can be rewritten at each fine time step as follows:(15)∀j∈〚1,m〛,[C1A1−1C1T+C2A2−1C2T]︸sumofSchurcomplementsλn+j=−(u̇1n+j∣Γfree+u̇˜2n+j∣Γfree)︸velocitiesobtainedfromfreeproblemsandinterpolationwhereA1=Mlump/γΔtis the operator related to the explicit domain (β=0) andA2=(1/γΔT)(M+βΔT2K)is the one related to the linear elastic implicit domain. Note also that the interface operator remains constant through the time.λn+jis the interface force at timetn+j=tn+jΔt.u̇1n+j∣Γfreeis the free interface velocity of the domain with the finest time step at timetn+jandu̇˜2n+j∣Γfreeis the approximation of the free interface velocity of the domain with the large time step at the same time. This approximation can be obtained by the following linear interpolation:(16)∀j∈〚1,m〛,u̇˜2n+j∣Γfree=(1−jm)u̇2n∣Γfree+jmu̇2n+m∣ΓfreeThe GC method, detailed in Algorithm 1, thus consists in performing a single coarse time step on the “free” problem on domain Ω2. Solution on Ω1 is then advanced with fine time stepping through the coarse time increment by solving the “free”, “interface” and “linked” problems. The “linked” problem is finally solved on Ω2. Note that implicit linear system is solved twice per coarse time step. As it will be shown in Section 4.3.2, the associated CPU time becomes negligible as soon as large time steps ratios can be reached. Note also that computing the internal forces is the most expensive part of the explicit solution procedure and is done only once per fine time step. As shown in Eq. (5), internal forces are indeed computed from the displacement predictor which remains the same in “free” and “linked” solutions.Algorithm 1GC algorithm from tntotn+m.Computeu̇1n+1freeandu̇2n+mfree(Free problems in Ω1 and Ω2)for j=1 to j=m (Loop over Ω1until reaching timetn+m) doifj>1thenComputeu̇1n+jfree(Free problem in Ω1)end ifComputeu̇˜2n+j∣Γfreewith formula (16)Computeλn+j(Solve interface problem with formula (15))ifj<mthenComputeu̇1n+j(Linked problem in Ω1)end ifend forComputeu̇1n+mandu̇2n+m(Linked problems in Ω1 andΩ2)In this section, the two finite element softwares used in this work are presented as well as their Python binding implementation. Low intrusive scripting of the GC algorithm as well as the interface operators computation through these bindings is then described.In this work, two FE codes are used within the code-coupling framework:•Zset/Zébulon[5] is a non-linear implicit FE code jointly developed by MINES ParisTech, Onera and NWNumerics. Zset/Zébulonis suited to smooth non-linear transient dynamics analysis. Its main programming language is C++ but a general purpose Python binding is also available.Europlexus[6] is a FE software jointly developed by CEA (CEA Saclay, DMT, France) and the European Joint Research Center (JRC Ispra, Italy). It is based on an explicit algorithm. Besides its good ability to handle fluid/structure interaction, it is also suitable to fast transient dynamics and to non-smooth problems. Fortran is the native programming language of Europlexus.The several algebraic operations required to implement the algorithm described in Sections 2.2 and 2.3 could be hard-coded directly within the native software source codes. This however requires an access and a good knowledge of these different source codes and is thus very intrusive and difficult to extend to different couples of software and especially to commercial softwares. However, the increasing importance of scripting language bindings and especially Python bindings to commercial and academic softwares is nowadays remarkable. Attempts to standardize Python interfaces in the field of computational sciences, such as the CGNS project [34,35], are also worth noting. Python is particularly interesting for scientific applications since it comes with NumPy, a module which adds support for large multi-dimensional arrays and matrices, along with a large library of high-level algebraic functions to operate on these arrays (broadcasting, linear solvers, Fourier transforms, etc.) [36]. Data one want to operate on (essentially nodal displacements, velocities or forces vectors) are generally stored in Fortran or C++ arrays. These arrays can be easily handled in Python-written scripts since Numpy׳s Application Programming Interface (API) allows Numpy arrays to share pointers to the first element of Fortran or C++ arrays. Arrays are thus not copied but shared in memory. Using Python/NumPy to perform algebraic operations on large data arrays is therefore as efficient in terms of CPU performance as hard-coding in the original programming language.Applied to FE codes, these scripting interfaces allow to have more flexible input data descriptions like in Abaqus or Code_Aster[37] for instance. The Python interpreter can also be embedded within C++ or Fortran written codes [38]. This means that some parts of the application can occasionally call the Python interpreter to run some Python code. In this case, such interfaces allow to have a control on data during the computation process. For the needs of the present study, the Python interpreter has to be called from three critical points in both Zset/Zébulon and Europlexus algorithms:•before the first increment to initialize variables in the Python script,at the beginning of an increment, basically to apply new external forces,at the end of an increment, basically to extract and eventually interpolate kinematic quantities and to solve the interface problem.publish a function to apply nodal external forces on specified node sets (i.e. the interface nodes),publish a function to extract nodal kinematic values from specified node sets (velocities in the present algorithm),implement a mechanism to validate or invalidate an increment in order to be able to solve the “linked” problem after the “free” problem with no spurious time advance. This mechanism has to be controlled from the Python outer script,eventually publish the NumPy array view of the lumped mass vector (in explicit code).The Schur complement of the explicit domain at the interface is defined by the following formula:(17)S1=C1A1−1C1T=γΔtC1M−1C1TComputing this operator is straightforward sinceMis replaced by the lumped mass matrixMlump. The EuroplexusPython interface publishes a function that gives access to the internal storage ofMlumpthrough a NumPy vector with no memory copy.The Schur complement of the implicit domain at the interface is defined by the following formula:(18)S2=C2A2−1C2T=γΔTC2(M+βΔT2K)−1C2TComputing the Schur complement of the elastic linear implicit operator over the interface is performed by launching a classical Zset/Zébulon instance with null Neumann and Dirichlet boundary conditions as well as null initial velocity over the whole domain. The python interface is then used to apply a canonical loading for each interfacial DOF (i.e. value of 1 on the current dof and 0 elsewhere). This is done within fictitious increments (no time advance) using the increment invalidation mechanism described previously. Resulting interfacial velocities are gathered into a NumPy array. The interface operator (left-hand side of Eq. (15)) is then computed and explicitly inverted with NumPy. This procedure is as efficient as a hard-coded implementation as soon as the global operator factorization is conserved. Note also that under these hypotheses (coupling implicit domain with linear elastic material and explicit domain) the interface operator remains constant through the time. Its calculation is then done once at the beginning of the simulation.Code-coupling involves network communications between the several code instances which are implemented in the Python scripts through the mpi4py module.33mpi4py implements a Python interface to MPI.Solutions of problems (12) are performed by each code instance with their own Newmark time integration scheme. An extraction of interfacial velocities to NumPy vectors is performed and the coarse scale velocity is transferred through MPI to the fine scale Python instance. Time on the explicit domain is then advanced and time interpolation (Eq. (16)) as well as solution of (15) is performed with NumPy. Resulting interfacial forces are transferred back to the coarse scale Python instance and problems (14) are then solved independently by each code instance. This process is illustrated in Fig. 4.Code-coupling simulations with different time step ratios are performed on a stiffened composite panel relatively representative of an aircraft׳s subassembly. The model characteristics and the results obtained are presented in this section.The geometry, the mesh and the domain decomposition adopted for the stiffened composite panel are shown in Fig. 5. The central impacted area of the panel (in blue) as well as the impactor is processed with Europlexus and the complementary part which has relatively larger time step is processed with Zset/Zébulon. The Newmark parameters are γ=0.5 and β=0 in the explicit domain and γ=0.5 and β=0.25 in the implicit domain. The stacking sequence is[90/45/0/−45]sfor the T-shape stiffeners and[90/45/0/0/−45]sfor the skin. The characteristic size of elements (hexahedrons and wedges) ranges from 5mm to 0.25mm in the impact area. Note also that each ply is modeled with one element in the thickness direction. Reduced integration elements are set in explicit domain. The size of the problem is about 1million degrees of freedom with 94% of nodes located in the implicit domain. The size of the interface is 4428 degrees of freedom. The 8mm radius impactor has an initial velocity of 10ms−1 and a mass of 0.2kg, thus resulting in a 10J impact. The time step of the impacted area is fixed to 1.10−8s during the study, which is in accordance with the stability criterion internally computed by Europlexus. We then perform computations with different time step ratios with m ranging from 10 to 1000 (implicit time steps ranging from 1×10−7s to 1×10−5s). A fully explicit simulation is also performed to make comparisons with the proposed method.The elastic damage model used in this work in the explicit domain is a simplified version of the mesoscale laminate model OPFM (Onera Progressive Failure Model) [39]. The main modification from the original model consists in not taking into account the viscosity whose effects are assumed to be of second order due to the high loading rates. This simplification allows for an explicit formulation of the material model that is described in Appendix A. The material model used in the implicit domain is only the linear orthotropic elastic part of the material model used in the explicit domain.The OPFM model was initially implemented for Zset/Zébulon through the Z-mat utility. Z-mat is a programming environment dedicated to material models development. It is originally part of the Zset/Zébulon solver [5] but can also be linked to Europlexus through its Z-europlexus interface. Using the OPFM model in the current code-coupling environment is then straightforward and almost code-independent. Note however that the Z-mat modularity can decrease the CPU time performance in comparison to a hard-coded constitutive law.In the present work, no cohesive zone model is introduced so delamination is not taken into account.

@&#CONCLUSIONS@&#
