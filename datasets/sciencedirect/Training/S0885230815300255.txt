@&#MAIN-TITLE@&#
A silent speech system based on permanent magnet articulography and direct synthesis

@&#HIGHLIGHTS@&#
This paper introduces a ‘Silent Speech Interface’ with the potential to restore the power of speech to people who have completely lost their voices.Small, unobtrusive magnets are attached to the lips and tongues and changes in magnetic field are sensed as the ‘speaker’ mouths what s/he wants to say.The sensor data is transformed to acoustic data by a speaker-dependent, learned transformation over parallel acoustic and sensor data.The machine learning technique used here is Mixture of Factor Analysis.Results are presented for 3 speakers which demonstrate that the SSI is capable of producing ‘speech’ which is both intelligible and natural.

@&#KEYPHRASES@&#
Silent speech interfaces,Speech rehabilitation,Speech synthesis,Permanent magnet articulography,Augmentative and alternative communication,

@&#ABSTRACT@&#
In this paper we present a silent speech interface (SSI) system aimed at restoring speech communication for individuals who have lost their voice due to laryngectomy or diseases affecting the vocal folds. In the proposed system, articulatory data captured from the lips and tongue using permanent magnet articulography (PMA) are converted into audible speech using a speaker-dependent transformation learned from simultaneous recordings of PMA and audio signals acquired before laryngectomy. The transformation is represented using a mixture of factor analysers, which is a generative model that allows us to efficiently model non-linear behaviour and perform dimensionality reduction at the same time. The learned transformation is then deployed during normal usage of the SSI to restore the acoustic speech signal associated with the captured PMA data. The proposed system is evaluated using objective quality measures and listening tests on two databases containing PMA and audio recordings for normal speakers. Results show that it is possible to reconstruct speech from articulator movements captured by an unobtrusive technique without an intermediate recognition step. The SSI is capable of producing speech of sufficient intelligibility and naturalness that the speaker is clearly identifiable, but problems remain in scaling up the process to function consistently for phonetically rich vocabularies.

@&#INTRODUCTION@&#
Despite speech being our preferred and most natural form of communication, normal speech communication can be impossible or undesirable in some situations. Adverse noise conditions might make speech unintelligible and there are diseases that lead to a person losing their voice or having their ability to speak severely impaired. These include trauma, cancer of the larynx, and some neurological disorders. Sometimes audible speech may not be desirable, e.g. private conversations in public areas. The main obstacle to communication in these situations derives from the acoustic speech signal: its quality is severely affected or non-existent in some situations, whereas it is desirable to avoid generating it in other situations. In all these situations silent speech interfaces (SSIs) can help.A SSI is a system that enables speech communication in the absence of audible speech by exploiting other biosignals associated with speech production (Denby et al., 2010). Several types of SSIs have been proposed using different sensing technologies to capture speech-related biosignals. Some work has been done, with limited success, using brain–computer interfaces (BCIs) such as intracranial electrocorticography (ECoG) (Brumberg et al., 2010, 2011; Herff et al., 2015) or electroencephalography (EEG) (Wester, 2006; Brigham and Vijaya Kumar, 2010) to decode the brain activity associated with particular thoughts or intentions of a subject. Other SSIs use the electrical activity of the articulator muscles. The most widespread technology for capturing this information is surface electromyography (sEMG) (Jou et al., 2006; Schultz and Wand, 2010; Janke et al., 2012; Wand et al., 2014; Zahner et al., 2014; Deng et al., 2014). Alternatively, SSIs can also be based on the movement of the speech articulators. Different technologies have been used to capture articulator motion including video (Petajan, 1984; Petajan et al., 1988; Matthews et al., 2002), ultrasound (Cai et al., 2013), both video and ultrasound (Hueber et al., 2010, 2011), electromagnetic articulography (EMA) (Toda et al., 2008; Toutios and Narayanan, 2013), magnetic resonance imaging (MRI) (Badin et al., 2002; Birkholz and Jackel, 2003) and radar (Toth et al., 2010). In this paper we employ an alternative approach for capturing articulator movement: permanent magnet articulography (PMA) (Fagan et al., 2008; Gilbert et al., 2010; Hofe et al., 2013a,b; Cheah et al., 2015). In PMA a set of magnets are attached to the articulators (typically the lips and tongue) and the magnetic field generated while the user ‘speaks’ is captured by a number of sensors located around the mouth. Compared with other techniques for capturing articulator movement such as EMA or sEMG, PMA has the potential to be unobtrusive as there are no wires coming out of the mouth or electrodes attached to the skin.The speech-related biosignals generated during speech production can then be used to determine the acoustic signal associated with those signals. The most common way of doing this would be to decode the message encoded in the biosignals using automatic speech recognition (ASR), and then use a text-to-speech (TTS) synthesiser to generate the final acoustic signal from the recognised text. Although this approach for speech reconstruction has several advantages, such as rapid development by using readily available ASR and TTS tools and the possibility of obtaining a better speech signal reconstruction (especially of the voicing11The speech biosignals generally provide little information about the voicing of speech, particularly when the SSI is used by laryngectomees (Gonzalez et al., 2014).) by exploiting the textual representation in the TTS synthesiser, it also has drawbacks (Hofe et al., 2011). First, the approach is constrained to the language and vocabulary of the recogniser. Second, speech articulation and its associated auditory feedback are disconnected due to the variable delay introduced by the ASR and TTS steps. Third, the non-linguistic information encoded in the articulatory data (e.g. subject's gender, age or mood) is normally lost after the ASR step. These drawbacks, particularly the second one, may affect the willingness of a SSI user to engage in social interactions. This means that, at best, a recognise-then-synthesise system would be like having an interpreter. To address these problems, we can resort to an alternative approach for speech restoration: direct speech synthesis from the biosignals without an intermediate recognition step.The direct synthesis (DS) approach attempts to model the relationship between the speech-related biosignals and their acoustics. In comparison with the recognise-then-synthesise approach, DS has the advantage that is not limited to a specific vocabulary and is language-independent. Moreover, it can allow real-time speech synthesis. There is also the possibility that real-time auditory feedback might enable the user to learn to produce better speech: like learning to play an instrument. At best, DS could restore the user's voice, lost by excision of the larynx. Assuming that the biosignals represent articulatory data, as with PMA, and that a parametric representation of speech (i.e. a vocoder) is adopted, the modelling of the articulatory-to-acoustic mapping presents some challenging problems. First, this mapping is known to be non-linear (Atal et al., 1978; Qin and Carreira-Perpiñán, 2007; Neiberg et al., 2008; Ananthakrishnan et al., 2012). Furthermore, in some cases the mapping is non-unique, that is, the same articulatory data might correspond to different acoustic realizations. The reason for this non-uniqueness is that typically the sensing technology used by the SSI only provides an incomplete picture of the speech production process and some of the information about this process is missing or not well captured.Several techniques have been proposed in the literature for representing the articulatory-to-acoustic mapping. In general, these techniques can be classified into two categories: model-based and stereo-based. Model-based techniques such as those proposed in Schroeter and Sondhi (1994), Birkholz et al. (2008), Toutios et al. (2011), Toutios and Narayanan (2013) use articulatory data to drive an articulatory synthesiser, which implements a physical model of speech production that can be controlled using a small set of control parameters (Rubin et al., 1981; Maeda, 1982). Thus, these techniques attempt to find a mapping between the articulatory data and the control parameters of the synthesiser. Stereo-based techniques, in contrast, learn the direct correspondence between the articulatory and acoustic domains using parallel data, i.e. simultaneous recordings of articulatory and speech data. To learn the transformation between these domains from the parallel data, several approaches have been proposed including statistical approaches based on Gaussian mixture models (GMMs) (Toda et al., 2008, 2012b; Nakamura et al., 2012), hidden Markov models (HMMs) (Hueber et al., 2012), shared Gaussian process dynamical models (Gonzalez et al., 2015), neural networks (Desai et al., 2009), support vector regression (Toutios and Margaritis, 2005), and a concatenative, unit-selection approach (Zahner et al., 2014). Most of these approaches were originally developed for voice conversion (VC) (Stylianou et al., 1998; Toda et al., 2007) and, in general terms, the techniques developed for VC can also be applied to stereo-based articulatory-to-acoustic tasks.In this paper we present a silent speech system that is able to convert articulator motion data captured using PMA into audible speech. From the two DS approaches outlined above, we opt for a stereo-based approach for two reasons. First, the availability of parallel datasets enables the direct modelling of the PMA-to-acoustic mapping using machine learning techniques. The second reason is that current models of speech production (i.e. articulatory synthesisers) are still not mature enough compared to other approaches such as statistical parametric speech synthesis. In our proposed technique, simultaneous recordings of PMA and audio data are used to learn the mapping between the articulatory and acoustic domains. These parallel recordings are used during the training phase to estimate the joint probability distribution of PMA and speech parameter vectors. To represent the distribution, a generative approach based on mixture of factor analysers (MFA) is proposed in this work. Then, during normal usage of the SSI, the speech-parameter posterior distribution given the PMA data is evaluated in order to convert the captured articulatory data into an acoustic signal. Two alternative conversion algorithms are investigated in this work for transforming PMA parameter vectors to speech parameter ones. The first one is based on the well-known minimum mean square error (MMSE) estimator. A limitation of this algorithm is that it works on a frame-by-frame basis; thus imposing no temporal constraints on the reconstructed speech signal. To encourage smooth trajectories on the reconstructed speech parameters, we also investigate the application of the maximum likelihood estimation (MLE) algorithm proposed by Tokuda et al. (2000), Toda et al. (2007), which takes into account the statistics of both the static and dynamic speech parameters, to our specific problem. The proposed techniques are evaluated using objective and subjective quality measures on parallel datasets with PMA and audio material recorded for several speakers.This work forms part of our continuing effort to develop an acceptable and discrete PMA-based SSI for laryngectomy patients. Key milestones in our previous work that build up to this paper are as follows:•First, in Gilbert et al. (2010), Hofe et al. (2013b), speech recognition from PMA data was reported to achieve similar accuracy results to using audio on isolated words and connected digits recognition tasks.Then, in Hofe et al. (2013a), the study of PMA-based speech recognition was successfully extended to multiple speakers.More recently, extensive investigation into the effectiveness of PMA data for discriminating the voicing, place and manner of articulation of English phones was presented in Gonzalez et al. (2014).With respect to the direct synthesis approach, a feasibility study was presented in Hofe et al. (2011) in which speech formants were estimated from PMA data.As previously stated, our long term plan is to build an SSI that is able to generate high quality speech from PMA data in real time. For laryngectomy patients, this will involve simultaneously recording both PMA data and the patient's voice before laryngectomy. Then, after laryngectomy has been performed, the direct synthesis models trained on the patient's voice will be used to generate speech. In cases where it is impractical to record parallel data before the operation we can, for instance, record acoustics only and then, after laryngectomy, ask the patient to mime along to their own pre-recorded voice to provide the sensor data stream. In cases where the voice has been destroyed prior to the laryngectomy, patients could be asked to mime along to a ‘donor voice’.The rest of this paper is organised as follows. First, in Section 2, the functional principles of the PMA technique are outlined. Section 3 presents the mathematical details of the direct synthesis technique we use in this paper for generating speech from PMA data. Then, in Section 4, the technique is evaluated on parallel databases containing PMA and acoustic data. The results obtained are discussed in Section 5. Finally, we summarise this paper and outline future work in Section 6.The principle of PMA is that the motion of the articulators may be determined by attaching a set of magnets to the articulators and measuring the resultant magnetic field variations using a set of magnetic sensors located around the mouth. These field variations may then be used to determine the speech which the user wishes to produce. It should be noted that the magnetic field detected at each sensor is a composite of the field from each magnet and that the contribution from each magnet is a non-linear function of its position and orientation. Due to the complexity of the interaction between magnets and the sensed field, it is not currently the intention that the sensor information be used to determine the Cartesian positions/orientations of the magnets, but rather that the composite field be mapped to speech features.A number of implementations of PMA have been investigated in recent years (Fagan et al., 2008; Gilbert et al., 2010; Hofe et al., 2013a,b). Earlier prototypes provided acceptable recognition performance but were not particularly satisfactory in terms of their appearance, comfort and ergonomic factors for the users. To address these limitations, the latest PMA device was developed based on a user-centric approach (Cheah et al., 2015). The prototype was re-designed based on feedback from user questionnaires and through discussion with stakeholders including clinicians, potential users and their families.The new PMA device has much improved appearance, portability and miniaturised hardware. Nevertheless, the prototype showed a comparable performance to its predecessor (Cheah et al., 2015; Gonzalez et al., 2014). Key components of the device include a set of six Neodynium Iron Boron (NdFeB) permanent magnets attached to the lips and tongue as illustrated in Fig. 1. These magnets are currently attached using Histoacryl surgical tissue adhesive (Braun, Melsungen, Germany), but would be surgically implanted for long term use. The remainder of the PMA system is composed of a set of four tri-axial Anisotropic Magnetoresistive (AMR) magnetic sensors mounted on a bespoke wearable headset, a control unit, a rechargeable battery and a processing unit (e.g. computer/tablet PC). Detailed information on these hardware modules and their operation is presented in Cheah et al. (2015).During data acquisition the outputs of the first three magnetic sensors in Fig. 1 (i.e. 9 channels) are used to capture the magnetic field changes arising from the movements of the magnets attached to the articulators. The output of the fourth sensor (which is placed further away from the mouth) is used as a reference for background cancellation to compensate for the effect of the earths magnetic field on the other three sensors. The acquired data is sampled at 100Hz and transmitted, either through a USB connection or via Bluetooth, to a computer/tablet PC for further processing. The PMA data is first low-pass filtered to remove 50Hz electrical noise, and then normalised prior to further processing (Hofe et al., 2013b; Cheah et al., 2015).In this section we present the details of the proposed technique for speech parameter generation from PMA data. Formally, the aim of this technique is to find a mapping functiony=f(x) for transforming source feature vectorsxinto target feature vectorsy. In our case, the source vectors are derived from the PMA data captured by the SSI, while the target vectors correspond to a parametric representation of speech, typically Mel-frequency cepstral coefficient (MFCC) parameters (Fukada et al., 1992). To model the PMA-to-acoustic mapping, we resort to a statistical approach in which the parameters of the mapping function are learned from training data containing parallel recordings of PMA and acoustic data. The proposed approach consists of two phases. Firstly, in the training phase, the parallel data is used to learn the parameters of the joint distribution of source and target vectors p(x,y). The details of the training phase are given in Section 3.1. Then, in the conversion phase, the learned parameters are used to derive the conditional distribution p(y|x) which, in turn, allows us to find the target acoustic vector associated with a particular observation (i.e. a PMA feature vector). This is discussed in Section 3.2.Letxandybe the PMA and acoustic parameter vectors with dimensions Dxand Dy, respectively. Instead of directly modelling the relationship between both variables asy=f(x), we assume thatxandyare outputs of an underlying stochastic process whose statevis not directly observable. Furthermore, we will assume thatDv≪Dx,Dy, such that the latent variable offers a more compact representation of the data. Then, the relationship between the latent space and the observed variables can be expressed as,(1)x=fx(v)+ϵx,(2)y=fy(v)+ϵy,whereϵxandϵyare noise processes. To make inference tractable, a common assumption in latent variable models is to considerϵxandϵyas Gaussian with zero mean and diagonal covariancesΨxandΨy, respectively.Assuming thatvencodes the vocal tract shape at a given time instant, the mapping functionsfxandfywill be non-linear as indicated above. Althoughvmight not have any physical interpretation, the non-linearity offxandfywill still hold in general terms. Hence, in order to accurately modelfxandfy, we have to deploy non-linear regression techniques. In this work we adopt a mixture of factor analysers (MFAs) Ghahramani and Hinton (1996) approach in which the mapping functions are approximated in a piecewise linear fashion. The functions are approximated by a mixture of K factor analysis (FA) models Anderson (2003), each of which has the following form,(3)x(k)=Wx(k)v+μx(k)+ϵx(k),(4)y(k)=Wy(k)v+μy(k)+ϵy(k),where k=1, …, K is the FA model index;Wx(k),Wy(k)are linear transformation matrices (a.k.a. factor loadings);μx(k),μy(k)are vectors that allow the data to have a non-zero mean andx(k),y(k) denote local approximations ofxandyaround the meansμx(k)andμy(k), respectively. The number of local models K can be optimised by cross-validation or using a validation set. The above equation can be written more compactly as,(5)z(k)=Wz(k)v+μz(k)+ϵz(k),wherez=[x⊤,y⊤]⊤,Wz(k)=[Wx(k)⊤Wy(k)⊤]⊤,μz(k)=[μx(k)⊤,μy(k)⊤]⊤, andϵz(k)∼N(0,Ψz(k)), withΨz(k)being the following diagonal covariance matrix,(6)Ψz(k)=Ψx(k)00Ψy(k).Using the generative model in (5), we can now write the joint pdf of source and target vectors as the following mixture distribution,(7)p(z)=∑k=1Kπ(k)p(z|k),where π(k) are the mixture weights and the likelihood p(z|k) is given by(8)p(z|k)=∫p(z|v,k)p(v|k)dv,withp(z|v,k)=N(Wz(k)v+μz(k),Ψz(k))(deduced from (5)). As in standard factor analysis, the factorsvare assumed to be distributed according top(v|k)=N(0,I). Under this assumption, it can be shown (see e.g. Appendix B of Bishop (2006)) that p(z|k) above simplifies to a Gaussian distribution with meanμz(k)and reduced-rank covariance matrix given byΣz(k)=Ψz(k)+Wz(k)Wz(k)⊤. This matrix can also be expressed in terms of the correlations between the source and target vectors by defining the following partitions,(9)Σz(k)=Σxx(k)Σxy(k)Σyx(k)Σyy(k)=Ψx(k)+Wx(k)Wx(k)⊤Wx(k)Wy(k)⊤Wy(k)Wx(k)⊤Ψy(k)+Wy(k)Wy(k)⊤.Finally, the parameters of the MFA model{(π(k),μz(k),Wz(k),Ψz(k)),k=1,…,K}are estimated using the expectation-maximization (EM) algorithm proposed in Ghahramani and Hinton (1996) from a training dataset consisting of pairs of source and target feature vectorszi=[xi⊤,yi⊤]⊤,i=1,…,N.The conversion phase of the proposed approach for direct speech synthesis addresses the problem of transforming articulatory data into audible speech. In this section we will only address the problem of transforming PMA feature vectors into speech parameter vectors, relying on the corresponding vocoder for obtaining the final time-domain acoustic signal from the estimated speech parameters. Formally, the PMA-to-acoustic conversion problem involves finding the target speech vectorythat corresponds to an observed PMA feature vectorx. In our statistical-based conversion system, the information about this mapping is available in the form of the conditional distribution p(y|x), which is derived from the joint distribution p(x,y) in (7) as,(10)p(y|x)=∑k=1KP(k|x)p(y|x,k),where(11)P(k|x)=π(k)Nx;μx(k),Σxx(k)∑k′=1Kπ(k′)Nx;μx(k′),Σxx(k′),(12)p(y|x,k)=Ny;μy|x(k),Σy|x(k).The mean and covariance of the k-th component conditional distribution p(y|x, k) are obtained using the properties of the joint Gaussian distribution:(13)μy|x(k)=μy(k)+Σyx(k)Σxx(k)−1x−μx(k),(14)Σy|x(k)=Σyy(k)+Σyx(k)Σxx(k)−1Σxy(k),whereΣxx(k),Σyy(k),Σxy(k), andΣyx(k)are given by (9).From (10) we see that p(y|x) adopts the form of a mixture distribution with possibly more than one mode. Hence, different estimated values for the speech parameters may be obtained depending on the specific estimator employed in the conversion process. In the following, we introduce two different conversion techniques based on two well-known statistical estimators: MMSE and MLE considering the dynamic speech features.The MMSE estimator is defined as the conditional expectation ofygiven the observationx:(15)yˆ=E[y|x]=∫yp(y|x)dy.By substituting the expression of the conditional distribution p(y|x) in (10) into (15), the estimator finally becomes,(16)yˆ=∑k=1KP(k|x)A(k)x+b(k),whereA(k) andb(k) are derived from the k-th component conditional mean in (13) as,(17)A(k)=Σyx(k)Σxx(k)−1,(18)b(k)=μy(k)−A(k)μx(k).We can see from (16) that no continuity constraints are imposed when reconstructing the speech parameter trajectories in the MMSE estimator, which may lead to reduced speech quality. In order to address this issue, an MLE-based conversion technique is introduced in the next section.Letxtandytbe the source and target parameter vectors at frame t, respectively. From the sequence of acoustic speech vectorsY=y1⊤,…,yT⊤⊤we define the first-order difference parameters (i.e. dynamic features) at time t as,(19)Δyt=yt−yt−1.The augmented target vector containing both the static and dynamic parameters is then denoted byy¯t=[yt⊤,Δyt⊤]⊤and similarly the sequence of augmented target vectors byY¯=[y¯1⊤,…,y¯T⊤]⊤. For the purpose of MLE, it is convenient to express the relationship betweenYandY¯as the following linear transformation,(20)Y¯=RY,whereRis the following (2·Dy·T)×(Dy·T) block matrix,(21)R=100…00100…00010…00−110…00⋮⋮⋮⋱⋮⋮000…01000…−11,with0,1and −1denoting the Dy×Dyzero, identity and negative identity matrices, respectively. In the above matrix we have assumed that Δy0=y0.From the sequence of articulatory dataXthe MLE conversion algorithm tries to recover the sequence of acoustic speech parametersYthat simultaneously maximises the likelihood of the static and dynamic parameters. Mathematically, this can be expressed as follows,(22)Yˆ=argmaxYp(Y¯|X)=argmaxYp(RY|X),whereYˆ=[yˆ1⊤,…,yˆT⊤]⊤is the estimated sequence of acoustic speech parameters and the likelihoodp(Y¯|X)is obtained by assuming independence among frames (see Toda et al. (2007, 2008)),(23)p(Y¯|X)=∏t=1Tp(y¯t|xt).and the conditional distributionp(y¯t|xt)is again a mixture distribution as in (10). It must be pointed out, however, that this distribution is now derived from a joint distributionp(x,y¯)representing PMA feature vectors and augmented speech parameter vectors (i.e. with static and dynamic features).Direct maximisation of (22) is not possible because of the hidden mixture component sequencek=(k1, k2, …, kT) that appears in (23) as a consequence ofp(y¯t|xt)being a mixture distribution. Hence, we adopt the iterative EM algorithm proposed in Tokuda et al. (2000), Toda et al. (2007). LetY¯be the sequence of augmented acoustic speech parameters to be optimised by the EM algorithm. Similarly,Y¯oldis the current estimate of the augmented sequence. Then, the EM algorithm proceeds by iteratively optimising the following auxiliary Q-function with respect toY¯,(24)Q(Y¯,Y¯old)=∑t=1T∑k=1KP(k|xt,y¯told)logp(y¯t,k|xt)∝−12(RY)⊤Φ(RY)+(RY)⊤λ,whereΦis the following (2·Dy·T)×(2·Dy·T) block matrix,(25)Φ=C100…00C20…0⋮⋮⋮⋱⋮000…CT,andλis the following (2·Dy·T)-dimensional vector,(26)λ=m1⊤,m2⊤,…,mT⊤⊤.The (2·Dy)×(2·Dy) matricesCtand (2·Dy)-dimensional vectorsmt(t=1, …, T) that appear in (25) and (26), respectively, are defined as the following expected values,(27)Ct=∑k=1KP(k|xt,y¯told)Σy¯|x,t(k)−1,(28)mt=∑k=1KP(k|xt,y¯told)Σy¯|x,t(k)−1μy¯|x,t(k),whereμy¯|x,t(k)andΣy¯|x,t(k)are the mean vector and covariance matrix of the conditional distributionp(y¯|xt,k)and are given by (13) and (14), respectively.Finally, by setting the derivatives of (24) w.r.t.Yto zero and solving, we obtain the following expression for updating the estimated sequence of acoustic speech parameters,(29)Ynew=(R⊤ΦR)−1R⊤λ.The above equation is iteratively applied until a certain stopping criterion is met (e.g. a number of iterations is reached). The EM algorithm guarantees that each iteration of the iterative procedure increases the log-likelihood logp(RY|X) and, consequently, a better speech reconstruction is expected after each iteration. As initial estimate of the speech parameters we use the MMSE estimate in (16).As opposed to the MMSE conversion technique, the MLE technique in (29) performs a sequence-by-sequence mapping, rather than a frame-by-frame conversion. Thus, it is expected that more accurate speech parameter reconstructions can be obtained by the MLE technique. The drawback of this technique, in comparison with the MMSE technique, is that it is difficult to implement it in real time due to the sequence-by-sequence conversion process. Nevertheless, recent work (Toda et al., 2012a; Moriguchi et al., 2013) has extended the MLE technique to enable real time voice conversion.In this section we evaluate the reconstruction performance of the proposed technique for PMA-to-acoustic conversion on parallel datasets containing both PMA and acoustic data. Since our goal in this work is to evaluate the feasibility of direct speech synthesis from PMA data, results are only reported for non-impaired subjects. Evaluation of the proposed technique for laryngectomy patients, where we may not be able to directly record parallel data, is left for future work.Two parallel PMA-and-acoustic databases with different phonetic coverage were recorded. The first one follows the TIDigits speech database (Leonard, 1984) and consists of sequences of up to seven connected English digits. The vocabulary is made up of eleven words: the digits from ‘one’ to ‘nine’ plus ‘zero’ and ‘oh’. With this database we aim to establish that our method creates intelligible output. In order to perform an in-depth analysis of the reconstruction accuracy at the phone level, a second corpus was designed in a more systematic manner. We know from previous work (Gonzalez et al., 2014) that the ability of PMA for detecting the manner of articulation and voicing of speech sounds is limited and, therefore, we need to determine to what extent this limitation affects direct synthesis. The vocabulary in this case consists of 48 isolated consonant-vowel (CV) syllables obtained by combining 12 consonants with 4 vowels, as shown in Table 1. The construction of this vocabulary was as follows. From the set of English vowels, we choose those four most distinctive from the articulation point of view. Thus, [a i u] were chosen because they are at three of the corners in the International Phonetic Association (IPA) vowel chart. The fourth corner would be [ɑ ɒ], however [ɔ], which is also close to the fourth corner, was selected because we thought it was easy to pronounce for British English speakers. Unvoiced consonants were preferred over voiced ones due to the limited accuracy of PMA for detecting voicing. Apart from that, the consonants were chosen to have a high coverage of the IPA consonant chart, maximising the number of CV minimal pairs differing in the manner of articulation of the consonants.Parallel data was recorded for the two vocabularies described above by adult subjects with normal speaking ability in a sound proof booth. To prevent fatigue, the recording sessions were carried out on different days and short breaks were allowed during each recording session. For the TIDigits database, two male speakers (M1 and M2) and one female speaker (F1) were involved. The total amount of data for each speaker was 5.54min (231 sentences) for speaker M1, 10.50min (385 sentences) for speaker M2 and 8.46min (308 sentences) for speaker F1. For the CV database, 958 individual consonant-vowel syllables comprising 15.76min of data were recorded only for speaker M1. In each recording session, the audio and 9-channel PMA signals were recorded simultaneously at sampling frequencies of 16kHz and 100Hz, respectively, using an AKG C1000S condenser microphone and the in-house PMA device shown in Fig. 1, which was specifically designed to fit speaker M1's anatomy. Next, background cancellation was applied to compensate for the effect of the Earth's magnetic field on the captured articulatory data. Finally, all data was endpointed in the audio domain using an energy-based algorithm to prevent modelling the silence parts, during which the speech articulators may adopt any position.In the case of PMA, the background-cancelled, 9-channel signals are first segmented into overlapping frames using a 25ms analysis windows with 10ms overlap. Next, in order to combat the loss of information produced when using PMA for acquiring articulatory data and to better capture contextual phonetic information, a sliding-window approach is employed in which consecutive frames are concatenated together to form super-frames. From the sequence of PMA frames, the super-frames are formed by concatenating the ω neighbouring frames around each particular PMA frame. Because of the high dimensionality of the resulting super-frames, the partial least squares (PLS) technique (De Jong, 1993) is applied to reduce the dimensionality of the super-frames and obtain the final PMA parameter vectors used by direct synthesis. The number of principal components retained after PLS are those explaining the 95% of the total variance.The STRAIGHT vocoder (Kawahara et al., 1999) is used in this work for parametrising the acoustic signals. The speech parameters, which include the spectral envelope, aperiodicity spectrum and F0 value, are extracted from the audio signals at the same frame rate as that for the PMA signals. Then, the spectral envelopes are represented as 25-order Mel-frequency cepstral coefficients (MFCCs) (Fukada et al., 1992). As PMA does not have direct access to voicing information, the F0 value and aperiodicity are discarded and, consequently, the reconstructed speech signals are synthesised unvoiced (i.e. as ‘whispered’ speech).Finally, we apply mean and variance normalisation to the PMA and speech parameter vectors using the statistics computed for the training dataset in order to facilitate statistical training.A 10-fold cross-validation scheme is used to evaluate the proposed techniques. Thus, the data available for each speaker is randomly divided into ten sets with equal number of utterances: nine of the sets are used for training and the remaining one for testing. This process is then repeated 10 times and results obtained for the 10 rounds are averaged.For evaluating the accuracy of the PMA-to-acoustic mapping, both objective and subjective quality measures are employed in this paper. In the objective evaluation the Mel-cepstral distortion (MCD) measure (Kubichek, 1993) between the MFCCs extracted from the original audio signals and those estimated from PMA data is compute as follows:(30)MCD[dB]=10ln102∑d=1Dcd−cˆd2,where cdandcˆdare the d-th MFCC of the original and reconstructed signal, respectively, and D=24 in our case. The zero-order MFCC is not included in the above distortion since it describes the energy of the frame and in this paper we only focus on spectral-envelope reconstruction. As a distortion measure, smaller MCD results indicate better reconstruction accuracy.For evaluating the techniques subjectively, an anonymous listening test was conducted by 25 subjects. The only requisite for participating in the test was to be adult and native English speaker. In the test, participants were asked to listen carefully to several resynthesised speech samples through a web-based interface and rate them in terms of quality, intelligibility and naturalness (see below for more details). Participants were asked to conduct the test in a quite place while wearing head-phones, setting the volume to a comfortable hearing level. Subjects were allowed to replay the speech stimuli as many times as they wanted. The samples presented to each listener were randomly chosen from the set of available synthesised utterances.Fig. 2shows contour plots for the average MCD results achieved by the MMSE and MLE conversion algorithms described in Sections 3.2.1 and 3.2.2, respectively, on the TIDigits database as a function of the number of mixture components used in the MFA model and the length of the sliding window ω used to extract the PMA parameter vectors. The results in the figure correspond to the average distortion computed for the three speakers (M1, M2 and F1) and the 10 rounds in the cross-validation scheme. As expected, the MCD results greatly improve when more mixture components are used in the MFA model, because the non-linear PMA-to-acoustic mapping is represented more accurately. In this regard, we see that for this task the optimum number of mixtures is 64. Moreover, we also see that there is a significant improvement in the conversion accuracy when longer windows are used to extract the PMA feature vectors, as this helps to reduce the uncertainty during the conversion process by taking into account more contextual information. The price for increasing the number of mixtures and the length of the sliding window is in the time it takes to convert sensor data to audio and the delay for the ‘speech’ to begin. By comparing the two conversion algorithms we see that the MLE-based algorithm performs slightly better than the MMSE-based algorithm on average, but the differences between both algorithms almost disappear when long context windows are used (i.e. 200–260ms). This seems to indicate that little is gained by performing the utterance-level conversion achieved by the MLE algorithm when long context windows are used. Conversely, the short-term temporal correlations captured by the contextual windows seem to be more important for the mapping.Fig. 3shows example reconstructed spectrograms obtained by the MMSE and MLE methods for the utterance six one five eight two when a 64 mixture-component MFA and a context window of ω=200ms are used. As can be seen, speech formants are quite accurately estimated by both methods, but the spectral details are lost as a consequence of statistical averaging carried out when estimating the MFA model, leading to the well-known problem of over-smoothing (Toda et al., 2005; Zen et al., 2009). We tried the global variance (GV) conversion algorithm proposed by Toda et al. (2005, 2007) to alleviate this problem, but the results we obtained were no better than those obtained by the MLE algorithm alone. In general, we see that the vowels and fricative consonants are well estimated. However, the stop sounds (e.g. [k] in six and [t] in two at times 0.28s and 1.20s, respectively) are blurred. This is due to the complex dynamics of these sounds and the limited ability of PMA to detect information about the airflow during articulation.The MCD results obtained by the MLE conversion system with a 64-mixtures MFA model for each of the three speakers in the TIDigits database are shown in Fig. 4. As can be seen, there is a noticeable difference between the conversion accuracy achieved for speaker M1 and that obtained for speakers M2 and F1. The reason of this behaviour, as already discussed in our previous work (Hofe et al., 2013a), is that the PMA prototype used for data recording was specifically designed for M1's anatomy.Finally, a comparison between the GMM-based articulatory-to-acoustic conversion technique proposed by Toda et al. (2007, 2008) and our MFA-based mapping is shown Fig. 5. For a fairer comparison both approaches are evaluated using 64-mixture models and the MLE-based conversion algorithm. We evaluate our proposal using different dimensions for the latent space variablevin (5). The dimensions are 5, 10, 15, 20, and 25, the latter being the dimensionality of the speech parameter vectors. As can be seen, both methods perform almost equally except when the dimensionality of the latent space in the MFA-based conversion system is very small (i.e. 5 or 10). In this case, the quality of synthetic speech is slightly degraded due to the difficulty of capturing the correlations between the acoustic and PMA spaces in such latent spaces. For dimensions greater than 15, we see that both approaches (GMM and MFA) report more or less the same results, with the benefit that our proposal is more computationally efficient because of the savings of carrying out the computations in the reduced-dimension space.We conducted a listening test to evaluate speech quality and naturalness. Speech intelligibility was not assessed for this database because informal listening revealed that the converted samples were completely intelligible.22The direct synthesis technique can produce speech of surprisingly high quality: the reader may listen to examples on the demos section of http://www.hull.ac.uk/speech/disarm. The identity of the speaker comes over strongly, because the mapping is trained to an individual voice.Intelligibility experiments for the CV database are presented in the next section. For the TIDigits database, an XAB test was first carried out to evaluate speech quality. In the test, a speech sample was resynthesised without voicing (i.e. as whispered speech) using the STRAIGHT vocoder and presented to the listener as the reference X. Also, two different versions of the same sample converted from PMA data by our proposed method and Toda's technique were also presented to the subject in random order as A and B. Then, the listener was asked to choose which of A or B was more similar to the reference X, offering also the possibility of no preference (N/P) if both A and B sounded equally close to X. In order to evaluate the effect on perceived speech quality of the latent space dimensionality in the MFA model, different versions of the same speech sample were resynthesised from PMA data using dimensions of 5, 15 and 25 forv. Each listener evaluated 8 pairs of randomly selected A-B sentences for each condition (i.e. latent dimensionality), thus making a total of 24 sample pairs evaluated per listener. For obtaining the resynthesised samples, mixtures models with 64 components and a context window of 200 ms length were employed. The conversion method chosen in both cases was the MLE algorithm.Fig. 6shows the results of the XAB test. It can be seen that, even when a low dimensionality is chosen for the hidden variable in the MFA model, no significant differences between speech synthesised by the MFA and GMM approaches were perceived. As the dimensionality of the latent space increases, so do the number of times listeners judge that there are no differences between both approaches. Thus, we can conclude that our approach can be seen as an efficient approximation to Toda's conversion method.Next, we conducted a mean opinion score (MOS) test on speech naturalness. Subjects were asked to judge the naturalness of individual speech samples using a five-point scale: from 1 (completely unnatural) to 5 (completely natural). Five systems were evaluated:•Original: the original speech samples with no modification.STRAIGHT: vocoded speech using STRAIGHT.Whisper: vocoded speech using STRAIGHT but synthesised as whispered speech (i.e. without voicing).DS: direct synthesis, that is, speech converted from PMA data by our proposed MLE-based method using a 64-component MFA model.DS-voicing: same as before, but now speech is synthesised with voicing taken from the original files. In other words, the spectral envelope is estimated from PMA data by our method and the voicing information (i.e. aperiodicity, F0 and voicing decision) is taken from the original files.Each listener evaluated 8 randomly chosen samples for each system, thus making a total of 40 samples evaluated per subject. The order of the samples were randomised to control for order effect bias. The results of the MOS test are shown in Fig. 7. It can be seen that speech naturalness is degraded after the analysis-synthesis process carried out by STRAIGHT due to artefacts introduced by this vocoder. This degradation is further amplified when speech stimuli are resynthesised with no voicing and, hence, listeners considered whispered speech quite unnatural. Surprisingly, listeners judge that the naturalness of DS speech is not significantly lower than that of whispered speech. This is a exciting result if we consider that the whispered samples are directly obtained from the original, natural speech samples, while the DS samples are obtained through an error-prone conversion process such as direct synthesis. We also see in the figure that when the DS samples are synthesised with voicing, their naturalness is significantly enhanced, outperforming even the whisper process.The objective MCD results obtained for the CV database are shown in Fig. 8. Again, we see that the MMSE and MLE conversion algorithms perform equivalently. Compared to the results obtained for the TIDigits database in Fig. 2, we see that the MCD figures for the CV database are slightly better. On the one hand, a possible explanation for this is that the utterances in the CV corpus consist of single-syllable words, while those in the TIDigits database have multiple words. On the other hand, the CV database contains only data for speaker M1, who is the user for whom the PMA prototype was designed. For this database we see that the best results are obtained when using a 128-mixture MFA and a PMA frame window spanning 240ms. This is the set-up we will use in the rest of this section.Next, to study in-depth the errors made by the conversion algorithms, we performed phone-level comparisons between the reconstructed MFCCs obtained from PMA data and the original MFCCs extracted from the ground-truth acoustic signals. We did that by first force-aligning the word-level transcriptions of the ground-truth signals using a context-dependent speech recogniser adapted to the speaker's voice. Then, the timing information included in the resultant phonetic transcriptions was used to segment the acoustic signals into phones and the MCD measure was computed for each individual phone. In doing so we assumed that the ground-truth and reconstructed signals were synchronous. Furthermore, only the stable part of the phones was used for computing the MCD distortion in order to avoid considering coarticulation effects. We assumed that the stable part corresponds to the 50% central segment of the phone.The detailed MCD results obtained for each phone and conversion method are shown in Fig. 9. The results are presented as box plots, each box showing the first three quartiles (i.e. 25%, 50% and 75%) of the error, while the whiskers extending up to 1.5 times the interquartile range (i.e. Q3–Q1). We see that, again, the results obtained by both conversion algorithms are very similar. From the point of view of the different phones, we can made the following observations. Firstly, it can be observed that the vowels are quite accurately reconstructed in both cases. The consonants, however, are not always consistently well reconstructed. In general, we see that direct synthesis performs poorly in reconstructing the sounds articulated in the back of the mouth (i.e. [kh]), which the current PMA prototype is not capturing well as no magnet is placed in this area. Other consonant sounds for which the reconstruction error is higher than the mean are the plosives [ptk]. In this case, as commented above, the problem lies in the difficulty in modelling the dynamics of these sounds (i.e. a hold phase where the vocal tract is closed followed by a short burst in which the air is suddenly released), together with the limitations of PMA for accessing airflow information. Apart from these problems, the ability of the direct synthesis technique to synthesise accurately phones sharing the same place of articulation but a different manner (e.g. [nlr]) is remarkable. It might be that contextual information such as coarticulation is well captured by PMA, helping to reduce the uncertainty associated with the articulatory-to-acoustic mapping.We evaluated the intelligibility of the resynthesised speech samples by conducting a listening test involving 25 human subjects. In the test, the subjects were asked to type the syllable they heard when presented with a speech stimulus. No further instructions were given to the subjects apart from that the stimuli were comprised of consonant-vowel syllables and that they might be nonsense words. In particular, the consonants and vowels used to construct the syllables were not revealed to the subjects. Because of this freedom, it was later found during the analysis of the results that some subjects were biased in choosing correct words in English with similar pronunciation (e.g. food for [fu] or zoo for [su]) when uncertain about the transcription of a particular stimulus. Despite this, we preferred this form for evaluating speech intelligibility over other forms (e.g. asking the subjects to choose the transcriptions from a list) because we considered it provides us with a better measure of intelligibility.In the test, each subject was presented with 24 stimuli chosen at random, consisting of 24 different syllables formed by combining the 12 consonants with two random, but different, vowels. Later, when analysing the responses of the subjects, the original syllables and the subject responses were transcribed phonetically in order to account for possible homophones of the same syllable (e.g. to, too or two). Then, subject responses were compared with the transcriptions of the original stimuli to see whether they match.Table 2the accuracy results obtained for each CV syllable, that is, the percentage of each individual syllable which was correctly transcribed by the subjects in the listening test. In addition, the overall average (AVG) results for each consonant and vowel are also shown. With an overall accuracy of 68%, it can be seen that the results are very diverse, ranging from syllables that are always well transcribed (e.g. [fa], [tu], [lɔ]) to those such as [ku] that were not correctly transcribed on any occasion. Speaking roughly, we see that the worst results are obtained for the syllables containing plosive consonants (i.e. [ptk]) and those in which the consonant sound is articulated in the back of the mouth (i.e. [kh]), as in the objective results shown in Fig. 9. Other syllables for which poor results are obtained are those starting with the consonants [θ] and [r]. For [θ], as will be further discussed later, a large percentage of the errors (70.37% of them) correspond to confusions with [f]. For [r], more than half of the total errors (58.33%) are due to [r] being confused with [l].Next, we conducted an analysis of the type of errors made by listeners when transcribing the speech stimuli in terms of deletions, insertions and substitutions. To perform this analysis, the phonetic transcriptions of the listener responses were manually aligned with the reference consonant-vowel transcriptions of the stimuli and the number of deletion, insertion and substitutions errors were counted for each phone. For example, [pa] would count as an substitution error for the vowel [ɔ] in the analysis if the original stimuli was the syllable [pɔ]. Similarly, [u] would count as a deletion error of the consonant if the original syllable was [hu]. Finally, we count [blu] and [taɔ] as insertions errors for the consonant and vowel, respectively, if the original syllables were [bu] and [ta].The results of the error analysis are shown in Fig. 10. Again, as in the objective results shown in Fig. 9, it can be seen that far fewer errors are made for the vowels than for the consonants. In the case of the consonants, big differences exist between different groups of consonants: [pθrkh] are erroneously transcribed more than 40% of times, other consonants as [fʃls] are quite accurately transcribed (less than 10% of errors), while the renaming consonants [mtn] are in the middle of the table, with transcription errors approximately equal to 20%. Regarding the type of errors, it can be seen that most of them correspond to substitutions. These will be discussed below in more detail. For the vowels, a large percentage of the errors are also due to insertions. These errors correspond to phones, usually vowels, identified by the subjects at the end of the syllables that are not present in the original stimuli. The origin of these errors may be due to the endpointing algorithm, which leaves a small fraction of the initial and final silences in the utterances. During the silences the speech articulators may adopt any position and, hence, it is possible that the short silences left by the endpointing algorithm are synthesised as audible speech. For the consonants, the second most frequent errors are the deletions. These errors might be due to the phone being omitted because its duration is very short, as in the stop [p], or PMA not capturing enough information to distinguish the phone, as in [h].For the purpose of better understanding the intelligibility results shown in Fig. 10, it is illustrative to compare those results with the objective results achieved by the MLE-based system in Fig. 9. We see that both figures are visually similar, thus indicating that, as can be expected, the phones with the highest MCD values are more likely to be mis-recognised. This visual analysis is confirmed when we compute the Pearson correlation coefficient between the results in both figures: the correlation between the average MCD for each phone in Fig. 9 and the sum of errors in Fig. 10 is ρ=0.60. This correlation is further increased up to ρ=0.75 when the phones [θr] are omitted from the analysis: these are the ones for which the intelligibility and MCD results are less similar. Below, we suggest why the intelligibility and MCD results differ for these phones.The last statistical analysis we performed on the data from the intelligibility test was an analysis of the consonant confusions due to the substitution errors shown in Fig. 10. The results of this analysis are presented in Fig. 11as a confusion matrix. The interpretation of the matrix is as follows. Rows correspond to the actual consonant in the original stimuli. Columns correspond to substitutions errors, that is, the consonant that the subjects reported hearing. Finally, the cells contain the number of confusions for each pair of consonants (e.g. [k] is confused with [h] 4 times).In general, we can see that the confusions are more uniformly distributed for the consonants that have fewer errors in Fig. 10, such as [m f l s]. On the other hand, the consonants [p θ r k h], which are the ones that are more frequently confused, posses a less uniform distribution in the errors, being those consonants often confused with another particular consonant. For example, it can be seen that, among the 15 substitutions errors for [p], 67.7% of them (10) correspond to confusions with [h]. By listening to the erroneously transcribed speech stimuli for [p] we realise that the problem is that the onset on [p] is not accurately estimated and is oversmoothed, so that this consonant is easily confused with [h] when accompanied with a vowel. Another interesting confusion is that of [θ] with [f]. Although articulated differently, both consonant are fricatives and are acoustically similar, which might explain why they are so often confused. The confusion of [r] with [l] may be due to the two phones being acoustically similar because they are articulated in roughly the same position of the mouth and both are approximant consonants. Regarding the confusions for [k] and [h], it is hard to extract any meaningful conclusions about the confusions of these consonants, as no magnet is currently attached in the velar and glottal areas in the PMA prototype. We see that, for example, [k] is often confused with [t] and [h] with [p]. For the confusions of [k] with [t], a possible explanation is that both are plosive consonants with similar acoustics. In the case of [h], it might be that PMA is picking some information from the tongue or lips that is similar to that captured when [p] is articulated. Finally, it is worth mentioning that the pattern of confusions in Fig. 11 is somewhat similar to that obtained by means of speech recognition experiments in other SSIs Hueber et al. (2008), Wand and Schultz (2011).

@&#CONCLUSIONS@&#
In this paper we have presented a system for producing audible speech from speech-articulator movement captured using a technique known as permanent magnet articulography. We have successfully demonstrated that the proposed technique is able to generate speech of sufficient intelligibility and quality for some vocabularies. This is a big step in our long-term goal of developing a discrete and reliable SSI that will ultimately allow laryngectomees to recover their voice. However, before the proposed technique can be applied in an realistic treatment scenario, a number of questions need to be addressed. A first question is related to the capabilities of PMA for modelling the vocal tract. As demonstrated by the results presented in this paper, the current prototype has some limitations for detecting certain aspects of speech articulation (e.g. the manner of articulation, voicing and the phones articulated at the back of the mouth). A second question relates to the quality of reconstructed speech. This includes improving its naturalness by also recovering the prosodic information (i.e. voicing information and stress) and also improving the conversion accuracy for a large vocabulary. Finally, another important question concerns the practical implementation of the proposed speech restoration system to patients who have already lost their voice and for whom it is impossible to record the parallel data used to train the system. Solutions to all these questions are currently under development.