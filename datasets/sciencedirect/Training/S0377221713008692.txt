@&#MAIN-TITLE@&#
Superquantile regression with applications to buffered reliability, uncertainty quantification, and conditional value-at-risk

@&#HIGHLIGHTS@&#
We present a generalized regression technique centered on a superquantile (also called conditional value-at-risk).It yields more conservatively fitted curves than classical least-squares and quantile regressions.We show the existence and possible uniqueness of regression functions.We discuss the stability of regression functions under perturbations.We propose a goodness-of-fit criterion.

@&#KEYPHRASES@&#
Generalized regression,Superquantiles,Conditional value-at-risk,Uncertainty quantification,Buffered failure probability,Stochastic programming,

@&#ABSTRACT@&#
The paper presents a generalized regression technique centered on a superquantile (also called conditional value-at-risk) that is consistent with that coherent measure of risk and yields more conservatively fitted curves than classical least-squares and quantile regression. In contrast to other generalized regression techniques that approximate conditional superquantiles by various combinations of conditional quantiles, we directly and in perfect analog to classical regression obtain superquantile regression functions as optimal solutions of certain error minimization problems. We show the existence and possible uniqueness of regression functions, discuss the stability of regression functions under perturbations and approximation of the underlying data, and propose an extension of the coefficient of determination R-squared for assessing the goodness of fit. The paper presents two numerical methods for solving the error minimization problems and illustrates the methodology in several numerical examples in the areas of uncertainty quantification, reliability engineering, and financial risk management.

@&#INTRODUCTION@&#
Analysts and decision makers are often concerned with a random variable describing possible ‘cost,’ ‘loss,’ or ‘damage.’ The interest may be focused on a single ‘system’ or could involve study and comparison across a multitude of systems and designs. In either case, it may be beneficial to attempt to approximate such a loss random variable Y in terms of an n-dimensional explanatory random vector X that is more accessible in some sense. This situation naturally leads to least-squares regression and related models that estimate conditional expectations. While such models are adequate in many situations, they fall short in contexts where a decision maker is risk averse, i.e., is more concerned about upper-tail realizations of Y than average loss, and views errors asymmetrically with underestimating losses being more detrimental than overestimating. We focus on such contexts and therefore maintain an orientation of Y that implies that high realizations are unfortunate and low realizations are favorable. Of course, a parallel development with an opposite orientation of the random variable Y, focused on profits and gains, and concerns about overestimating instead of underestimating is also possible but not pursued here.Quantile regression (see Gilchrist, 2008; Koenker, 2005 and references therein) accommodates risk-averseness and an asymmetric view of errors by estimating conditional quantiles at a certain probability level such as those in the tail of the conditional distribution of Y. While suitable in some contexts, quantile regression only deals with the signs of the errors and therefore is overly ‘robust’ in the sense that large portions of a data set can change dramatically without impacting the best-fitting regression function. A quantile corresponds to ‘value-at-risk’ (VaR) in financial terminology and relates to ‘failure probability’ in engineering terms. Quantile regression informs the decision maker about these quantities conditional on values of the explanatory random vector X. However, a quantile is not a coherent measure of risk in the sense of Artzner, Delbaen, Eber, and Heath (1999) (see also Delbaen, 2002); it fails to be subadditive. Consequently, a quantile of the sum of two random variables may exceed the sum of the quantiles of each random variable at the same probability level, which runs counter to our understanding of what ‘risk’ should express. Moreover, quantiles cause computational challenges when incorporated into decision optimization problems as objective function, failure probability constraint, or chance constraint. The use of quantiles and the closely related failure probabilities is therefore problematic in risk-averse decision making; see (Artzner et al., 1999; Krokhmal, Zabarankin, & Uryasev, 2011; Rockafellar & Royset, 2010; Rockafellar & Uryasev, 2000, 2013) for a detailed discussion.A superquantile of a random variable, also called conditional value-at-risk, average value-at-risk, and expected shortfall,1We prefer the application-neutral name ‘superquantile’ when deriving methods applicable broadly.1is an ‘average’ of certain quantiles as described further below. It is a coherent measure of risk well suited for risk-averse decision making and optimization; see (Wang & Uryasev, 2007) for its application in financial engineering, (Kalinchenko, Veremyev, Boginski, Jeffcoat, & Uryasev, 2011) for military applications, and (Rockafellar & Royset, 2010) for use in reliability engineering. While this risk measure has reached prominence in risk-averse optimization, there has been much less work on regression techniques that are consistent in some sense with it. In this paper, we derive such a superquantile regression methodology, study its properties, and propose means to assess the goodness-of-fit. The importance of such a regression methodology becomes apparent by considering the following two situations.Suppose that a loss is given by a random variable Y, but our primary concern is with the conditional loss given that an explanatory random vector X takes on specific values. We aim to select these values judiciously in an effort to minimize the conditional loss. We denote byY(x)the conditional random variable Y given thatX=x∈Rn. Of course, ‘minimizing’Y(x)is not well-defined and a standard approach is to minimize a risk measure ofY(x); see for example (Krokhmal et al., 2011; Rockafellar & Uryasev, 2013). An attractive choice is to use a superquantile measure of risk, which as mentioned above is coherent and also computationally approachable. While in some contexts a superquantile ofY(x)can be evaluated easily for anyx∈Rn, there are numerous situations, especially beyond the financial domain, where only a data base of realizations ofY(x)is available for various x. In the latter situation, there is a need for building an approximating model, based on the data, for the relevant superquantile ofY(x)as a function of x. We refer to this as superquantile tracking. In comparison, if the goal were to minimize the expectation ofY(x), then least-squares regression would yield a model that approximates that conditional expectation. Likewise, if the goal were to minimize a quantile ofY(x), quantile regression would provide a model of the conditional quantile. While these models are valuable for analysts and decision makers focused on the expectation and quantile risk measures, they do not provide estimates of conditional superquantiles. In essence, the same need for estimating conditional superquantiles arises in reliability engineering when the goal is to determine a ‘design’ x with buffered failure probability ofY(x)being no larger than a given probability level, which corresponds to a constraint on a superquantile ofY(x)(Rockafellar & Royset, 2010).Another situation arises when the explanatory random vector X is beyond our direct control, but the dependence between the loss random variable Y and X makes us hopeful that, for a carefully selected regression functionf:Rn→R, the random variablef(X)may serve as a surrogate for Y. When the distribution of X is known, at least approximately, and f has been determined, then the distribution off(X)is usually easily accessible. That distribution may then serve as input to further analysis, simulation, and optimization in place of the unknown distribution of Y. Such surrogate estimation may arise in numerous contexts. ‘Factor models’ in financial investment applications (see for example Conner, 1995; Knight & Satchell, 2005), where Y may be the loss associated with a particular asset and X a vector describing a small number of macroeconomic ‘factors,’ is a result of surrogate estimation. ‘Uncertainty quantification’ (see for example Eldred, Swiler, & Tang, 2011; Lee & Chen, 2009) considers the output of a system described by a random variable Y, for example measuring damage, and estimates its moments and distribution from observed realizations as well as knowledge about the distribution of the input to the system characterized by a random vector X. A main approach here centers on surrogate estimation withf(X)serving as an estimate of Y. In this situation, an essential question is what criterion should be used for selecting f. Clearly, one would like the error random variableZf≔Y-f(X)to be small in some sense. However, minimizing the mean-squared error ofZfwould not reflect a greater concern about underestimating Y, i.e., underestimating losses, than overestimating. We may want to assess the error ofZfin a manner that is ‘consistent’ with our use of a superquantile as risk measure and weigh large levels of underestimation more heavily than smaller levels.In this paper, we develop a ‘generalized’ regression technique that addresses the issue of superquantile tracking and surrogate estimation. The technique is an extension of least-squares and quantile regression, which center on expectations and quantiles, respectively, to one that focuses on superquantiles.The foundation of least-squares and quantile regression is the fact that mean and quantiles minimize the expectation of certain convex random functions. A natural extension to superquantile regression could then possibly involve determining a random function that when minimizing its expectation, we obtain a superquantile. However, such a random function does not exist (Chun, Shapiro, & Uryasev, 2012; Gneiting, 2011), which has lead to studies of indirect approaches to superquantile tracking grounded in quantile regression.For a random variable with a continuous cumulative distribution function, a superquantile equals a conditional expectation of the random variable given realizations no lower than the corresponding quantile. Utilizing this fact, studies have developed kernel-based estimators for the conditional probability density functions, which are then integrated and inverted to obtain estimators of conditional quantiles. An estimator of the conditional superquantile is then finally constructed by integrating the density estimator over the interval above the quantile (Cai & Wang, 2008; Scaillet, 2005) or forming a sample average (Kato, 2012). These studies also include asymptotic analysis of the resulting estimators under a series of assumptions, including that the data originates from certain time series.A superquantile of a random variable is defined in terms of an integral of corresponding quantiles with respect to the probability level. Since the integral is approximated by a weighted sum of quantiles across different probability levels, an estimator of a conditional superquantile emerges as the sum of conditional quantiles obtained by quantile regression; see (Leorato, Peracchi, & Tanase, 2012; Peracchi & Tanase, 2008), which also show asymptotic results under a set of assumptions including the continuous differentiability of the cumulative distribution function of the conditional random variables. Similarly, (Chun et al., 2012) utilizes the integral expression for a superquantile, but observes that a weighted sum of quantiles is an optimal solution of a certain minimization problem; see (Rockafellar & Uryasev, 2013). Analogously to the situation in least-squares and quantile regression, an optimization problem therefore yields an estimator of a conditional superquantile. Though, in contrast to the case of least-squares and quantile regression, the estimator is ‘biased’ due to the error induced by replacing an integral by a finite sum. Under a linear model assumption, (Chun et al., 2012) also constructs a conditional superquantile estimator using an appropriately shifted least-squares regression curve based on quantile estimates of residuals. In both cases, asymptotic results are obtained for a homoscedastic linear regression model. Under the same model, (Trindade, Uryasev, Shapiro, & Zrazhevsky, 2007) studies ‘constrained’ regression, where the error random variableZf=Y-f(X)is minimized in some sense, for example in terms of least square or absolute deviation, subject to a constraint that limits a superquantile ofZf. While this approach does not lead to superquantile regression in the sense we derive below, it highlights the need for alternative techniques for regression that incorporate superquantiles in some manner.The need for moving beyond classical regression centered on conditional expectations is therefore now well recognized and has driven even further research towards estimating conditional distribution function, i.e.,Prob(Y(x)⩽y)for ally∈R, using nonparametric kernel estimators (see for example Hall & Muller, 2003) and transformation models (see for example Hothorn, Kneib, & Buhlmann, in press). Of course, conditional distribution functions provide the ‘full’ information aboutY(x)including its quantiles and superquantiles, and therefore also provide a means to inform a risk-averse decision maker. In this paper, however, we directly focus on superquantiles, which we believe deserve special attention due to their prominence in risk analysis.A framework for ‘generalized’ regression is laid out in Rockafellar, Uryasev, and Zabarankin (2008), Rockafellar and Uryasev (2013) and regression functions are obtained as optimal solutions of optimization problems of the formminfE(Zf), whereEis a measure of error and f is restricted to a certain class of functions such as the affine functions. Least-squares regression is obtained byE(Zf)=E[Zf2], quantile regression with the Koenker–Bassett measure of error, but many other possibilities exist. While it is not possible to determine a measure of error that is of the expectation type and yields a superquantile, in this paper we show that when allowing for a broader class of functionals, a measure of error that generates a superquantile is indeed available. Such a measure of error is also hinted at in our recent paper (Rockafellar & Royset, in press), but the present paper gives the first comprehensive treatment. In contrast to previous studies towards superquantile tracking, which utilize indirect approaches and quantile regression, we here offer a natural extension of least-squares and quantile regression. We replace the mean-squares and Koenker–Bassett error measures by a new error measure, and then simply minimize that error ofZfto obtain a regression function. Under few assumptions, we establish the existence of a regression function, discuss its uniqueness, and examine stability under perturbations of the distribution of(X,Y)for example caused by sampling. We omit a discussion of simple linear models with independent and identically distributed (i.i.d.) noise as we believe that there is little need for quantile and superquantile regression in such contexts as least-squares regression with an appropriate shift suffices. In fact, we do not separate models into (additive) deterministic and stochastic terms. In many applications, especially in the area of uncertainty quantification, heteroscedasticity and dependence are prevalent making linear i.i.d. and additive models of little value.The main contributions of this paper is the development of a novel regression technique that naturally extends least-squares and quantile regression to contexts where one seeks to assess regression errors not by squaring them, as in the case of least-squares regression, or by looking at their signs, as in the case of quantile regression, but by weighing larger levels of underestimation increasingly heavily in a manner consistent with superquantiles. We develop the fundamental theory for the new regression technique by examining the issues of existence, uniqueness, stability, rate of convergence, and goodness of fit.Section 2 describes measures of regret and error, first in the context of quantile regression and then for the extension to superquantile regression. Section 3 defines superquantile regression as the minimization of a measure of error, discusses existence and uniqueness of the regression function, and provides asymptotic results. Section 4 proposes an approach for assessing the goodness-of-fit of regression function obtained by superquantile regression. Section 5 deals with computational methods for superquantile regression and Section 6 gives illustrative examples.While our development centers on superquantiles, it is beneficial to maintain a parallel description of quantiles. As we see below, quantile regression, which is achieved by minimizing a Koenker–Bassett error of the random variableZf, provides a road map for the construction of superquantile regression, which is simply achieved by minimizing another measure of error. We start, however, with definitions of quantiles, superquantiles, and corresponding measures of regret and error.Forα∈[0,1], theα-quantile of a random variable Y with cumulative distribution functionFYis defined asqα(Y)≔min{y∈R|FY(y)⩾α}.Its quantiles are as fundamental to Y as the distribution function, but are problematic to incorporate in risk analysis and optimization due to their lack of coherency as well as computational challenges. Superquantiles have more favorable properties. Forα∈[0,1), theα-superquantile of a random variable Y is defined as(1)q¯α(Y)≔11-α∫α1qβ(Y)dβ.Since a superquantile is a coherent measure of risk and by the virtue of being an ‘average’ of quantiles is also more stable than a quantile in some sense, it is well suited for applications. Forα=1, we defineq¯α(Y)≔supY(the essential supremum). Sinceq¯0(Y)=E[Y], we therefore focus onα∈(0,1)throughout the paper to avoid distractions by these special cases.In reliability terminology, quantiles and superquantiles correspond to failure and buffered failure probabilities. The failure probability of a loss random variable Y isp(Y)≔Prob(Y>0)=1-FY(0),which corresponds top(Y)=1-αwithαsuchthatqα(Y)=0if there is no probability atom at zero. Analogously to the latter expression, the buffered failure probability (see Rockafellar & Royset, 2010) of a loss random variable Y is defined as(2)p¯(Y)≔1-αwithαsuchthatq¯α(Y)=0.A requirement thatp¯(Y)⩽1-αis therefore equivalent to the constraint thatq¯α(Y)⩽0. Consequently, in applications with a buffered failure probability constraint on a (conditional) loss random variableY(x)as well as when the goal is to minimize a superquantile ofY(x)directly, there are needs for estimatingq¯α(Y(x))as a function ofx∈Rn. Quantiles and superquantiles are connected through a trade-off formula that leads to quantile regression as discussed next.Bothα-quantiles andα-superquantiles,α∈[0,1), of a loss random variable Y are expressed in terms of an optimization problem involving the quantity(3)Vα(Y)≔11-αE[max{Y,0}],which is a measure of regret that quantifies the displeasure with realizations of Y above zero; see (Rockafellar & Uryasev, 2013). Quantiles and superquantiles then follow as(4)qα(Y)∈argminC0∈R{C0+Vα(Y-C0)}(5)q¯α(Y)=minC0∈R{C0+Vα(Y-C0)},where in factqα(Y)is the lowest optimal solution if multiple exists.The expression forqα(Y)is the essential building block for quantile regression, but since we ultimately would like to go beyond the class of constant functions as candidates for a regression function we need to pass to a measure of errorEαconstructed fromVαby settingEα(Y)≔Vα(Y)-E[Y]for any loss random variable Y (withE[|Y|]<∞). A measure of error quantifies the degree of ‘nonzeroness’ in a random variable; see (Rockafellar & Uryasev, 2013). Direct application of this definition and a recognition that a constant term in an objective function is immaterial with respect to the optimal solution gives that(6)qα(Y)∈argminC0∈REα(Y-C0)andEα(Y)=11-αE[max{Y,0}]-E[Y]=Eα1-αmax{Y,0}+max{-Y,0}is a (scaled) Koenker–Bassett error (Koenker, 2005). Quantile regression centers on computing this argmin with “minimizing the error ofY-C0overC0∈R” replaced by “minimizing the error ofY-f(X)over a class of functionsf:Rn→R”, often taken to be the affine functions. We viewqα(Y)as the ‘closest’ scalar to the random variable Y under a Koenker–Bassett error.If our goal simply were to estimateq¯α(Y)of a loss random variable Y for a givenα∈(0,1), the above expressions would have sufficed, possibly passing to an empirical distribution given by a sample ifFYis unknown. In the present context, however, connections with the underlying explanatory random vector X and the focus on the ‘approximation’ of Y warrants a parallel development to that of quantile regression centered on a superquantile. In view of the above review of quantile regression, it is clear that superquantile regression will involve the minimization of some measure of error that returns the superquantile as argmin.2Classical least-squares regression can be viewed similarly as returning a (conditional) expectation as argmin when minimizing mean-square measure of error, i.e.,E[Y]=argminC0∈RE[(Y-C0)2].2The next subsection develops such a measure by first constructing a corresponding measure of regret.We start this subsection by establishing the finiteness of a superquantile under the assumption that the loss random variable Y has a finite second moment and writeY∈L2(Ω)≔{Y:Ω→R|E[Y2]<∞}.We know from Rockafellar and Uryasev (2013) thatq¯αis a convex, positively homogenous, monotonic, and averse3We recall that a functionalF:L2(Ω)→R‾=R∪{-∞,∞}is averse ifF(X)>E[X]for all nonconstantX∈L2(Ω).3functional onL2(Ω)forα∈(0,1). From Rockafellar and Royset (in press, Theorem 3), we also know that it is bounded and we repeat this result with a new proof. We adopt the notationσ2(Y)=E[(Y-E[Y])2].Proposition 1ForY∈L2(Ω)andα∈(0,1)one has that(7)q¯α(Y)⩽E[Y]+11-ασ(Y).Suppose that the quantileqα(Y), viewed as a function of the probability level, is continuous atα. LetIαbe the indicator function of the interval[qα(Y),∞)with probability1-α. We then have by the Schwartz inequality that(1-α)q¯α(Y-E[Y])=E[(Y-E[Y])Iα]⩽E[(Y-E[Y])2]E[Iα2]=σ(Y)1-α.Then, sinceq¯α(Y-E[Y])=q¯α(Y)-E[Y], the result follows from dividing by1-α. Thus, (7) is valid under the continuity assumption about the quantile, which is true for all but at most countably manyα. By continuity of both sides of (7) with respect toα, it must then hold for allα∈(0,1). □The measure of regret that serves in the context of superquantile regression is defined for any loss random variable Y andα∈(0,1)as(8)V‾α(Y)≔11-αV‾0(Y),where(9)V‾0(Y)≔∫01max{0,q¯β(Y)}dβ.These expressions appear in Rockafellar and Royset (in press), where their discovery, which is related to the Hardy–Littlewood transform, is described. Here, we provide an alternative, direct proof of how they lead to a superquantile. We start, however, with two preliminary results and the definition of a corresponding error measure.Lemma 1ForY∈L2(Ω),(10)V‾0(Y)⩽σ(Y)+max{0,E[Y]+σ(Y)}.From (7) and (9) we have(11)V‾0(Y)⩽∫01max{0,θY(β)}dβforθY(β)=E[Y]+11-βσ(Y).We consider three cases. In Case 1, we suppose thatθY(β)⩾0for allβ∈[0,1]. Then the right hand side of (11) is given by(12)∫01θY(β)dβ=E[Y]+σ(Y)∫01(1-β)-1/2dβwith∫01(1-β)-1/2dβ=2.Therefore,V‾0(Y)⩽E[Y]+2σ(Y)in Case 1. In Case 2a, we suppose thatθY(β)⩽0for allβ∈(0,1). Then obviouslyV‾0(Y)⩽0. Finally, in Case 2b, letθY(β)<0for someβ∈(0,1), but not all. Then necessarilyσ(Y)>0andE[Y]⩽-σ(Y), andθY(β)strictly increases with respect toβ. Letα¯be the uniqueβ∈(0,1)withθY(α¯)=0, namely when1-α¯=(σ(Y))/(-E[Y]). Then we have that∫01max{0,θY(β)}dβ=∫α¯1θY(β)dβ=(1-α¯)E[Y]+σ(Y)∫α¯1(1-β)-1/2dβ=(1-α¯)E[Y]+2σ(Y)1-α¯=σ(Y)2E[Y]2E[Y]+2σ(Y)σ(Y)-E[Y]=σ(Y)2-E[Y]⩽σ(Y).Thus, in Case 2b we getV‾0(Y)⩽σ(Y). The conclusion then follows by putting together the cases. □We observe that forα∈(0,1),V‾αis a convex, positively homogeneous, monotonic, and averse functional onL2(Ω), which follows from the properties of the superquantile (Rockafellar & Uryasev, 2013), and by the above result it is also finite, and consequently continuous. A corresponding measure of error is defined forY∈L2(Ω)by(13)E‾α(Y)≔V‾α(Y)-E[Y]and referred to as a superquantile error. Obviously,E‾αis also convex and positively homogeneous. It also satisfies the following properties.Proposition 2For anyα∈(0,1)andY∈L2(Ω), a superquantile error satisfies(a)E‾α(Y)=0whenY≡0,E‾α(Y)>0whenY≢0, andE‾α(Y)⩾min{1,α/(1-α)}|E[Y]|.Sinceq¯β(0)=0for allβ∈[0,1], (a) follows trivially.SinceV‾αis averse, we have that forY∈L2(Ω),E‾α(Y)=V‾α(Y)-E[Y]>E[Y]-E[Y]=0when Y is not a constant. To complete part (b), we therefore only need to consider nonzero constants. If Y is a positive constant K, then11-α∫01max{0,q¯β(Y)}dβ-E[Y]>∫01max{0,q¯β(Y)}dβ-E[Y]=K-E[Y]=0.If Y is a negative constant K, then11-α∫01max{0,q¯β(Y)}dβ-E[Y]=11-α∫01max{0,K}dβ-E[Y]=0-E[Y]>0,which completes part (b).Sinceq¯β(Y)⩾E[Y]for allβ∈[0,1], we have wheneverE[Y]⩾0the bound11-α∫01max{0,q¯β(Y)}dβ-E[Y]⩾11-α∫01max{0,E[Y]}dβ-E[Y]=α1-αE[Y].WhenE[Y]<0,11-α∫01max{0,q¯β(Y)}dβ-E[Y]⩾11-α∫01max{0,E[Y]}dβ-E[Y]=-E[Y].Part (c) then follows by combining the two results.□By Proposition 2 and the above discussion,E‾αis a regular measure of error in the sense of Rockafellar and Uryasev (2013).We are now ready to show that a superquantile is a unique optimal solution of optimization problems involvingV‾αandE‾α. As mentioned, the connection between a superquantile andV‾αis also reached in Theorem 7 of Rockafellar and Royset (in press) through different means. The direct proof in the present paper and the connection with a superquantile error are new.Theorem 1Superquantile as optimal solutionForY∈L2(Ω)andα∈(0,1),(14)q¯α(Y)=argminC0∈R{C0+V‾α(Y-C0)}=argminC0∈RE‾α(Y-C0).Letφ(C)=C+V‾α(Y-C)andψβ(C)=max{0,q¯β(Y)-C}. These are both convex functions of C, andψβis nonincreasing. We can use the criterion thatC‾∈argminCφ(C)⇔φ+′(C‾)⩾0,φ-′(C‾)⩽0,where, because of the monotonicity ofψβ,φ+′(C)=1+11-α∫01(ψβ)-′(C)dβ,φ-′(C)=1+11-α∫01(ψβ)+′(C)dβ,(ψβ)+′(C)=-1ifq¯β(Y)>C,0ifq¯β(Y)⩽C,(ψβ)-′(C)=-1ifq¯β(Y)⩾C,0ifq¯β(Y)<C.Therefore∫01(ψβ)+′(C)dβ=∫01(ψβ)-′(C)dβ=-(1-γ)forC=q¯γ(Y),in which case(ψβ)′(C)=(ψβ)+′(C)=(ψβ)-′(C)=1-(1-γ)/(1-α). Thus,(ψβ)′(C)=0corresponds toC=q¯γ(Y)forγ=α. Consequently, the first equality of the theorem holds. The second follows directly from (13) and the fact that a constant in an objective function is immaterial with regard to the argmin. □Being analogous to (4) and (6), the foundations for quantile regression, the expressions (14) provide the path to superquantile regression as developed in the remainder of the paper. In fact, Theorem 1 shows thatq¯α(Y)is the uniquely ‘closest’ scalar to Y in the sense of the superquantile error.While not the focus here, the optimal value in (14) defines a measure of risk (see Rockafellar & Royset, in press)R‾α(Y)≔minC0∈R{C0+V‾α(Y-C0)}=q¯α(Y)+V‾α(Y-q¯α(Y))forY∈L2(Ω)analogously toq¯α(Y)in (5). A corresponding measure of deviation, which quantifies the nonconstancy in a random variable, is given byD‾α(Y)≔minC0∈RE‾α(Y-C0)=R‾α(Y)-E[Y].We note that parallel to (1) (see Rockafellar & Royset, in press),R‾α(Y)=1/(1-α)∫α1q¯β(Y)dβand, consequently,D‾α(Y)=11-α∫α1q¯β(Y)dβ-E[Y].The measures of regret, error, risk, and deviationV‾α,E‾α,R‾α, andD‾α,α∈(0,1), form a family of risk quadrangles in the sense of Rockafellar and Uryasev (2013) that corresponds to the statisticq¯α. The measure of deviationD‾αplays a central role in the remainder of the paper as it facilitates simplifications, goodness-of-fit tests, and computational methods.Theorem 1 and the development leading to quantile regression direct us to a new regression methodology that is centered on a superquantile error. The next subsection poses the regression problem, provides its properties, and discusses stability under perturbations. The section ends with a discussion of superquantile tracking.While Theorem 1 shows that the ‘best’ scalar approximation of a random variable Y in the sense of a superquantile error is the corresponding superquantile, we now go beyond the class of constant functions to utilize the connection with an underlying explanatory random vector X. We focus on regression functions of the formf(x)=C0+〈C,h(x)〉,C0∈R,C∈Rm,for a given ‘basis’ functionh:Rn→Rm. This class satisfies most practical needs including that of linear regression wherem=nandh(x)=x. Extensions beyond this class are also possible but not dealt with here.For anyh:Rn→Rmandα∈(0,1), we define the superquantile regression problemP:minC0∈R,C∈RmE‾α(Z(C0,C)),whereZ(C0,C)≔Y-(C0+〈C,h(X)〉)is the error random variable, whose distribution depends onC0,C,h, and the joint distribution of(X,Y). We denote byC‾⊂Rm+1the set of optimal solutions of P and refer to(C‾0,C‾)∈C‾as a regression vector.The objective functionE‾α(Z(·,·))is well-defined and finite when the distribution of(X,Y)and h is such thatZ(C0,C)∈L2(Ω)for allC0∈R,C∈Rm. A sufficient condition that ensures this property is thatY,h1(X),…,hm(X)∈L2(Ω)as shown next, where we adopt the notationH=h(X),Hi=hi(X),i=1,2,…,m.Lemma 2IfY,H1,…,Hm∈L2(Ω), thenZ(C0,C)∈L2(Ω)for allC0∈R,C∈Rm.LetM<∞be such thatE[Y2]⩽MandE[Hi2]⩽M,i=1,2,…,m. Since|〈C,H〉|⩽‖C‖∑i=1m|Hi|and〈C,H〉2⩽‖C‖2∑i=1m(Hi)2, we find thatE[|〈C,H〉|]⩽‖C‖mMandE[〈C,H〉2]⩽‖C‖2mM. Consequently,(15)E[(Y-C0-〈C,H〉)2]⩽E[(Y-C0)2]+2|E[(Y-C0)〈C,H〉]|+E[〈C,H〉2]⩽M+2(‖C‖m1/2M+(M+|C0|)‖C‖mM)+‖C‖2mM.□In surrogate estimation,C‾0+〈C‾,h(X)〉, with(C‾0,C‾)∈C‾, provides the best approximation of Y in the sense of a superquantile error. For example, after having computed(C‾0,C‾), the analysis could proceed with examining the moments, quantiles, and superquantiles ofC‾0+〈C‾,h(X)〉as surrogates for the corresponding quantities of Y. If X is Gaussian and h is affine, thenC‾0+〈C‾,h(X)〉is a Gaussian approximation of Y easily examined and utilized in further studies. It may also be of interest to examineC‾0+〈C‾,h(X)〉under hypothetical distributions of X.A direct consequence of the Regression Theorem in Rockafellar and Uryasev (2013) (see also Theorem 3.1 in Rockafellar et al., 2008) we obtain that a regression vector can equivalently be determined from a measure of deviationD‾α.Proposition 3Suppose thatY,H1,…,Hm∈L2(Ω). Then, the set of regression vectorsC¯of P is equivalently obtained asC‾={(C‾0,C‾)∈Rm+1|C‾∈argminC∈RmD‾α(Z0(C)),C‾0=q¯α(Z0(C‾))},whereZ0(C)≔Y-〈C,h(X)〉.Proposition 3 implies computational advantages as the(m+1)-dimensional optimization problem P is replaced by a problem in m dimensions with a simpler objective function, which we fully utilize in Sections 5 and 6. Moreover, the result also proves beneficial in analysis of regression vectors.The existence of a regression vector is ensured by the next result, which also provides conditions for uniqueness.Theorem 2Existence and uniqueness of regression vectorIfY,H1,…,Hm∈L2(Ω), then P is a convex problem with a set of optimal solutionsC‾that is nonempty, closed, and convex.(a)C‾is bounded if and only if the random vector X and the basis function h satisfy the condition that〈C,h(X)〉is not constant unlessC=0.If in addition, for every(C0,C),(C0′,C′)∈Rm+1, withC≠C′, there exists aβ0∈[0,1)such that(16)0⩽q¯β(Z(C0,C)+Z(C0′,C′))<q¯β(Z(C0,C))+q¯βZC0′,C′for allβ∈[β0,1), thenC‾is a singleton.SinceY∈L2(Ω)implies thatE‾α(Y)<∞by Lemma 1, we deduce the two first conclusions from Theorem 3.1 in Rockafellar et al. (2008). Hence, we only need to show thatC‾is a singleton.Suppose for the sake of a contradiction that(C0,C),(C0′,C′)∈C‾and(C0,C)≠C0′,C′, with corresponding optimal valueξ⩾0, i.e.,ξ=E¯α(Z(C0,C))=E‾αZC0′,C′. We consider two cases.First, suppose thatξ=0. By Proposition 2,Z(C0,C)=ZC0′,C′=0and consequentlyC0+〈C,H〉=C0′+〈C′,H〉,which implies that〈C-C′,H〉=C0′-C0. Under the assumption that〈C,h(X)〉is only constant whenC=0, we must have thatC-C′=0. Then, alsoC0′-C0=0follows, which contradicts the hypothesis that(C0,C)≠(C0′,C′).Second, suppose thatξ>0. IfC=C′, then a direct consequence of Proposition 3 and the fact that every random variable has a unique superquantile at each probability level, is that alsoC0=C0′, which again contradicts our hypothesis. Consequently, we focus on the case withC≠C′, for which there exists aβ0such that (16) holds for allβ∈[β0,1). Trivially, thenmax0,q¯βZ(C0,C)+ZC0′,C′<max{0,q¯β(Z(C0,C))}+max0,q¯βZC0′,C′forβ∈[β0,1). Ifβ∈(0,1)is such thatq¯βZ(C0,C)+ZC0′,C′<0, thenmax0,q¯βZ(C0,C)+ZC0′,C′⩽max{0,q¯β(Z(C0,C))}+max0,q¯βZC0′,C′as the left-hand side vanishes and the right-hand side is nonnegative. Hence,∫01max0,q¯βZ(C0,C)+ZC0′,C′dβ<∫01max{0,q¯β(Z(C0,C))}dβ+∫01max0,q¯βZC0′,C′dβand also(17)E‾α(Z(C0,C)+ZC0′,C′)<E‾α(Z(C0,C))+E‾αZC0′,C′.LetC0″,C″=(1/2)(C0,C)+(1/2)C0′,C′and therefore2ZC0″,C″=Z(C0,C)+ZC0′,C′.By the optimality ofξ, the positive homogeneity ofE‾α, and (17), we find that2ξ⩽2E‾αZC0″,C″=E‾α2ZC0″,C″<E‾α(Z(C0,C))+E‾αZC0′,C′=2ξ,which cannot hold. In view of this contradiction, the conclusion follows. □While Theorem 2 gives a sufficient condition for uniqueness of the regression vector, in general uniqueness cannot be expected. For example, suppose that the random vector(X,Y), with X scalar valued, has the possible and equally likely realizations(1,1),(2,2), and(3,1). Then,q¯β(Z0(C))=max{1-C,2-2C,1-3C}forβ>2/3andE[Z0(C)]=4/3-2C. It is straightforward to show that forα>2/3, anyC∈[-1,1]minimizesD‾α(Z0(·)). Consequently, in view of Proposition 3, anyC∈[-1,1], with a correspondingC0=max{1-C,2-2C,1-3C}, minimizesE‾α(Z(·,·))forα>2/3. The minimum error is2/3.A unique regression vector is indeed achieved in the normal case as stated next.Proposition 4Suppose that(H,Y)is normally distributed with positive definite variance–covariance matrix. Then,C‾is a singleton.LetΣbe the variance–covariance matrix of(H,Y), with Cholesky decompositionΣ=LL⊤. For anyβ∈(0,1)andC∈Rm,Z0(C)is also normal with meanE[Z0(C)]=〈C∼,E[(H,Y)]〉and varianceσ2(Z0(C))=〈C∼,ΣC∼〉, whereC∼=(-C,1). Thus,q¯β(Z0(C))=E[Z0(C)]+kβσ(Z0(C))=E[Z0(C)]+kβ‖L⊤C∼‖,wherekβ=ϕ(Φ-1(β))/(1-β), withϕandΦbeing the standard normal probability density and cumulative distribution functions, respectively.ForC,C′∈Rm, withC≠C′, there is no constantk>0such that(-C,1)=k(-C′,1). LetC∼=(-C,1)andC∼′=(-C′,1). SinceΣis positive definite, the upper-triangular matrixL⊤is unique and full rank. Consequently, the null space ofL⊤contains only the zero vector andL⊤(C∼-kC∼′)≠0for all scalarsk>0. Since the triangle inequality for two vectors holds strictly whenever the two vectors cannot be expressed as a positive multiple of each other, we therefore find that‖L⊤C∼+L⊤C∼′‖<‖L⊤C∼‖+‖L⊤C∼′‖.Now suppose for the sake of a contradiction thatC,C′∈Rmboth minimizeD‾α(Z0(·))and attain the minimum valueξ∈R, butC≠C′. LetC″=(1/2)C+(1/2)C′,C∼″=(-C″,1), andγα=∫α1kβdβ/(1-α)>0. Then,D‾αZ0C″=11-α∫α1q¯β(Z0(C″))dβ-E[Z0(C″)]=E[Z0(C″)]+γα‖L⊤C∼″‖-E[Z0(C″)]=γα2‖L⊤C∼+L⊤C∼′‖<γα2(‖L⊤C∼‖+‖L⊤C∼′‖)=12(E[Z0(C)]+γα‖L⊤C∼‖-E[Z0(C)])+12E[Z0(C′)]+γα‖L⊤C∼′‖-E[Z0(C′)]=12(D‾α(Z0(C)))+12(D‾α(Z0(C′)))=12(ξ+ξ)=ξ.However, this contradicts the optimality ofC,C′and we reach the conclusion. □We next turn to consistency and stability of the regression vector. Of course, the joint distribution of(X,Y)is rarely available in practice and one may need to pass to an approximating empirical distribution generated by a sample. Moreover, perturbations of the ‘true’ distribution of(X,Y)may occur due to measurement errors in the data and other factors. We consider these possibilities and let(Xν,Yν)be a random vector whose joint distribution approximates that of(X,Y)in some sense. For example,(Xν,Yν)may be governed by the empirical distribution generated by an independent and identically distributed sample of sizeνfrom(X,Y). Presumably, asν→∞, the approximation of(X,Y)by(Xν,Yν)improves as stated formally below. Regardless of the nature of(Xν,Yν), we define the approximate error random variableZν(C0,C)≔Yν-C0-〈C,h(Xν)〉,and the corresponding approximate superquantile regression problemPν:minC0∈R,C∈RmE‾α(Zν(C0,C)).The next result shows that as(Xν,Yν)approximates(X,Y), a regression vector obtained fromPνapproximates one from P, which provides the justification for basing a regression analysis onPν. Below, we let→ddenote convergence in distribution andHν=h(Xν)andHiν=hi(Xν),i=1,2,…m.Theorem 3Stability of regression vectorSuppose that(Xν,Yν),ν=1,2,…, and(X,Y)aren+1-dimensional random vectors such that(Xν,Yν)→d(X,Y)and that the basis function h is continuous except possibly on a subsetS⊂RnwithProb(X∈S)=0. Moreover, letHi,Y∈L2(Ω),supνEHiν2]<∞,i=1,2,…,m, andsupνE[(Yν)2]<∞.IfC‾0ν,C‾νν=1∞is a sequence of optimal solutions ofPν, withα∈(0,1), then every accumulation point of that sequence is a regression vector of P.Let(C0,C)∈Rm+1be arbitrary. By the continuous mapping theorem (see for example Theorem 29.2 Billingsley, 1995),Zν(C0,C)=Yν-C0-〈C,h(Xν)〉→dZ(C0,C)=Y-C0-〈C,h(X)〉.By the assumed moment conditions, there exists a constantM<∞that bounds from above the termsmaxiE[|Hi|],maxiE[(Hi)2],supν,iEHiν,supν,iEHiν2,E[|Y|],E[Y2],supνE[|Yν|],supνE[(Yν)2].In view of Lemma 2 and its proof, we deduce that(18)E[(Yν-C0-〈C,Hν〉)2]⩽M+2(‖C‖m1/2M+(M+|C0|)‖C‖mM)+‖C‖2mMfor allν. Hence,Zν(C0,C)is uniformly integrable (for fixedC0,C) and(19)E[Zν(C0,C)]→E[Z(C0,C)]<∞;see (Billingsley, 1995), Theorem 25.12 and its corollary.By Rockafellar and Royset (in press, Theorem 4), a sequence of random variables converges in distribution to a random variable if and only if the correspondingα-superquantiles, viewed as functions of the probability levelα, converge uniformly on every closed subset of(0,1). Consequently,q¯β(Zν(C0,C))→q¯β(Z(C0,C))uniformly inβon closed subsets of(0,1). Moreover, since the 0-superquantile coincides with the expectation, (19) implies thatq¯0(Zν(C0,C))→q¯0(Z(C0,C))also holds. These facts and the observation that the superquantile of any random variable is continuous and nondecreasing as a function of the probability level, ensure that for any∊>0andδ∈(0,1), there exists an integerν(∊,δ)such that for allν⩾ν(∊,δ),(20)supβ∈[0,1-δ]|q¯β(Zν(C0,C))-q¯β(Z(C0,C))|⩽∊2(1-δ).Then,(21)∫01-δmax{0,q¯β(Zν(C0,C))}dβ-∫01-δmax{0,q¯β(Z(C0,C))}dβ(22)⩽∫01-δ|q¯β(Zν(C0,C))-q¯β(Z(C0,C))|dβ(23)⩽∫01-δ∊2(1-δ)dβ=∊2for allν⩾ν(∊,δ). Following an argument similar to that in Lemma 1, we find that(24)∫1-δ1max{0,q¯β(Z(C0,C))}dβ⩽δ1/2σ(Z(C0,C))+max{0,δE[Z(C0,C)]+δ1/2σ(Z(C0,C))}.Moreover, the reasoning that lead to (18) also gives(25)|E[Z(C0,C)]|⩽M+|C0|+‖C‖mM.These facts show that there exists a positive constantM̃<∞(which depends onC0and C) such that|E[Z(C0,C)]|,σ(Z(C0,C))⩽M̃. Hence, from (24), we find that(26)∫1-δ1max{0,q¯β(Z(C0,C))}dβ⩽3M̃δ1/2.Let∊<12M∼andδ∊=(∊/(12M∼))2. Then,3M∼δ∊1/2=∊/4and(27)∫1-δ∊1max{0,q¯β(Z(C0,C))}dβ⩽∊4.An identical result holds forZν(C0,C). Consequently, for allν⩾ν(∊,δ∊),∫01max{0,q¯β(Zν(C0,C))}dβ-∫01max{0,q¯β(Z(C0,C))}dβ⩽∫01-δ∊max{0,q¯β(Zν(C0,C))}dβ-∫01-δ∊max{0,q¯β(Z(C0,C))}dβ+∫1-δ∊1max{0,q¯β(Zν(C0,C))}dβ+∫1-δ∊1max{0,q¯β(Z(C0,C))}dβ⩽∊2+∊4+∊4=∊.This fact, (19), and the assumption that(C0,C)is arbitrary, imply thatE‾α(Zν(·,·))→E‾α(Z(·,·))pointwise onRm+1. Lemma 1 and the above moment assumptions imply thatE‾α(Zν(·,·))andE‾α(Z(·,·))are finite-valued functions. They are also convex, which follows directly from the convexity ofE‾αonL2(Ω)and the affine form ofZνand Z as functions ofC0and C. Consequently, by Theorem 7.17 in Rockafellar and Wets (1998),E‾α(Zν(·,·))epiconverges toE‾α(Z(·,·)). The result then follows from Theorem 7.31 in Rockafellar and Wets (1998). □When the approximating problemPνis constructed using an independent identically distributed sample of sizeνfrom the distribution of(X,Y), we obtain the following corollary which follows from the properties of the empirical distribution.Corollary 1Suppose that the basis function h is continuous except possibly on a subsetS⊂RnwithProb(X∈S)=0and thatHi,Y∈L2(Ω),i=1,2,…,m. Moreover, let(Xν,Yν)be distributed according to the empirical distribution generated by an independent and identically distributed sample of sizeνfrom the distribution of(X,Y). Then, the conclusion ofTheorem 3holds.We next examine the rate of convergence of regression vectors obtained from the approximate problemPνto those of P corresponding to the ‘true’ distribution. It appears difficult to obtain asymptotic distribution theory for superquantile regression without additional assumptions, which among other consequences should ensure unique optimal solutions of P. We prefer another route that leads to a rate of convergence result under mild assumptions.Quantification of the stability of the set of optimal solutions of an optimization problem under perturbations depends on a ‘growth condition’ of the problem, which is difficult to quantify for P; see Rockafellar and Wets (1998, Section 7J). Consequently, we focus on the better behaved∊-regression vectors of P defined for∊>0asC‾∊≔(C0,∊,C∊)∈Rm+1|E‾α(Z(C0,∊,C∊))⩽minC0∈R,C∈RmE‾α(Z(C0,C))+∊,with an analogous definition of the∊-regression vectors ofPνdenoted byC‾∊ν. The rate with whichC‾∊νtends toC‾∊depends, naturally, on the rate with which(Xν,Yν), underlyingPν, tends to(X,Y)of P in some sense. Before we make a precise statement, we introduce a convenient notion of distances between any two nonempty setsA,B⊂Rm+1. Forρ⩾0, letdˆρ(A,B)≔inf{η⩾0|A∩ρB⊂B+ηB,B∩ρB⊂A+ηB},whereBis the Euclidean ball inRm+1with unit radius and center at the origin. Roughly,dˆρ(A,B)is the smallest amount the sets need to be ‘enlarged’ to ensure they contain the other one, with an exclusive focus on points no further from the origin thanρ. This restriction facilitates the treatment of unbounded sets.As we see next, the rate of convergence is directly related to the rate with which the random vectorΔν≔(Hν-H,Yν-Y),describing the approximation error, tends to zero.Theorem 4Rate of convergence of regression vectorSuppose that(Xν,Yν),ν=1,2,…, and(X,Y)aren+1-dimensional random vectors generatingPνand P, respectively. Moreover, letHi,Y∈L2(Ω),supνEHiν2<∞,i=1,2,…,m, andsupνE[(Yν)2]<∞. Letρ0>0be such thatρ0B∩C‾≠∅andρ0B∩C‾ν≠∅.Then, forρ>ρ0, there exist positive constantsk1,k2, andk3(dependent onρ) such that for any∊>0andν=1,2,…,dˆρC‾∊ν,C‾∊⩽1+4ρ∊E[‖Δν‖]k1max0,log1E[‖Δν‖]+k2+k3‖E[Δν]‖wheneverE[‖Δν‖]>0anddˆρ(C‾∊ν,C‾∊)=0otherwise.By Theorem 3(a) of Rockafellar and Royset (in press), forβ∈[0,1),(28)|q¯β(Zν(C0,C))-q¯β(Z(C0,C))|⩽11-βE[|Zν(C0,C)-Z(C0,C)|]=11-βE[|〈C∼,Δν〉|]⩽11-β‖C∼‖E[‖Δν‖],whereC∼=(-C,1). Then, forδ∈(0,1),(29)∫01-δmax{0,q¯β(Zν(C0,C))}dβ-∫01-δmax{0,q¯β(Z(C0,C))}dβ⩽∫01-δq¯β(Zν(C0,C))-q¯β(Z(C0,C))dβ⩽‖C∼‖E[‖Δν‖]∫01-δ11-βdβ=-‖C∼‖E[‖Δν‖]logδ.Letρ>ρ0and M be an upper bound on first and second moments of|Hi|,|Hiν|,|Y|, and|Yν|as in the proof of Theorem 3. Then, for‖(C0,C)‖⩽ρ, it follows by (25) that|E[Z(C0,C)]|⩽M+ρ+ρmMand by (15) thatσ(Z(C0,C))⩽(M+2(ρm1/2M+(M+ρ)ρmM)+ρ2mM)1/2,with identical bounds for|E[Zν(C0,C)]|andσ(Zν(C0,C)). LetMρbe the larger of the two previous right-hand sides.By (24), analogously to (26), we have that for‖(C0,C)‖⩽ρ,(30)∫1-δ1max{0,q¯β(Z(C0,C))}dβ⩽3Mρδ1/2and similarly withZ(C0,C)replaced byZν(C0,C).We also find that for‖(C0,C)‖⩽ρ,(31)|E[Zν(C0,C)]-E[Z(C0,C)]|=|〈C∼,E[Δν]〉|⩽‖C∼‖‖E[Δν]‖⩽(1+ρ)‖E[Δν]‖.Then, collecting the results of (29)–(31), we obtain that for‖(C0,C)‖⩽ρ,(32)|E‾α(Zν(C0,C))-E‾α(Z(C0,C))|⩽∫01max{0,q¯β(Zν(C0,C))}dβ-∫01max{0,q¯β(Z(C0,C))}dβ+|E[Zν(C0,C)]-E[Z(C0,C)]|⩽∫01-δmax{0,q¯β(Zν(C0,C))}dβ-∫01-δmax{0,q¯β(Z(C0,C))}dβ+∫1-δ1max{0,q¯β(Zν(C0,C))}dβ+∫1-δ1max{0,q¯β(Z(C0,C))}dβ+|E[Zν(C0,C)]-E[Z(C0,C)]|⩽-(1+ρ)E[‖Δν‖]logδ+6Mρδ1/2+(1+ρ)‖E[Δν]‖We next determine the choice ofδ∈(0,1)that minimizes the previous bound and consider two cases. First, if0<kρ(E[‖Δν‖])2<1,withkρ≔(2(1+ρ)/(6Mρ))2,then differentiation gives that the bound is minimized withδ=kρ(E[‖Δν‖])2. Second, ifkρ(E[‖Δν‖])2⩾1,thenMρ⩽4(1+ρ)E[‖Δν‖]/6and the bound-(1+ρ)E[‖Δν‖]logδ+6Mρδ1/2+(1+ρ)‖E[Δν]‖⩽-(1+ρ)E[‖Δν‖]logδ+4(1+ρ)E[‖Δν‖]δ1/2+(1+ρ)‖E[Δν]‖for anyδ∈(0,1). Consequently, combining the two cases, there exist constantsk1,k2, andk3(which depend onρ), such that for‖(C0,C)‖⩽ρ,|E‾α(Zν(C0,C))-E‾α(Z(C0,C))|⩽k1E[‖Δν‖]max0,log1E[‖Δν‖]+k2E[‖Δν‖]+k3‖E[Δν]‖⩽E[‖Δν‖]k1max0,log1E[‖Δν‖]+k2+k3‖E[Δν]‖Direct application of Example 7.62 and Theorem 7.69 of Rockafellar and Wets (1998) then yields the conclusion forE[‖Δν‖]>0, where the additional coefficient(1+4ρ/∊)originates in that theorem. Finally, ifE[‖Δν‖]=0, then, in view of (28) and the fact that this implies that‖E[Δν]‖=0, we find that for‖(C0,C)‖⩽ρ,|E‾α(Zν(C0,C))-E‾α(Z(C0,C))|=0The final conclusion then follows by again invoking Example 7.62 and Theorem 7.69 of Rockafellar and Wets (1998). □Theorem 4 shows that the distance betweenC‾∊νandC‾∊is almost proportional toE[‖Δν‖], but with a minor correction by a logarithmic term. If the approximation(Xν,Yν)is caused by measurement errors of magnitude1/ν, i.e., the absolute value of each component of(Xν-X,Yν-Y)is no greater than1/νalmost surely, thenE[‖Δν‖]⩽m+1/νand the expressions can be simplified. Forξ>0,logx⩽xξfor sufficiently largex∈R. Consequently, for anyξ∈(0,1)and sufficiently largeν,dˆρ(C‾∊ν,C‾∊)⩽1+4ρ∊kν1-ξ,wherek>0can be determined fromk1,k2,k3, and m. That is, the Euclidean distance between an∊-regression vector ofPνto one of P isO(νξ-1)forξ∈(0,1)arbitrarily close to zero.We next turn to the situation where we seek to estimateq¯α(Y(x))forx∈Rn, or a subset thereof, with the goal of eventually minimizing, at least approximately,q¯α(Y(x))by a judicious choice of x. Of course, with incomplete knowledge about the distributions ofY(x)this is a difficult task that can be achieved only approximately. For example, there is no guarantee that a regression functionf=C‾0+〈C‾,h(·)〉, with(C‾0,C‾)∈C‾obtained by solving P usingα∈(0,1), tracksq¯α(Y(x)), i.e.,f(x)=q¯α(Y(x))for allx∈Rn. The hope of such ‘exact’ tracking becomes even less realistic when P must be replaced by an approximationPνas typically required in practice. However, ‘local’ tracking is possible, at least approximately, with an appropriate weighing of the data available as we discuss next.We consider the situation where there is a sample ofY(x)for a set of x, but the sample is not large enough to allow pointwise estimation ofq¯α(Y(x))for every x of interest. There may even be no x for which there are multiple samples ofY(x). Concentrating on a particularxˆ∈Rn, we hope to estimateq¯α(Y(xˆ))by using samples fromY(x)for x nearxˆ, weighted appropriately. The weights should be nonnegative, sum to one, and can be thought of as an artificially constructed probability distribution associated with the sample. Specifically, suppose thatxi,i=1,…,ν, are the points where the sample is observed andyi,i=1,…,ν, are the corresponding realizations ofY(xi). When estimating a superquantile atxˆ, we put more ‘trust’ on sample points taken nearxˆand consequently the weight of(xi,yi)may be inversely proportional to‖xi-xˆ‖, with an appropriate adjustment ifxˆcoincides with anxi.A justification for the approach follows directly from Theorem 3 through the next proposition.Proposition 5Suppose that the assumptions ofTheorem 3hold and that the probability distribution of(X,Y)is degenerate atxˆ∈Rn+1in the sense thatProb((X,Y)⩽(x,y))=φ(y), for ally∈Randx⩾xˆ, whereφ(y)=Prob(Y(xˆ)⩽y), andProb((X,Y)⩽(x,y))=0otherwise. IfC‾0ν,C‾νν=1∞is a sequence of optimal solutions ofPν, withα∈(0,1), then along every convergent subsequence we have thatC‾0ν+〈C‾ν,h(xˆ)〉tends toq¯α(Y(xˆ)).For the given degenerate distribution of(X,Y),C0+〈C,h(X)〉=C0+〈C,h(xˆ)〉almost surely. Consequently, P reduces to the error minimization problem of Theorem 1 andC‾0+〈C‾,h(xˆ)〉=q¯α(Y(xˆ))for every(C‾0,C‾)∈C‾. The conclusion then follows from Theorem 3. □Suppose that the weights of(xi,yi),i=1,2,…,ν, in the above construction are chosen to approximate the degenerate distribution of Proposition 5, for example by setting them inversely proportional to‖xi-xˆ‖. Then, in view of Proposition 5, a solution ofPν, constructed using those weights as an artificial probability distribution for(Xν,Yν), leads to an approximation of the considered superquantile atxˆ. Of course, this procedure can be repeated for different pointsxˆto generate a ‘global’ assessment ofq¯α(Y(x))as a function of x and eventually facilitate optimization over x. Moreover, the process can be repeated with new or augmented sample points in a straightforward manner. In a situation where a sample is not fully randomly generated but x-points are determined by an analyst, the approach may even motivate scattering those points near a point of interestxˆinstead of concentrating them all atxˆexactly. The former approach certainly results in a better ‘global’ understanding of a superquantile as a function of x, but may prove to be a more economical route to estimate a superquantile atxˆtoo. We examine this situation numerically in Section 6.Regression modeling must be associated with means of assessing the goodness-of-fit of a computed regression vector. In least-squares regression, the coefficient of determinationR2=1-SSResSST,whereSSResdenotes the residual sum of squares andSSTthe total sum of squares, provides a means for such an assessment. WhileR2cannot be relied on exclusively, it provides an indication of the goodness of fit that is easily extended to the present context of superquantile regression. In our notation,(33)R2=1-E[Z(C0,C)2]σ2(Y),and similarly when passing to an approximate random vector(Xν,Yν). From Example 1’ in Rockafellar and Uryasev (2013), we know that the numerator in (33) is an error measure applied toZ(C0,C)and that it corresponds to the deviation measureσ2(·). Moreover, the minimization of that error ofZ(C0,C)results in the least-squares regression vector. According to Rockafellar and Uryasev (2013), these error and deviation measures are in correspondence and belong to a ‘risk quadrangle’ that yields the expectation as its statistic. This observation motivates the following definition of a coefficient of determination for superquantile regression model.Definition 1In superquantile regression, the coefficient of determination of a regression vector(C0,C)∈Rm+1is given by(34)R‾α2(C0,C)≔1-E‾α(Z(C0,C))D‾α(Y).In fact, a similar definition can be formulated for any generalized regression consisting of minimizing an error ofZf, with then another measure of error in the numerator and a corresponding deviation measure, in the sense of Rockafellar and Uryasev (2013), in the denominator. As in the classical case, higher values ofR‾α2are better, at least in some sense. However,R‾α2⩽1, which is apparent from the nonnegativity of the error and deviation measures. Indeed, P aims to minimize the error ofZ(C0,C)by wisely selecting the regression vector(C0,C)and thereby also maximizesR‾α2. The error is ‘normalized’ with the overall ‘nonconstancy’ in Y as measured by its deviation measure to more easily allow for comparison of coefficients of determination across data sets.It is possible to obtain large coefficients of determination by adding explanatory terms to a regression model, i.e., increasing m, but without necessarily achieving a more useful model. Hence, it is usual in least-squares regression to also evaluate an adjusted coefficient of determination that penalizes any term added to the model that does not reduce variability substantially. This quantity only increases if a new term reducesSSRes/(ν-m)as seen by the definition(35)RAdj2=1-SSRes/(ν-m)SST/(ν-1),whereνis the number of observations. Naturally, then, we define an adjusted coefficient of determination for superquantile regression similarly in the case where the distribution of(X,Y)has a finite support of cardinalityν.Definition 2In superquantile regression, the adjusted coefficient of determination of a regression vector(C0,C)∈Rm+1is given by(36)R‾α,Adj2(C0,C)≔1-E‾α(Z(C0,C))/(ν-m)D‾α(Y)/(ν-1).Again, similar expressions are available for other generalized regression techniques.The computational task of carrying out superquantile regression consists of solving the convex optimization problem P, or in practice the approximate problemPνdue to incomplete distributional information and other sources of approximations. In this section, we describe convenient means for solvingPνwhen(Xν,Yν)has a discrete joint distribution withνpossible realizations. Regardless of the distribution of(Xν,Yν), a reformulation ofPνin terms of the deviation measureD‾αis beneficial. In view of Proposition 3, the task of determining a regression vector(C‾0ν,C‾ν)reduces to that of minimizingD‾αZ0ν(·), settingC‾νequal to an optimal solution, and then settingC‾0ν=q¯αZ0ν(C‾ν). Since it is straightforward to compute every superquantile of a random variable with a discrete probability distribution, we focus on the minimization problem, which takes the following form after writing out the expression for the deviation measure in this caseDν:minC∈Rm11-α∫α1q¯βZ0ν(C)dβ-EZ0ν(C).The next subsections describe two computational methods for solvingDνwhen the distribution of(Xν,Yν)is discrete.While one might at first get the impression that numerical integration is required in solvingDν, this may not actually be needed as shown next. Suppose that(Xν,Yν)has a discrete distribution with support(xj,yj),j=1,2,…,ν, andProb((Xν,Yν)=(xj,yj))=1/νforj=1,2,…,ν. This is the case typically encountered in applications, where(xj,yj),j=1,2,…,ν, is the data assumed to be equally likely to occur. We then obtain significant simplifications inDν.For any fixedC∈Rm, the cumulative distribution function ofZ0ν(C)is a piecewise constant function with at mostνsteps. The range of the distribution function is{0,1/ν,2/ν,…,1}or a subset thereof. By partitioning the integral overβinDνaccording to this range, accounting for the fact that the integral starts atα, the problem can in this case be written as(37)minC∈Rm11-α∑i=ναν∫βi-1βiq¯βZ0ν(C)dβ-EZ0ν(C),whereνα≔⌈να⌉, with⌈a⌉being the smallest integer no smaller thana∈R,βνα-1=α, andβi=i/ν, fori=να,να+1,…,ν. In view of (4) and (5),(38)q¯β(Z0ν(C))=minUβ∈RUβ+11-βE[max{Z0ν(C)-Uβ,0}]=qβ(Z0ν(C))+11-βEmaxZ0ν(C)-qβ(Z0ν(C)),0for eachβ∈[0,1). However, the special piecewise-constant structure of the cumulative distribution function ofZ0ν(C)implies thatqβZ0ν(C)is constant as a function ofβon(βi-1,βi)for everyi=να,να+1,…,ν. Consequently,Uβ,β∈(α,1)in (38) can be replaced by a finite number of variables so that (37) takes the formminC∈Rm11-α∑i=ναν∫βi-1βiminUi∈RUi+11-βEmaxZ0ν(C)-Ui,0dβ-EZ0ν(C).The last integral simplifies further since forβ∈(βν-1,βν)=(1-1/ν,1),q¯βZ0ν(C)=M(C)≔maxj=1,2,…,νyj-〈C,xj〉.Consequently, (37) takes the formminC∈Rm11-α∑i=ναν-1∫βi-1βiminUi∈RUi+11-βEmaxZ0ν(C)-Ui,0dβ+M(C)ν(1-α)-EZ0ν(C).The order of minimization is immaterial and we can equivalently considerminC∈Rm,U∈Rν-να11-α∑i=ναν-1∫βi-1βiUi+11-βEmaxZ0ν(C)-Ui,0dβ+M(C)ν(1-α)-E[Z0ν(C)],where we letU=(Uνα,Uνα+1,…,Uν-1). Fori=να,να+1,…,ν-1, we defineai≔∫βi-1βi11-βdβ=log(1-βi-1)-log(1-βi).Using this notation, (37) simplifies further tominC∈Rm,U∈Rν-να11-α∑i=ναν-1(βi-βi-1)Ui+11-α∑i=ναν-1EmaxZ0ν(C)-Ui,0ai+M(C)ν(1-α)-EZ0ν(C).By introducing another set of auxiliary variables and using the standard transcription technique for handling max-functions, we reach the linear programDLPν:minC,U,V,W11-α∑i=ναν-1(βi-βi-1)Ui+1ν(1-α)∑i=ναν-1∑j=1νaiVij+1ν(1-α)W-1ν∑j=1ν(yj-〈C,h(xj)〉)s.t.yj-〈C,h(xj)〉-Ui⩽Vij,i=να,…,ν-1,j=1,…,ν0⩽Vij,i=να,…,ν-1,j=1,…,νyj-〈C,h(xj)〉⩽W,j=1,…,νC∈RmU=(Uνα,…,Uν-1)∈Rν-ναV=(Vνα,1,…,Vν-1,ν)∈R(ν-να)νW∈R.This equivalent reformulation ofDνinvolvesm+(ν-να)(ν+1)+1variables and2(ν-να)ν+νinequality constraints. Whileνα=⌈να⌉may be relatively close toνin practice, the linear program could become large-scaled whenνis large and decomposition algorithms may be needed.Alternatively, we consider next a numerical integration-based scheme that avoids some auxiliary variables and constraints, and also handles the situation when the distribution of(Xν,Yν)is not uniformly discrete.The integral inDνis easily approximated by standard numerical integration schemes. Suppose that the interval[α,1]is divided intoμsubintervals, whereα⩽β0<β1<⋯<βμ-1<βμ⩽1andwi⩾0,i=0,1,…,μ, are factors specific to the integration scheme. An approximation ofDνthen takes the formDν,μ:minC∈Rm11-α∑i=0μwiq¯βi(Z0ν(C))-EZ0ν(C).For largeμ, an optimal solution ofDν,μis close to that ofDν, as seen next, under conditions that are satisfied by essentially all commonly used numerical integration schemes.Proposition 6Suppose that for any continuous functiong:[α,1]→R, a numerical integration scheme with discretization pointsα⩽β0<β1<⋯<βμ-1<βμ⩽1and factorswi⩾0,i=0,1,…,μ, satisfies∑i=0μwig(βi)-∫α1g(β)dβ→0asμ→∞. Let{C‾ν,μ}μ=1∞be a sequence of optimal solutions ofDν,μunder this numerical integration scheme. Then, every accumulation point of{C‾ν,μ}μ=1∞is an optimal solution ofDν.For anyC∈Rm,q¯β(Z0ν(C))is finite and continuous as a function ofβ. Consequently, the assumption on the numerical integration scheme applies and the objective function ofDν,μconverges pointwise to that ofDν, asμ→∞. The objective functions are also finite and convex in C, which follows directly from the convexity ofq¯αonL2(Ω)and the affine form ofZ0νas a function of C. Consequently, by Theorem 7.17 in Rockafellar and Wets (1998), the objective function ofDν,μepiconverges to that ofDνand the conclusion follows from Theorem 7.31 in Rockafellar and Wets (1998). □While specialized solvers such as Portfolio Safeguard (American Optimal Decisions, Inc., 2011) handleDν,μdirectly with little difficulty under many circumstances, the problem is typically nonsmooth and standard nonlinear programming solvers may fail. However, following a simple reformulation ofDν,μ, utilizing (5), yields the following equivalent linear program, where we assume for convenience thatβμ<1:minC,U,V11-α∑i=0μwiUi+11-βi∑j=1νpjVij-∑j=1νpj(yj-〈C,h(xj)〉)s.t.yj-〈C,h(xj)〉-Ui⩽Vij,i=0,1,…,μ,j=1,…,ν0⩽Vij,i=0,1,…,μ,j=1,…,νC∈Rm,U=(U0,U1,…,Uμ)∈Rμ+1,V=(V0,1,…,Vμ,ν)∈R(μ+1)ν.Ifβμ=1, then a straightforward modification is required based on the fact thatq¯1(Z0ν(C))=maxj=1,2,…,νyj-〈C,xj〉. The linear program consists ofm+μ+1+ν(μ+1)variables and2ν(μ+1)constraints, which may be substantially less than what follows from the analytical integration approach for largeν. In practice, we find that a moderately largeμsuffices as shown next.In this section, we illustrate superquantile regression in three numerical examples. The first example is artificially constructed, with known conditional superquantiles. The second example is an instance from the uncertainty quantification literature. The third example arises in investment analysis. Computations are mostly carried out in Matlab version 7.14 on a 2.26gigahertz laptop with 8.0gigabytes of RAM using Portfolio Safeguard (American Optimal Decisions, Inc., 2011) with VAN as the optimization solver forDν,μ. When solvingDLPνwe employ GAMS version 23.7 with the CPLEX 12.3 solver on a 4.0gigabytes, 2.50gigahertz laptop.We start by considering a loss random variableY=X1+X2∊,almostsurely,where∊is a standard normal random variable andX=(X1,X2)is uniformly distributed on[-1,1]×[0,1], with∊,X1, andX2independent. We consider a regression function of the formf(x)=C0+C1x1+C2x2and setα=0.90.We first examine the computational effort required to obtain an approximate regression vector. Table 1shows computing times for solvingDLPνfor increasingly larger sample sizesνobtained by independent draws from(∊,X1,X2). While the results correspond to single instances ofDLPν, the times vary little between two samples of the same size and the computing times are therefore representative. As expected from the discussion at the end of Section 5.1, the computing time grows quickly as the sample sizeνincreases. In addition to the inconvenience of long computing times, memory requirements become problematic.DLPνhas a special structure and we anticipate significant reduction in computing times and memory needs resulting from tailored algorithms. However, the development of such algorithms is beyond the scope of the paper.Second, we consider the alternative approach based on solvingDν,μ. While this approach introduces a numerical integration error, Proposition 6 indicates that the error is negligible for largeμ. In fact, as we see next empirically, moderately largeμsuffices. Moreover, the substantial reduction in problem size, as compared to that ofDLPν, reduces computing times dramatically.Sinceq¯β(Z0ν(C))may be nonsmooth as a function ofβ, standard numerical integration error bounds may not apply. However, sinceq¯β(Z0ν(C))is continuous and nondecreasing as a function ofβ, the use of left-endpoint and right-endpoint numerical integration rules inDν,μprovide lower and upper bounds on the optimal value ofDν, respectively. Table 2shows solution vectors(C0,C1,C2)forμ=100,μ=1000, left-endpoint, right-endpoint, and Simpson’s numerical integration rules, and sample sizes ofν=100and ν=10,000. Each solution ofDν,μis obtained quickly, in about 0.5 and 5 seconds forν=100and ν=10,000, respectively; see the last column of Table 2. We also show the corresponding coefficient of determinationR‾α2for each instance. Forν=100, the solutions andR‾α2are insensitive to the numerical integration rule as well asμ. The obtained solutions are essentially identical to the regression vector obtained fromDLPν; see Row 8 of Table 2. For μ=10,000, we note some differences but magnitudes are small. In this case, we are unable to solveDLPνdue to its size. We observe that as indicated by the coefficients of determination, the linear modelf(x)=C0+C1x1+C2x2does not fully capture the variability of the data and a study of other models may be warranted. However, we omit such an investigation and instead turn to superquantile tracking.Third, we examine conditional values of Y given realizations ofX=(X1,X2), i.e., superquantile tracking. Forx=(x1,x2),Y(x)=Y|X=xis normally distributed with meanx1and variancex22. Consequently, it is straightforward to compute thatq¯0.9(Y(x))=x1+1.7550x2. Table 2 shows vectors that only trackq¯0.9(Y(·))approximately, asC0,C1, andC2deviate from 0, 1, and1.755, respectively. In fact, there is in general no guarantee that every regression function f will satisfyf(x)=q¯α(Y(x))for all x, even for large sample sizes. As indicated by Proposition 5, however, a superquantile ofY(x)can be estimated by approximating a degenerate distribution of(X,Y)at x. Table 3shows such ‘local’ estimates ofq¯0.9(Y(x))nearx=(0.5,0.5). Specifically, usingν=500we computeC0,C1, andC2by solvingDLPνas above, with X sampled uniformly from[-1,1]×[0,1]. We repeat these calculations 10 times with independent samples and obtain the aggregated statistics of Column 2 of Table 3. The second row gives an approximate 95% confidence interval for the mean value ofC0+0.5C1+0.5C2across the 10 meta-replications. The interval containsq¯0.9(Y((0.5,0.5)))=1.3775, but is somewhat wide. Proposition 5 indicates that sampling from a smaller set[0.45,0.55]×[0.45,0.55]will tend to improve the estimate ofq¯0.9(Y((0.5,0.5))). Column 3 of Table 3 illustrates this effect, by showing results comparable to those of Column 2 and Row 2, but for the smaller interval. As expected, the confidence interval forC0+0.5C1+0.5C2narrows around the correct value. The last column shows similar results, but now for sampling of X uniformly on[0.495,0.505]×[0.495,0.505]. The estimate ofq¯0.9(Y((0.5,0.5)))improves only marginally, with the residual uncertainty being due to the inherent variability in the (relatively small) samples. The narrow sampling interval causes the last estimate to be similar to that obtained by the standard empirical estimate from 500 realization ofY((0.5,0.5)), which yields the confidence interval(1.312,1.462).While sampling on smaller sets gives better local estimates ofq¯0.9(Y(x)), the global picture deteriorates. The last three rows of Table 3 show corresponding approximate 95% confidence intervals forC0,C1, andC2, respectively. WhileC0+C1x1+C2x2generated by the set[-1,1]×[0,1]provides a reasonably good global picture ofq¯0.9(Y(x)), the smaller sets lose that quality as seen from the wide confidence intervals. In view of the above results, we see that an analyst that can choose “design points,” i.e., points x at which to sampleY(x), should balance the need for accurate local estimates with that of global estimates. In fact, even if the primary focus is on estimatingq¯α(Y(x))for a given x, as we see in this example, it may be equally effective to spread the samples of X near x instead of exactly at x, and then obtain some global information aboutq¯α(Y(·))too. Our methodology provides a flexible framework for estimatingq¯α(Y(x))even if there is only a small number of realization ofY(x), or even none, available. The estimates are based on realization ofY(x′)forx′near x. None of the numerical examples in this paper include data with more than one realization ofY(x)for any x.The next example arises in uncertainty quantification of a rectangular cross section of a short structural column, with depth d and width w, under uncertain yield stress and uncertain loads, see (Eldred et al., 2011). Assuming an elastic-perfectly plastic material, a limit-state function that quantifies a relationship between loads and capacity is described by the random variable(39)Y=-1+4X1wd2X3+X22w2d2X32,almostsurely,where the bending moment loadX1and the axial loadX2are normally distributed with mean 2000 and standard deviation 400, and mean 500 and standard deviation 100, respectively, and the material’s yield stressX3, is lognormally distributed with parameters 5 and 0.5, withX1,X2, andX3independent. We observe that the second term in (39) is the ratio of moment load to the column’s moment capacity, and the third term is the square of the ratio of the axial load to the axial capacity. The constant-1is introduced for the sake of a translation such that positive realizations of Y represent ‘failure’ and nonnegative ones correspond to a situation where load effects remain within the capacity of the column. (We note that the orientation of the limit-state function is switched compared to that of Eldred et al. (2011) for consistency with our focus on ‘losses’ instead of ‘gains.’) We set the widthw=3, and the depthd=12.We seek to quantify the ‘uncertainty’ in Y by surrogate estimation. Of course, in this case, this is hardly necessary; direct use of (39) suffices. However, in practice, an analytic expression for a limit-state function, as in (39), is rarely available. One then proceeds with determining a regression functionf:R3→R, based on a sample of input-output realizations, such thatf(X), withX=(X1,X2,X3), approximates Y in some sense. To mimic this situation, we consider a sample of size 50,000 drawn independently from X, the corresponding realizations of Y according to (39), and two forms of the regression function. The first model is linear and takes the formf1(x)=C0+C1x1+C2x2+C3x3and the second one utilizes basis functionsh1(x)=x1/x3andh2(x)=(x2/x3)2and is of the formf2(x)=C0+C1x1/x3+C2x22/x32.In view of (39), we expectf1to be unable to capture interaction effects between variables and its explanatory power may be limited. In contrast,f2uses the correct basis functions, but even thenf2(X)may deviate from Y due to the finite sample size used to determine the regression vector. Table 4confirms this intuition by showing approximate regression vectors for both models over a range of probability levelsαas well as for the least-squares (LS) regression. The vectors are obtained in less than 15 seconds by solvingDν,μ, with ν=50,000, μ=1000, and Simpson’s rule. The last column of Table 4 showsR‾α2(classical coefficient of determination according to (33) in the case of least-squares regression), which is low forf1and high forf2as expected.In uncertainty quantification and elsewhere, surrogate estimates such asf1(X)andf2(X)are important input to further analysis and simulation. Table 5illustrates the quality of these surrogate estimates in this regard by showing various statistics off1(X)andf2(X)as compared to those of Y. Row 2, Columns 3–10 show estimated mean, standard deviation, superquantiles at 0.75, 0.9, 0.99, 0.999, probability of failure, and buffered probability of failure (see (2)) of Y, respectively, using a sample size of107and standard estimators. Coefficients of variation for these estimators are ranging, approximately, from10-5for the mean to0.02for the probability of failure. Rows 3–6 of Table 5 show similar results, using the same sample, forf1(X), withα=0.999,0.99,0.9, and0.75, respectively. We notice that asαincreases,f1(X)becomes increasingly conservative. In fact, forα=0.999,f1(X)is conservative in all statistics. Superquantile regression with smallerαfails to be conservative for some ‘upper-tail’ statistics. Interestingly,f1(X)based onαis conservative for all superquantiles up to and includingq¯αin these tests. These observations indicate that in surrogate estimation the probability levelαshould be selected in accordance with the superquantile statistic of interest. We can then expect to obtain conserve estimates even for relatively poor surrogates. Row 7 of Table 5 gives corresponding results forf1(X)under the least-squares regression fit. While this fit provides an accurate estimate of the mean (see Column 3), the upper-tail behavior is represented in a nonconservative manner.Rows 8–12 of Table 5 show comparable results to those above, but for thef2(X)models. As also indicated in Table 4,f2(X)is a much better surrogate of Y thanf1(X)and essentially all quantities improve in accuracy. For example,f2(X)based on superquantile regression overestimates the buffered failure probability only moderately withα=0.999,0.99, and0.9, and slightly underestimate withα=0.75; see the last column of Table 5. In contrast, least-squares regression underestimates the buffered failure probability substantially even for this supposedly ‘accurate’ model. Of course, least-squares regression centers on conditional expectations and as basis for estimating tail behavior may hide potentially dangerous risks.The last example is a case study taken from the “Style Classification with Quantile Regression” documentation in Portfolio Safeguard (American Optimal Decisions, Inc., 2011) and deals with the negative return of the Fidelity Magellan Fund as predicted by the explanatory variables Russell 1000 Growth Index (X1, RLG), Russell 1000 Value Index (X2, RLV), Russell Value Index (X3, RUJ), and Russell 2000 Growth Index (X4, RUO). (We change the orientation from ‘return’ to ‘negative return’ to be consistent with the orientation of a loss random variable in the present paper.) The indices classify the style of the fund; see (American Optimal Decisions, Inc., 2011) for details. There areν=1264total observations available.We start by considering a linear modelf1(x)=C0+C1x1+C2x2+C3x3+C4x4and compare the obtained approximate regression vectors for least-squares, quantile, and superquantile regression underα=0.75and 0.90, as shown in Table 6.Dνis solved throughDν,μwith Simpson’s rule andμ=1000, while quantile regression is carried out directly in Portfolio Safeguard’s Shell Environment (American Optimal Decisions, Inc., 2011). Table 6 also shows the coefficients of determination, where for least-squares regression we use (33). The fits are good and a majority of the variability in the data is captured. However, the small values ofC4and also the corresponding p-value from the least-squares regression point to the possible merit of droppingX4(RUO) as explanatory variable. We from now on focus on superquantile regression. A new modelf2(x)=C0+C1x1+C2x2+C3x3yields the approximate regression vectors of Table 7, which also shows the obtained adjusted coefficients of determinationR‾α,Adj2. The switch fromR‾α2toR‾α,Adj2enable us to better compare fits across models with different number of explanatory variables. In comparison, adjusted coefficients of determination forf1, withα=0.75and 0.90, are 0.8732 and 0.8719, respectively. Consequently, the fit improves slightly by droppingX4(RUO).We further reduce the model to a single explanatory variable and examine the four possibilities in Table 8. We find thatR‾α,Adj2deteriorates, but only moderately for the modelC0+C1X1. This simple model captures much of the variability in the data set. A somewhat poorer fit is achieved byX2(RLV), which is illustrated in Fig. 1forα=0.90. That figure also depicts the corresponding quantile and least-squares regression lines. It is apparent that superquantile regression provides a distinct perspective from the other regression techniques of potential significant value to a decision maker.

@&#CONCLUSIONS@&#
We present a superquantile regression methodology centered on the minimization of a measure of error analogous to classical least-squares and quantile regression. We establish the existence of a regression function, discuss its possible uniqueness, and its stability under perturbation, for example caused by sample approximations of a true distribution. A new coefficient of determination allows us to quantify the goodness of fit. We show that superquantile regression requires the solution of a linear program, as in the case of quantile regression, or alternatively of an optimization problem with superquantile (conditional value-at-risk) constraints. Our computational tests demonstrate that superquantile regression is computationally tractable, provides new insight about tail-behavior for quantities of interest, and offers a complementary tool for the risk-averse decision maker.