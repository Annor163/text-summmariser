@&#MAIN-TITLE@&#
A flexible approach to distributed data anonymization

@&#HIGHLIGHTS@&#
We propose a flexible approach for de-identifying distributed biomedical datal.Horizontal and vertical data distribution are handled in a consistent manner.Our method supports a broad spectrum of anonymization methods and privacy criteria.Supported algorithms include optimal methods, heuristics and clustering algorithms.Applicable criteria include k-anonymity, l-diversity, t-closeness and d-presence.

@&#KEYPHRASES@&#
Personal data protection,Distribution,Privacy,Anonymization,Commutative encryption,Secure multi-party computing,SMC,

@&#ABSTRACT@&#
Sensitive biomedical data is often collected from distributed sources, involving different information systems and different organizational units. Local autonomy and legal reasons lead to the need of privacy preserving integration concepts. In this article, we focus on anonymization, which plays an important role for the re-use of clinical data and for the sharing of research data. We present a flexible solution for anonymizing distributed data in the semi-honest model. Prior to the anonymization procedure, an encrypted global view of the dataset is constructed by means of a secure multi-party computing (SMC) protocol. This global representation can then be anonymized. Our approach is not limited to specific anonymization algorithms but provides pre- and postprocessing for a broad spectrum of algorithms and many privacy criteria. We present an extensive analytical and experimental evaluation and discuss which types of methods and criteria are supported. Our prototype demonstrates the approach by implementing k-anonymity,ℓ-diversity, t-closeness andδ-presence with a globally optimal de-identification method in horizontally and vertically distributed setups. The experiments show that our method provides highly competitive performance and offers a practical and flexible solution for anonymizing distributed biomedical datasets.

@&#INTRODUCTION@&#
Collaboration and data sharing have become core elements of biomedical research. Examples are international projects like the International Cancer Genome Consortium ICGC with its goal to “make the data available to the entire research community” [1], and BBMRI-LPC aiming “to help scientists to have better access to large European studies on health” [2]. Also, from the perspective of public funders, sharing of research data has become a request, and principles of sharing have been formulated [3,4]. Besides the international projects mentioned above, there are research projects on national, regional, and institutional levels, which collect, integrate, and share data.The process of managing data from collection to analyses and also to sharing can be illustrated by different phases [5]. Research data is collected and managed, which may be accompanied by further processes, such as quality assurance. Sharing is initiated by allowing other researchers to get an overview over available data which fit their research objectives. Typically, access to core data is limited, and data access committees are involved before data use agreements (DUAs) are signed and data is released. Then, this data is integrated and used for new analyses.There is a growing understanding of risks related to data sharing: disclosure of sensitive biomedical data may lead to harm for individuals, especially when different sources are available for linkage (for an overview see [6]). Basically, national laws and regulations, such as the HIPAA Privacy Rule [7], as well as international regulations, such as the European Directive on Data Protection [8], mandate stringent protection of personal data. In recent years, there has been extensive work on ethical, legal, social/societal issues (ELSI) of biomedical and genomic research and on data sharing, e.g., [9,10], which we will not further address in this article.Anonymization is an important privacy measure when releasing and sharing sensitive datasets. As an important example, the HIPAA Privacy Rule has defined concrete measures to prevent re-identification. These include methods of statistical disclosure control. Basically, fuzziness is introduced to a degree which balances remaining semantics and usability against risk reduction. K-anonymity is a well known and understood privacy criterion, focusing on quasi-identifiers. These are attributes that are required for analyses but are associated with a high risk of reidentification. A dataset is k-anonymous if each data item cannot be distinguished from at leastk-1other data items regarding the quasi-identifiers [11]. Introducing k-anonymity is a measure against linkage attacks which may lead to identity disclosure when accessible data is combined with an attackers background knowledge [12].Data is often collected from distributed sources, involving different types of data, different information systems, and different organizational units. Pseudonymity is another privacy measure of relevance, which leads to distribution. Here, directly identifying data is separated from medical data, and the links between identifiers and corresponding pseudonyms are secretly kept by a honest broker [13]. In general, data can be distributed vertically or horizontally. The former means that different sites hold different subsets of the attributes for a common set of individuals, so pseudonymity is a typical example. The latter means that different sites hold data with the same set of attributes for different individuals, for example, data for individuals in their region. Health services research is an example where integration of horizontally distributed data is needed, and disclosure has to be prevented.In this article, we will focus on anonymization of datasets which are horizontally or vertically distributed. Existing approaches have focused on limited sets of privacy criteria, which in practice must often be combined with further criteria to prevent unintended disclosure of sensitive data. In most cases, specific algorithms were implemented which employ specific types of data transformations and search strategies. In contrast, we see are requirement for flexible solutions which allow the implementation of a broad spectrum of methods. Here, we agree with [14,15], that the suitability of methods depends on use cases. As efficient generic solutions do not exist, and as many approaches have unclear performance characteristics, we will also address performance questions. They are of relevance in situations which require near real-time updates, e.g. when the course of an infectious disease is analyzed over different areas.We will present a flexible and efficient approach to distributed data anonymization in the semi-honest model. It is based upon a secure multi-party computing (SMC) protocol, which constructs an encrypted global view out of horizontally or vertically distributed datasets. To this global view a broad spectrum of anonymization algorithms and privacy criteria can be applied. Thus, centralized versions of a large number of data anonymization algorithms are supported, and we will provide a detailed overview in the discussion. We will show the flexibility of our solution by anonymizing data with a broad spectrum of privacy criteria, including k-anonymity,ℓ-diversity, t-closeness andδ-presence, using a globally optimal data anonymization algorithm. Most related approaches in the distributed setting implement heuristic methods, as their coding models result in large search spaces. While it has been shown that these heuristics combined with, e.g., local recoding, can outperform optimal algorithms using single-dimensional global recoding in terms of data quality, we chose such an algorithm as these have said to be very well suited for the biomedical domain [14].We present an extensive analytical and experimental evaluation of our solution and show that it offers highly competitive execution times. The performance of our approach can be accurately estimated with a model that only depends on basic data characteristics. Our protocol relaxes the guarantees of traditional secure multiparty computations by exchanging non-anonymized – but encrypted – subsets of the data. We present effective means to lower privacy risks and discuss a trade-off between privacy, data quality and efficiency. Together with estimates derived from our model, this can be utilized to tailor our method to project specific requirements.

@&#CONCLUSIONS@&#
We have presented a secure multi-party computing protocol that enables a novel and flexible anonymization method for distributed data. Prior to the data anonymization procedure an encrypted global view of the dataset is constructed, which is then anonymized. Our approach is the first to support a broad spectrum of privacy criteria and anonymization algorithms. This includes variants of common criteria for protecting datasets from membership, attribute and identity disclosure. The supported methods include heuristics, including clustering algorithms, and optimal methods. In our examples we used a globally optimal algorithm with k-anonymity,ℓ-diversity, t-closeness andδ-presence. We have motivated our approach by examples from the biomedical domain.We have added an extensive experimental evaluation of our method and developed an analytical model that can be used to accurately estimate the overhead caused in terms of computational costs and transferred data volumes. Our experiments and comparisons have shown that it offers highly competitive performance and thus provides a practical solution for anonymizing distributed biomedical datasets. Our prototype of a globally optimal algorithm is the first efficient implementation of a such a method in a distributed environment. According to El Emam et al. this class of algorithms is very suitable for the biomedical domain [14]. Reasons include that it results in datasets, which are well suited for biomedical analyses and provides reproducible and understandable results that can be adjusted by non-experts (e.g., by changing generalization hierarchies or choosing another transformation from the search space).In future work, there are multiple ways to extend our concept and our implementation. For example, first experiments have shown that employing additional data compression methods can reduce data volumes by a factor of up to 2.5. In case of vertical distribution, commutative encryption of the data items could be replaced with symmetric encryption (e.g., AES). This would enable additional speedups, especially for datasets with many distinct values. We measured a speedup of roughly 40% for the CUP dataset, but only 2% for the IHIS dataset.Further performance gains can be achieved for datasets with many insensitive attributes. In our current implementation we handle these analogously to quasi-identifying and sensitive attributes, i.e., we apply commutative encryption. An alternative is to use a much more efficient symmetric cipher and only encrypt the keys for this cipher (one key per party) commutatively. This allows the last party to decrypt all insensitive attributes in the anonymized representation.Our concept can easily be extended to cover hybridly distributed data. In this case the tuple identifiers need to be preserved during the integration phase, as they are needed to correlate the individual subsets. The remainder of the protocol can be executed analogously to the horizontal setup.