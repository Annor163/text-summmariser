@&#MAIN-TITLE@&#
A novel ensemble of classifiers that use biological relevant gene sets for microarray classification

@&#HIGHLIGHTS@&#
Ensemble of classifiers integrating prior biological knowledge.Relevant gene sets as an informed feature selection technique.Biological plausible interpretation of ensemble architecture.Publicly available datasets covering biclass and multiclass scenarios.Model comparison with classical approaches and standard ensemble alternatives.

@&#KEYPHRASES@&#
Microarray classification,Biological knowledge,Ensemble learning,Knowledge integration,Classifier fusion,

@&#ABSTRACT@&#
Since the introduction of DNA microarray technology, there has been an increasing interest on clinical application for cancer diagnosis. However, in order to effectively translate the advances in the field of microarray-based classification into the clinic area, there are still some problems related with both model performance and biological interpretability of the results. In this paper, a novel ensemble model is proposed able to integrate prior knowledge in the form of gene sets into the whole microarray classification process. Each gene set is used as an informed feature selection subset to train several base classifiers in order to estimate their accuracy. This information is later used for selecting those classifiers comprising the final ensemble model. The internal architecture of the proposed ensemble allows the replacement of both base classifiers and the heuristics employed to carry out classifier fusion, thereby achieving a high level of flexibility and making it possible to configure and adapt the model to different contexts. Experimental results using different datasets and several gene sets show that the proposal is able to outperform classical alternatives by using existing prior knowledge adapted from publicly available databases.

@&#INTRODUCTION@&#
With the advent of microarray technology, tumor classification based on microarray gene expression data has become one of the most active research topics in bioinformatics. In this context, classification of microarray samples represents a well-studied problem in statistics and machine learning, where a large number of successful methods have been suggested [1]. However, it has also been shown that commonly used baseline classifiers pose intrinsic drawbacks in achieving accurate and reproducible results. In order to obtain more robust microarray data classification methods, several authors have investigated the benefits of using ensemble alternatives applied to genomic research [2].As a particular case in the use of ensemble systems, ensemble feature selection represents an efficient method proposed by Opitz [3] which can also achieve high classification accuracy by combining base classifiers built with different feature subsets. In this context, the works of Kuncheva and Jain [4] and Oliveira et al. [5] study the application of different genetic algorithms alternatives for performing feature selection with the aim of making classifiers of the ensemble disagree on difficult cases in order to introduce diversity. Reported results on both cases showed improvements when compared against other alternatives.Díaz-Uriarte and Álvarez de Andrés [6] studied the use of random forests for multiclass classification of microarray data and proposed a new method of gene selection in classification problems based on this classifier. Using simulated and real microarray datasets, the authors showed that random forests can obtain comparable performance to other methods including DLDA (Diagonal Linear Discriminant Analysis), K-NN (K-Nearest Neighbor) and SVM (Support Vector Machine).Peng [7] presented a novel ensemble approach based on seeking an optimal and robust combination of multiple classifiers. The proposed algorithm begins with the generation of a pool of candidate base classifiers based on gene sub-sampling and then, it performs the selection of a subset of appropriate base classifiers to construct the classification committee based on classifier clustering. Experimental results demonstrated that the proposed approach outperforms both baseline classifiers and classical ensemble algorithms, such as Bagging [8] and Boosting [9,10].Liu and Huang [11] applied rotation forest to microarray data classification using principal component analysis, non-parametric discriminant analysis and random projections to perform feature transformation in the original rotation forest. In all the experiments, the authors reported that the proposed approach outperformed classical Bagging and Boosting alternatives.Chen and Zhao [12] presented an ensemble of classifiers based on correlation analysis for microarray data classification. Gene features are first extracted by correlation analysis in order to reduce dimensionality. Then, a pool of candidate base classifiers is generated to learn the subsets which are re-sampled with PSO (Particle Swarm Optimization). Finally, best classifiers are selected using EDAs (Estimation of Distribution Algorithms). They compared the results obtained against some advanced artificial techniques on four benchmark databases reporting best recognition rates.More recent approaches include the works of Liu and Xu [13], Kodell et al. [14], Anand and Suganthan [15], Haferlach et al. [16], Zeng and Liu [17] and Yang et al. [18] that present different ensemble alternatives applied to microarray data classification by introducing variations at data, feature, classifier and combination levels [19].Although numerical analysis of microarray data seems quite consolidated, the true integration of numerical analysis and biological knowledge is still a long way off [20]. In addition to classification performance, there is also hope that microarray studies uncover molecular disease mechanisms. However, in many cases the molecular signatures discovered by the algorithms are unfocused from a biological point of view [21]. In fact, they often look more like random gene lists than biologically plausible and understandable signatures. Moreover, an additional shortcoming of standard classification algorithms is that they treat gene-expression levels as anonymous attributes, but a lot is known about the function and the role of many genes in certain biological processes.In this scenario, some authors have claimed that classifiers should take into account both model performance and biological interpretability of their results. With the goal of effectively incorporating prior knowledge into the classification process of microarray data, we have developed genEnsemble. The aim of the proposed model is to incorporate relevant gene sets obtained from our previous WhichGenes server [22] in order to make accurate predictions easy to interpret in concert with existing knowledge. The study carried out in this research work was inspired in past successful results [23] and aims to borrow information from existing biological knowledge to improve both predictive accuracy and interpretability of the final generated classifier.The structure of the paper is as follows. Section 2 presents previous approaches that integrate biological knowledge in the classification process by using ensemble alternatives. Section 3 describes the proposed knowledge-based ensemble model and details how base classifiers are initially selected and further combined. Section 4 introduces the experimental framework, discusses the obtained results and analyses the main characteristics of the generated ensemble. Finally, Section 5 outlines main conclusions and identifies future research lines.As previously stated, the inclusion of additional knowledge sources in the microarray classification process can alleviate several problems related with model performance (e.g.: overfitting and/or the inability of the model to generalize properly) while improving interpretability of results. Related with this issue, the integration of knowledge can prevent the discovery of the obvious, complement a data-inferred hypothesis with references to already proposed relations, avoid overconfident predictions and allow to systematically relate the analysis findings to present knowledge [24]. Mainly motivated by the previous commented advantages, some authors have proposed different alternatives for the integration and use of external knowledge in ensemble-based predictors.In this context, the work of Pang et al. [25] combines a random forests classifier with pathway information. The main objective of this work is to explore the capacity of random forests to assess the importance of variables in order to derive the enrichment of pathways. In this sense, the work is more focused on the functional analysis than in the classification task.Later, Wei and Li developed NPR (Nonparametric Pathway-based Regression) [26], a modification of the Boosting schema. This work proposes that, at each step of the boosting procedure, a classifier focused on each metabolic pathway is trained and the best one is selected. At the end of the process, a classifier based on several base learners with biological criteria is obtained and the best pathways during the boosting are also reported. During their experiments, the authors included different pathways related to breast cancer disease.More recently, Binder and Schumacher developed PathBoost [27], an improved version of their BRR (Boosting Ridge Regression) algorithm previously proposed [28]. BRR is based on Boosting, but also incorporates a gene selection schema. PathBoost improves BRR by incorporating gene networks in order to jointly select those network-related genes. They derived a gene network from KEGG by creating a gene–gene connection every time there are relations in the KEGG database.Finally, Daemen et al. [29] proposed an ensemble model able to integrate three different data sources (metabolic pathway information, protein-protein interactions and miRNA-gene targeting) by exploiting the properties of kernel methods. They incorporated the relations between genes with similar functions but active in alternative pathways in a LS-SVM (Least Squares SVM) classifier using spectral graph theory. The graph-related information was subsequently utilized by the SVM for the calculation of patient similarity.Although all the previous approaches reported valuable improvements related with model performance or interpretability, they are constrained by the use of a few limited sources of biological information or the utilization of a specific classifier/algorithm.Taking into consideration the state-of-the-art and with the goal of overcoming existing limitations of previous works we have developed genEnsemble, a classification model in which the knowledge is freely represented using biologically relevant and problem-related gene sets. This set-based approach allows both (i) the homogenization of knowledge coming from multiple and different sources of biological information in an intuitive way for the expert and transparent for the model and (ii) the effectively use of this knowledge as a natural way of performing the necessary feature selection process for microarray classification.By following this straightforward approach, the proposed model is able to integrate data and knowledge through the training of several classifiers that use the initial genet sets provided by the expert as a feature selection filter directly applied to the input microarray samples. With the goal of guaranteeing the diversity in the generated ensemble, the proposed model is able to use different types of base classifiers due to the fact that it defines a heterogeneous model for the combination of classification techniques based on abstract outputs (i.e.: taking into consideration only the class label assigned by each base classifier).As a result, given a microarray dataset and a group of gene sets provided by the expert, there are four main steps required by genEnsemble in order to construct the final classification model: (i) generation of candidate classifiers, (ii) evaluation of candidate classifiers, (iii) selection of base classifiers and (iv) training of base classifiers. The following subsections introduce and explain each phase in detail.As previously stated, in order to facilitate the diversity in the final ensemble the proposed model does not impose any limitation to the number or type of base classifiers. For this reason, we have selected four different and well-known classification algorithms previously applied to microarray data: (i) IBk, (ii) Naïve Bayes Simple (NBS), (iii) Support Vector Machines (SVM) and (iv) C4.5 classifier [30].Together with the type of classifiers, those gene sets provided by the expert will determine the number of candidate classifiers initially generated by genEnsemble. In the proposed model, each gene set is applied to the whole microarray dataset as a filter for constructing different subsets of training. Each subset (containing only the expression level of those genes related with the corresponding gene set) is subsequently used to train a classifier of each type so, given a fixed number of N available classification algorithms (in our case N=4) and a variable number of M gene sets provided by the expert, genEnsemble will create and train N×M candidate classifiers.Given the knowledge-based approach inherent to the input gene sets initially provided by the expert, it is highly probable that not all the generated candidate classifiers will be accurate enough to notably contribute to the final ensemble. Moreover, the existence of certain candidate classifiers can introduce noise in the model which would disturb the global classification accuracy. For this reason, genEnsemble introduces a novel evaluation step in which the expected performance of each candidate classifier is assessed.The proposed evaluation method applied in genEnsemble is based on an internal k fold-cross validation strategy applied over the training data. By using this evaluation scheme, all the samples of the training set are used for classification taking into consideration the underlying class distribution and guaranteeing that there is no overlapping between training and test sets.In order to precisely measure the accuracy of each candidate classifier we use Cohen's Kappa statistic [31]. This index compensates for classifications that may be due to chance and it is considered a standard statistically-robust meter for measuring the accuracy in multiclass problems [32]. Possible values range from 0 (random classification) to 1 (perfect classification) and it is calculated as shown in Expression (1)(1)Kappa=Pr(a)−Pr(e)1−Pr(e)where Pr(a) represents the observed accuracy and Pr(e) stands for the probability which is due to chance. The value of Pr(e) is computed taking into consideration the marginal distribution of each class for each classifier as shown in Expression (2)(2)Pr(e)=∑c=1kPr(Creal,c)Pr(Cclassifier,c)The evaluation method implemented in genEnsemble does not compute the kappa index as a global performance measure for each candidate classifier. Instead of this, we calculate the specific kappa value for each class being categorized by each candidate classifier (kappa by class).In order to perform this calculation, a partial confusion matrix for each class should be generated. As an example, Table 1shows the partial confusion matrix associated to a given class A that can be easily derived from the standard confusion matrix of any classifier in a multiclass problem.In Table 1, CAAis the number of samples of class A correctly classified, CANstands for the number of samples incorrectly classified as being member of A, CNArepresents the number of samples of class A that were misclassified (labeled with other class) and CNNdenotes the number of samples being member of a class distinct from A that were categorized with a class different to A.Using the information of the partial confusion matrix represented in Table 1, Expressions (3) and (4) show how to compute the corresponding values of Pr(a) and Pr(e), respectively(3)Pr(a)=CAAN+CNNN(4)Pr(e)=CAA+CANN×CAA+CNAN+CNA+CNNN×CAN+CNNNwhere N represents the number of samples stored in the partial confusion matrix.For binary problems, the partial confusion matrix of each class generates the same value for the kappa index, so its calculation can be simplified by directly using the kappa value corresponding to the original confusion matrix.Starting from the accuracy values (kappa by class) previously obtained for each candidate classifier, the selection step of genEnsemble searches for those classifiers with a better estimated performance for each class. In order to identify valuable classifiers, we define a global minimum percentage threshold, λ, which is applied to the best kappa value obtained for each class with the goal of selecting those classifiers with a kappa greater than this limit.It should be noted that the final kappa value representing the cut point (i.e., identifying if a given candidate classifier is accurate enough for being selected as base classifier) is different for each class. Moreover, each base classifier maintains a record with those classes for which it was selected, being its associated classes.The last phase in the construction of the ensemble model is the training of those base classifiers finally selected. In the proposed model, base classifiers are trained using the original data without any type of transformation, opposed to way of other well-known schemes like one-vs-all (OVA) or one-vs-one (OVO). The training phase finishes with the construction of the final ensemble that is composed by the trained base classifiers together with their corresponding estimated performance values. These indexes will be later used for generating a combined output from their individual classifications.In order to put together these four phases comprising the whole process, Fig. 1presents the pseudo-code of the proposed approach in which candidate classifiers are initially constructed and evaluated and base classifiers are later selected and trained.Once the model is constructed and trained following the pseudo-code algorithm presented in Fig. 1, the model is ready for the classification of new samples. In the proposed approach, the classification process involves two main phases: first, base classifiers label the new sample and then, their outputs are combined in a unique final classification. Although the first step is very similar to other well-known approaches (e.g.: Bagging), the second step (combination) is carried out using a scheme that takes advantage of the previously estimated performance values of each candidate classifier (kappa by class).The proposed combination strategy uses a variation of the classical weighted voting schema in which, apart from the weights assigned to the classifiers, it is also taken into consideration the list of their associated classes. Therefore, given a new sample the output of each base classifier (assigned class) will be only used if it matches with their corresponding associated classes. In such a situation, the partial weight assigned to the vote of each base classifier will be the kappa by class value previously computed for this ‹classifier, class› pair.Taking into consideration that the number of base classifiers finally voting depends on both the predicted class and their associated classes, the final confidence value for each potential class is divided by the number of classifiers that have this class as associated. Expression (5) shows how to compute the similarity degree of each class, ci, with a given sample, x(5)S(ci,x)=∑j=1Nδ(Cj,ci,x)Nwhere Cjstands for a classifier that contains ciin its associated classes, N is the number of classifiers that own the class cias associated andδ(Cj,ci,x)represents the particular vote of a base classifier calculated by means of Expression (6)(6)δ(Cj,ci,x)=kappabyclassifxwas classified asci0otherwisewhere kappa by class is computed using Expressions (1) and (2).The classification result provided by genEnsemble is a vector containing the confidence values for all the potential classes. The class with a greater confidence value will be selected as the final classification for the new sample.In order to test the performance of the proposed approach, the accuracy of genEnsemble is compared against both several well-known ensemble alternatives and different successful classifiers for analysing two groups of DNA microarray datasets involving various tumor tissue samples. The three datasets contained in each group are related to (i) breast cancer and (ii) leukemia. In our experiments, two different sources of biological information are used: SABiosciences11http://www.sabiosciences.com/.metabolic sub-pathways and OMIM (Online Mendelian Inheritance in Man) database.22http://www.ncbi.nlm.nih.gov/sites/entrez?db=omim.Three publicly available breast cancer datasets coming from the GEO (Gene Expression Omnibus) database33http://www.ncbi.nlm.nih.gov/geo/.were used to carry out part of the experimentation. The goal of this experiment was to identify if the estrogen receptor of the samples is active (ER+) or inactive (ER−), which is interesting as the aggressiveness of the treatment depends on it. Table 2shows the distribution of samples in datasets GSE2034, GSE2990 and GSE3494.In order to complement the study with a multiclass problem, three public leukemia datasets were also used. These datasets were taken from the previous studies of Gutiérrez et al. [33], Bullinger et al. [34] and Valk et al. [35]. We have selected samples from each dataset belonging to 4 different groups of acute myeloid leukemias including (i) promyelocytic (APL), (ii) inversion 16, (iii) monocytic and (iv) other AMLs. The distribution of samples is shown in Table 3.For the selection of the relevant gene sets representing prior explicit available information the following sources were used: (i) 33 metabolic sub-pathways related to existing cancers in SABiosciences previously analyzed in studies by Tai and Pan [36] and Wei and Li [26] and (ii) some groups extracted from the OMIM database. OMIM groups used in the classification of the leukemia datasets (Table 3) are those that correspond to various types of leukemia (myeloid, monocytoid, promyelocytic and general leukemia) while the groups used in the classification of breast datasets (Table 2) are those that correspond to breast cancer. All the gene sets were downloaded using our WhichGenes server [22].With the goal of correctly assessing the performance of the proposed model, we conducted a 10 fold-cross validation experiment [37] for each one of the six datasets. The results obtained by genEnsemble were compared with those achieved by four previously applied classifiers (i.e.: IBk, NBS, SVM and J48 implementation of C4.5 release 8 algorithm) plus three classical ensemble alternatives (i.e.: Bagging [8], AdaBoost [38] and Stacking [39]) using different combinations of these successful base classifiers. All the experiments were executed in the AIBench platform [40] using a Weka [41] plugin.Based on previous successful results [42,43], genEnsemble was configured to use a global minimum percentage threshold λ of 84% for the selection of the best base classifiers. Moreover, IBk technique was configured to use three nearest neighbors. J48 algorithm was setup to build a pruned tree using a confidence factor of 0.25 for pruning. The SVM model was a C-SVC with a linear kernel. Bagging classifiers were configured to perform 10 iterations with a bag size of 100%. Version M1 of the AdaBoost algorithm was selected and configured to carry out 10 iterations. Finally, an IB1 classifier was selected as meta-classifier for the Stacking ensemble.As previously commented, each gene set acts as a filter from the microarray dataset in order to select only the expression information of those genes belonging to the set. In the case of the proposed genEnsemble model, each candidate classifier was trained with each gene set. The rest of the analyzed models were trained using the union of all the genes belonging to the available gene sets.

@&#CONCLUSIONS@&#
