@&#MAIN-TITLE@&#
Recursive head reconstruction from multi-view video sequences

@&#HIGHLIGHTS@&#
A particle filter method is proposed to estimate pose and shape of faces in videos.Noisy or outlier points are handled using multi-hypotheses sampling in the filter.The approach is evaluated on synthetic data to validate the method.Comparison with the Levenberg–Marquardt optimization is performed.Visual and biometric results on real videos confirm the efficiency of our method.

@&#KEYPHRASES@&#
3DMM,Particle filter,Shape estimation,Facial biometry,

@&#ABSTRACT@&#
Face reconstruction from images has been a core topic for the last decades, and is now involved in many applications such as identity verification or human–computer interaction. The 3D Morphable Model introduced by Blanz and Vetter has been widely used to this end, because its specific 3D modeling offers robustness to pose variation and adaptability to the specificities of each face.To overcome the limitations of methods using a single image, and since video has become more and more affordable, we propose a new method which exploits video sequences to consolidate the 3D head shape estimation using successive frames. Based on particle filtering, our algorithm updates the model estimation at each instant and it is robust to noisy observations. A comparison with the Levenberg–Marquardt global optimization approach on various sets of data shows visual improvements both on pose and shape estimation. Biometric performances confirm this trend with a mean reduction of 10% in terms of False Rejection Rate.

@&#INTRODUCTION@&#
The recent rise of biometric techniques stimulates their use to automate the process of people recognition in a wide variety of systems, from computer locking devices to people authentication in airports. For each application, a compromise has to be found between the recognition rate of the biometric system on the one hand, and its easiness of use, cost and computation time on the other hand. The different types of biometric identifiers used for human recognition (fingerprints, iris, face, veins, etc.) have different requirements in terms of acquisition and do not lead to the same recognition accuracy.Among all of them, facial biometry offers the advantage of being easily acquired without any contact with sensors, but suffers from specific issues of acquisition conditions (illumination, pose, facial expression). This is especially the case in video surveillance or in recognition systems designed to avoid behavior constraints in order to simplify the process from the user point of view. As such systems are not intrusive for users and due to the easiness of face acquisition, specific work has focused on face reconstruction and comparison methods. Moreover, facial biometry is sometimes the only biometric identifier available. To solve the different problems outlined above, the field of face recognition has been an active research area for many years, first on still images [30,9,5], then on video [23]. This extension is particularly interesting since video-based systems become more and more affordable, and have the advantage of increasing available observations. When people move about in uncontrolled scenarios, the information from a face observed under different poses in the sequence can be merged, and is then compared to a reference picture. Among existing face recognition algorithms, a number of methods are based on the comparison of frontal views (the reference view is generally the frontal picture on ID documents). A frontal view therefore has to be generated from the acquisitions. This can be performed via a 3D reconstruction of the face using the acquired images, from which synthesized views at any pose can be derived. Given the specificity of the face reconstruction problem (as opposed to object reconstruction without prior knowledge), model-based methods are privileged as they limit the risk of aberrant reconstruction, achieving a compromise between the information coming from the observations and the prior knowledge on the class of faces.Most existing algorithms designed to estimate parameters of such 3D models are based on a single image input and highly depend on the quality of the observations [5,27]. Nevertheless, in order to obtain more accurate results, it is interesting to use several images to consolidate the reconstruction. In [2], the authors proposed to fuse images based on stereovision. The use of video sequences has not been widely exploited, except for structure from motion methods, where images are considered as an ensemble to estimate the model parameters [10]. In [32], the authors extend a single image based method to video sequences by fusing the estimations obtained at each instant independently, without verifying the model coherence. However, temporal constraints between states estimated at successive instants are not integrated in the process, which would improve results.To propose a real-time working system, we have to exploit the incoming video frames on the fly. To this end, we propose a new method based on the update of a 3D head model by using a particle filter framework, which extends the work in [13], and has, to our knowledge, never been proposed. An important feature of the proposed approach is that previous observations are implicitly taken into account to estimate the model at the current instant. The key of our algorithm is to integrate the unknown shape coefficients in the particle state and to consider them as static parameters, unlike the pose which varies over time. Besides an adaptation to real data, we propose here an improved algorithm for face estimation, robust to noisy or aberrant detections thanks to multiple hypotheses handling, contrary to common gradient methods which optimize a unique solution associated with a given set of observations.In Section 2, we first present the chosen head model, before giving an overview of methods which estimate the associated parameters, both for single and multiple input images. In Section 3, we detail how to adapt a particle filter method to handle static parameters for facial shape estimation in video sequences, and propose some alternatives to improve this static parameter estimation. Section 4 presents how the observations are exploited in the particle filter and used to generate the frontal view. Section 5 details a method which is compared to our particle filter-based method in Section 6. This alternative method is based on a Levenberg–Marquardt optimization to estimate the pose and the shape. Experiments are done on both synthetic and real data. They are first analyzed on visual illustrations, to demonstrate the improvements at the image level. Then, since our final goal is to improve facial recognition performances by improving the head reconstruction using video sequences, an evaluation based on biometric performances is also proposed, before concluding with the perspectives of our method.The method we propose for face reconstruction from video sequences relies on a head model which is described in this section. We will then present the existing methods to estimate its parameters.As underlined previously, many facial biometric systems must be able to work with unconstrained user behavior, which implies handling non-frontal poses in the input images.Since most recognition algorithms are based on the comparison between frontal views, a frontal view has to be generated from the acquisitions. This can be performed via a 3D reconstruction of the face from which images at any pose can be derived.There are many ways to reconstruct a 3D object from a set of views. We can distinguish purely geometric approaches, which can be applied to any object, from model-based methods, which use some prior information on the object to reconstruct, and are therefore specific to a class of objects. Among generic methods based on one or a set of views, the best known algorithms are based on shape from shading [35], structure from motion [35] or stereovision [6,18,4]. For the latter, an important constraint is to perform point matching between images which therefore need to be acquired under quite similar points of view. Reconstruction algorithms can also exploit other devices like 3D-scans, depth sensors [36] or structured light projectors [34]. Here, we limit our approach to image-based methods.There exist several solutions to reconstruct a 3D object from a set of views, and we chose model-based methods to exploit prior knowledge on the object, here the face. The contribution of the prior is twofold: first, it can be used to initialize a solution corresponding to a valid shape (for instance, a mean model), then, it prevents the algorithm from delivering a solution which does not belong to the face space.We use a 3D deformable shape model constructed in a similar way as the 3D Morphable Model (3DMM) introduced in [5]. This model has been chosen for several reasons, the first being its 3D modeling (by opposition to 2D), necessary when faces under any pose are considered (2D models learned on frontal views indeed cannot be used with non-frontal face inputs). Moreover, as the final aim is to establish a comparison score between the frontal view of the estimated face and its corresponding ID picture, it is necessary to adapt the model so that it fits the observed identity as well as possible. The 3DMM allows for this adjustment, as it describes the deformations of a mean face on two levels:•The shape space, characterized by a mean shapeS‾and a set of deformationssi,i=1,…,Mcomputed by principal component analysis over a database of aligned head scans. Each instance of this model can then be written as:(1)S=κS‾+∑i=1Mαisiwhereαi,i=1,…,Mare the weighting parameters which characterize the similarity with the mean shape andκis a scaling factor. The mean shapeS‾is defined by a set ofnv3D vertices, and each vectorsicorresponds to deformations associated with this set of points. A mesh is then defined from these vertices by adding facets to describe the entire head surface.The texture, that associates a color with each vertex of the mesh, is stored in a texture map (Fig. 1) independently of the shape parameters.Like the shape space, the texture space can be described by a set of texture eigenvectorsti,i=1,…,M′and a mean texture mapT‾. Any instance of the model is then a linear combination of these vectors, so that:(2)T=T‾+∑i=1M′βitiwhereM′is the number of texture eigenvectors, andβi,i=1,…,M′are the weighting parameters for the texture similar toαiin Eq. (1) for the shape.A probability is associated with each shape parameter value, as follows:(3)p(αi)∼e-12αi2σi2withσitheith eigenvalue of the shape covariance matrix. A similar probability can be written for the texture parameters.In this article, the parameter estimation focuses on the geometrical part of the model, and the texture is then estimated in a second step. Some instances of the morphable shape model are given in Fig. 2illustrating its variations depending on the parameter values.A first algorithm to estimate the shape and texture parameters of the 3DMM was proposed in the seminal paper [5]. This algorithm is based on the optimization of a similarity score between the input image and a rendered view synthesized given the estimated pose, shape and texture. In case of perfect fitting, the input image and the rendered one should be exactly the same. The optimization is performed using stochastic gradient descent, in order to speed up the process and to avoid local minima.In [26], the authors introduced a faster method, based on Analysis-by-Synthesis as previously mentioned. The difference image (between the input image and the synthesized one) is expressed as a function of different derivative terms with respect to the unknown variables to estimate (pose, texture and shape parameters, and illumination). Because this equation is linear when some of the parameters are fixed, the proposed method first optimizes iteratively the rigid transformation, then the shape, the illumination, and finally the texture parameters. Thus, dimensions are reduced for each of these optimization steps, and most of them are linear (when estimating one set of parameters, the others are fixed). Similar results are obtained in [5], but with a computation time divided by five.In [27], the authors add features to the cost function in order to expand the convexity domain around the optimum, and thus limit the problem of local minima. Moreover, the estimation accuracy is improved as the use of multiple features leads to a better fit between the model and the observations.Nevertheless, even if the model fits the observations on a single image well, the 3D fitting is not guaranteed. Actually, due to the projection from the 3D world onto the image plane and the occlusions of some parts of a face in an image, some information is missing and therefore the estimation might be erroneous. This is why new algorithms based on multiple image fitting have been proposed. We present them briefly in the next subsection.In [2], the fitting algorithm proposed in [27] was adapted to use a set of images acquired simultaneously. Besides the estimation of the pose and the model parameters, the authors also estimate the camera calibration parameters. This method improves the results of algorithms relying only on a single image, but as for previous methods, noisy observations can lead to an inaccurate estimation. When facing non-frontal poses, the extraction of features such as points of interest (eye corners, ears, etc.) might be difficult. Moreover, when using gradient based optimization methods, the final estimation highly depends on the initialization: if the starting point is too far from the real value, the solution can be stuck in a local minimum.To fit a head model to a face seen in a video sequence, some methods simply apply an additional selection step over all available images to extract the best one according to some criteria. One of the fitting methods introduced previously can then be applied on the single chosen image [7]. In [32], two experiments were proposed; the first one consists in estimating the parameters using each frame independently before making a fusion by a linear combination of these estimations. The second one uses all input images together to optimize parameters, leading to a single estimation based on the whole sequence.The advantage of using stereovision or video sequences is that it guarantees a better 3D estimation as the model is fitted on observations under various poses. Moreover, there is more robustness to outliers or noisy detections, because one point being badly detected on one image may be correctly detected on other ones. If the feature points used for the reconstruction are well detected in most of the images, the estimation will lead to a fitting towards the good detections. Moreover, using stereovision or video sequences allows consolidating the estimation between views under various poses. Nevertheless, no specific method has been proposed to sequentially update the model using a video sequence. Indeed, stereovision is often based on images acquired simultaneously by a set of cameras, and the video-based method proposed in [32] is applied offline. We present here a new video-based approach, using sequences acquired from a set of calibrated cameras. Unlike the previous methods, we propose updating the parameter estimation online, with each new incoming observation. Thus, at each instant, we obtain an estimation built on all previous views, which can be exploited before the end of the acquisition.In this paper, our goal is to estimate the parameters of the shape model introduced in Section 2.2. The methods which have been presented previously iteratively update an initial estimate, and the output is a unique instance of the morphable model. Unlike these types of algorithms, we propose here representing the previous estimation as a density, which characterizes the probability of realization over the whole shape space. This allows us to cope with the inherent nature of noisy data and to maintain multiple hypotheses during the estimation process, that are reinforced or eliminated with new frames.We rely on the Gaussian assumption made in [5] to define the prior distribution of the model shape parameters. This initial density is then updated each time new information is available. Given a new frame (or a set of frames when multiple views are available), our goal is to update the previous distribution characterizing the prior constraint and the past observations by taking the current ones into account. This can be done using a Bayesian approach, which allows a compromise between the parameter validity and the correlation with the observations.Several declinations of the Bayesian theory for sequential updating can be cited here, such as the Kalman filter [16], the extended Kalman filter [28], the Unscented Kalman [15] and the particle filter [8]. Due to the non-linearity of the involved functions (perspective projections, projection of a 3D object leading to partial occlusions) and the multi-modal distributions we handle, we choose to work with particle filters. As developed in Section 4.1, the particle filter offers a structure that maintains multiple hypotheses over time, which is useful when feature extraction is difficult and leads to outliers.In this work, we use a particle filter to jointly estimate the pose and the facial shape parameters over a video sequence. In the experiments, the camera extrinsic and intrinsic parameters are known, and not subject to the estimation. We first describe how to estimate dynamical states (position, pose, etc.) and then discuss how to integrate and estimate static parameters in the filter.The particle filter algorithm [3,8] aims at filtering a hidden state x by representing it as a density updated at each instant t given the observationsyt. At each time t, the hidden statextcan be derived from the previous statext-1given an evolution equation:(4)xt=g(xt-1)+μtwith g a (possibly non-linear) function characterizing the system dynamics, andμtthe associated noise. The observations at time t are derived from the corresponding statextaccording to the following equation:(5)yt=h(xt)+ηtwith h the (possibly non-linear) observation function andηtthe noise associated with the observations. The set of observations from instant 1 to t is denoted byy1:t.The goal is to estimate at each time t the filtering densityp(xt|y1:t)by a set of N elementsxt(i)called particles representing possible states at time t. Each particlext(i)is associated with a normalized weightwt(i)which characterizes its likelihood. The density is approximated using the Monte-Carlo method given the set of N particlesP=(xt(i),wt(i)),i=1,…,N:p̃(xt|y1:t)=∑i=1Nwt(i)δxt(i)(xt)whereδxt(i)(xt)=1ifxt=xt(i),0elsewhere.Given the initial particle distribution{(x0(i),w0(i)=1/N),i=1,…,N}, the following steps will be repeated to estimate the a posteriori density at each instant:•Prediction: each particlext-1(i)moves from timet-1to time t given the probabilityp(xt|xt-1)associated with the system dynamics (Eq. (4)).Correction: each weightwt(i)is then updated according to the particle likelihood given the current observationsp(yt|xt(i)):(6)wt(i)∝wt-1(i)p(yt|xt(i)).This likelihood score defines how probable the state represented by each particle is, taking current observations and model knowledge into account.Finally, a resampling is performed if the particle weights are too scattered. This is characterized by the Effective Sample Size (ESS) approximated by1∑i=1Nwt(i)2.Let us now consider the case of filtering with unknown static parameters. This is especially useful when some parameters of the object to track are unknown, and when their values impact the evolution and observation functions. For instance, if we consider the size of the object as a parameter, it is clear that it will influence the observation function, as, the bigger it is, the larger its projection in the image for a given pose. To simplify the notations, we consider the concatenation of all unknown parameters in a vectorθ(here, the shape parametersαiand the scale factorκ). As previously mentioned, the aim is to estimate sequentially the dynamic statext(the pose) given the observations(y1:t). However, we now take into account the unknown parameters in the evolution and observation equations. The parameterθis a characteristic of the object and is supposed to be constant over time, but as it is unknown, it has to be estimated. We propose updating this estimation along the sequence, and we noteθtthe current estimation at time t. With these notations, the previous system can be rewritten as follows:(7a)xt=g(xt-1,θt-1,μt)=gθt-1(xt-1,μt)(7b)yt=h(xt,θt,ηt)=hθt(xt,ηt)The functions g and h now depend onθ, meaning that the hidden state evolution depends on the parameter values, as do the observations given a state. In addition to the state estimation, a second task is to determine the static parameter vectorθ.In [17], the authors reviewed existing methods to deal with unknown parameters in particle filter algorithms. We do not hereby consider offline methods, which handle a set of observations and optimize the unknown static parameters and the poses globally. Instead, we focus on online methods which update the pose and parameter estimation recursively given the incoming observations. Thus, we can have the best estimation available at each instant, since it is computed from the previous estimations updated with the current observations. If the last computed estimation is good enough (for instance if it fits well with the observations), there is no need to continue the process. Otherwise, the last estimation is used for further improvement using new frames.Letp(θ)be the prior distribution of the unknown static parameters. Our algorithm aims at estimating iteratively the vectorθt∗, corresponding to the shape estimation at time t, and the associated posext. The probability ofθtat time t given the observations can be obtained by integrating over all possible values of the hidden statex1:t:(8)p(θt|y1:t)=∫Xp(x1:t,θt|y1:t)dx1:twhereXis the hidden dynamic state space. To estimate the value ofθiteratively, this static parameter can be integrated into the hidden state, thus increasing the size of the particle state [29,22]. The joint densityp(θt,x1:t|y1:t)can then be evaluated using Monte-Carlo methods.Recall thatx∈Xis the dynamic state of dimensionnx, andθ∈Θthe vector of static parameters of dimensionnθ. The complete vector to estimate is then{x,θ}∈X×Θ, with dimensionnx+nθ. Each particle will then be represented by a dynamic state partxt(i)and a static state partθt(i).The integration of static parameters into the hidden state requires the application of an artificial move on the particle static part in order to explore the associated space. By definition, since the parameters are static, the dynamic function which determines their evolution in time is the identity function. In this case, the parameters do not change, and the only values tested over the sequence are the ones sampled at the particle initialization step, as they are not modified afterward. After some resampling steps, a few or even only one values will still be represented, and this impoverishment leads to a wrong estimation of the parameter values. A workaround to this issue is to apply an artificial move on the static parameters from frame to frame. Different types of moves which are listed below have already been proposed. We also propose two variants that improve the estimation process.Gaussian noise. A first method consists in considering the static parameters as dynamic ones, as in [22]. Therefore, at each time t, a Gaussian noise is added to the perturbation, which can be considered as an artificial evolution:(9a)θt+1=θt+εt+1(9b)εt+1∼N(0,W)with W a covariance matrix characterizing the noise to add. Thanks to this step, diversification is introduced for the static part of the particle filter, and other states, than the ones initially sampled, can be evaluated.Adaptive Gaussian noise depending on particle weight. With the previous move based on Gaussian noise addition, there is a loss of precision after the alteration, since good particles can be moved far away from their initial good position. To limit this effect, we propose making the covariance matrix W dependent on each particle weightwt(i). If the weight is high (meaning that the particle is in agreement with the observations), we use a covariance matrix with small values to make a local exploration of the space. Conversely, if the weight is low, a noise with a higher covariance will be applied to the static parameters in order to move the particle in other subspaces.Resample-move algorithm. In [11], each move applied on the static state is generated by a MCMC step (Monte Carlo Markov Chain). With this method, the new static states which are only generated are accepted under some likelihood conditions. This idea, called Resample-Move algorithm, has been introduced in [12]. The method works as follows: at each time t, a MCMC move is applied to each particle. This move is generated by a kernelKt(x1:t′,θ′|x1:t,θ)havingp(x1:t,θ|y1:t)as invariant distribution. The move can be limited to the static stateθ, and can be obtained by the Metropolis–Hastings algorithm, computed in two steps:•sample a new candidate for the static parameter:θ′∼p(θ′|θ),samplev∼U[0,1], a random sample from the uniform distribution between 0 and 1. If(10)v⩽min1,py1:t|x1:t,θ′py1:t|x1:t,θBy directly applying this formula, the computational cost is increased at each timestep, as more frames have to be considered for the likelihood computation. Indeed, as the static parameters are modified between frames, all likelihood values with respect to previous observations need to be recomputed. To limit this effect, we introduce a periodΔT, which defines the number of frames to be taken into account for the MCMC validation. The move is then accepted if:(11)v⩽min1,pyt-ΔT:t|xt-ΔT:t,θ′pyt-ΔT:t|xt-ΔT:t,θwithΔT=0if only the current observations are used.For each particle, the sampling step(12)θt′(i)∼pθt(i),x1:t(i)allows for a local diversification of the static parameters only. Like systematic Gaussian noise addition, we propose to evaluate another sampling step, based on the prior distribution of the static parameters, and independently of the particle current shape parametersθt(i). Thus, new subspaces can be explored and moves are allowed anywhere, as long as the acceptance condition in Eq. (10) is satisfied. With this method, it becomes possible to get a particle out of a local maximum. Nevertheless, if the space dimension is too high with very few areas with high likelihood, the probability to accept a move will be very low.Particle filters that include unknown static parameters have been detailed from a theoretical point of view this past decade but very few applications have been proposed in the field of image processing. One use of such methods was nevertheless given in [22], where the authors evaluated dimensions of simple geometrical objects in video sequences, by considering these values as unknown static parameters. The relation between the unknown dimensions and the observations remains simple, and there is no correlation between the different static values.In this paper, we want to estimate the unknown parameters of a more complex model, namely the shape coefficientsαiand the scaleκ(Eq. (1)) of the shape model presented in Section 2.2. Let us underline that the observations are highly dependent on the shape parameters, and this relation will be exploited to estimate them using the available observations. Pose and shape parameters must be estimated jointly, otherwise a pose error will be compensated by a parameter error and conversely. As illustrated in Fig. 2, due to the construction of our shape model, each parameterαiin Eq. (1) modifies the whole face because each associated deformation eigenvector impacts all vertices. The process leading from the shape parameters to the observed image is as follows:1.model deformation given(α1,…,αM)and the scaleκ,rotation and translation in the 3D world reference,projection onto the image plane.The dependence onθneeds therefore to be considered when using the observation function, due to the shape parameter impact on the 3D position of each point. The global system to consider is the following:(13a)xt=g(xt-1,μt)(13b)yt=h(xt,θ,ηt)=hθ(xt,ηt)For our study, instead of applying the usual prediction process of the particle filter (Eq. (13a)), we favor the use of feature points detected in the current frame to initialize the pose. Indeed, to handle low framerates, a very high number of particles would be necessary to cover the space of all possible poses, and only a few of them would be relevant. To avoid this step, we compute the particle poses directly given the current feature point detections. To this end, an initial posext0is estimated by fitting the mean model using the method presented in [31]. This algorithm computes the translation, the rotation and the scale which minimize the least mean square error between two sets of 3D feature points. This is done by computing the Singular Value Decomposition (SVD) of the covariance matrix of these two sets of points, which is then used to determine the posext0. A posext(i)is then sampled aroundxt0for each particle, using a Gaussian noise (Eq. (14)).Algorithm 1 presents the particle filter with two exclusive possibilities of move applied to the static parameters. In the case of Gaussian sampling, a move is automatically applied to each unknown parameter. When using MCMC, the move is applied conditionally to the gain in terms of likelihood between the previous and the new sampled states (Eq. (10)).Algorithm 1Static shape parameter estimation with a particle filterSample the shape parametersθfrom a prior Gaussian distribution to initialize the set of particlesθ0(i),w0(i)=1/N),i=1,…,NDefine the move to apply:Move=Gaussian_Samplingor MCMCfort=1→NframesdoInput: 2D feature point positions (possibly noisy).Mean shape model fitting to estimate the initial posext0using the method by [31].fori=1→Ndo– Sample around the estimated pose:(14)xt(i)=xt0+nx,withnx∼N(0,Σ).if (Move=Gaussian_Sampling) thenSample around the previous shape parameters:θt(i)=θt-1(i)+nθ, withnθ∼N0,1w(i).end if– Update the weight with the likelihoodpyt|xt(i),θt(i):wt(i)∝wt-1(i)pyt|xt(i),θt(i), as in Eq. (6), but taking the parameter values into account.end forResamplingif (Move=MCMC) thenfori=1→NdoApply a MCMC moveend forend ifend forIn our application, each particle state is decomposed into a dynamic part (the posext) and a static part (the scaleκand shape parametersαi, such thatθt=κ,α1,…,αM) and must be updated and evaluated with the incoming observations. In this part, we detail how we use the images for these steps, and introduce a new way to handle noisy observations based on the particle filter structure. Then, we present the texture extraction process once the shape evaluation is done.At each time t, the particle weights are updated by computing the likelihood of their states with the current images. This can be done with commonly used criteria, such as edge or silhouette scores [22], which compare the edges in the input images and the ones of the projected object, or feature point projection, to verify their proximity to the detections [25]. These feature points are also used to initialize the pose as shown in Algorithm 1, by fitting the mean model on these points. As we have to handle non-frontal views, noisy or outlier detections can happen, which impact this initial pose estimation. If this pose is wrong, all particles will be badly initialized (Eq. (14)), and further shape estimation and pose improvement will then not be possible. In this part, we focus on the first pose estimation performed at each instant, and propose to exploit the multi-hypotheses structure of the particle filter to improve this step.Different types of detectors can be learned to detect specific feature points of the face, using approaches such as AdaBoost, RealBoost and SVM-learning or Bayesian networks, which are detailed in [33]. Unfortunately, when faces vary depending on pose and acquisition conditions in the videos, it is a challenge to perfectly detect all facial feature points of a face. Depending on the video properties, some points can be missing, while others can be badly detected, as shown in Fig. 3. It is therefore important to take this uncertainty into account in order to handle outliers obtained with the feature point detectors.In Algorithm 1, the pose for each particle is computed by adding some noise to the pose estimated from the set of detected feature points. Instead of using the same noisy pose version for all particles, we propose to exploit the property of multiple hypotheses representation of the particle filter by assigning different poses to particles, which are computed from different sets of feature points. Thus, we capitalize on the multi-hypothesis aspect of the particle filter to manage the outliers obtained with the feature point detectors, since some of the computed poses will be close to the correct one even if bad detections have been found (as all points are not necessarily used for this pose computation). More precisely, given a new frame, the pose of each particle is then computed as follows:1.Select a set of valid points common to all particles:(a)For each feature point detected in all views of a given timestamp, compute its 3D coordinates and consider it as valid if the geometric reconstruction error considering the camera calibration parameters is below a given thresholdσc. The outcome of this first step is a set of 3D feature points of the face.Compute the pose parameters by fitting the mean 3D model on these reconstructed 3D points. This is done following the method in [31], combined with a RANSAC procedure to eliminate wrong feature points. The aim of this step is to find the 3D pose minimizing the distance between the 3D points selected at step (a) and the corresponding points in the 3D model. This distance is called geometric reconstruction error.For each particle:(a)For each feature point not used previously, sample it given a probabilitypDrelated to its detection confidence, and reconstruct the 3D point with the selected detections. Here again, the 3D point is kept only if the geometric reconstruction error is below a thresholdσc.Compute the pose parameters by fitting the deformed model (given the particle shape parameters) to the valid 3D points which have been reconstructed. No RANSAC algorithm is used at this step, as we want to see whether a pose is in accordance with the selected feature points.The distributionpDcan be learned for each detector D given its outputs over an annotated database. To estimate it, we construct a histogram for each detector over the interval of its responses, characterizing the rate:(Number of good detections/Number of detections)for each bin. These histograms are then approximated by a density function in the form of a sigmoid function, characterizing the detector performances. Fig. 4shows this rate given the detector output (horizontal axis) and the estimated sigmoid which has been fitted on these data. We can note that the curves differ from one detector to another, due to their different discriminative power (left eye and right eyebrow corner). A detectionoDassociated with a confidence c is then kept ifpD(c)>u,u∼U[0,1], where u is sampled for each particle. Fig. 5shows the points selected with this sampling method for a set of 10 particles.The advantage of this method is twofold. First, unlike the pose sampling proposed in Eq. (14) of Algorithm 1, no noise is added to an initial pose estimated with the mean model. Here, each particle pose is optimized following the method in [31] using its own shape parameters and the observations.Secondly, instead of using the same set of feature points for all particles based on a binary decision with respect to its confidence and a fixed threshold, this method adds diversity to the sets of points. Good points having an average confidence may still be selected for some particles, and conversely, noisy detections or outliers with good confidence may also be rejected. For instance, in Fig. 5, we can see that the confidence of the right ear detection is not high enough to pass above the previously fixed threshold. Nevertheless, it is sampled for some particles, and is then used to estimate the pose. Other criteria, like edge comparison, will then differentiate the good particles from the bad ones, and assign higher weights to particles that selected the best set of feature points. Algorithm 2 summarizes the global workflow of this algorithm, taking the feature point sampling into account, unlike Algorithm 1. Only the systematic noise addition has been kept for the static parameter moves in this version.Algorithm 2Static shape parameter estimation with a particle filter and feature points managementSample the shape parametersθfrom a prior Gaussian distribution to initialize the set of particlesθ0(i),w0(i)=1/N,i=1,…,Nfort=1→NframesdoInput: noisy 2D feature point positions.fori=1→Ndo– Sample around the previous shape parameters:θt(i)=θt-1(i)+nθ, withnθ∼N(0,Σθ).– Sample a subset of feature points given their confidence and fit the pose of the current shape model using the method by [31].– Update the weight with the likelihoodpyt|xt(i),θt(i):wt(i)∝wt-1(i)pyt|xt(i),θt(i).end forResamplingend forFor each frame and each camera, a texture map is extracted from the corresponding image to get color information on the face. For each pixel, a score is associated with this color, in order to define its quality. As the texture of the face is best viewed in a frontal view with respect to the camera optical axis, the quality is defined using a criterion expressing how frontal each facet of the model mesh is. Thus, for each pixel(x,y)in the texture map which is visible in the input image, its qualityqual(x,y)is computed as:(15)qual(x,y)=n→f(x,y)·z→wheren→f(x,y)is the normal to the face represented at pixel(x,y)andz→the direction vector of the camera optical axis.To obtain the most complete texture map, an intermediate step of texture map fusion has to be performed. We use a linear combination of the texture mapsTMiextracted from different views v(v=1,…,V)at time t and weighted by the criterion presented above. Each pixelTM(x,y)of the resulting texture map is computed from the corresponding pixels in the extracted texture mapsTMvas follows:(16)TM(x,y)=∑v=1Vqualv(x,y)TMv(x,y)Finally, a frontal view is generated using the shape deformations and the texture map that have been computed from the video sequence. This global workflow is illustrated in Fig. 6.To evaluate the proposed particle filter, we compare it to an optimization method based on the Levenberg–Marquardt (LM) algorithm [24]. This method attempts to iteratively minimize an error defined with criteria similar to those used in the particle filter, by mixing gradient descent and Gauss–Newton algorithms. Unlike the particle filter method, this method is global, meaning that it estimates jointly the poses for all frames and the shape parameters (the same for the whole sequence). We use the levmar library available online to this end [20].Lety1:T=y1,…,yTbe the set of observations available in the video sequence, such as feature point positions, gradients, silhouettes, andu=x1,…,xT,θthe vector containing the unknown values, which are the poses and shape parameters. We apply a global optimization using all observations together. The feature pointsỹcorresponding to the estimated poses and deformationsũcan be projected in order to compare them to the feature point detections, corresponding to the observations y. The idea of the algorithm is then to minimize the error‖y-ỹ‖2, considering the following energy:(17)Epoints=∑t=1T1D(t)∑p=1D(t)xproj(p,t,Xt,θ)︸ỹ-xdet(p,t)︸y2where t is the frame index,D(t)is the number of detected feature points at timet,pthe index of these feature points,xdet(p,t)their 2D positions andxproj(p,t,Xt,θ)the projection of the corresponding points from the model in the images given the current estimation of poseXtand shapeθ. Letx2dbe the 2D homogeneous coordinates of the projection; the valuexproj(p,t,Xt,θ)is computed as follows:(18)x2d=ARtκS‾(p)+∑i=1Mαisi(p)+Ttxproj(p,t,Xt,θ)=x2d[0]x2d[2],x2d[1]x2d[2](2D-coordinates after normalization)withTt=(xt,yt,zt)the translation andRtthe rotation matrix derived from the poseXtat timet,κandαi,i=1,…,M, respectively the scale and the deformation values, and A the3×3projection matrix given the intrinsic camera parameters, which are supposed to be known in our experiments.In addition to the retroprojection criterion, other criteria are used, as the ones introduced in [25]. In our case, the error then depends on the feature point projection term, an internal edge term, a silhouette term and a validity term for the shape parameters. The associated energy is a linear combination of those terms, with weightsηc,ηsandηmodelwhich need to be empirically determined. Except for the shape validity term (Emodel), all terms are computed for every frame. The function to minimize is then:(19)E=1T∑t=1T1D(t)∑p=1D(t)xproj(p,t,Xt,θ)-xdet(p,t)2+1T∑t=1Tηcdc(t,Xt,θ)2+ηsds(t,Xt,θ)2+ηmodelEmodelwithdcanddstwo distances characterizing the error between the model and the observations for the internal edges and the silhouette criteria respectively, and the last term corresponding to the realism of the face (like the associated probability proposed in Eq. (3) or a regularity criterion for the deformed mesh).The Levenberg–Marquardt algorithm needs an initial valueu0for the unknown poses and shape parameters. The deformation parameters are set to zero (mean model), the scaleκ0to a mean value calculated over a database and the poses are estimated using the method in [31] combined with RANSAC. The Levenberg–Marquardt algorithm uses the Jacobian of E, with respect to the unknown variables, to optimize the output state u. If we only use the feature points and the prior criteria, an analytical expression can be computed (Appendix A). If other criteria are included, a closed form solution is no longer available, and the Jacobian must be evaluated by finite differences, which increases computing time.

@&#CONCLUSIONS@&#
