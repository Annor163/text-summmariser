@&#MAIN-TITLE@&#
Bayes shrinkage estimation for high-dimensional VAR models with scale mixture of normal distributions for noise

@&#HIGHLIGHTS@&#
The proposed methods are useful for high-dimensional VAR models with small data.A formula for an optimal shrinkage parameter is derived under a Bayes framework.The proposed methods are superior in computation time and performance.

@&#KEYPHRASES@&#
Consistency,Cross validation,Granger causality,Multivariate,t,-distribution,Penalized least squares (PLS),Score function,

@&#ABSTRACT@&#
We propose Bayesian shrinkage methods for coefficient estimation for high-dimensional vector autoregressive (VAR) models using scale mixtures of multivariate normal distributions for independently sampled additive noises. We also suggest an efficient selection procedure for the shrinkage parameter as a computationally feasible alternative to the traditional MCMC sampling methods for high-dimensional data. A shrinkage parameter is selected at the minimum point of a newly proposed score function which is asymptotically equivalent to the mean squared error of the model coefficients. The selected shrinkage parameter is presented in a closed form as a function of sample size, level of noise, and non-normality in data, and it can be efficiently estimated by using a suggested variation of cross validation. Consistency of both of the cross validation estimator and proposed shrinkage estimator is proved. The competitiveness of the proposed methods is demonstrated based on comprehensive experimental results using simulated data and high-dimensional plant gene expression data in the context of coefficient estimation and structural inference for VAR models. The proposed methods are applicable to high-dimensional stationary time series with or without near unit roots.

@&#INTRODUCTION@&#
Vector autoregressive (VAR) models are very useful for analyzing the interrelationship among a set of time series variables, whose application includes forecasting economic time series (Sims, 1980, 1982; Doan et al., 1984; Bańbura et al., 2010), and structural inference in areas such as biology (Opgen-Rhein and Strimmer, 2007) and neuroscience (Harrison et al., 2003). However, since the number of parameters in a VAR model grows quadratically with the dimensionality of the model, estimating a high-dimensional VAR model faces significant challenges such as overfitting with given sample points, while high-dimensional models have recently received a lot of attention (Fan and Li, 2006) due to improvement in data collection technologies.In high-dimensional cases, the penalized least squares (PLS) methods such as the ridge and LASSO regressions have often been used as alternatives to the ordinary least squares (OLS) method (Hoerl and Kennard, 1970; Tibshirani, 1996) among various estimation methods of the VAR coefficients. More recently, the shrinkage method for large-scale covariance matrix estimation developed by Schäfer and Strimmer (2005) has been applied to VARs and it outperformed the ridge and LASSO regressions for high-dimensional data with relatively small sample sizes (Opgen-Rhein and Strimmer, 2007). We will call this shrinkage approach as the nonparametric shrinkage (NS) method because it does not assume any specific parametric distribution for data. While being successful, NS is prone to overshrink estimates of the VAR coefficients for large sample sizes (Opgen-Rhein and Strimmer, 2007). The shrinkage parameter controlling the shrinkage rate of the NS estimates is selected under the assumption of independence between data points (Schäfer and Strimmer, 2005), which is not appropriate for time series data.In this paper, we propose an alternative shrinkage method for VAR models in a Bayesian framework. It is known that the NS estimates can be interpreted as posterior mode estimates when the VAR coefficients have independent normal priors under the assumption of multivariate normal distributions for additive noises (Litterman, 1986). In general, estimates based on posterior distributions of the model coefficients approach the true value of the coefficients as more and more data accumulate if the sampling model is correct (Gelman et al., 1995). This phenomenon is expected to help circumvent the overshrinkage in the NS method.In order to address a wide class of real data analysis tasks, we develop a Bayesian framework under the assumption of scale mixtures of multivariate normal distributions. Scale mixtures of univariate normal distributions have been used for modeling outliers and analysis of robustness of Bayesian models (Andrews and Mallows, 1974; West, 1984). Special cases of scale mixtures of multivariate normal distributions include the multivariate Studentt-distribution (Ni and Sun, 2005) and multivariate Laplace distribution (Eltoft et al., 2006). We propose iterative procedures for inferences based on posterior distributions using conjugate priors and non-conjugate priors, respectively. The Ridge and NS estimates also arise as special cases of the posterior mode estimates.Moreover, for the selection of the shrinkage parameter, which is a hyperparameter in Bayesian VAR shrinkage, we propose a computationally feasible alternative to traditional methods such as the maximization of marginal likelihoods (Chib, 1995; Litterman, 1986; Lopes et al., 1999; Koop and Korobilis, 2009) and Bayesian MCMC sampling methods (Ni and Sun, 2005), for high-dimensional VAR models. For this purpose, we propose a score function of the shrinkage parameter as an improved marginal likelihood, taking account of the difference between the marginal posterior distribution and its limiting distribution in the context of information theory (Akaike, 1978). We show that the expected value of the proposed score function is asymptotically equivalent to the mean squared error of the posterior mode estimates, and we select the minimum point of an upper bound on the expected value as the shrinkage parameter. The selected shrinkage parameter is presented in a closed form which provides a clear interpretation about the effects of the sample size, level of noise in data, and level of non-normality of the noise distribution to the shrinkage level. Next, as a computationally feasible methodology for obtaining the selected shrinkage parameter, a variation of multifold cross validation (CV) is suggested in a way that alleviates the bias in the multifold CV method (Hastie et al., 2001).We also show the consistency of the proposed shrinkage estimator as well as the multifold cross validation estimator under the assumption of VAR processes with additive noises sampled from scale mixtures of multivariate normals. It is known that the multifold cross validation is not consistent in linear regression where the goal is to select the best model among candidates unless the size of validation sets increases dominatingly fast (Shao, 1993). However, it is noted that in nonparametric regressions where the goal is to estimate mean functions or smoothing parameters, the multifold cross validation including leave-one-out cross validation possesses consistency (Yang, 2007). In this paper, we consider VARs with stochastic predictor variables unlike standard linear regressions. The proof technique is closely related to Wong (1983) developed for kernel nonparametric regression. As a result, the assumptions made for proving the consistency suggest that the size of validation sets in multifold cross validation does not need to increase as fast as in Shao (1993).We conducted an extensive simulation experiment for evaluation and comparison of various standard VAR estimation methods under various data-generating conditions, e.g., low- and high-dimensions of the model, VAR models with near unit roots, mild and strong correlations between noise variables, and multivariatet-distributions. The experimental results based on the simulated data and 800-dimensional plant gene expression data demonstrate that the suggested shrinkage methods produce favorable results compared with the other methods.This paper is organized in six sections as follows. In Section  2, we develop a Bayesian framework for VAR models with scale mixtures of multivariate normal distributions and with conjugate and non-conjugate prior distributions. The posterior mode estimates are computed iteratively by a weighted least squares method. In Section  3, we propose a score function of the shrinkage parameter, and a closed form for the selected shrinkage parameter is presented. A computational methodology for obtaining the shrinkage parameter value is described, which is called parameterized cross validation. In Section  4, consistency of the parameterized cross validation and multifold cross validation is stated explicitly. In Section  5, we present extensive experimental results based on simulated data and high-dimensional plant gene expression data to demonstrate the competitiveness of the suggested methods. Discussions are given in Section  6 with some concluding remarks.Letyt=(yt1,yt2,…,ytd)′,t=−p+1,−p+2,…,T, be ad-dimensional time series. We consider the VAR model(1)yt=Ψ′xt+ϵt,wherext=(1,yt−1′,…,yt−p′)′is a(dp+1)-vector of predictors,Ψis a coefficient matrix of size(dp+1)×d, andϵtis a noise vector. We assume thatϵtis independent and identically distributed from a scale mixture of multivariate normal distributions. That is,ϵtcan be represented as a product(2)ϵt=ztδt−1/2,wherezthas a multivariate normal distributionNd(0,V)andδthas some distribution on(0,∞). In the case of univariate distributions, scale mixtures of univariate normal distributions cover a wide class of continuous, unimodal, and symmetric distributions such as discrete mixtures of normals, Studentt-distributions, Laplace or double-exponential, and logistic distributions (Andrews and Mallows, 1974). In the case of multivariate distributions, some specific examples of scale mixtures of multivariate normal distributions are given as follows.Example 2.1Multivariate Studentt-distribution (Ni and Sun, 2005). Suppose thatϵt|δt,V∼Nd(0,δt−1V)andδt|ν∼Gamma(ν/2,ν/2),whereGamma(α,β)represents the gamma distribution with shapeαand rateβ. Then,ϵthas the multivariate Studentt-distributiontν(0,V)with density functionf(ϵt|V,ν)=Γ((ν+d)/2)(πν)d/2Γ(ν/2)|V|−12(1+1ν(ϵt)′V−1ϵt)−(ν+d)/2,whereΓ(⋅)represents the gamma function.Example 2.2Multivariate Laplace distribution (Eltoft et al., 2006). Suppose thatϵt|δt,V∼Nd(0,δt−1V)andδt|θ∼InvGamma(1,1/θ),whereInvGamma(α,β)represents the inverse gamma distribution with shapeαand scaleβ. Then,ϵthas the multivariate Laplace distribution denoted byML(θ,0,V)with density functionf(ϵt|V,θ)=2(2π)d/2θ|V|−12K(d/2)−1(2θ(ϵt)′V−1ϵt)(θ2(ϵt)′V−1ϵt)(d/2)−1,whereKm(⋅)represents the modified Bessel function of the second kind and orderm.We generalize the Bayesian framework proposed in West (1984) for univariate distributions to scale mixtures of multivariate normal distributions. Withϵt=yt−Ψ′xt, the vectorytis conditionally normally distributed as(3)yt|δt,Ψ,V∼Nd(Ψ′xt,δt−1V).For the prior ofΨ, we take the multivariate normal distribution as(4)ψ|V,λ∼Nd2p+d(0,(1−λ)(T−1)λV⊗Idp+1),0<λ<1,whereψ=vec(Ψ)=(ψ11,ψ21,…,ψdp+1,1,ψ12,…,ψdp+1,d)′is the vectorization ofΨ, and⊗is the Kronecker product. Note that the hyperparameterλ, which is also called the shrinkage parameter in this paper, controls the tightness of the prior distribution around0. For the prior ofV, we take the inverse Wishart distribution with scale matrixL0and degree of freedomm0as(5)V|L0,m0∼InvWishart(L0,m0),L0≻0,m0>d−1,which is conjugate to normal distributions, whereL0≻0implies thatL0is positive definite. Its density is given byπ(V|L0,m0)=|L0|m0/22dm0/2Γd(m0/2)|V|−m0+d+12exp(−12trace(L0V−1)),whereΓd(⋅)is the multivariate gamma function.In the rest of the paper, we will useπ, for simplicity, as a generic symbol for prior or posterior distributions as long as confusion is not likely. With the sampling model given in (3) and the priors in (4) and (5), the conditional posterior distributions ofΨandVare the multivariate normal and inverse Wishart distributions given by(6)ψ|Y,Δ,V,λ∼Nd2p+d(ψ̂(Δ,λ),V⊗K(Δ,λ)),V|Y,Δ,λ∼InvWishart(L(Δ,λ),m1),whereY=(y1,…,yT)′,X=(x1,…,xT)′,Δ=diag(δ1,…,δT),(7)K(Δ,λ)−1=X′ΔX+(T−1)λ1−λIdp+1,Ψ̂(Δ,λ)=K(Δ,λ)X′ΔY,L(Δ,λ)=L0+Y′ΔY−Y′ΔXΨ̂(Δ,λ),m1=m0+T,andψ̂(Δ,λ)is the vectorization ofΨ̂(Δ,λ). We note that the modes of the conditional posterior densities are given byΨ̂(Δ,λ)andV̂(Δ,λ)=L(Δ,λ)/(m1+d+1).The mode of the marginal posterior densityπ(Ψ,V|Y,λ)is computed iteratively as follows. Since the probability density forV−1/2ϵtis a mixture of spherical normal densities, we denote the probability density forϵtby(8)g(‖V−1/2ϵt‖2)|V|−12,wheregis a nonincreasing function. We define a nonnegative functionh(x)=−2ddxlogg(x),x>0.Assuming that the marginal posterior density is differentiable, we can get, fromπ(Ψ,V|Y,λ)∝π(Ψ|V,λ)π(V|L0,m0)|V|−T2∏t=1Tg(‖V−1/2(yt−Ψ′xt)‖2),that(9)ddψijlogπ(Ψ,V|Y,λ)=ddψijlogπ(Ψ|V,λ)+∑t=1Txti(yt−Ψ′xt)′V−1ejh(‖V−1/2(yt−Ψ′xt)‖2),whereej∈Rdis thejth standard basis vectorej=(0,…,1,…,0)′. Eq. (9) is equal to zero at the posterior mode. By using the prior distributionπ(Ψ|V,λ)in (4) and the conditional posterior modes in (6) and (7), we can derive that the marginal posterior modeΨ̂(λ)satisfies(10)Ψ̂(λ)=Ψ̂(Δ∗,λ),whereΔ∗=diag(δ1∗,…,δT∗)with(11)δt∗=h(‖V̂(λ)−1/2(yt−Ψ̂(λ)′xt)‖2),t=1,…,T,and(12)V̂(λ)=V̂(Δ∗,λ)=L(Δ∗,λ)/(m1+d+1).As for theδt∗in Eq. (11), we can see that theδt∗values are the conditional expectations ash(‖V−1/2(yt−Ψ′xt)‖2)=E[δt|Y,Ψ,V].This result can be derived by using that, forϵt=(ϵt1,…,ϵtd),−ddϵtjlogg(‖ϵt‖2)=h(‖ϵt‖2)ϵtj,j=1,…,d,and thatg(‖ϵt‖2)=∫0∞ϕd(ϵtδt1/2)δtd/2f(δt)dδt,whereϕd(⋅)andf(δt)represent the density functions of the standard multivariate normal distribution and the latent variableδt, respectively.Note that the form of the conditional posterior modeΨ̂(Δ,λ)in (7) does not involve the noise covariance matrixV. This is due to the use of the conjugate priors in (4) and (5).Alternatively, we can take the non-conjugate priors defined as the multivariate normal distribution(13)ψ|λ∼Nd2p+d(0,(1−λ)(T−1)λId2p+d),0<λ<1,and the inverse Wishart distribution in (5). Note that the prior distribution ofΨin (13) does not depend onV. In this case, the conditional posterior distributions ofΨandVare expressed as(14)ψ|Y,Δ,V,λ∼Nd2p+d(ψ̂NCJ(Δ,V,λ),KNCJ(Δ,V,λ)),π(V|Y,Δ,λ)∝|V|−m1+d+12|KNCJ(Δ,V,λ)|1/2exp(−12trace(LNCJ(Δ,V,λ)V−1)),whereKNCJ(Δ,V,λ)−1=V−1⊗X′ΔX+(T−1)λ1−λId2p+d,ψ̂NCJ(Δ,V,λ)=KNCJ(Δ,V,λ)vec(X′ΔYV−1),LNCJ(Δ,V,λ)=L0+Y′ΔY−Y′ΔXΨ̂NCJ(Δ,V,λ),m1=m0+T.The conditional posterior distribution ofVis not an inverse Wishart distribution. However, from the form of the posterior densityπ(V|Y,Δ,λ)in (14), we see that it can be approximated by the inverse Wishart distribution,InvWishart(LNCJ,m1), withLNCJ=LNCJ(Δ,V,λ). Then, the mode of the marginal posterior densityπ(Ψ,V|Y,λ)is computed iteratively by(15)Ψ̂NCJ(λ)=Ψ̂NCJ(Δ∗,V̂NCJ(λ),λ),V̂NCJ(λ)=LNCJ/(m1+d+1)=V̂NCJ(Δ∗,Ψ̂NCJ(λ),λ).The valuesΔ∗can be computed by (11) based on the estimates with the conjugate priors.In this paper, the marginal posterior modeΨ̂(λ)in (10) is called the shrinkage estimator ofΨwith the shrinkage parameterλ. Another marginal posterior modeΨ̂NCJ(λ)in (15) is also a shrinkage estimator ofΨ. Note that the computation ofΨ̂NCJ(λ)is made iteratively after the matricesΨ̂(λ),Δ∗, andV̂(λ)are computed in (10), (11), and (12), as a fine-tuning step.We can rewriteΨ̂(λ)in the formΨ̂(λ)=Sxx(λ)−1Sxy(λ),whereSxx(λ)=(1−λ)⋅Sxx+λIdp+1andSxy(λ)=(1−λ)⋅Sxy,withSxx=1T−1X′Δ∗X,Sxy=1T−1X′Δ∗Y.As a special case, if the noise vector is normally distributed, theng(x)∝exp(−x/2)andh(x)=1. In this case the diagonal matrixΔ∗becomes the identity matrix. IfΔ∗is the identity matrix andλ=0, thenΨ̂(λ)is equivalent to the ordinary least squares estimator. IfΔ∗is the identity matrix and the data matricesXandYare standardized matrices (i.e., with each column having the mean of zero and standard deviation of one), thenΨ̂(λ)has the same form as the NS estimator used in Opgen-Rhein and Strimmer (2007). In the case of multivariate Studentt-distributions in Example 2.1, we haveg(x)∝(1+x/ν)−(ν+d)/2andh(x)=(ν+d)/(ν+x). Sincex≥0, we haveh(x)≤1+d/ν, which implies thatδt∗≤1+d/ν. As noted in West (1984), a relatively large value ofδt∗can indicate existence of outliers in the data.We will propose a score function ofλfor selecting an “asymptotically optimal” value ofλ. Let(Ψ∗,V∗)denote the value of(Ψ,V)that minimizes the Kullback–Leibler (KL) divergence between the assumed true distributionf∗(Y)and the distributionf(Y|Ψ,V)from our probability model (1), i.e.,DKL(f∗;f)=∫f∗log(f∗/f). The score functionQto consider is defined byQ(λ;Y,Ψ∗)=−logf(Y,Ψ∗|λ),wheref(Y,Ψ|λ)is the joint density function derived from our probability models (3)–(5).The expected value of the considered score functionQis closely related to the mean squared error (MSE) asymptotically, where(16)MSE(Ψ̂(λ))=E(Ψ∗,V∗)[‖Ψ̂(λ)−Ψ∗‖F2]is the MSE ofΨ̂(λ)and‖A‖F2=∑i,jaij2is the squared Frobenius norm of a matrixA. The relationship between the proposed score function and the MSE can be explained via the following identity:(17)Q(λ;Y,Ψ∗)=−logf(Y|λ)−logπ(Ψ∗|Y,λ),wheref(Y|λ)is the marginal likelihood ofλandπ(Ψ∗|Y,λ)is the marginal posterior density atΨ=Ψ∗. Hence, the score function is a modification to the marginal likelihood by the marginal posterior term. The marginal posterior term can be considered as a difference between the marginal posterior distribution and its limiting distribution, measured by cross entropy as−logπ(Ψ∗|Y,λ)=−∫ξ(Ψ−Ψ∗)logπ(Ψ|Y,λ)dΨ, whereξ(Ψ−Ψ∗)is the Dirac-delta function and it represents the limit of posterior distributions (Gelman et al., 1995). We will discuss below the relationship between the MSE and each term of the score function in (17).The expected value of the marginal likelihood term may be regarded as a KL divergence as(18)E(Ψ∗,V∗)[−logf(Y|λ)]∝E(Ψ∗,V∗)[logf(Y|Ψ∗,V∗)f(Y|λ)]=DKL(f(⋅|Ψ∗,V∗);f(⋅|λ)).The marginal likelihood is the likelihood function averaged over the parameters with respect to the prior distribution asf(Y|λ)=∫f(Y|Ψ,V)π(Ψ,V|λ)d(Ψ,V)=Eλ[f(Y|Ψ,V)]. Suppose that(19)f(Y|λ)=Eλ[f(Y|Ψ,V)]=f(Y|Ψ˜,V˜)for someΨ˜andV˜. Then the expected value (18) can be understood as measuring the difference between the model with the fixed parameter(Ψ∗,V∗)and the model with the parameter(Ψ˜,V˜).As a special case, if(Ψ∗,V∗)is the true model parameter, the marginal likelihood is written as−logf(Y|λ)=−logf(Y|Ψ˜,V˜)∝−∑t=1Tlogg(‖V˜−1/2((Ψ∗−Ψ˜)′xt+ϵt)‖2),whereg(x)is the nonincreasing function defined in (8). In the case of multivariate normal distributions, we have−logg(x)∝x/2, which leads toE(Ψ∗,V∗)[−logf(Y|λ)]∝(T−1)E(Ψ∗,V∗)[‖(Ψ˜−Ψ∗)V˜−1/2‖Sxx2],where‖A‖S2=trace(A′SA)andSxx=(T−1)−1X′X.The parameterΨ˜in (19) approximates the posterior modeΨ̂(λ)because we can guarantee thatπ(Ψ˜,V˜|Y,λ)=f(Y|Ψ˜,V˜)π(Ψ˜,V˜|λ)f(Y|λ)=π(Ψ˜,V˜|λ)>0asT→∞by taking an appropriate prior density, whereas it is known thatπ(Ψ,V|Y,λ)→0asT→∞for fixed(Ψ,V)≠(Ψ̂(λ),V̂(λ)).From the conditional posterior densities in (6), we can infer that the marginal posterior densityπ(Ψ|Y,λ)is a mixture of multivariate normal distributions with the modeΨ̂(λ). Assuming that the posterior distribution is asymptotically normal, (e.g., see Kim, 1998), we have that asymptoticallyE(Ψ∗,V∗)[−logπ(Ψ∗|Y,λ)]∝E(Ψ∗,V∗)[‖ψˆ(λ)−ψ∗‖G(ψˆ(λ))2],whereG(ψˆ(λ))is the inverse covariance matrix defined byG(ψˆ(λ))=−d2dψdψ′logπ(Ψ|Y,λ)|ψ=ψˆ(λ).The selected valueλ∗ofλis obtained by minimizing an upper bound of the expected value ofQwith respect toλas given below.Theorem 3.1Suppose that the sampling model is given in   (1) and (2), and the priors are given in   (4) and (5). Letδ1be a random variable described in   (2)   having a distribution with finite expectationsE[δ1]andE[δ1−1]. Then an upper bound ofE(Ψ∗,V∗)[Q(λ;Y,Ψ∗)]is minimized whenλ=λ∗, where(20)λ∗=d2p+dη∗(T−1)+d2p+dwithη∗=d(m0+T)‖Ψ∗‖F2trace(L0)+TE[δ1]E[δ1−1]trace(V∗).ProofSee Appendix A.□IfTis large enough, then the hyperparametersL0andm0may be ignorable andη∗is approximated by(21)η∗≈d‖Ψ∗‖2E[δ1]E[δ1−1]trace(V∗).If the entries ofΨ∗are bounded by a constantM, i.e.,|ψij∗|≤M, then we have‖Ψ∗‖2≤M2(d2p+d).From Theorem 3.1, we can say that (i)λ∗gets smaller asTincreases, (ii)λ∗gets closer to 1 asdincreases if the number of nonzero coefficients and trace(V∗) are of orderd, and (iii)λ∗increases asd−1trace(V∗)gets larger. A larger value ofd−1trace(V∗)implies that the data are contaminated with a larger amount of noise, so that the correspondingly largerλ∗may be used to reduce the noise effect.Moreover, from Jensen’s inequality, we get a lower bound of the productE[δ1]E[δ1−1]≥E[δ1]E[δ1]−1=1,where equality holds ifδ1is a constant, that is, the noise vectorϵthas a multivariate normal distribution. Hence, we can say thatλ∗is smaller if the noise is more normally distributed. Conversely, ifδ1is distributed to take more dispersed values so that the distribution of the noise vectorϵtis more heavy-tailed, thenλ∗is larger. For example, ifδ1has a gamma distribution withα=ν/2andβ=ν/2as in Example 2.1, thenE[δ1]=1andE[δ1−1]=ν/(ν−2), so the product isE[δ1]E[δ1−1]=ν/(ν−2). We can see thatE[δ1]E[δ1−1]decreases to 1 asν→∞, whereasE[δ1]E[δ1−1]increases to∞asν→2.To sum up, the interpretation of the selected “asymptotically optimal” value ofλgives a more refined insight into the role of the shrinkage parameter in addition to that given in Schäfer and Strimmer (2005).Although the selected valueλ∗ofλin (20) has a simple analytic expression, the valueη∗therein is yet a function of unknowns. We will outline a computational methodology for finding the valueη∗based on data.Conventional cross validation (CV) methods, including theK-fold CV method, have been used in regression and time series analysis (Seber and Lee, 2003; Tsay, 2005) among others. However, the CV methods evaluate performances of the estimators which are computed only on a part of the data called the training set, and it leads to a biased evaluation result (Hastie et al., 2001). As a remedy for this bias, we will use a variation of theK-fold CV, which we will call the parameterized CV (PCV), motivated by Koo et al. (2008).Assuming thatTis relatively large, we can see in (21) thatη∗is approximated by a quantity that depends only on the parametersΨ∗,V∗,E[δ1], andE[δ1−1]. Therefore we apply theK-fold CV for estimatingη∗rather than directly estimatingλ∗. In aK-fold CV, we randomly partition the set of data pairs(xt,yt),t=1,…,T, intoKsets of nearly equal sizes. Then we select thekth set among theKsets as the validation data and use the remainingK−1sets as the training data. Letvkdenote the validation data andukdenote the training data. We select a valueλˆCVat the minimum of mean squared prediction errorPEK(λ)for the validation data asλˆCV=argminλPEK(λ)where(22)PEK(λ)=1T∑k=1K∑t∈vk‖yt−Ψ̂uk(λ)′xt‖2andΨ̂uk(λ)is the estimator computed only on the training datauk. Note that the sizeTk=|uk|=T−|vk|is smaller thanT, and we haveT1≈⋯≈TK. Based on the result (20), an estimateηˆofη∗is computed according toλˆCV=(d2p+d)/(ηˆ(T1−1)+d2p+d), which is equivalent to(23)ηˆ=(d2p+d)(1−λˆCV)(T1−1)λˆCV.Then the estimateλˆBSis determined by(24)λˆBS=d2p+dηˆ(T−1)+d2p+d.The suggested PCV method is expected to alleviate the bias of theK-fold CV method (Hastie et al., 2001). Moreover, by using the mean squared prediction error given in (22), we can estimate other hyperparameters such as the degree of freedomνof multivariatet-distributions. Therefore, the PCV is an alternative to the maximization of marginal likelihood for selectingλand also MCMC methods for selectingν.In this section, we briefly introduce a shrinkage estimation technique considered in Opgen-Rhein and Strimmer (2007), and develop a modified version in which cross correlations between variables are incorporated in the estimation.Assuming that each componentytjof the time series are weakly stationary, i.e., the mean and the variance are time-invariant, we divide it by its sample standard deviation as preprocessing. Then, shrinkage estimators ofΨbecome scale-invariant (Litterman, 1986). We consider estimating the variancesσj2ofytjin order to re-scale the estimated coefficient matrix, that is, the(i,j)th entry of the estimated coefficient matrixΨ̂(λ)is re-scaled asψ˜ij(λ,γ)=ψ̂ij(λ)σˆj(γ)σˆ((i−2)modd)+1(γ),i=2,3,…,dp+1,andψ˜1j(λ,γ)=ψ̂1j(λ)σˆj(γ),j=1,…,d, whereσˆj2(γ)are the estimated variances as given in (25).Letsj2denote the sample variancesj2=(T−1)−1∑t=1T(ytj−ȳj)2. The shrinkage estimatorσˆj2(γ)for a given shrinkage parameterγis obtained by shrinking the sample variance toward the median of all the sample variances as(25)σˆj2(γ)=(1−γ)sj2+γsmed2,wheresmed2is the median ofsj2,j=1,…,d, and0≤γ≤1. In Opgen-Rhein and Strimmer (2007), an optimal valueγˆofγis obtained by minimizing the mean squared error of the estimatorσˆj2(γ)with respect toγ(Schäfer and Strimmer, 2005), which is given in a closed form as(26)γˆ=∑j=1dVar̂(sj2)∑j=1d(sj2−smed2)2,whereVar̂(sj2)is an estimated variance ofsj2. We suggest a new version ofVar̂(sj2)in (26) by modifying the method of Opgen-Rhein and Strimmer (2007) in consideration of the serial dependence in the data as follows. Letwtj=(ytj−ȳj)2andw̄j=T−1∑t=1Twtj. Then, sincesj2=(T−1)−1∑t=1Twtj, we have(27)Var̂(sj2)=1(T−1)2∑t=1T∑τ=1TCov̂(wtj,wτj).Schäfer and Strimmer (2005) assumed thatCov(wtj,wτj)=0fort≠τin (27). However, regarding{wtj}as a covariance-stationary time series, we suggest to use in expression (27) the estimatesCov̂(wtj,wτj)given by(28)Cov̂(wtj,wt+k,j)=1T∑τ=1T−|k|(wτj−w̄j)(wτ+|k|,j−w̄j)for|k|≤T−1. Expression (28) is used in the literature for time series analysis (Hamilton, 1994; Wei, 2005). Wei (2005) showed that the estimatorCov̂(wtj,wt+k,j)in (28) is preferred for small sample sizes compared with the estimatorT(T−k)−1Cov̂(wtj,wt+k,j), in the context of the bias and variance of an estimator.For convenience’ sake, we will denote ‘convergence in probability’ by⟶p. In this section, we will show consistency of the proposed shrinkage estimators with the shrinkage parametersλˆCV(22) andλˆBS(24), i.e.,Ψ̂(λˆCV)⟶pΨ∗,Ψ̂(λˆBS)⟶pΨ∗,whereΨ∗is a true coefficient matrix. LetΔ=diag(δ1,…,δT)denote the random variables satisfying the sampling model (2), i.e.,ϵt=yt−(Ψ∗)′xt=ztδt−1/2. LetΔ∗=diag(δ1∗,…,δT∗)andΔuk∗denote the values ofΔsuch thatΨ̂(λˆBS)=Ψ̂(Δ∗,λˆBS)andΨ̂uk(λˆBS)=Ψ̂uk(Δuk∗,λˆBS)are the modes of the marginal posterior densities.We make the following assumptions.(A1)In theK-fold CV procedure withK≥2, the sample is randomly partitioned into sets of almost equal sizes, i.e.,{1,2,…,T}=v1∪v2∪⋯∪vKwith||vk1|−|vk2||≤1for anyk1,k2=1,…,K.E[δt],E[δt−1],E[δt1/2],E[yt], andE[xtxt′]exist and invariant over time.(a)(T−1)−1X′Δ∗X⟶pΣxxand(T−1)−1X′Δ∗Y⟶pΣxy, whereX=(x1,…,xT)′,Y=(y1,…,yT)′,Σxx=E[xtδtxt′], andΣxy=E[xtδtyt′].(Tk−1)−1Xuk′Δuk∗Xuk⟶pΣxxand(Tk−1)−1Xuk′Δuk∗Yuk⟶pΣxyfork=1,…,K, whereuk={1,2,…,T}−vk={t1,…,tTk},Xuk=(xt1,…,xtTk)′, andYuk=(yt1,…,ytTk)′.E[xtxt′]is strictly positive definite.mink|vk|→∞asT→∞, andζmin(|vk|−1Xvk′Xvk)>mfor allT>T0for someT0,m>0, whereζmin(S)is the minimum of the eigenvalues of a matrixS.Assumption (A2) implies that the time seriesytis weakly stationary. From Assumption (A2), we can derive that the matricesΣxx=E[xtδtxt′]=E[δt]E[xtxt′]andΣxy=E[xtδtyt′]=E[δt]E[xtxt′]Ψ∗+E[δt1/2]E[xt]E[zt′]=ΣxxΨ∗exist and they are time-invariant. Assumption (A3) states that the weighted sample cross covariance matrices converge to the weighted cross covariance matrices in probability. In general, the convergence of the sample cross covariance matrices can be proven by assuming that the time series is a stationary VAR process (Fuller, 1996). From Assumptions (A2) and (A4), we can show that the matrixΣxx=E[δt]E[xtxt′]is strictly positive definite. Assumption (A5) requires that the size of validation data inK-fold CV increases and the sample covariance matrix is nonsingular.Note that the numberKinK-fold CV is not assumed to be fixed. But Assumption (A3)(b) implies that the training set sizeTkmust increase in order for the sample covariance matrices to converge. Moreover, Assumption (A5) requires the size of validation sets|vk|to increase. The size of validation sets does not have to increase dominatingly fast as in Shao (1993), but it cannot be as small as one, i.e., as in the case of leave-one-out cross validation (Hastie et al., 2001).Theorem 4.1 shows the consistency ofΨ̂(λˆCV).Theorem 4.1Consistency ofK-Fold Cross ValidationUnder Assumptions  (A2)–(A5), it holds thatΨ̂(λˆCV)⟶pΨ∗.ProofSee Appendix B.□Corollary 4.2 shows that the consistency ofΨ̂(λˆBS)can be obtained based on the consistency ofΨ̂(λˆCV).Corollary 4.2Consistency of Parameterized Cross ValidationUnder Assumptions  (A1)–(A5), ifΨ̂(λˆCV)⟶pΨ∗, thenΨ̂(λˆBS)⟶pΨ∗.ProofSee Appendix C.□

@&#CONCLUSIONS@&#
