@&#MAIN-TITLE@&#
Latent semantics in language models

@&#HIGHLIGHTS@&#
The unsupervised techniques of language modelling are investigated.We use global semantics, local semantics, and morphology information in our models.We experiment with modelling of six different languages.Final models dramatically reduce the perplexities and improve machine translation.

@&#KEYPHRASES@&#
Language models,Latent Dirichlet allocation,Semantic spaces,Stemming,HAL,COALS,Random indexing,HPS,LDA,Machine translation,Moses,

@&#ABSTRACT@&#
This paper investigates three different sources of information and their integration into language modelling. Global semantics is modelled by Latent Dirichlet allocation and brings long range dependencies into language models. Word clusters given by semantic spaces enrich these language models with short range semantics. Finally, our own stemming algorithm is used to further enhance the performance of language modelling for inflectional languages.Our research shows that these three sources of information enrich each other and their combination dramatically improves language modelling. All investigated models are acquired in a fully unsupervised manner.We show the efficiency of our methods for several languages such as Czech, Slovenian, Slovak, Polish, Hungarian, and English, proving their multilingualism. The perplexity tests are accompanied by machine translation tests that prove the ability of the proposed models to improve the performance of a real-world application.

@&#INTRODUCTION@&#
Language modelling is an essential part in many tasks of natural language processing (NLP). Speech recognition, machine translation, optical character recognition, and many other disciplines strongly depend on the language model and thus every improvement in language modelling can also improve the performance of the whole system.In this paper we explore fully unsupervised methods for language modelling (which require no labelled data and no information about language itself). To prove their multilingualism we experiment with several languages including highly inflectional as well as low-inflection languages. We incorporate three different families of languages (Slavic, Uralic, and Germanic) into our experiments. As representatives of Slavic languages we experiment with Czech, Slovenian, Slovak, and Polish. Uralic languages are represented by Hungarian, and Germanic languages by English. All languages we investigate in this paper except English are characterized by a high level of inflection and relatively free word order (from the syntactic point of view, the words in a sentence can usually be reordered in several ways to carry a slightly different meaning). Properties of these languages complicate the language modelling task. The great number of word forms and large number of possible word sequences lead to a much higher number of n-grams. Data sparsity is a common problem of language models, but for highly inflected languages this problem is even more evident.The highly inflected languages in this paper belong rather among non-mainstream languages, for which the language modelling task has not gained as much attention as it has for English, for example. We thus believe there is considerable potential for improvements. However we provide experiments also for English to compare our methods with the state of the art.In this paper we extend our work on the application of semantic spaces in language modelling (Brychcín and Konopík, 2014), where we have achieved significant improvements in perplexity and in machine translation task especially with HAL, COALS and RI models. Thus, these models are investigated more deeply in this paper.We attempt to improve language modelling by adding long-range semantic dependencies. We choose latent Dirichlet allocation (LDA) (Blei et al., 2003) for that task, because it has already been shown by many researchers that LDA improves language modelling (see, e.g., Tam and Schultz, 2005, 2006; Watanabe et al., 2011).The performance of these language models is further enhanced by our unsupervised stemming algorithm called high precision stemmer (HPS)11A description of the algorithm and its implementation is available at http://liks.fav.zcu.cz/HPS.introduced in Brychcín and Konopík (2015). We have already tested our stemmer in language modelling tasks and the results indicate that HPS performs best compared to other unsupervised stemmers.To the best of our knowledge we are first to try to combine these three sources of information (i.e. local semantics, global semantics, and morphology).In the context of this article, we work with various methods for modelling semantic relations between words, and use them to improve our language models. The backbone principle of methods for discovering hidden meaning from a plain text is the formulation of distributional hypothesis in Firth (1957) that says “a word is characterized by the company it keeps”. The direct implication of this hypothesis is that the word meaning is related to the context where it usually occurs and thus it is possible to compare the meanings of two words by statistical comparisons of their contexts. This implication was confirmed by empirical tests carried out on human groups in Charles (2000).Several authors have made huge efforts to give an overview of the current state of the art in computational methods for extracting meaning from text (Turney and Pantel, 2010; Riordan and Jones, 2011; McNamara, 2011).All the methods for extracting meaning can be approximately summarized in two categories. Authors Riordan and Jones (2011) and McNamara (2011) categorize these methods into context-word and context-region approaches. In this paper we use the notation local context and global context, respectively, because we think this notation describes the principle of meaning extraction better. These two categories are briefly described in the following Sections 2.1 and 2.2. Additionally, to give a better idea of how these two approaches differ, Fig. 1shows an example of global context and local context semantics of words.Models based upon the distributional hypothesis usually represent the meaning as a point in a multi-dimensional space. Thus, one meaning is represented as a single vector. These models are then referred to as the vector-space models (VSM). The vector representation enables an easy comparison of word meanings by computing distances between the vectors.Global semantics models assume that words that are close in meaning will occur in similar pieces of text (documents). These methods are usually based on the bag-of-words hypothesis, which assumes the word order has no meaning.Latent semantic analysis (LSA) (Deerwester et al., 1990) is a model for discovering global semantic relationships between words. A term-document matrix is constructed and then the singular value decomposition (SVD) is used to reduce the dimension of the matrix and to smooth the values of the matrix.In Hofmann (1999) the probabilistic latent semantic analysis (PLSA) model was introduced, which is in fact the Bayesian version of LSA. PLSA assumes that the document is a mixture of topics and a topic is simultaneously a mixture of words. The probabilistic output makes this model more easily applicable in many tasks.PLSA was later extended to LDA (Blei et al., 2003), which places the Dirichlet prior to the document-topic distribution as well as the topic-word distribution. LDA is thus the proper model even for unseen documents that has often been criticized in the case of PLSA.The motivation for using such methods in language modelling is the fact that text can continuously change in domain, topics, writing style, and so on and it is not possible to recognize these changes only from a short history of words (let us say three words if we use four-gram language models). A much larger history is needed to observe these changes, where of course the data sparsity problem is much more evident. Inhibition of word order thus leads to far fewer possible combinations of histories.LDA is used for boosting the probabilities of words that are likely to co-occur in the same document (as will be described in Section 4.3). This is independent of the position of words in the document (the document is a bag of words). These word probabilities only depend on the document itself (global context). This brings long-range dependencies, and language models are thus adapted to the current domain of text.The second approach to modelling the semantics of words is to use only their local context. Local semantic models assume that the meaning of a word is related to the short context around the word. Methods based on this assumption usually use a small context window (let us say four words in both directions). These methods do not require text that is naturally divided into documents, which can be found advantageous compared to the methods mentioned in the previous section (LSA, PLSA, LDA).Because of the short context, these methods can take word order into account, so the methods model semantic as well as syntactic relations between words. There are a lot of methods for deriving word meaning from the local context. We have already experimented with five of them in Brychcín and Konopík (2014). In this paper we continue our research and use only the three best performing methods in language modelling (HAL, COALS, and RI).Hyperspace analogue to language (HAL) (Lund and Burgess, 1996) is a very simple method for building semantic space. HAL goes through the corpus and records the co-occurring words around the target word (in some small context window – typically four words in both directions). HAL distinguishes between left and right context of the target word and records them separately. Co-occurring words are weighted inversely to the distance from the target word. This results in the co-occurrence matrixM=|W|×2|W|, where |W| is the vocabulary size. The word vector dimension is 2|W| because of distinction between left and right context.Correlated occurrence analogue to lexical semantics (COALS) (Rohde et al., 2004) is an extension of the HAL model. It starts almost identically to HAL, but it does not distinguish between left and right context. After recording the co-occurrence information, the raw counts of the matrixMare converted into the Pearson's correlations. Negative values are zeroed and other values are replaced by their square roots. The optional final step, inspired by LSA (Deerwester et al., 1990), is to apply the SVD reduction to the matrixM, resulting in the smoothing of values and also the discovery of latent semantic relationships between words.Random indexing (RI) (Sahlgren, 2005) uses a completely different approach to recording co-occurrence statistics compared to HAL and COALS. For each word in the vocabulary, RI starts by creating high-dimensional index vectors randomly filled with few 1s and −1s (Sahlgren et al. (2008) recommend to fill index vectors with two 1s and two −1s). The dimension is typically of the order of thousands. Such vectors are very sparse and thus unlikely to overlap. The index vectors are assumed to be nearly orthogonal. The algorithm then iterates over the corpus and for each target word it sums all the co-occurring words’ index vectors. These sums are recorded in the matrixM. Even though, RI performs a little worse than the other two methods in language modelling, we use it again because it is computationally very undemanding.In Brychcín and Konopík (2014) we also tested BEAGLE (Jones and Mewhort, 2007) and P&P (Purandare and Pedersen, 2004) models, but these models did not perform as well in language modelling as the above mentioned methods.There are several interesting methods which we have not investigated in language modelling yet, but which are certainly worth mentioning.Similar to Purandare and Pedersen (2004), the authors of Reisinger and Mooney (2010) and Reisinger and Mooney (2010) address the common problem of VSMs, where each word is represented with a single vector, which clearly fails to capture homonymy and polysemy. In their approach, contexts for a single word are clustered to create several meanings of the word. Similar studies, where the word meaning is disambiguated according to the context, can be found in Dinu and Lapata (2010), Erk and Padó (2010) and Huang et al. (2012).Recently, the neural-network models for learning word representations get attention from many researchers. Artificial neural networks hold an implicit ability to store all seen data (words) in their weights. In theory, it should be enough to infer word meanings. However, many researcher specifically design network architectures to support inference of semantic information. For example, recurrent neural networks can use a memory to see sequences and the context.In Mikolov et al. (2013), authors introduce skip-gram and continuous bag-of-words (CBOW) models based on a simple single-layer architecture. They proved that even such a simple neural-network architecture can bring promising results. Huang et al. (2012) introduced a model based on neural network, which uses both local and global context via a joint training objective for modelling semantics of the words. They outperform models that use only local context on the word similarity tasks.In other papers, words are usually regarded to as an independent entities without any relationship between morphologically related word forms. Luong et al. (2013) came with an idea to represent words as a composition of morphemes using the recursive neural network (RNN). The word semantics is than learned by neural language models (NLM).Over the past few years, great attention has been concentrated on the exploration of semantic information in language modelling. Further, discovering latent semantic relations between words is more interesting because there are many methods that work in an unsupervised manner. These methods are usually based on assumptions introduced in the previous section (i.e. the distributional hypothesis and the bag-of-words hypothesis). In the context of this article we distinguish three directions of improvements in language modelling (using global context semantics: Section 3.1; using local context semantics: Section 3.2; and using morphological information: Section 3.3).The use of global semantics in language modelling is motivated by the assumption that documents (long contexts) may differ in domains, topics, and writing styles. This also means that they have a different probability distribution of words. This assumption is used for adapting language models to the long context (domain, topic, style of particular documents). A method such as LSA, PLSA, or LDA is used to find out which documents (which global contexts) are similar and which are not. This long-context information is added to standard n-gram models to adapt them to the global context. The group of language models that benefits from this idea is sometimes called topic-based language models.An important study on the application of LSA to language modelling was presented in Bellegarda (2000). Significant reductions in perplexity (down to 33%) and improvements in speech recognition of English [word error rate (WER) was decreased by 16%] were achieved in this paper. Several authors later obtained good results with PLSA (Gildea and Hofmann, 1999; Wang et al., 2003) and LDA (Tam and Schultz, 2005, 2006) approaches.Topic tracing language models (TTLM) are investigated in Watanabe et al. (2011). These models are based on LDA and PLSA and integrate the ability to dynamically track changes in topics. The tracking is based upon focused text information and previously estimated topics. This research proves that TTLM significantly improves speech recognition of English.The language models based on semantic composition are described in Mitchell and Lapata (2009). Word vectors are constructed from LDA (word distribution over topics) or SSM (simple semantic space based on word co-occurrence statistics). Different vector compositions are investigated to represent the history of upcoming words in the language model. Their composition models reduce the perplexity of English corpora when combined with a baseline.The second direction is to use local context semantics for language modelling, where usually class-based language models or similar architectures are used. Individual words are clustered into much smaller number of classes, which reduces the data sparsity problem that the language models try to tackle.One of the pilot studies in unsupervised language modelling methods was Brown et al. (1992), where class-based language models of English were introduced. The clustering algorithm builds clusters in a way that will minimize the entropy of the bigram class-based language model. This problem was reformulated so as to maximize the average mutual information between word clusters in whole training corpora [maximum mutual information (MMI) clustering]. To achieve this, classes keep the words that are most probable in the given context (one word window in both directions), which is essentially similar to the distributional hypothesis on which the methods investigated in this paper are based (words occurring in similar contexts are likely to have similar meanings). The MMI algorithm gives very satisfactory results but its computational complexity is very problematic.This approach was later extended by Martin et al. (1998) in order to improve the complexity and to work with trigrams (not only bigrams as in Brown's MMI clustering). This clustering algorithm was also used to create class-based language models of Russian and English in Whittaker and Woodland (2003). Linear interpolation with a baseline model improved the perplexity of Russian by 23% and that of English by 7.9%. The authors also present a 2.2% relative reduction of WER in speech recognition of English.In Deschacht et al. (2012), the Latent words language model (LWLM) was introduced. This model uses a very similar idea to the methods in Brown et al. (1992) and Martin et al. (1998), but the solutions differ. LWLM represents the word clusters as a latent variable in a graphical model and these clusters consist of the words that are most probable in the given context window. Gibbs sampling or the expectation maximization (EM) algorithm is used for inference. LWLM improved the perplexity of language modelling by 14–18% on three English corpora. LWLM groups words according to their local contexts and thus models the semantic relationships between words. LWLM provides an efficient framework that is able to work with a wider context than the MMI approach and with lower complexity.In Brychcín and Konopík (2014) we were the first to apply semantic spaces (working with a small context window) to language models. Our clustering method based on semantic spaces build clusters of words that are similarly distributed in the corpus. We experimented with modelling of Czech, Slovak, and English and achieved perplexity reductions ranging between 10 and 18%. These language models were also able to significantly improve the BLEU score in machine translation tests.Neural networks are becoming more attractive for language modelling in recent years (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010). They reach the state-of-the-art performance and successfully compete with n-grams. Neural networks can be designed to capture words contextual meaning which enables them to estimate similarity between words. The word sequences that have never been seen before receive high probability if they are made of words that are semantically similar to words forming an already seen sentence. Given virtually unlimited word combinations such an ability can dramatically increase model performance.Some researchers also experiment with enrichment of neural networks with an external source of information. For example in Mikolov and Zweig (2012), an approximation of LDA topics is fed into the network input alongside with words to add global semantic information.A third important direction for improving language modelling is to use morphological information about language. Many authors have already proved that this kind of information can be very useful for the modelling of inflectional languages, which, as stated above, are important for this paper. These approaches usually use supervised methods (lemmatization, part-of-speech tagging, etc.), but unsupervised methods of stemming also exist.In Oikonomidis and Digalakis (2003), the authors used stems for language modelling of Greek. Class-based language models and maximum entropy language models were investigated. The authors present small but significant improvements in the WER of speech recognition.Another approach to the modelling of Arabic was investigated in Kirchhoff et al. (2006). Several different approaches to incorporating morphological information (stems, roots, affixes, morphemes) into language models were tested, with a special focus on factored language models (FLMs) as an architecture. These models successfully improved the performance of speech recognition of Arabic.Language modelling and speech recognition of Turkish using stems, endings, and morphemes was also investigated in Arsoy et al. (2006). The authors present significant improvements in WER by application of their combined model, which uses information about the morphology of Turkish.In Oparin (2008), experiments with morphological random forests (using information about stems, lemmas, parts-of-speech, etc.) in the Czech and Russian languages were shown, with the conclusion that they can be used effectively for inflectional languages.In Brychcín and Konopík (2011) we studied language modelling of Czech and Slovak. We used lemmatization and part-of-speech tagging to derive word clusters for class-based language models and achieved perplexity improvements ranging between 10 and 30% for both languages depending on the amount of training data.We outlined three main directions (i.e. local semantics, global semantics, and morphology) in language modelling on which researchers often focus their attention. To put our research into the context of the state-of-the-art we can state that our method is based on all these sources of information. As will be shown later, these sources of information enrich each other; moreover, their combination dramatically improves language modelling.The rest of the article is organized as follows. Section 4 describes how the latent semantics is discovered in unlabelled corpora and how it is incorporated into the language models. Our experiments are described in Section 5, which is followed by discussion of the results and the conclusion.This section describes our language models and how these models are acquired from an unannotated corpus. The baseline is introduced in the following Section 4.1. The next sections present various sources of information, such as the morphology (Section 4.2), global semantics (Section 4.3), and local semantics (Section 4.4), which are used to improve language modelling. Finally, Section 4.5 describes how these sources of information are combined together with the baseline.We are using the modified Kneser–Ney interpolation (introduced in Chen and Goodman, 1998), which is the state-of-the-art approach for smoothing methods. The formula for smoothing of word probabilities is(1)P(wi|wi−n+1i−1)=cnt(wi−n+1i)−D(cnt(wi−n+1i))∑wicnt(wi−n+1i)+γ(wi−n+1i−1)P(wi|wi−n+2i−1),where P() is the probability given by the modified Kneser–Ney interpolation model and cnt() is the count of the n-gram. The goal of discounting function D(cnt) is to save some probability mass for lower-order models. The normalization functionγ(wi−n+1i)∈(0,1)makes the probability distribution sum up to 1. The definitions and derivations of these functions can be found in the original paper.The main advantage of the modified Kneser–Ney smoothing is the clever way in which it calculates the unigram probability distribution(2)P(wi)=N1+(•wi)N1+(••),where symbol • means an arbitrary word (class) andNr(wi−n+1i)is the number of n-grams with frequency r (i.e. the number of such n-grams, wherecnt(wi−n+1i)=r). In other words, the unigram probability ofwiis given by the number of different bigrams ending inwidivided by the total number of different bigrams.We use HPS (see Brychcín and Konopík, 2015) as a stemming algorithm. HPS is a new unsupervised stemming algorithm that uses both lexical and semantic information about words to decide how to stem a particular word. The idea of HPS is to split the stemming into two stages. The first stage is based upon a clustering where stemming candidates are selected. The second stage uses a maximum entropy classifier with stemming-specific features that is trained on these candidates.In our case, stemming is a mapping functionmS:w→sthat maps wordsw∈Wto stems s∈S. Note that stemming is contextually independent (in any context the same word always receives the same stem).We suppose the stemming should be very helpful for languages with rich morphology. We use three ways of incorporating stemming information into the language modelling. The first way is to use class-based language models with stems representing classes, and the other two ways are used to improve global semantic (Section 4.3) and local semantic language models (Section 4.4).Class-based language models are the state-of-the-art approaches to language modelling. The main task of the approach is to replace the statistical dependencies between words with dependencies among a much lower number of word classes, thus reducing the data sparsity problem. In our case the class consists of all words with the same stem.The probability estimation of a wordwiconditioned by its historywi−n+1i−1(where n is the length of the n-gram) is given by the following formula(3)PS(wi|wi−n+1i−1)=P(wi|si)P(si|si−n+1i−1).The probabilityP(si|si−n+1i−1)is calculated in the same way as in formula (1), but words are replaced with stems. To calculate the probabilityP(wi|si), we use Good-Touring smoothing.For modelling global context properties we use the well-known topic model LDA (Blei et al., 2003) and our extension of LDA enriched with stem information: stem-based LDA (S-LDA).LDA represents documents as a mixture of topics where each topic is simultaneously a mixture of words. Assume we have a set of documentsD={D1,D2 …,DM} each containing a sequence of words. Letwidenote a word at position i in a corpus, diis a document index to which this word belongs and ziis a hidden topic label for this word that we try to discover. The graphical model representation is depicted in Fig. 2(a). The generative process of a word corpus in LDA is as follows:1For each documentDm∈D, sample a distributionθm∼ Dirichlet(α) over topics 1≤zi≤K, whereαis a vector of hyper-parameters of Dirichlet distribution.For each topic, sample the word distributionϕk∼ Dirichlet(β) over words1≤wi≤|W|, whereβis a vector of hyper-parameters of Dirichlet distribution.For each position i in a corpus:(a)Sample a topic label zifrom the distributionθdi.Sample the wordwifrom the distributionϕzi.For inference of topic assignments, we use Gibbs sampling, which needs to computeP(zi=k|z¬i,w,α,β), the probability of topic assignment at position i in the corpus given all other topic assignments for all words. According to Griffiths and Steyvers (2004) this leads to a simple formula(4)P(zi=k|z¬i,w,α,β)∝cnt¬i,k(wi)+βwi∑j=1|W|[cnt¬i,k(j)+βj]·cnt¬i,k(di)+αk∑l=1K[cnt¬i,l(di)+αl],wherecnt¬i,k(wi)is the number of times the topic k has been assigned to a wordwi, except the position i in the corpus. Thecnt¬i,k(di)denotes the count of how many times the topic k occurs in the document diagain except for the position i in the corpus.From the topic assignments we can easily obtain estimates ofθmandϕk:(5)P(wi=j|zi=k,β)=ϕk(j)≈cntk(j)+βj∑j=1|W|[cntk(j)+βj](6)P(zi=k|di,α)=θk(di)≈cntk(di)+αk∑l=1K[cntl(di)+αl].Finally, to derive the probability of a wordwiin the context of the whole document (global context) we need to marginalize out the topic variable(7)PLDA(wi|di)=∑k=1KP(wi|zi=k)P(zi|di)=∑k=1Kϕk(wi)θk(di).In the second part of this section we describe our extension of LDA called S-LDA (shown in Fig. 2(b)). The generative process of S-LDA starts in the same way as the LDA does. Firstly, the topics ziare generated. For each zithe stem siis sampled from the Dirichlet distribution and finally the word formwiis selected.In many languages, especially those with rich morphology, the suffixes contain morpho-syntactic information of the word. In topic models such as LDA based on the bag-of-words approach, the word order has no meaning and the syntactic information is inhibited. We assume the base forms of the words (approximated by stems) contain a satisfactory amount of information to infer topics. Moreover, taking these properties into account, we suppose that this model will deal with the data sparsity problem better.Because the variables siandwiare both observed in a corpus andP(wi|si)is constant during sampling zi, the inference process is almost the same as for LDA (in formulas (4) and (5), the variablesw,wi, and W are only replaced withs, si, and S, respectively, wheresdenote stems on all positions and S is a set of different stems).Finally, the unigram probability according to S-LDA is given by(8)PS−LDA(wi|di)=P(wi|si)∑k=1KP(si|zi=k)P(zi|di)=P(wi|si)∑k=1Kϕk(si)θk(di),whereP(wi|si)is calculated in the same way as in Section 4.2.According to our previous research (Brychcín and Konopík, 2014), the well-performing semantic spaces in language modelling were discovered to be:•Hyperspace analogue to language (HAL) (Lund and Burgess, 1996).Correlated occurrence analogue to lexical semantic (COALS) (Rohde et al., 2004).Random indexing (RI) (Sahlgren, 2005).Since words in these semantic spaces are represented as real-valued vectors, we can apply clustering methods. The main assumption is that words within the same cluster should be semantically substitutable (i.e. they make sense at the same position in the appropriate context).The selection of a suitable clustering algorithm is crucial for such a task. According to the study in Zhao and Karypis (2002), we selected the repeated bisection algorithm because of its efficiency and acceptable computational requirements. We use the implementation from the CLUTO software package (Karypis, 2003). As a measure of the similarity between two words, we use the cosine similarity of word vectors, calculated as the cosine of the angle between corresponding vectors.Since we already have word clusters, we can easily define the mapping functionm:w→c, wherew∈Wdenotes a word and c∈C denotes a word cluster. Class-based language models seem to be a suitable architecture for applying this kind of information to the language models.(9)P(wi|ci−n+1i−1)=P(wi|ci)P(ci|ci−n+1i−1).The class (cluster) at position i is given byci=m(wi). The probabilityP(ci|ci−n+1i−1)is calculated in the same way as in formula (1), but words are replaced with word classes. To calculate the probabilityP(wi|ci)we use Good-Touring smoothing. The mapping function for particular semantic spaces will be denoted as mHAL, mCOALS, and mRI.Similar to the previous section, we also extend these local semantic models with stemming information. During the building of semantic space we can simply use stems instead of word forms and the mapping function becomesm:w→s→c, wherew∈W, s∈S, and c∈C and everything else remain unchanged. The mapping functions using semantic spaces together with stemming will be denoted as mS−HAL, mS−COALS, and mS−RI.In this paper we work with two ways of combining language models: linear interpolation (see Section 4.5.1) and its extension, bucketed linear interpolation (see Section 4.5.2).As in our previous works we use a simple but very effective linear interpolation to combine different language models(10)PLI(wi|wi−n+1i−1)=∑k=1Kλk·Pk(wi|wi−n+1i−1),where λkis the weight of the kth language model Pk(). We use the expectation maximization (EM) algorithm described in Dempster et al. (1977) to calculate optimal weights λk, which maximizes the likelihood of the held-out data. Using linear interpolation it is quite straightforward to combine different sources of information such as our global and local context language models.The linear interpolation can be extended to a method called bucketed linear interpolation, where weights become the function of the frequency of word history (Bahl et al., 1983). The main idea is that the weights λkshould be different for words with histories of varying frequencies. For example we expect that the word n-gram language model would produce the best probability estimates (receive the highest weight) for very often frequented histories. The formula of linear interpolation is transformed to(11)PBLI(wi|wi−n+1i−1)=∑k=1Kλk(wi−n+1i−1)·Pk(wi|wi−n+1i−1).The weights λk() certainly cannot be different for each possible frequency of history because of data sparsity. Instead, the whole frequency spectrum is divided into buckets, where each bucket holds some range of frequencies. Histories with frequencies in the same bucket receive the same weights. The number of buckets can be tuned but it generally depends on the amount of training data available. The more training data are available, the more buckets can be used.In our previous research (Brychcín and Konopík, 2014), it was shown that bucketed linear interpolation produces slightly better results compared to simple linear interpolation when combining several language models.

@&#CONCLUSIONS@&#
