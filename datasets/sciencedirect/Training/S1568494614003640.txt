@&#MAIN-TITLE@&#
Neural network control of nonlinear dynamic systems using hybrid algorithm

@&#HIGHLIGHTS@&#
A hybrid method is proposed to control a nonlinear dynamic system.This hybrid algorithm combines gradient method and Kohonen algorithm to obtain faster convergence.The proposed algorithm can considerably reduce networking time.

@&#KEYPHRASES@&#
Neural network control,Nonlinear systems,Gradient descent method,Supervised and unsupervised learning,Self-organizing map,Hybrid learning,

@&#ABSTRACT@&#
In this paper, a hybrid method is proposed to control a nonlinear dynamic system using feedforward neural network. This learning procedure uses different learning algorithm separately. The weights connecting the input and hidden layers are firstly adjusted by a self organized learning procedure, whereas the weights between hidden and output layers are trained by supervised learning algorithm, such as a gradient descent method. A comparison with backpropagation (BP) shows that the new algorithm can considerably reduce network training time.

@&#INTRODUCTION@&#
The design of the motion control of nonlinear systems has attracted an ever increasing interest. Various advanced methods have been proposed to obtain satisfactory tracking performances. These include robust adaptive control [1,2], sliding mode control [3,4], backstepping control [5,6], and decentralized control [7,8]. However, it is now well know that nonlinear dynamical systems can exhibit extremely complex dynamic behaviour and for this reason it becomes necessary to use intelligent control techniques. More recently, techniques like neural networks, fuzzy logic and genetic algorithm have been applied with some success to problems of control and identification of nonlinear systems for several domains of application [9–14]. Moreover, in some works, both Neural Networks and advanced methods are used to get the better performances [15–22]. For example [18], propose adaptive Neural Network decentralized backstepping output-feedback control for nonlinear large scale systems with time delays. In [21], robust adaptive Neural Networks control was developed for a class of uncertain MIMO nonlinear systems with input nonlinearities. A neural network, one of the most popular intelligent computation approaches, has the capability to approximate any desired nonlinear function to an arbitrary degree of accuracy [23]. The principal type of neural networks based control scheme is the feedforward neural networks (FNN) [24–28]. The most commonly used method to train FNN is based on backpropagation (BP) algorithm [29]. Although the gradient descent based learning method has achieved many practical successes in the neural control of nonlinear systems, this method usually encounters problem of slow convergence. A number of learning algorithms have been proposed in the literature in an attempt to speed up this method [30–38]. Training speed is significantly increased by using various second orders algorithm [39–41]. Many other algorithms with the emphasis on hybrid techniques have been developed to accelerate the training method of feedforward neural network [42–49].Our proposed work is motivated by the very recently introduced hybrid algorithm called a FN-based (Fuzzy Neighbourhood) hybrid which combines unsupervised and supervised learning [45]. In this approach, the weights between input and hidden layers were determined according to an unsupervised procedure relying on the Kohonen algorithm with fuzzy neighbourhood function and the weights between hidden and output layers were updated according to a supervised procedure based on gradient descent method. By combining the above two modes, the learning algorithm converge much faster than other well-known algorithms.The main issues in the field of neural networks based control include the architecture of the identifier and controller, and the algorithm to be used in training their parameters [50,51]. The proposed control architecture adopted in this paper is the basis for many current neurocontroller. The neurocontroller contains two neural networks. One is the neural model (NM) and the other is the neural controller (NC). In this approach, a neural model (NM) is first trained to model the forward dynamic of the plant with a recently proposed approach called a FN (Fuzzy neighbourhood)-based hybrid [45]. Then, an untrained controller network is placed in series with the network model of the plant. Finally, the weights for the neural controller (NC) are trained using a proposed hybrid algorithm, while holding the neural model weights constant. In this paper, we propose a learning method for the neural controller (NC). This proposed hybrid approach uses a Kohonen algorithm with fuzzy neighbourhood for clustering the weights of the hidden layer and gradient descent method for training the weights of the output layer.The paper is organized as follows: in “Problem statement”, we formulate the control problem and explain the control architecture method using two neural networks. In “Learning algorithm for the neural controller”, the control of a nonlinear plant using hybrid algorithm is illustrated. In “Simulation results”, simulation results and comparisons between two learning algorithms are given. Finally, in “Conclusion”, we present the main conclusions.Assume that the unknown nonlinear system to be considered is expressed by:(1)y(k)=f[y(k−1),y(k−2),…,y(k−n),u(k),u(k−1),…,u(k−m)]where y(k) is the scalar output of the system, u(k) is the scalar input to the system, n and m are the output and the input orders respectively, f[…] is the unknown nonlinear function to be estimated by a Neural Network. The control problem is to select a control input u(k), such that the output of the system y(k) tracks the given reference ydas close as possible.A feedforward neural network is used to learn the system (1) and FN (Fuzzy Neighbourhood)-based hybrid algorithm [45] is employed to train the weights. The block diagram of identification system is shown in Fig. 1. Since the input to neural network is(2)P=[y(k−1),y(k−2),…,y(k−n),u(k),u(k−n),…,u(k−m)]The neural model for the unknown system (1) can be expressed as:(3)yˆ(k)=fˆ[y(k−1),y(k−2),…,y(k−n),u(k),u(k−1),…,u(k−m)]whereyˆ(k)is the output of the neural model andfˆis the estimate of f.The weights of the neural model are adjusted to minimize the cost function given by:(4)E(I)=12(e(I))2wheree(I)=y(k)−yˆ(k)is the identification error, the difference of the outputs between the plant and the neural model, y(k) is the plant output andyˆ(k)is the neural model output.To solve control problem the specialized control structure [27] is considered. This strategy consists of the system and two feedforward neural networks (FNN), the NM (neural model) for identification and the NC (neural controller) for control as shown in Fig. 2.The cost function to be minimized for the neural controller is given by(5)E(C)=12(e(C))2wheree(C)=yˆ(k)−ydis the control error, between the output of NMyˆ(k)and the desired value yd.In this section, we introduce the hybrid learning algorithm which combines gradient descent method [29] and Kohonen algorithm [52] to obtain faster convergence of network parameters.The neural model used in this work is a feedforward neural network shown in Fig. 3. The network structure consists on an input layer, a hidden layer and an output layer.The output of each neuron in the hidden layer is as follows:(6)oj=11+exp(−netj)(7)netj=∑iwji⋅xi−θj,i=1,2,…,n,j=1,2,…,Lwhere ojis the output of jth neuron in the hidden layer, xiis the ith input variable, wjiis the connection weight between the ith neuron in the input layer and the jth neuron in the hidden layer, θjis the threshold of the jth neuron in the hidden layer, n and L are the numbers of neurons in the input and hidden layers, respectively.The output of each neuron in the output layer is as follows:(8)yˆk=11+exp(−netk)(9)netk=∑jwkj⋅oj−θk,k=1,2,…,mwhere wkjis the connection weight between the jth neuron in the hidden layer and the kth neuron in the output layer, θkis the threshold of the kth neuron in the output layer, m is the numbers of neurons in the output layer.The BP algorithm has become the standard algorithm used for training FNN. It is a generalized least mean squared algorithm that minimizes a criterion which equals to the sum of the squares of the errors between the actual and the desired outputs.We consider a FNN with three layers (Fig. 4): an input layer, a hidden layer and an output layer. The intermediate layer is considered like a self organizing map of Kohonen [52].Our approach consists of two stages. In the first stage, shown in detail in Fig. 1, a neural network is trained to approximate the input–output behaviour of the plant. When the error between plant and the neural model becomes sufficiently small, we proceed to a second stage, shown in Fig. 2 that indicates the structure normally utilized for training the neural controller based on hybrid algorithm. The output of the block neural network controller is the input to neural model, obtained in the previous stage. This stage concerns the training procedure of the feedforward neural controller. It is composed of two phases.During the first phase of training, the weight of the winning b is tuned by the difference between the input vector and the weight vector. Not only the winning node is learning but also its neighbourhood nodes are learned as well. Therefore, their weight vectors become more similar to the input pattern. As a result, the respective node is more likely to win at future presentations of this input pattern. The effect of this procedure is that only a small number of hidden units are excited by a given input vector. If the weight vector of the winner is attracted to a region in input space, the neighbours are also attracted, although to a lesser degree.During the second phase of training, weights leaving from the winner neuron and its neighbours are adjusted by the gradient descent method. The learning algorithm can be described as two stages process as below:The neural model (NM) is trained using the FN-based hybrid learning algorithm [45]. The weights from the input neurons to intermediate layer, and from the intermediate layer to the output neurons, are adjusted so as to minimize the cost function (4). For more details about FN-based hybrid, refer to [45].The weight tuning update for the input layer using Kohonen procedure [52] is given by(10)Δwji(I)(t)=ε(I)(t)hbi(I)(t)(x−wji(I)(t))where ɛ(I)(t) is the learning rate parameter andhbi(I)(t)is the neighbourhood function centred around the winning b, both ɛ(I)(t) andhbi(I)(t)are varied dynamically during learning for best result.The weight tuning update for the output layer using gradient descent procedure [29] is given by(11)Δwkj(I)(t)=−α(I)∂E(I)∂wkj(I)+β(I)(t)Δwkj(I)(t−1)=−α(I)(t)(yk−yˆk)⋅yˆk(1−yˆk)oj(I)+β(I)(t)Δwkj(I)(t−1)α(I)(t) is the learning rate, β(I)(t) is the momentum rate. yk andyˆkare the plant and the neural network identifier outputs, respectively.oj(I)is the output of jth neuron in the hidden layer.The step by step procedure of the learning control method is listed as follows:Step 1: Initialize the weights at small random values.Step 2: Present an input vector x to the NC.Step 3: The winner neuron b is selected by evaluation of the distance measure between the input and the neuron weights [52]:(12)||x−wb(C)||=mini||x−wb(C)||The distance measure can be any distance norm, in the practice mostly the Euclidean one is used.Step 4: Calculate(13)ε(C)(t)=ε0exp−tTwhere ɛ0 is its initial value, T is the total number of iterations and t is the current iteration.Step 5: Calculate(14)σ(C)(t)=σ0σfσ0tTwhere σ0 and σfcontrol the initial and final values of neighbourhood width.Step 6: A Gaussian neighbourhood function is calculated as follows:(15)hbi(C)(t)=ε(C)(t)exp−(b−i)22σ(c)2,hbi(C)(t)∈[0,1]where b denotes the index of the winner neuron and i denotes the index of any neuron. The factors ɛ(C)(t) and σ(C)(t) are the learning rate factor and the neighbourhood width factor, respectively. They are decreasing functions of time. The neighborhood functionhbi(C)(t)usually is equal to 1 for the winner neuron and decrease with the distance of the neurons from the winner.Step 7: The weights of the winner and its neighbouring neurons are modified as follows [52]:(16)Δwji(C)(t)=hbi(C)(t)(x−wji(C)(t))The weights leaving from the winner neuron and its neighbours are modified based on the gradient descent method [29]:(17)Δwkj(C)(t)=−α(C)∂E(C)∂wkj(C)+β(C)⋅Δwkj(C)(t−1)=−α(C)(t)⋅(yˆk−yd)⋅∑jwkj(I)⋅yˆk⋅(1−yˆk)⋅wji(I)⋅oj(I)⋅(1−oj(I))⋅u⋅(1−u)⋅oj(C)+β(C)⋅Δwkj(C)(t−1)α(C)(t) is the learning rate, β(C)(t) is the momentum rate. ydandyˆkare the desired and the neural network identifier outputs, respectively.oj(C)is the output of jth neuron in the hidden layer.Step 8: The algorithm is stopped when the value of the error function (5) has become sufficiently small, otherwise go back to step 2.In this section, simulation results are presented and discussed in order to evaluate the performance of the proposed algorithm and to compare it with the conventional BP algorithm (with a momentum term). Two examples are provided. First example is dynamical equation and second example is a general benchmark problem.Simulation results were obtained using a personal computer Pentium 4 2.8GHz.The nonlinear dynamical system [24] to be controlled is given by:(18)y(k+1)=y(k)1+y2(k)+u3(k)The objective in the present example is to control the plant so as to track a desired output given by the following 100 samples of data.(19)yd=1.5sin2πk10+sin2πk25The model has two inputs u(k) and y(k) and a single output y(k+1), and system identification was initially performed with the plant input with amplitude uniformly distributed over the interval [−2,+2]. Training samples and testing samples contained 100 and 100 data points, respectively. The desired output was normalized in the interval [0.1, 0.9]. The controller is a one hidden layer FNN, with 20 neurons in the hidden layer and one neuron in the output layer. The neurocontroller uses the two inputs, the current state y(k) and the desired state yd(k), to produce an output which is u(k). The initial weight values of the conventional BP are set to small random values.A learning rate of 0.015 and momentum of 0.15 is used in BP algorithm and parameters for the proposed algorithm are: ɛ0=0.5, α(C)=0.15, β(C)=0.15, σ0=2, σf=0.5. These parameters are determined by trial and error tests.Fig. 5compares the mean squared control errors of the two methods. It can be seen from Fig. 5 that the hybrid algorithm converges rapidly than the BP algorithm.Figs. 6 and 7show the performance of the controllers with hybrid algorithm and BP algorithm, respectively.In [53], Box and Jenkins gave 296 pairs of data measured from a gas furnace system with a single input u(t) being gas flow rate and a single output y(t) being CO2 concentration in outlet gas.Following previous researchers [54] in order to make a meaningful comparison, the inputs of the prediction model are selected as u(t−4) and y(t−1), and the output is y(t) The aim of control task is to keep the concentration as close as possible to the reference trajectory by manipulating the flow rate. Training samples and testing samples contained 196 and 100 data points, respectively.The desired output was normalized in the interval [0.1, 0.9]. The controller is a one hidden layer FNN, with 20 neurons in the hidden layer and one neuron in the output layer. The neurocontroller uses the two inputs, the current state y(k) and the desired state yd(k), to produce an output which is u(k). The initial weight values of the conventional BP are set to small random values. A learning rate of 0.2 and momentum of 0.5 is used in BP algorithm and parameters for the proposed algorithm are: ɛ0=0.5, α(C)=0.15, β(C)=0.15, σ0=2, σf=0.5. These parameters are determined by trial and error tests.Fig. 8compares the mean squared control errors of the two methods. It can be seen from Fig. 8 that the hybrid algorithm converges rapidly than the BP algorithm.Figs. 9 and 10show the performance of the controllers with hybrid algorithm and BP algorithm, respectively.The algorithm presented here enables us to define an appropriate number of neuron in the hidden layer of Feedforward neural network, based on clustering method. The number of active hidden neuron directly affects the generalization and training time, which are two important factors in neural network modelling. The backpropagation algorithm updates all network weights after every input–output case. The total number of weights is where n, N, and m denote the input vector dimension, the hidden node number and the output node number, respectively.Letting P denotes the number of the selected hidden nodes (1≤P<N), the hybrid algorithm updates only parameters of the selected hidden nodes after every input–output case. The total number of weights becomes ωh=(n+1)·P+(P+1)·m.The computed weighted sum number in the case of the proposed hybrid algorithm is inferior to that of the backpropagation algorithm since P is always inferior to N. The nonlinearity computation number depends on the selected hidden node number (P) for each presented pattern. This fact makes the learning speed of the proposed algorithm is faster than that of the backpropagation.The comparative performances of the proposed training algorithm and Backpropagation BP algorithm, such as the number of iterations, total time of convergence and training error are summarized in Table 1. It can be seen that the proposed training algorithm takes minimum number of iterations for convergence as compared to the standard Backpropagation algorithm in all two examples. From the results we can also see, when achieving the same error, the proposed training algorithm is faster than the popular BP in terms of computation time.

@&#CONCLUSIONS@&#
