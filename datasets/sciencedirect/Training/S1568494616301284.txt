@&#MAIN-TITLE@&#
A new pruning method for extreme learning machines via genetic algorithms

@&#HIGHLIGHTS@&#
We propose a pruning method for ELM using genetic algorithms (GA).Our proposal estimates the leave-one-out (LOO) error using the PRESS statistic.Our proposal, called GAP-ELM, was tested on 7 real world datasets.GAP-ELM was compared with MLP and RBF neural networks and showed competitive results.

@&#KEYPHRASES@&#
Extreme learning machines,Pruning methods,Genetic algorithms,

@&#ABSTRACT@&#
Extreme learning machine (ELM) is a recently proposed learning algorithm for single hidden layer feedfoward neural networks (SLFN) that achieved remarkable performances in various applications. In ELM, the hidden neurons are randomly assigned and the output layer weights are learned in a single step using the Moore-Penrose generalized inverse. This approach results in a fast learning neural network algorithm with a single hyperparameter (the number of hidden neurons). Despite the aforementioned advantages, using ELM can result in models with a large number of hidden neurons and this can lead to poor generalization. To overcome this drawback, we propose a novel method to prune hidden layer neurons based on genetic algorithms (GA). The proposed approach, referred as GAP-ELM, selects subset of the hidden neurons to optimize a multiobjective fitness function that defines a compromise between accuracy and the number of pruned neurons. The performance of GAP-ELM is assessed on several real world datasets and compared to other SLFN and a well known pruning method called Optimally Pruned ELM (OP-ELM). On the basis of our experiments, we can state that GAP-ELM is a valid alternative for classification tasks.

@&#INTRODUCTION@&#
Single-hidden layer feedforward neural networks (SLFNs) have been successfully applied in various classification and regression problems like fault detection [1] and face recognition [2]. Among the training methods for SLFN, one can cite methods based on first-order optimization algorithms (gradient descent) as the error backpropagation used to train multilayer perceptrons (MLPs) [3]. Despite its popularity, gradient descent methods are known to have a very slow convergence.In [4], Huang et al. proposed a new procedure to train SLFNs and the resulting model was termed as the extreme learning machine (ELM). ELM relies on fixed-weight hidden neurons (nodes) with non-linear activation functions [4]. The weights of the hidden neurons are assigned randomly. After that, a batch learning procedure is used to adjust the output layer weights. The ELM has two main advantages: the first one is the training speed when compared to most learning algorithms such as MLPs and support vector machines (SVMs) [5]. A second advantage is the use of a single hyperparameter (number of hidden neurons).It is well known that neural networks with a large number of hidden neurons may have problems with the performance on unseen cases due to training data overfitting. Nevertheless, a neural network with a small hidden-layer suffers from training data underfitting. Taking these statements into account, defining the number of a hidden-layer neurons is an important issue for extreme learning machines. A possible approach consists on pruning unnecessary neurons during the training process [6].Some previous works have tackled the reduction issue of the hidden-neurons such as the optimally pruned ELM (OP-ELM). To do so, OP-ELM ranks the best neurons using multiresponse sparse regression (MRSR) and then selects the optimal number of neurons by leave-one-out (LOO) [7]. An improvement of this methodology named Tikhonov Regularized OP-ELM (TROP-ELM) was made by introducing a L1 regularization (penalty) to rank the neurons and a L2 penalty to prune the network [8]. Other methodologies have dealt with finding the suitable number of neurons based on adding (removing) neurons to (from) the hidden-layer [9–11]. Recently, Bayesian methods are exploited how to learn the output weights of ELM and a sparse version called Sparse Bayesian ELM (SBELM) was presented in [12]. Even though OP-ELM and TROP-ELM outperform the standard ELM, a large number of hidden neurons remain in the trained model due to the minimization of training error in ranking neurons, resulting therefore in a highly computational cost [12].Genetic algorithms (GAs) have been used to solve optimization problems in many different real-world areas, such as medicine, bio-informatics, economics, chemistry and so on [13,14]. GAs are optimization tools which can be used to generate useful solutions to search problems. Due to the underlying features of GAs, some optimization problems can be solved without the assumption of linearity, differentiability, continuity or convexity of the objective function. Unfortunately, these desired features are not found in many classical mathematical methods when applied to the same kind of problems.In the literature, we can find several works combining GAs and ELMs. In [15], an weighted ELM for imbalanced classes is proposed. The GA is used to assign higher weights to examples belonging to the minority class and lower weights to the majority class [15]. In [16] GAs were used to optimize the weights of connections between the input layer and the hidden-layer, the hidden-layer neuron biases, the set of input variables, as well as the number of neurons and activation function of each neuron. Even though the method presented promising results, the fundamentals behind ELM had been somewhat lost, since the hidden-layer weights and biases are updated. Following the same idea of weight adjustment, Lahoz et al. applies a bi-objective genetic algorithm to minimize both the mean square error and the number of hidden neurons [17]. We must emphasize at this point that it would be interesting to work with only those neurons generated at random (with no weight adjustment) such as the aforementioned proposals OP-ELM and TROP-ELM do, since we suppose each and every required neuron for a high-performance model is already in the set of randomly generated neurons.To combine the aforementioned advantages of ELMs and GAs, this work uses a GA to prune unnecessary or similar hidden-neurons while keeping the performance when compared with standard classifiers. Our proposal deals with a multi-objective optimization problem in a priori sense (i.e., a multi-objective problem is turned into a single one). The first objective is to achieve high-performance classifiers while the second is to prune as many neurons as possible from the full set of neurons, which were generated in previous phase (in the beginning of the process). To do this, we also propose a new a priori multi-objective fitness function which incorporates a cost of pruning in its formulation.The remaining part of this paper is organized as follows. In Section 2 we review the fundamentals of the ELMs. In Section 3 we present the optimally pruned ELM we use for comparison. In Section 4, we introduce genetic algorithms which are necessary to understanding of our proposal presented in Section 5 and then, in Section 6, we present our simulations. Finally, the paper is concluded in Section 7.ELM is a single-hidden layer feedforward network proposed in [4]. For a network with p input units, q hidden neurons and c outputs, the i-th output at time step t, is given by(1)oi(t)=miTh(t),wheremi∈ℝq,∀i∈{1,…,c}, is the weight vector connecting the hidden neurons to the i-th output neuron, andh(t)∈ℝqis the vector of hidden neurons’ outputs for a given input patternx(t)∈ℝpfrom a data set{x(t)}t=1n. The vector h(t) itself is defined as(2)h(t)=[f(w1Tx(t)+b1),f(w2Tx(t)+b2),…,f(wqTx(t)+bq)]T,where blis the bias of the l-th hidden neuron,wl∈ℝpis the weight vector of the l-th hidden neuron and f(·) is a sigmoidal activation function. The weight vectors wlare randomly sampled from a normal or even from a uniform distribution.Let H=[h(1) h(2) ⋯ h(n)] be a q×n matrix whose t-th column is the hidden-layer output vectorh(t)∈ℝq. Similarly, let D=[d(1) d(2) ⋯ d(n)] be a c×n matrix whose the t-th column is the desired (target) vectord(t)∈ℝcassociated with the input pattern x(t), t=1, …, N. Finally, let M=[m1m2 ⋯ mc] be a q×c matrix, whose i-th column is the weight vectormi∈ℝq, i=1, …, c. Thus, these three matrices are related by the linear mapping(3)D=MTH,where the matrices D and H are known and built from the data, while the weight matrix M is unknown. Nevertheless, the weight matrix M can be easily computed by means of the Moore–Penrose pseudo-inverse method as follows(4)M=(HHT)−1HDT.Assuming that the number of output neurons is equal to the number of classes, the class index i* for a new input pattern is given by the following decision rule:(5)i*=argmaxi=1,…,c{oi},where oiis computed as in Eq. (1).Optimally pruned extreme learning machine (OP-ELM) comprises three main steps:•SFLN constructionRanking the neurons using MRSRSelect the number of neurons using a LOO validationIn the first step, a SLFN is constructed with a large number of hidden layer neurons. To improve robustness and generality, the OP-ELM methodology uses a combination of three different sorts of kernels instead of the sigmoid kernel. Possible kernel types are linear, sigmoid and Gaussian.The second step consist on building a multiresponse sparse regression (MRSR) that maps the outputs of the hidden layer to the targets. MRSR can be seen as an extension of the least angle regression (LARS) algorithm and hence is actually a variable ranking technique, rather than a selection one [7]. More details about the MRSR can be find in [18].After ranking the neurons, the decision for the best number of neurons in the model is taken by a leave-one-out (LOO) validation method. However, the LOO method can be very time consuming, if the data set is large. To overcome this drawback, the authors use the PREdiction Sum of Squares (PRESS) statistics, which can give a direct and exact formula for the calculation of the LOO [7].Genetic algorithm is a meta-heuristic search method inspired by natural evolution, such as inheritance, mutation, natural selection and crossover. This meta-heuristic can be used to generate useful solutions to optimization and search problems. Due to the characteristics of GA, it is easier to solve various problems by GA than other mathematical methods which do have to rely on the assumption of linearity, differentiability, continuity or convexity of the objective function.In a genetic algorithm, a population of candidate individuals (or solutions) to an optimization problem is evolved toward better solutions by natural selection, i.e., a fitness function. In this population, each individual has a set of genes (gene vector), named chromosome, which can be changed by mutation or combined generation-by-generation with other one to build new individuals by reproduction processes which use crossover. The most common way of representing solutions is in a binary format, i.e., strings of 0s and 1s. Despite that, other kind of encodings are also possible.The first population – set of solutions – is generated in a pseudo-randomized mode and it evolves in cycles, usually named generations. A value of the fitness function (its accuracy) for each individual ranks how ‘good’ the solution is. One can calculate the fitness value after decoding the chromosome and the best solution of the problem to be solved has the best fitness value. This value is used in order to guide the reproduction process where individuals with high fitness value have more chance to spread out their genes over the population.Standard implementation of genetic algorithm for natural selection is the roulette wheel. After the selection process, the selected individuals are used as input to other genetic operators: crossover and mutation. Crossover operator combines two strings of chromosomes, but mutation modifies a few bits in a single chromosome.A single objective GA is presented below.1.Initiate t=0, where t stands for the generation;Generate initial population P(t), randomly;For each individual i in P(t)Evaluate the fitness function;While stopping criteria is not achievedSelect some individuals from P(t);Apply crossover operator to selected individuals;Apply mutation operator to selected individuals;Compute t=t+1;Evaluate fitness function for the new population P(t);Select the best individual or solution.Our proposal, called GAP-ELM, is based on a priori multi-objective genetic algorithm which aims at improving (maximizing) the classifier accuracy over the training dataset while reducing (minimizing) the number of hidden neurons. The individual with only one chromosome is represented by a binary vector of genes where each one is set to either “one” (true) whether a certain hidden neuron will be used in the hidden-layer or “zero” (false) otherwise.The scalarization-based fitness function [19] is a balance between the value of the resulting classifier accuracy when we take into account the genes that were set to “one” and the proportion of neurons pruned from the hidden-layer. In our approach each individual has as many hidden neurons as the number of genes set up to “one”. The idea is to eliminate those hidden-layer neurons that are unnecessary or very similar to others. For this reason, the population can evolve to the best solution or at least to a better one.As stated, an ELM has q neurons in the hidden-layer, thus an individual also has q genes in the chromosome, since one individual is made of only one chromosome. Generally speaking, we model an individual with genetic material as g=[g1g2…gi…gq−1gq], where gi∈{0, 1} stands for the i-th hidden-layer neuron from all the candidate neurons.11Candidate neurons are those ones whose we generate randomly.Therefore, in order to eliminate a certain neuron from the hidden-layer we must set gito zero. Hence, we have as many pruned neurons as the number of “zeros” in the vector g.To illustrate the proposed approach we present simple examples of how to model individuals for a GAP-ELM from five neurons generated at random. The first example is about having all of the five neurons in the hidden-layer. To do so, we need to set up a gene vector g to [11111]. The second example, we eliminate the second hidden-neuron by an individual with genetic material as g=[10111]. If the genetic material is set up as g=[10001], we have just two hidden neurons. The idea behind this individual modeling is presented in the Fig. 1The proposed fitness function relies on maximizing both the accuracy and the pruning rate. In this context, we look for high accuracy and few hidden neurons. Alternatively, we can also state that our fitness function is based on minimizing both the error rate and the ratio of the number of hidden neurons after the pruning process to the total number of candidate neurons, henceforth called neuron rate.To have a simple fitness function for minimization in a priori sense [19], we weighted the relation between the error and neuron rate by a factor α∈[0, 1] as follows(6)fitness(g)=α*error_rate(g)+(1−α)*neuron_rate(g)where error_rate(g) is the ratio of the number of misclassification patterns to the number of training patterns, α∈[0, 1] is a factor of importance22We can see alpha as the cost of pruning. The lower the alpha value, the lower the number of hidden neurons.one gives to the error rate reduction; as well as(7)neuron_rate(g)=∑i=1qgiq,where∑i=1qgiis upper bounded by q. We present the algorithm for GAP-ELM in Section 5.3.Considering the proposed fitness function, the error rate can be obtained using any cross validation procedure on the training set. However, using standard procedures like 10-fold cross validation and LOO can lead to a high computational cost. To overcome this issue, we computed the error rate using the LOO method through the Prediction Sum of Squares (PRESS) statistic.The PRESS statistic provides a direct and exact way of computing the LOO error for linear models. The PRESS statisticeiloofor the model when the i-th pattern is left out can be obtained by(8)eiloo=di−hiTm−i1−zi,i,where m−iis the solution for the linear system when, as mentioned, the i-th pattern is left out of the training set and zi,iis the i-th main diagonal term for the hat matrix Z, so thatZ=HH†=H(HTH)−1HT.Thus, the leave-one-out estimate of the local mean integrated squared errore¯loocan be obtained as(9)e¯loo=1n∑i=1n(eiloo)2,where n is the number of training patterns. More information about PRESS can be found in [20,7].GAP-ELM algorithm for a genetic algorithms-based pruned ELM can be described as follows.1.Initiate t=0, where t stands for the current generation;Generate q hidden neurons randomly;Generate initial populationP(t), i.e., a set of{g}i=1sand its related matrices{Hi}i=1s, where s is the number of individuals at the generationt, randomly;For each individual i in P(t)Solve the linear system described in Eq. (3) by Eq. (4);Evaluate the fitness function shown in Eq. (6);Whilet≤tmax, wheretmaxis the maximum number of generationsSelect individuals i from the population P(t);Apply crossover operation to selected individuals;Apply mutation operation to selected individuals;Compute t=t+1;Solve the linear system described in Eq. (3) by Eq. (4);Evaluate the fitness function shown in Eq. (6);Select the best individual or solution, i.e., go.We carried out simulations so that 80% of the full data set were randomly selected for training purposes and the remaining 20% of the examples were used to assess the classifier generalization performances. Tests with real-world benchmarking datasets were also evaluated in this work. We used UCI machine learning datasets: Adult, Breast Cancer Winconsin, Haberman, HIV, Pima Indians Diabetes and Ripley. Some information about evaluated data sets is presented in Table 1, such as data set name, abbreviation and number of Patterns (# Patterns). One can notice that we have handled large data sets such as Adult, HIV and Ripley with 48808, 3272 and 1250 patterns, respectively.The training process for ELM were based on a grid search with 10-fold cross-validation to tune the number of hidden neurons. The activation functions for all ELM hidden neurons were the hiperbolic tangent. For OP-ELM and GAP-ELM, we defined the number of hidden neuron at the beginning of training process as the average of hidden neurons for ELM, hence OP-ELM and GAP-ELM started with the same number of neurons. The idea is to have a comparison between OP-ELM and GAP-ELM when a small number of hidden neurons is used.Initially, the GA randomly creates a population of feasible solutions. Each solution is a string of binary values in which each bit represents the presence (1) or absence (0) of a neuron in the hidden layer. For the next generation, we take into account the fact that 10% of the best individuals are selected due to a elitist selection scheme, 80% of new individuals were generated as result of applying the crossover operator and then the remaining individuals were obtained by mutation. Our population has 50 individual and the maximum number of generations is 300. However, in general, the algorithm stops when the average relative change in the best fitness function value is less than or equal to 10−5.In Table 2, we report performance metrics (mean value and standard deviation of the recognition rate, i.e., accuracy) on testing set averaged over 30 independent runs. We also show the average number of hidden neurons (#HN) and training time cost (#TC) during all the training process.By analyzing Table 2, one can infer that the accuracies of the GAP-ELM were equivalent to those achieved by the ELM and OP-ELM. In some cases, as shown in this table for Haberman (HAB) and Breast Cancer (BCW) the accuracies of GAP-ELM classifiers were even better. On the other hand, it is possible to notice a significant reduction in the number of hidden neurons provided by GAP-ELM when compared to OP-ELM.In addition to the previous results, Fig. 2(a) shows the accuracies of OP-ELM, ELM and GAP-ELM and Fig. 2(b) shows the average number of hidden neurons for α in range [0.96, 1.00] for 30 independent runs with all data sets when applied to ADU, BAN, BCW and HAB datasets. Similar results are presented in Fig. 3(a) and Fig. 3(b) for HIV, PID and RIP datasets.As expected, when α is increased, GAP-ELM accuracies tend to increase and so the number of hidden neurons. Although GAP-ELM seems to be sensible to the value of α, it is possible to notice that in almost all cases presented (with different α), GAP-ELM reduced the number of hidden neurons while keeping the accuracy at a similar level.Another interesting point can be observed when α is set to 1.0. In this situation, the multiobjective formulation is turned into a single objective fitness function that only considers the accuracy. As a result, GAP-ELM achieved similar (or even higher) accuracies than OP-ELM and ELM in almost all datasets with a number of neurons similar to OP-ELM.A second set of experiments was conducted with a fixed number of hidden neurons. Along with ELM, OP-ELM and GAP-ELM, two widely used SFLN were assessed: the Radial Basis Function (RBF) and MLP neural networks. For the experiments, 40 hidden neurons were used for MLP, RBF and ELM. OP-ELM and GAP-ELM started the pruning process with 40 hidden neurons. Results are shown in Table 3.Similarly to that observed in previous experiments, GAP-ELM achieved accuracies comparable to both OP-ELM and ELM but with significant reduction on the number of hidden neurons. On the other hand, it is possible to notice an increase in the training time. When comparing GAP-ELM with the RBF and MLP, it is noticeable that the accuracies were similar, however GAP-ELM presents a faster training procedure and also less hidden neurons.

@&#CONCLUSIONS@&#
In this work, we propose a pruning method for ELMs based on Multiobjective GAs. The proposed approach was called Genetic Algorithms for Pruned ELM (GAP-ELM). GAP-ELM is built upon a multiobjective fitness function considering both the accuracy and the reduction in the number of hidden neurons. The accuracy is estimated by the use of a Leave-one-out cross validation procedure called PRESS statistic. The use of PRESS statistic avoids computations related to standard cross validation procedures and thus reduces the final computational cost of GAP-ELM.The performance of GAP-ELM was assessed on 7 real world datasets and compared to ELM, RBF, MLP and OP-ELM. All methods were compared according to three criteria: accuracy, number of hidden neurons and training time. On the basis of our experiments we can state that GAP-ELM is a valid alternative for classification tasks, since none of the methods was capable to outperform GAP-ELM in all performance criteria at the same time. Future works include the design of an ELM pruning method with a fixed number of neurons in the hidden layer.