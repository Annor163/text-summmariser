@&#MAIN-TITLE@&#
A hierarchical knowledge-based approach for retrieving similar medical images described with semantic annotations

@&#HIGHLIGHTS@&#
We propose an image retrieval system that considers the semantic of medical images.It relies on semantic annotation of the images and evaluation of their similarities.Image similarity is computed using ontological relations in a hierarchical fashion.It provides a means of capturing the semantic correlations between image contents.Evaluation on the retrieval of liver CT images proved the interest of this system.

@&#KEYPHRASES@&#
Image retrieval,Semantic image annotation,Semantic-based distances,Ontologies,Computed tomographic (CT) images,Liver lesions,

@&#ABSTRACT@&#
Computer-assisted image retrieval applications could assist radiologist interpretations by identifying similar images in large archives as a means to providing decision support. However, the semantic gap between low-level image features and their high level semantics may impair the system performances. Indeed, it can be challenging to comprehensively characterize the images using low-level imaging features to fully capture the visual appearance of diseases on images, and recently the use of semantic terms has been advocated to provide semantic descriptions of the visual contents of images. However, most of the existing image retrieval strategies do not consider the intrinsic properties of these terms during the comparison of the images beyond treating them as simple binary (presence/absence) features. We propose a new framework that includes semantic features in images and that enables retrieval of similar images in large databases based on their semantic relations. It is based on two main steps: (1) annotation of the images with semantic terms extracted from an ontology, and (2) evaluation of the similarity of image pairs by computing the similarity between the terms using the Hierarchical Semantic-Based Distance (HSBD) coupled to an ontological measure. The combination of these two steps provides a means of capturing the semantic correlations among the terms used to characterize the images that can be considered as a potential solution to deal with the semantic gap problem. We validate this approach in the context of the retrieval and the classification of 2D regions of interest (ROIs) extracted from computed tomographic (CT) images of the liver. Under this framework, retrieval accuracy of more than 0.96 was obtained on a 30-images dataset using the Normalized Discounted Cumulative Gain (NDCG) index that is a standard technique used to measure the effectiveness of information retrieval algorithms when a separate reference standard is available. Classification results of more than 95% were obtained on a 77-images dataset. For comparison purpose, the use of the Earth Mover’s Distance (EMD), which is an alternative distance metric that considers all the existing relations among the terms, led to results retrieval accuracy of 0.95 and classification results of 93% with a higher computational cost. The results provided by the presented framework are competitive with the state-of-the-art and emphasize the usefulness of the proposed methodology for radiology image retrieval and classification.

@&#INTRODUCTION@&#
Diagnostic radiologists need to maintain high interpretation accuracy while maximizing efficiency in the face of increasing volumes of images per study. They are now confronted with the challenge of efficiently and accurately interpreting cross-sectional studies that often contain thousands of images [1]. Currently, this is largely an unassisted and time-consuming process, and a radiologist’s accuracy is established through training and experience. Despite this training, there is substantial variation in interpretation among radiologists [2], and accuracy varies widely [3]. A promising approach to maintain interpretative accuracy in this “deluge” of data is to integrate computer-based assistance into the image interpretation process. Many general-purpose image retrieval systems have been proposed in the literature [4]. Among these systems, an emerging technique that may assist radiology interpretation is content-based image retrieval (CBIR) [5]. This framework assists users in finding similar images within large collections of images. For medical purposes, the role of CBIR is powerful: in addition to enable similarity-based indexing, it could provide computer-aided diagnostic support based on image content and on other meta-data associated with images.The main idea of CBIR is to search for similar images based directly on their visual contents. Image retrieval is usually performed by image example, where a query image is given as input and an appropriate distance is used to find the best matches in the corresponding feature space [6]. In general, images are indexed using quantitative features extracted from regions of interest (ROI) of the images (e.g., lesions) and focus on their contents (e.g., shape, texture) [7]. Although these features are powerful to describe the image content in an automated fashion, they are often not discriminating enough to comprehensively characterize medical images. In addition, the performance of most CBIR systems is constrained by the low-level properties of these features because they cannot efficiently model the user’s high-level expectations [8] (referred to as the semantic gap problem). Since this problem remains unsolved, research in image retrieval focuses on new methods to characterize the image content with higher level semantics, closer to that familiar to the user and potentially more useful in retrieving similar-appearing images [9].In recent work on image retrieval that incorporates semantics, the images were characterized using a set of semantic terms [10–12] in a process referred to as “semantic annotation”. Such terms can be directly derived from the terminology provided by the radiologists in their reports [13] or automatically predicted from computational imaging features [14]. The semantic terms can be used to describe a variety of information about the image content (e.g., lesion shape, patterns of enhancement), and they are directly linked to the user’s high-level understanding and descriptions of image features [15] (Fig. 1). These terms can improve diagnostic decision making by enabling radiologists to search databases of images for cases that are similar in terms of shared high-level imaging features to the cases which they are working on. Based on these considerations, incorporating semantic features into CBIR systems can be a promising attempt to bridge the semantic gap between the visual description of an image and its meaning [16]. However, most of the existing CBIR strategies do not consider the intrinsic semantic properties of the terms during the comparison of the images. Consequently, there is an unmet need that we propose to address by presenting a new framework that includes semantic features in images and that enables retrieval of similar images in large databases.When images are described using semantic terms, they are usually modeled as a set of terms, referred to as “bag-of-words” (BOW) [17]. BOW models have been successfully used in natural language processing [18] to capture a summary of the semantics of text based on word content. BOW models are represented as vectors of numerical (or binary) values, where each element represents the probability of occurrence (or presence/absence) of a term. Most of the classical BOW approaches assume that every term describing an image is independent of other features – that there are no intrinsic relations between the words that are contained in a bag. However, such an assumption is often oversimplified; while in text the independence assumption is reasonable, in the case of images annotated with diverse semantic features, this assumption is problematic [19]. The relations among terms are crucial since these features usually have a strong semantic correlation with each other (e.g., relations between anatomy, imaging observations).Consider an example of medical image clustering with three imagesIA-C. Each image contains a lesion in the liver. We use a basic vocabulary to describe the lesion shape appearing in these images: {ovoid, round, irregular}. The three images are annotated as presented:IAis annotated with ovoid;IBis annotated with round; andICis annotated with irregular. By considering a classical BOW strategy, these three images could be represented in a 3-dimensional space asIA=(1,0,0),IB=(0,1,0)andIC=(0,0,1). Fig. 2(a) shows this representation in a classical Euclidean space. By considering this representation, it is difficult to group the three images into relevant clusters of interest because each image is equidistant to every other image. However, we know intuitively that the imagesIAandIBare more similar than the imagesIAandIC(resp.IBandIC) since the terms ovoid and round are semantically closer than the terms ovoid and irregular (resp. round and irregular). Therefore, if we no longer hold the terms in the bag as orthogonal, it seems natural to “bend” the axis and thus enable easy clustering them into two groups (Fig. 2(b)).This basic example highlights the need of considering the semantic relations of the terms for image retrieval purpose. Although several efforts have been conducted in computer vision [20,21] and medical imaging [22] to integrate semantics into image retrieval applications, most of the proposed approaches are dedicated to automatic annotation of the images with semantic terms, and they do not focus on the potential relations among the semantic terms during the retrieval step. Consequently, there is an opportunity to improve image retrieval applications by considering these semantic aspects. To this end, our approach to improving CBIR considers the semantic relations between the terms when assessing the distance between images described with BOW. Indeed, assessing the distance between vectors is the basis of determining the similarity in most medical information retrieval applications [23,24]. However, to enable the integration of such semantic relations into a distance function, we have to find solutions to three major problems: (1) how to model the relationship between the terms, (2) how to quantify a semantic proximity between terms, and (3) how to use these relations in computing the distance between images described as BOW.Recent works in information retrieval [25] have shown that considering controlled vocabularies, such as ontologies, for image annotation can open up new research directions to deal with these problems. Ontologies can be used to model the relations between terms and can provide a solution to deal with our first issue. In addition, the problem of quantifying a semantic proximity between terms belonging to an ontology has been studied in the field of natural language processing [26]. Numerous kinds of measures have been proposed to assess the semantic similarity between terms and can be used as robust solutions to our second issue. Finally, in the domain of histogram comparison, new distances resting on hierarchical merging strategies have been proposed to consider the relative proximity between the bins avoiding bins correlation issues. The cornerstone of these distances is to consider the (intrinsic) multilevel semantic correlations between the distributions modeled by the histograms. Such distances could then be adapted to consider semantic similarity between terms when assessing the distance between BOW and can be considered as a solution to our last issue. Consequently, it appears that, by coupling these three strategies there is an opportunity to consider term semantic relations when comparing BOW in order to improve image retrieval applications.We describe a new semantic framework, devoted to the retrieval of similar medical images in large databases. The main idea is to annotate the images using terms belonging to a medical ontology and to consider the semantic ontological relations among these terms when comparing the images for visual similarity. By exploiting linguistic relationships between semantic features, this framework could lead to more accurate radiology reporting and diagnoses.This article is organized as follows. Section 2 proposes a state of the art about semantic proximity and vector distances, as well as the different contributions that we present in this article. Section 3 describes the proposed semantic framework, dedicated to the comparison of medical images described with semantic terms. Section 4 gathers experiments enabling to assess the relevance of this framework. Conclusions and perspectives will be found in Section 5.Image annotation is a complex task that has been widely studied in the domains of computer vision and image retrieval. System performance relies on the choice of the terms being used to describe the content of the images: this choice is highly dependent of the application, the users needs, and the user experience. Consequently, it leads to different possible descriptions for a same image, thwarting good performance of CBIR systems based purely on semantic image descriptions. To deal with this issue, recent works in the semantic domain [27] used controlled vocabularies for annotating the images. A controlled vocabulary provides a set of pre-defined terms with definitions that can facilitate the annotation of large sets of images since it provides standard terms for describing the features in images. In medical imaging, recent works [11] have investigated computer-aided methods to support diagnosis by providing a database of annotated images that can be retrieved by similarity, which further indicates that semantic annotations from a controlled vocabulary can lead to more accurate diagnoses.Ontologies, which are related to controlled terminologies but also provide explicit specification of relations among terms, provide a formal way to model knowledge [28]. As they are machine-accessible and usually built from a consensus of domain experts, they represent a powerful way to structure semantic terms belonging to a particular knowledge source. In the context of medical imaging, numerous ontologies are being developed to organize biomedical concepts in a comprehensive manner (e.g., Medical Subject Headings (MeSH), International Classification of Diseases (ICD taxonomy), Systematized Nomenclature of Medicine – Clinical Terms (SNOMED CT), Unified Language of Radiology Terms (RadLex)) [29]. Ontologies specify different kinds of taxonomic relations among the terms (e.g., subtype/supertype, homonyms, synonyms relationship) and can be seen as an oriented graph in which semantic concepts are linked by taxonomic relations. As most medical ontologies contained subtype/supertype relations (i.e., is_a relations), we will focus on such relations in the remainder of the article.An important aspect of ontologies that makes them particularly valuable for CBIR is that their relations can be leveraged to compute semantic likeness between terms. Indeed, their hierarchical structures make it possible to directly assess a semantic proximity value between two terms belonging to the tree structure [30]. Fig. 3presents an extract of the RadLex ontology and the idea of evaluating similarity between terms belonging to a hierarchy. In the next section, we study and compare different measures that have been proposed in the literature to evaluate the proximity between semantic terms contained in an ontology.For the last decades, numerous kinds of measures have been proposed to assess the semantic similarity between terms belonging to an ontology [31]. Theses measures are generally gathered in three groups: Edge-based measures consist of directly inferring the semantic similarity between terms from the ontology structure [32]; Feature-based measures compute the semantic similarity between terms regarding the degree of overlap between sets of ontological features [33] and Measures based on information content exploit the notion of information content, by associating appearance probabilities to each concept in the taxonomy, computed from their occurrences in a text corpus [34].As specific ontological features and text corpora are not always available in the medical context (and their definitions require considerable human efforts), both feature-based measures and measures based on Information Content may not be well adapted in our applicative context in which more automated strategies are required. In addition, the authors of [35] have experimentally shown that the use of edge-based measures could lead to better results than the ones obtained with information content measures in the biomedical context. For all these reasons and for the sake of genericity, we will focus on the remainder of this section on the use of edge-based approaches to assess the semantic likeness between semantic terms belonging to an ontology.In order to quantify a semantic similarity value between terms, an intuitive edge-based method has been originally proposed by Rada et al. [36]. It calculates the similarity between the terms by computing the minimum path length connecting their corresponding ontological nodes via taxonomic links. The underlying idea is that the longer is the path, the more semantically far the terms are. Since the original definition of this measure, several improvements have been proposed in the literature [32]. One of the major improvement has been to also consider the relative depth of the terms in the ontology. Indeed, since a concept becomes more specialized as long as it is recursively refined in the hierarchy, the depth of a term in the ontology is an important dimension. Consequently, most of the proposed measures act by counting the number of taxonomic links from each term to their Least Common Subsumer (LCS) (i.e., the most concrete taxonomical ancestor that subsumes these two terms) and also the number of links of the LCS to the root of the ontology. Among them, Al-Mubaid and Nguyen have proposed in [37] a measure based on a cluster-based strategy that combines both the minimum path length and the taxonomical depth of the considered branches. The definition of this measure was also extended to deal with terms belonging simultaneously to multiple ontologies [38], enabling to evaluate the term similarity from complementary sources of knowledge.Since these measures enable to consider both the relative path length between the terms and their relative depth in the ontology, they present powerful and comprehensive properties. Furthermore, it has been shown in [35] that these measures outperform other existing ones for the comparison of semantic terms belonging to biomedical ontologies.In general, the similarity between images described with BOW is evaluated by computing the distances between these bags. The classical approaches are based on the vector space model (VSM) that has proved to be very popular in the domain of text retrieval [39]. Such approaches consider a BOW as a fixed-dimensional vector where each vector element represents the probability of occurrence of a term within a document.In general, vector similarity is evaluated using “element-to-element” distances that only compare the contents of the corresponding elements of the vectors (e.g., ManhattanDL1or EuclideanDL2distances). As they only compare corresponding vector elements, these distances can be computed linearly and can be used to measure similarities for large datasets. However, these distances ignore the potential semantic proximity between neighboring elements.Another way of evaluating vector similarity is to consider “cross-element” distances that compare corresponding vector elements as well as non-corresponding ones. Based on this property, these distances account for the semantic proximity between the vector elements. The Cosine-Similarity Measure (CSM) defines the similarity between two vectors to be the cosine of the angle between them, which is identical to the normalized inner product of the two vectors. This measure has proven to be very popular for query-document and document-document similarity in text retrieval [39]. Several extensions of the CSM dealing with hierarchical domain structure and semantic proximity have been proposed in the literature [40,41]. However, as their computation requires to compare each vector element to all the elements of the corresponding vector (quadratic in the worst case), their computational costs remain hardly tractable when measuring similarities for large datasets.In the domain of histogram comparison, “cross-bin” distances have been proposed to compare histograms in a “cross-element” fashion [42,43]. Practically, this can be done by assigning, to each pair of bins, a weight (i.e., a numerical value, called “ground-distance”) modeling the degree of semantic proximity between the compared bins. These weights are usually stored by using a (dis) similarity matrix [42]. Based on this paradigm, some of the authors of this article have recently proposed a cross-bin distance called HSBD (Hierarchical Semantic-Based Distance) resting on a hierarchical merging strategy [44,45]. Its computation relies on the iterative merging of the semantically closest bins of the histograms to create coarser histograms of higher semantic levels. This hierarchical strategy is the cornerstone of this distance: it enables to consider the (intrinsic) multilevel semantic correlations between the distributions modeled by the histograms. In addition, as the distance value is obtained by computing iteratively a chosen element-to-element distance, its computational cost is lower than quadratic costs required for cross-bin distances.This distance, which has been involved in image retrieval applications, has provided encouraging results. However, this distance has been proposed in the context of low-dimensional histogram comparison, and the ground-distances between the elements of the vectors have to be manually defined by the user using his/her background knowledge.Based on these methodological considerations, we propose in this article a new semantic framework, devoted to the retrieval of similar medical database images described with high-level semantic annotations. The contribution of the current study is threefold:•we employ semantic terms, which belong to a controlled vocabulary extracted from a biomedical ontology, for the annotation of radiological images. These terms can improve diagnostic decision making by enabling radiologists to search databases of images for cases that are similar in terms of high-level visual features to a new query image. The use of a controlled vocabulary for image annotation guarantees the comparability property of the database images characterized with semantic terms;we propose to consider the semantic similarity between terms during the retrieval of similar database images. To this end, we use an ontological edge-based measure, which enables to automatically quantify the semantic term similarity from the ontological structure, coupled to the HSBD distance, which takes into account these term similarities when comparing images described as BOW. This strategy provides a potential solution to the actual issues of BOW approaches that assume that every term describing an image is independent of other features;we extend the HSBD distance to enable the comparison of high-dimensional vectors of semantic features. The main advantage of this vector distance is to consider the semantic multiscale similarities among the terms when comparing images characterized with BOW, with a lower computational cost than the ones induced by the classical approaches in CBIR.Use of semantic information associated with images is not new. However the incorporation in medical image retrieval systems of semantic similarities in an ontological fashion is innovative and can be considered as a robust solution to the semantic gap problem. To show the interest of considering the relations among terms in the context of the retrieval of medical images, we propose to apply it to two different tasks: ranking and classification of computed tomographic (CT) images of the liver. We evaluate the effectiveness and the gain of considering the HSBD distance combined to ontological relations to retrieve relevant similar images in a database compared to other distances of the state-of-the-art.

@&#CONCLUSIONS@&#
