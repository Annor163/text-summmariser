@&#MAIN-TITLE@&#
Initial assessment of reliability of a self-administered web-based neuropsychological test battery

@&#HIGHLIGHTS@&#
Web-based tests showed reliability comparable to traditional and computerized tests.Processing speed had higher reliability estimates than memory measures.A two week retest interval produced significant practice effects on all tests.Practice effects were more pronounced on the memory measures.

@&#KEYPHRASES@&#
Computerized,Intraclass correlation,Internet,Cognitive,Memory,

@&#ABSTRACT@&#
IntroductionWeb-based neuropsychological testing can be an important tool in meeting the increasing demands for neuropsychological assessment in the clinic and in large research studies. The primary aim of this study was to investigate practice effects and reliability of self-administered web-based neuropsychological tests in Memoro. Due to lack of consistent analysis and reporting of reliability in the literature, especially intraclass correlation coefficients (ICC), we highlight how using different ICC measures results in different estimates of reliability.Method61 (31 females) participants (mean age 53.3 years) completed the Memoro tests twice with a median of 14 days between testing.ResultsPractice effects were detected for all cognitive measures (d = 0.32–0.61), most pronounced for memory measures. Reliability estimated using two-way random effects single measure absolute agreement ICC(2,1) were between 0.55 and 0.74. Two-way mixed effects average measure consistency ICC(3,2), ranged from 0.79 to 0.89. Reliability was highest for the processing speed task and lower for the memory tasks.ConclusionsMemoro tests had test-retest reliability similar to that of traditional, computerized and web-based test batteries used clinically and in research. It is important to carefully choose and specify the ICC implemented, as ICC(2,1) and ICC(3,2) give different results and reflect reliability of different measures.

@&#INTRODUCTION@&#
Today neuropsychological testing is moving from its traditional pen-and-paper format with an examiner to computers and even further to self-administrated web-based testing (Bilder, 2011; Resch, McCrea, & Cullum, 2013; Zygouris & Tsolaki, 2014). Web-based testing is gaining in popularity as it is flexible, inexpensive, and has no geographical boundaries (Darby, Fredrickson, Pietrzak, Maruff, & Woodward, 2014; Haworth & Harlaar, 2007). Web-based testing was embraced early by sports medicine and for concussion management due to its mobile and flexible nature (Erlanger et al., 2003).Web-based tests are also being used in assessment of aging, mild cognitive impairment and dementia related cognitive changes (Darby et al., 2014; Dougherty et al., 2010; Trustram Eve & de Jager, 2014), other disease-related cognitive changes (Medalia, Lim, & Erlanger, 2005), and more generally in individual and potential large-scale assessments of cognitive functions (Silverstein et al., 2007). Web-based tests identify the same cognitive constructs as traditional tests, and are even preferred above traditional tests by persons who are tested (Hansen, Haferstrom, Brunner, Lehn, & Håberg, 2015). Web-based tools might facilitate large cohort studies and help meet the increasing demand for cognitive assessment associated with the aging population and accompanying cognitive changes and disease. The International Association of Gerontology and Geriatrics (IAGG) recently recommended that all individuals ≥ 70 years should have their memory tested at least once annually (Morley et al., 2015). Such a change in clinical practice will require large resources, and computerized or web-based testing is one way to meet the new demands.It is imperative that web-based tests can document good psychometric properties, as these are critical for the tests’ usability both in the clinic and in research. While the reliability of computerized tests has been well documented, the test-retest reliability of web-based tests remains underexplored (Gates & Kochan, 2015; Zygouris & Tsolaki, 2014). Furthermore, reliability estimates are in some cases only available for the computerized version and not the web-based version. This is problematic as it can be argued that when the manner of administration changes, the test becomes a new test (Bauer et al., 2012).A challenge in the literature on web-based testing is the lack of consistent assessment of test-retest reliability. Pearson’s r or one of the several intraclass correlation coefficients (ICCs) are commonly reported, alternatively Spearman’s rho if the statistical assumptions of the Pearson’s r and ICC are not met. Since Pearson’s r is insensitive to systematic error (e.g. practice effects or fatigue effects) using an ICC method has been recommended because it takes into account both the within-subject change and the systematic group change over time (Vaz, Falkmer, Passmore, Parsons, & Andreou, 2013). This recommendation is, however, not universally accepted as it can be argued that practice effects are not a flaw to be corrected, but rather a natural phenomenon (Rousson, Gasser, & Seifert, 2002). Adding further complexity to this picture, there exist multiple ICC variants with regard to both the computational processes and nomenclature (McGraw & Wong, 1996; Shrout & Fleiss, 1979), leading to a multitude of measures. These variations have caused confusion in the field. Furthermore, lack of specification of ICC variant used, and application of incorrect ICC variants in the literature can lead to misconstrued impressions of the reliability of the tests (Bruce, Echemendia, Meeuwisse, Comper, & Sisco, 2014; Krebs, 1986; Weir, 2005).We have developed Memoro, a self-administered web-based neuropsychological test platform for assessment of memory and related cognitive functions. We have recently investigated concurrent validity (Hansen et al., 2015). However, no test is valid unless it is reliable.The primary aim of this study was to assess practice effects and reliability, i.e. consistency of test results across administrations, of the self-administered web-based tests in Memoro. The second aim was to highlight how different variants of intraclass correlation results in different estimates of reliability. As noted above, the literature on reliability has not always been precise or correct in its use of ICC. This has significant impact on the reliability measurement as illustrated using test-retest data from Memoro.

@&#CONCLUSIONS@&#
The results of the present study showed that the selected Memoro tests have test-retest reliability similar to traditional, computerized and web-based test batteries used in the clinic and in research. Three weeks or shorter test-retest intervals are not recommended for the Memoro memory measures unless using alternative stimuli sets, due to practice effects reducing the stability of performance, especially if absolute differences are of interest. Explicit specification of which ICC variant is used and whether single or average measures are reported, and providing a rationale behind this choice is very important. Our results showed that the discrepancy between the different measures can be large, thus significantly influencing the interpretation of the test scores and how reliable a test is perceived to be. While Memoro shows reliability coefficients similar to other batteries, a higher level of reliability is the ideal, especially for clinical use. In the future development and quality assurance of Memoro it will be important to investigate and address technical and human factors that may have contributed to measurement error and reduced reliability.