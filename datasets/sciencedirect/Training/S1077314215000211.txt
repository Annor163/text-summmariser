@&#MAIN-TITLE@&#
Discriminative structured dictionary learning with hierarchical group sparsity

@&#HIGHLIGHTS@&#
A powerful discriminative dictionary learning method is proposed.Our method is built upon collaborative hierarchical group sparsity.Dictionary and classifier are simultaneously learned.An efficient alternating iterative scheme is presented.Our method has shown excellent performance on several image recognition tasks.

@&#KEYPHRASES@&#
Discriminative dictionary learning,Structured sparse coding,Group sparsity,Image classification,

@&#ABSTRACT@&#
Learning adaptive dictionaries for sparse coding has been the focus of latest research as it provides a promising way to maximize the efficiency of sparse representation. In particular, learning discriminative dictionaries rather than reconstructive ones has demonstrated significantly improved performance in pattern recognition. In this paper, a powerful method is proposed for discriminative dictionary learning. During the dictionary learning process, we enhance the discriminability of sparse codes by promoting hierarchical group sparsity and reducing linear prediction error on sparse codes. With the employment of joint within-class collaborative hierarchical sparsity, our method is able to learn adaptive dictionaries from labeled data for classification, which encourage coefficients to be sparse at both group level and singleton level and thus enforce the separability of sparse codes. Benefiting from joint dictionary and classifier learning, the discriminability of sparse codes is further strengthened. An efficient alternating iterative scheme is presented to solve the proposed model. We applied our method to face recognition, object recognition and scene classification. Experimental results have demonstrated the excellent performance of our method in comparison with existing discriminative dictionary learning approaches.

@&#INTRODUCTION@&#
In recent years, sparse representation has drawn much attention from the computer vision community and led to state-of-the-art results in many computer vision tasks, e.g. image classification [1–4] and image restoration [5–8]. The success of sparse representation based classification is attributed to the fact that high-dimensional image data from the same class lie on a low-dimensional manifold and thus can be coded using a few representative elements (the so-called atoms). The collection of such elements is often referred to as a dictionary.The choice of dictionary is one of the fundamental considerations in employing sparse representation based models. While predefined dictionaries such as off-the-shelf bases like wavelets [5,7] have been successfully applied to sparse modeling in many signal processing applications, many reconstructive dictionary learning methods [9–12] have shown that noticeable performance improvement can be obtained by learning adaptive dictionaries from data themselves. The learned reconstructive dictionaries are adapted to the underlying structures of data and hence are able to improve the efficiency of sparse coding.However, the reconstructive dictionaries often suffer from the insufficiency of discrimination in complex recognition tasks. In fact, a dictionary becomes useful for sparsity-based recognition when it not only enjoys excellent sparse-representational power but also has the ability to induce discriminative representation for samples from different categories. As a result, many discriminative dictionary learning methods [13–25] have been proposed to learn both reconstructive and discriminative dictionaries for sparse coding.In this paper, a powerful discriminative dictionary learning method is proposed for sparse coding based classification. In the proposed method, we simultaneously learn a structured dictionary with hierarchical group sparsity and train a linear classifier for classification. By promoting joint within-class collaborative hierarchical sparsity in sparse codes, our method is able to learn dictionaries adapted to the underlying structures of data. The learned dictionaries encourage samples from different categories to exhibit distinct hierarchical group sparsity patterns, making the sparse codes more separable between classes. Meanwhile, benefiting from joint dictionary learning and classifier training, the learned dictionaries are both reconstructive and discriminative. As a result, the discriminability of sparse codes and the discrimination of active groups of dictionary atoms are further strengthened. Experimental results on face recognition, object recognition and scene classification have demonstrated the excellent performance of our method in comparison with many state-of-the-art discriminative dictionary learning methods.

@&#CONCLUSIONS@&#
Learning discriminative sparse representations from labeled data has drawn much interest in computer vision community. In order to obtain efficient representations with structured sparsity and strong discriminability for image classification, we introduced the concept of collaborative hierarchical group sparsity and the integration of classification feedback into discriminative dictionary learning. Our dictionary learning model is constructed by combining reconstruction error, joint within-class hierarchical group sparsity and linear prediction error into a unified minimization framework. Benefiting from using the joint within-class collaborative hierarchical sparsity, our method encourages signals from the same category to share the same hierarchical sparsity patterns, which promotes the separability of the sparse codes associated with different signal categories. The discriminability of sparse codes is further strengthened by joint dictionary construction and classifier learning. An efficient alternating iterative scheme is developed to solve the proposed model. Our method is applied to image classification by simultaneously learning dictionary, sparse representation and classifier from image features. We applied our method to face recognition, object classification and scene recognition. The experimental results have demonstrated the excellent performance of our method. In future, we would like to investigate higher-level structured sparsity for discriminative dictionary learning.Two Effective solvers for the ChiLasso Problem (2) have been proposed by Sprechman et al. in [36,39]. We denote these two solvers as ChiLasso-Solver-A and ChiLasso-Solver-B respectively.ChiLasso-Solver-A[36]. The basic idea of ChiLasso-Solver-A is to use ADMOM [59] iterations to divide the overall sparse coding problem into two subproblems: (1) breaking the multi-signal case into p single-signalℓ1regressions; (2) treating the multi-signal case as a single group Lasso-like problem. For this purpose, we rewrite the ChiLasso Problem (2) as a constrained optimization model(A.1)argminC‖Y-DC‖F2+λ1∑i‖ci‖1+λ2ψS(B)s.t.C=B.Then the ADMOM iterations for solving (A.1) are given as follows:(A.2)C(t+1)=argminC12‖Y-DC‖F2+λ1∑i‖ci‖1+Tr(C⊤P(t))+θ2‖B(t)-C‖F2;(A.3)B(t+1)=argminBθ2‖B-C(t+1)‖F2+Tr(B⊤P(t))+λ2ψS(B);(A.4)P(t+1)=P(t)+θ(C(t+1)-B(t+1)).The problem of (A.2) is signal-signal separable and thus can be solved by updatingCcolumn by column, i.e.(A.5)ci(t+1)=argminc12‖yi-Dc‖22+λ1‖ci‖1+pi⊤c+θ2‖bi(t)-c‖22,which can be solved by applying SpaRSA [60]. The problem of (A.3) is group separable and thus can be separated into|S|optimization problems in vectorial form as follows:(A.6)argminfλ2‖f‖2-q⊤f+θ2‖z-f‖22,wheref,zandqare column vectors by concatenating the columns ofB(Pk),C(Pk)(t+1)andP(Pk)(t)respectively. This minimization problem can be solved by simple vectorial thresholding, i.e.,(A.7)f=max{0,‖z+θq‖2-λ2}θ‖z+θq‖2(z+θq)if‖z+θq‖2>00if‖z+θq‖2=0.ChiLasso-Solver-B[39]. In [39], the SpaRSA framework is employed to generate a sequence of iterates{C(t)}t∈N, which converges to the solution of (2) under certain conditions. At each iteration,C(t+1)is obtained by solving:(A.8)minZ12‖Z-V(t)‖22+λ1α(t)∑i‖zi‖1+λ2α(t)ψG(Z),whereV(t)=[v1(t),…,vp(t)]is defined a matrix with its ith column given byvi(t)=ci(t)-1α(t)DT(Dci(t)-yi), and{α(t)}t∈Nis some sequence of parameters withα(t)∈R+which determine the convergence conditions about the algorithm. In the aforementioned formulation, all terms in the cost function can be group separable. Thus, the problem of (A.8) can be solved independently for each group, that is(A.9)C(Pk)(t+1)=argminZ12‖Z-V(Pk)(t)‖F2+λ1α(t)∑i‖zi‖1+λ2α(t)‖Z‖F.The sub-gradient of (A.9) for the case where the optimumZ∗≠0is inspected asV(Pk)(t)-(1+λ2α(t)‖Z∗‖F)Z∗∈λ1α(t)∂‖Z∗‖1. It can be observed that each element of(1+λ2α(t)‖Z∗‖F)Z∗is the solution of the well known scalar soft thresholding operator. We can setG=Tλ1α(t)(V(Pk)(t)), whereTλ(X)denotes the matrix obtained when applying the soft-thresholding operator with parameterλto each element ofX. From the information above, it can be easily inferred that‖Z∗‖F2=‖Z∗‖F2(‖Z∗‖F+λ2α(t))2‖G‖F2and then obtain‖Z∗‖F=‖G‖F-λ2α(t). Since all terms are positive, this can only hold as‖G‖F>λ2α(t), which shows a vectorial thresholding condition on the solutionZ∗in terms of‖G‖F. It’s easy to show that‖G‖F<λ2α(t)is a sufficient condition forZ∗=0.Therefore, the corresponding closed-form solution for each subproblem (A.9) is given by(A.10)C(Pk)(t+1)=max{0,‖G‖F-λ2α(t)}‖G‖FGif‖G‖F>00if‖G‖F=0.This ChiLasso-Solver-B provides such solution in closed-form, requiring just two thresholding, both linear in the dimension ofY.