@&#MAIN-TITLE@&#
A variance-based Bayesian framework for improving Land-Cover classification through wide-area learning from large geographic regions

@&#HIGHLIGHTS@&#
A wide-area learner.Efficient sampling for training.Classify with variances.

@&#KEYPHRASES@&#


@&#ABSTRACT@&#
Common to much work on land-cover classification in multispectral imagery is the use of single satellite images for training the classifiers for the different land types. Unfortunately, more often than not, decision boundaries derived in this manner do not extrapolate well from one image to another. This happens for several reasons, most having to do with the fact that different satellite images correspond to different view angles on the earth’s surface, different sun angles, different seasons, and so on.In this paper, we get around these limitations of the current state-of-the-art by first proposing a new integrated representation for all of the images, overlapping and non-overlapping, that cover a large geographic ROI (Region of Interest). In addition to helping understand the data variability in the images, this representation also makes it possible to create the ground truth that can be used for ROI-based wide-area learning of the classifiers. We use this integrated representation in a new Bayesian framework for data classification that is characterized by: (1) learning of the decision boundaries from a sampling of all the satellite data available for an entire geographic ROI; (2) probabilistic modeling of within-class and between-class variations, as opposed to the more traditional probabilistic modeling of the “feature vectors” extracted from the measurement data; and (3) using variance-based ML (maximum-likelihood) and MAP (maximum a posteriori) classifiers whose decision boundary calculations incorporate all of the multi-view data for a geographic point if that point is selected for learning and testing.We show results with the new classification framework for an ROI in Chile whose size is roughly 10,000 square kilometers. This ROI is covered by 189 satellite images with varying degrees of overlap. We compare the classification performance of the proposed ROI-based framework with the results obtained by extrapolating the decision boundaries learned from a single image to the entire ROI. Using a 10-fold cross-validation test, we demonstrate significant increases in the classification accuracy for five of the six land-cover classes. In addition, we show that our variance based Bayesian classifier outperforms a traditional Support Vector Machine (SVM) based approach to classification for four out of six classes.

@&#INTRODUCTION@&#
Our interest in pixel-level classification of satellite images11In reality, pixel-level classification of a satellite image amounts to carrying out land-type classification of an array of points in that portion of the earth’s surface that is viewed in the image. Before classification, the multispectral data in a satellite image goes through a processing step called orthorectification that “maps” the array of pixels in an image to an array of latitude/longitude (lat/long for short) coordinates.is driven by the role that such classifications can play in solving problems related to the geolocalization of everyday photographs and videos of outdoor scenes. Using spatial relationships between different land-types - say between arable land, a pond, and a nearby road - to establish correspondences between the “objects” seen in a photograph and those extracted from the satellite images can significantly reduce the candidate locations for where the photograph was taken. For obvious reasons, reliable pixel level classification of satellite data is a necessary prerequisite to the development of such solutions to geolocalization problems.While there has been much work during the last several decades on the classification of multispectral data in satellite images Anderson (1976); Unsalan (2003), as such this work cannot directly be used for solving geolocalization problems in general. Before we explain the reasons for why that is the case, note that the traditional approaches to satellite data classification use standard statistical pattern recognition techniques, with the more recent contributions also using decision trees, random forests, neural networks, support vector machines, and so on DeFries and Chan (2000); Duro et al. (2012); Huang et al. (2002); Marchisio et al. (2010).One thing that is common to practically all these traditional approaches is that the training and the testing data for constructing a classifier are drawn from the same satellite image. There are very few examples where authors have derived the decision boundaries in one satellite image and shown usable classification results in a different satellite image in a given geographic area.22We draw the reader’s attention to Wilkinson’s survey Wilkinson (2005) whose major conclusions are just as valid today as they were when the survey was published in 2005. This survey covered 574 experiments in satellite image classification as reported in 138 publications over a period of 15 years. Wilkinson concluded that despite the large body of published research, virtually no progress had been made in satellite image classification over the time period covered by the survey. One of the reasons he highlighted for this lack of progress was the common practice of the researchers drawing their training and testing data sets from the same satellite image.In practically all these cases, the classifiers derived for one satellite image fail to perform adequately on the other satellite images even in the same general geographic region. And, when such results have been shown, it is usually for an adjacent area for which the satellite images were captured at the same time as for the image from which the training data was drawn.While these traditional approaches may suffice for solving, say, land-cover resource management and forecasting problems, they come up short when solving problems related to the geolocalization of photographs and videos. The reason has to do with the fact that the data in satellite images is affected by the change of seasons, the off-nadir/elevation angle associated with a satellite view, the sun angle, and so on. The logic of the algorithms that one might use for geolocalizing a photograph becomes simpler if the information extracted from the satellite images is invariant to all of the aforementioned changes to the maximum extent possible. The easiest way to achieve such invariance is to use ALL of the satellite data that may be available for a given geographic region. When decision boundaries in a feature space are based on all of the data - meaning data recorded at different times of the year, with different off-nadir/elevation angles, with different sun angles, etc. - any discriminations one is able to make in that feature space are likely to possess the desired properties of invariance. Said another way, our classifier would be able to make distinctions between the variances associated with the objects that look more or less the same around the year and the variances associated with the objects that change significantly with, say, seasons.An additional aspect related to using satellite imagery for solving the problems of geolocalization is that a photograph (or a video) may have been recorded anywhere in a large geographic region of interest whose area may far exceed what is typically associated with a single satellite image. This requires that the land-cover classifications be carried out for the entire ROI using all of the satellite data available for the region.For reasons stated above, this paper addresses the problem of land-cover classification from a larger geographical perspective than has traditionally been the case in the past. We want to be able to classify all of the data in an ROI (Region of Interest) that can be much larger than the area covered by a typical single satellite image. Consider, for example, the Chile ROI shown in Fig. 1. This ROI, of size 10,000 km2 is covered by a total of 189 satellite images in the WorldView2 dataset. Our goal is to see if it is possible to create decision boundaries from all of this data taken together so that the overall classification rate for the entire ROI shown in Fig. 1 would be at a usable level of accuracy.Obviously, before we can design a classifier at the level of an ROI, we must first come to grips with the data variability over the ROI. Understanding data variability at the scale of a large ROI presents its own challenges and can be thought of as a “Big Data” problem. The challenges are created by the typical fast-response and dynamic-storage needs of any human-interactive computer system that must work with very large variable-sized datasets.33By very large, we mean datasets that are hundreds of gigabytes in size.We have addressed these challenges by developing a special software tool (named PIMSIR for “Purdue Integrated MultiSpectral Image Representation” tool) that is custom designed to achieve the following:•Rapid visualization of all of the data in an ROIRapid visualization of the geographic area overlaps between the satellite images. Understanding the overlaps is important because any probabilistic modelling of the data at any given geographic point is predicated on how much data is available at that point through overlapping satellite views.44In general, probabilistic modelling is with respect to spatial distribution of the observed data. However, in order to address view-to-view data variability issues, one can also talk about probabilistic modeling with respect to the viewing dimension.Rapid visualization of the variability of the spectral signatures55We use the term spectral signature to refer to the 4 or 8 spectral band values at each pixel.both spatially and across the views.This tool is run on a cloud-based cluster of five physical computing nodes, each with up to 48 cores and 256 GB of RAM, that are connected with a 10 Gb network switch. The system is supported by a network storage server with 24 TB of storage.Even after understanding the extent of data overlap and variability, there remains the big issue of what classification strategy to use for the data. Machine learning now gives us tens of choices for classifiers and it’s not always clear at the outset as to which choice would work the best for a given problem. In this paper, based on our analysis of the data overlap and of the view-to-view variability that we have seen, we chose to design ML (maximum-likelihood) and MAP (maximum a posteriori) classifiers by probabilistic modeling of NOT the spectral signatures themselves, but of the variability in the spectral signatures. We show our ROI based results obtained with this Bayesian classifier, and for comparison, also with the more commonly used SVM classifiers in Section 8 of this paper. As the reader will see, this comparison justifies our intuition regarding the superiority of variance-based Bayesian classification vis-a-vis the more traditional approaches (as exemplified by SVM-based classification).Moreover, even after a choice is made regarding the classification strategy, there remains the complex problem of how to generate on an ROI basis the positive and negative examples for the different land-cover types for training and testing a classifier. Obviously, it would be very challenging for a human to scan through an entire ROI and manually select such examples. What is needed is a human-in-the-loop random sampling strategy that has the power to yield positive and negative samples that adequately represent ROI based distributions for the different land-cover types. In our classifier training protocol, we use a random sampler based on the Metropolis-Hastings algorithm to select a small number of ROI subregions for presentation to a human and it is for the human to decide whether or not to use that subregion for generating the positive examples for a given land-cover label. (What if a randomly selected subregion is mostly over water while the human is seeking positive examples of high vegetation?) After the human has accepted a subregion, he/she can zoom into the subregion and with quick mouse clicks mark the positive and negative examples. This entire process takes about 30 minutes for each land-cover label for a large ROI of the size that we will show experimental results on later in this paper.In the rest of this paper, Section 2 presents the details of the PIMSIR tool for understanding data overlaps and variability in all the satellite images available for a given ROI. Section 3 then presents the ROI-based protocol we use for generating the positive and the negative examples for training and testing the classifiers. Subsequently, in Section 4 we discuss the theoretical framework used for probabilistic modeling of the variances and how these probabilities can be used for ML and MAP classifiers for land-cover classification. Section 5 briefly describes the SVM classifiers that we used to compare our Bayesian classifier with. Section 6 discusses the notion of “compatible classes.” The point here is that certain types of geographical points can be expected to show predictable seasonal variations that are best handled by defining a set of compatible classes. This is particularly the case for agricultural land. So we suggest that the geographical points that possess this property be given compound land-class labels that reflect these predictable variations. For experimental validation, we first present an overview of how the experiments were conducted in Section 7 and then proceed to show the actual results in Section 8. The results shown demonstrate that ROI based wide-area learning significantly outperforms single satellite image based learning using both our Bayesian and the more traditional SVM classifiers. In addition we show that our Bayesian classifier outperforms the traditional SVM based classification for four out of six classes.As alluded to in the Introduction, we have created a software tool called PIMSIR for the purpose of understanding data variability at the ROI level. On the one hand, PIMSIR gives us a holistic representation of all the satellite images that cover an ROI, and, on the other, it allows the data provided by multiple satellite images in any given subregion to be viewed and manipulated easily regardless of whether that subregion has to pull in information from overlapping or non-overlapping images.What makes PIMSIR versatile is that the data structure it creates for an ROI is dynamic - in the sense that an ROI is allowed to be covered by an arbitrary number of satellite images, with arbitrary degrees of overlap between the images. As to the size of the ROI that can be accommodated in this representation, that depends on the number of images. For example, the Chile ROI covers about 10,000 km2 and consists of 189 overlapping images, each with a spatial resolution of two meters per pixel.66Most of the satellite images in this dataset contain four band data: Red, Blue, Green, and Near Infrared (NIR). We plan to incorporate satellite images with arbitrary number of bands in the next version of PIMSIR.Its corresponding PIMSIR structure takes 208 GB of disk storage. As mentioned in the Introduction, PIMSIR runs on a cloud-based cluster of high-performance physical nodes, each with up to 48 cores and 256 GB of memory. This allows a large ROI to be accommodated entirely in the fast memory of just one node. For obvious reasons, that speeds up the construction of the PIMSIR structure. However, it is not a necessary requirement that an entire ROI fit in the fast memory of a single node.In what follows, we will describe how we create the PIMSIR structure for a given ROI. Subsequently, we will describe the information that is stored in PIMSIR for each geographical point in the ROI.Given a raw WorldView2 Multispectral Image (MSI), the first necessary preprocessing step is to apply the Top-of-Atmosphere (ToA) reflectance correction Updike and Comp (2010) to the images in order to normalize out the view-angle (with respect to the sun angle) variability. Then, the corrected image goes through the orthorectification process that maps the pixels to geo lat/long coordinates.77The satellite images used for the experiments in this paper were orthorectified by Ryan Smith and Nathan Campbell of GeoEye using the functionality provided by the publicly available GDAL library. The orthorectification formulas that convert pixel coordinates of the raw satellite images into lat/long coordinates utilize a DEM (Digital Elevation Map) model of the portion of the earth’s surface that corresponds to the satellite image. These formulas are iterative and involve various approximations for warping the image data onto the earth’s surface, in addition to the interpolation needed to create a uniformly sampled array in the lat/long coordinates. The errors in orthorectification computations are exacerbated by any errors in the DEM model. For this study, the GMTED2010 DEM at 7.5 arc-sec resolution was used. One could achieve better orthorectification using carefully selected ground control points, but this was not done for the images used in this study. The ToA reflectance correction is based on an implementation of the algorithm in Updike and Comp (2010) by Craig Stutts of ARA.After all of the corrections mentioned above have been applied to the data, creating the PIMSIR structure involves the following steps:1.Construct a bounding box for an ROI.Rasterize the bounding box with a matrix of sampling points, taking into account implicitly the spatial resolution desired. The results we show in this section are for the case when each cell in the bounding box represents a 2m x 2m area.Scan the bounding box and, for each cell, calculate its geo lat-long coordinates.Fetch the list of satellite images for the lat-long coordinates at a sampling point. Project the bounding-box sampling point into each image and:•Record the four MSI pixels that are nearest to the projected point in the satellite image. (The point projected into an image will, in general, not correspond exactly to any of the pixel locations in the image.)Apply bilinear interpolation to the spectral signatures at the four nearest neighbors and return this answer for the image in question.Pool together the spectral signatures collected from all the satellite images that see the geo-point and store them in a compact data structure whose pointer is held by the bounding-box point in question.Each location in the PIMSIR array points to a linked list of nodes, the number of nodes being equal to the number of images that can see that location. At each node, we allocate N bytes for the spectral signatures extracted from the corresponding satellite image, where the value of N is declared in the header segment of the file that carries the .pimsir suffix. In our current implementation, we useN=17bytes per satellite image. Four of these bytes are reserved for the spectral value in each of the four primary bands, and one byte reserved for the pointer to a file that contains the meta data for that satellite image.88This obviously limits us to a maximum of 256 overlapping satellite images for any geo-point. We have yet to see a case where that condition would be violated. Nonetheless, in order to “future-proof” PIMSIR, we plan to use two bytes for the pointer to the metafile in the next version of PIMSIR.For an ROI of size, say, 10,000 km2, the resulting representation may take a couple of hundred gigabytes - the precise value depending on how many satellite views need to be accommodated. Should this amount of memory not be available as RAM, we can always resort to using the virtual memory address space with the help of POSIX functions like mmap with some loss of performance with regard to the time taken for construction of the integrated representation.Given the PIMSIR representation for an ROI, one can then investigate how the data varies with respect to any of the independent variables such as the view angle, the time of the year, etc., as the reader will see in the rest of this paper. What is even more significant, this representation allows for fast construction of probability distributions of special signatures over an ROI. Subsequently, these distributions can be plugged directly into various machine learning algorithms for the learning of classification boundaries.PIMSIR makes it very convenient to view how much data is available in the different subregions of an ROI. This is an important issue since the applicability of the different types of approaches for ROI-based pixel and object classification depends on the extent of the data that is available at the ground level. To elaborate, suppose all of the geographical points in a subregion were covered by, say, a single satellite view, that subregion would not lend itself to the application of multi-view logic as described in Section 6.Using the PIMSIR representation, this section shows the data variability results for the Chile ROI. PIMSIR for the Chile ROI was constructed using all 189 satellite images that intersect with this ROI. Just for the purpose of visualization, what is displayed in Fig. 1 is the RGB part of the spectral signature in just the first satellite image at each point of the ROI-based bounding box. The satellite images relevant to each sampling point are sorted by their names (that, in general, may be considered to be arbitrary strings). We refer to the display that is constructed from all the first images in this sorted list at each sampling point as the ‘1-overlap display’ (in order to distinguish it from a more general N-overlap display to be shown presently). For the N-overlap display, we show pixels at only those geographic sampling points where there exists data in all of the first N overlapping images in the sorted list - the pixel value chosen for display is the RGB value in the Nth image. It is in this manner that Fig. 2shows the first 16 overlap displays. The image at the upper left corner is for the 1-overlap display; it is the same image that was shown earlier in Fig. 1. The second image in the top row is for the 2-overlap display, and so on. Note that what’s being displayed at each sampling point in an N-overlay display for any N has no bearing whatsoever on the data variability calculations at the point.The blocky artifacts in Fig. 2 (and in Fig. 1 also) are caused by several factors: (1) The image-to-image variability that can be attributed to the different look angles for the sensors aboard the satellites, sun angle variations, and the time of the year when the data was recorded; (2) Small errors in the application of the Top-of-Atmosphere correction to the pixels; and (3) small errors in the image rectification process.When we examine the overlaps in all of the satellite images for the Chile ROI, we go from one extreme where there is only a single image at a given geo-point to the other extreme where we have 49 satellite images looking at the same geo-point. Since it would occupy too much space to show all of these overlaps, in what follows we will show the results for just the case of 49 overlaps. Fig. 3shows the portions of the Chile ROI where we have data from 49 images. At each cell of the ROI-based bounding box, we arbitrarily selected the RGB from one of the 49 satellite images for constructing the display in Fig. 3. Note again that the goal of this display is merely to indicate where we have a total of 49 views looking at the same geo-point. The spectral variability results that are shown in the next subsection correspond to this case of the geo-points covered by these 49 images.We now show results of our spectral variability investigation over the set of geo-points depicted in Fig. 3. Recall, we have a total of 49 satellite images contributing MSI data to each of these points. We will report on the data variability as produced by the following three sources:1.Seasonal changesView angle changesContent changesHowever, before showing the variability as caused separately by each of these three sources, we will first show the overall data variability through what we refer to as variability heat maps. Fig. 4shows an example of such a heat map. The values depicted in Fig. 4 correspond to maximum spectral range, rmax, at each point. This parameter is calculated using the formulas:(1)ri=max(SSi)−min(SSi)(2)rmax=maxi({ri})In these formulas, SSiis the set of 49 spectral measurements for a band i at a given point. The value max (SSi) is the maximum of 49 such band values at a given point and min (SSi) is the minimum of the same set of spectral signatures. Therefore, the value of riis the largest variation within the ith band across all 49 satellite images. And the largest of these variations among the four spectral bands is the maximum range, denoted rmax. Note that these calculations are carried out after the spectral values are normalized to lie between 0 and 1.0. The data normalization for each image is a part of what is accomplished by the Top-of-Atmosphere correction mentioned earlier in Section 2. The red areas in Fig. 4 correspond to the heat map values above 0.15. On account of the data normalization, this means that we have more than 15% within-band variation at the points that are shown as red in Fig. 4. The points that are shown as green have their within-band variability under 15%. Rapid visualization of the variability allows us to systematically try many different cutoff thresholds and, in this case, 0.15 was chosen as it produced the most visually informative heatmap.Comparing the images in Figs. 3 and 4, one can infer that the orthorectification errors in the 49 satellite images are sufficiently small and can be ignored - this is an important by-product of this study since one is always concerned about the quality of orthorectification. If this error had been large, the green strands in Fig. 4 would not correspond to the roads in Fig. 3. Also note that the spectral signatures are significantly “invariable” across the 49 images for the road pixels. In other words, for this particular area in the ROI, the points of the earth’s surface that correspond to the roads produce the same spectral signatures regardless of when the satellite image was taken and the look angle for the image.We can draw the same conclusion in urban areas. Figs. 5and 6show an urban area covered by 25 overlapping satellite images near the center of the city of Santiago. This area contains many tall buildings. While the major roads in Fig. 5 are easily visualized in the heat map of Fig. 6, notice also the lining up of some of the finer detail in the heat map and how it matches the scene structure shown in Fig. 5. This leads us to conclude that even in dense urban areas the orthorectification errors are not so large as to render our integrated representation completely useless. It would be safe to attribute the variability that is visible in Fig. 6 to the different occlusions caused by tall buildings in the satellite images from different view angles.Fig. 7shows an example of data variation in a vegetation covered area as caused by seasonal change and Fig. 8shows variation due to view-angle change. For the latter case especially, even when the actual land-cover remains the same, the occlusions created by tall structures cause the spectral signatures collected for a geo-point to vary from one image to another. Data variations may also be caused by a piece of land being under development during the time when the images were recorded. In this case, the same area can completely change from one land type to another. Fig. 9shows such an example.While all the previous examples illustrated data variability caused by different sources, it’s also good to know that there can exist areas where the data stays constant, more or less. Fig. 10shows the same road area in three different images (selected randomly from the 49 satellite images for the subregion shown earlier in Fig. 3). In Fig. 11we plot the spectral signatures with respect to the view angles and with respect to time at a particular geo-location at the center of the three image patches in Fig. 10.The left plot of Fig. 11 shows the variability with respect to the look angle and the right shows the variability with respect to the month in which the data was recorded. As the reader can see, there is much less variation in the data for the point chosen. This result is consistent with the overall data variability heat map shown in Fig. 4.We now present a data annotation procedure for creating positive examples for the different land-types for the training of the classifiers presented in the next section. Since ROI based wide-area learning involves a very large amount of data, and since it would be much too frustrating for a human to have to scan all of the images that cover an ROI for identifying the positive examples, the procedure we present samples the ROI and throws up small patches, along with their contexts, for a human to see and categorize.The system starts out by showing the entire ROI (as, for example, shown in Fig. 1) to the annotator. The annotator can zoom in and out in order to become familiar with the general content of the ROI. At any given geo-point, the annotator can also scan across all of the overlapping views. Subsequently, the system throws up M randomly chosen subregions for each land-type for the human to annotate, M is typically a small user-specified integer. (We usedM=3for the experimental results in Section 7.) The size of such subregions is set at system initialization time.99As we will mention in greater detail later, we typically set the size of a subregion to be approximately 64 square kilometers.The randomization is carried out by the Metropolis-Hastings algorithm that is applied to the image coverage histogram of the sort shown in Fig. 12.1010Drawing samples according to the image coverage distribution yields a larger amount of training data in less time. The greater the number of overlaps at a geo-point, the larger the number of training samples that can be labeled at once at that geo-point. For Metropolis-Hastings sampling, we used the Perl-based implementation presented in Kak (2014).The annotator then visually examines the geographical content in the vicinity of the drawn subregion and is given the option of rejecting the subregion if it lacks the land-type for which the annotator is seeking positive examples.For each of the subregions thus accepted, the system randomly generates up to 20 1km-by-1km patches. To give the reader a sense of the size of these patches, for 2m/pixel data, each patch is represented by a 500 × 500 array of pixels that can easily be displayed fully in a typical 1024 × 768 computer monitor display. This allows the annotator to clearly see and recognize the land-types within a patch and its vicinity. The annotator can also zoom into a patch in order to resolve any ambiguities regarding the land-type of a pixel or a group of pixels. When the annotator sees a blob of pixels in which all pixels appear to possess the same land-type, he/she can draw a polygon around the blob and mark the entire blob as constituting a set of positive instances for that land-type. The annotator can also quickly check the overlapping satellite images and decide whether to mark the blob within the same polygon in all the overlapping images as well. In this manner, the annotator collects around 2000 samples from the images within the patches and their vicinity in each subregion for each land class. When the parameter M is set to 3, that makes for a total of 6000 positive instances for each land-type collected from three subregions.Obviously, one must exercise some care with regard to the size of the subregions used. If this size is too small, the samples collected may not capture sufficient variability needed for properly training a classifier. Fig. 13shows some marked subregions within the Chile ROI that were used for training the classifiers described in the next section. The subregions shown in the figure are of size 64 square kilometers, which is roughly 25% of the size of a typical satellite image. The size of a typical satellite image is about 10000 × 7000 pixels (or 20km × 14km) at 2 meters per pixel spatial resolution.In this section, we describe in detail the Bayesian framework that we have developed for land-type classification. Instead of directly using feature vectors for classification, we model the variations in the feature space for each class and use these models for land-type classification. The intuition behind this approach is that different classes exhibit different variations in their spectral signatures (and therefore in any derived feature space). Our results presented in Section 8 show that this reasoning is well-founded.We group the variations into two categories, within-class and between-class variations. To understand this distinction, without loss of generality, consider one particular class of land-type, for example ‘Trees’. One can definitely expect inter-image and inter-view differences in spectral signatures for pixels belonging to this class due to different categories of trees, different angles of satellite views, seasonal changes and so on. For example, perpetually green trees such as pine trees are likely to produce the same spectral signature across seasons, whereas other trees are more likely to exhibit large variations in their spectral signatures in different seasons. We refer to such variations as within-class variations. Correspondingly, differences in the spectral signatures between two different land-types, such as between buildings and trees, are referred to as between-class variations. It is important to realize that the nature of within-class variations itself might be different from class to class.We chose a Bayesian framework for classification as it offers a practical and powerful way of modelling both within-class and between-class variations in a single unified framework. In addition, as mentioned earlier, we aim to develop a system that can fuse information from multiple views and the Bayesian framework easily lends itself to fulfilling this objective.The foundations of our Bayesian framework lie in the prior contributions by Mogahaddam Moghaddam et al. (2000), Mogahaddam et al. Moghaddam (2002), and Aksoy and Haralick Aksoy and Haralick (2000); 2001). In Moghaddam (2002); Moghaddam et al. (2000), the authors use the difference in feature vectors for face recognition and show good performance. In Aksoy and Haralick (2000); 2001), the authors use a similar probabilistic approach for image retrieval. They use a likelihood-based similarity measure to classify the difference between two feature vectors (images) as ‘relevant’ or ‘irrelevant’. The authors demonstrate better performance using this metric when compared to a geometric-distance based metric. In those works, a difference between two feature vectors can belong to one of two classes, a relevant or irrelevant class. For example, in Moghaddam (2002); Moghaddam et al. (2000) all variations between any two images are grouped together irrespective of the identity of the human subject.Our Bayesian framework differs from these prior contributions in that we model the probability distributions of the within-class and between-class variations for each land-type class separately. This helps us to capture the intuition that the nature of variations themselves could be different for each class. For example the variations within the ‘Trees’ class due to seasonal changes or due to different varieties of trees are very different from the variations within the ‘Buildings’ class due to the usage of different construction materials.Before discussing the details of our Bayesian framework, in what follows, we will first present details about the feature space and the target classes used for classification. We will also introduce the symbolic notation used later in this section.Feature space - The Normalized Difference Vegetation Index (NDVI) was developed specifically to identify and characterize vegetation in aerial images DeFries and Townshend (1994); Kerdiles and Grondona (1995). Essentially it exploits the difference in how plants respond to incident light in the photosynthetically active region (PAR) and infrared region. The index is based on easy-to-compute spectral band ratios that show good performance for classifying vegetation. More recently in Marchisio et al. (2010); Wolf (2010) the authors have extended the NDVI like features and applied them for general land-type classification. We use these features for our framework. In subsequent discussions, we will refer to these features as Band Difference Ratios (BDR). Our dataset consists of 4-band satellite images (Red, Blue, Green, and NIR), which yields a 10-dimensional feature vector for each geo-point. The 10 dimensions correspond to the six band ratios (two bands taken together at a time) plus the four spectral signatures themselves.Since each geo-point in our PIMSIR representation can receive data from multiple overlapping views, each geo-point can have multiple 10-dimensional feature vectors associated with it. This is a significant departure from conventional methods that focus on selecting the best visually representative satellite image for each geo-point. Our probabilistic framework enables us to easily fuse data from all available views at the same time.Target classes - We use six land-type classes, namely Buildings, (Asphalt) Roads, Active Crop Field, Soil, Water, and Trees. The class ‘Active Crop Field’ refers to both crop fields and areas that contain grass and shrubs. Obviously the framework can be extended to an arbitrary number of classes. We have selected classes that one would expect in any meaningful attempt at land-type classification.Similarity measures for classification - In general, Bayesian classifiers make a probabilistic assessment of the class assignment of a test sample based on its “distance” from the distributions for the target classes. In this paper we have investigated two different similarity measures and analyzed their performance on land-type classification. In one case, we only model the within-class variations, and, in the second, we use a similarity measure that incorporates both within-class and between-class variations.Notation - We denote a feature vector byF→. The difference between two feature vectorsF→aandF→bwill be represented byΔ→ab=F→a−F→b.CiWwill denote the within-class variations for the ith class andCiBwill denote the between-class variations for the ith class (ie., the variation between class i and all the other classes taken together).For this case, we do not take the between-class variations into account. We only model the within-class variations. The underlying assumption is that any givenΔ→abmust belong to one of theCiWfor some i.Training - For each class, we model the probability distribution of the within-class variations as a unimodal multivariate Gaussian distribution with a zero-mean and an estimated covariance matrix. Given two feature vectorsF→aandF→bthat belong to two geo-points of the same class i, bothΔ→ab=F→a−F→bandΔ→ba=F→b−F→a=−Δ→abbelong to the within-class variations of class i. Therefore the probability distribution of the within-class variations will have a zero mean value. Obviously one can use more complex distributions than the Gaussian distribution to model the variations. Our experimental results validate our assumption of a Gaussian distribution. The covariance matrix for each class is estimated from its positive training samples. For the ith class, the probability distribution of the within-class variations is denoted as(3)p(Δ→|CiW)=N(0,EiW),whereEiWdenotes the estimated covariance matrix for the within-class variations of class i andCiWis as described in the Notation in Section 4.Testing - For a candidate geo-point Pk, we compute the 10- dimensional feature vector for each available overlapping view. For each such view, we then compute the difference between the corresponding feature vector of Pkand each feature vector in our training database. The maximum a-posteriori probability that one such difference vectorΔ→belongs to the within-class variations of class i is given by(4)Si(Δ→)=P(CiW|Δ→)=P(Δ→|CiW)P(CiW)P(Δ→)Recall that in this case we model only the within-class variations. Using the above stated assumption thatΔ→must belong to one of theCiWfor some i, the denominator in Eq. 4 is given by(5)P(Δ→)=∑iP(Δ→|CiW)P(CiW)Since the denominator term in Eq. 4 is the same for each class, we do not have to compute it explicitly. If we assume equal prior probabilities for each class, then Eq. 4 reduces to a Maximum Likelihood measure as shown below.(6)Si(Δ→)=P(Δ→|CiW).We compute this probability for each training feature vector from each class. The class label assigned to this view of Pkis then the class label of the training feature vector that maximizes this probability.1111Since we are modeling the within-class variations as unimodal Gaussians, this calculation is done efficiently through the computation of Mahalanobis distances using the covariances for the different classes. Note that this cannot be done for the case where we model both the within-class and between-class variations.Since a geo-point can have multiple overlapping views and thus multiple feature vectors, it is possible to assign multiple class labels to the same geo-point. Thus an interesting problem with using multi-view data is to find an optimum way to combine multiple class labels from multiple feature vectors for a particular geo-point. In Section 6, we present a practical and novel approach to handle this problem.Shown in Algorithms 1and 2is a brief algorithmic description for designing the classifier based on only the within-class variations.In this strategy, we incorporate both within-class and between-class variations in the similarity measure. We again model the between-class variations as unimodal multivariate Gaussian distributions. One could argue that between-class variations are better represented by a more complex multi-modal distribution. For this paper, as a starting point, we use the simpler unimodal distribution for computational convenience. However, extending this part of the classifier design to multi-modal distributions for between-class variations is one of our future goals.Training - The within-class variations are modeled as in the previous case. The probability distribution of the between-class variations for the ithclass, is denoted as(7)p(Δ→|CiB)=N(0,EiB),whereEiBdenotes the estimated covariance matrix for the variations between the feature vectors of class i and those of the other classes andCiBis as described in the Notation in Section 4.Testing - For each view of a test candidate geo-point Pkwe compute the difference between the corresponding feature vector of Pkand all the training feature vectors. The MAP probability that one such difference vectorΔ→belongs to the within-class variations of class i is defined as(8)Si(Δ→)=P(CiW|Δ→)=P(Δ→|CiW)P(CiW)P(Δ→)Now comes the key difference between this strategy and the previous strategy. Since we explicitly model the between-class variations, thereforeΔ→can either belong to the within-class variations of class i or the between-class variations of class i. Hence we can rewrite the similarity measure as(9)Si(Δ→)=P(Δ→|CiW)P(CiW)P(Δ→|CiW)P(CiW)+P(Δ→|CiB)P(CiB)To this view of Pkwe can then assign the class label of the training feature vector that maximizes the above probability.However, it is possible that Pkdoes not belong to any of the classes under consideration. Thus we can go a step further with the observation that forΔ→to actually be a within-class variation of class i, we require(10)P(CiW|Δ→)>P(CiB|Δ→)Using Eq. 10, Eq. 9 reduces to(11)Si(Δ→)>12If the above condition is not satisfied for any class i, then geo-point Pkcan be assigned a label ‘Other’. Note that this could be used to identify geo-points that belong to unmodeled classes such as clouds.We use the majority voting notion as mentioned in Section 6 to combine the information from multiple class labels.We present a brief algorithmic implementation of the above described strategy in Algorithms 3and 4.Since the variance based Bayesian framework presented in Section 4 is novel - novel in the context of satellite imagery - we must compare it with the current practice in satellite image classification. Considering that Support Vector Machines (SVM) are widely used for classifying satellite data, our Section 8 will compare the wide-area classification results obtained with our Bayesian framework and with SVM. As we will show in that section, using the same training and testing data sets, the Bayesian framework of Section 4 outperforms SVM for four out of six classes. This section provides a brief overview of how we have used SVM in our comparison studies.Note that, in contrast to our variance based framework in which classification rules are applied to the differences of feature vectors, SVM as popularly used are meant to be applied directly to feature vectors. We should also point out that since we apply the ToA reflectance correction to the data (Section 2), the resulting reflectance values are already normalized to the 0–1 range. By their definition, the BDR features that we extract have values between −1 and 1. Thus additional data normalization is not required for the SVM classifiers.We trained two different SVM classifiers, one with a Radial Basis Function (RBF) kernel and the other with a linear kernel. For multi-class classification, the former uses a “one-against-one” approach Knerr et al. (1990). Thus if we have n classes, the system usesn(n−1)2classifiers, each of which acts on two classes. For the linear kernel SVM, we use a “one-versus-the-rest” approach. We discuss the performance of our Bayesian classifier vis-a-vis both types of SVM classifiers in Section 8.For the reasons discussed in Section 2, we can expect that the different satellite views of the same geo-location will yield different classes for some of the points on the earth. While some of these variations in the class labels can be attributed to classification errors, others are due to genuine seasonal differences in the land-cover types. We now present the notion of “Compatible Classes” for dealing with the latter case.Say that for a particular geo-point, two different views give rise to the two labels ‘Soil’ and ‘Active Crop Field’. We can think of them as compatible class labels. Such geo-points can be classified as belonging to a super-class ‘Agricultural Land’. Choice of such compatible classes can be based on prior knowledge about the geographic area under consideration. For example, in Taiwan where paddy fields are very common, ‘Water’ can be considered as compatible with ‘Active Crop Field’. The notion of compatible classes enables us to bring in information from meta-data, such as the time of year when the satellite images were taken, in order to make better judgments on the compatibility of multiple class labels.When the number of target classes is large, one could try to design a system that automatically learns such rules of compatibility. This would require collecting ground-truth labels in a more extensive and elaborate manner and then using data mining techniques to detect any regular/periodic relationships between different class labels. For example, seasonal alternations between the ‘Active Crop Field’ and the ‘Soil’ labels will exhibit periodicity that could be learned.In cases where labels obtained from multiple views cannot co-exist, that is, when the labels are incompatible, ordinarily we apply majority voting to the labels from each view - unless the data at that location lends itself to the following logic: When several views are available at a geo-point, if the temporally earlier views consistently1212By ‘consistent’ we mean that there are no temporal changes in the class labels.give a vegetation label to a geo-point and the later views consistently call it a building or a road, we could safely assume that the land-type has changed permanently and we could just use the more recent label for that location.Fig. 14 shows an example of incompatible land-types. In the example, ‘Soil’ and ‘Buildings’ are incompatible. Fig. 15shows an example of compatible land-types. In the example, ‘Soil’ and ‘Active Crop Field’ can fall into a more general class such as ‘Agricultural Land’.The accuracy of using temporal consistency to combine labels will be affected by how well the orthorectified overlapping views line up together. This is especially true for pixels near the boundaries of objects as, for instance, the edge pixels of the building in Fig. 14. One can use additional higher level logic such as morphological operations and spatial filters to clean up such noisy pixels. In any case, from the standpoint of what is needed for geolocalization, the benefits of developing classifiers that are invariant to inter-view and inter-image variations far outweigh the effects of such noisy pixels.

@&#CONCLUSIONS@&#
This paper first presented the PIMSIR tool for creating an integrated representation for all satellite images, including those that are overlapping, that cover a large geographic area. We showed how this tool can be used to understand the data variability in the images - variability that runs across the images wherever they are overlapping, spatial variability, and temporal (and seasonal) variability. Subsequently, we showed how one’s understanding of this variability can be used to design Bayesian classifiers that are trained on a wide-area basis, as opposed to the more traditional approach of training classifiers using the data in just a single image. A necessary requirement of wide-area learning is statistical sampling of all of the satellite data for generating the ground truth. The overall training framework we provided included a Metropolis-Hastings random sampler for generating the points to be used for training and testing in proportion to the extent of satellite coverage over the different parts of the geographic area. Despite the fact that we used only a small number of subregions thus selected for generating the ground truth, we showed that wide-area based learning classifiers gave us better classifications for five out of six classes compared to the traditional approach of using a classifier based on the data extracted from just one image.Using the same training and the testing data sets, we also compared the performance of the variance-based Bayesian classifiers with the more traditional SVM classifiers. The conclusion here was that the variance-based framework led to superior classifications for four out of six land-cover classes.With regard to future extensions of this research, a very important issue that remains to be addressed is testing for sufficiency of the ground-truth data as yielded by the Metropolis-Hastings based random sampler. We want the ground-truth data to represent all of the diversity - speaking statistically, of course, - that exists in an ROI for each land-cover class. At the same time, we do not wish to collect any more ground-truthed data than is necessary on account of the high cost of the human labor involved in supplying the human judgments. We therefore need some sort of a feedback loop that uses the “test on unseen data” sort of strategy presented in Sections 7 and 8.3 to evaluate the quality of the data collected for ground-truthing up to a given point so that a decision can be made whether or not to collect additional data.For another future direction of investigation, we believe that, in the variance-based Bayesian classifier, we would get superior results when we also incorporate between-class variations provided we create a multi-modal model for such variations. As we mentioned earlier in the paper, perhaps the best future approach would be to use the EM algorithm for creating such a multi-modal model.For yet another future direction, we think our results would become even more impressive if we could enrich the spectral-signatures based feature space with spatial features. It is impossible to take into account the spatial context of a geo-point when classification is carried out just on the spectral signatures at each point. As we saw in Section 8, it is rather easy for the building pixels to be confused with the road pixels since we can expect the spectral signatures of certain kinds of rooftops (say, the flat roofs that are made with concrete slabs) to yield nearly the same signatures as the roads. Using spatial and, perhaps, texture properties of the blobs that surround the pixels would be one way to mitigate such inter-class confusions.