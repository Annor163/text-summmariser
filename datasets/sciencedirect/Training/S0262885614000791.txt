@&#MAIN-TITLE@&#
A framework for joint estimation of age, gender and ethnicity on a large database

@&#HIGHLIGHTS@&#
A framework for joint estimation of age, gender and ethnicity in a single step;A novel finding on feature dimensionality in estimating age, gender and ethnicity;A rank theory based analysis of dimensionality problem in using CCA based methods;A ranking of CCA and PLS based methods under our joint estimation framework;Investigation of LS formulations of the CCA based methods for our problem.

@&#KEYPHRASES@&#
Joint estimation of age, gender and ethnicity,A general framework,Partial least squares (PLS),Canonical correlation analysis (CCA),Regularized CCA,Kernel PLS (KPLS),Kernel CCA (KCCA),Regularized KCCA,Least squares CCA,

@&#ABSTRACT@&#
Human age, gender and ethnicity are valuable demographic characteristics. They are also important soft biometric traits useful for human identification or verification. We present a framework that can estimate the three traits jointly. The joint estimation framework could deal with the mutual influence of age, gender, and ethnicity implicitly. Under this joint estimation framework, we explore different methods for simultaneous estimation of age, gender, and ethnicity. The canonical correlation analysis (CCA) based methods, and partial least squares (PLS) models are explored under our joint estimation framework. Both the linear and nonlinear methods are investigated to measure the performance. We also validate some extensions of these methods, such as the least squares formulations of the CCA methods. We found some consistent ranking of these methods under our joint estimation framework. More importantly, we found that the CCA based methods can derive an extremely low dimensionality in estimating age, gender and ethnicity. An analysis of this property is given based on the rank theory. The experiments are conducted on a very large database containing more than 55,000 face images.

@&#INTRODUCTION@&#
Recently human age estimation in face images has become an active research in computer vision and pattern recognition [1,2], because of many potential applications in the real world. Age estimation is useful for creating an age-specific human–computer interaction (AS-HCI) system [3], electronic customer relationship management (ECRM) [4], and business intelligence [5].In addition to age estimation, face images can also be used to extract gender and ethnicity information. The three major characteristics, i.e., age, gender and ethnicity, are valuable demographic information of an individual or statistics about a population.Automated estimation of the demographics is of great value in practice, such as business intelligence, local community planning, and new school locating [6]. Age, gender and ethnicity are also useful soft biometric traits that can be used for human identification or verification.However, current computational techniques are still not robust enough for practical uses. Thus it is demanding to develop a robust and effective system to recognize age, gender and ethnicity for a given individual or a population.In the literature, there are different methods for the estimation of each single trait, e.g., age estimation [7–12,1], gender classification [13–17], or ethnicity estimation [18–21]. However, there are very few approaches to estimate all three traits together.In Ref. [22], a classification of ethnicity, gender, and age groups was executed, with each trait classified independently. This kind of approach can be illustrated by the framework shown in Fig. 1. An implicit assumption is that the three traits can be classified independently, and there is no relation among the three traits.In our previous studies [23,24], we found that age estimation can be influenced by the gender and ethnicity differences significantly. In Ref. [24], we show that the age estimation errors can be increased significantly on the Yamaha age and gender database, when the males and females are mixed together for age estimation. This result explains why the previous approaches, e.g., Refs. [9–12], executed the age estimation for males and females, separately, on the Yamaha database. In Ref. [23], we studied the influence of gender and ethnicity on age estimation systematically, using the MORPH database [25]. We found that the age estimation errors can be increased when the estimation is performed across gender, ethnicity, or across both.To deal with the influence of gender and ethnicity on age estimation, we proposed a framework [23] which has a three-step procedure on the extracted features from face images: (1) dimensionality reduction, (2) gender and ethnicity group classification, and (3) age estimation performed on each classified group. This framework can be illustrated by Fig. 2. Although this framework mainly focuses on improving age estimation, it also gets the gender and ethnicity characteristics in the second step.In this paper, we present a new framework which can estimate the age, gender, and ethnicity jointly in a single step. This single-step framework is much simpler than our three-step procedure [23], and can deal with the influence of gender and ethnicity on age estimation implicitly. Because the labels of gender and ethnicity for each aging pattern are integrated into the single-step age estimation process, we call it implicitly dealing with the influence of gender and ethnicity on age estimation. In contrast, an explicit approach is the three-step procedure [23] where the gender and ethnicity groups are recognized first, before performing age estimation. The new framework of single-step, joint estimation has not been investigated before by other researchers, to the best of our knowledge.We explore several different methods under our new framework for a single-step, joint estimation of age, gender, and ethnicity. The basic idea of our approaches is to explore the multi-label regression formulation for the joint estimation problem.Based on the multi-label regression formulation, our joint estimation framework is very simple, as shown in Fig. 3.To solve the multi-label regression problem, we explore two broad categories of methods, i.e., the partial least squares (PLS) models [26,27] and canonical correlation analysis (CCA) based methods [28,29]. We found that these methods can do dimensionality reduction and joint estimation of age, gender, and ethnicity all together within a single step. This is an interesting observation in our problem, and may inspire new explorations of these methods for other pattern recognition problems.Specifically, we found that the canonical correlation analysis (CCA) based methods, including linear CCA, regularized CCA, and kernel CCA, can find only three basis vectors to project the original features of several thousand dimensions. Thus, only three dimensions of features are needed (after the transformation) to estimate all three traits. This is a novel finding in estimating age, gender and ethnicity [6]. Further, we use the rank theory to analyze the feature dimensionality problem in using the CCA based methods for our specific task. Hopefully, our analysis may inspire more investigations for other recognition problems to derive similar results with a minimum number of feature dimensions.The PLS based methods, i.e., linear PLS and kernel PLS, can reduce the dimensionality to 20 or 30 from thousands (in an original feature space), and can achieve a good performance to estimate all three traits [30]. However, the dimensionality based on PLS methods cannot reach a small number as the CCA based methods.Under our joint estimation framework, we compare the CCA based methods with the PLS based, and derive a ranking of these methods in solving the joint estimation problem.Further, we explore some extensions of the CCA based methods, such as the recent development of least square formulations [31]. The least square CCA methods have shown excellent performance on traditional machine learning databases, however, it is unknown whether these methods can perform well in our problem.Given face images, a high dimensionality could be resulted in, e.g., thousands of features are extracted using the biologically-inspired features (BIFs) proposed in Ref. [12] for age estimation. Here, we show that the BIF can be used to represent the face images for all of the three characteristics, i.e., the age, gender, and ethnicity.We also investigate whether the performance has any changes when other learning methods are used for age estimation. For this purpose, the CCA or PLS based methods are used to generate new features that have low dimensionality and discriminative capability, but not predict the ages. The aging functions are learned by other methods for age estimation.Our major contributions in this paper include:1.A framework is presented for joint estimation of age, gender and ethnicity in a single step;A novel finding on feature dimensionality in estimating age, gender and ethnicity;A rank theory based analysis of the dimensionality problem in using the CCA based methods;A ranking of the CCA and PLS based methods in joint estimation of age, gender, and ethnicity;An investigation of the least squares formulations of the CCA based methods for the joint estimation problem.In the remaining of the paper, we describe the extracted features briefly, and then introduce the CCA based methods and do dimensionality analysis using the rank theory in Section 3. The least squares formulations of CCA are also introduced and compared to the standard methods. The PLS and kernel PLS methods are described in Section 4. The experimental evaluations are presented in Section 5, and finally we give some discussions and draw conclusions.Recently, the biologically-inspired features (BIFs) [32] have shown good performance in age estimation [12,24], as well as object category recognition [33,34] and face recognition [35]. A specially-designed BIF with two layers [12,24,23], i.e., the simple layer S1 and complex layer C1, shows much lower age estimation errors than previous approaches. The S1 layer contains a set of Gabor filters with parameters designed based on the visual cortex models [33], with the form,(1)Gxy=exp−x′2+γ2y′22σ2×cos2πλx′,where x′=xcosθ+ysinθ and y′=−xsinθ+ycosθ are the rotations of the Gabor filters with angle θ which varies between 0 and π. The aspect ratio is fixed as γ=0.3, the effective width σ, the wavelength λ as well as the filter sizes s were adjusted accordingly as in Ref. [33]. The filter banks can start from 5×5 or 7×7. Details about the related parameters can be found in Table 1 in Ref. [12].The orientation θ varies from 0 to π uniformly with different intervals, resulting in different numbers of total orientations, such as 4, 6, 8, 10, and 12.The C1 layer contains some non-linear operations including the “MAX” pooling and an “STD” operation [12], in order to have some invariance to translation, rotation, and scaling, as well as a characterization of the aging details.In our study, we use the BIF method for feature extraction. Different from Refs. [12,24,23], here we use the BIF to characterize faces for all three traits, i.e., age, gender, and ethnicity. Given the BIF features, our focus is to investigate the proposed single-step framework for joint estimation of age, gender, and ethnicity. Both the CCA and PLS based methods are explored under the joint estimation framework. Further, we study the minimum number of features needed for the joint estimation problem, and which methods can derive a minimum number of features.Canonical correlation analysis (CCA) is introduced by Hotelling [28] to describe the linear relation between two multidimensional variables as the problem of finding basis vectors for each set such that the projections of the two variables on their respective basis vectors are maximally correlated [28,29].The CCA methods have been applied to solve some computer vision problems, e.g., image annotation [36], action classification [37], and face recognition [38,39]. But the CCA methods have not been exploited before by other researchers for age estimation or recognizing all three traits (age, gender and ethnicity), to the best of our knowledge. More importantly, there are few studies on the dimensionality issue in CCA based methods in solving computer vision or pattern recognition problems.We will briefly describe the CCA method and its extensions in this section, and do dimensionality analysis of the CCA based methods for our problem.Let p-dimensional x and q-dimensional y denote the two sets of real-valued zero-mean random variables (i.e., x∈Rpand y∈Rq). Let p×N matrix X be the data matrix of the first set, and q×N matrix Y be the data matrix of the second set. N is the number of training samples. The canonical correlation analysis (CCA) computes two projection vectors, wx∈Rpand wy∈Rq, such that the correlation coefficient(2)ρ=wxTXYTwywxTXXTwxwyTYYTwyis maximized [28,29]. Since ρ is invariant to the scaling of Wxand Wy, CCA can be formulated equivalently as(3)maxwx,wywxTXYTwy,subject towxTXXTwx=1, andwyTYYTwy=1.It can be shown [29] that Wxcan be obtained by solving the following generalized eigenvalue problem,(4)XYTYYT−1YXTwx=λXXTwx,where λ is the eigenvalue that corresponds to the eigenvector Wx. It has also been shown [29] that multiple projection vectors under certain orthonormality constraints consist of the top l eigenvectors of the generalized eigenvalue problem in Eq. (4). Thus the feature dimension of data X can be reduced to a lower value.In our study, X represents the data matrix, and Y represents the label space. After the dimension of data X is reduced, we use a least squares fitting to build the relation between the dimension reduced feature and label Y. Then the prediction of Y for the test data is based on the least square fitting result. This simple least square fitting method can work well, and is also applied to other CCA extensions.In regularized CCA or rCCA, a regularization term is added to each data set to stabilize the solution [31]. The corresponding generalized eigenvalue problem is given by(5)XYT1−γyYYT+γyI−1YXTwx=λ1−γxXXT+γxIwx,where 0≤γx≤1 and 0≤γy≤1. It has been pointed out [27] that (1) when γx=0, γy=0 (5) is to solve the standard CCA; (2) when γx=1, γy=1 (5) is to solve the PLS eigenvalue problem and (3) by continuously changing of γx, γy a regularized CCA is solved. In our study, we set γx=γy=0.09, and found that the rCCA is significantly better than the standard linear CCA. The γxand γy cannot take too large values, e.g., close to 1, which may result in larger estimation errors and lower accuracies.Kernel CCA, or KCCA, is to apply the kernel trick to the CCA [29].In KCCA, the directions wxand wycan be rewritten as the projection of the data onto the direction α and β, as(6)wx=Xα,wy=Yβ.Let Kx, Kybe the kernel matrices corresponding to the two representation. Then the canonical correlation can be written as(7)ρ=αTKxKyβαTKx2α⋅βTKy2β,which is to be maximized.We used the Gaussian kernel for data X and linear kernel for label set Y, based on the property of our problem.To force non-trivial learning on the correlation by controlling the problem of overfitting, a regularization can be applied to the KCCA. Then the generalized eigenvalue problem becomes(8)Kx+κI−1KyKy+κI−1Kxα=λα,where κ>0 is a regularization parameter. In our experiments, we set κ=0.05. It is called the regularized kernel CCA, or simply rKCCA. If κ is set to zero, it becomes the standard KCCA without regularization. In our study, this regularization is helpful, although the accuracy improvement is not big in some cases. Experimentally, we found the same thing as CCA or rCCA, that is, only three feature dimensions are needed for rKCCA or KCCA in estimating age, gender and ethnicity (see Figs. 5, 6, and 7).In addition to solving the generalized eigenvalue problems, there are other formulations for the CCA optimization problem. To have a better and deeper understanding of the CCA based methods for our joint estimation of age, gender and ethnicity, we also explore other formulations of CCA in our work.Very recently, the least squares formulations have been proposed to deal with the CCA optimization [31]. It has been shown to perform better than the generalized eigenvalue problem in a couple of machine learning problems. Here, we investigate the performance of these new formulations for our joint estimation problem.The classical CCA technique can be extended to the least squares formulations [31], using the regularization method. Similar to ridge regression [40], the 2-norm regularized least squares CCA formulation, called LS2-CCA, can be obtained. It minimizes the following objective function by using the target matrix T,(9)L2Wλ=∑j=1q∑i=1NxiTwj−Ti,j2+λ||wj||22,where T is defined by(10)T=YYT−12Y,W=[w1, ⋯, wq], q is the number of output labels, N is the number of training examples, and λ>0 is the regularization parameter.In addition to the 2-norm formulation, the 1-norm can also be used. The 1-norm regularized least-squares CCA formulation is called LS1-CCA, which minimizes the following objective function:(11)L1Wλ=∑j=1q∑i=1NxiTwj−Ti,j2+λ||wj||1.The LS1-CCA can be solved efficiently by some software packages that are publicly available [31].It has been shown that the least squares formulations of the CCA can be equivalent to the classical CCA under some conditions [31]. We evaluate if the performance of the LS-CCA methods is better than other CCA techniques in our problem, so that various CCA formulations can be explored under our joint estimation framework.The LS-CCA can also hold the low dimensionality property as the classical generalized eigenvalue formulations. In our experiments, we found that three dimensions are used for the LS-CCA methods in our joint estimation problem (see Experiments).Now we analyze the dimensionality after mappings by the CCA-based methods under our joint estimation framework.Assuming XXTis nonsingular, Eq. (4) can be written as(12)XXT−1XYTYYT−1YXTwx=λwx,or(13)Mwx=λwx,with M=(XXT)−1XYT(YYT)−1YXT. Please note that XXTcan be singular, for example, when the number of features is larger than the number of samples. In this case, a dimensionality reduction may be performed to make it nonsingular.Now, we analyze the rank of matrix M based on the linear algebra theory. Suppose the covariance matrices of XXTand YYTare positive definite. Then we have(14)rankM=rankXXT−1XYTYYT−1YXT≤minrankX,rankY≤minrX,cX,rY,cY=minpqN,where r(·) and c(·) denote the number of rows and columns of a matrix, respectively. We have r(X)=p, c(X)=N, r(Y)=q and c(Y)=N.Based on the rank analysis in Eq. (14), we know that the rank of the matrix M is at most equal to the minimum value among the three numbers, p, q, and N. Thus, the number of non-zero eigenvalues of matrix M is less than or equal to min{p,q,N}. As a result, the eigenvalue problem in Eq. (12) has at most min{p,q,N} eigenvectors, corresponding to non-zero eigenvalues.Our rank analysis is similar to Theorem 3.1 in Ref. [31], which proved that the eigenvalue problem of CCA has q non-zero eigenvalues based on the assumption that Y has rank q.In our case of estimating age, gender and ethnicity, we have p=4376, q=3, and N=10,530 based on our experimental setup in this study. It is obvious that min{p,q,N}=3. So the maximum number of non-zero eigenvalues of Eq. (12) is three. Based on the assumption that XXTis nonsingular, the generalized eigenvalue problem in Eq. (4) is equivalent to the problem in Eq. (12). Thus, we can say that the generalized eigenvalue problem in Eq. (4) has at most three non-zero eigenvalues in our study, under the assumption that XXTis nonsingular.Thus, we only need at most three eigenvectors corresponding to the three non-zero eigenvalues to project the data of X, even if it is a large scale problem with the number of training examples N=10,530.Our experiments (see Section 5) show that it is true to use only three feature dimensions (or three projections of the data X) for estimation.On the other hand, we found that we do need to use three feature dimensions to estimate the age, gender and ethnicity altogether in our experiments (see Section 5). This indirectly indicates that the three traits are different, which means one trait cannot include another. So the rank of matrix Y is three, rather than less than three.Another interesting thing that we observed in our experiments (see Section 5) is that when more than three features are used for estimation, the estimation error or accuracy will almost keep the same (straight lines in Figs. 5, 6, and 7). This kind of phenomenon has not been observed often in many computer vision problems. Further, the performance of the PLS methods has a very different behavior with respect to the feature dimension (see Figs. 5, 6, and 7). Our work may inspire further exploration of the CCA based methods and comparison with the PLS for other computer vision and pattern recognition problems.For rCCA, let us denote Rxx=(1−γx)XXT+γxI, Ryy=(1−γy)YYT+γyI, with 0≤γxand γy≤1. Obviously, Rxxand Ryyare positive definite. So, Eq. (5) can be written as(15)Rxx−1XYTRyy−1YXTwx=λwx,and because(16)rankRxx−1XYTRyy−1YXT=rankXYTRyy−1YXT≤minrankXYT,rankRyy−1YXT≤minrankXYT,rankYXT=rankXYT≤q,therefore, the system in Eq. (15) has at most q non-zero eigenvalues.For rKCCA, we can do similar rank analysis. Denote G=(Kx+κI)−1Ky(Ky+κI)−1Kx, then rank(G)=rank(Ky(Ky+κI)−1Kx)≤min{rank(Kx), rank(Ky)}. Since we used linear kernel for Y, rank(Ky)=rank(YTY)=rank(Y). Thus, we can show that the eigenvalue problem in Eq. (8) has at most q non-zero eigenvalues.In the next section, we briefly introduce other methods that can be applied to our joint estimation framework, too, and compare them with the variety of CCA based methods experimentally.The partial least squares (PLS) algorithm [26,27] can also model the relation between two data sets. It uses latent variables to learn a new space to make the data correlate to each other. The linear and kernel PLS methods have been adapted to estimate age, gender and ethnicity in our recent study [30]. We can compare the PLS based methods with the CCA, to measure the performance difference quantitatively.Given two data matrices, X and Y, as defined in Section 3.1, and let N be the number of training samples, the PLS method computes two weight vectors, w and c, such that it is the solution to the following optimization problem:(17)covtu=max|w|=|c|=1covXTw,YTc,wherecovtu=tTuNdenotes the sample covariance between the score vectors t and u, and t=XTw and u=YTc. The classical optimization problem can be solved by the nonlinear iterative partial least squares (NIPALS) algorithm [41]. The NIPALS algorithm starts with random initialization of the Y space score vector u and repeats a sequence of iterations until convergence [41].After the extraction of the score vectors t and u at each iteration, matrices X and Y are deflated by subtracting their rank-one approximations based on t and u. Different forms of deflation can define several variants of the PLS [27].Regressions can be performed for both X and Y on t1 and u1, respectively, after extraction of score vectors t1 and u1, such that we have(18)X=p1t1T+X1,Y=q1u1T+Y1,wherep1=Xt1t1Tt1andq1=Yu1u1Tu1are the loading vectors, while X1 and Y1 are the residuals. If the L2 norm of the residual Y1 is small enough, the iteration process will terminate, otherwise, the above process is repeated until k steps. Then we get(19)X=p1t1T+⋯+pktkT+Xk,Y=q1u1T+⋯+qkukT+Ykor(20)X=PTT+RXY=QUT+RY,where P=[p1, p2, ⋯, pk] and Q=[q1, q2, ⋯, qk] are the loading matrices, T=[t1, t2, ⋯, tk] and U=[u1, u2, ⋯, uk] are the score matrices, and RXand RYare the residuals.As a result, we can have the regression relation:(21)Y=BTX+RY,where B can be estimated [27] by(22)B=XUTTXTXU−1TTYT.More details of the PLS method can be referred to Ref. [41].When a strong nonlinear relation exists between two sets of data X and Y, the kernel trick can be used to derive the kernel PLS [42,43].Since the development of kernel machines, especially the kernel support vector machine (K-SVM) [44], the kernel trick has been used for many non-linear mappings. Similar to other kernel machines, the kernel PLS maps the data into a high dimensional feature space to realize the non-linearity.Let xi(p-dimensional vector) and yi(q-dimensional vector) denote the feature vector and the corresponding label vector for the i-th sample, for i=1, 2, ⋯, N. A nonlinear mapping ϕ maps the feature vector xito a high-dimensional space Ω, and Φ=[ϕ(x1) ϕ(x2) ⋯ ϕ(xN)] denotes the matrix of the mapped X-space data using the kernel trick. The inner product of two vectors xiand xjis then replaced by the kernel function k(xi, xj). The KPLS can then be derived, resulting in the change of Eqs. (21) and (22) to(23)Y=BTΦ+RY,and(24)B=ΦUTTKU−1TTYT,where K=ΦTΦ is the Gram matrix. The (i, j)-th element of K is computed by the kernel function k(xi, xj). Then, the kernel PLS regression estimate of the output for a given input sample x can be written in the form of(25)y=YTUTKT−1UTk,where k is a vector whose i-th element is given by ki=k(x, xi).

@&#CONCLUSIONS@&#
