@&#MAIN-TITLE@&#
Enhanced control of a wheelchair-mounted robotic manipulator using 3-D vision and multimodal interaction

@&#HIGHLIGHTS@&#
Wheelchair-mounted robotic arm integrates 3D computer vision with multimodal input.Object recognition was improved by combining RGB information and 3D point clouds.Hybrid input (using both gestures or speech) outperformed using a single modality.Input performance was validated for daily living tasks: feeding and dressing.

@&#KEYPHRASES@&#
3D vision,Multi-modal interface,Wheelchair mounted robotic manipulator,Assistive robotics,

@&#ABSTRACT@&#
This paper presents a multiple-sensors, 3D vision-based, autonomous wheelchair-mounted robotic manipulator (WMRM). Two 3D sensors were employed: one for object recognition, and the other for recognizing body parts (face and hands). The goal is to recognize everyday items and automatically interact with them in an assistive fashion. For example, when a cereal box is recognized, it is grasped, poured in a bowl, and brought to the user. Daily objects (i.e. bowl and hat) were automatically detected and classified using a three-steps procedure: (1) remove background based on 3D information and find the point cloud of each object; (2) extract feature vectors for each segmented object from its 3D point cloud and its color image; and (3) classify feature vectors as objects after applying a nonlinear support vector machine (SVM). To retrieve specific objects, three user interface methods were adopted: voice-based, gesture-based, and hybrid commands. The presented system was tested using two common activities of daily living -- feeding and dressing. The results revealed that an accuracy of 98.96% is achieved for a dataset with twelve daily objects. The experimental results indicated that hybrid (gesture and speech) interaction outperforms any single modal interaction.

@&#INTRODUCTION@&#
Wheelchair-mounted robotic manipulators (WMRMs) have been developed to assist users with motor impairments to accomplish activities of daily living (ADL), such as feeding, dressing, and retrieval of daily objects (Amat, 1998). WMRMs can improve operational functions of users with upper extremity motor impairments (UEMIs) and reduce their needs for human assistance (Redwan Alqasemi, 2010).Previous studies showed that a WMRM could benefit users, such as quadriplegics due to spinal cord injuries (SCIs) (Garber et al., 2003) and cerebral palsy (Kwee et al., 2002). Most UEMIs lack fine motor skills required to grasp and retrieve daily objects. To bridge this gap, we want to enhance current WMRMs to recognize and manipulate objects automatically. Machine vision based interfaces are key to accomplish these goals (Yanco, 2000). Vision-based systems can provide UEMIs with great autonomy and less reliance on human assistants. For example, Tanaka et al.,(2010) developed a WMRM for delivering, retrieving, and positioning objects with two web cameras utilizing 2D information (one for object recognition, the other for face detection). Besides recognizing objects with little movement, 2 D visual information has also been applied to detect motion and track moving objects in the development of an intelligent wheelchair in a smart home system (Ktistakis et al., 2015). For more reliable object recognition and human body parts detection, object recognition algorithm consolidating both 2D and 3D information was developed for this study. Two 3-D sensors (KinectTM and PrimeSenseTM) were used for daily object recognition, grasping, and positioning within the human body.The aforementioned optic sensors were successfully integrated with commercially-available assistive robots (Kevin Edwards, 2006). Previously, a JACO™ (Kinova®) robotic arm with six degrees of freedom (DoF) was integrated with a vision-based system for automatic object retrieval due to its light weight and ease to be mounted to motorized wheelchair (Jiang et al., 2014, Jiang et al., 2013). Nevertheless, research related to human robot interaction (HRI) addressing the problem of WMRMs’ control is still lacking. While traditional modalities, such as joysticks, keypads, or teach pendants, have been designed to control commercial robotic arms (Jiang et al., 2013), most of them do not consider the operational limitations of UEMIs. Our paper explores intuitive control modalities (i.e. voice and gesture) as well as autonomous control to enhance WMRM operation.The focus of the current paper is to improve on the main drawback of a previous autonomous vision-based system that was developed for users to control a WMRM using gesture commands (Jiang et al., 2014). In this system daily objects were automatically recognized using 2D information, while human body parts (face and hands) were recognized and localized using both color and depth information. In this previous system, users with UEMIs could independently perform daily tasks. However, with the previous system, we observed two shortcomings. First, objects were not reliably recognized when the orientation or positions of the objects were altered. Since the cameras were on fixed position, placing objects in different positions often resulted in a distorted view of the objects. This distortion occurs because only 2D information was used to represent objects, and visual information from the occluded views was lost. To solve this problem and to improve object recognition, we used 3D information as well as 2D color data to represent objects as 3D point clouds. The second limitation of our previous system was to limit users to control the WMRM using either voice or gesture commands. While voice commands were intuitive, they were vulnerable to noisy environment. Gesture commands responded quickly but required lengthy periods of learning. To tackle these limitations and leverage on each control modality, a hybrid control modality was developed in this study. Voice and gesture interface were enabled simultaneously, so that users could use either modality for each command. To summarize, the contribution of this paper is three-folds: (1) improving the versatility and reliability of object recognition algorithm by incorporating both 3D point cloud and RGB information; (2) enabling hand gesture, speech-, and hybrid (hand gestures and speech) control; and (3) developing a smart system by the integration of multimodal interface with autonomous object recognition and retrieval, and evaluating this system with more complex daily tasks based on real-world requirements.The rest of the paper is organized as follows:Section 2 provides a review of the related work. In Section 3, the architecture of the system is illustrated. In Section 4 the approaches for gesture recognition, object recognition, and robotic control are explained in details. Section 5 presents the experiments and results. Section 6 discusses and concludes the paper.

@&#CONCLUSIONS@&#
In this paper, a 3D vision-based system was implemented to more efficiently control a WMRM. The system consists of four modules that synergistically allow UEMIs to perform daily tasks (i.e. dressing and feeding) more effectively. Three input (gesture, speech, and hybrid) modalities were tested. A gesture recognition algorithm was presented using both 3D spatial and 2D color information. The algorithm could detect and recognize twelve different daily objects automatically. The users’ face and hands were tracked to automatically position those recognized objects to the respective body parts.An experiment was conducted to test the performance of the presented object recognition algorithm. A dataset with twelve daily objects (i.e. cups, coke, and cap) were created. An eight-fold cross validation led to an average accuracy of 98.43% for the pipeline with CVFH descriptor and 98.96% with the CVFH+HOG descriptor. Two daily tasks (feeding and dressing) were conducted with eight subjects. Three interaction methods were tested, using gesture-based, speech-based, and hybrid interfaces. The results revealed that the hybrid interface outperformed using single control modalities.Future work will consist of recruiting subjects with UEMIs to evaluate the feasibility and usability of the presented system in real-word scenarios.