@&#MAIN-TITLE@&#
Visualization of boundaries in volumetric data sets through a what material you pick is what boundary you see approach

@&#HIGHLIGHTS@&#
Human-centric boundary extraction criteria and new boundary model.A novel boundary visualization method though a what material you pick is what boundary you see approach.Point-to-material distance measure.A complete application.

@&#KEYPHRASES@&#
Direct volume rendering,Transfer function,Boundary visualization,3-D edge detection,

@&#ABSTRACT@&#
Transfer function design is a key issue in direct volume rendering. Many sophisticated transfer functions have been proposed to visualize boundaries in volumetric data sets such as computed tomography and magnetic resonance imaging. However, it is still conventionally challenging to reliably detect boundaries. Meanwhile, the interactive strategy is complicated for new users or even experts. In this paper, we first propose the human-centric boundary extraction criteria and our boundary model. Based on the model we present a boundary visualization method through a what material you pick is what boundary you see approach. Users can pick out the material of interest to directly convey semantics. In addition, the 3-D canny edge detection is utilized to ensure the good localization of boundaries. Furthermore, we establish a point-to-material distance measure to guarantee the accuracy and integrity of boundaries. The proposed boundary visualization is intuitive and flexible for the exploration of volumetric data.

@&#INTRODUCTION@&#
Direct volume rendering (DVR) is a widely used visualization technique to demonstrate the internal structures of volumetric data [1,2]. The applications of DVR range from medical visualizations to atmospheric simulations. However, designing an appropriate transfer function (TF) is still particularly challenging which limits the application of DVR. TFs [3] are designed to map data properties (e.g. scalar value, gradient) to optical properties (e.g. color, opacity). Color is used to generate a visual distinction between different data properties [4]. Opacity determines the visual degree of each voxel in volumetric data sets.The visualization of the boundaries of three-dimensional (3-D) computed tomography (CT) and magnetic resource imaging (MRI) volumetric data has a wide range of applications and great significance for disease diagnosis and screening [5]. In order to highlight boundaries, gradient magnitude (∇f) has often been employed as a data property [3,6]. The most common TFs designed to visualize boundaries are multi-dimensional, such as intensity-gradient (f−|∇f|) histogram [7] and LH histogram [8]. The user interfaces of these sophisticated TF designs provide various widgets to help users to extract regions from the histograms. However, there exist two major obstacles in designing TFs for boundaries. First, the precise localization and detection of the boundaries is difficult. TFs are designed by assigning color and opacity to the extracted regions with the help of the widgets. Sometimes, the regions representing different boundaries are connected due to noise. Therefore, misclassification among different boundaries may occur while extracting regions. When the interference reaches a certain level, it is impossible for users to pick out the boundaries with high quality. Second, semantics is difficult to be integrated into boundary TF design. Multi-dimensional TFs concentrate on data properties which are difficult to be understood by non-experts, such as physicians and radiologists. They have to first translate their domain knowledge into the form of computer graphics. Sometimes a slight change of the translation may lead to a dramatic difference in the final rendering result. It is a tedious task to fine tune the widgets to generate high-quality TFs by non-experts via trial-and-error process.In this paper, we first propose three boundary visualization criteria and our boundary model. Compared with the classic two-material boundary model which is employed by LH histogram, our model only focuses on one material to achieve the human-centric interaction and introduce edge points for the precise localization of boundaries. Based on this boundary model we propose a what material you pick is what boundary you see boundary visualization that accepts direct manipulations on the original volumetric data, which enables the integration of semantics. Users can directly pick out the material of interest in the 2-D sequential images to transmit semantics. In addition, we utilize 3-D canny edge detection to ensure the good detection and localization of boundaries. Furthermore, we establish a point-to-material distance measure to guarantee the accuracy and integrity of boundaries. Without the tedious region extraction in a newly defined multi-dimensional TF domain and concerning about the quality of the TF design, users can focus on the exploration of the volumetric data intuitively.This paper is organized as follows: In Section 2, we discuss relevant methods in literature. In Section 3, we describe the general notion of boundary extraction criteria and our boundary model. In Section 4, we present our boundary extraction method based on our boundary model. In Section 5, we establish the TF to visualize boundaries. In Section 6, we use several concrete examples to demonstrate the advantage of our proposed method. In Section 7, we provide the limitations, followed by the conclusion in Section 8.

@&#CONCLUSIONS@&#
