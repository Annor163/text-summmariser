@&#MAIN-TITLE@&#
Evaluation of state-of-the-art segmentation algorithms for left ventricle infarct from late Gadolinium enhancement MR images

@&#HIGHLIGHTS@&#
We present a standardisation for evaluating segmentation algorithms for late enhancement imaging of infarct in the left ventricle.We demonstrate that widely used fixed thresholding methods such as the standard deviation method can have inaccuracies and is user-dependent.We provide a consensus ground truth obtained with statistical modelling on the benchmark datasets. Future algorithms can thus be benchmarked to provide a more reliable result.

@&#KEYPHRASES@&#
Late Gadolinium enhancement,Segmentation,Algorithm benchmarking,

@&#ABSTRACT@&#
Studies have demonstrated the feasibility of late Gadolinium enhancement (LGE) cardiovascular magnetic resonance (CMR) imaging for guiding the management of patients with sequelae to myocardial infarction, such as ventricular tachycardia and heart failure. Clinical implementation of these developments necessitates a reproducible and reliable segmentation of the infarcted regions. It is challenging to compare new algorithms for infarct segmentation in the left ventricle (LV) with existing algorithms. Benchmarking datasets with evaluation strategies are much needed to facilitate comparison. This manuscript presents a benchmarking evaluation framework for future algorithms that segment infarct from LGE CMR of the LV. The image database consists of 30 LGE CMR images of both humans and pigs that were acquired from two separate imaging centres. A consensus ground truth was obtained for all data using maximum likelihood estimation.Six widely-used fixed-thresholding methods and five recently developed algorithms are tested on the benchmarking framework. Results demonstrate that the algorithms have better overlap with the consensus ground truth than most of the n-SD fixed-thresholding methods, with the exception of the Full-Width-at-Half-Maximum (FWHM) fixed-thresholding method. Some of the pitfalls of fixed thresholding methods are demonstrated in this work. The benchmarking evaluation framework, which is a contribution of this work, can be used to test and benchmark future algorithms that detect and quantify infarct in LGE CMR images of the LV. The datasets, ground truth and evaluation code have been made publicly available through the website: https://www.cardiacatlas.org/web/guest/challenges.

@&#INTRODUCTION@&#
In recent years, the translation of image analysis tools to the clinical environment has remained limited despite their rapid development. Although algorithms are extensively validated in-house following development, it is often not clear how they compare to other existing algorithms. Algorithm designers are faced with the challenging task of cross comparing their algorithm’s performance. The absence of a common pool of data along with evaluation strategies has limited algorithm translation into the clinical workflow Moreover, as larger cohort data sets become available, the need for reducing the manual labour involved in image analysis is becoming more importantBenchmarking of algorithms on common datasets provides a fair test-bed for comparison. It is thus a very important activity as we move from bench to the bedside in the medical image processing community. In recent years, several conferences and meetings within the medical image processing community have provided a platform to benchmark algorithms from multiple research groups. These challenges invite participants to submit their algorithms and test them on common data. The results from the test are then evaluated and compared using common evaluation metrics. In the past, a few challenges have been organised, each with its own unique theme. There exists an index of past challenges within the medical image processing community and it can be found on the Cardiac Atlas project page in https://www.cardiacatlas.org/web/guest/challenges. In the cardiovascular imaging domain, some recent challenges include left atrial fibrosis and scar segmentation (Karim et al., 2013), left ventricle segmentation (Suinesiaputra et al., 2014), right ventricle segmentation (Petitjean et al., 2015), cardiac motion tracking (Tobon-Gomez et al., 2013) and coronary artery stenosis detection (Kirisli et al., 2013).Cardiovascular magnetic resonance (CMR) imaging can be used to comprehensively assess the viability of myocardium in patients with ischaemic heart disease. Myocardial infarction can be visualised and quantified using inversion recovery imaging 10–15 min after intravenous administration of Gadolinium contrast. This imaging technique is known as late Gadolinium enhancement (LGE) imaging. Experimental models have shown excellent agreement between size and shape in LGE CMR and areas of myocardial infarction by histopathology (Kim et al., 1999; Wagner et al., 2003). Infarct size from CMR is also a primary endpoint in many clinical trials (see Desch et al., 2011 for a complete list).Recent studies have also demonstrated how infarct size, shape and location from pre-procedural LGE can be useful in guiding ventricular tachycardias (VT) ablation (Estner et al., 2011; Andreu et al., 2011). These procedures are often time-consuming due to the preceding electrophysiological mapping study required to identify slow conduction zone involved in re-entry circuits. Post-processed LGE images provide scar maps, which can be integrated with electroanatomic mapping systems to facilitate these procedures (Andreu et al., 2011). Clinical implementation of these developments necessitates a reliable, fast, reproducible and accurate segmentation of the infarcted region. Moreover, as use of LGE-based infarct volume estimation becomes more clinically relevant, standardisation will facilitate more consistent interpretation.A short overview of previously published infarct detection algorithms for the left ventricle (LV) is presented here. Table 1 lists the algorithms surveyed and highlights some of their important features. A common method for detecting infarct in the LV is the fixed-model approach, whereby intensities are thresholded to a fixed number of standard deviations (SD) from the mean intensity of nulled myocardium or blood pool (Flett et al., 2011). In the rest of the paper this will be known as the n-SD method, wheren=2,3,4,5or 6. A second common fixed-model approach is the full-width-at-half-maximum (FWHM) approach, where half of the maximum intensity within a user-selected hyper-enhanced region is selected as the fixed intensity threshold (Amado et al., 2004). Using this threshold, a region-growing process is employed from user-selected seeds. These seeds are selected to be within infarcted regions such that they can be segmented with region-growing.As the aforementioned approaches require user input, making them prone to inter- and intra- observer variation, other approaches that are automatic have been developed. Hennemuth et al. (2008) modelled the intensities of homogeneous tissue in LGE CMR with a Rician distributions and an expectation-maximization (EM) algorithm was used for fitting the data. Pop et al. (2013) fitted Gaussian mixture models to myocardial tissue pixel intensities and correlated with histology. In Detsky et al. (2009), clustering in a feature space of steady-state andT1*intensity values provided the segmentation which was shown to provide good correlation with FWHM. Tao et al. (2010) employed automatic thresholding using the Otsu method on bi-modal intensity histograms of myocardium and blood pool. More recently, the use of the graph-cut technique in image processing has been applied to segment infarct in several methods (Lu et al., 2012; Karim et al., 2014; Karimaghaloo et al., 2012). An advantage of this technique is that constraints can be placed on the resulting segmentation, allowing segmentation boundary regularization with region-based properties. It also predicts which pixels are statistically most likely to be infarct based on prior probability distribution models.In this paper we propose an evaluation framework for future algorithms that segment and quantify infarct from LGE CMR images of the LV. To demonstrate the framework, five algorithms were evaluated by comparing against a consensus segmentation of experienced observers. The algorithm and observers were both provided the myocardium segmentation. The algorithms were also provided with training data sets. Algorithms evaluated in this work were submitted as a response to the open challenge, put forth to the medical imaging community at the Medical Image Computing and Computer Assisted Intervention (MICCAI) annual meeting’s workshop entitled as Delayed Enhancement MRI segmentation challenge. There were thirty LGE CMR data of the LV from both human and porcine cohorts used for the challenge. The data were divided into test (n=20) and training (n=10) sets. Each participant designed and implemented an algorithm which segmented the infarct in each dataset. The datasets are publicly available via the Cardiac Atlas project challenge website https://www.cardiacatlas.org/web/guest/ventricular-infarction-challenge.

@&#CONCLUSIONS@&#
CMR continues to play an important role in imaging and quantifying infarct in the LV. Several algorithms have been proposed for its quantification but it is not clear how they compare or perform relative to one another. Furthermore, algorithms have only been tested on centre- and vendor-specific images. The translation of such algorithms into the clinical environment thus remains challenging. Benchmarking frameworks, providing a common dataset and evaluation strategies, is important for clinical translation of these algorithms. The proposed benchmarking framework provides thirty datasets, with fifteen datasets in each cohort: patient and porcine. Datasets in the two separate cohorts were acquired using different scanner vendors and field strength (1.5T and 3T), resolutions and acquisition protocols (2D and 3D). The ground truth is often absent in such datasets, and to this end, the framework provides with a powerful expert observers’ consensus ground truth. The proposed framework remains publicly available for accessing the image database, uploading segmentations for evaluation and contributing manual segmentations for improving the consensus ground truth on the datasets.