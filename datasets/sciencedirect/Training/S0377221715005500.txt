@&#MAIN-TITLE@&#
Six Sigma performance for non-normal processes

@&#HIGHLIGHTS@&#
We analyze Six Sigma performance for non-normal processes.We examine changes in failure rates for exponential, Gamma and Weibull processes.Higher quality improvement effort may be required for non-normal processes.Reporting the Sigma level as an indication of the quality can be misleading.Wrong Six Sigma projects can be selected when systematically assuming normality.

@&#KEYPHRASES@&#
Quality management,Six Sigma,Non-normal distributions,Optimization,Project selection,

@&#ABSTRACT@&#
Six Sigma is a widely used method to improve processes from various industry sectors. The target failure rate for Six Sigma projects is 3.4 parts per million or 2 parts per billion. In this paper, we show that when a process is exponential, attaining such performances may require a larger reduction in variation (i.e., greater quality-improvement effort). In addition, identifying whether the process data are of non-normal distribution is important to more accurately estimate the effort required to improve the process. A key finding of this study is that, for a low kσ level, the amount of variation reduction required to improve an exponentially distributed process is less than that of a normally distributed process. On the other hand, for a higher kσ level, the reverse scenario is the case. This study also analyzes processes following Gamma and Weibull distributions, and the results further support our concern that simply reporting the Sigma level as an indication of the quality of a product or process can be misleading. Two optimization models are developed to illustrate the effect of underestimating the quality-improvement effort on the optimal solution to minimize cost. In conclusion, the classical and widely used assumption of a normally distributed process may lead to implementation of quality-improvement strategies or the selection of Six Sigma projects that are based on erroneous solutions.

@&#INTRODUCTION@&#
Since its launch in the 1980s, Six Sigma has generated great expectations and hype in the business world, driven largely by reports of significant business gains, e.g., GE reported 11% revenue and 13% earnings increases from 1996 to 1998, AlliedSignal had savings of $1.5 billion in 1998, and Harry and Schroeder (1999) claimed “10% net income improvement, a 20% margin improvement, and a 10–30% capital reduction” for each Sigma improvement. These business gains were based on management and technical methods for improving processes with a theoretical aim of a failure rate of 3.4 parts per million (ppm) or 2 parts per billion (ppb), depending on certain assumptions. Important assumptions in Six Sigma are that the process is normal and its specifications are two-sided.The assumption of normal distribution is generally appropriate for many technological manufacturing systems (e.g., machines, tools, and robots). Although not all manufacturing processes follow a normal distribution, this assumption remains widely used and recognized in the field of quality. Invariably, the normality test should be a first step in any approach using the normality assumption. However, for service or transaction systems, which have become increasingly predominant in various applications, even in manufacturing ones, the normality assumption comes into question. For example, in supply chain management and goods production systems, fill rates and the probability to meet specified customer service levels are not governed by normal distributions (Nourelfath, 2011). In addition, in project management where a project will more likely finish late, the normality assumption does not hold; in particular, this is the case when the number of activities is too small to assume a normal distribution based on the central limit theorem. Another example is the procurement process of oil and gas companies, where it was observed that cycle time data more closely resemble a Gamma distribution rather than a normal distribution.Identified in a survey of the relevant literature, English and Taylor (1993) provide a good study on the robustness of the normality assumption when the distribution is not normal. Montgomery and Runger (2011) studied the error associated with non-normality in the estimation of defects in parts per million; the authors recommended applying transformational functions to the underlying non-normal variable until a normal transformed variable is found. A major drawback of this trial-and-error approach is the lack of physical meaning in the resulting transformed variable, rendering it unappealing to non-academic practitioners. Other researchers suggested fitting the appropriate distribution to the data and then calculating the parameters that would yield the desired parts per million (Somerville & Montgomery, 1996; Clements, 1989; Farnum, 1996; Rodriguez, 1992).The main objective of this paper is to evaluate non-normally distributed data in Six Sigma performance methodology. Using the exponential distribution case, we first analyze the amount of variation reduction required to improve the process. The efforts required to achieve the performance goal of Six Sigma are then compared between the normal and exponentially distributed cases. Then, we generalize our study to processes using Gamma and Weibull distributions. Finally, two optimization models are developed: the goal of the first model is to find the optimal trade-off between quality-improvement program costs and costs incurred as a result of poor quality, and the second model addresses the optimal selection of process-improvement strategies. These optimization models analyze the impact of a poor estimation of quality-improvement effort on optimal solutions to minimize costs.It is often the case in the professional and academic communities of quality to set 3.4 parts per million as the goal of Six Sigma without due consideration for the underlying process distribution. This could lead to erroneous estimation of the efforts and costs to achieve that goal when the process is not normal, as often is the case in service applications. The contribution of this research is that it provides guidelines to help managers to more accurately estimate the efforts required to achieve the performance goal of Six Sigma, and analyze the robustness of proposed quality optimization models under inaccurate probability distributions.The remainder of this paper is organized as follows: Section 2 presents the relevant literature; Section 3 analyzes the Six Sigma performance methodology in exponential, Gamma and Weibull distribution processes; Section 4 presents two optimization models to illustrate the effect of inaccurate estimations of probability distributions; and Section 5 provides our concluding remarks.

@&#CONCLUSIONS@&#
In the existing literature, Six Sigma theory and implementation have not been sufficiently studied for service processes. Only a relatively small number of papers have dealt with this important issue. A challenge to applying Six Sigma methodology in service processes is the fact that, in most cases, the underlying processes are non-normal. However, the majority of the existing studies are based on the normality assumption. Unlike the prior literature, this paper has developed an approach for Six Sigma performance evaluation without assuming normal probability distributions. In comparison to the existing studies, the contributions of this study are twofold. First, we have evaluated the Six Sigma performance using exponential, Gamma and Weibull distributions. Next, two optimization models were developed to analyze the effect of inaccurately estimating the quality effort. To the best of our knowledge, this is the first time that the Six Sigma performance methodology has been evaluated to analyze the robustness of quality optimization models under inaccurate probability distributions. Managerial insights were provided through the paper using many illustrative numerical examples. Guidelines were then provided to help managers to more accurately estimate the efforts required to achieve the performance goal of Six Sigma. We demonstrated that, by using the exponential distribution rather than the normal distribution, the variation reduction required to improve a process at a lower than given kσ level is less than that when the process is beyond that level. Moreover, we have shown that achieving the performance goal of Six Sigma when the process is exponential is more demanding than that for the normally distributed case. The consequences of non-normality on the failure rate were analyzed for Gamma and Weibull distributions. These analyses demonstrate that attaining the Six Sigma performance goals require varying different effort levels based on the distribution type. Therefore, it is reasonable that the distribution type is accounted for and specifically calculated when an organization plans for its next Six Sigma initiative. We have shown that achieving the performance goal of Six Sigma in the exponential, Weibull and Gamma cases requires a greater variation reduction than that for the normally distributed case, although for less than the Six Sigma level, it is possible that less variation reduction is required. Finally, using two optimization models, we have analyzed the robustness of the optimal solution to minimize cost when an exponential process is assumed to be normal. The results indicated that an incorrect estimation of the probability distribution, and thus the quality effort may lead to erroneous solutions when selecting quality programs and Six Sigma projects.The purpose of this article is not to advocate for changing the term Six Sigma or cast doubt on its business value. Instead, we hope to highlight the effect of distribution types in an effort to promote professionalism and accuracy in regard to the 3.4 parts per million or 2 parts per billion target performance goals made by Six Sigma practitioners. That is, instead of systematically assuming normal distributions in Six Sigma projects, managers need to make significant efforts and a sense of awareness to identify the right probability distribution for each process.This paper does not identify which distribution is better for service systems. We are currently developing a theoretical model to explain why the normality assumption is highly questionable for cycle times in service processes. In addition, no field study is provided to support our claims. Future work will aim to apply the results of this study to the Commercial Department of a Gas and Oil company in Kuwait.