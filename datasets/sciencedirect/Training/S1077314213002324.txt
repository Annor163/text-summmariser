@&#MAIN-TITLE@&#
Orthogonal locally discriminant spline embedding for plant leaf recognition

@&#HIGHLIGHTS@&#
OLDSE explicitly considers the intrinsic leaf manifold structure.OLDSE takes local structure and discriminant information into consideration simultaneously.OLDSE seeks to find a set of orthogonal basis functions.

@&#KEYPHRASES@&#
Plant leaf recognition,Local spline embedding (LSE),Maximum margin criterion (MMC),Orthogonal locally discriminant spline embedding (OLDSE),Manifold learning,

@&#ABSTRACT@&#
Based on local spline embedding (LSE) and maximum margin criterion (MMC), two orthogonal locally discriminant spline embedding techniques (OLDSE-I and OLDSE-II) are proposed for plant leaf recognition in this paper. By OLDSE-I or OLDSE-II, the plant leaf images are mapped into a leaf subspace for analysis, which can detect the essential leaf manifold structure. Different from principal component analysis (PCA) and linear discriminant analysis (LDA) which can only deal with flat Euclidean structures of plant leaf space, OLDSE-I and OLDSE-II not only inherit the advantages of local spline embedding (LSE), but makes full use of class information to improve discriminant power by introducing translation and rescaling models. The proposed OLDSE-I and OLDSE-II methods are applied to recognize the plant leaf and are examined using the ICL-PlantLeaf and Swedish plant leaf image databases. The numerical results show compared with MMC, LDA, SLPP, and LDSE, the proposed OLDSE-I and OLDSE-II methods can achieve higher recognition rate.

@&#INTRODUCTION@&#
Plant recognition based on leaf images is very important and necessary to ecological protection and automatic plant recognition system. It is well known that how to validly extract classification features is central to the plant recognition based on leaf images. Currently, the widely used features for plant recognition based on leaf image can be divided into color, shape, and texture features [1–13]. Plant recognition based on leaf color feature is one of the most widely used techniques. The earliest plant recognition studies used color as comparing feature between images [1–3]. A simple color similarity between two images can be measured by comparing their color histograms. More complex color features may involve looking into spatial relationship of multiple colors or looking at the color histograms in automatically segmented regions of the leaf image. Shape is one of the most important characteristic of the plant leaf. Two basic approaches to shape analysis exist: region-based and boundary-based (contour-based) [4]. Region-based systems typically use moment descriptors [5] that include geometrical moments, Zernike moments and Legendre moments [4]. Boundary-based systems use the contour of the objects and usually give better results for leaf images that are distinguishable by their contours. Curvature scale space methods and deformable templates [6] are some of the common techniques used in contour-based shape recognition. Texture can be described as the spatial patterns formed by the surface characteristics of a leaf that manifests itself as color or grayscale variations in the image. Texture analysis and matching can be done in spatial or frequency domain. Commonly used texture features are gray-level co-occurrence matrices, local binary patterns, Markov random fields, and Gabor wavelets [7–11]. Although they have proven to be effective in some real-world applications, these techniques have not fully considered the following characteristics of plant leaf image data: (1) Diversity. Plant leaf images differ from each other in a thousand ways. They vary with period, location, and illumination conditions, and contain a lot of noise and outliers. (2) Holism. The existing methods only concentrate on feature extraction of every single leaf images while not considering the leaf images as a whole to conduct dimensionality reduction and feature extraction. (3) Geometry. They neglect the intrinsic geometrical structures of leaf image subspace. It inevitably increases the difference of the intra-class leaf images and decreases the difference of the inter-class leaf images simultaneously, which will lead to a heavy weakening of their performances on plant recognition. Moreover, most of these methods are computationally expensive, thus limiting their utility in plant leaf image data sets.Over the last decade, a large number of nonlinear manifold learning algorithms have been proposed under the assumption that the input data set lies on or near some low-dimensional manifold embedded in a high-dimensional unorganized Euclidean space. Among the most well known are isometric feature mapping (ISOMAP) [14], locally linear embedding (LLE) [15–16], Laplacian eigenmaps (LE) [17], Hessian-based locally linear embedding (HLLE) [18], maximum variance unfolding (MVU) [19], local tangent space alignment (LTSA) [20], and local spline embedding (LSE) [21]. Each manifold learning algorithm attempts to preserve a different geometrical property of the underlying manifold. Local approaches such as LLE, HLLE, LE, LTSA, and LSE aim to preserve the proximity relationship among the data, while global approaches like ISOMAP aim to preserve the metrics at all scales. These nonlinear methods yield impressive results on some benchmark artificial and real world data sets due to their nonlinear nature, geometric intuition, and computational feasibility.An important advantage of manifold learning [14–21] compared with conventional approaches concerns how the data are treated mathematically. Conventional approaches typically produce a smaller data space from linear combinations of the original data. One common example is PCA which seeks a low-dimensional linear subspace spanned by the eigenvectors corresponding to the largest eigenvalues of the covariance matrix of all samples. However, for plant leaf images, the assumption of global linearity is a severe constraint since they are sensitive to period, location, and illumination conditions and there is no reason to believe that the leaf image data are linearly separable from each other. Manifold learning approaches recognize this fact and allow the data to be nonlinearly related, which results in the fact that manifold learning approaches can much more accurately capture the proper information relationships among the data thus allowing for accurate recognition. Fig. 1shows a simple example that 150 leaf images of two kinds of plants are mapped into two-dimensional subspace by local spline embedding (LSE). The size of each image is 32×32 pixels, with 256 gray-levels per pixel. Thus, each leaf image is represented by a point in the 1024-dimensional ambient space. The left and right leaf images respectively correspond to the points with green and cyan circles in the two-dimensional embedding subspace. As can be seen, the leaf images are divided into two parts. The circles and the asterisks represent leaf images of different classes. It can be clearly seen that the sample points of each class exhibit a sub-manifold distribution. The results demonstrate that LSE can successfully find the discriminative directions, but the directions are not optimal for leaf recognition task. This is because in trying to preserve local structure in the embedding, the LSE implicitly emphasizes the natural clusters in the data. More importantly, LSE is capable of capturing the intrinsic leaf manifold structure to some extent.In this study, we take an alternative view of manifold learning to develop two orthogonal locally discriminant spline embedding methods (OLDSE-I and OLDSE-II) for plant leaf recognition. The goal of OLDSE-I or OLDSE-II is to map the plant leaf images into a plant leaf subspace for analysis. Different from principal component analysis (PCA) [22] and linear discriminant analysis (LDA) [23] which can only deal with flat Euclidean structures of plant leaf space, The OLDSE-I/OLDSE-II finds an embedding that not only inherits the advantages of local spline embedding (LSE) [21] which uses local neighborhoods as a representation of the local geometry so as to preserve the local structure, but makes full use of class information to improve discriminant power by introducing translation and rescaling models. In this way, a plant leaf subspace that best detects the essential plant leaf manifold structure can be obtained. It is worthwhile to highlight several aspects of the proposed approaches here:(1)An efficient subspace learning algorithm for plant recognition should be able to discover the nonlinear manifold structure of the leaf image space. Our proposed OLDSE-I and OLDSE-II algorithms explicitly considers the intrinsic leaf manifold structure which is modeled by an adjacency graph.OLDSE-I/OLDSE-II takes local structure and discriminant information into consideration simultaneously and attempts to manage the trade-off between LSE, which is based mainly on preserving local geometry and maximum margin criterion (MMC), which emphasizes discriminant power.OLDSE-I/OLDSE-II shares some similar properties to LSE, such as a local neighborhood preserving character. However, their objective functions are ultimately different. OLDSE-I/OLDSE-II computes an explicit linear mapping from the input space to the reduced space, while in LSE, the mapping is implicit and it is not clear how new data samples can be embedded.Although OLDSE-I and OLDSE-II seek to find a set of orthogonal basis functions and further improve their recognition accuracy, they use different orthogonalization processes in which their fundamental difference lies.The rest of this paper is organized as follows: Section 2 briefly describes the LSE algorithms. In Section 3, the OLDSE-I and OLDSE-II algorithms are developed. A variety of the experimental results are presented in Section 4. Finally, we provide some concluding remarks and future work in Section 5.Given a data set of n data pointsX=[x1,x2,…,xn]∈RD×n, the goal of dimensionality reduction is to project the high-dimensional data into a low-dimensional feature space. Let us denote the corresponding set of n points in the reduced space asY=[y1,y2,…,yn]∈Rd×n, with d≪D, in which yiis a low-dimensional representation of xi(i=1,2,… ,n).LSE [21] is a recently proposed manifold learning method for nonlinear dimensionality reduction. This method is developed from the framework of part optimization and whole alignment. Each data point is represented in different local coordinate systems by part optimization. But its global coordinate should be maintained unique. Whole spline alignment is used to achieve this goal. The outline of LSE can be summarized as follows:Step 1: Identify neighbors. For each data point xi, LetX=[xi1,xi2,…,xik]∈RD×kdenote the collection of its k nearest neighbors. Use the KNN or ɛ-ball criterion [17] to identify the indices corresponding to the k nearest neighbors.Step 2: Obtain tangent coordinates. Perform a singular decomposition of the centralized matrix of Xi, we haveStep 3: Global embedding. For the ith local tangent space projection, letYi=[yi1,yi2,…,yik]∈Rd×kcontain the corresponding global coordinates of the k data points. Further, denote the rth row of Yiby[yi1(r),yi2(r),…,yik(r)]. We determine d spline functionsgi(r):Rd↦R,r=1,2,… ,d,such that the coordinate components can be faithfully mapped:Because Yiis unknown, the desirable splines not only can satisfy the conditions in Eq. (3) but also make the reconstruction error to be formulated explicitly in terms of Yi. The spline developed in Sobolev space meets our tasks. Without loss of generality, we consider how to deal with one coordinate component by fixing the upper script r in (3). For clarity, we use simple notations to replacegi(r),tij, andyij(r)in (3), namely,g←gi(r),tj←tij, andyj←yij(r). Formally, we have(4)yj=g(tj)j=1,2,…,k.To construct g, we use the following minimization of the penalized sum of squares:(5)J(g)=∑j=1k(yj-g(tj))2+λJsd(g),whereλ>0is a regularization, andJsd(g)is a smoothness penalty in d dimensions on g. Duchon [24] demonstrated that under some constraints, there is a unique spline function that minimizes J(g):(6)g(t)=∑i=1lβipi(t)+∑j=1kαjϕj(t),(7)s.t.∑j=1kαjpi(tj)=0i=1,…,lwherel=(d+s-1)!/(d!(s-1)!),{pi(t)}i=1lare a set of polynomials inRdof total degree less than s, and ϕj(t) is a Green’s function [24].(8)ϕj(t)=‖t-tj‖2s-d·log(‖t-tj‖)ifdis even,‖t-tj‖2s-difdis odd.Now, substituting the constraints in (4) into (6) and combing (7) together, the coefficientsα=[α1,α2,…,αk]T∈Rkandβ=[β1,β2,…,βl]T∈Rlcan be solved via the following linear equations:(9)A·αβ=y0,where y=[y1,… ,yk]Tand A is defined as(10)A=KPPT0∈R(k+l)×(k+l),in which K is k×k symmetrical matrix with elements Kij=ϕj(ti),and P is a k×l matrix with elements Pij=pj(ti).The value of J(g) in (5) can be approximately evaluated as follows [25]:(11)J(g)≈∑j=1k(yj-g(tj))2+λαTKα.Here, the regularization parameterλcontrols the amount of smoothness of the spline near the k neighboring data points. With a small enoughλ, the first term in Eq. (11) can be neglected. Therefore, we have(12)J(g)∝αTKα=yTBy,where B is the upper left k×k subblock of A−1.Now considering d splines in (3), under which the local coordinates are mapped to the global coordinates. Adding a subscript i to A, we get Ai. The error of constructing such d splines for mapping d coordinate components can be summed together, we have(13)E(Yi)=tr(YiBiYiT),where tr is the trace operator, and Biis the upper left k×k subblock ofAi-1.Summing all the reconstruction errors together, we have(14)E(Y)=sumi=1ntr(YiBiYiT).Let Sibe a column selection vector such that YSi=Yi. The objective function is converted to the following form(15)E(Y)=∑i=1ntrYSiBiSiTYT=tr(YSBSTYT)=tr(YMYT),where S=[S1,… ,Sn], B=diag(B1,… ,Bn), and M=SBST.To uniquely determine Y, we impose the constraint YYT=I. Then, the minimum of E(Y) for the d -dimensional global embedding is given by the d eigenvectors of the matrix M, corresponding to the 2nd to (d+1)st smallest eigenvalues of M.For visualization, the goal of dimensionality reduction methods is to project the original data set into a low-dimensional space that faithfully preserves the intrinsic structure as well as possible, while for classification, it aims to map the data into a feature space in which the samples from different classes could be clearly separated. LSE [21] is an effective dimensionality reduction method to visualize the high dimensional data in 2-dimensional or 3-dimensional space. However, some limitations are exposed when it is applied to recognition tasks. One is the out-of-sample problem. LSE can yield an embedding directly based on the training data set but, because of the implicitness of the nonlinear map, when applied to a new sample, they cannot find the sample’s image in the embedding space. Another is that the classical LSE neglects the class information, which will inevitably impair their performances on pattern recognition. In this paper, we propose two orthogonal locally discriminant spline embedding techniques (OLDSE-I and OLDSE-II). OLDSE-I/OLDSE-II aims to take full advantage of class information and orthogonal subspace to improve discriminant power of the original LSE. On the one hand, we encode discriminant information in preserving local geometrical structure of the data samples by linear operators such as translation, rescaling, and rotation. Here, maximum margin criterion (MMC) [26] is incorporated to explore the optimal linear operators. Specifically, we find a linear transformation V which projects the high-dimensional data samples on a low-dimensional space. The dimensionality reduction matrix V is obtained by optimizing an objective function, which holds the strong discriminant power of MMC and at the same time preserve the intrinsic geometry of the neighbors as LSE in the reduced space. On the other hand, in order to further improve the discriminability of the proposed method, we also use the Gram–Schmidt or Duchene–Leclercq orthogonalization step for obtaining orthogonal basis functions which can provide a more faithful representation for the input data.In order to overcome the out-of-sample problem, an explicit linear mapping from X to Y, i.e. Y=VTX, is imposed. Thus the objective function for the original LSE can be converted to the following form:(16)J1(Y)=mintr(YMYT)=mintr(VTXMXTV).Once linear transformation matrix V is determined, mapping new data points to the lower dimensional space becomes trivial. Considering a new test data sample xtthat needs to be mapped, the test sample is projected onto the subspace using the dimensionality reduction matrix V. So we have:yt=VTxt.Therefore, mapping the new data point reduces to a simple matrix vector product.Based on the analysis mentioned above, it can be found that the linear approximation to the original LSE seeks to preserve local structure defined by the nearest neighbors as much as possible. It often fails to preserve within-class local geometry, which is very important for pattern classification, because the nearest neighbors may belong to different classes due to influence of complex variations, such as period, location, and illumination conditions. Thus, in order to improve its classification ability, we attempt to construct different translation and rescaling models for each class. Let’s use the same example as in Fig. 1 to illustrate this point. Fig. 2shows two-dimensional embedding of plant leaf images after translation, where it can be seen that the discriminability is significantly improved. Moreover, a rescaling coefficient is introduced to the proposed algorithm and the discriminability is also improved, which can be found in Fig. 3. It should be noted that building appropriate translation and rescaling models for each class can improve the recognition accuracy of the classical LSE significantly. However, when applying the proposed algorithm to real world data, how to explore the optimal translation vectors and rescaling coefficient is still an open problem. In this study, MMC is adopted to find the optimal translation vectors and rescaling coefficient, which will be explored by transformation.MMC [26] aims at maximizing the average margin between classes in the projected space. Let Swand Sbbe the within-class scatter matrix and the between-class scatter matrix defined by(17)Sw=∑i=1c∑j=1nixji-mixji-miT,(18)Sb=∑i=1cni(mi-m)(mi-m)T,where c is the number of classes, m is the total sample mean vector, miis the average vector of the ith class, niis the number of samples in the ith class, andxjiis the jth sample in the ith class. The objective function of MMC under projection matrix V is(19)J(V)=maxtr{VT(Sb-Sw)V}.Confining the column vectors in V to be unit vectors, V that maximizes Eq. (19) can be calculated through the following eigenvalue equation(20)(Sb-Sw)v=λv.It is reasonable to use MMC in our study. On the one hand, MMC makes it possible to implicitly model approximately optimal translation and rescaling operators. On the other hand, comparing MMC with the classical LDA, the former avoids calculating the inverse within-class scatter, i.e. (Sw)−1Sbis substituted by Sb−Sw. This can not only make the computation more efficient but avoid the small sample size (SSS) problem of the within-class scatter.In order to obtain optimal linear discriminant embedding, we produce an explicitly linear mapping from X to Y, i.e. Y=VTX. This linear transformation matrix V is obtained by optimizing an objective function, which captures the discrepancy of the local geometries in the reduced space and introduces the maximum margin criterion (MMC) simultaneously. That is to say, this objective optimization satisfies Eqs. (16) and (19). Then, the problem can be written as the following multi-object optimized problem:(21)mintr(VTXMXTV)maxtr(VT(Sb-Sw)V).Furthermore, there are two constraints in LSE, that is(22)VTXXTV=I(23)Ye=0.Eq. (23) requires the outputs{yi}i=1nto be centered on the origin, which removes the translational degree of freedom. In contrast to the original LSE, our proposed method should neglect this constraint, that is to say, we should make use of the translational factor to maximize the average margin between different classes in the embedded space. So the above optimization can be solved as the following constrained objective function:mintr(VTXMXTV)maxtr(VT(Sb-Sw)V)(24)s.t.VTXXTV=I.The constrained multi-object optimization is conducted to minimize the reconstruction error and maximize the margin between difference classes simultaneously. We formulate this discriminator by using the linear manipulation as follows:(25)mintr{VT(XMXT-β(Sb-Sw))V}s.t.VTXXTV=I.where β is a scaling parameter to unify the different measures of XMXTand Sb−Sw. To solve the above optimization problem, we use the Lagrangian multiplier:(26)∂∂Vtr{VT(XMXT-β(Sb-Sw))V-λ(VTXXTV-I)}=0Thus, we can get(27)(XMXT-β(Sb-Sw))v=λXXTv.Let the column vectors v1,v2,… ,vdbe the d smallest generalized eigenvectors of XMXT−β(Sb−Sw) and XXTcorresponding to the d smallest eigenvalues. The transformation matrix V which minimizes the objective function is as follows:(28)V=[v1,v2,…,vd].We call the new linear subspace method as locally discriminant spline embedding (LDSE). In practical problems, one often suffers from the difficulty that XXTis singular. This stems from the fact that sometimes the number of samples in the training set is much smaller than the dimension of each data point. To address the complication of a singular XXT, a PCA step is adopted to project the data set to a PCA subspace so that the resulting matrix XXTis nonsingular.It is well known that the generalized eigenvectors obtained by solving Eq. (27) are nonorthogonal. The two methods to produce orthogonal basis vectors are discussed in the following.We use the Gram–Schmidt orthogonalization to produce orthogonal basis vectors, which is called OLDSE-I. Set g1=v1, and assume that k−1 orthogonal basis vectors g1,g2,… ,gk−1 have been worked out, thus gkcan be computed as follows:(29)gk=vk-∑i=1k-1giTvkgiTgigi.Then G=[g1,g2,… ,gd] is the transformation matrix of OLDSE. The main procedure for the orthogonal locally discriminant spline embedding algorithm is summarized in Table 1.Inspired by the work in [27], we propose another method for orthogonalizing the basis functions derived from the LDSE algorithm, which is called OLDSE-II. Let L=XMXT−β(Sb−Sw). The OLDSE-II algorithm seeks to find a set of orthogonal basis vectors v1,v2,… ,vdby solving the following optimization problem:(30)mintr{VTLV}s.t.v1Tv2=v1Tv3=…=vd-1Tvd=0v1TXXTv1=v2TXXTv2=…=vdTXXTvd=1.It is easy to check that v1 is the eigenvector of the generalized eigenproblemLv=λXXTvassociated with the smallest eigenvalue. Since XXTis always nonsingular in the PCA subspace, v1 is the eigenvector of the matrix (XXT)−1L associated with the smallest eigenvalue.In order to get the kth basis vector, we minimize the following objective function(31)minvkTLvkwith the constraintsv1Tvk=v2Tvk=⋯=vk-1Tvk=0,vkTXXTvk=1.To solve the above optimization problem, we use the Lagrangian multiplier:Jk=vkTLvk-λvkTXXTvk-1-μ1v1Tvk-⋯-μk-1vk-1Tvk.We set the partial derivative of Jkwith respect to vkto zero and obtain(32)2Lvk-2λXXTvk-μ1v1-⋯-μk-1vk-1=0.Multiplying the left side of Eq. (32) byvkT, we obtain(33)2vkTLvk-2λvkTXXTvk=0.Multiplying the left side of Eq. (32) successively byv1T(XXT)-1,…,vk-1T(XXT)-1,now we can obtain a set of k−1 equations as follows:(34)μ1v1T(XXT)-1v1+⋯+μk-1v1T(XXT)-1vk-1=2v1T(XXT)-1Lvkμ1v2T(XXT)-1v1+⋯+μk-1v2T(XXT)-1vk-1=2v2T(XXT)-1Lvk……μ1vk-1T(XXT)-1v1+⋯+μk-1vk-1T(XXT)-1vk-1=2vk-1T(XXT)-1Lvk.We defineμk−1=[μ1,… ,μk−1]T,Vk−1=[v1,… ,vk−1], andQk-1=Vk-1T(XXT)-1Vk-1.So Eq. (34) can be represented in a matrix equationQk-1μk-1=2Vk-1T(XXT)-1Lvk.Thus(35)μk-1=2Qk-1-1Vk-1T(XXT)-1Lvk.Let us now multiply the left side of Eq. (32) by (XXT)−12(XXT)-1Lvk-2λvk-μ1(XXT)-1v1-⋯-μk-1(XXT)-1vk-1=0.This can be expressed using matrix notation as2(XXT)-1Lvk-2λvk-(XXT)-1Vk-1μk-1=0.With Eq. (35), we obtain(36)I-(XXT)-1Vk-1Qk-1-1Vk-1T(XXT)-1Lvk=λvk.As shown in Eq. (33),λis just the criterion to be minimized, thus vkis the eigenvector of(37)Rk=I-(XXT)-1Vk-1Qk-1-1Vk-1T(XXT)-1Lassociated with the smallest eigenvalue ofRk.The main procedure for the iteratively orthogonal locally discriminant spline embedding algorithm is summarized in Table 2.In this section, we provide an analysis of the computational complexity of OLDSE-I and OLDSE-II as a function of the number of samples n,the input dimension D,the intrinsic dimension d,and other related factors if necessary. In the tables, k is the number of nearest neighbors, p denotes the number of iterations, and l indicates the number of polynomials in a spline function. The complexity of OLDSE-I or OLDSE-II is dominated by three parts: nearest neighbor graph construction, alignment matrix manipulation, and solving orthogonal eigenvectors. First, OLDSE-I and OLDSE-II performs nearest neighbor searches, which has computational complexity O(Dnlogn). Subsequently, They perform the alignment matrix M and related manipulation in O(n(k+l)3)+O(Dn2+D2n),and they solve orthogonal basic vectors in O(dD2+d2D) and O(pn3), respectively. The complexity of main steps in OLDSE-I and OLDSE-II is listed as Table 3.

@&#CONCLUSIONS@&#
