@&#MAIN-TITLE@&#
On the convergence and origin bias of the Teaching-Learning-Based-Optimization algorithm

@&#HIGHLIGHTS@&#
A geometric interpretation is applied to explain population convergence.A property of TLBO introduces an origin bias in the Teacher Phase.A converged population will continue searching in the direction of the origin for better solutions.The origin bias is directly tied to convergence.A faster rate of convergence yields a higher success rate for problematic functions.

@&#KEYPHRASES@&#
Teaching-Learning-Based-Optimization,Bias,Origin,Convergence,Evolutionary computing,

@&#ABSTRACT@&#
Teaching-Learning-Based-Optimization (TLBO) is a population-based Evolutionary Algorithm which uses an analogy of the influence of a teacher on the output of learners in a class. TLBO has been reported to obtain very good results for many constrained and unconstrained benchmark functions and engineering problems. The choice for TLBO by many researchers is partially based on the study of TLBO's performance on standard benchmark functions. In this paper, we explore the performance on several of these benchmark functions, which reveals an inherent origin bias within the Teacher Phase of TLBO. This previously unexplored origin bias allows the TLBO algorithm to more easily solve benchmark functions with higher success rates when the objective function has its optimal solution as the origin. The performance on such problems must be studied to understand the performance effects of the origin bias. A geometric interpretation is applied to the Teaching and Learning Phases of TLBO. From this interpretation, the spatial convergence of the population is described, where it is shown that the origin bias is directly tied to spatial convergence of the population. The origin bias is then explored by examining the performance effect due to: the origin location within the objective function, and the rate of convergence. It is concluded that, although the algorithm is successful in many engineering problems, TLBO does indeed have an origin bias affecting the population convergence and success rates of objective functions with origin solutions. This paper aims to inform researchers using TLBO of the performance effects of the origin bias and the importance of discussing its effects when evaluating TLBO.

@&#INTRODUCTION@&#
A new technique has risen in the field of global optimization called Teaching-Learning-Based-Optimization (TLBO) [8,9]. The method is based on the philosophy of teaching and learning and mimics the influence of a teacher on the output of learners. Unlike many other global optimization algorithms, TLBO does not require user-defined algorithm-specific control parameters (e.g., Genetic Algorithms use a mutation rate, crossover rate and a few others). Instead, these control parameters are probabilistically determined during runtime and therefore tuning is not required in order to improve performance. TLBO does however still require common control parameters such as population size and number of generations; thus, it has been referred to as an algorithm-specific parameter-less method [7]. Proper tuning of the common control parameters is still required to improve performance; however, the burden of tuning these control parameters is comparatively less, making TLBO a very desirable optimization method.TLBO has been shown to attain comparatively good results on many constrained and unconstrained benchmark functions [1,9,16], multi-objective benchmark functions [11], and engineering problems [8,10,12,13]. It is shown to typically outperform many global optimization algorithms, solving the same benchmark problems with fewer function evaluations, better solution, and higher success rate [5–12]; thus TLBO has been shown to be an effective global optimization method. An interesting and insightful story unfolds around TLBO, as discussed below, which shows the repercussions that inexact experiment replications can have on the conclusion of an algorithm's performance, and alludes to the importance of being critical when evaluating an algorithm.Rao et al. [9] originally conducted experiments on five sets of non-linear unconstrained benchmark functions to evaluate TLBO's comparative performance with other Evolutionary Algorithms (EAs). The authors state that “the results show better performance of [the] TLBO method over other nature-inspired optimization methods for the considered benchmark functions”. In the note by Ĉrepinŝek et al. [1], the authors reproduce these experiments using prototypical TLBO implementations from [8,9]. During their tests they reveal several issues and misconceptions that appeared in the original papers [8,9]. Most notably, that the five experiments conducted with TLBO in [9] were not performed using similar experimental settings for direct comparison to the results published using other EAs. In response to this note, Waghmare [16] repeated these experiments using the TLBO code from the appendix of [6] and argued that their work presents “comparatively better results” to that of [1]. Unfortunately, experiment 1 by Waghmare [16] is completed using a population size of 10, as stated in their Table 8, whereas Rao et al. [9] and Ĉrepinŝek et al. [1] (from their publicly available code) had used a population size of 20. Different results are to be expected, thus experiment 1 in [16] is somewhat irrelevant for comparison purposes. In addition to this, Ĉrepinŝek et al. [2] reiterate that duplicate removal was used for all unconstrained problems in [1], as was the case in [9], whereas [16] did not apply the technique. As well, Ĉrepinŝek et al. [2] reveal that the number of fitness evaluations using the publicly available code in [6] is approximated post-run and is not exact. Despite these differences, Ĉrepinŝek et al. [2] provide statistical evidence that the “results are mostly insignificantly different, despite Waghmare's claim of significance”. During our replication of these experiments, we had much difficulty trying to reproduce the results of [16] using the same code obtained from [6]; although, we did modify the code to accommodate a maximum number of fitness evaluations, which may explain the difference in performance. In fact, our results were actually much closer to those in [1].

@&#CONCLUSIONS@&#
