@&#MAIN-TITLE@&#
Artificial neural networks as speech recognisers for dysarthric speech: Identifying the best-performing set of MFCC parameters and studying a speaker-independent approach

@&#HIGHLIGHTS@&#
The best performing set of MFCC parameters for dysarthric speech was studied.A speaker-independent dysarthric ASR model based on ANNs is proposed.The ASR systems trained by mel cepstrum with 12 coefficients provided the best accuracy.The proposed speaker-independent ASR model provided 68.38% word recognition rate.The highest word recognition rate of the speaker-dependent ASR systems was 95%.

@&#KEYPHRASES@&#
Dysarthria,Automatic speech recognition,Artificial neural network,Mel-frequency cepstral coefficients,

@&#ABSTRACT@&#
Dysarthria is a neurological impairment of controlling the motor speech articulators that compromises the speech signal. Automatic Speech Recognition (ASR) can be very helpful for speakers with dysarthria because the disabled persons are often physically incapacitated. Mel-Frequency Cepstral Coefficients (MFCCs) have been proven to be an appropriate representation of dysarthric speech, but the question of which MFCC-based feature set represents dysarthric acoustic features most effectively has not been answered. Moreover, most of the current dysarthric speech recognisers are either speaker-dependent (SD) or speaker-adaptive (SA), and they perform poorly in terms of generalisability as a speaker-independent (SI) model. First, by comparing the results of 28 dysarthric SD speech recognisers, this study identifies the best-performing set of MFCC parameters, which can represent dysarthric acoustic features to be used in Artificial Neural Network (ANN)-based ASR. Next, this paper studies the application of ANNs as a fixed-length isolated-word SI ASR for individuals who suffer from dysarthria. The results show that the speech recognisers trained by the conventional 12 coefficients MFCC features without the use of delta and acceleration features provided the best accuracy, and the proposed SI ASR recognised the speech of the unforeseen dysarthric evaluation subjects with word recognition rate of 68.38%.

@&#INTRODUCTION@&#
Dysarthria is a neurological impairment that damages the control of the motor speech articulators, which the malfunction is caused by the lack of control over the speech-related muscles, the lack of coordination among them, or their paralysis. It is often associated with irregular phonation and amplitude [1,2]. As a result of the impairment, the speech signal is compromised and its intelligibility is reduced [3,4]. According to [5], low intelligibility is one of the most detrimental social characteristics of dysarthria that affects different aspects of the lives of people with such disability.Automatic Speech Recognition (ASR) systems identify the uttered word(s) represented as an acoustic signal and rely on a given lexicon to recognise the spoken word(s). They have several applications in health care, the military, telephony, and other domains [6]. They can be very helpful for speakers with dysarthria, because the disabled persons are often physically incapacitated and unable to use keyboards [7,8].Most state-of-the-art commercial ASR systems are designed for speakers without speech disabilities, (i.e. non-speech disordered) and exclude those with speech disabilities [9]. These ASR systems record lower performance for individuals who suffer from dysarthria (specifically severe dysarthria [10,11]) than for people without speech disabilities as dysarthric speech is different from normal speech [12–14]. Therefore, there has recently been a trend towards creating specialised ASR systems for individuals with dysarthria instead of using ASR systems designed primarily for speakers without speech disabilities [3,10,15,16]. Thus, it is necessary to propose an ASR model specifically built for users with dysarthria that delivers adequate accuracy; specialised systems have generally achieved comparatively better performance for people with speech disorders [2,4,10,16].According to [9], it is easier for people with dysarthria to utter isolated words rather than a continuous sequence of words. Similarly, it is more effective when the size of the ASR vocabulary is small and includes only simple words with one or two syllables in order to boost recognition rates with reduction or minimisation of dysarthric ASR errors [9]. Therefore, isolated-word and small-vocabulary ASR models are in greater demand for dysarthric speech recognition [10,15].Although ASR technologies for dysarthria have been considered, previous studies show that ASR systems for users with dysarthria have not yet attained an adequate performance level in terms of generalisability because of the complex issues related to dysarthric speech [13]. For example, increased variability due to physical fatigue and frustration of individuals with dysarthria, as well as variations in the severity levels of the disease, make it difficult to produce an ASR model to be used by most individuals with dysarthria.A speaker-dependent (SD) ASR system is capable of recognising the speech of users whose acoustic data have been captured while training the system [17]. If an unknown speaker uses the system, the accuracy of the system is reduced. In the context of dysarthric ASR system, as the performance of an ASR system is reduced without proper training data, a major problem with SD paradigm is that people with dysarthria may be rapidly fatigued by the effort needed to provide the vocabulary in order to train the ASR system [18,19].Dysarthric speaker-adaptive (SA) ASR models are usually trained as normal speaker-independent (SI) ASR models, but they adapt to new users’ data while the systems are being used by the disabled people. Particularly, the systems learn the speech of new users every time they utter a new word. These systems may provide low recognition rates during early stage of usage, but the performance will gradually improve over longer time of usage [20]. Hence, they do not recognise dysarthric speech properly out-of-the-box. Furthermore, from the perspective of performance, none of these models are capable of identifying speech uttered by unforeseen users accurately, and therefore, they are not suitable for applications such as in the public telephone network [17].In contrast, SI speech recognisers are trained with databases containing utterances of several speakers. These systems are capable of recognising the speech of a variety of users more accurately than speaker dependent or adaptive ASR systems, including the speech of users whose acoustic data have not been provided during the training process. Consequently, an SI ASR model for users with dysarthria is required in order to recognise more accurately the speech of a wide range of users with speech disabilities; such ASR system can then be accessed by speech-disabled people using public services [11]. As an illustration, usage of banking phone services requires a user to input menu commands by pressing physical numeric buttons located on a phone. Normal people can easily press the keypad buttons, but dysarthric people may be unable to do that, because they are usually physically handicapped. As such, an ASR system is extremely useful in facilitating the disabled people to utter the numbers associated with the menu commands. SD or SA ASR models are incapable of performing this function with sufficient accuracy for new users, but SI ASR systems may be capable of providing the required generalisability and performance so that speech-disabled people can communicate their instructions by using public services. Therefore, building SI ASR systems designed for users with dysarthria is an important topic that should be investigated further.In order for an ASR system to be operable, acoustic features of utterances must be presented to the system using a process called Feature Extraction. The usage of Mel-Frequency Cepstral Coefficients (MFCCs) is the most common feature-extraction method in ASR applications, which represent speech signals in cepstral domain [20]. It is a representation defined as the real cepstrum of a windowed short-time signal derived from the Fast Fourier Transform of that signal, in which the frequency bands are spaced on the mel scale equally (inspired by the human auditory perception system). MFCCs have been widely used for several speech-disorder signal-processing tasks such as speech disability classification [21,22] and dysarthric speech recognition. The MFCCs are usually presented as mel cepstrum with 12 coefficients, their first and second derivatives.MFCCs have been proven to be an appropriate representation of dysarthric speech [3,23,24]. Although it is advisable to use all MFCC-based feature sets (i.e. MFCCs (12 coefficients), Delta-MFCCs, and Delta/Delta MFCCs and log energies) together for training an ASR system for normal speakers, it remains unexplored if all the MFCC features and/or its combination with the first and second derivative should be used as inputs to dysarthric ASR systems based on Artificial Neural Networks (ANNs). This question should be looked into because normal speech is different from dysarthric speech, and the same goes for the respective acoustic features. Selecting the best representation set of dysarthric acoustic features is a crucial issue because it may directly influence the recognition accuracy of dysarthric ASR systems.Hence, this paper attempts to resolve the above issues relating to dysarthric ASR systems. The objectives of this study are:1.To identify the most effective MFCC-based feature set for representing dysarthric acoustic signals in order to provide an ANN-based dysarthric ASR model. The MFCC parameters considered here are mel cepstrum with 12 coefficients, their first and second derivatives, and all the acoustic features.To study the application of ANNs in a fixed-length, isolated-word SI dysarthric ASR system. The vocabulary size is 11 including 10 digits and silence.The first objective was achieved by providing 28 SD ANN-based ASR systems over seven dysarthric subjects, the results of which were compared. For each speaker, four speech recognisers were provided, and each of them was trained by one set of the MFCC parameters (12 MFCCs, Delta-MFCCs, Delta-Delta MFCCs, and all sets together). The energy information was not considered here, because it is often difficult for dysarthric individuals to maintain a steady volume; hence, according to Green et al. [18], the energy information may not be useful. The second objective was accomplished by proposing an automatic dysarthric digit recogniser, which provides spoken-numerical-command capability; this model is very useful for users with speech disabilities who are physically incapacitated. The accuracy and recognition rate of the proposed SI ASR system were measured by using evaluation data collected from people with severe, moderate, and mild dysarthria respectively. It is pertinent to note that no acoustic sample from the evaluation subjects was considered during the SI ASR training in order to highlight the generalisability and speaker independency of the proposed ASR model.This section surveys the studies of state-of-the-art ASR technologies for users with dysarthria. Most of the experimental dysarthric speech recognition systems resorted to SD or adaptive approaches because of the above-mentioned dysarthric speech issues. As an illustration, Hasegawa-Johnson et al. [16] provided two isolated-word SD ASR systems (10-digit vocabulary) based on the data collected from three subjects with dysarthria: one female and two males with one control subject. The speech samples were recorded using an array of seven microphones and four cameras mounted on top of a computer monitor. The first system was a phone-based Hidden Markov Model (HMM), and the second was a fixed-length isolated-word ASR system based on Support Vector Machines (SVMs). The former was successful for two subjects, but it failed for one of the subjects with the tendency to delete consonants in a word. Similarly, the SVM-based ASR failed to perform for one of the subjects with dysarthria, because he suffered from stuttering, but it was successful for the other two subjects. The authors concluded that HMM-based dysarthric ASR models may provide robustness against large-scale word-length fluctuations, and SVM-based models can handle the deletion or reduction of consonants.Selouani et al. [3] proposed another SD ASR based on HMMs for English and French speakers with dysarthria for continuous speech. The ASR system was trained using speech materials collected from four dysarthric speakers in the Nemours database and one control speaker; the authors did not mention the severity of the subjects’ disabilities. The training speech samples were presented by mel cepstrum with 12 coefficients, their first and second derivatives, and their log energies. The training set is composed of 50 sentences (300 words), and the test is composed of 24 sentences (144 words). The average recognition rate of this SD system was 70% for the four dysarthric subjects.STARDUST, a HMM-based ASR system for users with severe dysarthria, was introduced in [10,18]. In this system, a new HMM was trained every time the user uttered a new word. The training and evaluation data were obtained from five individuals with dysarthria and were presented as mel cepstrum with 12 coefficients inclusive of their first derivatives. The speech samples were collected by Andrea DA.400 microphone array or Acoustic Magic Voice Tracker array at distances of 0.5–3m from the participants. The system was an isolated-word ASR and included a 10-digit vocabulary. In another example of isolated-word HMM-based ASR for users with dysarthria [25], a small and medium vocabulary size (SD) ASR system for spastic dysarthria was studied.ANN-based ASR system have been successfully employed for normal speech as reported in the literature (such as [26–28]). ANNs are mathematical models inspired by natural neural systems that learn the function by capturing information from given input and output samples. Jayaram and Abdelhamied studied the application of ANNs in a SD dysarthric speech recognition system but with limited success [29]. The authors applied ANNs in a 10-word ASR system to recognise the speech of one speaker with severe dysarthria. They provided two recognisers: the first was trained using MFCC parameters and the second using the formant frequencies; it was found that the first system performed better than the second and outperformed five human listeners. The results of this study are not concrete, because the ASR system was trained and evaluated using only one subject with dysarthria.Several studies on ANN/HMM hybrid ASR showed that the hybrid model is a suitable platform for normal ASR [26]. However, the applications of hybrid ASR for users with dysarthria have not been widely studied, because proper ANN training data to perform dysarthric phoneme recognition are not easily achievable. Neural networks within the ASR hybrid approach (normal speakers) are usually applied to provide the language model since the hybrid approach is phone-based; nonetheless, for dysarthric speech this is a challenging task, because identifying the phones and labelling them, i.e. segmenting the speech utterances for dysarthria, is a difficult, error-prone, and time-consuming process due to low speech intelligibility of the disabled persons. Moreover, since the unintelligibility of dysarthric speech is because of the combination of many articulatory behaviours that can lead to phonemic insertion errors in or around words [30,31], dysarthric ASR approaches that consider word-based units may be more successful than those which depend on phone based units.Despite the above SD and SA systems, there had been a few unsuccessful attempts to provide SI ASR systems for users with dysarthria. They were unsuccessful because the error rates were too high for these speech recognition systems to be of any practical application. For example, Sanders and his colleagues [13] studied how a normal, SI HMM-based ASR system behaved when it was used by people with dysarthria. The ASR, trained with non-speech-disordered speech data, was evaluated with dysarthric data acquired from two male speakers with mild dysarthria. The results for the two evaluation subjects showed WER of 15.4% and 41% respectively. However, the same ASR system had better performance when it was trained and tested with dysarthric data, (i.e. as a SD ASR). The SD ASR system for the same two dysarthric subjects had WER of 2.6% and zero respectively. Similar results were described by Talbot for the ENABL project [14]. The author verified a commercial ASR system with data collected from 10 individuals with dysarthria (five males and five females) and reported that the error rate was as high as 71%.Sharma and Hasegawa-Johnson considered the database used in this study to provide two isolated-word HMM-based speech recognisers for users with dysarthria (one SD and another SA) [19]. For the adaptive model, they provided an isolated-word, SI ASR system for speakers without disabilities first; the system was based on the TIMIT database, in which Perceptual Linear Prediction (PLP) coefficients were extracted as acoustic features. Subsequently, the authors utilised the speech of seven speakers with dysarthria from the UA-Speech database to verify the normal ASR as a SA dysarthric ASR system. The maximum average recognition rate for the SA systems was 36.8% and 30.84% for the SD systems. However, to the best of our knowledge, they did not provide any SI model for speakers with dysarthria. The method proposed here achieved better results even as an SI ASR system.Therefore, the literature review shows that there is no SI ASR model designed specifically for individuals with dysarthria. This study explains how ANNs can be trained by word-based acoustic features to be used as a fixed-length automatic digit recogniser, which has the capability of recognising speech of unknown dysarthric individuals. Such speaker-independent ASR system will benefit a wider range of people with dysarthria. Moreover, each of the previous studies considered a different set of acoustic features. Therefore, it is important to identify which MFCC-based feature set represents dysarthric acoustic features most effectively.

@&#CONCLUSIONS@&#
In this paper we studied the application of ANNs in an SI ASR model for individuals with dysarthria. In addition, several SD ANN-based speech recognisers for users with dysarthria were provided, and the results were compared in detail. The purpose is to investigate and to ascertain the best MFCC-based feature set that can represent dysarthric acoustic features; the representation is then used by an ANN-based SI ASR system designed for individuals with dysarthria. The performance of the proposed ASR models was measured in terms of word recognition rate and accuracy of evaluation. Speech samples of the subjects with speech disabilities were evaluated by the proposed SI ASR systems for each dysarthric severity level separately. The speech data of the evaluation subjects were not included for the training of the SI speech recognisers. This exclusion of speech data of the evaluation subjects allows the generalisability of the proposed models to be evaluated.The results show that mel cepstrum with 12 coefficients can be selected as the best set of MFCC acoustic features in order to train an ANN-based ASR system for speakers with dysarthria. The WRR of the dysarthric SI ASR model, trained with mel cepstrum including 12 coefficients achieved an average of 68.38%. It also produced 98.07% WRR for the speech of unanticipated speakers without speech disabilities. The highest WRR of speaker-dependent ASR models was 95%.