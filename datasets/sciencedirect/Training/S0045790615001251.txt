@&#MAIN-TITLE@&#
Three-layered hierarchical scheme with a Kinect sensor microphone array for audio-based human behavior recognition

@&#HIGHLIGHTS@&#
Proposed three-layered hierarchical human behavior recognition uses only audio data.The use of Kinect sensor microphone array for data capture and fusion is explored.The proposed scheme increased recognition robustness compared with conventional GMM.

@&#KEYPHRASES@&#
Kinect sensor,Human behavior recognition,Sensing data fusion,Voting-GMM,State machine diagram,

@&#ABSTRACT@&#
This study develops a hierarchical scheme with three processing layers for human behavior recognition. The proposed scheme is an audio-based approach that employs a microphone array of the Kinect sensor for sensing and acquiring acoustic data to classify human behavior. The three processing layers, namely the feature layer, acoustic event classification layer, and specific behavior recognition layer, are interrelated, and the sensing data fusion, Gaussian mixture model with a classification tree, and state machine diagram to regulate human behavior are employed in these three layers, respectively. With enhanced performance of the feature and acoustic event classification layers, the proposed scheme exhibits increased human behavior classification accuracy. Human behavior recognition experiments were conducted in a research office, and three specific office behavior modes, namely “Laboratory meeting,” “Classmate chatting,” and “Laboratory study interaction,” were effectively classified using the proposed method.

@&#INTRODUCTION@&#
Human behavior recognition has attracted substantial attention in recent years [1–8]. It has practical application in fields such as home security, office surveillance, and elderly healthcare. With advances in pattern recognition methods, artificial intelligence algorithms, and hardware sensing devices, a satisfactory smart recognition system without a personal viewer will be realized in the near future. Microsoft Kinect, a popular sensor device [9,10], performs outstandingly in human gesture recognition [11]; it is expected to accelerate the development of automatic human behavior recognition for smart home and smart office applications.Human behavior recognition studies have mainly focused on image- or video-based categorization of methods [3–7]. Video-based human behavior recognition employs image-sensor-derived or conventional-CCTV-derived video stream data for image processing and recognition. In [4], high-level text reports and human behavior explanations are obtained from a video stream captured by a signal camera. Social behavior recognition is explored in [5], in which a continuous video is segmented into numerous actions by establishing a temporal context model that combines the features of spatio-temporal energy models and agent trajectories. In abnormal behavior recognition in [6], an extension of abnormal face recognition studies, detecting contextual abnormal human behavior in video surveillance is focused on. In [7], behavior classification is attempted on the basis of the image data of three-dimensional Zernike moments, and the classification is effective for behaviors with low-order moments [7].Although several studies have focused on human behavior recognition, the vast majority is video-based; behavior recognition studies employing only recorded acoustic data without captured video or image data are extremely rare. Furthermore, despite the Kinect sensor’s highly functional gesture recognition performance [11], video-based studies have rarely employed the Kinect sensor as the data acquisition receiver. Moreover, the Kinect sensor can be employed in audio-based human behavior recognition by embedding four microphones in a well-arranged microphone array [9,10]. This study proposes an audio-based approach for human behavior recognition, and a three-layered hierarchical scheme that employs a Kinect sensor microphone array is developed. The proposed hierarchical scheme contains three independent operation layers, namely the feature layer, acoustic event recognition layer, and behavior classification layer. In the acoustic event recognition layer, the Gaussian mixture model (GMM) [12,13], is employed for classifying 17 specific vocal and nonvocal acoustic events. For enhancing GMM recognition and acoustic event classification, two GMM-based approaches-hierarchical GMM with a vocal and nonvocal classification tree and voting-GMM with a classification tree – are proposed. In the feature layer, a sensor data fusion scheme is utilized by the Kinect microphone array. To further enhance the two aforementioned GMM-based methods, two data-fusion-based approaches-fusion-based GMM with a hierarchical tree and fusion-based voting-GMM with a hierarchical tree – are presented. In the behavior classification layer, a state machine diagram [14,15] with state transition triggered by the recognized acoustic event is used to formulate three office human behaviors. The proposed three-layered hierarchical scheme employing the Kinect microphone array for audio-based human behavior recognition has several advantages over those without the array:•a new three-layered hierarchical human behavior recognition scheme that uses only acoustic data;the utilization of the Kinect microphone array for acoustic data fusion in human behavior recognition;increased recognition robustness compared with a conventional GMM, arising from the use of the voting approach and the event classification tree scheme.The remainder of this paper is organized as follows. Section 2 details the utilization of GMM to recognize acoustic data that is captured by the microphone array of the Kinect sensor. Section 3 introduces human behavior recognition by using a three-layered hierarchical scheme and Kinect sensors. Section 4 presents the experiment results where the effectiveness and performance of the proposed scheme in human behavior recognition are demonstrated. Finally, concluding remarks to summarize this work are given in Section 5.This study employs the Kinect microphone array and a general microphone as the audio receiver and recorder that senses a specified source in office environments. The microphone array in the Kinect sensor contains four microphones and therefore supplies four channels of acoustic data. The captured acoustic data is saved for signal analysis and recognition in the WAV/PCM format by using Microsoft Windows SDK.Fig. 1shows how the acoustic data captured from the Kinect microphone array are recognized using GMM classifiers. Each of the four microphones in the array is considered as an independent audio source channel for capturing identical acoustic data; therefore, the acoustic data from each microphone is analyzed and recognized individually. As shown in Fig. 1, the acoustic data is preprocessed through feature extraction, and the extracted acoustic feature is recognized using the established GMM acoustic event models. Feature extraction–extracting the primary characteristics of the acoustic data – is critical for subsequent GMM calculations. After feature extraction, acoustic data belonging to the time domain are transformed as parameters belonging to the frequency domain. In this study, the recognition system contains numerous vocal acoustic events generated by a person’s voice; therefore, the linear predictive cepstral coefficient parameter, frequently employed in automatic speech recognition, is considered as a feature of the Kinect microphone array-sensed acoustic data. The arrayed microphones are especially useful in effectively receiving audio data near or moderately far from the Kinect device, which helps alleviate the problem of sound level variation between vocal and nonvocal acoustic events.For classification in this study, three specific behavior modes in a research office environment, “Lab. meeting,” “Classmate chatting,” and “Lab. study interaction,” are designed. To conveniently formulate these three behaviors, acoustic events occurring during these behaviors should be defined. In this study, 17 acoustic events (nine vocal and eight nonvocal) are defined in the behavior recognition system.The GMM is used to describe the acoustic event, and each of the 17 acoustic events has a corresponding GMM acoustic model. In addition, two GMMs, detailed in Section 3, describing vocal and nonvocal acoustic events are established. Given the acoustic nature of this study, GMM is a more appropriate statistical model than the hidden Markov model is for describing an acoustic event [16,17]; the HMM is especially employed in speech recognition.A GMM is a weighted sum of M Gaussians and is denoted as follows [12,13]:(1)λ={wi,μi,Σi},i=1,2,…,M,∑i=1Mwi=1,wherewiis the weight of the mixtures,μiis the mean, andΣiis the covariance. To derive the GMM parameters for a certain acoustic event class, the expectation–maximization (EM) algorithm recommended in [18] is applied. This popular algorithm mainly includes the initialization phase, the expectation calculation phase, and the phase of maximizing the expectation value of the acoustic data [18]. By using the EM algorithm, nine GMMs are trained for their corresponding nine vocal acoustic events, eight GMMs are trained for their corresponding eight nonvocal acoustic events, and two GMMs are trained for the vocal and nonvocal event categorization in the behavior recognition system. For rapid computing, the number of mixtures is set as one.In the test phase of acoustic event recognition, recognition is performed according to the established GMMs. The classifier deployed here is essentially a GMM classifier containing m separate GMMs. Setting m depends on the practical recognition applications; for example, event recognition to decide whether the test acoustic event is vocal or nonvocal requires a setting of m=2. Consider that the GMM classifier operates over a predefined time interval covering n acoustic feature vectors (n acoustic frames) of D dimensionsX={xi|i=1,2,…,n}and m acoustic events. During the recognition phase, the class ofXis determined by maximizing a posteriori probabilityP(λs|X)[12,13]:(2)sˆ=maxs={1,2,…m}P(λs|X)=maxs={1,2,…m}f(X|λs)f(X)·P(λs).Note that(3)f(xi|λs)=∑j=1Mwj·bj(xi),and(4)bj(xi)=1(2π)D/2·|Σs|1/2·exp-12(xi-μs)T(Σs)-1(xi-μs).By using these recognition derivations, the test acoustic dataXwith n acoustic frames is recognized as one of m acoustic event classes indicated bysˆat the end of the recognition test phase.The proposed three-layered hierarchical scheme employing Kinect sensors for human behavior recognition contains three individual, interrelated processing layers. Fig. 2depicts the structure of the proposed hierarchical scheme. The three layers in the hierarchical structure are the (1) feature layer for acquiring acoustic data and performing Kinect senor fusion, (2) acoustic event classification layer with tree-structured GMM for recognizing acoustic events, and (3) human behavior recognition layer with state machine diagrams for recognizing specific human behaviors. The developed scheme is implemented to classify three categories of office human behaviors, namely Lab. meeting, Classmate chatting, and Lab. study interaction.As mentioned in Section 2, the acoustic data captured by the Kinect microphone array is forwarded to the GMM classifier for recognizing specific acoustic events. When a large number of acoustic events are present in the recognition system, satisfactory recognition accuracy cannot be maintained because a large number of GMMs are required to calculate the likelihood degree of the captured data. Categorization of acoustic events defined in the system is helpful in overcoming this problem and increasing the accuracy of the recognized outcome. Therefore, a hierarchical GMM approach with an acoustic event classification scheme is proposed. All acoustic events are divided into two main categories, namely vocal and nonvocal. Vocal events are acoustic events generated by a person’s voice, such as speaking, chatting, coughing, and singing. Nonvocal events are all acoustic events not generated by a person. Fig. 3illustrates the improved hierarchical GMM with a vocal and nonvocal classification tree. Tables 1 and 2list the vocal and nonvocal acoustic events defined in the present study. As shown in Table 1, the nine vocal events mainly contain the active voices of the laboratory leader, four laboratory members, and four visitors. These nine events are collected and used to establish the unique vocal GMM. In addition, a nonvocal GMM is trained using the eight classes of nonvocal voices listed in Table 2. In the hierarchical tree structure shown in Fig. 3, 19 acoustic GMMs are required for event recognition: nine GMMs representing vocal acoustic events, eight representing nonvocal acoustic events, the additional vocal GMM, and the additional nonvocal GMM. In the proposed GMM with a hierarchical vocal and nonvocal classification tree, the test acoustic data is fed to the first level to calculate two likelihood scores by using (3) to verify whether the data belongs to either category of vocal events. If the likelihood score of vocal GMM is larger than that of nonvocal GMM, the test acoustic data is recognized as vocal and forwarded to the second level for further recognition. Subsequently, the test data is recognized as one of the nine defined vocal acoustic events by using (2). With an additional GMM recognition layer, the required number of comparisons of the derived likelihood scores [see (2)] is considerably reduced, consequently improving the event recognition performance. For example, if the test acoustic data is recognized as a vocal event, only ten comparisons of GMM likelihood scores are required – one in the first level and nine in the second level – which is superior to the 17 comparisons required in a conventional GMM without a hierarchical classification tree.In this study, nonvocal Events 3 and 4, opening of the laboratory door and knocking on the laboratory door, respectively, are vital and are the key events in the overall recognition system. The performance of human behavior recognition on the three office behaviors is largely dependent on the recognition accuracy of these two events. The criticality of nonvocal Events 3 and 4 is evident in the following section.Although the hierarchical GMM with an event classification tree is more efficient than the conventional GMM without event classification schemes, an improved approach for calculating the essential GMM classifiers is required. A GMM method employing the voting scheme [19], voting-GMM, is designed to enhance the conventional GMM classifier. Fig. 4displays the pseudocode form that explains the developed voting-GMM method. Voting-GMM feeds each acoustic frame into the GMM classifier to evaluate the likelihood score by using (3). In the voting-GMM scheme, the GMM acoustic event model with the highest derived likelihood score receives one vote. All acoustic frames n from the test data are considered as n votes for each GMM voting. The GMM with the highest votes, that is, the model with the highest vote counter value in the pseudocode shown in Fig. 4, is the acoustic event classification result. Compared with the conventional GMM in which the classifier employs the accumulated likelihood scores to make only one decision to choose the GMM event model, voting-GMM makes n+1 decisions to choose the GMM event model, with n complete votes for n GMM event model voting and one accumulated vote for the final GMM event model. Therefore, the accuracy of the proposed voting-GMM is higher than that of the conventional GMM, and acoustic event recognition of nonvocal Events 3 and 4 by using voting-GMM produces a higher human behavior recognition performance; these aspects are detailed in the following sections.The feature layer, located at the bottom of the three-layered hierarchical recognition scheme, is used to acquire acoustic data and perform sensing data fusion. This layer is therefore considered as the input layer. As mentioned in Section 2, the Kinect sensor microphone array contains four microphones that receive acoustic data. Each of these four microphones is used as an independent audio source channel for individual acoustic data processing, that is, feature extraction and model recognition calculations.In this study, employing sensing data fusion for recognition calculations is appropriate because it covers all acoustic data in the overall sensing environment. Data fusion is popular in pattern recognition, including speech pattern recognition. The two main categories of data fusion are feature-based fusion and model-based fusion. This study adopts model-based data fusion for acoustic event recognition. The employed model-based data fusion combines each likelihood score derived from GMM classifier calculations; the combined likelihood score together with data from multiple audio source receivers produces more accurate event recognition decisions than the conventional model does with its single likelihood score containing data from only one audio source receiver. In addition to the four Kinect sensor microphones, an independent microphone is added to the sensing environment for capturing acoustic data comprehensively. Two fusion-based acoustic event recognition approaches are developed: fusion-based GMM with a hierarchical tree and fusion-based voting-GMM with a hierarchical tree.For acoustic event recognition employing fusion-based GMM with a hierarchical tree, the combined likelihood score from multiple audio source channels is designed as follows:(5)sˆ=maxs={S1,S2}P(λs|X)=maxs={S1,S2}f̃(X|λs)f̃(X)·P(λs),(6)f̃(xi|λs)=fM(xi|λs)+fK-M1(xi|λs)+fK-M2(xi|λs)+fK-M3(xi|λs)+fK-M4(xi|λs)5,whereS1andS2denote vocal and nonvocal GMM event models, respectively;Mis the general microphone andK-Mi,i=1,2,3,4is the set of four microphones in the Kinect microphone array.(7)vˆ=maxv={V1,V2,…,V9}P(λv|X)=maxv={V1,V2,…,V9}f̃(X|λv)f̃(X)·P(λv),(ifsˆ=S1)(8)Nˆ=maxN={N1,N2,…,N8}P(λN|X)=maxN={N1,N2,…,N8}f̃(X|λN)f̃(X)·P(λN).(ifsˆ=S2)For the proposed fusion-based voting-GMM, because of the criticality of nonvocal Events 3 and 4 in behavior recognition, the voting-GMM with a hierarchical tree discussed earlier is first employed to recognize these two nonvocal events by using the additional microphone without data fusion. Subsequently, the proposed fusion-based voting-GMM with a hierarchical tree is employed to perform recognition calculations on the other 15 acoustic events by using the four arrayed microphones, as follows:(9)f̃(xi|λs)=fK-M1(xi|λs)+fK-M2(xi|λs)+fK-M3(xi|λs)+fK-M4(xi|λs)4.Eqs. (5) and (7) are maintained unchanged in fusion-based voting-GMM. However, (6) is then substituted with (9) because calculations are required only for data from the four microphones of the Kinect sensor. In addition,N={N1,N2,…,N8}in (8) is substituted withN={N1,N2,…,N6}because the recognition of nonvocal Events 3 and 4 have already been completed by the voting-GMM with a hierarchical tree.State machine is a popular technique for simulating operative procedures [14,15]. State machine has been crucial in automatic speech recognition for tasks such as modeling the grammar of natural languages for generating a language model. This study uses state machine diagrams to model three office human behaviors, namely Lab. meeting, Classmate chatting, and Lab. study interaction, as shown in Figs. 5–7, respectively.These three state machine diagrams are left-to-right state transitions and contain the start, final, and internal states. The occurrence of a certain acoustic event triggers the corresponding state transition, in which the state machine stays in the original state or moves to the next state.As shown in Fig. 5, the behavior Lab. meeting is recognized when the final state is reached. Nonvocal Event 3, which represents the opening of the laboratory door, is vital for the occurrence of this behavior. When nonvocal Event 3 is not successfully recognized by the acoustic event classifier, the state of “Entering lab.” cannot be reached; therefore, actions “Printer printing,” “Whiteboard writing,” and “On the phone” defined in the behavior mode cannot occur. Such recognition misses affect the performance of the state machine diagram, resulting in poor recognition accuracy of Lab. meeting. Enhancements to the recognition of nonvocal Event 3 by using the proposed voting-GMM with a hierarchical classification tree have been introduced in the previous section. Similarly, criticality of the accurate recognition of nonvocal Event 4 in addition to that of nonvocal Event 3 can be observed in the state diagram of the behavior Classmate chatting (Fig. 6). The proposed voting-GMM with a hierarchical classification tree for recognition of nonvocal Event 4 improves the behavior recognition performance. Compared with the state machine diagrams of Lab. meeting and Classmate chatting, that designed for Lab. study interaction (Fig. 7) is simple and contains fewer active states.

@&#CONCLUSIONS@&#
Image- or video-based categorization of methods is generally employed in human behavior recognition works, and it is rarely seen to perform behavior recognition using only recorded acoustic data without captured video or image data. In fact, audio-based human behavior recognition apparently has a competitive advantage in the issue of personal privacy. In this study, a three-layered hierarchical scheme with a microphone array of the Microsoft Kinect sensor is developed for audio-based human behavior recognition. The proposed hierarchical scheme involves three operation layers, namely the feature layer (input layer), acoustic event recognition layer (middle layer), and human behavior classification layer (output layer). In the feature layer, a well-arranged microphone array embedded in the Kinect sensor is employed to receive acoustic data and perform sensing data fusion. The main purpose of the acoustic event recognition layer in the presented three-layered hierarchical scheme is to recognize specific acoustic events by the GMM classifier. For performing categorization of acoustic events and further increasing the accuracy of acoustic event recognition, hierarchical GMM with an event classification tree is therefore developed. In the human behavior classification layer, this study uses state machine diagrams to model office human behaviors. Three state machine diagrams with left-to-right state transitions are developed for Lab. meeting, Classmate chatting, and Lab. study interaction. The experimental results proved the superiority and high feasibility of the proposed scheme in human behavior recognition. The proposed voting-GMM with a hierarchical tree exhibits the optimal performance in recognizing 17 office acoustic events. The designed state machine diagram with the state transition driven by fusion-based voting-GMM with a hierarchical tree exhibits the highest office behavior classification accuracy. Future studies can consider additional Kinect audio and video data for increasing recognition precision. In addition, the feasibility of applying the developed human behavior recognition scheme in smart homes can be evaluated.