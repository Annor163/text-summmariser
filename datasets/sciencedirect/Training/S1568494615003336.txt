@&#MAIN-TITLE@&#
Evolving accuracy: A genetic algorithm to improve election night forecasts

@&#HIGHLIGHTS@&#
Election night forecasting depends on finding homogenous groups of polling stations.The industry standard relies on human intuition and K-means clustering.We propose a genetic algorithm to find grouping solutions.Our method outperforms human groupings by far.The concepts are demonstrated with local Austrian election data from 2010.

@&#KEYPHRASES@&#
Election night forecasting,Genetic algorithm,Ecological regression,Constituency clustering,

@&#ABSTRACT@&#
In this paper, we apply genetic algorithms to the field of electoral studies. Forecasting election results is one of the most exciting and demanding tasks in the area of market research, especially due to the fact that decisions have to be made within seconds on live television. We show that the proposed method outperforms currently applied approaches and thereby provides an argument to tighten the intersection between computer science and social science, especially political science, further. We scrutinize the performance of our algorithm's runtime behavior to evaluate its applicability in the field. Numerical results with real data from a local election in the Austrian province of Styria from 2010 substantiate the applicability of the proposed approach.

@&#INTRODUCTION@&#
When the last ballots have been cast and the last polling station closes, the fruits of a stressful afternoon are brought to bear: the first election forecast is being broadcast over the air. Much of the work behind it, however, actually took place long before that, starting weeks before the election and culminating shortly after noon.Forecasting elections is arguably the most demanding and stressful but also the most exciting task market researchers can perform [6]. The term election forecast can mean different things. Karandikar et al. [14] and Morton [21] give a summary of different meanings, and when we speak of election forecasting in this paper, we mean exclusively what they termed a results-based forecast, a forecast based on partially counted votes without any exterior information like polls or surveys. Contrast this e.g. with an approach that uses external, i.e. historical, data in [4]. In the traditional forecasting process, after weeks of preparation, decisions have to be made in split seconds, possibly on live television. The preparation in the weeks before the election is a tedious process that involves many person hours and is error prone. This paper improves the current situation of the industry by contributing solutions based on genetic algorithms11Preliminary results of this research project were presented at ACM GECCO conference 2011 [10].to the most expensive and fragile elements of the field.The remainder of this paper is structured as follows. First we will give an introduction to the methodology that constitutes the foundation of industry standard election forecasting as it is practiced today. Then the elements of this forecasting process that are especially expensive and fault intolerant are identified. In Section 2 we describe how genetic algorithms can be used to find near optimal solutions to the problems identified above. The devised algorithm is described in detail and evaluated using a standard set of indicators and real data from the field. Results of this analysis are presented in Section 3. Finally, we offer some concluding remarks and suggestions for further developments.Forecasting elections is a business that depends on meticulous preparations and accurate knowledge of the political processes behind the scenes. In the beginning of televised live election night forecasting, sometimes disastrous miscalculations paved the way for numerous endeavors, that were undertaken to improve the status quo [22,20]. Today, election forecasting using a methodology termed ecological regression [8,3,12,15,16,9], engages in the daunting task of comparing polling stations or constituencies of a geographical entity from a past election to the present one that is meant to be forecast. To make things worse, past election results do not easily translate into new election results because of old people dying and young ones becoming eligible to vote. Assuming, admittedly somewhat naively, that new voters behave in general similar to old voters, this transition becomes merely an exercise in multiplying old vote shares with the number of new voters. Any deviance in voting behavior will be accounted for in the regression model introduced later.This only works because (1) not all polling stations provide their results at the same time and (2) voters that go to one polling station will behave similar to voters at another one.Other frameworks are used to forecast elections well before they take place, usually with the aim of only predicting the winner, and not producing precise estimates for vote shares [28,7,27,25].In a multi party system for any given election there are multiple parties competing against each other for votes. Voters can cast these votes at polling stations which are usually located close to their homes. It is also clear, that at least for developed democracies, parties have a history of performances in past elections. Any election forecast uses (at least) two elections, one in the past and the current one with the overall aim of predicting the vote shares of the current one. Since voters have formed an opinion and elect a party accordingly, not all parties will end up with the same share of votes, when comparing two elections. For two competing theories of how this might happen, see [17,18].Note that there are two different kinds of vote shares that can be used as performance metrics for parties: either the proportion of the total electorate voting for a party or the proportion of the constituency that actually did cast a valid vote, that voted for a party. In the following, these quantities are called %Elec and %Vald, respectively. Most clients will be interested in the latter one, as it constitutes the post-election political reality.In the regression-based model the performance p of a party i at a current election is a linear combination of all j parties’ performances at the reference election plus the proportion of nonvoters (NV), for all k polling stations. To simplify things, the nonvoters are considered to be just another ordinary party and are thus included in the j parties. So the following equation has to be estimated for all j parties to link the old election results from polling station k to the new election results at that polling station:(1)pi,k=∑jxj,kpj,k+pNV,kThe factor xj,kin the equation above is the quantity of interest in the election forecasting process. This quantity can be considered as a transition multiplier. For instance a value of xj,k=0.6 for two parties i, j means that in the current election party i could mobilize 60 percent of the last time voters of party j for its own cause at polling station k. If i=j, xi,kboils down to the proportion of traditional i voters the party could again re-win at the current election (at this polling station); for all i≠j, the different xj,ksum up to the votes that were won by party i from competing parties. All x*kof a polling station k together make up a matrix with as many rows as parties in the current election and columns as parties in the old election. This matrix projects the old election's vote shares into the space of the new election. In the most trivial example, the same, let us say 4 parties compete in both elections. This means that the equation from above needs to be estimated four times for each polling station, leading to a 4×4 projection matrix for each polling station.Obviously, a projection matrix can only be established for polling stations that already reported their results. The polling stations that did not yet report their results are then to be forecast. As stated earlier, it is assumed that any trend visible from the already declared polling stations will also apply to the polling stations not yet counted. So the idea is now to use the already obtained projection matrices on the old election results from those polling stations still missing. When, and this is quite quickly happening during an election day, more than one polling station have their results reported multiple projection matrices will be available. Then a cell-based average function over the available projection matrices is used to obtain an overall matrix.Unfortunately, not all polling stations will follow the general trend, or will follow it only to some extend. Therefore, care must be taken in choosing the projection matrices that are used as input in computing the overall matrix.As stated above, this method relies on the assumption that voters will behave similarly. However, consider that on the math side of things, as the used regression models are unbounded,22We use here the term unbounded to address linear regression's inherent tendency to fit straight lines (as opposed to the familiar sigmoid curves of logistic regression).this method is a linear approximation of the choices the electorate makes. Also, since regression can be considered as computing an average over a number of data points, xj,kcan take extreme values if there are heterogeneous trends between polling stations. This poses a problem to the election forecasting model as percentages below 0 and above 100 cannot be accounted for by voter mobilization.The solution to this problem lies in grouping polling stations together that will exhibit a similar trend in the transition from the reference election to the current election. By doing so, the coefficients of the model, remain within the 0, 100 interval and thus interpretable. In other words, the linear approximation works if and only if the polling stations in each group are homogeneous enough. So the mean of the individual projection matrices are computed only for a subset of the available matrices.So to summarize, in the election forecasting process, the relationships between old and new party results are used to project the results for yet missing polling stations. By means of multiple regression models, the transition multipliers are estimated per polling station. The transition multipliers of similar polling stations are then combined into averages. These averages are then used to compute the votes the parties are likely to obtain in the missing locations.Traditionally the grouping, the identifying of polling stations that will exhibit similar trends, is done by experienced senior researchers using K-means clustering (see [19]) and constant size binning techniques. This process is usually very time consuming (and thus expensive), as there are no fixed rules and many different possibilities have to be evaluated by hand. Additionally, there is no guarantee that the groupings found in such a way will actually be homogeneous. Given the small number of possible combinations that can be tried in manual assessment, they are even quite unlikely to be related at all. If the resulting groups of polling stations, however, are homogeneous enough, stable forecasts will be available at a very early state of the vote counting process.To summarize, for any election forecasting endeavor using the aforementioned method, the grouping of polling stations into homogeneous clusters is crucial. The search for a perfect grouping is a tedious and time consuming process especially given the huge number of possible combinations.In this section we will describe the genetic optimization procedure we used to improve the quality of the grouping solutions and thus the quality of election forecasts. We will first cast a closer look at the groupings of polling stations and ways on assessing the quality of an election forecast. Then we will present the pseudo code of the genetic algorithm used for the optimization process.When grouping polling stations together into homogeneous groups, some issues have to be considered:•Groups should exceed a minimum size.Each group needs to contain polling stations that will be declared early in the race.Overly large or small groups should be avoided.Being able to attribute external meaning to the grouping aids in interpretation.The method used in election forecasting is based on regression. As regression becomes computationally unstable when too few data points are available, small groups are a hazard to the computation. It is also important to consider that only a fraction of the polling stations in each group will actually be available to estimate the regression coefficients (i.e., the transition multipliers) early on election night. Conversely, overly large groups are as well problematic, as large groups are often typical of cluster algorithms which thereby fail to detect structure in the data. In this case, one large, average, typical group is found, with only a few outliers appertaining to the other groups. Finally, it can be helpful for an interpretation of the election results to be able to describe the groups and thus characterize the environments of the voters causing a certain trend.For the optimization problem at hand we used publicly available data33Election results of Austria are available from the website of the Austrian Federal Ministry of the Interior at http://www.bmi.gv.at/cms/bmi_wahlen/.on a local election in the Austrian province of Styria from 2010. We used the Styrian part of Austria's general election of 2008 to predict the outcome. The data available is polling station data aggregated to the constituency level, resulting in 542 data points. There were seven parties competing, eight including nonvoters.The selected election data is typical for the industry in that it is rather difficult to predict. Styria consists of mostly rural communities in secluded alpine valleys with a few larger cities in between. The largest share of the votes, however, originates in the provincial capital of Graz. As rural and urban areas have very little in common regarding voting behavior, the prediction becomes tricky.To assure a sensible grouping, the data points were split into ten groups initially. This would yield approximately 54 constituencies per group, giving ample data points for an early estimation effort. In a simulated election forecast it was pretended that a part of the constituencies had not yet been declared. This part amounted to 90 percent of the entire electorate, roughly spread out over 36.9 percent of the constituencies.The quality of an election forecast was established by considering the root mean squared error (RMSE) of the forecast with respect to the actually observed election outcome. The exact quantity that was used to measure the deviation was varied. Details will be given in Section 3.RMSE was used in spite of Armstrong [1] arguing against it. His main critique is the poor performance of RMSE as an indicator in forecasting long-run time series data and its sensitivity to outliers. While this is well founded, it does not apply to the election forecasting problem. Here, the shortest time series possible is used. Furthermore sensitivity to outliers is an asset, since clients and the television audience will be sensitive to them as well.To optimize groupings in an election forecast, genetic algorithms can be used. The idea of any genetic algorithm is to start with a number of random solutions. The best of these random solutions are then combined together and combined with fresh randomness, so to speak. In a next step, these children solutions are recombined once more. These steps are repeated for a very large number of times until they converge toward a stable and near optimal solution. The mechanics of genetic algorithms are closely modeled after evolution on a genetic level as observed throughout nature. Genetic algorithms are generally considered to provide excellent results on a wide range of optimization problems, see [23,13] for applications to clustering.At the core of a genetic algorithm lie chromosomes, as in natural genetics as well. Each of these chromosomes reflects a particular solution to the optimization problem at hand. Each of these chromosomes, or solutions, is being evaluated at the hands of a pre-defined target function, for instance the deviance between predicted and observed vote shares. In this example, a chromosome is the total grouping structure for all constituencies. Chromosomes are made up of genes, which represent the group membership of individual constituencies. The closer a candidate solution, a chromosome, gets to reality as observed, the better it performs. By recombining the genes of two chromosomes a child is produced, and an improvement with respect to the target function is aspired.Often enough, this simple recombination of parent DNA leads into a dead end. A dead end, in terms of operations research is a local optimum of a function. Think of an optimization problem as the search for the highest peak in an unknown mountainous region. One way to find this peak, is to climb up until there are no rocks left to climb, on every side there are only descents to find. We thus have found a peak. However, perhaps it is not the highest peak, perhaps we need to descend into a valley to climb a yet higher peak. If that is the case, conventional peak search is at an end, since all options correspond only to descents. We thus need to climb down in hopes of finding a higher peak some other place. In terms of genetic algorithms, fresh genetic information is introduced by means of mutation and re-seeding, the introduction of totally random chromosomes into the population to allow for a fresh start that might lead to an even higher peak.The algorithm we found most suitable for the problem at hand was initialized with a set of random solutions and progressed using a number of genetic operators. The algorithm was implemented using R [24]; all plots were produced with ggplot2 [29].The optimization problem can be broken down to a clustering problem with additional constraints described above. We solve this optimization problem by adapting a standard genetic algorithm, e.g. as surveyed by Blum and Roli [2], and summarized in Table 2. The chromosomes of the algorithm are different grouping solutions, with each constituency being represented by one gene per solution. A gene expresses a constituency's group membership.This adapted algorithm uses three genetic operators: random re-seeding of populations, one and two point crossovers and mutation. Additionally, the algorithm adheres to the principles of elitism and elite mixture breeding. The random re-seeding of the population serves to keep the gene pool fresh with alternatives to stay out of local optima. This is done by forcing a part of a new generation's chromosomes to be totally random.The crossover operator combines two parent chromosomes into one child chromosome, cutting randomly at one or two points. Because of elite mixture breeding, at least one parent comes from the top performing solutions, while the other parent is selected from a larger pool. Mutation is implemented by randomly changing genes in a chromosome. Each gene has the same probability of being mutated. Each new generation consists of a share of the top performing chromosomes of the old generations, a proportion of entirely random chromosomes and the remainder being offspring produced as described above. Table 1gives the proportions and probabilities for the parameters and operators, respectively. These have been established experimentally, with the constraints denoted above in mind.At 500 generations, the algorithm produced a near optimal solution (see Section 3.3). The characteristics of this solution with respect to the competing parties’ vote share in the groups are pictured in Fig. 1. The optimized solution describes groups that are most homogeneous in their voting behavior; hence for instance Group B has above average votes for the Conservatives (VP) and below average votes for left wing parties (SP, GR). Group A is even more sharply discriminated: here only the Social Democrats (SP) achieve large above average results. Not all groups, however, concentrate on partisan logic. Group C, for instance, exhibits an above average vote share for the Greens (GR). At the same time, also the right wing party FP is overly popular in this group. This is an indication that there is a similar trend in those constituencies that prefer small parties over large ones, protesting the political establishment. Yet other groups, like H, do not seem to follow any pattern that can be captured by examining party vote share in this group.

@&#CONCLUSIONS@&#
