@&#MAIN-TITLE@&#
Document image binarization using local features and Gaussian mixture modeling

@&#HIGHLIGHTS@&#
Background removal technique based on adaptive median filtering and thresholdingA Local Co-occurrence Map with local contrast can distinguish between document text and document stains and background.Low complexity approach with fast and accurate binarization results

@&#KEYPHRASES@&#
Binarization,Handwritten documents,Historic documents,Classification,Background estimation,

@&#ABSTRACT@&#
In this paper, we address the document image binarization problem with a three-stage procedure. First, possible stains and general document background information are removed from the image through a background removal stage. The remaining misclassified background and character pixels are then separated using a Local Co-occurrence Mapping, local contrast and a two-state Gaussian Mixture Model. Finally, some isolated misclassified components are removed by a morphology operator. The proposed scheme offers robust and fast performance, especially for both handwritten and printed documents, which compares favorably with other binarization methods.

@&#INTRODUCTION@&#
Document images commonly arise from historical documents, books or printed documents that are digitized using a scanning device. The advancement of imaging devices, such as scanners and digital cameras, has widely facilitated the digitization of paper-printed material, including historical documents and books. Many libraries throughout the world, such as the British Library in London, UK,11http://www.bl.uk/aboutus/stratpolprog/digi/digitisation/.have digitized books, manuscripts and other printed material from their collection, which are available online as images. We can extract the text information from these document images using Optical Character Recognition (OCR) techniques. Nevertheless, to enhance the performance of OCR algorithms, a number of preprocessing steps are systematically applied, including page skew detection, artifact and noise removal, document page layout analysis and document image binarization [1–4]. In this paper, we address the problem of background removal and document image binarization.Scanned documents often contain undesired textual noise, such as specks, dots, black borders, lines, and hole-punch marks. Background estimation and removal is a preparatory step that enhances the quality of the document images and is beneficial for binarization techniques [5–8]. For example, historic document images often suffer from different types of degradation that render document image binarization and character recognition very challenging tasks. In summary, the main objective of background removal techniques is to remove all these degradations from a document image and enhance the discrimination of characters from the page background.After the original document images have been enhanced, the output of most document processing systems is a bi-level image containing characters and background. Image binarization can then be performed either on a global or a local basis. Conventional binarization techniques of gray-scale documents were initially based on global thresholding algorithms (clustering approaches) [9], which have proved to be efficient in converting simple gray-scale images into a binary form but are inappropriate for complex documents, and degraded documents. For this purpose, the local binarization techniques of Niblack [10], Sauvola [11] and Bernsen [12] have been extensively used by the document image processing community. There are numerous specialized binarization techniques for document images (see [13] for a more detailed review). Here, we will outline several important binarization methods that have appeared so far.In [1], Papamarkos proposed a neuro-fuzzy technique for binarization and gray-level (or color) reduction of mixed-type documents. Badekas and Papamarkos [13] proposed a binarization technique that combines the results of multiple binarization algorithms using a Kohonen Self-Organizing Map (KSOM) neural network. In [14], the binarization results of many independent techniques were initially produced and then combined with a Kohonen Self-Organizing Map (KSOM). Badekas et al. [15] also introduced a binarization technique, specialized for color documents, where the resulting “binary” image contains the detected text regions with black characters in white background leaving the remaining original color parts of the document intact. In [16], Makridis and Papamarkos introduced a two-stage approach to image binarization. The first stage included a background removal technique that was based on fixed-size median filtering of the document image. Once the background was removed, the second stage aimed at creating 2D clusters of neighboring pixels of similar intensity, i.e., document characters and background. Binarization was then performed by identifying 2 clusters (text-background) using the multithresholding technique of Reddi et al. [17].Gatos et al. [18] (GPP method) estimated the document background by an adaptive threshold which labels each pixel as either text or background. To estimate the background surface, they used Sauvola's binarization algorithm to roughly extract the text pixels and calculated the background surface from them by interpolation of neighboring background pixels intensities. For the other pixels, background surface is set to the gray level of the original image. Ntirogiannis et al. [19] proposed a modular system for handwritten document binarization. Background is initially estimated via an inpainting procedure starting from the Niblack binarization output. The background estimate is then normalized to smooth great variations and is used as an input to Otsu's global thresholding which removes most unwanted noise but also some faint characters. Therefore, the local binarization algorithm of Niblack is also used, but initialised using the stroke width information, extracted by skeletonization of Otsu's output, window size and contrast information. The two binarization outputs are combined at connected component level.In [20], Su et al. demonstrated the use of local contrast image thresholding in estimating the text stroke width more accurately. In [6], Lu et al. performed background estimation using a modified version of 1D iterative polynomial smoothing to compensate for several degradation types. Text-stroke edges are then identified via Otsu's global thresholding on L1-norm horizontal and vertical edge detection. Document text pixels are extracted, since they are surrounded by text stoke edges and feature lower intensity levels.Hedjam et al. [7] used grid-based modeling and impainting techniques to recover text pixels starting from an under-binarization result using Sauvola's technique. The proposed technique featured smooth and continuous strokes, due to its spatially adaptive estimation of the text pixels' statistical features. Moghaddam and Cheriet [8] presented an adaptive form of Otsu's thresholding for binarization. Based on a rough binarization result, they produce an estimated background and a stroke gray level map using a multi-scale framework. This estimated background is further refined using the AdOtsu method, which is an adaptive, parameterless form of Otsu's thresholding, which is generalized to a multiscale setup. Finally, skeleton-based post-processing is employed to remove possible artifacts and sub-strokes.Valizadeh and Kabir [21] devised a novel feature space consisting of the structural contrast and the intensity value of each pixel. Structural contrast relates text stroke width, pixels' intensities and their relationships with their neighbors at stroke width distances. This results in a 2D image representation where text and background pixels are separable. Clustering is performed by partitioning the feature space into small regions. Then, using the result of another binarization algorithm with at least 50% successful labeling (Niblack), each region is classified either as background or text, according to the prevailing number of text or background pixels in the region. The reverse procedure produces the document binary image.Howe [2] performed binarization by minimizing a global energy functional inspired by Markov Random Fields, where a) the image Laplacian edge map is employed to distinguish between ink and background in the energy data fidelity term and b) ink discontinuities are enforced in the binarization result by incorporating a Canny edge detector into the smoothness term. Howe also introduced a procedure to automate the optimal parameter selection for his algorithm.Ramirez-Ortegon et al. [22] introduced the concept of transition pixel, i.e., calculating intensity differences over a small neighborhood, which can then be employed by common gray-level thresholding algorithms to produce a binarization result (transition method). This was further refined in [23], where an unsupervised thresholding was proposed for unimodal histograms, assuming Gaussian priors for the distribution of character and background neighborhoods. In [4], the method was enriched with a mechanism to remove binary artifacts after binarization. An auxiliary image is calculated via minimum-error-rate thresholding. The connected components of the auxiliary and the original binary image are compared in terms of an intersection ratio to remove possible binarization artifacts. In [24], Ramirez-Ortegon et al. explored possible effects of inaccurate estimations of the transition proportion on the estimated thresholds. In [25], Ramirez-Ortegon et al. proposed the use of skewed log-normal, instead of symmetrical Gaussian, priors [23] for the background and character clusters.Lelore and Bouchara [3] introduced the FAIR binarization algorithm, where they ran the S-FAIR (simplified) algorithm for two threshold values: one giving a noiseless binarization output but with important edges missing and another containing all character edges but with some misclassification noise. The S-FAIR algorithm first performs text localization using the Canny algorithm. A Gaussian Mixture Model is then used to classify pixels around edges to belong either to the text or the background image or to a third class where pixels cannot be attributed with certainty to text or background. The FAIR algorithm merges the two outputs with a “max” rule. Finally, a post-filtering process classifies unknown pixels using a variety of rules. The most important feature is an iterative procedure where the text labeled regions grow into the unknown using morphological dilation and the previous EM algorithm is used to define the final class of these regions. Final unknown areas are connected morphologically and labeled according to neighboring pixels.In this paper, the authors extend the previous work of Makridis and Papamarkos [16] toward a more automated three-stage document image binarization system. In the first stage, the background removal technique in [16] is enhanced by automating the window size selection for the median filter and improving the threshold selection between the document image and the background estimate. In the second stage, the proposed local neighborhood representation is redesigned to also include local contrast information to enhance the presence of character outlines. Binarization is then performed by separating two clusters of document characters and background artifacts that were not removed in the first stage of background removal. Clustering is performed using Mixtures of Gaussians (MoG). The Gaussian with lowest value mean corresponds to the character cluster. The local neighborhood representation share a similar concept with those introduced by Valizadeh and Kabir [21] and Ramirez-Ortegon et al. [22], however, the proposed multidimensional representation is different to the 1D representations discussed in [21,22]. Contrast information for binarization was also used by Su et al. [20], however, in this work contrast is incorporated into a local intensity representation forming a joint, rather than an isolated feature. Similarly, Gaussian modeling for binarization has been employed before by Hedjam et al. [7] and Ramirez-Ortegon et al. [23], but here it is applied on the novel LCM representation. Moreover, MoG-based clustering is a common clustering technique in pattern recognition, thus it is the application that is novel here. In the final post-processing stage, small-size 8-connected clusters are removed to eliminate possible binarization noise.The paper is organized as follows: Section 2 sets the essential notation and outlines the system. Section 3 describes the background removal process in detail; Section 4 describes the binarization stage using GMM clustering; Section 5 explains the post-processing step; Section 6 presents the evaluation results of the proposed methodology and finally Section 7 concludes this paper.Let I(x,y) be the initial color document image of size 3×M×N, where x,y denote integer samples across the horizontal and vertical axes. The desired output of a document image binarization algorithm is a bi-level M×N image IBN(x,y) that attributes the value 255 (white) to background pixels and the value 0 (black) to character pixels. It consists of three stages: a) the Background Removal stage, b) the Image Binarization stage and c) the post-processing stage, which are then presented in detail.Background removal is a preprocessing stage in a document binarization system that can eliminate the presence of artifacts, including stains, paper cuts, paper coloring and opposite-page ink leaks, prior to binarization.The first step is to map the three-channel RGB image to an one-channel intensity image that detains all the useful information from all color channels. One method is to simply average all three channels to create the intensity image, which has been shown not to be effective in our experiments. Another method is to move to another color space, such as the Hue–Saturation–Luminance (HSL) cylindrical color space, where the color information (HS channels) is isolated from the Luminance (L) channel, which is kept for further processing (as implemented by MATLAB's rgb2gray function). Several techniques have also been proposed that attempt to produce gray-scale images with visual contrast similar to the color contrast [26,27]. In [28], a linear transform is proposed that converts a color image to a gray-scale image in such a way that the variance of the transformation is maximized and at the same time, the gray-scale image preserves the brightness of the color image. Also, Kanan and Cottrell [29] proposed new techniques for general color to gray conversion. Recently, Moghaddam and Cheriet [30] developed a new heuristic technique that is based on a dual transformation, color reduction and interpolation. In order to ensure that all useful information from all color channels is conveyed to the grayscale image, we perform Principal Component Analysis [31] on the multichannel image. The principal component image is then retained as the grayscale image. This methodology for grayscale conversion is pursued in our system. In Fig. 1, we can see an example of a color document image conversion to grayscale using PCA. The final grayscale image appears to have increased contrast compared to a typical grayscale conversion.The proposed background removal algorithm is based on the observation that the aforementioned artifacts can be isolated from the original image by performing low-pass filtering of long window size [16]. This long-window low-pass filtering can essentially filter out the document characters, as they are generally small-size high-pass details, leaving only an image containing artifacts and the document background that needs to be removed. Median filtering is more preferable to ordinary low-pass filtering, since this will not create new intensity values in the document image, but will simply replace the character intensity values with background or artifact intensity values. Nonetheless, the size of the median filter window needs to be defined. In [16], Makridis and Papamarkos used a fixed window size, which was defined by the user. In this study, we propose to automate the procedure, by starting with a small median filter window of size G=5. After median filtering the input image I(x,y), we measure the standard deviation of every possible 3×3 image patch. If the standard deviation of the majority of image patches (e.g., 98%) is greater than a threshold value SI(e.g., SI=6), this implies that image still contains character information and the median filter window has to increase by 5, i.e., G←G+5. This procedure is repeated until most 3×3 patches have low standard deviation, i.e., low-order texture, background. The final image IMED(x,y) is an estimate of the document background. The above values of 98% and SI=6 have been determined by experimentation on the DIBCO [32–36] image datasets and remain unchanged. A more detailed study to determine the statistical properties of a background image is presented by Ramirez-Ortegon et al. [25], where similar values for SIare reported. A more extensive investigation of this parameter goes beyond the scope of this paper, since it does not appear to greatly affect performance.To remove the document background from the document image and form the “No-Background” INBG(x,y) image, a simple comparison classifies every pixel (x,y) as background or text. If the absolute difference between the original image intensity I(x,y) and the IMED(x,y) is below a selected threshold value T, then this pixel must be part of the document background and is attributed the value white, i.e., INBG(x,y)=255. In the opposite case, this pixel is very different from the background image and thus must be a character pixel. Therefore, we set INBG(x,y)=I(x,y).In Fig. 3, we depict the various stages of the background removal algorithm. An original document image of size 682×690 is depicted in Fig. 3(a). The background image estimate for G=20 is shown in Fig. 3(b) and the final estimate for G=30 in Fig. 3(c). The document image with the estimated background removed for various values of T are shown in Fig. 3(d), (e). Selecting larger values of T, more parts of the document image are classified as background and thus are removed (transformed to 255) from the image. Hence, the value of T can define the background removal strength of the algorithm. However, selecting larger values for T may remove character information apart from unwanted noise.One can make the selection of T more adaptive, by calculating a histogram of |I(x,y)−Imed(x,y)|. Dividing histogram values by the number of image pixels, we get an approximate probability density estimate piof the previous difference. In most document images we encountered in this study, this density seems to be a decreasing function (see Fig. 2). Thus, an adaptive threshold value of T can be set at the point, where this probability falls below a fraction q of this maximum value, i.e., qmax(pi) with q∈[0,1]. This provides a more general threshold which is more adaptive for different images than selecting a specific value for T. Lowering q removes more background information, while increasing q leaves more background information unprocessed. In our system, we tend to keep the background removal stage less strict, so as not to accidentally remove character parts or outlines in the background removal stage. In Fig. 3(f), the background removal result is depicted using a value of q=0.5. Although thresholding is now more adaptive to a variety of document images, the parameter q has to be manually selected. The specification of this parameter remains key to the performance of the binarization stage, as it will be explained in the experimental section.This image is then presented to the image binarization algorithm, described in the next section. The proposed algorithm is summarized below:Document image background removal algorithm1.Transform the initial M×N color document image I(x,y) to an 1-channel image I(x,y), using only the Principal Component.Set a neighborhood size G×G, where G=5.Estimate(1)IMEDxy=medianGIxyCalculate the standard deviation σ(x,y) of every 3×3 patch in IMED(x,y).If the number of patches that satisfy the condition σ(x,y)<SIis less than 0.98MN then set G←G+5 and repeat steps 3, 4, 5.Estimate a value for T, where the normalized histogram piof |I(x,y)−Imed(x,y)| falls below qmax(pi).The document image without background INBG(x,y) is given by:(2)INBGxy=Ixy,if|Ixy−Imedxy|>T255,if|Ixy−Imedxy|≤TDocument Image Binarization is defined as the process where a grayscale document image I(x,y) is transformed into a bi-level image IBN(x,y), where IBN(x,y)=0 for each pixel (x,y) that is attributed to a document character and IBN(x,y)=255 for each pixel (x,y) that is attributed to background. Local thresholding methods seem to offer more stable solutions, exploiting local statistical measurements, including the local mean, standard deviation, entropy and contrast.Some other local character properties that can be exploited to perform binarization are the following:•Pixels belonging to the same character are geometrically close.Pixels belonging to the same character should feature similar intensity values.Any local area (neighborhood) that includes the outline of a character should have increased contrast, compared to areas containing only background or only character pixels.In Fig. 4, we show some examples of the above principles in a document image. These principles were also discussed in a more mathematical manner in [22].In this section, we will use these properties to create a Local Co-occurrence Mapping (LCM) that will assist us in discriminating between the character and the background pixels. The first two properties were initially discussed in [16], leading to the introduction of a Symmetrical Frequency Map (SFM) that was used to perform binarization. Here, we extend this framework to use these three properties simultaneously and increase binarization performance.To emphasize proximity and connectivity between neighboring character pixels, the main concept is to devise a co-occurrence map in the following manner. The image is divided into every distinct Q×Q patch. This implies that these patches are created with 1-pixel overlap from the original image. Let (xci,yci) be the center pixel of the i-th patch. Each distinct patch is then transformed to the following (Q2−1) points in the 2D space given by:(3)INBGxciyciINBGxci+dx,yci+dy,∀−Q/2≤dx,dy≤Q/2In other words, each pixel in the i-th patch is transformed to a 2D point containing the intensity of the central patch pixel and the pixel's intensity. The whole procedure is visualized in Fig. 5. We note that the combination of the center pixel with itself is not included in the formation of this group of 2D points, since it does not offer any information about connectivity. Thus, each patch produces a set of (Q2−1) 2D points denoted by IW(ti), where ti=1,…,(M−Q+1)(N−Q+1) is an index that runs through all possible image patches. Repeating the procedure for all possible Q×Q patches of the image yields the Local Co-occurrence Mapping (LCM), i.e., the new image representation IW(k), where k represents the 2D-point index. The new image representation is of size 2×(M−Q+1)(N−Q+1)(Q2−1). Calculating the 2D histogram of the 2D points IW(k), we acquire the Symmetrical Frequency Map (SFM), as proposed by Makridis and Papamarkos [16]. In Fig. 6(a), a typical SFM histogram is depicted.One can observe the basic properties of this histogram. First of all, the SFM plot is symmetric over the main diagonal, because in two overlapping patches for i.e., Q=3, one can get the symmetric points [INBG(x,y)INBG(x+1,y+1)]T[INBG(x+1,y+1)INBG(x,y)]Tand are counted twice. The most important property is that there are two main concentrations of points: one where the center pixel takes higher intensity levels along with its neighboring pixels and one where the center pixels and its neighbors take lower intensity levels. The first point-cluster represents background pixels and the second point-cluster represents character pixels. The same trend appears in most printed or handwritten document images in our experiments using the DIBCO [32] image database. The only difference is there might be more visible clusters, due to paper stains or other artifacts of different intensity (see Fig. 7(a)). However, these small clusters can be re-grouped in two main clusters: one of lower intensity denoting characters and one of higher intensity denoting background. This can be achieved during the clustering phase and will be discussed in a later section.Observing the original SFM histograms in many document images, we made the following observations. Firstly, the character cluster is usually shorter and smaller compared to the background cluster, since characters constitute only a small part of the image, compared to background pixels. This will hinder the task of any clustering attempt to estimate the character cluster, since the background cluster dominates the SFM histogram. In addition, this mapping is usually following the background removal stage, which implies that many pixel values will be set to 255 by the background removal process. This will cause a huge concentration of points around the point (255,255), which will make the character cluster barely visible and thus really difficult to be identified by a clustering algorithm.The main proposal here is to remove all 2D-points whose central pixel value equals to 255 from IW(ti). These points have already been classified as background and therefore should not be part of the binarization process. After removing these points, the SFM histograms change considerably. The character clusters are more visible compared to the background cluster. In addition, the actual task that is required to solve here has also changed. After removing the pixels that have been classified as background, this image binarization step aims at discriminating between the character and the misclassified background or artifact pixels. In Figs. 6(b), 7(b), the improvement in the two previous SFM histograms is depicted. The new SFM histograms in either case contain two prominent clusters : the character and the artifacts cluster. The character cluster is now much stronger, compared to previous SFM histograms. After removing the background pixels, the proportion of character and artifact pixels is now comparable. This will improve clustering performance, since the character cluster is more clearly separable than previously.Another improvement in the LCM framework is to remove 2D points far from the main diagonal. Ideally, character pixels should have similar intensity values with the central pixel, allowing for some slight deviation. Thus, pixels far from the main diagonal should be attributed to local noise and should be removed. We measure the distance d of each 2D point from the main diagonal and if it exceeds a threshold then it is rejected. The choice of threshold d should be carefully selected, as we will see in the experimental section. A narrow choice of d results into character thinning. A rather large choice of d may undermine performance, since it incorporates noise. Optimal values for d will be discussed in the experimental section.One can also use different neighborhood patterns around each central pixel, such as cross or diamond neighborhoods. This produced inferior results in our experiments. Also, the proposed 2D point representation resembles the 2D point representation proposed by Valizadeh and Kabir [21], with the difference being that their points contain structural contrast and local intensity and they look at neighboring pixels at stroke-width distance.So far, we have incorporated the first two of the three previously mentioned local character properties in the LCM representation. The third property emphasizes the existence of strong contrast in the Q×Q neighborhood, which denotes the existence of character outlines. To include this information in the LCM, we will simply calculate local contrast for each Q×Q image patch and its value will be incorporated in the LCM representation as a third dimension. The contrast of each patch C(IW(ti)) is calculated by the following equation:(4)CIWti=maxIWti−minIWtimaxIWti+minIWtiThe above definition of contrast is known as the Michelson contrast [37] and is recommended for patterns, where the amount of bright and dark pixels in the examined area is almost equal. The use of contrast to estimate text stroke width was also discussed in [20], using a similar definition of contrast. We also experimented with other textural measures that can identify character outlines (strong edges), including standard deviation and entropy, but the use of contrast seemed to be more stable in our experiments. The value of constrast is greater for patches containing character outlines (the desired patches), whereas is smaller for background patches. To move the desired cluster toward small values, in a similar manner to the previous 2D LCM representation and in order to suppress its range values, we propose the following nonlinear mapping to the original C(⋅) values.(5)iCu=2551−tanh2CuThe nonlinear function tanh(⋅) serves as a method of increasing separation between the two clusters: character outlines and low-contrast patches. In Fig. 8, we depict the original contrast histogram of a document image and the proposed mapping iC(⋅), which features improved range and the character outlines cluster mapped to lower values. The new iC(⋅) values are used to form the novel 3D LCM representation, as follows:(6)IWk=INBGxciyciINBGxci+dx,yci+dyiCINBGxciyci,∀−Q/2≤dx,dy≤Q/2As one can observe, the local contrast information of each patch is added as another feature to the previous 2D LCM, creating a novel 3D LCM feature, aiming at enhancing character outline binarization.Once the LCM representation has been established, image binarization can be achieved by performing clustering on the data points IW(k). There exist numerous methods to perform clustering. In this work, we examine the application of Mixtures of Gaussians (MoG) modeling to address the clustering problem. Mixtures of Gaussians (MoG) is a weighted sum (mixture) of different multidimensional Gaussians that can be used to model any arbitrary probability density function (pdf) that does not follow a particular known distribution.(7)px=∑1KaiNmiΣiwhere x is a random vector that is observed from the data, aiare the mixing coefficients, miis the mean vector and Σiis the covariance matrix of the i-th multivariate GaussianNmiΣi. In the special case that the arbitrary data distribution features relatively disjoint clusters of data, MoG can be employed to perform clustering by fitting each individual Gaussian of the mixture to each data cluster. The essentials of general multidimensional MoG were established in [38,39], where the estimation of the MoG's parameters are performed using the Expectation–Maximization (EM) algorithm. The MoG estimation is sensitive to the initialisation of its parameters. To accelarate MoG training, one can initialize the EM using the result of a simple clustering algorithm, including the K-Means, the Fuzzy C-Means and the Harmonic K-Means algorithm.We will employ the EM algorithm, as described in [39], to perform clustering of the LCM data Iw(k). Our clustering problem is very constrained and these constraints should be used in the initialisation of the EM algorithm. First of all, we are looking at identifying 2 clusters (characters-artifacts), thus K=2. This implies that the mixing coefficients should be initialised by ai=0.5. The initialization of the means miis also very important. As previously observed, the desired clusters are usually centered on the main diagonal. In addition, the character cluster should be centered near the beginning of the main diagonal (dark intensities) whereas the artifacts cluster should be placed in the opposite part of the main diagonal (lighter intensities). Consequently, the character mean can be initialized as e.g., m1=[202020]T, whereas the artifacts mean can be initialised as e.g., m2=[230230230]T. Finally, to simplify calculations, we can assume that the Gaussians' covariance matrices are diagonal and use random initialization for their variances. In the previous section, we mentioned the case of discovering more than 2 concentrations of LCM points, due to significant paper stains, or different text color. In this case, we can initialise the EM using 3 or more Gaussians (as necessary) and equidistant initialisation on the main diagonal. After the convergence of the EM, we can merge the new middle clusters with either the text or the background cluster, depending on the distance between their means.Once the EM algorithm has converged, we have to use the LCM points that correspond to the character cluster (the cluster with the lowest mean vector) to form the binarised image IBN(x,y). The classification rule is straightforward: “if any LCM data point in each Q×Q neighborhood is classified to the character cluster, then the corresponding central pixel (xci,yci) is set to black, i.e., IBN(xci,yci)=0. The remaining pixels are set to white i.e., IBN(xi,yi)=255.”The proposed algorithm is summarized, as follows:Document Image Binarization Algorithm1.Use the proposed Background Removal algorithm to create the image INBG(x,y).For every Q×Q neighborhood in the image, create the 3D LCM representation Iw(k) using (6). Neighborhoods whose central pixel has been classified as background are not used in the LCM representation.Identify two clusters on the LCM representation using the MoG-EM algorithm and the initialization discussed earlier.Initialize the M×N matrix IBN(x,y)=255.If any pixel in each Q×Q neighborhood is classified to the character cluster, then the corresponding central pixel (xci,yci) is set to zero, i.e., IBN(xci,yci)=0.The final stage aims at removing artifacts from the previous binarization stage. Isolated blobs or small misclassified noisy items can be removed using a mathematical morphology step. We use MATLAB's bwlabel command to identify connected objects with 8-connectivity in the binary output of the 3D LCM algorithm. The command returns an annotated image containing all the different connected components with 8-connectivity that exist in the image. If some of these components are small in size, they should be noisy artifacts, as described earlier. Therefore, we remove all those connected components that contain less than 20pixels. Of course, this threshold relates to the image's resolution and has to be adapted accordingly. This choice seemed to work well for the DIBCO image databases that were our main experimental ground. In Fig. 9(f), we can see the result of post-processing on the previous 3D LCM binary image (d). Many of the previous binarization errors have been removed. Of course, there are several more complicated post-processing methods, one can use to improve the binarization output, such as those proposed in [6,4], however, we wanted to keep the computational complexity of our algorithm as low as possible.

@&#CONCLUSIONS@&#
In this paper, the authors propose a novel document image binarization system that can be applied on both machine-printed and handwritten document images. The system consists of three stages. During the background removal stage, an estimate of the background image is calculated via adaptive median filtering. The background is removed by statistical thresholding of the differences between the estimated background and the document image. In the next stage, Local Co-occurance map (LCM) is calculated as described earlier. This representation aims at grouping together pixels of similar intensity value and similar contrast, thus creating two dominant clusters: character and remaining background. Clustering is performed using a Mixture-of-Gaussian (MoG) model of two Gaussians. In the last stage, some isolated binary artifacts are removed by morphological 8-connected object segmentation.The proposed approach is robust to severe degradation of the document images. The inclusion of contrast seems to improve the inclusion of character outlines in the binarization results. The method performs quite well in our experiments and DIBCO benchmarks. Although, it is not the best performing method, it is a low-complexity good performing method that can be used in environments, where computational power use is important. Nonetheless, the method is very sensitive to the amount of background removal performed in the first stage, which is controlled by the parameter q. In this study, we have used a value of q=0.6 for the printed images and a value of q=0.4 for handwritten images, that seemed to work well in our experiments.The authors would also like to extend the method to work for multi-color documents. Although it is trivial to extend the number of clusters in the MoG model, it would be preferable if the system could automatically identify the number of colors and configure the number of clusters accordingly. In addition, the authors would like to look into a more automated method to define the value of q in the background removal stage and the cluster size in the post-processing stage. Another extension can be to change the Gaussian distribution assumption for the background and character cluster for skewed distributions, including the log-normal distribution, as observed by Ramirez-Ortegon et al. [24].