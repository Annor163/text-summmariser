@&#MAIN-TITLE@&#
Rate-independent detection of atrial fibrillation by statistical modeling of atrial activity

@&#HIGHLIGHTS@&#
We developed a new statistical model to detect atrial fibrillation from ECG.Unlike existing R–R interval-based algorithms, our method targets atrial activity and is heart rate independent.The algorithm can work even if the patient has a pacemaker or is taking rate-control drugs, or if other heart rate issues occur simultaneously with AF.The performance of the proposed method is demonstrated on real ECG data.Our assessment showed a comparable performance to the R–R interval-based algorithms.

@&#KEYPHRASES@&#
Atrial fibrillation,Atrial activity analysis,Feature extraction and classification,P-wave absence,Electrocardiogram,Gaussian mixture model,R–R interval variability,

@&#ABSTRACT@&#
In this study, we propose a P-wave absence (PWA) based method for atrial fibrillation (AF) identification over a short duration of electrocardiogram (ECG). The algorithm constructs a statistical model of normal sinus rhythm (SR) P-waves using a training set. Features extracted from P-waves are taken as an input to the Expectation–Maximization algorithm to create a Gaussian mixture model (GMM) of the P-wave feature space. The model is then used to identify PWA and detect AF. The algorithm performs AF identification in a single beat, and through post-processing of successive outputs using a majority voter determines the PWA over seven beats. The MIT-BIH Atrial Fibrillation Database was used to evaluate the algorithm. Classification using the majority voter showed a sensitivity of 98.09%, a specificity of 91.66%, a positive predictive value of 79.17% and an error of 6.88%. The performance of the proposed classifier is comparable to current R–R interval (RRI)-based algorithms, yet is able to detect short episodes of AF and performs rate-independent AF determination. The proposed algorithm targets atrial activity rather than ventricular activity that is targeted in RRI-based algorithms. It provides a patient specific detection of AF using a simple classifier, and can be leveraged as a tool to detect AF onsets/offsets over short AF episodes even when a patient's heart rate is controlled.

@&#INTRODUCTION@&#
It is the focus of this manuscript to present a novel heart rate independent algorithm to detect the presence of atrial fibrillation (AF) episodes in rate-controlled patients with paroxysmal AF. AF is the most common cardiac arrhythmia. It affects an estimated 2.3 million people in the United States, and this number is only expected to increase as the general population ages. It is estimated that by 2050, 16 million Americans will suffer from AF [1,2]. Symptoms of AF such as dizziness, shortness of breath, chest pain, or heart palpitations reduce quality of life. Automatic detection of AF could provide cardiologists with significant information for accurate and reliable diagnosis and monitoring of AF and is crucial for clinical therapy. Rate control methods such as pacemakers or rate-control drug therapy eliminate the irregular heart rate associated with AF, resulting in false diagnoses [3,4]. Therefore, there is a strong need for algorithms capable of quickly and reliably detecting AF, even in cases where the heart rate is controlled through medication or with a pacemaker. However, monitoring AF remains an open area of research when the heart rate is controlled.AF is the disorganized propagation of electrical activity in the atria that causes the atria to fail to contract in an organized fashion. As a result, the atrial depolarization wavefront, the P-wave, which is measured during sinus rhythm (SR) devolves into a series of fibrillatory waves (i.e., f-waves) in the surface electrocardiogram (ECG). Additionally, the fractionated electrical wavefront stimulates the atrioventricular (AV) node inconsistently, producing irregular ventricular contractions. Based on this observation, current methods to detect AF can be divided into two main approaches: (i) R–R interval (RRI) analyses detect the irregular ventricular heartbeat that is often associated with atrial arrhythmia, and (ii) a combined RRI and atrial activity (AA) approaches detect a lack of organized atrial activity in the ECG in addition to the heartbeat irregularity. The most dominant methods in the literature [5–14] are based on RRI analysis. However, current top-performing RRI-based algorithms process and make an AF determination using relatively long windows of data (50–100 beats), which can obscure the onset and offset of AF episodes. As a result, very short AF episodes may not be detected. Furthermore, RRI algorithms are rate-based and cannot work if the patient has a pacemaker or is taking rate-control drugs, or if other heart issues, such as atrioventricular (AV) block, occur simultaneously with AF. AF is one of the most common causes of stroke in the world [15]. So the implication is very important to know if a patient has AF or not even with a pacemaker.In recent years, several methods have been developed to detect the AF presence using both RRI and AA information [16–19]. Targeting an AA-based method allows for the rate-independent detection of very short episodes of AF because determinations can be made with only a few beats. The main challenge with a P-wave absence (PWA)-based technique is that the ECG is nonstationary, and the atrial activity has a relatively low signal-to-noise ratio compared to ventricular activity. As a result, algorithms which focus on AA exclusively [20] do not perform as well as RRI based algorithms. The existing approaches combine the PWA-based techniques with the RRI analysis to improve the performance for robust detection of very short episodes of AF; however, those methods remain dependent on the RRI information and cannot be used for rate-controlled AF patients. The present study presents a rate-independent AA-based PWA algorithm that achieves a comparable performance to RRI algorithms and is capable of making an AF determination over only a few beats. With the reduced determination time, shorter episodes can be detected, providing clinicians with a tool to automatically collect data and research the importance of short AF episode durations and short cessations of AF episodes for a wide range of patients including those who are under rate-control treatments.Our proposed approach identifies a segment of the signal preceding the QRS complex as a P-wave under consideration. An anomaly detector is trained on normal SR P-waves and identifies P-wave absence as a beat which does not contain a P-wave similar to ones seen in the training set. Hence, P-wave under consideration which varies dramatically from the learned P-wave morphology is classified as AF. As explained in Section 2, the algorithm is executed in two phases: the training phase and the testing phase. In both phases, preprocessing and feature extraction is performed on the supposed P-wave or P-wave under consideration. The training phase creates a statistical model that describes the feature space distribution of P-waves in the training set. The testing phase calculates a score that reflects the likelihood that a new beat contains a P-wave. In this way, P-wave absence (PWA) and, in turn, AF is detected in cases that the likelihood score is lower than a threshold. The proposed method is evaluated with a paroxysmal ECG database, and the results are reported in Section 3.The MIT-BIH AF Database from Physiobank [21] was used for validation of the proposed AF detection algorithms. The dataset includes 25 long-term (10h) ECG recordings with AF (23 paroxysmal and two persistent) and contains 299 AF episodes (about 93.4h). Out of the total 23 paroxysmal recordings, this study uses only the 20 recordings that contain sufficient SR data to construct a training set.A total of 10min of training data for each record is selected from a period of SR at least 35min long. Training intervals and the periods of wait time between the intervals are determined by considering what recordings are practical to acquire during a clinic visit. Training data is selected from two 5-min intervals spaced 10min apart. Additionally, to ensure training data is not immediately following or preceding AF, a minimum of 7.5min of SR were required to precede and succeed the two 5-min training sets.Fig. 1shows a block diagram of the proposed method. A third-order butterworth bandpass filter is applied with poles at 0.5Hz and 50Hz to reduce baseline wander and powerline noise. Then R-wave detection and P-wave extraction are performed in two steps. First, R markers are placed at the point of maximum absolute derivative on the QRS complex. Then, the supposed P-waves are extracted from the segment preceding the R-wave marker of the ECG recording. The training data is SR only without much change in heart rate during the interval. Thus, the onset and offset of the P wave is reliably identified as a manually identified and unchanging amount of time (i.e., number of data points) before the R wave. The time difference between P onset/offset and the R-wave marker can be used as an estimate of P onset and offset for SR data. The training data sets are manually reviewed and individual beats that contain irregularities are removed from the training set. In the test data, potential P wave segments are automatically identified by an interval preceding the QRS complex, which is more easily identified, by leveraging the time difference between P onset/offset and the R-wave marker that had already been defined in the training set.A total of nine features are extracted: six features that estimate the morphology and three statistical features. The six morphological features are obtained by segmenting the P-wave into six contiguous sections and taking the mean value of each section. Dividing the signal into segments and averaging gives a rough additional estimate of shape and location in the time axis. In addition, early and late potential features are the most common P-wave amplitude features, which have been used in the clinical literature for several relevant applications such as detection of patients at risk for paroxysmal atrial fibrillation during sinus rhythm [22], prediction of atrial fibrillation after ischemic stroke [23], and risk assessment of atrial fibrillation after coronary artery bypass surgery [24]. These amplitude features are extracted by averaging the amplitude of the P-wave at different intervals (i.e., first 20ms or last 10, 20, or 30ms). Our morphological features were motivated by those common P-wave amplitude features. If we consider that the average P-wave duration is 120ms and divide it by six contiguous sections, we get about 20ms per division, which is inline with the clinically used late and early potential features.To obtain these six features, the P wave segment is partitioned into six contiguous sections of equal duration. The mean of each sub-segment is calculated to obtain the six figures. This process can also be understood as applying a moving average filter with length equal to 1/6th the P wave segment and then decimating the results to obtain six data points. The number of sub-segments chosen has clear trade-offs: more sub-segments add features but describe the P wave more accurately; with fewer sub-segments, information is lost. Six sub-segments have been chosen as value empirically determined to perform well.The three statistical features include variance, skewness, and kurtosis. All statistical measures used apply to the distribution of the amplitudes of P wave samples and does not consider sample order or temporal information. The variance describes the dispersion of the P wave sample amplitudes. The skewness and kurtosis quantify the asymmetry and peakedness of the amplitude distribution, respectively. There are only two significant qualities to know about the statistical moments equations that were used in this study: (1) they are calculated using the unbiased sample statistics formulas and (2) the equation of kurtosis defines the kurtosis of the normal distribution as 3 (rather than 0) [25].As shown in Fig. 1A, training begins by extracting a feature vector for each beat in the training set. The feature space distribution of SR P-waves is used to create a statistical model that generalizes SR P-waves. The nine-dimensional feature space is modeled by a multivariate Gaussian mixture model (GMM). The Expectation–Maximization (EM) algorithm is employed to generate the model for the training set [26].The iterative application of the EM algorithm converges on a model to describe the feature space distribution of the training set. The EM algorithm is an iterative method that performs likelihood maximization. Given a data set, X, and a statistical model with a set of parameters, θ, the EM algorithm finds θ which maximizes P(X|θ). In the context of this method, the statistical model is a multivariate GMM whose probability distribution function is defined in Eq. (1).(1)p(x)=∑j=1NZwjNd(μj,Σj)where d is the dimensionality of the data, x is a d-dimensional vector of random variables, NZis the number of hidden Gaussian distributions, j is the index of a hidden Gaussian distribution,wj=p(z=j), where z is the hidden random variable, which hidden Gaussian distribution is drawn fromNdis a normal/Gaussian distribution with dimensionality d, μjis the mean vector of length d for hidden distribution j, Σjis the d×d covariance matrix from hidden distribution j.The unknown parameters are defined in Eq. (2).(2)θ=[w1,…,NZ,μ1,…,NZ,Σ1,…,NZ]The parameters are adjusted iteratively in two steps to determine the optimal values: the Expectation step (E step) and the Maximization step (M step). Parameter values are initiated randomly. In the E step, p(zj|x) is evaluated for all x∈X. In the M step, θ for each hidden distribution is updated with the fuzzy inclusion of data points based upon p(zj|x). For example, to find the mean, each data point is weighted by the probability that is originated from the given distribution. The E step and M step are iterated until the algorithm converges, resulting in the parameter set, θ, for which the model fits the data set, X, with the maximum likelihood.Because the EM algorithm operates with a predefined number of hidden distributions and the outcome depends upon the initialization, the optimal model is determined iteratively. Beginning with one hidden distribution, ten models are created. The model with the most successful initialization is chosen. Similarly, a best model with two hidden distributions is determined. The number of hidden distributions is increased until the quality of the model (measured by log-likelihood) does not improve by more than 1% when an additional hidden distribution is added. In this paper, the EM model is defined by the set of parameters μ, Σ, andw(with unique parameters for each hidden distribution). μ1,…,jrepresent the means for each of the j hidden distributions, Σ1,…,jrepresent the covariance matrices for each of the j hidden distributions, andw1,…,jrepresent the probabilities for each of the j hidden distributions. These parameters are used in the test/processing stage to detect AF.Test set evaluation begins with P-wave feature extraction as shown in Fig. 1B. The classifier determines if the feature vector constitutes PWA or AF. The first classifier stage calculates the Mahalanobis distance of the feature vector from each hidden distribution center. The calculation is shown in Eq. (3).(3)Mj(f)=(f−μj)TΣj−1(f−μj)where f is the feature vector, j is the index of the hidden distribution, Mjis the M distance with respect to jth hidden distribution, μjis the mean/center of the jth hidden distribution, Σjis the covariance matrix of the jth hidden distribution and T denotes the matrix transpose.The M distance is then scaled by a spread parameter which adjusts the generalization of the model. Increasing the spread allows for a wider range of variation in normal SR. Adjusting the spread parameter has the same effect as adjusting the threshold parameter, although the relationship between the two is inverse and non-linear. For the purposes of this method, the spread was empirically chosen with a value of 500; smaller or larger values result in thresholds which must be specified with greater precision much closer to 0 or 1, respectively. Each scaled M distance is passed through a radial basis kernel function, scaled again according to the prevalence of that distribution in the training set, and summed together. This described calculation is termed the pScore and is shown in Eq. (4).(4)pScore(f)=∑j=1NZwjexp−ln(1/2)sMj2where NZis the number of hidden distributions, s is the spread parameter (set to s=500),wjis the weight applied to the jth distribution (equal to the probability of the jth distribution). The pScore reflects the likelihood that a feature vector extracted from the segment preceding the QRS complex is also a P-wave. A pScore of 1 reflects a near absolute certainty that the evaluated segment is a P-wave, while a pScore of 0 reflects a near absolute certainty that the evaluated segment is not a P-wave. The pScore is compared to an empirically determined threshold, Tp(as will be explained in Section 3), to determine whether or not PWA and, in turn, AF are present. Fig. 2shows the simple example of the pScore as a function of M distance.The final stage of classification is the optional majority voter post-processing as shown in Fig. 1C. AF determinations outputs (or PWA determinations) are defined as a binary (0 or 1) output depending on whether or not the algorithm determined PWA. Sequential AF determination outputs are determined as the sequential 0s and 1s that are calculated for a test data. The majority voter applies a majority vote to sequential outputs to eliminate errors in an N-modular redundant fashion. By taking a majority vote of an odd number of sequential AF determination outputs, classification errors in a minority of beats are ignored. This stage eliminates the cases of noise, artifact, and/or ectopy isolated to a single beat that are not clinically meaningful and potentially cause classification errors. The majority voter postprocessing is performed in an automated fashion. As can be seen in Fig. 1, the training and testing algorithm is independent of the value of N. So with little computational complexity we can calculate the result of the majority voter for any required N. If N is selected to be 1 it means that there is no majority voter post-processing in the algorithm.

@&#CONCLUSIONS@&#
