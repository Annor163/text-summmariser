@&#MAIN-TITLE@&#
An adaptive neural control scheme for articulatory synthesis of CV sequences

@&#HIGHLIGHTS@&#
We developed a control scheme that estimates motor commands from trajectories of fleshpoints on the selected articulators.We used these motor commands to reproduce the trajectories of the underlying articulators in a 2nd order dynamical system.We implemented the control scheme using fuzzy neural networks, and tested it on consonant–vowel sequences with good results.

@&#KEYPHRASES@&#
Articulatory synthesis,Speech motor control,Neural networks,Fuzzy logic,Mass spring damper,

@&#ABSTRACT@&#
Reproducing the smooth vocal tract trajectories is critical for high quality articulatory speech synthesis. This paper presents an adaptive neural control scheme for such a task using fuzzy logic and neural networks. The control scheme estimates motor commands from trajectories of flesh-points on selected articulators. These motor commands are then used to reproduce the trajectories of the underlying articulators in a 2nd order dynamical system. Initial experiments show that the control scheme is able to manipulate the mass-spring based elastic tract walls in a 2-dimensional articulatory synthesizer and to realize efficient speech motor control. The proposed controller achieves high accuracy during on-line tracking of the lips, the tongue, and the jaw in the simulation of consonant–vowel sequences. It also offers salient features such as generality and adaptability for future developments of control models in articulatory synthesis.

@&#INTRODUCTION@&#
There are mainly two types of synthesis methods in text-to-speech (TTS) applications: concatenative and articulatory synthesis (Birkholz, 2005). Concatenative synthesis uses the stored speech waveforms of phonemes or words pronounced by the human speakers to generate intelligible output. Its applications are limited to the languages and speakers available. In contrast, articulatory synthesis simulates the movements in the speech apparatus of human speakers for the TTS application. It has a stronger physiological basis and is able to produce a larger number of utterances than the concatenative method. In fact, the method offers additional benefits beyond TTS, in applications such as the facial animation (Badin et al., 2002), the medical treatment of speech disorders (Kröger et al., 2008), and the articulatory-phonetic studies in automatic speech recognition (King et al., 2007).However, it remains a challenging task to reproduce the vocal tract trajectories through automatic control in current articulatory synthesis research. A complete articulatory synthesizer usually includes three functional components: an anatomical model, an acoustic model, and a control model. Studies on the anatomical and the acoustic models have developed rapidly in the past decades (Buchaillard et al., 2009; Birkholz et al., 2007; Cook, 1990), but there is a lack of study in the control model. The desired control model should be able to reproduce realistic articulatory trajectories in different phonetic contexts and with different speaking rate. Existing control models often operate manually in a codebook fashion, which applies a set of linguistic rules to define the articulatory targets such as the velocity and the position profile of a particular speech sound (e.g., a phone). Such a synthesis-by-rule approach was initially implemented in the cord-tract model of Ishizaka et al. (1975) and the task-dynamic articulatory model of Saltzman and Munhall (1989). In their approach, each phone usually has one spatial target in the codebook. The articulatory movements for the sequential phonetic strings such as syllables, words, and sentences, are generated by interpolating and/or approximating the targets (Birkholz et al., 2011; Perrier et al., 2005). Different from the codebook approach, Nelson (1983) suggested that the articulatory movements were the result of optimized control similar to that of a second-order dynamical system. Löfqvist and Gracco (2002) supported the view, and they observed that a cost minimization principle could well explain the trajectory curvature of the articulatory kinematics.The main difficulty lies in the dynamics of speech motor control. In the literature, many methods have been proposed to model the the positions and velocities of the speech articulators during speech production, for example, the task dynamic model of Saltzman and Munhall (1989), the models of Perrier et al. (2003) and Buchaillard et al. (2009) based on the Equilibrium Point Hypothesis (EPH) of Feldman (1986). However, the speech dynamics are highly non-linear and contain many uncertainties, which are difficult to describe using the precise mathematical models. There is an urgent need for more adequate modeling methods to realize efficient speech motor control. In this paper, we re-formulated the articulatory dynamics using a mass-spring damper (MSD) in a 2-dimensional articulatory synthesizer. We used the fuzzy neural networks (FNNs) to deal with the unmodeled uncertainties and non-linearity in an adaptive neural controller. In contrast to using the fixed-structured neural networks for the non-linear modeling (Richmond, 2009), the proposed controller embedded a learning algorithm and an adaptive control law to determine the structure and the parameters of the neural topology simultaneously during off-line learning. In other words, FNNs learn the speech dynamics, or the mapping between the input and the output, and store the information in the neural topology. During on-line tracking, the controller estimated a series of motor commands from trajectories of flesh-points on the selected articulators. Then it used these motor commands to reproduce the trajectories for the underlying articulators. We used a set of consonant–vowel (CV) sequences to demonstrate the learning and the tracking process in the simulations. Then the controller manipulated the MSD to reproduce the smooth trajectories in the selected articulators. Our experiments showed that it achieved high accuracy during on-line tracking of the lips, the tongue, and the jaw in the CV sequences.The rest of the paper is organized as follows. Section 2 introduces the background and the theoretical basis of speech motor control. Section 3 formulates the articulatory dynamics in the MSD based 2-D vocal tract system. Section 4 describes the structure, the learning algorithm, and the adaptive laws of the proposed E-FNN controller. Section 5 describes the experimental settings and the simulation procedures. Section 6 discusses the results. Section 7 concludes this paper.In speech motor control, the equation of motion governs the dynamics of the articulators. It is analogous to the MSD (Kröger et al., 1995; Perrier and Ostry, 1996; Kelso et al., 1986), which follows Newton's law(1)u+Fe+Ff=Mz¨+Bz˙+Kz,where M, B, and K are the mass, damping, and stiffness coefficients of the speech articulators (e.g., the tongue tip and the lower lip) in the anatomical model of an articulatory synthesizer. Feis the external force due to the gravity factor and the air pressure inside the tract, and Ffis the friction force between the adjacent muscular structures, which can be assumed to be negligible due to the saliva. We describe the articulators using the motion vectors, termed vocal tract variables (TVs), wherez,z˙, andz¨are the position, the velocity, and the acceleration parameters, respectively. u is the input force or the activation level of the muscular structures which control the TVs. We refer to the set of muscular activation forces as the motor variables (MVs).In comparison, the task dynamic model of Saltzman and Munhall (1989) distinguishes three kinds of variables at two levels: the activation coordinates at the inter-gestural level, and the model articulator coordinates (the actual spatial position of the articulators) and tract-variable coordinates (the location and degree of constriction of the articulators) at the inter-articulator level. In this paper, the MVs are analogous to the activation coordinates. The TVs are not replicates of Saltzman and Munhall's model articulator coordinates, nor are they the same as the tract-variable coordinates. Instead they are the pellet position/coordinates at the selected constriction locations on the articulators (more on these variables in Section 3).The equation of motion describes the quasi-incompressibility of the speech articulators during the speech production (Kim and Gomi, 2007). It yields close-loop solutions by choosing the appropriate time-variant variables. For example, the Equilibrium Point Hypothesis (EPH) of Feldman (1986) used the equilibrium positions as the time-variant variables, the shift of which results in the movements of the articulators. Perrier and Ostry (1996), Perrier et al. (2003), and Buchaillard et al. (2009) applied the EPH control concept in a finite element model of the tongue, and solved the differential equation using combined Newton-Raphson and Newmark method. In contrast, Saltzman and Munhall (1989) considered the stiffness coefficients as the time-varying variables. They used a pseudo-Jacobian inversion matrix to calculate the gestural control parameters for the desired articulatory movements in the differential equation. The concept still resembles the EPH method, since the stiffness directly affects the velocity with which the equilibrium length is restored (Boersma, 1998). The concept is also used in the articulatory synthesizer of Birkholz (2005), Kröger et al. (1995, 2009). However, it requires explicitly defining the gestural scores and the activation intervals for the system state profile[z˙,z]in the control model, which are highly error prone especially at the phonetic boundaries (Kelso et al., 1986). Another way to solve the differential equation is to use the time-varying input force functions to reproduce the system state profile[z˙,z], the velocity and position trajectories (Kröger et al., 1995). The coefficients M, K, and B assume values that are close to human tissues in the vocal tract. In this manner, the equation of motion in (1) simplifies to an ordinary differential equation. For example, van den Doel and Ascher (2008) formulated a wall displacement model(2)p(x,t)=Mz¨(x,t)+Bz˙(x,t)+Kz→(x,t),where the driving force is from the air pressure p inside the tube. Additional discretization techniques such as the leap-frog scheme (Boersma, 1998) and the Newmark methods (van den Doel and Ascher, 2008) are then used to solve the equation during articulatory and acoustic simulation. One major drawback of this approach is the high computation cost which renders it inefficient for on-line articulatory control.Moreover, the dynamic MSD system in (1) is highly non-linear and contains uncertainties which are difficult to describe using precise mathematical model. There are mainly three difficulties.1.Human vocal system consists of soft tissues as well as bony structures, e.g., the hard palate. Consequently, the MSD system contains unmodeled variabilities in the M, B, and K parameters, which vary from speaker to speaker (e.g., physiological differences) and for the same speaker under different conditions (e.g., emotional states). The stiffness of muscular tissues also changes during activation (Duck, 1990; Perrier et al., 2003).The articulatory movements are affected by the phonetic structure of continuous speech, which introduces unmodeled variabilities.During speech production, the modeling of constriction is not linear. Though the articulatory movements are smooth between vowel targets, the transitions to/from the consonants such as plosives, nasals, and laterals, are not so. For example, when the tongue tip hits the alveolar ridge during [d/t] production, the collision introduces points of discontinuity in the vocal tract at the onset of the closure, rendering the model non-linear (Birkholz et al., 2011).Neural networks (NNs) have shown advantages in non-linear modeling of dynamic control systems. For example, Saltzman and Munhall (1989) proposed to use Jordan's recurrent neural networks (RNNs) (1986) to incorporate the temporal dynamics and learning algorithm in the control model. Hirayama et al. (1993) applied NNs to learn the inverse dynamics of speech motor control. More recently Fang (2009) used a general regression neural model to infer motor commands from the articulatory measurements. However, these are fix-structured NNs, which use a trial-by-error approach to determine the parameter and structure in the neural controller. As a result, the controller performance is subject to the experimenter's decision rather than the property of the dynamic system. In this aspect, NNs with fuzzy logic, or FNNs are more appropriate than the fix-structured NNs (Wang, 1997). They have been used to improve speech motor control in articulatory synthesis. For example, Kröger et al. (2009) used self-organizing maps to learn the motor commands and the tract variables from phonetic sequences, and obtained encouraging results in the articulatory synthesizer. FNNs have yet to reach their full potential.Previously we have introduced an adaptive neural controller, termed the generalized dynamic fuzzy neural network (GD-FNN) controller (Wu et al., 2001; Er and Gao, 2003). The controller has shown excellent performance in terms of tracking accuracy and computational efficiency for several non-linear dynamic systems with unmodeled variabilities, e.g., an inverted pendulum, a robot manipulator (Gao and Er, 2003), and a drug delivery system (Gao and Er, 2005). In this study, we applied the adaptive neural control model to reproduce the articulatory trajectories of the vocal apparatus in a 2-dimensional (2-D) articulatory synthesizer. The GD-FNN infers knowledge about the articulatory dynamics and stores the information in the neural structures and the fuzzy logics. The proposed control scheme is an extended version of GD-FNN, referred to as E-FNN. It integrates the radial basis function neural network (RBF-NN), the fuzzy inference network (FIN), and the recurrent neural network (RNN) in one neural topology. The recurrent layer is added to the original GD-FNN to deal with the temporal dynamics in the articulatory speech patterns (Jordan, 1986). The complete E-FNN controller also embeds a learning algorithm and an adaptive control law to determine the structure and the parameters of the neural topology simultaneously. Our main hypothesis is that it is possible to deal with the uncertainty and the non-linearity in the mapping between the muscle activities, the MVs, and the articulatory trajectories, the TVs, in speech motor control. Unlike the TVs, the MVs are usually hard to measure or not completely retrievable in human speech production. In this study, the E-FNN learns to predict the MVs from the TVs using the generalization abilities of fuzzy logics and NNs. We then couple the E-FNN model with a proportional integral derivative (PID) controller to manipulate a MSD system to reproduce the continuous and smooth articulatory trajectories of the desired consonant–vowel (CV) sequences. We test the tracking accuracy of the E-FNN controller on electromagnetic articulography (EMA) data of the vocal tract in CV articulation.The controllability canonical form for a 2nd order time-variant non-linear system is (Slotine and Li, 1991),(3)z¨(ts)=fn(z_,ts)+gn(z_,ts)u(ts)+dn(ts),wherez_=[z˙,z]is the state vector, velocity and position, of the system, fnand gnrepresent the non-linearities of the mapping from the input u to the output z, and d represents the uncertainties and external disturbances of the dynamic system, and dnis the unmodeled uncertainties. If we further define the non-linear dynamic functionfn(z_,ts)=f(z_,ts)+Δf(z_,ts), and the control gaingn(z_,ts)=g+Δg(z_,ts), where f and g are the nominal parts, Δf and Δg are the unknown parts or the uncertainties of f and g (Lin and Li, 2012), the canonical form can be re-written as,(4)z¨(ts)=f(z_,ts)+gu(ts)+d(ts),whered(ts)=Δf(z_,ts)+Δg(z_u(ts),ts)+dn(ts)is the unknown uncertainties. From the equation of motion of the MSD system in (1) and the controllability canonical form in (4), we have(5)f=−BMz˙(t)−KMz(t),and(6)g=1M.Since g≠0 for allz→, the system is controllable (Lin and Li, 2012; Slotine and Li, 1991; Gao and Er, 2003). Furthermore, the function f and d are assumed to be bounded in human vocal system.We focus on the control of the vocal tract including the lips, the tongue, and the jaw in a 2-D articulatory synthesizer, as shown in Fig. 1, which was constructed by Mermelstein (1973) based on the X-ray image of a human speaker. The vocal tract has elastic walls, which are analogous to the MSD. The TVs are the pellet points at the selected constriction locations on the articulators, as shown in Fig. 1. The 12 TVs include the x–y coordinates of the tongue root (TRx, TRy) relative from its neutral or resting position, the tongue body (TBx, TBy), the tongue tip (TTx, TTy), the lower lip (LLx, LLy), the upper lip (ULx, ULy), and the lower incisor (LIx, LIy). The MVs represent the muscular forces, which underly the TVs in the 2-D vocal tract. The 8 MVs cover one intrinsic tongue muscle: superior longitudinal (SL), which retracts or flaps the tongue tip; four extrinsic tongue muscles: anterior genioglossus (GGa), posterior genioglossus (GGp), hyoglossus (HG) and styloglossus (SG), which change the shape and position the tongue dorsum: body and root; three facial muscles: masseter (MA) which raises the lower jaw, risorius (RO) and orbicularisoris (OO) which constrict, round, and spread the lips. The vocal cords, shown as the glottis in Fig. 1, are not included in this model for two reasons. Firstly the control of vocal cords is more effective with stiffness parameters than the MV parameters (Flanagan et al., 1975). Secondly the vocal cords can cause non-unique mapping between the TVs and the MVs because it can compensate for vocal tract changes in speech production (Schroeter and Sondhi, 1994). For example, if we are to model the simple voicing contrast for the [p/b] and the [t/d] pairs, additional control variables regarding the timing of glottal excitation need to be specified in the vocal cords. Therefore, in the present model, it is not used as a control variable.As shown in Fig. 2(a), during off-line training, the proposed E-FNN model learns the inverse characteristics between the input MV, u, and the output TV, z, in the dynamic MSD system. Since the exact MVs are unknown, and the parameters M, B, and K vary from speaker to speaker and for the same speaker in different phonetic contexts, the E-FNN controller uses the reference MVs, ur, (details given in Section 5) and the desired TVs, zd, to learn the system non-linearities and dynamics through an embedded learning algorithm. Using the training data pairs, the algorithm determines the structure and the parameters of the E-FNN such as the number of hidden neurons and the weights systematically and automatically through an iterative supervised learning (Section 4.2). During on-line tracking, as shown in Fig. 2(b), instead of looking for exact MVs, the PID controller generates the compensation output and the tracking error at each sample time for the overall control system. It embeds an adaptive control law, which uses the error rate as the weight update criteria and stores the system dynamics and the mapping functions in the E-FNN (Section 4.3). In this manner, the E-FNN controller infers the muscular activation patterns from trajectories of flesh-points on the selected articulators.The E-FNN architecture is shown in Fig. 3, which has a total of five layers. It incorporates the Takagi–Suegeno–Kang-type fuzzy inference system, the RBF-NN, and the RNN in a connectionist structure, which is extended from the GD-FNN (Wu et al., 2001; Er and Gao, 2003; Gao and Er, 2005). We add the recurrent layer to account for the temporal dynamics in speech motor control (Jordan, 1986). Nodes and links in layer one and two act as a fuzzifier, while nodes and links in layer four act as a defuzzifier. We usexi(l)to denote the ith input of a node in the lth layer, andyi(l)to denote its corresponding output in layer l. The function of the node in each layer is given in the following.•Layer 1: The input layer. Each node transmits the input variable to the next layer directly:(7)yi(1)=xi(1),i=1,…,Ni.For the inverse control model, Ni=36, which includes the position, velocity and acceleration of the 12 TVs:[z¨,z˙,z].Layer 2: The membership function layer. It specifies the degree to which an input variable belongs to a fuzzy set using Gaussian membership function:(8)yi(2)=exp−(xi−cij)2σij2,where cijand σij, i=1, …, Ni, j=1, …, Nj, are the center and the width of the Gaussian function for the jth term in the ith input variable. These parameters are obtained in the learning procedure.Layer 3: The rule layer. The number of nodes indicates the number of fuzzy rules. The output of a rule node indicates the firing strength of its corresponding rule, defined as(9)yj(3)=y(6)∏i=1Nixi(3),where y(6) is the output of the recurrent layer.Layer 4: The weight layer. The TSK-type fuzzy output weights are obtained in the structure learning procedure. The node outputyk(4): k=1, …, Nkis the weighted sum of the incoming signals, which is a fuzzy “OR” operation:(10)yk(4)=∑k=1Nkxk(4)=∑j=1Njyj(3)wjk,which integrates the fired rules on the same consequence neuron. The weight is:(11)wjk=K0+∑i=1NiKixi,where K's are manually-set and real-valued parameters.Layer 5: The defuzzification layer. Each node in this layer corresponds to one output variable. The output function is defined as(12)yo(5)=∑k=1Nkxk(5)wok∑k=1Nkxk(5),wherexk(5)=yk(4),wokis the link weight from the kth term in layer four to the oth output variable in layer five, o=1, …, No, and No=Nk. In the control model, No=8, which is the number of the MVs in the dynamic MSD system. The neural function generates a output in [0, 1], which is the normalized activation level of the MVs.Recurrent layer: it calculates the firing strength of the recurrent variablerk=yk(4)to the rule layer. The number of recurrent nodes is the same at that of the output node in layer four. The node acts as a delay line to account for the contextual information in the temporal patterns. The node function is defined as(13)yk(6)=11+e−rk.The function can be interpreted as a global membership function, which “remembers” the history of discourse in the recurrent variables (Jordan, 1986). The recurrent outputs are fed back to the rules nodes in layer three, which stores the firing history of the fuzzy rules.The learning algorithm enables simultaneous learning of the E-FNN structure and parameter, which was proposed and implemented in our previous studies (Wu et al., 2001; Gao and Er, 2003; Er and Gao, 2003). Structure learning determines the number of membership functions in layer two and the number of fuzzy logic rules in layer three. Parameter learning determines the Gaussian parameters in layer two and the link weights in layer four, i.e., the membership functionyi(2)(cij,σij), and the weight parameterswjk. It uses the semi-closed fuzzy set for membership learning and the linear least square method for weight learning. Structure learning automatically creates or deletes fuzzy rules according to the system error and the error reduction ratio in the E-FNN controller. Learning repeats for each input and output data-pair. The parameters and the structure of the E-FNN are tuned automatically on the training data. Initially there are no fuzzy rules in layer three, and they are created or deleted automatically as the learning proceeds. Detailed mathematical descriptions of the learning algorithm, convergence analysis, and stability analysis of the FNN based controller in dynamic modeling are given in (Gao and Er, 2003).After obtaining the initial value of the weight vectorwijduring the learning process, the E-FNN based controller embeds an adaptive control law to adjust the vector to compensate for the modeling errors in the learning algorithm (Gao and Er, 2005). In this study, the E-FNN controller is connected with a PID controller via adaptive control, as shown in Fig. 2(b). The PID controller serves as a feedback compensator which also stabilizes the inverse dynamic modeling (Gao and Er, 2005; Lin and Li, 2012). The adaptive control law is designed as follows,(14)uc(ts)=uE-FNN(zd,ts)−uPID(ts).The PID control output is given by,(15)uPID=Kpe(t)+Ki∫e(t)dt+Kde˙(t),where e is the tracking error: e(t)=zd−z between the desired target position and the displacement of the MSD. The matrix K=[Kp, Ki, Kd] contains real numbers, and the proper choice of K affects the convergence speech of the tracking performance. The adaptive law adjusts the weight vectors in layer three and four of the E-FNN to minimize the square error E between the desired target position and the estimated position,(16)E(ts)=12uPID2.The discrete gradient method is used to minimize E. The adaptive law of the weight vector is derived as (Wu et al., 2001),(17)ΔW=−ηδEδW(18)=−ηδEδuS-FNNδuS-FNNδW(19)=−ηδ12(uc(ts)−uS-FNN(ts))δuS-FNNδuS-FNNδW(20)=ηuPID(ts)ϕ(zd,ts)where η>0 is the learning rate.Simulation includes two stages: off-line learning and on-line tracking. In the first stage, the learning algorithm decides the initial weight parameter and the fuzzy rules of the E-FNN topology. It models the inverse dynamics between the motor commands and the tract variables. For this stage, we need to train the E-FNN on parallel MV and TV data. Ideally the training data consist of MVs and TVs measured on the human speech apparatus, such as the electromyographic (EMG) and the EMA recordings. The EMG recordings describe the level of muscular forces in the EPH model, while the EMA recordings describe the corresponding articulatory movements.In this study, we used the CV sequences from the multichannel articulatory (MOCHA) database, which consists of two speakers: one male (MSAK0) and one female (FSEW0), each uttering 460 TIMIT sentences (Wrench, 1999). 807 CV syllables are available in the training data. Each CV sequence has a syllable initial plosive (with or without stress) for every combination of the vowels [=, E, A, 4, K] and the plosives [p/b, t/d, k/g] in the pilot study. The EMA data in MOCHA records the movements of the articulators, or the 12 TVs. Similar to Mermelstein's 2-D model, the bridge of the nose and the upper incisor are taken as the reference point in the x–y coordinates (Browman and Goldstein, 1992). The trajectory vectors are z-normalized to have zero mean and unit variance, similar to Richmond (2009). The EMA data have at a sampling rate of 500Hz.Reliable EMG data are usually difficult to obtain in articulatory studies, for example, through needle insertion (Baer et al., 1988). In their experiments, Baer et al. (1988) observed that the tongue muscles, GGa, GGp, SG, and HG have distinctive level of EMG activation for the cardinal vowels in [pVp] sequences. For example, a single threshold of EMG level can distinguish a front vowel from a back vowel, and each vowel group has consistent activation patterns. The claim was supported by Buchaillard et al. (2009) in the modeling of tongue muscles for cardinal vowel production. To this end, we use an alternative set of reference MVs derived from linguistic studies and the existing EMG recordings to initialize the learning process. As shown in Table 1, each phone represents a cognitive linguistic unit. It corresponds to the motor activity of the muscles, which are normalized to the [0, 1] interval to suit the NN input. For example, GGa is activated (=1) for the front vowel [=], GGp for the high vowels [i] and [u], SG for the back vowels [4] and [u], and HG for the low vowel [=]. To reduce the variability of jaw positions, the MA activation is lower for the low vowel [=] and [4] than for the high vowels [i] and [u]. For the plosive pairs, OO and RO is activated for the labial [p/b], SL and SG for the alveolar [t/d], GGa and GGp for the velar [k/g].During on-line tracking, the E-FNN controller retrieves the muscular activations in the CV sequences and reproduces the desired articulatory trajectories. We used a step function to simulate the muscle activation from the consonant to the vowel. The reference MVs in Table 1 are then updated by the PID compensator on the phonetic segments. The off-line learning in this study is analogous to the babbling stage in human speech acquisition while the on-line tracking corresponds to the imitation stage (Kröger et al., 2009; Bailly, 1997).The E-FNN learning algorithm operates at a sampling rate of 100Hz. The MOCHA training data are divided randomly into five sets, four of which are used for off-line training, the other one used for on-line tracking. The structure and parameter of the E-FNN are determined simultaneously on the four training sets of MV and TV data pairs. Using the learning algorithm, a total number of 23 fuzzy rules are created after training. Fig. 4shows that the root mean square error (RMSE) converges after training on 250 samples.The trained E-FNN estimates the MVs given the desired articulatory trajectories. It couples with the PID compensator for on-line adaptive control of the MSD system. The controller manipulates the MSD to infer the muscular activation patterns and to reproduce the desired articulatory trajectories in the 2-D articulatory synthesizer. Mermelstein's 2-D vocal tract model with a tract length of 17.5cm is divided into 89 tube sections, with a uniform length of Δx=1.966×10−3m and a thickness of Δy=1×10−2m (Mermelstein, 1973; Boersma, 1998). The tube wall property is measured in the relax cheeks of a human adult speaker (Ishizaka et al., 1975; Birkholz and Jackèl, 2004), M0=21kg/m2, B0=8000kg/m2s, K0=845, 000kg/m2s2. The mass, damping, and stiffness parameters of the MSD manipulator in (1) are calculated as:(21)M=M0ΔxΔy=4.129×10−4kg,(22)B=B0ΔxΔy=0.157kg/s,(23)K=K0ΔxΔy=16.615kg/s2.The system state profile for the neutral or resting position are set asz˙=0, and z=0. The gains of the PID compensator are set as Kp=25, Ki=30, and Kd=5. The learning rate is η=0.005.

@&#CONCLUSIONS@&#
