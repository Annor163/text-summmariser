@&#MAIN-TITLE@&#
An empirical investigation of Web session workloads: Can self-similarity be explained by deterministic chaos?

@&#HIGHLIGHTS@&#
We investigate the models used in Web traffic generators.We examine deterministic chaos as an alternative explanation of self-similarity.Deterministic components are found to be significant in Web session data.Deterministic and stochastic predictive models of the data are equally accurate.

@&#KEYPHRASES@&#
Web traffic,Session workload,Traffic modeling,Chaos theory,Nonlinear time series analysis,

@&#ABSTRACT@&#
Several studies of Web server workloads have hypothesized that these workloads are self-similar. The explanation commonly advanced for this phenomenon is that the distribution of Web server requests may be heavy-tailed. However, there is another possible explanation: self-similarity can also arise from deterministic, chaotic processes. To our knowledge, this possibility has not previously been investigated, and so existing studies on Web workloads lack an adequate comparison against this alternative. We conduct an empirical study of workloads from two different Web sites: one public university, and one private company, using the largest datasets that have been described in the literature. Our study employs methods from nonlinear time series analysis to search for chaotic behavior in the web logs of these two sites. While we do find that the deterministic components (i.e. the well-known “weekend effect”) are significant components in these time series, we do not find evidence of chaotic behavior. Predictive modeling experiments contrasting heavy-tailed with deterministic models showed that both approaches were equally effective in modeling our datasets.

@&#INTRODUCTION@&#
By one estimate, over two billion people now use the Internet (Staff, 2010). This user population now demands rich-media content in their Web experience. They want to play massively-multiplayer online games, receive IPTV, and store their data in the “cloud” – and all of this consumes huge amounts of bandwidth. Estimates of Internet traffic growth continue to show large year-over-year compound growth, reaching 1300 exabytes by 2016. Web traffic continues to be a large segment of this traffic, accounting for 5.4 exabytes of traffic per month in 2012 and is expected to rise to 17.5 exabytes per month in 2016, a compound annual growth rate of 35% (peak traffic is growing even faster) (Staff, 2012). Moreover, the number of web users worldwide is also increasing rapidly, rising from 2.03 billion in 2010 to 2.80 billion by 2015 (Staff, 2010). These massive traffic volumes mean that performance testing of network systems using realistic data is essential, and yet infeasible; it is simply not possible to indefinitely store a sample of network data covering an operationally significant period of time. Instead, modern performance testing relies on traffic generators to produce synthetic network traffic with the same characteristics as real-world data (Botta et al., 2012). This in turn requires accurate characterization of that traffic. Since Leland et al.’s landmark paper (Leland et al., 1994), network traffic in general has been known to be self-similar; informally, this means that observations of traffic intensity will tend to be identically distributed irrespective of the time scale of these observations. More recent findings also extend this result to Web sessions (a measure that captures user behavior), which have also been found to be self-similar (Arlitt & Jin, 2000; Arlitt & Williamson, 1997).Leland et al.’s classic explanation for self-similarity in network traffic rests on two points: first, network traffic had been found to be self-similar in earlier research (Meier-Hellstern et al., 1991). Second, Leland proposes that Ethernet traffic can be viewed as the aggregation of many renewal–reward processes (Mandelbrot, 1969; Taqqu & Levy, 1986) whose inter-renewal times exhibit infinite variance (i.e. follow a heavy-tailed distribution). An aggregate of many of these processes is known to be self-similar. This explanation is also posited by Arlitt to account for self-similarity in Web session lengths (Arlitt & Jin, 2000). However, this account neglects a possible alternative explanation for self-similarity: deterministic chaos. It is well-known that Web traffic (indeed all network traffic) has periodic components that are driven by the time of day, the day of the week, etc. – a phenomenon popularly referred to as the “weekend effect.” These deterministic dynamics also operate at a range of time scales (albeit tending towards longer scales), and could potentially play a significant or even dominant role in causing Web sessions to appear self-similar. Chaotic maps have been previously proposed as an alternative explanation for self-similarity in network traffic (see for instance Erramilli et al., 1995; Mondragon et al., 2001), but have not been examined for Web sessions.Web session generators are used in much the same way as network traffic generators; that is, they provide a data source for load testing of Web applications. The risk in leaving the possibility of nonlinear determinism unexplored is that simulated session data may in fact diverge from reality much more than is currently expected. Given the traffic and user-population growth described above, it seems plain that any widely-used Web application will face an increasingly challenging load profile as time progresses. Thus, load-testing of Web applications will only grow more stringent with time. However, as the frequency of Web sessions in a test increases, any discrepancies between the Web session generator and real-world behavior will be magnified. If self-similarity in Web sessions arises from deterministic components, then the traffic models used in load testing are ultimately misleading. Thus, until and unless the alternative of deterministic chaos is investigated, the model for Web session self-similarity proposed in Arlitt and Williamson (1997) and employed in Goseva-Popstojanova et al. (2006a) – and thus all session–generation systems based on them – will remain debatable. Such models are heavily used during the development of web services to analyze the performance and capacity of various server configurations and software alternatives (push vs. pull services). These models become even more critical when we consider the next generation of cloud-based infrastructure and services based upon service-orchestrated architecture (SOA) models.Our goal in this article is to provide a rigorous empirical investigation of the possibility of deterministic chaos in Web session durations. Using the methods of nonlinear time series analysis (Kantz & Schreiber, 2003), we will investigate web server logs from two organizations (one public university, one private company) that cover much longer periods of time than earlier studies of self-similarity in Web sessions. We will first test the supposition that deterministic, rather than stochastic, processes (a necessary precondition to being chaotic in nature) best explain the datasets. We will then attempt to extract a chaotic invariant from these datasets, and thus evaluate this alternative explanation of Web session self-similarity. Finally, we will compare two forecasting algorithms (one deterministic, one stochastic) on these datasets, to determine which approach more accurately models network traffic.The remainder of this article is organized as follows. In Section 2, we provide essential background. In Section 3, we describe the methodology for our data collection and analysis, and we attempt to extract chaotic invariants in Section 4. We describe our predictive modeling experiments in Section 5, and close with a summary and discussion of future work in Section 6.In the classic study by Leland et al. (1994), the number of Ethernet packets per time unit on a busy network was plotted for time periods ranging from 100s to 0.01s; the resulting time series seemed to have identical characteristics at all length scales, i.e. the traffic was self-similar. Multiple subsequent studies have confirmed this result. This fact has several implications for network engineers. Firstly, source models for network traffic can be very simply constructed using heavy-tailed distributions; indeed, Leland et al. proposed that the reason why network traffic was self-similar was that file sizes (and hence transfer sizes) on the network followed a heavy-tailed distribution. Secondly, common measures of the “burstiness” of network traffic become heavily dependent on the time scale of the observation if traffic is self-similar, which is a highly undesirable characteristic (Leland et al., 1994). However, our concern in this article is Web traffic and Web system workloads, which are strongly influenced by user behaviors. Merely recording the number of bytes sent or received gives little insight into user interaction dynamics, and so a different metric is required.Modern Web applications are complex pieces of software that are often mission-critical (e.g. for e-commerce vendors). Today, browsers must render a huge variety of active content (Javascript code, Flash movies, ActiveX controls, etc.), hosted from a wide variety of software platforms. Users navigate through this tidal wave of information (or at least attempt to) by following links within a website’s structure; user navigation patterns, and the activities they engage in while on a site, are a prime focus of the discipline of Web usage mining (Bordogna & Pasi, 2010; de Campos et al., 2010; Mobasher et al., 2000; Srivastava et al., 2000; Tseng et al., 2008). They are also a key focus of the literature on web systems reliability. Unlike more traditional software systems, the primary workload for Web systems is interactive search and retrieval, rather than computation. Furthermore, the user population is ill-defined; every person on the planet with an Internet connection is a potential user, and so no “standard” user profile exists. Thus, Web systems engineers must use fundamentally different metrics to measure their workloads, and to establish design goals for their systems (Tian et al., 2004).Some of the metrics proposed for website workload measurement are drawn from the network engineering literature: the number of requests made to the server (hits), the number of bytes transferred, and the number of users (actually the number of unique IP addresses) would be familiar to any network engineer. They are measured from the server logs kept by each individual Web server. However, these are not particularly effective in understanding the workload of a Web system:•The number of hits means little without knowing what objects were requested.The number of bytes transferred illustrates bandwidth usage but tells us nothing about the (interactive) operations leading to those transfers.The “number of users” is misleading because most individual users do not have a static IP address. Usually, IP addresses are dynamically assigned from a limited pool, and so one address may correspond to several users.A more effective metric (which captures aspects of both traffic volume and user behavior) is the number of Web sessions and their duration (Tian et al., 2004). A Web session is defined as a sequence of actions taken contemporaneously by a particular user at a particular website. The length of a session thus depends on the nature of the users’ interaction with a website; for example, a user watching a feature-length movie streamed from a website will have a much longer interaction (and receive a much greater volume of data) than a user who merely checks the current weather on a news site. This metric is unique to Web traffic, because it captures the notion of user behavior, as recorded by the sequence of HTTP requests made to the site by the user.The Web session is one of the most popular units of measure for website traffic analysis, workload analysis, and user behavior modeling, and has been utilized by numerous researchers (Arlitt & Jin, 2000; Arlitt & Williamson, 1997; Arlitt et al., 1998; Goseva-Popstojanova et al., 2004, 2006a, 2006b). Cherkasova and Phaal (1998) proposed the Web session metric, with further discussion in Menasce et al. (1999, 2000); note, however, that Web sessions were discussed in an earlier paper by Crovella and Bestavros (1997). Since that time, the Web session metric has been employed in Web mining studies to create personalization algorithms (Eirinaki & Vazirgiannis, 2003; Mobasher et al., 2000); studies of search engine utilization (Jansen & Spink, 2003); and was the proposed unit of measure in a load-managing algorithm for improved quality of service (Cherkasova & Phaal, 2002). Sessions offer much finer-grained information than the standard number of users metric, and can be the basis for much more detailed investigations (e.g. studying the number of bytes transferred, errors and types of errors per session (Goseva-Popstojanova et al., 2006a; Huynh & Miller, 2009), the number of requests, the length of the session, and inter-session arrival times (Arlitt & Jin, 2000).Despite its popularity, the Web session is a difficult metric to capture. HTTP is a stateless protocol, meaning that no history of actions is maintained between one user request and the next. Web applications that require state information, for instance, often use session cookies to simulate a stateful connection (although this workaround introduces security vulnerabilities (Tappenden & Miller, 2009)). In order to determine the end-point of a session, the typical approach is to define a Session Time-out Threshold (STT), which defines a period of user inactivity that signals an end to a given session. In recent work, Goseva-Popstojanova et al. (2006a, 2006b) use STT=30min; this is a common value used by other researchers (Berendt, 2001; Mahoui & Cunningham, 2000; Mat-Hassan & Levene, 2005; Spiliopoulou et al., 2003). This value was rounded up from a suggested STT of 25.5min due to Catledge and Pitkow (1995), who claimed that the most “statistically significant” events occurred within 1.5 standard deviations (2.5min) from the mean time between each user event (9.3min). By contrast, Huynh and Miller (2009) proposed a probabilistic model to determine the STT from the Web logs of a site, meaning that the STT would be site-dependent. Although this approach is attractive as websites vary greatly, the current study will employ the 30-min threshold for comparability with previous studies.

@&#CONCLUSIONS@&#
Our goal in this article was to provide a rigorous empirical investigation of the possibility of deterministic chaos in Web session durations. We collected web access logs from two separate organizations: one is a department at a very large public university; the other is a private company. The university website is important to the operation of the department, but is not mission-critical; the corporate website, by contrast provides one of the core revenue streams for the company, and is thus a mission-critical website. From these logs, we extract a total of six datasets, each covering one continuous period of operation (none of which are less than 1month). Each dataset is a time series that records the number of open Web sessions in 1-min intervals. We then employ the methods of nonlinear time series analysis to test for the presence of chaotic behavior in these datasets. In three of the datasets (two from the university web site, one from the corporate site), we determined that the deterministic components of the signal (i.e. the well-known weekend effect) appear to dominate the stochastic components of the signal. However, a subsequent check for the presence of deterministic chaos was negative; we were not able to extract a correlation dimension for any of these datasets. We then contrasted deterministic and stochastic models in a time-series prediction experiment on all six datasets, finding that the two models performed very similarly. Ultimately, we found that there was insufficient evidence to favor deterministic models over the existing model of heavy-tailed processes.In future work, we plan to replicate our predictive modeling study using web access logs from a broader array of websites. We first want to check if RBF networks and FARIMA models are still equally effective across a broad range of websites. If they are, we will seek to understand why two such radically different models would have such similar performance on these datasets. One intriguing possibility is that the mix of stochastic and deterministic components might be nearly “balanced;” if so, Web session models for load testing should incorporate this characteristic (perhaps as a fine-grained hybridization of RBF networks and heavy-tailed processes). We will also investigate intelligent algorithms for determining the length of a session. The use of a simple timeout threshold for capturing such a complex idea as user behavior is almost certainly less than optimal, and this will directly impact the accuracy of the Web session metric.