@&#MAIN-TITLE@&#
Moment Ratio estimation of autoregressive/unit root parameters and autocorrelation-consistent standard errors

@&#HIGHLIGHTS@&#
A new Moment Ratio estimator is proposed for an AR(p) model of regression errors.The new estimator gives CIs with less size distortion than popular alternatives.Newey–West HAC standard errors can far overstate regressor precision.Lack of cointegration does not necessarily imply a regression is spurious.The new estimator is applied to a quadratic trend model of US GDP.

@&#KEYPHRASES@&#
Method of Moments,Autoregressive processes,Unit root processes,Regression errors,Consistent covariance matrix,Real GDP growth,

@&#ABSTRACT@&#
A Moment Ratio estimator is proposed for an AR(p) model of the errors in an OLS regression, that provides standard errors with far less median bias and confidence intervals with far better coverage than conventional alternatives. A unit root, and therefore the absence of cointegration, does not necessarily mean that a correlation between the variables is spurious. The estimator is applied to a quadratic trend model of real GDP. The rate of change of GDP growth is negative with finite standard error but is insignificant. The “output gap,” often used as a guide to monetary policy, has an infinite standard error and is therefore a statistical illusion.

@&#INTRODUCTION@&#
Serial correlation is a pervasive problem in time series models in econometrics, as well as in statistics in general. When, as is often the case, positive serial correlation is present in both the errors and the regressors, it has long been well known that the Ordinary Least Squares (OLS) estimates of the standard errors are generally too small, and hence the derivedt-statistics too large (Bartlett, 1935; Quenouille, 1952). If the form and parameters of the error serial correlation were known, it would be elementary to compute correct standard errors for OLS regression coefficients. However, observed regression residuals are typically much less persistent than the unobserved regression errors. Correlations estimated directly from the regression residuals therefore provide inadequate indication of the serial correlation that is actually present. This problem is particularly severe as the persistence in the errors approaches or even reaches a unit root.11A Supplementary Information file (see Appendix A), containing several figures that were omitted for reason of space, is available as an electronic annex to this article, as are Matlab programs implementing the proposed Moment Ratio Estimator.The present paper proposes a Moment Ratio (MR) estimator for the parameters of an Autoregressive (AR) model of the errors in an OLS regression. Although it is computed from the conventional residual correlation coefficients, it reduces their negative bias, and provides standard errors with far less median bias and confidence intervals with far better coverage than conventional alternatives. The MR estimator is in the spirit of the Median Unbiased estimator of Andrews (1993) and McCulloch (2008), but does not require laborious Monte Carlo simulation of the distribution of the sample autocorrelations. By allowing the AR order to increase with the sample size by the same formula that is commonly used for Newey–West (1987) Heteroskedasticity and Autocorrelation Consistent (NW-HAC) covariance matrix, the MR estimator shares its consistency with respect to autocorrelation, while greatly mitigating its substantial finite-sample bias.A unit root in the errors requires reposing the problem, greatly increases the variance of OLS slope coefficients, and renders the intercept unidentified, but otherwise presents no insurmountable difficulties. In particular, the presence of a unit root in the errors does not by itself indicate that OLS slope coefficients are spurious. A unit root test similar in spirit to that of Andrews and Chen (1994) is developed that has only moderate size distortion in simulations using a trend line regression. An MR-HAC estimator that generalizes the White (1980) Heteroskedasticity Consistent Covariance (HCC) matrix is proposed to capture regressor-conditional heteroskedasticity. It is found to have less finite-sample bias and better coverage than the NW-HAC estimator in simulated regressions.The Moment Ratio estimator is applied to a quadratic trend model of US real GDP. OLS, NW-HAC, and even AR(4) standard errors are found to greatly overstate the precision of the coefficients, and all give the misleading impressions that the terminal output gap of −4.80% in 2014Q2 is significantly negative, and that the rate of change of GDP growth is significantly negative. The Moment Ratio persistence coefficient estimate is quite close to unity, and a unit root in the errors cannot be rejected. Using the preferred “blended” covariance matrix, the “output gap” has infinite variance and therefore is illusory, and the estimated secular rate of decline of GDP growth of −2.82% per annum per century is statistically insignificant. Nevertheless, expected GDP growth in 2014Q2 of +2.30% per annum remains strongly significant and positive despite the absence of cointegration.The Moment Ratio estimator is particularly simple and intuitive in the case of AR(1) errors. Accordingly, Section  2 first develops the “MR(1)” model, i.e. the MR estimator in the case of AR(1) errors, and compares the derived standard errors to conventional alternatives. Section  3 then develops the MR(p) estimator, which extends the MR(1) estimator to a more general AR(p) error process. Section  4 applies the MR estimator to a quadratic trend model of US real GDP. Section  5 concludes. Several figures in the submitted paper were omitted here for reasons of space but are included in a Supplementary Information file provided online with this article (see Appendix A). These figures are referred to here as SI-1, etc.Section  2.1 reviews basic formulas and establishes notation for a linear regression whose errors follow a general autocorrelated process. Section  2.2 develops the MR(1) estimator, i.e. the Moment Ratio estimator when the errors follow a stationary AR(1) process. The unit root case requires reposing the problem somewhat, and is treated in Section  2.3. Section  2.4 discusses the inherent weaknesses of the Newey–West (1987) HAC covariance matrix. Section  2.5 investigates the Monte Carlo properties of the MR(1) estimator, while Section  2.6 develops a unit root test for the AR(1) case.Consider a time-series linear regression of the form(1)y=Xβ+ε,whereXis ann×kmatrix of exogenous regressors whose first column is a vector of units so thatβ1is the intercept. We assume that then×1error vectorεhas mean0, is independent ofX, and, if stationary, has a time-invariant autocovariation structure,Γ=E(εε′)=(γ|i−j|).The OLS estimator ofβ,βˆ=(X′X)−1X′y=β+(X′X)−1X′ε,then has covariance matrix(2)C=Cov(βˆ)=E((X′X)−1X′εε′X(X′X)−1)=(X′X)−1X′ΓX(X′X)−1=γ0(X′X)−1X′RX(X′X)−1,whereRis the population autocorrelation matrix,(3)R=(ρ|i−j|)=Γ/γ0.(See, e.g., Greene, 2003, 193.)The vector of observed OLS residuals equals the “annihilator matrix”Mtimes the vector of unobserved errors:(4)e=(I−X(X′X)−1X′)ε=Mε.Definesj=trj(ee′),j=0,…,n−1,where we define thej-th order trace operatortrj()for ann×nmatrixA=(ai,j)bytrj(A)=∑i=1n−jai,i+j.The residual autocorrelations are then customarily computed from the residuals as(5)rj=sj/s0,j=0,…,n−1.In general,(6)Eee′=E(Mεε′M)=MΓM,so that(7)Esj=trj(MΓM)=γ0trj(MRM).Definition (5) follows Hayashi (2000, 408), Greene (2003, 268), and others, by taking the ratio of the sum ofn−jterms to that ofnterms, to obtain what might be called the weak autocorrelations. The strong autocorrelations, defined as the ratio of the average value ofetet−jto the average value ofej2, are larger by a factor ofn/(n−j). Percival (1993) notes that time series texts commonly refer to these as the “biased” and “unbiased” estimators, respectively, although both in fact can be biased. The Moment Ratio estimator developed in Sections  2 and 3 could as easily be based on the strong autocorrelations, with identical results, but this would require adding the additional factor to the formulas.Under the classic OLS assumptionΓ=γ0I, (2) becomes(8)COLS=γ0(X′X)−1.In this case,(9)s2=s0n−kis an unbiased estimator ofγ0. Furthermore,(10)CˆOLS=s2(X′X)−1is an unbiased estimator ofC. However, when, as is often the case, the errors and regressor(s) are both positively serially correlated,s2is no longer unbiased andCˆOLSwill underestimate the variances of theβˆj.IfΓ, or evenR, were known, Generalized Least Squares (GLS) would provide the efficient estimator ofβ, along with an unbiased estimate of its variance. Thus, for example, Canjels and Watson (1997) and Choi et al. (2008) recommend using Feasible GLS (FGLS), based on an estimate of the covariance structure. However, this covariance estimate must be based on the residuals of a preliminary OLS regression, and it is recommended that the Moment Ratio covariance matrix developed here be used for this purpose. This extension is not implemented in the present paper.In the simplest case of an AR(1) error structure,(11)εt=φεt−1+ut,where the innovationsutare i.i.d. with mean 0 and finite varianceσ2. Under (11), the population autocorrelations are(12)ρj=φj.The present subsection considers the stationary case|φ|<1, while Section  2.3 treats the unit root caseφ=1.Fig. 1illustrates the autocovariance function for a simple trend line regression(13)yt=β1+β2t+εt,whose errors are an AR(1) process withφ=0.9and innovation varianceσ2=1. The top line shows the true autocovariancesγ0,…,γn−1. These decay geometrically fromγ0=σ2/(1−φ2)=5.26by factors ofφand are all positive. A pseudo-random draw from this process was constructed with Gaussian innovations, and was fit by OLS to a linear trend with an intercept. The irregular line plots the estimated autocovariances of the residuals,γˆj=sj/n.(These are “weak” autocovariances corresponding to the “weak” autocorrelations defined in Eq. (5).) These estimated autocovariances initially behave in a qualitatively similar fashion to the true autocovariances. However, they exhibit several typical problems when positive serial correlation is present in both the errors and the regressors: first,γˆ0typically understates the true unconditional varianceγ0. Second, they typically decay faster initially than the true rateφ. Third, as noted by Percival (1993),∑j=1n−1γˆj=−γˆ0/2whenever a constant term is included in the regression, so that the sample autocorrelations of order 1 and higher are necessarily negative on average even if the true autocorrelations are all positive. And fourth, when serial correlation is high they tend to exhibit the spurious oscillations that characterize trend line residuals with unit root or near unit root errors (Nelson and Kang, 1981). Because of these qualitatively misleading properties, the highest order sample autocorrelations should be ignored altogether.The line in Fig. 1 identified as AR(1) is the autocovariance function of an AR(1) process fit by directly usingγˆ0andγˆ1as estimates of their limits in probabilityγ0andγ1. This function qualitatively captures the geometric shape of the true autocovariances, while it ignores the misleading behavior of the estimated autocovariances at the long end. However, it starts with too low a value (γˆ0=3.77instead of 5.26), and then decays at too fast a rate (r1=0.83instead of 0.90). Hence it will tend to give too small a value to the estimated standard errors of the regression parameters when substituted into Eq. (2). (The line in Fig. 1 identified as NW-HAC(4) is discussed in Section  2.4.)Fig. 2illustrates(14)Es2/γ0=tr0(MRM)/(n−k),the expectation of the “unbiased” estimators2relative to the true varianceγ0of the regression errors, as a function of the true AR(1) parameterφ, again in the case of the simple trend line model with sample sizen=100. Forφ>0this ratio is less than unity, and hences2is downward biased, despite its “bias correction”. This bias depends on the observed regressor matrixXby way ofM, but is also a function of the in-practice-unknown parameterφvia the correlation matrixR. Asφ=1is approached, this bias actually becomes −100%, sinceγ0is infinite while the expectation ofs2is necessarily finite when an intercept is included in the regression.In order to correct these deficiencies in the AR(1) case, we define the Moment Ratio function forr1as the ratio of the population moments whose sample counterparts definer1per (5) and (7):(15)ψ(φ;X)=tr1(MRM)/tr0(MRM).The value ofψ()depends onφthrough the correlation matrixR, and also onXviaM. Fortunately, however, it does not depend on the unknown coefficient vectorβor the error varianceγ0. Fig. 3illustrates this value as a function ofφ, for the special case of a trend line regression withn=100. A 45° line representing the true value ofφis also plotted. The value forφ=1is obtained in Section  2.3. It may be seen thatr1already has a small downward bias (in the Moment Ratio sense) whenφ=0, and that this downward bias increases asφincreases to 1. The bias may also be computed forφ<0, but was found to vanish asφ↓−1. Hence, only the more commonly encountered caseφ≥0is illustrated.This bias inr1as an estimator ofφmay be eliminated, simply by numerically evaluating the inverse of the functionψ(φ;X), with respect to its first argument, at the empiricalr1, when possible. Ifr1exceedsψ(1;X)(0.91 in the example), we instead use the point of nearest approach, namelyφ=1. Formally, the MR(1) estimator is defined by(16)φˆMR=arg minφ∈(−1,1]|r1−ψ(φ;X)|.This inverse function and its extension are illustrated in Figure SI-1 in the online Supplementary Information.Andrews (1993) has noted that a pattern similar to that in Fig. 3 emerges when the median of the Monte Carlo distribution of the OLS estimator ofφis plotted againstφ. Andrews proposes that an Exactly Median Unbiased estimator ofφbe computed by numerically inverting this median function. In a precursor to the present paper (McCulloch, 2008), the author implements and extends Andrews’ method to find exactly median unbiased estimators of all the coefficients of an AR(p) model for regression errors. However, the Moment Ratio approach of the present paper gives very similar results, without the tedious and noisy Monte Carlo simulation at each step of the numerical search.OnceφˆMRhas been found, the correlation matrixRmay be estimated by(17)RˆMR=((φˆMR)|i−j|).A natural estimator ofγ0, that would be unbiased by (7) if it were computed with the true value ofφ, would be(18)s0/tr0(MRˆMRM).However, this does not lead to a consistent estimate of the variance of the innovations in the limit asφapproaches unity, as discussed in Section  2.3. This problem can nevertheless be avoided by instead basing the estimator ofγ0on the innovations:ut=εt−φεt−1,t=2,…,n,oru=Dφε,where the(n−1)×nquasi-differencing operatorDφ=(dij)is defined bydi,i+1=1,di,i=−φ, anddi,j=0otherwise. The consistently estimated lastn−1innovations are thenuˆ=Dφe=DφMε,so that(19)Euˆuˆ′=DφMΓMDφ′=σ2DφMGMDφ′,whereG=(g|i−j|)=Γ/σ2=g0Ris the covariance matrix of the errors, normalized to unit variance for the innovations. In the AR(1) case,g0=1/(1−φ2). ThenEuˆ′uˆ=σ2tr0(DφMGMDφ′),so thatσˆˆMR2=uˆ′uˆ/tr0(DφMGMDφ′)would be an unbiased and consistent estimator of the innovation variance ifφwere known. Whenφis estimated consistently byφˆMRandGbyGˆMR=RˆMR/(1−(φˆMR)2),(20)σˆMR2=uˆ′uˆ/tr0(DφˆMRMGˆMRMDφˆMR′)becomes a consistent and at least approximately unbiased estimator. The Moment Ratio estimator of the covariance matrixCofβˆOLSas given in (2) is then(21)CˆMR=σˆMR2(X′X)−1X′GˆMRX(X′X)−1.If desired, the variance of the AR(1) errors may then be estimated by(22)γˆ0MR=σˆMR2/(1−(φˆMR)2).The unit root caseφ=1poses no insurmountable problems, so long as there is a constant term and a trend or trending variable(s) in the original regression (1), and so long as the regression residuals are not used to estimate the variance of the innovations. In particular, a unit root in the errors does not in itself indicate a “spurious regression” in which OLS coefficient slope estimates become meaningless. However, it does require reposing the problem, so as to replace certain undefined mathematical expressions with their limiting values. We assume thatφ∈(−1,1], so that a unit root is the only type of nonstationarity that will be encountered.As is well known, OLS coefficient estimates are inconsistent when both the regressor(s) and the errors contain a unit root with no drift (see e.g. Choi et al., 2008, 330). Conditional on the observed regressor(s) and the finite innovation variance, the limiting coefficient error is Gaussian with positive variance. Even so, there may be a problem of estimating the innovation variance from even the estimated innovations if the residuals themselves are not consistently estimated. Nevertheless, when the regression includes a constant and a time trend, the slope coefficient becomes√nconsistent: Hayashi (2000, 570) shows that after scaling the horizontal axis bynand the vertical axis by√n, the slope coefficient is Gaussian about its true value with a finite variance. Without the scaling, the slope coefficient is therefore Gaussian with standard error proportional to1/√n. Likewise, if the regressor is unit root with non-zero drift, the drift will dominate the unit root noise for largen, and so the regressor will act as if it were a time trend. This section is therefore limited to the case in which the regressors include either a time trend or a trending variable.Asφ↑1, each element of the unconditional covariance matrixΓbecomes infinite, holding the varianceσ2of the innovations in (11) constant. Furthermore, each element of the correlation matrixRbecomes unity in this limit. These matrices are therefore no longer useful or informative in this case, and the problem must be reposed without them. Although a random walk has infinite unconditional variance and covariances, its variances and covariances are all finite conditional on its value at any point in time, sayt=0. If the errors are a random walk that arise by accumulating white noiseu, we haveε=ε0+Nu,where(23)N=(1(i≤j))is then×nintegrator matrix, and the logical indicator1(i≤j)takes the value 1 when the expression is true and 0 when it is false. The conditional covariance matrix of the errors, taken conditional onε0, is thenΓ0=E(εε′∣ε0)=σ2W0,whereW0=NN′=(min(i,j)).Furthermore, so long as there is a constant term in the regression, the OLS residuals will sum to zero regardless of the value of the random walk att=0, so that the actual value ofε0does not matter for their properties. Eq. (6) then becomesEee′=E(ee′∣ε0)=MΓ0M=σ2MW0M,so that(24)Esj=σ2trj(MW0M).The limiting value ofψ(φ;X), as plotted in Fig. 2, then becomes(25)ψ(1;X)=trj(MW0M)/tr0(MW0M).Due to sampling error, the actual value ofr1could lie above or below this value. Any value ofr1aboveψ(1;X)should simply be identified withφˆMR=1as in (16).The innovation varianceσ2could be estimated without bias, using (24), bys0/tr0(MW0M).Unfortunately, however, this unbiased estimator, which is the limiting value of (19) taken together with (22), is not consistent, even in a trend line regression (see, e.g. Hayashi, 2000, 570–71). Simulations with Gaussian errors indicate that for a model in which only a constant term is estimated, its distribution is approximately a scaledχ2with 2.5 degrees of freedom, using eithern=100orn=1000. In a linear trend line model, it is approximately scaledχ2with 5 degrees of freedom, using eithern=100orn=1000. However, using the estimated innovations (which are in fact simply the first-differenced residuals in the random walk case being discussed here), Eq. (19) becomesEuu′=E(uu′∣ε0)=σ2D1MW0MD1′,whenceEu′u=E(u′u∣ε0)=σ2tr0(D1MW0MD1′),so that(26)σˆMR2=u′u/tr0(D1MW0MD1′)is an unbiased estimator when it is known that there is a unit root. Furthermore, simulations with Gaussian errors indicate that the distribution of this estimator is approximately scaledχ2withn−kdegrees of freedom, for both the mean and trend line models, and for bothn=100and 1000. Inference on the coefficients withtorFstatistics based on (26) and (27) should therefore be at least approximately valid.Having computedσˆMR2, we may estimate the covariance of the regression coefficients, conditional on the arbitrarily chosen reference pointε0, by(27)Cˆ0MR=σˆMR2(X′X)−1X′W0X(X′X)−1.If a reference point other thanε0, sayεt, is chosen, the conditional covariance of the errors becomesΓt=E(εε′∣εt)=σ2Wt,whereWt=(wi,jt)withwi,jt={min(|i−t|,|j−t|),(i>tandj>t)or(i<tandj<t),0,else .SinceMWtMdoes not depend on the choice oft, exactly the same values ofψ(1;X)andσˆMR2will be obtained. Furthermore, except for its first row and column,CˆtMR=σˆMR2(X′X)−1X′WtX(X′X)−1does not depend on the choice oft. The estimated standard error of the constant termβ1therefore does depend on the reference point defined by the choice oft, while those of the slope coefficientsβ2,…,βkdo not. In particular, the regressionFstatistic for the joint hypothesisβ2=0,…βk=0is invariant to the arbitrarily chosen reference point, and is meaningful despite the absence of cointegration among the regressors.If the one-step-ahead forecast ofyn+1(conditional on the timet=n+1values of the regressors) is of particular interest, the reference pointt=n+1may be useful. In this case,Wn+1=(min(n+1−i,n+1−j)).Another particularly natural reference point, one that does not single out any individual point in the sample period, is the mean errorε¯=ι′ε/n,whereι=(1,…,1)′. Conditioning on this value, we haveCˆε¯MR=σˆMR2(X′X)−1X′Wε¯X(X′X)−1,whereWε¯=ZW0Zand(28)Z=In−ιι′/n.The unit root covariance matrix could also be conditioned onεtfor anytoutside[0,n+1], but this complicates the formula somewhat, and so is not treated here.The unconditional unit root covariance matrixCˆURmay be obtained fromCˆ0MRor any of the other conditional unit root covariance matrices developed above, simply by replacing its (1, 1) element by∞. With no loss of generality, the remaining elements of its first row and column may be set to 0, since the variance of any linear combination of the intercept and the regressors is infinite regardless of these covariances. The inverse of the unconditional unit root covariance matrix will be of use below when we compute a blended covariance matrix in the near-unit-root cases. It may be seen from the partitioned matrix inversion rule that the inverse ofCˆURis block-diagonal, with 0 in the (1, 1) position, regardless of the covariances in the first row and column, so that again there is no loss of generality in setting them to 0.CˆURmay be similarly constructed fromCˆ0MRin the MR(p) case considered in Section  3.The truncated-kernel Heteroskedasticity and Autocorrelation Consistent (HAC) covariance matrix, introduced by Newey and West (1987), is now widely used by economists to “correct” the standard errors of OLS time series coefficients for serial correlation. Greene (2003, 201) reports that its use is now “standard in the econometrics literature”. Hayashi (2000, 409–12) mentions only it, the similar Quadratic Spectral HAC of Andrews and Monahan (1992), and the Vector Autoregression HAC (VAR-HAC) method of den Haan and Levin (1997) as appropriate methods for correcting OLS standard errors for serial correlation. The elementary text by Stock and Watson (2007) presents HAC as the only method worthy of mention.In the OLS case considered here, the Newey–West HAC estimator ofCsimplifies to(29)CˆHAC=(X′X)−1X′FX(X′X)−1,where(30)F=(eiejK(|i−j|))and(31)K(l)=max((m+1−l)/(m+1),0)is the truncated Bartlett Kernel function at laglfor some bandwidthm(Greene, 2003, 200). Most econometric packages provide “automatic bandwidth selection” for HAC, using a formula such as the following:(32)m=⌊4(n/100)2/9⌋,which just yieldsm=4forn=100. The notation⌊x⌋indicates the floor function ofx. Eq. (32) is used by Eviews, following a suggestion of Newey and West (1994). Stock and Watson (2007, 607) suggestm=⌊0.75n1/3−0.5⌋, which has very similar effect fornin the range 50–2000, and generally identical effect in the range 400–1000. Sun et al. (2008) note that while a power of 1/3 or 1/5 minimizes the asymptotic mean squared error of the long-run variance estimator for first and second order kernels, respectively, a power of 1/2 or 1/3 asymptotically minimizes a weighted average of Type I and Type II errors.At least three types of heteroskedasticity are commonly encountered in econometrics: in a priori heteroskedasticity, the variance of the errors is known up to a common constant, and Weighted Least Squares (WLS) provides efficient coefficient estimates and correct standard errors. In serially conditional heteroskedasticity, episodes of high and low variance errors are clustered, in which case a GARCH model is often appropriate (McCulloch, 1985; Bollerslev, 1986). And finally, in what might be called Regressor Conditional Heteroskedasticity (RCH), the variance of the errors may depend in an unknown way on the equation’s regressors. The White (1980) HCC and Newey–West HAC estimators correct consistently for RCH, though not necessarily for the other two forms of heteroskedasticity unless they coincidentally coincide.In the benchmark case of regressor-conditional homoskedasticity, the expectation of the HAC estimator becomes(33)ECˆHAC=(X′X)−1X′HHACX(X′X)−1,whereHHAC=(hi,jK(|i−j|)),andH=(hi,j)=MΓM.HAC thus effectively employs only the firstmsample autocovariances, and replaces the others with zeros. At the same time it down-weights the autocovariances it does not discard entirely by the factor(m+1−l)/(m+1). It also uses the regression residuals as if they were the errors themselves. For all three of these reasons, it tends in finite samples to underestimate the standard error for a regressor which is itself serially correlated. However, the amount by which it does this depends on both theρjand the degree of serial correlation of the regressors themselves.Aside from its RCH adjustment, the NW-HAC estimator in effect therefore replaces the already twice-downward-biasedγˆjOLSwith the triply-biasedγˆjOLSK(j), as depicted by the dash–dot line in Fig. 1. Although NW provides some improvement over the OLS standard errors, the already seriously deficient AR(1) standard errors are far superior. As the sample sizenrises without bound, the bandwidth determined by (32) also rises without bound, so that eventually every lag is included andK(l)becomes arbitrarily close to unity for eachl. Furthermore, as long asn1/2/malso rises to infinity, each autocorrelation is consistently estimated (Newey and West, 1994, 633). However, it is shown in Section  2.5 that in finite samples the estimator can be highly misleading.Politis (2011) finds that a flat-top kernel and a data-driven bandwidth somewhat alleviate the downward bias in the Newey–West estimator. In order to ensure that the resulting covariance matrix is positive semidefinite, he must rectify it by zeroing out any negative eigenvalues in its singular value decomposition. Nevertheless, he also uses residuals as if they were errors, with no compensation for their biased variance.Fig. 4shows the Moment Ratio Functionψ(φ,X), along with the Monte Carlo mean, median, quartiles, and 5th and 95th percentiles of the distribution ofr1, as a function of the true value ofφ, again for the trend line model withn=100, under the additional assumption of Gaussian errors. The median line is essentially the same as the median function inverted by Andrews (1993) to determine his Median Unbiased estimator of the autoregressive parameterφ. The Moment Ratio Function is almost indistinguishable from the median for values ofφnear 0, but rises slightly above the median asφincreases. Nevertheless, it remains well within the quartiles, and at least approximately corrects the median bias ofr1when inverted. The downward skewness of the distribution ofr1pulls the mean of the distribution down below the median by a comparable amount. This simulation was performed withm=99,999replications using MATLAB’s randn(‘state’) Gaussian random number generator, forφin steps of 0.01. The same seed was used for each value ofφin order to make each simulated percentile a smooth function ofφ. Because the Moment Ratio function lies above the median in Fig. 5forφ>0, and both are increasing functions ofφ,φˆMRis necessarily less than the Median Unbiased estimator computed by inverting the Monte Carlo median function atr1, and therefore provides a less conservative adjustment for serial correlation, at least for the trend line problem simulated.Figs. 5 through 9are based on 10,000 Monte Carlo replications of a trend line regression withn=100equally spaced and demeaned values of the time trend variable, Gaussian AR(1) errors, and various true values ofφranging from −0.2 to 1.0, inclusive. The “AR(1)” model is fit to the (weak) sample autocorrelationr1, in conjunction with an estimate ofγ0that is unbiased conditional onφ=r1per (7). The “HAC” results are based on a bandwidth ofm=4, using formula (32). Fig. 5 shows the Monte Carlo median of the estimated variance of the slope coefficient, relative to the true variance of the coefficient, so that a value of unity reflects the target of zero median bias. As expected, OLS is right on the mark when the errors are uncorrelated, but then deteriorates rapidly as the degree of serial correlation increases. With random walk errors, the true variance of the slope coefficient still finite, but is 180 times larger than the OLS estimate, because OLS takes no account at all of serial correlation.NW HAC provides some improvement over OLS forφgreater than approximately 0.13. However, it already has substantial negative median bias (median/true = 0.90) even when the errors are white noise. This is because HAC takes no account of the distinction, important in finite samples, between the residuals and the errors. However, the AR(1) model that fitsr1directly to its asymptotic counterpart is a big improvement over HAC for all values ofφ. Finally, the Moment Ratio estimator MR(1) is clearly the leader of the pack by this metric, except for OLS whenφis very near 0. It easily dominates AR(1) and therefore also HAC for all values ofφ. Because of the upper bound of unity imposed onφˆMR, even MR(1) is inevitably somewhat biased downwards, particularly asφapproaches 1.Fig. 6 gives the Monte Carlo means of the squared standard errors, relative to the true variance of the slope coefficient, rather than the medians as in Fig. 5. OLS, HAC and AR(1) all have mean biases qualitatively similar to the median biases shown in Fig. 5. However, MR(1) now has a positive mean bias for all but the highest values ofφ. Forφ<0.6, this bias is quite small, but for larger values it grows quickly, reaching a peak of +95% atφ=0.90(corresponding to a RMSE of +40% for the standard error itself). This mean bias is due to the fact that the expected squared standard error is a highly convex function ofφ, especially asφ=1is approached, so that overestimates ofφhave a much bigger impact on the mean than do underestimates. The medians in Fig. 5 are less affected by this factor, resulting in negative median biases throughout. Forφ>0.95, the upper boundφˆMR≤1becomes binding more frequently, with the result that under-estimates start to dominate, and the bias turns negative after 0.98. Atφ=1, the mean variance bias is −49%, corresponding to −30% in terms of the RMS standard error.For most practical purposes, the ultimately relevant criterion is not the mean or median bias of the estimated variances, but rather their ability to construct confidence intervals, and therefore hypothesis tests, with correct coverage. Fig. 7 shows the simulated coverage of a 95% confidence interval for the slope in the same trend line regression withn=100and Gaussian innovations. The horizontal line at 95% is the target representing perfect coverage. Since 10,000 replications were again used, the sampling standard error of the coverage of a 95% confidence interval is 0.22%. The intercept and slope coefficients were, without loss of generality, both set to 0 for this simulation.Despite its upward mean bias for the slope variance, the MR(1) size distortion is always negative and becomes increasingly negative throughout the range[−0.2,1]. Although it does not have perfect coverage, the MR(1) estimator always has substantially better performance than HAC or AR(1), and dominates OLS for allφ>0.14, by this metric. Figure SI-2 in the online Supplementary Information continues Fig. 7 down to 0% coverage, and shows that the relative position of the four lines continues until they reach theφ=1limit. The pattern for the estimated intercept is qualitatively similar to that shown for the estimated slope in Figs. 5–7, and hence is not shown here.The Monte Carlo distribution ofr1forφ=1, summarized at the right edge of Fig. 4, provides a simple test for a unit root, under the assumption of Gaussian errors, that is exact to within Monte Carlo sampling error: letpURbe the percentile of this distribution that corresponds to the observed value ofr1. IfpURis less than say 0.05, then a unit root can be rejected with a 5% one-tailed test size. Thus, any value ofr1less 0.778 will reject a unit root at the 5% level for the illustrated trend line regression withn=100. The only practical way to perform this test is with a Monte Carlo simulation comparable to that required for Andrews’ (1993) Median Unbiased estimator, but since the test is optional, and even then the simulation only needs to be performed under the nullφ=1, it is not nearly as computationally demanding as the Median Unbiased estimator.Although an AR(1) model is often a good first approximation to the autocovariation function of econometric time series regression errors, this model can be unnecessarily restrictive. The true model may not even be a finite order autoregressive process. However, a finite order AR(p) model(34)εt=∑j=1pφjεt−j+utcan approximate most covariance-stationary Gaussian processes to any desired precision, given a sufficiently high value ofp. (Long-memory fractionally integrated processes of the type proposed by Hosking (1984) may require special consideration. These interesting processes go beyond the scope of the present paper.)If an AR(p) process is covariance stationary, the Yule–Walker equationsγ0=∑j=1p∑k=1pφjφkγ|j−k|+σ2,γj=∑k=1pφkγ|j−k|,j=1,…,pmay be solved for the firstp+1autocovariancesγ0,…,γpin terms of the AR(p) coefficientsφjand the innovation varianceσ2. The remaining autocovariances may then be filled in recursively withγj=∑k=1pφkγj−k,j=p+1,…,n−1.Conversely, the AR(p) coefficients may be found in terms of the firstp+1autocovariances by means of(35)φ=(φ1⋮φp)=(γ0⋯γp−1⋮⋮γp−1⋯γ0)−1(γ1⋮γp).In what may be called the Asymptotic Method of Moments (AMM) estimator of the AR coefficients, the empirical residual weak autocovariancessj/nare directly used to estimate their limits in probabilityγjin (35):(36)φˆAMM=(s0⋯sp−1⋮⋱⋮sp−1⋯s0)−1(s1⋮sp).Although this simple estimator is consistent, it takes no account of the distinction between the errors and residuals and so is biased in finite samples, particularly as the process becomes more persistent and approaches a unit root. The AMM estimator defined in (36) is virtually equivalent to the results of an OLS regression of the residuals on their ownplags, with the exceptions that it treats the first and lastpresiduals symmetrically, and that it employs weak rather than strong autocorrelations.Generalizing (15), we define the Moment Ratio Function of orderpto be the matrix “ratio” of the exact finite sample population moments corresponding to the sample moments used to computeφˆAMM:(37)ψ(φ;X)=(Es0⋯Esp−1⋮⋱⋮Esp−1⋯Es0)−1(Es1⋮Esp)=(tr0(MGM)⋯trp−1(MGM)⋮⋱⋮trp−1(MGM)⋯tr0(MGM))−1(tr1(MGM)⋮trp(MGM)).The scaled autocovariance matrixG=Γ/σ2depends on the true autoregressive coefficientsφbut not onσ2. This Moment Ratio Function depends on the autoregressive coefficients throughGas well as the regressor matrixXthroughM, but fortunately the unknown innovation variance drops out. When the stationarity restrictionα<1is not binding, we define the Moment Ratio estimatorφˆMRsimply by settingψ(φˆMR;X)=φˆAMMand solving this system ofpnonlinear equations inpunknowns. The Moment Ratio estimator may therefore be thought of as the exact finite sample Method of Moments estimator, rather than a merely asymptotic Method of Moments estimator.As in the AR(1) model, the unit root case is not an insurmountable obstacle. However, it does require separate treatment. Following Andrews and Chen (1994), it is useful to rewrite (34) in the equivalent Augmented Dickey–Fuller (ADF) formεt=αεt−1+∑j=1p−1θj(εt−j−εt−j−1)+ut,where(38)α=∑j=1pφj,θj=−∑h=j+1pφh,j=1,…,p−1.We will call the parameterαthe persistence of the process. A necessary condition for stationarity isα=1. Althoughα<1is not a sufficient condition for stationarity, the caseα=1is the most common form of nonstationarity for the errors in econometric time series regressions. Hence, we will assume thatα=1is the only type of nonstationarity that may be present, and that when such a unit root is present, theθjdefine a stationary process for the first differences. In other words, we assume that the process is either stationary or integrated of order 1. More formally, we assume that the vectorφ=(φ1,…,φp)′of AR(p) coefficients is an element of the setΦfor which either all the reciprocal roots of the characteristic polynomial are inside the unit circle, or for which only one root is on the unit circle, that root being 1 and all the other reciprocal roots lying inside the unit circle.In the unit root caseα=1, the Yule–Walker equations readily determine then×nnormalized covariance matrixHof the error first differencesξt=εt−εt−1(39)H=Cov(ξ)/σ2,in terms of theθj, since we have assumed that at least the first differences are covariance stationary. Conditional on sayε0, the errors themselves then have finite covariance matrix,Cov(ε∣ε0)=σ2NHN′,whereNis the integrator matrix (23). Since the regression is assumed to include an intercept, the residuals sum to 0 regardless ofε0, and so have finite conditional as well as unconditional covariance, determined byCov(e∣ε0)=Cov(e)=σ2MNHN′M.In the caseα=1, we therefore define the Moment Ratio Function by(40)ψ(φ;X)=(tr0(MNHN′M)⋯trp−1(MNHN′M)⋮⋱⋮trp−1(MNHN′M)⋯tr0(MNHN′M))−1(tr1(MNHN′M)⋮trp(MNHN′M)).Generalizing (16) to the AR(p) case, we may now define the Moment Ratio EstimatorφˆMRin general as the point of nearest approach of the Asymptotic Method of Moments EstimatorφˆAMMto the Moment Ratio Function, in terms of the Euclidean norm:(41)φˆMR=arg minφ∈Φ‖φˆAMM−ψ(φ;X)‖.This problem was solved numerically for this paper with satisfactory results using a Nelder–Mead simplex algorithm (Press et al., 1992, 402–6), modified to be subject to the explicit constraintα≤1, and assigning a very high value to the objective function whenever other non-stationary roots were encountered. (An earlier version of this paper instead attempted to equate the firstpsample autocorrelations directly to individual moment ratio functions, with very ill-conditioned results in the vicinity of a unit root. Transforming the AR coefficients to the ADF persistence form (38) before taking the norm in (41) will give the same answer if the unit root restriction is not binding, but a somewhat different answer when it is binding. This distinction may be worth exploring in future work.)The lastn−pinnovations of an AR(p) process may be computed from the errors byu=Dφε,where the(n−p)×ninnovation-generating operatorDφ=(dij)is defined bydi,i+p=1,di,i+p−h=−φhanddi,j=0otherwise. The innovations estimated from the residuals are then(42)uˆ=Dφe=DφMε,so that in the stationary caseα<1,Euˆuˆ′=DφMΓMDφ′=σ2DφMGMDφ′,Euˆ′uˆ=σ2tr0(DφMGMDφ′),whenceσˆˆMR2=uˆ′uˆ/tr0(DφMGMDφ′)would be an unbiased and consistent estimator of the innovation variance, if the true value ofφwere known. Whenφis estimated consistently byφˆMR,σˆMR2=uˆ′uˆ/tr0(DφˆMRMGˆMRMDφˆMR′)becomes a consistent and at least approximately unbiased estimator. The Moment Ratio estimator of the covariance matrixCofβˆOLSas given in (2) is thenCˆMR=σˆMR2(X′X)−1X′GˆMRX(X′X)−1.(In order to treat the first and last residuals symmetrically in the simulations that follow, the innovations were in fact computed both forward and backward in time, and the resulting sums of squared innovations averaged.)When a unit root is present, the estimated innovations instead have finite covarianceCov(uˆ)=σ2DφMNHN′MDφ′,whenceσˆˆMR2=uˆ′uˆ/tr0(DφMNHN′MDφ′)is an unbiased and consistent estimate of the innovation variance when the autoregressive order and parameters are known and have unitary first order persistence. When the autoregressive parameters are consistently estimated and the estimate happens to have a unit root,σˆMR2=uˆ′uˆ/tr0(DφˆMRMNHˆMRN′MDφˆMR′)is a consistent and approximately unbiased estimator of the innovation variance. The Moment Ratio estimator of the covariance matrixCofβˆOLSas given in (2) and conditional onε0is thenCˆ0MR=σˆMR2(X′X)−1X′MNHˆMRN′MX(X′X)−1.Conditional onεt, similar reasoning yieldsCˆtMR=σˆMR2(X′X)−1X′MNtHˆMRNt′MX(X′X)−1.The autocovariation function and therefore the coefficient covariance matrix may now be estimated consistently for almost any covariance stationary or I(1) error process by setting the autoregressive orderpequal to(32′)p=⌊4(n/100)2/9⌋,the maximum lag considered by the formula (32) commonly used for the Newey–West HAC estimator. Asnincreases,pas well asn1/2/pincrease without bound, so that eventually every autocorrelation is utilized, and estimated consistently. For sample size 100, this formula givesp=4. No claim is made that (32′) is optimal, but merely that it is adequate to consistently capture the autocorrelation structure.Figures SI-3 and SI-4 in the Supplementary Information plot the Monte Carlo median and mean of the estimated slope variances using the OLS, HAC (m=4), AR(p=4) estimated by AMM, and MR(p=4) covariance matrices. Throughout the true DGP is AR(1), again using Gaussian errors and 10,000 replications. Despite the nuisance parameters that have been added for consistency in case the process is not AR(1), the results are qualitatively very similar to those in Figs. 5 and 6, where the true AR(1) structure is imposed. The figures are therefore omitted here for reasons of space. Although MR(p) and AR(p) have a greater downward median bias than MR(1) and AR(1), respectively, MR(p) again clearly dominates both AR(p) and HAC. AR(p) continues to clearly dominate HAC, except forφ<0.02. As is the case with MR(1) in Fig. 6, MR(p) does have a positive mean bias, which peaks atφ=0.9at +100%, but then reverses sign aboveφ=0.98.Figure SI-5 plots the 95% confidence interval coverages using the MR(p) estimator of the slope standard error, versus OLS, HAC and AR(p). The results are qualitatively very similar to those in Fig. 7 and therefore are omitted here for reasons of space. Despite the pronounced positive mean bias of the MR(p) estimator, its coverage is again under 95% throughout. Nevertheless, MR(p) dominates all the competitors in terms of coverage bias, except of course for OLS whenφis very close to 0. Coverages for MR(p) are included in Figs. 9, but without the comparison to HAC or AR(p). The greater generality of the MR(p) and AR(p) estimators over MR(1) and AR(1) does come at the cost of somewhat greater bias and reduced coverage, because they are not restricted to the true (but in practice unknown) AR(1) DGP. However, graphical comparisons (omitted here in the interest of space) show that this cost is small in comparison to the superiority of MR(p) over AR(p) and of AR(p) over HAC.Although the MR(p) estimator greatly improves the coverage of 95% confidence intervals based on the standard Studenttcritical values withn−kdegrees of freedom, it still has a systematic downward size distortion because estimation of the AR(p) parameters introduces an additional element of uncertainty that is not taken into account by this distribution. Bartlett (1935) has suggested instead using a critical value based on thetdistribution with a reduced “effective degrees of freedom” (EDOF), as implied by the increase in variance caused by the serial correlation adjustment. For coefficientβˆj, this would beEDOFj=(n−k)cˆj,jOLS/cˆj,jMR, and similarly for linear combinations of coefficients and for the denominator DOF inF-tests of joint hypotheses. This refinement would be worth investigating in future research but is not implemented in the present paper.The MR(1) unit root test discussed in Section  2.6 may easily be extended to the MR(p) case as follows: 1. EstimateφˆAMMvia (36). Designate the sum of these coefficients byαˆAMM, which will be our test statistic. 2. Then, compute Moment Ratio estimates of the autoregressive coefficients using (41), but with the restriction that their sumαˆURbe unity. 3. Convert the resulting restricted standard coefficientsφˆURto ADF form (αˆUR,θˆUR) using (38). 4. Generate the desired number of random samples with covariance determined byθˆUR. 5. Sum these to obtain unit root regression errors, and re-estimate the equation with the constructed unit root errors. 6. With synthetic data seti, re-estimateφˆiAMMand thenceαˆiAMMvia (38). The probability of obtaining a persistence as low asαˆAMMunder the null of a unit root (and withθ=θˆUR) is then estimated as the proportionpURof theαˆiAMMthat are less thanαˆAMM. Since the simulations do not require solving (41) on each replication, they run quite fast: 10,000 replications take less than 1.2 s in MATLAB on a laptop with a 2.50 GHz Intel®  Core™ i5-2520M CPU for the case illustrated.In the MR(1) case of Section  2.6, the unit root test size is correct by construction, to within Monte Carlo sampling error. In the MR(p) case, however, the test depends on usingθˆURas a proxy forθ, and therefore may have some size distortion. This was investigated by constructing 10,000 trend line regressions with random walk errors and conducting the unit root test described above with 10,000 replications each. The size distortion is generally moderate, though there is some positive distortion for nominal size in the range 0–0.18. Table 1tabulates selected values. For example, if the nominal size is 5%, the test in fact falsely rejects a unit root 6.0% of the time. In order to obtain a true type I error of 5%, a nominal size of 4.1% would have to be used. At a 5% test size, the Monte Carlo sampling standard error is approximately 0.2% on bothpURand the estimated actual size. Figure SI-7 (omitted here for reasons of space) plots the actual size versus the nominal size in greater detail.Ever since the seminal work of Nelson and Plosser (1982), econometric protocol has typically been to test for a unit root and then to either ignore the possibility of a unit root if it can be rejected at some convention test size, say 5%, or else to impose a unit root (typically by re-estimating the equation in first differences) if it cannot be rejected, even when the point estimate falls short of a unit root. However, this practice unfortunately produces results that can change discontinuously and substantially with the data. Fig. 8 shows the coverage of a notional 95% confidence interval for the slope coefficient in our illustrative trend line regression, at the top (dashed line) when a unit root is imposed and at the bottom (thin solid line) when the MR(p) standard errors are always used, as a function of the true AR(1) parameterφof the error DGP, along with a horizontal line indicating the 95% ideal value. Always imposing a unit root leads to a “95% CI” that includes the true value in 10,000 cases out of 10,000, unlessφis 0.95 or higher, and therefore is ordinarily far too conservative a strategy. On the other hand, the pure MR(p) estimator, which only uses the unit root formula when the point estimate of the persistence is actually unity, can yield far too small a confidence interval.This discontinuous behavior can easily be corrected, however, simply by blending the unit root and non-unit root covariance matrices using a continuous function of the unit root test statisticpUR. It would be inappropriate to use a weighted average of the covariance matrices themselves directly, since the unconditional unit root covariance matrix indicates infinite variance for the intercept. However, the unconditional unit root precision matrix (inverse covariance matrix) merely has a zero in this position, which will not dominate a linear combination unless the unit root case is given a 100% weight. Therefore, for a weightwUR<1on the unit root precision matrix, our proposed blended covariance matrix takes the form:(43)CˆBlend=(wURCˆUR−1+(1−wUR)CˆMR−1)−1,whereCˆURis the unconditional unit root covariance matrix, constructed fromCˆ0MRas discussed at the end of Section  2.3. IfwUR=1, simply setCˆBlend=CˆURinstead.(MATLAB will appropriately invertCˆURin (43) despite its infinite (1, 1) entry, but will generate a non-fatal error message. This distracting message can be avoided by replacing the∞with 1, inverting the resulting nonsingular matrix, and then replacing its (1, 1) entry with 0.)The central heavy solid line in Fig. 8 shows the coverage usingCˆBlend, withwUR=min(1,10pUR).This formula effectively embraces the unit root estimate if a unit root cannot be rejected at the 10% level, but does not completely ignore it unless it can be absolutely rejected withpUR=0. It may be seen that this choice ofwURprovides a very good correction of the coverage for all values ofφ, and even slightly over-corrects forφbetween 0.9 and 0.96. Replacing the 10 in the formula with 20 (corresponding to embracing the unit root if it cannot be rejected at the 5% level) was found to over-correct excessively, while replacing it with 5 (corresponding to a 20% test size) was found to under-correct consistently. Experiments withn=30, performed at the request of a referee, suggest that the optimal choice ofwURshould increase with the sample size, but the present paper makes no attempt to implement this refinement. Bunzel and Vogelsang (2005) similarly transition smoothly from I(0) to I(1) covariance estimators using a continuous function of a unit root test statistic, while Harvey et al. (2007) interpolate I(0) and I(1)t-statistics using a continuous function of both an ADF unit root test and a KPSS stationarity test.Although the primary emphasis of the present paper has been the case of regressor-conditional homoskedasticity, a Heteroskedasticity and Autocorrelation Consistent MR estimator (MR-HAC) may be constructed as a generalization of a slightly improved version of the White (1980) HCC estimator: the original HCC estimator for the simple case of OLS regression isCˆHCC=(X′X)−1X′FHCCX(X′X)−1,whereFHCC=(fijHCC),withfiiHCC=ei2;fijHCC=0,i≠j.This estimator has a downward finite sample bias because the regression residuals generally have smaller variance than the errors themselves. Furthermore, when non-trivial regressors are present, the residuals will be heteroskedastic even when the errors are homoskedastic. In the benchmark case of regressor-conditional homoskedasticity and in the absence of serial correlation, (6) becomesEee′=E(Mεε′M)=σ2M=σ2(mij),rather thanσ2I. However, both the bias and the induced heteroskedasticity may easily be removed by defining the Unbiased HCC estimator (HCC-U) as(44)CˆHCC-U=(X′X)−1X′FHCC-UX(X′X)−1,whereFHCC-U=(fijHCC-U),withfiiHCC-U=ei2/mii;fijHCC-U=0,i≠j.When serial correlation is present, our strategy will be to first approximate the serial correlation with a homoskedastic AR(p) process and to construct its MR estimator as above, and then to construct an unbiased and heteroskedasticity-consistent estimator in terms of the estimated innovations to the approximating process rather than the residuals themselves.When the approximating homoskedastic AR(p) process is stationary, letQbe the lower-triangular Cholesky decomposition of its scaled error autocovariance matrixG:G=Γ/σ2=QQ′.Then then×1extended innovation vectoru=Q−1εhas covariance matrixσ2InandCov(βˆ)=E((X′X)−1X′εε′X(X′X)−1)=E((X′X)−1X′Quu′Q′X(X′X)−1).(This extended innovation vector containspmore terms up front than the truncated innovation vector estimated by (42).) A natural estimator ofuisuˆ=Q−1e=Q−1Mε,which has covariance matrixE(uu′)=Q−1MΓMQ′−1=σ2Q−1MGMQ′−1=σ2S=σ2(sij).DefineV=(vij),withvii=uˆi2/siiandvij=0,i≠j,SinceEuˆi2=σ2sii,EV=σ2I. ReplacingG,Q,SandVby their MR estimates, we defineCˆMR-HAC=(X′X)−1X′QMRVMRQMR′X(X′X)−1.In the absence of serial correlation, so thatG=Q=I, this MR-HAC estimator reduces to the HCC-U estimator (44).When the approximating homoskedastic AR(p) process has a unit root, letZbe the lower Cholesky decomposition of the scaled covariance matrixHof the error first differences as defined in (39):H=E(ξξ′)/σ2=ZZ′.Then then×1extended innovation vector is given byu=Z−1ξand may be estimated byuˆ=Z−1ξˆ=Z−1N−1e=Z−1N−1Mε=Z−1N−1MNξ.This has covariance matrixE(uˆuˆ′)=σ2Z−1N−1MNHN′MN′−1Z′−1=σ2SUR=σ2(sijUR).Conditional on sayε0,Cov(βˆ∣εo)=E((X′X)−1X′εε′X(X′X)−1∣εo)=E((X′X)−1X′NZuu′Z′N′X(X′X)−1∣εo).SettingVUR=(vijUR),withviiUR=uˆi2/siiURandvijUR=0,i≠j,and replacingH,Z,SURandVURby their MR estimates, we define the unit root MR-HAC covariance matrix, conditional on sayε0, to be:Cˆ0MR-HAC=(X′X)−1X′NZMRVUR−MRZMR′N′X(X′X)−1.Fig. 9 illustrates the Monte Carlo 95% CI coverage for both the MR(p) and MR-HAC estimators. Surprisingly, there is essentially no difference in performance, despite the here-unnecessary greater generality of MR-HAC. Figure SI-8, omitted here for reasons of space, shows that the same is true for the median bias of the two estimators.The comparisons between the MR(p) and NW-HAC estimators discussed above and illustrated in Figures SI-5 and SI-6 are not entirely fair, since NW-HAC attempts to be more general with its RCH correction. This greater generality naturally costs it precision when the additional generality is not really necessary. The comparison between MR-HAC and NW-HAC in Fig. 9 overcomes this objection, by showing that MR-HAC has less size distortion than NW-HAC for allφ>−0.18. Figure SI-8 in the Supplementary Information shows that the same is true for their median bias for allφ>−2.0.An alternative way to make a fair comparison between the two approaches is to eliminate the RCH adjustment in NW-HAC by defining the “NW-AC” (for Autocorrelation Consistent) estimator, simply by replacing theeiejin (30) with their average values|i−j|/(n−|i−j|). In other words,(45)CˆNW-AC=(X′X)−1X′HˆNW-ACX(X′X)−1,whereHˆNW-AC=(K(|i−j|)s|i−j|/(n−|i−j|)).(Compare Eq. (33).) Coverages based on this covariance matrix are identified as “NW-AC” in Figs. 9. As expected, NW-HAC, with its here-unnecessary heteroskedasticity adjustment, is distinctly outperformed by NW-AC. Nevertheless, MR(p) clearly outperforms even NW-AC for all nonnegative values ofφ, in terms of both coverage (Fig. 9) and median bias (Figure SI-8).The additional lines “KVB” in Figs. 9 and SI-8 are based on the covariance estimator of Kiefer, Vogelsang and Bunzel (KVB 2000), as modified by Kiefer and Vogelsang (2002):CˆKVB=c(X′X)−1X′BX(X′X)−1,whereB=(eiej(n−|i−j|)/n)andcis an asymptotic correction for bias. The analytical computations of Abadir and Paruolo (1996, 2002) demonstrate that in terms of the above 2002 version of the statistic (which differs by a factor of 2 from that in the 2000 paper),c=5.588756592…. While KVB has the advantage that it makes no parametric assumptions about the form of the serial correlation, it is inconsistent in that the limiting covariance estimate is merely an unbiased random variable, rather than a limit in probability.It may be seen from Figs. 9 and SI-8 that the KVB estimator is superior to NW-HAC in terms of 95% CI coverage for allφ>0.23, and in terms of median bias for allφ>−0.20. However, KVB in turn is dominated by MR-HAC by either measure for allφ>−0.20.Fig. 10shows the logarithm of real US GDP for 1947Q1–2014Q2 (n=270observations), along with a quadratic trend line fit by OLS:logyt=a+b(t/4)+c(t/4)2+εt.The visual fit is quite good, and there is a pronounced decline in the local rate of growth over the sample period. Output appears to be distinctly below the local trend in the last five years of the sample. Such a deviation of output from local trend is often used as a guide to monetary policy (Taylor, 1993). All variables were normalized to 0 in the last observation, so that−a=εnis a measure of the “output gap” during 2014Q2, andbis a measure of the trend rate of growth during the same quarter. The integer time index was divided by 4 so that100bhas units of percent per annum. The annual rate of change of the growth rate is2c.Table 2gives the OLS estimates of the quadratic parameters, along with several measures of their standard deviations. The estimate of the interceptaindicates that the “output gap” in 2014Q2 was −4.80% relative to the quadratic trend. The estimate of the linear termbindicates that in the same quarter, the expected growth rate of GDP was down to 2.30% per annum. The estimate of the quadratic termcindicates that GDP growth is falling at a rate of 2.82 (2 × 1.41) percent per annum per century. The OLS standard errors, taken at face value, indicate that all three coefficients are strongly significantly different from zero. The popular Newey–West HAC standard errors shown in the next row of Table 2 are 98%–157% greater than the OLS standard errors, when a bandwidth ofm=4is employed, as determined by (32), but still indicate that all three coefficients are strongly significantly different from zero. However, even though these standard errors are consistent, in finite samples they are triply biased downwards, and hence still give pronounced size distortion, as demonstrated in the simulations in Section  2.5.The standard errors identified as AR(4) in Table 2 were obtained by fitting an AR(4) model to the first 4 residual autocorrelations, shown in Table 3, by means of the Yule–Walker equations as in (36). These standard errors are another 44%–76% larger than the HAC standard errors, depending on the coefficient. With these standard errors, the output gap falls from “strongly significant” (test size 0.01 or lower) to merely “significant” (test size 0.05 or lower), but the other two coefficients remain strongly significant. However, these standard errors still understate the uncertainty because residual autocorrelations generally show less persistence than the true autoregressive process.The MR(4) coefficients in Table 3 show even more persistence than the AR(4) AMM coefficients. Although the difference in total persistence is not great (α=0.958versus 0.936), the MR(4) standard errors in Table 2 are another 45%–46% higher than the unadjusted AR(4) standard errors. The intercepta, and therefore the “output gap” for 2014Q2, is now insignificant at even the weak 10% level, but the other two coefficients remain highly significant. Although the MR(4) estimate of the persistenceαfalls short of unity, the unit root test statisticpUR=0.105so that a unit root cannot be even weakly rejected. This exceeds 10%, so that the preferred “blended” covariance matrix simply equals the unconditional unit root covariance matrix. These standard errors are shown in the last line of Table 2. Although the terminal growth rate of 2.30% per annum remains highly significant, the rate of decline of the growth rate is now not even weakly significant.In sum, although a quadratic trend line fits US log GDP quite well visually, there is no evidence that the estimated “output gap” of −4.80% for 2014Q2 is anything more than a statistical illusion, despite its status as a guide for monetary policy (Taylor, 1993). Even by the MR(4) standard error of 3.29%, this value is not significantly different from zero. The preferred blended standard error for the intercept and therefore the output gap is actually infinite. The blended standard errors indicate that the apparent declining trend in the growth rate is not statistically significant. However, the terminal growth rate estimate of 2.30% per annum remains strongly significant, even though the blended covariance matrix is here equal to the unconditional unit root covariance matrix and therefore assumes no cointegrating relationship.

@&#CONCLUSIONS@&#
The proposed Moment Ratio Estimator for the autoregressive parameters of the errors in an OLS regression is computed from the conventional residual autocorrelation coefficients, but gives far less bias, and provides coefficient confidence intervals with far less size distortion, than the available alternatives considered. The estimator is in the spirit of the Median Unbiased estimator of Andrews (1993) and McCulloch (2008), but does not require a Monte Carlo simulation except for the optional unit root test. The presence of a unit root in the errors, and therefore the absence of a cointegrating relationship, does require reposing the problem, but does not by itself indicate that an OLS correlation between the variables is spurious.It has been shown that despite their consistency, the popular HAC standard errors of Newey and West (1987) can greatly overstate the precision of OLS coefficient estimates with sample sizes and serial correlation commonly found in economic studies. Although the author does not believe that the regressor-conditional heteroskedasticity targeted by the Newey–West HAC estimator is typically as serious a problem in econometric data as serial correlation, a “Moment Ratio-HAC” procedure is proposed that attempts to incorporate this consideration. Surprisingly, there is relatively little cost in terms of size distortion to employing it when errors are in fact independent of the regressors, at least in the simple model considered.Although the factorizations that motivate the MR estimator rely on the strong assumption that the errors are independent of the regressors, the author is optimistic that the MR estimator developed here will have favorable properties when the regressors are merely weakly exogenous. Feasible GLS estimates would provide more efficient estimates of the regression coefficients than the OLS estimates considered here, but any such GLS estimates should be based on preliminary MR estimates of the covariance structure.