@&#MAIN-TITLE@&#
Covariance based point cloud descriptors for object detection and recognition

@&#HIGHLIGHTS@&#
We introduce a covariance-based feature descriptor for object classification.The descriptor is compact (low dimensionality) and computationally fast.Adding new descriptor features amounts to the addition of a new row and column.There is no need to tune parameters such as bin size or number.The descriptor is naturally discriminative and subtracts out common data features.

@&#KEYPHRASES@&#
RGB-D data,Colored point clouds,Classification,Object recognition,

@&#ABSTRACT@&#
Processing 3D point cloud data is of primary interest in many areas of computer vision, including object grasping, robot navigation, and object recognition. The introduction of affordable RGB-D sensors has created a great interest in the computer vision community towards developing efficient algorithms for point cloud processing. Previously, capturing a point cloud required expensive specialized sensors such as lasers or dedicated range imaging devices; now, range data is readily available from low-cost sensors that provide easily extractable point clouds from a depth map. From here, an interesting challenge is to find different objects in the point cloud. Various descriptors have been introduced to match features in a point cloud. Cheap sensors are not necessarily designed to produce precise measurements, which means that the data is not as accurate as a point cloud provided from a laser or a dedicated range finder. Although some feature descriptors have been shown to be successful in recognizing objects from point clouds, there still exists opportunities for improvement. The aim of this paper is to introduce techniques from other fields, such as image processing, into 3D point cloud processing in order to improve rendering, classification, and recognition. Covariances have proven to be a success not only in image processing, but in other domains as well. This work develops the application of covariances in conjunction with 3D point cloud data.

@&#INTRODUCTION@&#
Three dimensional (3D) point clouds provide important cues to analyze objects or environments. They are, for instance, heavily used in topographical mapping, where an airplane or a satellite passes over an area and takes several snapshots with a laser range finder. Researchers like Schwalbe et al. [1] used such laser range finders to build a height map of a scanned area. In the area of mobile robotics, Weingarten et al. [2] have developed a sensor that generates point clouds usable by a robot to sense its environment, detect obstacles, and navigate without hitting them. Ip and Gupta [3] use dense point clouds of models for computer aided design (CAD) purposes. Allen et al. [4] use point clouds to accurately reconstruct historic monuments.One of the most important objectives for a mobile robot is to intelligently process information from the surrounding environment. In order to perform its tasks, the robot needs to be able to perceive its environment. For example, when navigating a robot has to recognize certain features that can be used as landmarks. During the DARPA Grand Challenge and Urban Challenge, laser range finders have been used extensively on-board different autonomous vehicles to build a 3D map of the immediate environment. When trying to grasp an object, a robot first needs to find the object in the scene. To execute this task, 3D object detection and classification have to be performed.There are also several uses of 3D point clouds in the medical domain. 3D scans of patients can be taken for the purpose of medical diagnosis. In orthotics, CAD models can be designed from the patients’ scans in order to develop and manufacture adapted orthoses (efficiently). In dentistry, 3D models of teeth can help to plan and design dental repairs.Common to almost all of these applications is the basic task of identifying and analyzing objects in a 3D point cloud. Object recognition using only depth data is a challenging task as it lacks many other informational cues such as color, intensity, and texture. The problem is exacerbated when multiple objects are cluttered in the point cloud as finding the different object boundaries becomes hard. Furthermore, sensor noise, surface reflectivity of the sensor beams, limited range of the 3D sensors, and the divergence of the sensor beams as the depth of the point increases, make object recognition in this setting extremely difficult.

@&#CONCLUSIONS@&#
