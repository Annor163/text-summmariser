@&#MAIN-TITLE@&#
Soft coverings and their parameter reductions

@&#HIGHLIGHTS@&#
We introduce soft coverings.We obtain the lattice structure of soft sets induced by soft coverings.We investigate the parameter reduction of soft coverings by means of knowledge on the attribute reduction for covering information systems.We present an algorithm to compute all the parameter reductions of soft coverings.We give an application on the evaluation problems to show the usefulness of the parameter reduction of soft coverings.

@&#KEYPHRASES@&#
Soft set,Rough approximation,Soft covering,Topology,Parameter reduction,

@&#ABSTRACT@&#
Soft set theory is a new mathematical tool to deal with uncertain information. This paper studies soft coverings and their parameter reductions. Firstly, we define a soft set on the power set of an initial universe and discuss its properties. Secondly, we introduce soft coverings and obtain the lattice structure of soft sets induced by them. Thirdly, we investigate parameter reductions of a soft covering by means of attribute reductions in a covering information system and present their algorithm. Finally, we give an application to show the usefulness of parameter reductions of a soft covering.

@&#INTRODUCTION@&#
There exist various types of uncertainties, imprecision and vagueness in our real life. Classical mathematics do not always successfully deal with the complicated problems with uncertainties. While probability theory, fuzzy set theory [31], rough set theory [21,22], interval mathematics and other mathematical tools are well-known and useful approaches to describe uncertainties. But all these theories have their own limitations, which is possibly due to the inadequacy of parameterization tools associated with these theories. For example, probability theory as a branch of mathematics studying random phenomena and their statistical laws needs to do a lot of test to verify the law of random phenomena; interval mathematics cannot handle information of continuous smooth change; membership functions in fuzzy set theory and upper and lower approximations based on approximation spaces in rough set theory are difficult to be determined. These theories cannot fully express parameters, that is, a large number of parameters cannot be determined.In 1999, Molodtsov [14] introduced the concept of soft sets, which can be seen as a new mathematical tool for dealing with uncertainty. This so-called soft set theory is free from the difficulties effecting the existing mathematics tools. Using soft set theory to describe or set objects with traditional mathematics tools is very different. We can describe approximately the original objects in soft set theory. There is no limiting conditions when objects are described. Researchers can choose parameters and their forms according Researchers’ needs. The fact that setting parameters is non-binding greatly simplifies decision-making process and then we can still do effective decisions under the circumstances of less information.Recently there has been a rapid growth of soft set theory. Maji et al. [15–18] studied soft set theory and used this theory to solve some decision making problems. Aktas et al. introduced a notion of soft group. Kong et al. [10] applied soft set approach in decision making problems. Ali et al. [3] defined some new operations on soft sets. Majumdar et al. [19] studied the problem of similarity measurement between soft sets. Feng et al. [7] introduced soft rough sets. Ge et al. [8] discussed relationships between topological spaces and soft sets. Li et al. [11] investigated relationships among soft sets, soft rough sets and topologies.Rough set theory initiated by Pawlak [21] is a mathematical tool to deal with vagueness and granularity in information systems. This theory is described by equivalence relations over the universe and handles the approximation of an arbitrary subset of the universe by two definable or observable subsets called lower and upper approximations. But in the actual problems, it is hard to obtain equivalence relations between objects, or there is no equivalence relations between objects. So the rough set model based on equivalence relations does not fully meet the actual needs.Covering rough sets [4,24] are an important extension of rough sets. Compared with rough sets, it often gives a more reasonable description to a subset of the universe. In recent years, covering rough set theory has attracted more and more attentions. The works of Zhu et al. [33–36] are fundamental and significant. Specifically, they proposed the concept of the reducible element [35] to solve several problems in covering based rough sets. And this concept has been adopted by numerous researchers [12,25,29,30].Because rough set theory has its limitations and shortcomings, and soft sets and rough sets describe the different types of uncertainty and can be combined to form a powerful mathematical tool for dealing with uncertain problems, then studying this mathematical tool is importance and necessary so that two theories play their strengths and make up their shortcomings. This will be an important research direction. Actually, a soft set is a parameterized family of subsets of the universe. Soft sets have not any restrictions on the approximate description of objects, and they might form a covering of the universe. Covering rough sets can process data organized by a covering of the universe. Both theories can deal with the uncertainties of data. Therefore, we introduce a new concept of soft coverings and define soft covering rough sets, which are viewed as the combination of soft sets and covering rough sets. This study presents a preliminary, but potentially interesting research direction.It is worthwhile to mention that parameter reductions of soft sets is a very important problem in soft set theory. The parameter reduction of soft sets means deleting parameters of soft sets which are no or less influence for obtaining the optimal decision, and reducing number of the parameters in decision-makings. Much effort has made on this problem. Maji et al. [16,18] proposed the concept of parameter reduction of soft sets. Chen et al. [5] pointed out that this concept in [18] is unreasonable, and then presented another concept of parameter reduction of soft sets. To overcome the problem of suboptimal choice in [5], Kong et al. [10] introduced the concept of normal parameter reduction of soft sets. But the normal parameter reduction is very complex and the algorithm is hard to understand. Ma et al. [20] investigated the normal parameter reduction and improved this algorithm in [10]. Ali et al. [2] gave another view on parameter reduction of soft sets.The organization of this paper is as follows: Section 2 briefly reviews some basic concepts about rough sets, soft sets, covering rough sets and lattices; Section 3 defines a soft set on the power set of initial universe and studies some relative properties; Section 4 introduces the concept of soft coverings and presents the upper and lower approximation operators based on a covering approximation space; Section 5 investigates parameter reductions of a soft covering and presents their algorithm; Section 6 gives an application to show the effectiveness and feasibility of parameter reductions of a soft covering; Section 7 summarizes the main points in this paper.In this section, we briefly recall some basic concepts about rough sets, soft sets, covering rough sets and lattices.Throughout this paper, U denotes an initial universe, E denotes a set of all possible parameters, 2Udenotes the power set of U and22Udenotes the family of all elements of 2U. We only consider the case where both U and E are nonempty finite sets.In this paper, “⋁”(disjunction), “⋀” (conjunction), “⇒” (implication), “⇔” (biimplication) are propositional connectives in mathematical logic. They are read as “or”, “and”, “if-then”, “if and only if”, respectively.Let R be an equivalence relation on U. Then a pair (U, R) is called a Pawlak approximation space. Based on (U, R), we can define the following two rough approximations:R_(X)={x∈U|[x]R⊆X},R¯(X)={x∈U|[x]R∩X≠∅}.R_(X)andR¯(X)are called the Pawlak lower approximation and the Pawlak upper approximation of X, respectively. In general, we refer toR¯(X)andR_(X)as Pawlak rough approximations of X.The Pawlak boundary region of X, denoted by BndR(X), is defined as the difference between Pawlak rough approximations of X, that is,BndR(X)=R¯(X)−R_(X). It is easy to see thatR_(X)⊆X⊆R¯(X).A set is Pawlak rough if its boundary region is not empty, that is, X is Pawlak rough ifR_(X)≠R¯(X). Otherwise, the set is definable.We can relax equivalence relations so that rough set theory can be applied to solve more complicated problems in practice. The classical rough set theory based on equivalence relations has been extended to coverings [28,32].Definition 2.1LetC⊆2U.(1)Cis called a covering of U, if⋃K∈CK=U. Furthermore, a coveringCof U is called a partition of U, if ∅∉C and K⋂K′=∅ for allK,K′∈C.(2)(U,C)is called a covering approximation space, ifCis a covering of U.Definition 2.2[[24]] Let(U,C)be a covering approximation space. For each X∈2U, putC_(X)=∪{K|K∈CandK⊆X},C¯(X)=∪{K|K∈CandK∩X≠∅}.ThenC_(X)andC¯(X)are called the lower approximation and the upper approximation of X with respect to(U,C)orC, respectively. In general, we refer toC_(X)andC¯(X)as rough approximations of X with respect to(U,C)orC.X is called a definable set ifC_(X)=C¯(X). Otherwise, X is called a rough set.LetCbe a covering of U. For every x∈U, denoteC(x)=∩{C∈C|x∈C},Cov(C)={C(x)|x∈U}.LetΔ={C1,C2,⋯,Cn}be a family of coverings of U. For every x∈U, denoteCi(x)=∩{C∈Ci|x∈C}(1≤i≤n),Δ(x)=∩{Ci(x)|1≤i≤n},Cov(Δ)={Δ(x)|x∈U}.Definition 2.3[[27]] LetCbe a covering of U. ThenCov(C)is also a covering of U, we call it the covering of U induced byC.[[27]] Let Δ be a family of coverings of U. Then Cov(Δ) is also a covering of U, we call it the covering of U induced by Δ.Definition 2.5[[27]] Let Δ be a family of coverings of U.(1) IfC∈ΔandCov(Δ)=Cov(Δ−{C}), thenCis called dispensable in Δ. Otherwise,Cis called indispensable in Δ.(2) Let Δ′⊆Δ. If Cov(Δ)=Cov(Δ′) and every element of Δ′ is indispensable in Δ′, then Δ′ is called a reduction of Δ.The set of all reductions of Δ and the set of all indispensable coverings of Δ are denoted by Red(Δ) and Core(Δ), respectively.Definition 2.6Let L be a set and ≤ a binary relation on L. Then ≤ is called a partial order on L, if(i)a≤a for each a∈L,a≤b and b≤a imply a=b for any a, b∈L,a≤b and b≤c imply a≤c for any a, b, c∈L.Definition 2.7[[6]] Let (L, ≤) be a poset and a, b∈L.(1)a is called a top (or maximal) element of L, if x≤a for each x∈L.b is called a bottom (or minimal) element of L, if b≤x for each x∈L.If a poset L has top elements a1, a2 (resp. bottom elements b1, b2), then a1=a2 (resp. b1=b2). We denote the sole top element (resp. the sole bottom element) by 1L(resp. 0L).Definition 2.8[[6]] Let (L, ≤) be a poset, S⊆L and a, b∈L.(1)a is called an above boundary in S, if x≤a for each x∈S.b is called an under boundary in S, if b≤x for each x∈S.a=supS(or⊔S), if a is a minimal above boundary in S.b=infS(or⊓S), if b is a maximal under boundary in S.Let (L, ≤) be a poset, S⊆L. If S has ⊔S (resp. ⊓S), then ⊔S (resp. ⊓S) is sole. But we cannot claim that ⊔S∈S (resp. ⊓S∈S) although ⊔S∈L (resp. ⊓S∈L).If S={a, b}, then we denote ⊔S=a⊔b and ⊔S=a⊔b.Remark 2.9[[6]] Let (L, ≤) be a poset and a, b∈L. Then a=a⊓b⇔a≤b⇔b=a⊔b.Definition 2.10[[6]] Let (L, ≤) be a poset.(1)L is called a lattice, if a⊔b∈L, a⊓b∈L for any a, b∈L.L is called a complete lattice, if ⊔S∈L, ⊓S∈L for each S⊆L.L is called a distributive lattice, if a⊔(b⊓c)=(a⊔b)⊓(a⊔c), a⊓(b⊔c)=(a⊓b)⊔(a⊓c) for any a, b, c∈L.Definition 2.11[[6]] Let L be a lattice with 1Land 0L. Let a, b∈L. b is called a complement element of a, if a⊔b=1L, a⊓b=0L.If L is a distributive lattice and a∈L has complement elements b1, b2, then b1=b2. We denote the complement element of a by ac.Example 2.12Let L=[0, 1]. For any a, b∈L, we define a≤b by b−a≥0. Obviously, 1L=1, 0L=0. It is easily proved that L is a complete distributive lattice.Definition 2.13[[6]] Let L be a distributive lattice. L is called a Boolean lattice or Boolean algebra, if each a∈L has the complement element ac∈L and L has 1Land 0L.Definition 2.14τ⊆2Uis called a topology on U, if(i)∅, U∈τ;A, B∈τ ⇒ A∩B∈τ;{Aα:α∈Γ}⊆τ⇒⋃α∈ΓAα∈τ.The pair (U, τ) is called a topological space. Every member of τ is called open. Its complement is called closed. A∈2Uis called a neighborhood of a point x∈U if x∈A and A contains an open set.The interior and closure of A∈2Uare denoted by int(A) and cl(A), respectively. They are defined as follows:int(A)orintτ(A)=⋃{B∈τ:B⊆A},cl(A)orclτ(A)=⋂{B∈τc:B⊇A}.Definition 3.1[[14]] Let A⊆E. A pair (f, A) is called a soft set over U, if f is a mapping defined by f:A→2U. We denote (f, A) by fA.In other words, a soft set over U is a parameterized family of subsets of the universe U. For e∈A, f(e) may be considered as the set of e-approximate elements of fA.To illustrate this idea, let us consider the following example.Example 3.2Let U={h1, h2, h3, h4, h5, h6} be a set of houses and A={e1, e2, e3, e4}⊆E a set of status of houses where ej(j=1, 2, 3, 4) stands for the parameters “beautiful”, “modern”, “cheap” and “in the green surroundings”, respectively.Now, we consider a soft set fA, which describes the “attractiveness of the houses” that Mr.X is going to buy. In this case, to define the soft set fAmeans to point out beautiful houses, modern houses and so on.Consider the mapping f given by “houses(.)”, where (.) is to be filled in by one of the parameters. For instance, f(e1) means “houses (beautiful)”, and its functional value is the set consisting of all the beautiful houses in U.Let f(e1)={h1, h2, h5}, f(e2)={h1, h6}, f(e3)={h3, h4}, f(e4)={h3, h4, h6}. Then the soft set fAis a parameterized family {f(ei)|i=1, …, 4}. Besides, fAis also described as Table 1, in which the value =1 whenever hi∈f(ej) (1≤i≤6, 1≤j≤4). Otherwise, the value =0.Definition 3.3[[17]] Let A, B⊆E and let fAand gBbe two soft sets over U. fAis called a soft subset of gB, if A⊆B and f(e)=g(e) for each e∈A. We denote it byfA⊆˜gB.Definition 3.4[[17]] Let A, B⊆E and let fAand gBbe two soft sets over U. fAand gBare called soft equal, if A=B and f(e)=g(e) for each e∈A. We denote it by fA=gB.Obviously, fA=gBif and only iffA⊆˜gBandgB⊆˜fA.Definition 3.5[[17]] Let A⊆E and fAa soft sets over U. Then the relative complement of a soft set fAis denoted by (fA)cand (fA)cis defined by(fA)c=fAcwhere fc:A→2Uis a mapping given by fc(e)=U−f(e) for each e∈A.Definition 3.6[[17]] Let A, B, C⊆E and let fA, gBand hCbe three soft sets over U.(1)hCis called the intersection of fAand gB, if C=A∩B and h(e)=f(e)∩g(e) for each e∈C. We denote hCbyfA∩˜gB.hCis called the union of fAand gB, if C=A∪B andh(e)=f(e),e∈A−B,f(e)∪g(e),e∈A∩B,g(e),e∈B−A.We denote hCbyfA∪˜gB.Although rough set theory and soft set theory are two different mathematical tools for modeling vagueness, there are some interesting connections between them. For example, Aktas et al. [1] showed that Pawlak's rough set model can be regarded as a special case of soft sets, and Feng et al. [7] illustrated that every soft set may be considered as an information system which is closely related with rough sets.Definition 3.7Let A⊆E. A pair (σ, A) is called a soft set over 2U, if σ is a mapping defined byσ:A→22U. We denote (σ, A) by σA.To illustrate the background of Definition 3.7, let us consider the following example.Example 3.8Suppose that U={h1, h2, h3, h4, h5, h6} is a set of six mobile phones by the universe set and A={age, price, reception} is a set of parameters of these mobile phones used to investigate. We denote these parameters as {a, p, r} and these parameters are characterized by the parameter values: {new, used, old}, {expensive, middle, cheap} and {excellent, good, bad}, respectively. The six mobile phones in U are classified by these parameters, represented in Table 2.Now, we consider a soft set σAover 2U, which specifically describes the “attractiveness of the mobile phone”. Consider the mappingσ:A⟶22Ugiven by “mobile phones (.)”, where (.) is to be filled in by one of the parameters in A. For instance, σ(a) means “mobile phones (age)”, and its functional values are the sets consisting of all the new mobile phones, the used mobile phones and the old mobile phones in U.It is easy to see from Table 2 that every parameter induces an equivalence relation on U. Thus, we get the equivalence classes as follows:for a the equivalence classes are {h1, h2, h6}, {h4}, {h3, h5},for p the equivalence classes are {h1, h5, h6}, {h2, h3}, {h4},for r the equivalence classes are {h1, h3}, {h5}, {h2, h4, h6}.Putσ(a)={{h1, h2, h6}, {h4}, {h3, h5}},σ(p)={{h1, h5, h6}, {h2, h3}, {h4}},σ(r)={{h1, h3}, {h5}, {h2, h4, h6}}.Then we obtain a soft set σAover 2U, which is the parameterized family of the power set of U. Thus, we can view the soft set σAas followsσA=age={new={h1,h2,h6},used={h4},old={h3,h5}},price={cheap={h1,h5,h6},middle={h2,h3},expensive={h4}},reception={good={h1,h3},bad={h5},excellent={h2,h4,h6}}Proposition 3.9Every soft set fAover U can be represented as a soft set σAover 2U.Let fAbe a soft set over U. For e∈A, put σ(e)={f(e)}. Then σAis a soft set over 2U. Thus fAcan be represented as a soft set σAover 2U. □Definition 3.10Let A, B⊆E and let σAand δBbe two soft sets over 2U.(1)σAand δBare called soft equal, if A=B and σ(e)=σ(e) for each e∈A. We write σA=δB.σAis called a soft subset of δB, if A⊆B and f(e)⊆g(e) for each e∈A. We writeσA⊆˜δB.Obviously, σA=δBif and only ifσA⊆˜δBandδB⊆˜σA.Definition 3.11Let A⊆E and σAa soft set over 2U.(1)σAis called null, if σ(e)=∅ for each e∈A. We denote it by ∅A.σAis called absolute, if σ(e)=2Ufor each e∈A. We denote it by UA.σAis called keeping intersection, if for each e∈A, M∩N∈σ(e) whenever M, N∈σ(e).ForA,B⊆2U, we defineA∩B,A∪BandA−Bas follows:A∩B={M|M∈AandM∈B},A∪B={M|M∈AorM∈B},A−B={M|M∈AandM∉B}.Particularly,Ac=2U−A.Definition 3.12Let A, B, C⊆E and let σA, δBand λCbe three soft sets over 2U.(1)λCis called the intersection of σAand δB, if C=A∩B and λ(e)=σ(e)∩δ(e) for each e∈C. We writeσA∩˜δB=λC.λCis called the union of σAand δB, if C=A∪B andλ(e)=σ(e),ife∈A−B,δ(e),ife∈B−A,σ(e)∪δ(e),ife∈A∩B.We writeσA∪˜δB=λC.Definition 3.13Let A⊆E and σAa soft set over 2U. Then the complement of σA, denoted by (σA)c, is defined by(σA)c=σAcor (σA)c=(σc, A) whereσc:A→22Uis a mapping given by σc(a)=2U−σ(a) for each a∈A.Proposition 3.14Let A, B, C⊆E and let σA, δBand λCbe three soft sets over 2U. Then(1)σA∪˜σA=σA.σA∪˜δB=δB∪˜σA.(σA∪˜δB)∪˜λC=σA∪˜(δB∪˜λC).Proof(1) and (2) are obvious.(3) Put(σA∪˜δB)∪˜λC=kA∪B∪C,σA∪˜(δB∪˜λC)=lA∪B∪C;σA∪˜δB=sA∪B,δB∪˜λC=tB∪C.For each e∈A∪B∪C, it follows that e∈A, or e∈B, or e∈C.Case1e∈C.(a)If e∉A and e∉B, then k(e)=λ(e)=t(e)=l(e).If e∉A and e∈B, then k(e)=s(e)∪λ(e)=δ(e)∪λ(e)=t(e)=l(e).If e∈A and e∉B, then k(e)=s(e)∪λ(e)=σ(e)∪λ(e)=σ(e)∪t(e)=l(e).If e∈A and e∈B, then k(e)=s(e)∪λ(e)=(σ(e)∪δ(e))∪λ(e)=σ(e)∪(δ(e)∪λ(e))=σ(e)∪t(e)=l(e).Case2e∉C.(a)If e∉A and e∈B, then k(e)=s(e)=δ(e)=t(e)=l(e).If e∈A and e∉B, then k(e)=s(e)=σ(e)=t(e)=l(e).If e∈A and e∈B, then k(e)=s(e)=σ(e)∪δ(e)=σ(e)∪t(e)=l(e).Thus(σA∪˜δB)∪˜λC=σA∪˜(δB∪˜λC). □Proposition 3.15Let A, B, C⊆E and let σA, δBand λCbe three soft sets over 2U. Then(1)σA∩˜σA=σA.σA∩˜δB=δB∩˜σA.(σA∩˜δB)∩˜λC=σA∩˜(δB∩˜λC).(1) and (2) are obvious.(3) Put(σA∩˜δB)∩˜λC=kA∩B∩C,σA∩˜(δB∩˜λC))=lA∩B∩C.For each e∈A∩B∩C, it follows that e∈A, e∈B and e∈C. Then k(e)=(σ(e)∩δ(e))∩λ(e)=σ(e)∩(δ(e)∩λ(e))=l(e), so(σA∩˜δB)∩˜λC=σA∩˜(δB∩˜λC). □Proposition 3.16Let A, B, C⊆E and let σA, δBand λCbe three soft sets over 2U. Then(1)(σA∪˜δB)∩˜λC=(σA∩˜λC)∪˜(δB∩˜λC).(σA∩˜δB)∪˜λC=(σA∪˜λC)∩˜(δB∪˜λC).(1)Put(σA∪˜δB)∩˜λC=k′(A∪B)∩C,(σA∩˜λC)∪˜(δB∩˜λC)=l′(A∩C)∪(B∩C).Obviously, (A∪B)∩C=(A∩C)∪(B∩C).For each e∈(A∪B)∩C, we have e∈A∩C or e∈B∩C.(a)If e∉A∩C and e∈B∩C, then e∉A, e∈B and e∈C. So k′(e)=δ(e)∩λ(e)=l′(e).If e∈A∩C and e∉B∩C, then e∈A, e∉B and e∈C. So k′(e)=σ(e)∩λ(e)=l′(e).This proof is similar to (1)Proposition 3.17Let A⊆E and σAa soft set over 2U. Then(1)((σA)c)c=σA.σA∪˜(σA)c=UA.σA∩˜(σA)c=∅A.(1)For each e∈A,(σ(e)c)c=(2U−σ(e))c=2U−(2U−σ(e))=σ(e).Then((σA)c)c=σA.PutσA∪˜(σA)c=γA.For each e∈A, γ(e)=σ(e)∪σc(e)=2U. That is,σA∪˜(σA)c=UA.The proof is similar to (2).In this section we define the concept of soft coverings and discuss rough approximations based on soft coverings.Definition 4.1Let A⊆E and σAa soft set over 2U. Then σAis called a soft covering over U, if ⋃K∈σ(e)K=U for each e∈A.Example 4.2In Example 3.2, we haveσ(a)={{h1, h2, h6}, {h4}, {h3, h5}}, σ(p)={{h1, h5, h6}, {h4}, {h2, h3}}, σ(r)={{h1, h3}, {h5}, {h2, h4, h6}}.Obviously,⋃K∈σ(a)K=⋃K∈σ(p)K=⋃K∈σ(r)K=U.Thus σAis a soft covering over U.Motivated by Pomykala's original idea about covering rough sets [24], we consider the lower and upper approximations about soft coverings, which gives rise to the following notions in a natural way.Definition 4.3Let A⊆E and σAa soft covering over U. Then a pair (U, σA) is called a soft covering approximation space. Based on (U, σA), for each e∈A, we define a pair of operationsσ_(e),σ¯(e): 2U⟶2Uas follows:σ_X(e)=∪{K∈σ(e)|K⊆X},σ¯X(e)=∪{K∈σ(e)|K∩X≠∅},σ_X(e)andσ¯X(e)are called the soft lower approximation and the soft upper approximation of X with respect to (U, σA), respectively. In general, we refer toσ_X(e)andσ¯X(e)as soft rough approximations of X with respect to (U, σA).X is called a definable set with respect to (U, σA) (briefly, X is called a e-definable set), ifσ_X(e)=σ¯X(e)for all e∈A. Otherwise, X is called a rough set with respect to (U, σA) (briefly, X is called a e-rough set).It is easy to show thatσ_X(e)={x∈U|∃K∈σ(e)s.t.x∈KandK⊆X},σ¯X(e)={x∈U|∃K∈σ(e)s.t.x∈KandK∩X≠∅}.Proposition 4.4Let A⊆E and σAa soft covering over U. Then for any e∈A and X, Y∈2U, we have(1)σ_∅(e)=σ¯∅(e)=∅;σ_U(e)=σ¯U(e)=U.σ_X(e)⊆X⊆σ¯X(e).X⊆Y⇒σ_X(e)⊆σ_Y(e);X⊆Y⇒σ¯X(e)⊆σ¯Y(e).If σEis keeping intersection, thenσ_X∩Y(e)=σ_X(e)∩σ_Y(e).σ¯X∪Y(e)=σ¯X(e)∪σ¯Y(e).σ_X∪Y(e)⊇σ_X(e)∪σ_Y(e);σ¯X∩Y(e)⊆σ¯X(e)∩σ¯Y(e).(1) and (3) are obvious.(2) Obviously,σ_X(e)⊆X.Suppose that x∈X. Since σ(e) is a covering of U, there exists K∈σ(e) such that x∈K. Then K⋂X≠∅. Sox∈σ¯X(e). ThusX⊆σ¯X(e). (4) Since X∩Y⊆X and X∩Y⊆Y, by (3), we haveσ_X∩Y(e)⊆σ_X(e)andσ_X∩Y(e)⊆σ_Y(e). Soσ_X∩Y(e)⊆σ_X(e)∩σ_Y(e).Suppose thatx∈σ_X(e)∩σ_Y(e). Then there exist K, K′∈σ(e) such that x∈K, x∈K′, K⊆X and K′⊆Y. Since σAis keeping intersection, there exists K0∈σ(e) such that K0=K∩K′. Now x∈K0 and K0⊆X∩Y. This impliesx∈σ_X∩Y(e). Soσ_X(e)∩σ_Y(e)⊆σ_X∩Y(e).Henceσ_X∩Y(e)=σ_X(e)∩σ_Y(e).(5) Since X, Y⊆X∪Y, by (3), we haveσ¯X(e)⊆σ¯X∪Y(e)andσ¯Y(e)⊆σ¯X∪Y(e). Soσ¯X(e)∪σ¯Y(e)⊆σ¯X∪Y(e).Suppose thatx∈σ¯X∪Y(e). Then there exists K∈σ(e) such that x∈K and K∩(X∪Y)≠∅, which implies that K∩X≠∅ or K∩Y≠∅. Sox∈σ¯X(e)orx∈σ¯Y(e). Thusσ¯X(e)∪σ¯Y(e)⊇σ¯X∪Y(e).Henceσ¯X(e)∪σ¯Y(e)=σ¯X∪Y(e).(6) This holds by (3). □Definition 4.5Let A⊆E and σAa soft covering over U. For each X∈2U, the soft sets fAover U is defined byf(e)=σ_X(e)(resp.f(e)=σ¯X(e))for each e∈A. We denote it by(σ_X)A(resp.(σ¯X)A).(σ_X)Aand(σ¯X)Aare called the lower soft set and the upper soft set induced by σAwith respect to X, respectively. In short, the lower soft set and the upper soft set are both called soft sets induced by soft coverings.Proposition 4.6Let A⊆E and σAa soft covering over U. For any X, Y∈2U, we have(1)(σ¯∅)A=(σ_∅)A=∅A;(σ¯U)A=(σ_U)A=UA.(σ_X)A⊆X⊆(σ¯X)A.X⊆Y⇒(σ_X)A⊆(σ_Y)A;X⊆Y⇒(σ¯X)A⊆(σ¯Y)A.If σAis keeping intersection, then(σ_X∩Y)A=(σ_X)A∩˜(σ_Y)A.(σ¯X∪Y)A=(σ¯X)A∪˜(σ¯Y)A.(σ_X∪Y)A⊇(σ_X)A∪˜(σ_Y)A;(σ¯X∩Y)A⊆(σ¯X)A∩˜(σ¯Y)A.This holds by Proposition 4.4. □Example 4.7In Example 4.2, put X={h1, h3} and Y={h1, h5}. Then the soft sets(σ_X)Aand(σ_Y)Aare respectively represented as:σ_X(a)=∅,σ_X(p)=∅,σ_X(r)={h1,h3}.σ_Y(a)=∅,σ_Y(p)=∅,σ_Y(r)={h5}.The soft set(σ_X)A∪˜(σ_Y)A=δAis represented as:δ(a)=∅,δ(p)=∅,δ(r)={h1, h3, h5}.For X∪Y={h1, h3, h5}, the soft set(σ_X∪Y)Ais represented as:σ_X∪Y(a)={h3,h5},σ_X∪Y(p)=∅,σ_X∪Y(r)={h1,h3,h5}.Obviously,(σ_X∪Y)A≠(σ_X)A∪˜(σ_Y)A. Besides, the soft set(σ¯X)Aand(σ¯Y)Aare represented respectively as:σ¯X(a)={h1,h2,h3,h5,h6},σ¯X(p)={h1,h2,h3,h5,h6},σ¯X(r)={h1,h3}σ¯Y(a)={h1,h2,h3,h5,h6},σ¯Y(p)={h1,h5,h6},σ¯Y(r)={h1,h3,h5}.The soft set(σ¯X)A∩˜(σ¯Y)A=ρAis represented as:ρ(a)={h1, h2, h3, h5, h6},ρ(p)={h1, h5, h6},ρ(r)={h1, h3}.For X∩Y={h1}, the soft set(σ¯X∩Y)Ais represented as:σ¯X∩Y(a)={h1,h2,h6},σ¯X∩Y(p)={h1,h5,h6},σ¯X∩Y(r)={h1,h3}.Thus,(σ¯X)A∩˜(σ¯Y)A≠(σ¯X∩Y)A.Theorem 4.8Let A⊆E and σAa soft covering over U. We denoteQ(A)={(σ_X)A|X∈2U}.Define a binary relation ≤ onQ(A)by(σ_X)A≤(σ_Y)A⇔X⊆YforanyX,Y∈2U,and define two operations ⊓, ⊔ onQ(A)by(σ_X)A⊓(σ_Y)A=(σ_X∩Y)A,(σ_X)A⊔(σ_Y)A=(σ_X∪Y)A.Then(Q(A),⊓,⊔)is a Boolean lattice. We call it a lower soft set-lattice.(1) (a) Let(σ_X)A∈Q(A). Obviously,(σ_X)A≤(σ_X)A. (b) Let(σ_X)A,(σ_Y)A∈Q(A). Suppose that(σ_X)A≤(σ_Y)Aandσ_AY≤(σ_X)A. Then X⊆Y, Y⊆X. So X=Y. Thus(σ_X)A=(σ_Y)A. (c) Let(σ_X)A,(σ_Y)A,(σ_Z)A∈Q(A). Suppose that(σ_X)A≤(σ_Y)Aand(σ_Y)A≤(σ_Z)A. Then X⊆Y, Y⊆Z. So X⊆Z. Thus(σ_X)A≤(σ_Z)A. Hence,(Q(A),≤)is a poset.(2) Let(σ_X)A,(σ_Y)A∈Q(A). Then X, Y∈2U. Since X∩Y∈2U,(σ_X)A⊓(σ_Y)A∈Q(A). (3) Let(σ_X)A,(σ_Y)A∈Q(A). Then X, Y∈2U. Since X∪Y∈2U,(σ_X)A⊔(σ_Y)A∈Q(A). Thus(Q(A),⊓,⊔)is a lattice.Obviously,Q(A)satisfies the distributive law,1Q(A)=(σ_U)A,0Q(A)=(σ_∅)Aand(σ_X)Ac=(σ_U−X)Afor each(σ_X)A∈Q(A).Hence(Q(A),⊓,⊔)is a Boolean lattice. □Theorem 4.9Let A⊆E and σAa soft covering over U. DenoteP(A)={(σ¯X)A|X∈2U}.Define a binary relation ≤ and two operations ⊓ and ⊔ onP(A)by(σ¯X)A≤(σ¯Y)A⇔X⊆YforanyX,Y∈2U,(σ¯X)A⊓(σ¯Y)A=(σ¯X∩Y)A,(σ¯X)A⊔(σ¯Y)A=(σ¯X∪Y)A.Then(P(A),⊓,⊔)is a Boolean lattice. We call it an upper soft set-lattice.This is similar to the proof of Theorem 4.8. □Theorem 4.10Let A⊆E and σAa soft covering over U. If σAis keeping intersection, thenτσ(e)={X∈2U|σ_X(e)=X}is a topology on U for each e∈A.(1) By Proposition 4.4,σ_∅(e)=∅∈τσ(e)andσ_U(e)=U∈τσ(e).(2) Since σAis keeping intersection,X∩Y=σ_X(e)∩σ_Y(e)=σ_X∩Y(e)∈τσ(e)for any X, Y∈τσ(e).(3) By Proposition 4.4,σ_X∪Y(e)⊆X∪Yfor any X, Y∈τσ(e). Since X⊆X∪Y andY⊆X∪Y,X=σ_X(e)⊆σ_X∪Y(e)andY=σ_Y(e)⊆σ_X∪Y(e). SoX∪Y⊆σ_X∪Y(e). Thusσ_X∪Y(e)=X∪Y.Hence τσ(e) is a topology on U for each e∈A. □In this section, we investigate parameter reductions of a soft covering and propose their algorithm.Soft set theory is a mathematical tool to deal with uncertain problems, which can be applied in decision-making problems. The parameter reduction of soft sets means deleting parameters of soft sets which are no or less influence for obtaining the optimal decision and reducing number of the parameters in decision-makings.For a soft covering, the research on its parameter reduction is important. Parameter reductions of a soft covering mean also finding the minimum or minor set of the parameters in decision-makings. Specific approach is first classifying parameters of a soft covering according to keeping relations invariant between any two elements in the universe and then finding the minimum or minor set of parameters for obtaining the optimal decision.Below, we present a method to determine dispensable parameters and study the original relations with respect to a parameter set. Based on these original relations, the discernibility matrix is proposed to compute parameter reductions of a soft covering.Let A⊆E and σAa soft covering over U. For any x∈U, e∈A and P⊆A, denoteNe(x)=∩{K∈σ(e)|x∈K},NP(x)=∩{Ne(x)|e∈P},Cov(e)={Ne(x)|x∈U},Cov(P)={NP(x)|x∈U}.Definition 5.1Let A⊆E and σAa soft covering over U. Then for each e∈A, Cov(e) is also a covering of U, we call it the covering of U induced by the parameter e.Let U={x1, x2, …, x9} and A={a, b, c}⊆E. Suppose that σAis a soft covering over U, defined as follows:σ(a)={{x1, x2, x6}, {x2, x4, x9}, {x2, x3, x5, x7, x8}},σ(b)={{x1, x4, x6, x9}, {x1, x2, x6}, {x3, x5, x7, x8}},σ(c)={{x1, x6}, {x1, x2, x6}, {x3, x5, x7, x8}, {x3, x4, x5, x7, x8, x9}}.Na(x2)={x1, x2, x6}∩{x2, x4, x9}∩{x2, x3, x5, x7, x8}={x2},Similarly, Na(x1)=Na(x6)={x1, x2, x6},Na(x3)=Na(x5)=Na(x7)=Na(x8)={x2, x3, x5, x7, x8},Na(x4)=Na(x9)={x2, x4, x9}.So Cov(a)={{x1, x2, x6}, {x2}, {x2, x3, x5, x7, x8}, {x2, x4, x9}}.Nb(x1)=Nb(x6)={x1, x4, x6, x9}∩{x1, x2, x6}={x1, x6},Similarly, Nb(x2)={x1, x2, x6},Nb(x3)=Nb(x5)=Nb(x7)=Nb(x8)={x3, x5, x7, x8},Nb(x4)=Nb(x9)={x1, x4, x6, x9}.So Cov(b)={{x1, x6}, {x1, x2, x6}, {x3, x5, x7, x8}, {x1, x4, x6, x9}}.Nc(x1)=Nc(x6)={x1, x6}∩{x1, x2, x6}={x1, x6},Similarly, Nc(x2)={x1, x2, x6},Nc(x3)=Nc(x5)=Nc(x7)=Nc(x8)={x3, x5, x7, x8},Nc(x4)=Nc(x9)={x3, x4, x5, x7, x8, x9}.So Cov(c)={{x1, x6}, {x1, x2, x6}, {x3, x5, x7, x8}, {x3, x4, x5, x7, x8, x9}}.Proposition 5.3Let e∈A⊆E and σAa soft covering over U. Then for any x, y∈U,(1)y∈Ne(x) if and only if Ne(y)⊆Ne(x).y∈Ne(x) and x∈Ne(y) if and only if Ne(x)=Ne(y).If σ(e) is a partition of U, then Cov(e)=σ(e).Denote σ(e)x={K∈σ(e)∣x∈K}.(1)Sufficiency. Obviously. Necessity. Suppose K∈σ(e)x. Then K∈σ(e) and x∈K. Since y∈Ne(x), we have y∈K. So K∈σ(e)y. Thus σ(e)x⊆σ(e)y. This prove Ne(y)⊆Ne(x).This holds by (1).Suppose K∈σ(e). Since σ(e) is a partition of U, we have K≠∅. Pick x∈K. Then |σ(e)x|=1 and so σ(e)x={K}. This implies K=Ne(x)∈Cov(e).On the other hand, suppose K∈Cov(e), then K=Ne(x) for some x∈U. Since σ(e) is a partition of U, we have |σ(e)x|=1. Denote σ(e)x={K′}. Then K=Ne(x)=⋂σ(e)x=K′ and so K∈σ(e).Hence Cov(e)=σ(e). □The following example illustrates that “Cov(e)=σ(e) implies σ(e) is a partition”does not hold.Example 5.4In Example 5.2, we have Cov(c)=σ(c). But σ(c) is not a partition.Let σ(e)={K1, K2, …, Kn}. For each x∈U, Ne(x) is the minimal element of Cov(e) including x, namely Ne(x) is the most complete description of x with respect to e. Every element Kiin σ(e) denotes a parameter value of e, namely, the collection of objects in U takes certain parameter value. Suppose that Ne(x)=K1∩K2, this implies that the possible values of x with respect to e are K1 or K2, namely, the relation between K1 and K2 is disjunctive.Definition 5.5Let A⊆E and σAa soft covering over U. Then for each P⊆A, Cov(P) is also a covering of U, we call it the covering of U induced by the parameter set P.In Example 5.2, we haveNA(x1)=NA(x6)={x1,x6},quadNA(x2)={x2},NA(x3)=NA(x5)=NA(x7)=NA(x8)={x3,x5,x7,x8},NA(x4)=NA(x9)={x4,x9}.So Cov(A)={{x1, x6}, {x2}, {x3, x5, x7, x8}, {x4, x9}}.Let P⊆A⊆E and σAa soft covering over U. Then for any x, y∈U,(1)y∈NP(x) if and only if NP(y)⊆NP(x).y∈NP(x) and x∈NP(y) if and only if NP(x)=NP(y).If σ(e) is a partition of U for each e∈P, then Cov(P) is also a partition of U.ProofDenote σ(e)x={K∈σ(e)∣x∈K}.(1)Sufficiency. Obviously. Necessity. y∈NP(x) implies y∈Ne(x) for each e∈P. By Proposition 5.3(1), Ne(y)⊆Ne(x) for each e∈P. Thus NP(y)⊆NP(x).This holds by (1).Obviously, NP(x)≠∅ for each x∈U and ⋃x∈UNP(x)=U.Suppose NP(x)⋂NP(y)≠∅. Then for each e∈P, Ne(x)⋂Ne(y)≠∅.Since σ(e) is a partition of U for each e∈P, we have |σ(e)x|=|σ(e)y|=1. Denoteσ(e)x={Kex}andσ(e)y={Key}. This impliesNe(x)=KexandNe(y)=Key. ThenKex⋂Key≠∅. Pickz∈Kex⋂Key.z∈KeximpliesKex∈σ(e)z. Thenσ(e)z={Kex}=σ(e)x.z∈KeyimpliesKey∈σ(e)z. Thenσ(e)z={Key}=σ(e)y.Thus for eache∈P,Kex=Key. This implies that y∈Ne(x) and x∈Ne(y) for each e∈P. Then y∈NP(x) and x∈NP(y). By (2), NP(x)=NP(y).Hence Cov(P) is a partition of U. □The following example illustrates that “if Cov(P) is a partition on U, then σ(e) is a partition U for each e∈P” does not hold.Example 5.8In Example 5.6, we have Cov(P) is a partition. But σ(a), σ(b) and σ(c) are not partitions.Obviously, every element of Cov(P) cannot be written as the union of other elements of Cov(P). Actually, NP(x) is the intersection of all the elements including x in every σ(e). This implies that NP(x) is the minimal element of Cov(P) including x, namely, NP(x) is the most complete description of x with respect to P. NP(x) is the covering class including x and Cov(P) can be viewed as the intersection of all σ(e) in σP, then it is the final classification of the universe U by covers in σPand may not be partition. NP(x)=∩{Ne(x)∈Cov(e)| e∈P} means the relation among Ne(x) is conjunctive.In classical rough set theory, for two objects, two equivalence classes including them are equal to each other or have an empty overlap. If their equivalence classes are equal, then these two objects are called indiscernible. Based on this statement, the method of discernibility matrix to compute all the reductions of classical rough sets was presented in [1]. However, for the soft covering, things are quite different and complex.Let A⊆E, σAa soft covering over U and e∈A. For two elements in σ(e), they may have nonempty overlap. This means that if σ(e) is employed to express a parameterized family of subsets of U and the elements in σ(e) express the relative parameter values, then the relative parameter values may have nonempty overlap. For the objects in the nonempty overlapping part, one possible description is to employ both of these two relative parameter values so that no information is lost. For example, for the parameter “height” with the relative parameter values “tall”, “middle” and “low”, a 1.7 meter-high man may be tall or middle, the most complete description to the man is tall or middle for losing no information. By this way, we can get the most complete description of every object in the universe by a covering induced by the parameter set.Let P⊆A⊆E and σAa soft covering over U. We define three relations on U as follows:R1P={(x,y)|NP(x)=NP(y)},R2P={(x,y)|NP(x)⊂NP(y)orNP(x)⊃NP(y)},R3P={(x,y)|NP(x)⊄NP(y)andNP(x)⊅NP(y)}.For P⊆A, there are three possible relations:R1P,R2P,R3P. We call them the original relations with respect to P. “NP(x)=NP(y)”, “NP(x)⊂NP(y)orNP(x)⊃NP(y)” and “NP(x)⊄NP(y)andNP(x)⊅NP(y)” are called the original relations between x and y with respect to P.We denoteRiA=Ri(i=1,2,3).We have the following statements to characterize these relations.Proposition 5.9Let P⊆A⊆E and σAa soft covering over U. DenoteR21P={(x,y)|NP(x)⊂NP(y)}. Then for any x, y∈U.(1)xR1Pyif and only if Ne(x)=Ne(y) for each e∈P.xR21Pyif and only if Ne(x)⊆Ne(y) for each e∈P and there is e0∈P such thatNe0(x)⊂Ne0(y).xR3Pyif and only if there exist e1, e2∈A such thatNe1(x)⊂Ne1(y)andNe2(x)⊃Ne2(y)or there exists e0∈P such thatNe0(x)⊄Ne0(y)andNe0(x)⊅Ne0(y).(1) Sufficiency. Suppose that Ne(x)=Ne(y) for each e∈P. Clearly, NP(x)=NP(y) holds.Necessity. Suppose that there exists e0∈P such thatNe0(x)≠Ne0(y), then at least one ofx∉Ne0(y)andy∉Ne0(x)holds. This implies NP(x)≠NP(y).(2) Sufficiency. Suppose that we have Ne(x)⊆Ne(y) for each e∈P. Then NP(x)⊆NP(y) holds. Besides, if there is exists e0∈P such thatNe0(x)⊂Ne0(y), then we havey∉Ne0(x). This implies NP(x)⊂NP(y).Necessity. Suppose that NP(x)⊂NP(y). Then x∈NP(y). So for each e∈P we have x∈Ne(y) which implies Ne(x)⊆Ne(y). And by NP(x)⊂NP(y) we have y∉NP(x), so there exists e0∈P such thaty∉Ne0(x), thusNe0(x)⊂Ne0(y).(3) Sufficiency. Suppose that there are e1, e2∈P such thatNe1(x)⊂Ne1(y)andNe2(x)⊃Ne2(y). Theny∉Ne1(x)which implies y∉NP(x) andx∉Ne2(y)which implies x∉NP(y), so NP(y)⊄NP(x) and NP(x)⊄NP(y).Suppose that there exists e0∈P such thatNe0(x)⊄Ne0(y)andNe0(x)⊅Ne0(y). Thenx∉Ne0(y)andy∉Ne0(x), so x∉NP(y) and y∉NP(x) which implies NP(x)⊄NP(y) and NP(x)⊅NP(y).Necessity. If NP(x)⊄NP(y) and NP(x)⊅NP(y), obviously, there exists e0∈P such thatNe0(x)⊄Ne0(y)andNe0(x)⊅Ne0(y). Suppose that NP(x)⊄NP(y) and NP(x)⊅NP(y) hold and there is no e0∈P such thatNe0(x)⊄Ne0(y)andNe0(x)⊅Ne0(y). Then by the proof of (2) there must be e1, e2∈P such thatNe1(x)⊂Ne1(y)andNe2(x)⊃Ne2(y), otherwise, it will lead to NP(x)⊃NP(y) or NP(x)⊂NP(y) which is a contradiction. □Definition 5.10Let P⊆A⊆E and σAa soft covering over U. e∈P is called dispensable in P, if Cov(P)=Cov(P−{e}). Otherwise, e is called indispensable in P.Definition 5.11Let P⊆A⊆E and σAa soft covering over U. P is called a parameter reduction of σA, if for each e∈P, e is indispensable in P and Cov(A)=Cov(P).The following theorem gives the decision condition which parameters are indispensable in a soft covering.Theorem 5.12Let A⊆E and σAa soft covering over U. Then e∈A is indispensable in A if and only if there is at least a pair of x, y∈U whose original relations with respect to A is changed after deleting e from A.Denote P=A−{e}.Necessity. Let e be indispensable in A. Then Cov(A)≠Cov(P). So there are x0, y0∈U such that y0∈NP(x0) and y0∉NA(x0). This implies NP(y0)⊆NP(x0) and NA(y0)⊄NA(x0). Thus, the original relations between x0 and y0 with respect to A are changed after deleting e from A.Sufficiency. For any x, y∈U, NA(x)=NA(y) if for each e∈A we have Ne(x)=Ne(y). If we delete a parameter from A and the original relations between x and y with respect to A is changed, then NA(x)=NA(y) could not hold. Now, suppose that we delete a parameter e from A and the original relations between x and y with respect to A are changed. Then at least one of NA(x)≠NP(x) and NA(y)≠NP(y) holds which imply Cov(A)≠Cov(P). Thus e is indispensable in A. □For each e∈A⊆E, we specify a Boolean variable “e”. If P={e1, e2, …, ek}, then we specify a Boolean function e1∨e2∨⋯∨ek.Denote⋁{e1,e2,…,ek}or⋁i=1kei=e1∨e2∨⋯∨ek,⋀{e1,e2,…,ek}or⋀i=1kei=e1∧e2∧⋯∧ek.Remark 5.13If there exist i, j (1≤i≤k, 1≤j≤l) such that si=tj, then⋀i=1kesi≤⋁j=1letj.Suppose U={x1, x2, …, xn}. For any xi, xj∈U, we denote a matrix(cij)n×nby M(U, σA), called the discernibility matrix of (U, σA), it satisfiescij=∅,ifxiR1xj{e∈A|Ne(xi)⊂Ne(xj)orNe(xj)⊂Ne(xi)},ifxiR2xj{e∈A|(Ne(xi)⊄Ne(xj))and(Ne(xj)⊄Ne(xi))}∪{es∧et|(Nes(xi)⊂Nes(xj))and(Net(xi)⊃Net(xj)),es,et∈A},ifxiR3xj.In the proposed method one has to compute NP(x) of every object and should examine xj∈NP(xi) (xi∈U) for any e∈A for construction of cij, which is different from discernibility matrix based on equivalence relations. Hu [9] pointed out that successive neighborhoods can be obtained with a linear time o(n), while the complexity of discernibility matrix is o(n2). Therefore, the time complexity of our method is o(N·n2), where n and N are the numbers of objects and parameters, respectively.Example 5.15In Example 5.2, the discernibility matrix M(U, σA) of (U, σA) is presented as follows:∅{a⋀b,a∧c}∅{a,b,c}{b,c}∅{a,c}{b,c}{a,b}∅{a,b,c}{b,c}∅{a,b}∅∅{a∧b,a∧c}{a,b,c}{a,c}{a,b,c}∅{a,b,c}{b,c}∅{a,b}∅{a,b,c}∅{a,b,c}{b,c}∅{a,b}∅{a,b,c}∅∅{a,c}{b,c}{a,b}∅{a,b}{a,c}{a,b}{a,b}∅Definition 5.16A discernibility function f(U, σA) for (U, σA) is a Boolean function of m Boolean variables e1, e2, …, emwith respect to the soft covering σAover U, and is defined as follows:f(U,σA)=⋀{⋁cij|1≤j≤i≤n,cij≠∅},where ⋁cijis the disjunction of all elements in cij.We have the following proposition for the core of σA.Proposition 5.17Let U={x1, x2, …, xn}, A={e1, e2, …, em}⊆E and σAa soft covering over U. ThenCore(σA)={e∈A|∃i,j(1≤j≤i≤n)s.t.cij={e}orcij={e∧e′|e′∈Pij}forsomePij⊆A}.Suppose that e∈Core(σA). Then Cov(A)≠Cov(A−{e}). This implies that there exist xi, xjwhose original relations with respect to A is changed after deleting e from A.If xiR2xj, then cij={e}.If xiR3xj, then cij={e} or there exists Pij⊆A such that cij={e∧e′| e′∈Pij}. If there exist i, j (1≤j≤i≤n) such that cij={e} or cij={e∧e′| e′∈Pij}, it is clear that e∈Core(σA). Hence we complete the proof. □Let P⊆A⊆E. Then Cov(A)=Cov(P) if and only if P∩cij≠∅ for any cij≠∅, where es∧etbelonging to P∩cijwith es∧et∈cijmeans {es, et}⊆P.Necessity. Suppose that Cov(A)=Cov(P). Then P contains a parameter reduction of σA. So for any xi, xj∈U with relations R2 or R3 with respect to A are also with relations R2 or R3 with respect to P. Hence P∩cij≠∅ for each cij≠∅.Sufficiency. Suppose that P∩cij≠∅ for every cij≠∅. Then it implies that we have enough covers of σPto keep the original relations with respect to A between all the objects in a universe, that is, P contains a parameter reduction of σA, which implies Cov(A)=Cov(P). □Corollary 5.19Let P⊆A⊆E and σAa soft covering over U. Then P is a parameter reduction of σAif and only if P is the minimal parameter set satisfying P∩cij≠∅ for cij≠∅.This holds by Proposition 5.18. □Definition 5.20Let A⊆E, σAa soft covering over U and f(U, σA) the discernibility function for (U, σA). By applying the multiplication and absorption laws, we have f(U, σA)=⋁⋀ Pk(Pk⊆A), where Pk={ekl| l≤pk} for any k≤q have no repetitive elements, then f(U, σA)=⋁⋀ Pkis called the minimal disjunctive normal form of f(U, σA). We denote it by f*(U, σA). That is,f*(U,σA)=⋁⋀Pk=⋁k=1q(⋀l=1pkekl).By the discernibility function, we have the following theorem to compute all parameter reductions of a soft covering.Theorem 5.21Let A⊆E, σAa soft covering over U, f(U, σA) the discernibility function for (U, σA) and f*(U, σA)=⋁⋀ Pkthe minimal disjunctive normal form of f(U, σA). Then Red(σA)={Pk| k≤q}.Proof(1) For any k≤q, we have ⋀Pk≤⋁cij. By the disjunction and conjunction laws, Pk∩cij≠∅ for any cij≠∅. Since f*(U, σA)=⋁⋀ Pk, it follows that for arbitrary Pkif we reduce a parameter e from Pk, letP′k=Pk−{e}, thenf*(U,σA)≠⋁t=1k−1(⋀Pt)⋁(⋀P′k)⋁(⋁t=k+1q(⋀Pt))andf*(U,σA)<⋁t=1k−1(⋀Pt)⋁(⋀P′k)⋁(⋁t=k+1q(⋀Pt)).If we still haveP′k∩cij≠∅for each cij≠∅ (1≤j≤i≤n), then⋀P′k≤⋁cijfor any cij≠∅. This impliesf*(U,σA)≥⋁t=1k−1(⋀Pt)⋁(⋀P′k)⋁(⋁t=k+1q(⋀Pt)),which is a contradiction. Hence there existsci0j0≠∅such thatP′k∩ci0j0=∅and this implies that Pkis the parameter reduction of σA.(2) For each Q∈Red(σA), we haveQ∩cij≠∅foranycij≠∅(1≤j≤i≤n),then ⋀Q≤⋁cij, sof(U,σA)⋀(⋀Q)=⋀⋁cij⋀(⋀Q)=⋀Q,this implies⋀Q≤f(U,σA).Suppose that for each k≤q we have Pk−Q≠∅, then for each k we can find ek∈Pk−Q. By rewritingf*(U,σA)=(⋁k=1qek)⋀Φ, since f*(U, σA)=f(U, σA), we have⋀Q≤⋁k=1qek. So there must beek0such that⋀Q≤ek0, this impliesek0∈Q, which is a contradiction. SoPk0⊆Qfor some k0. Since both Q andPk0are two parameter reductions of σA, we haveQ=Pk0.HenceRed(σA)={Pk|k≤q}.□Example 5.22In Example 5.15, sincef(U,σA)=⋀{⋁cij|1≤j≤i≤n,cij≠∅}=(a∨c)∧(a∨b)∧(b∨c)∧(a∨b∨c)∧((a∨b)∧(a∨c))=a∧(b∨c)f*(U,σA)=(a∨b)∧(a∨c),we have Red(σA)={{a, b}, {a, c}}, Core(σA)={a}.Next we give an algorithm on parameter reductions of a soft covering.Algorithm 5.23Let A⊆E and σAa soft covering over U. An algorithm of parameter reductions of σAis represented as follows:Input: A soft covering σA.Output: All parameter reductions of σAand the core of σA.Step 1.Calculate the discernibility matrix M(U, σA) of σA;Give discernibility function f(U, σA) of σA;Calculate the minimal disjunctive normal form f*(U, σA);Output all parameter reductions of σA.Fig. 1Pilots’ professional skills are critical in air transport. In order to ensure flight safety, we ought to establish scientific evaluation index systems to assess trainee pilot training.According to the basic principles of trainee pilot training evaluation, the evaluation indexes should include academic performance, psychological quality, health, driving ability, strain capacity, height, weight, education level, English proficiency, communication skill, character, diligence and hobby, etc. The more indexes we consider, the more comprehensive the evaluation result is. Therefore, we tend to consider many factors. Multiple indexes will undoubtedly provide a wealth of information. But they increase the complexity of trainee pilot training evaluation to a certain extent. One index is related to another to some extent and what they indicate may be overlap- ping. Different indexes will lead to inconsistent appraisal results. In order to understand fully the internal structure and all information of the indexes, we must carry on the scientific analysis of the original information, look for the relatively small number of indexes to synthesize information on all aspects, seize the principal contradiction and simplify the evaluation work.It is a very complicated to build a training evaluation index system for trainee pilots. This section will not talk about the construction and just apply parameter reductions of a soft covering to reduce the evaluation indexes of trainee pilot, then we obtain the key indexes (parameters) and the necessary indexes (parameters).Suppose that U={x1, x2, x3, x4, x5, x6, x7, x8} is the set consist of eight student pilots. Suppose there are six parameters used to evaluate the student pilot training quality, consisting a parameter set A={psychological quality, communicative ability, driving ability, academic performance, strain capacity, education level}={a,b,c,d,e,f} (say) and these parameters are characterized by the parameter values: {best, better, bad}, {good, bad}, {excellent, strong, normal} and {best, better, good}, {tall, middle,short}, {good, bad }, respectively. We have four specialists, denoted A, B, C, D, who give some different evaluation results as follows:For parameter “psychological quality”A: better={x1, x4, x5, x7},good={x2, x8},bad={x3, x6},B: better={x1, x2, x4, x7, x8},good={x5},bad={x3, x6},C: better={x1, x4, x7},bad={x2, x3, x5, x6, x8},D: better={x1, x4, x7},good={x5},bad={x2, x3, x6, x8}.For parameter “communicative ability”A: good={x1, x2, x3, x6},bad={x4, x5, x7, x8},B: good={x1, x2, x3, x5},bad={x4, x6, x7, x8},C: good={x1, x2, x3, x4},bad={x5, x6, x7, x8},D: good={x1, x2, x3},bad={x4, x5, x6, x7, x8}.For parameter “driving ability”A: excellent={x1, x2, x3},strong={x4, x5, x6, x7, x8},B: excellent={x1, x2, x3},strong={x4, x5, x6, x7},normal={x8},C: excellent={x1, x2, x3},strong={x4, x5, x6, x8},normal={x7},D: excellent={x1, x2, x3},strong={x4, x5, x6},normal={x7, x8}.For parameter “academic performance”A: good={x3, x6},better={x1, x2},best={x4, x5, x7, x8},B: good={x2, x3, x6},better={x1, x5},best={x4, x7, x8},C: good={x2, x3, x6},better={x1},best={x4, x5, x7, x8},D: good={x3, x5, x6},better={x1, x2, x4},best={x7, x8}.For parameter “strain capacity”A: good={x1, x4, x5, x7},better={x2, x8},best={x3, x6, },B: good={x4, x7},better={x1, x2, x5},best={x3, x6, x8},C: good={x1, x4, x5, x7},best={x2, x3, x6, x8},D: good={x4, x5, x7},better={x1, x3},best={x2, x6, x8}.For parameter “education level”A: high={x1, x2, x3, x4, },low={x5, x6, x7, x8},B: high={x1, x2, x3, x4, x5},low={x6, x7, x8},C: high={x1, x2, x4, x5},low={x3, x6, x7, x8},D: high={x1, x2, x4, x5},low={x3, x6, x7, x8}.Assume that these evaluation results from the four specialists are of the same importance. If we want to combine these evaluations together without losing information, we should unite the evaluation results for each parameter. Thus we can a covering with respect to each parameter evaluation, which completely reflect each trainee pilot's information. Now let us consider the mapping σ given by “trainee pilot(.)”, where (.) is to be filled in by one of the parameters A.So, for the parameter a, σ(a) means “trainee pilots’ psychological quality”, whose functional value is the family consisting of all sets: better trainee pilots {x1, x2, x4, x5, x7, x8}, good trainee pilots {x2, x5, x8}, bad trainee pilots {x2, x3, x5, x6, x8}.For the parameter b, σ(b) means “trainee pilots’ communicative ability”, whose functional value is the family consisting of all sets: good trainee pilots {x1, x2, x3, x4, x5, x6}, bad trainee pilots {x4, x5, x6, x7, x8}.For the parameter c, σ(c) means “trainee pilots’ driving ability”, whose functional value is the family consisting of all sets: trainee pilots with excellent driving ability {x1, x2, x3}, trainee pilots with strong driving ability {x4, x5, x6, x7, x8}, trainee pilots with normal driving ability {x7, x8}.For the parameter d, σ(d) means “trainee pilots’ academic performance”, whose functional value is the family consisting of all sets: good trainee pilots {x2, x3, x5, x6}, better trainee pilots {x1, x2, x4, x5}, best trainee pilots {x4, x5, x7, x8}.For the parameter e, σ(e) means “trainee pilots’ strain capacity”, whose functional value is the family consisting of all sets: good trainee pilots {x1, x2, x4, x5, x7}, better trainee pilots {x1, x2, x3, x5, x8}, best trainee pilots {x2, x3, x6, x8}.For the parameter f, σ(f) means “trainee pilots’ education level”, whose functional value is the family consisting of all sets: trainee pilots with high education level {x1, x2, x3, x4, x5}, trainee pilots with low education level {x3, x5, x6, x7, x8}.Actually, for each parameter we can get a covering instead of a partition and they are formed a soft covering, which describes the eight trainee pilots about psychological quality, communicative ability, driving ability, academic performance, strain capacity, education level, respectively. There are mathematically expressed as follows:σ(a)={{x1,x2,x4,x5,x7,x8},{x2,x5,x8},{x2,x3,x5,x6,x8}},σ(b)={{x1,x2,x3,x4,x5,x6},{x4,x5,x6,x7,x8}},σ(c)={{x1,x2,x3},{x4,x5,x6,x7,x8},{x7,x8}},σ(d)={{x2,x3,x5,x6},{x1,x2,x4,x5},{x4,x5,x7,x8}}.σ(e)={{x1,x2,x4,x5,x7},{x1,x2,x3,x5,x8},{x2,x3,x6,x8}}.σ(f)={{x1,x2,x3,x4,x5},{x3,x5,x6,x7,x8}}.To collect important evaluation results without losing information, for each parameter (or covering) we calculate intersection of all elements containing xi, which is the minimal description of trainee pilot xifor one evaluation parameter. There are mathematically expressed as follows:Na(x1)=Na(x4)=Na(x7)={x1,x2,x4,x5,x7,x8},Na(x2)=Na(x5)=Na(x8)={x2,x5,x8},Na(x3)=Na(x6)={x2,x3,x5,x6,x8};Nb(x1)=Nb(x2)=Nb(x3)={x1,x2,x3,x4,x5,x6},Nb(x4)=Nb(x5)=Nb(x6)={x4,x5,x6},Nb(x7)=Nb(x8)={x4,x5,x6,x7,x8};Nc(x1)=Nc(x2)=Nc(x3)={x1,x2,x3},Nc(x4)=Nc(x5)=Nc(x6)={x4,x5,x6,x7,x8},Nc(x7)=Nc(x8)={x7,x8};Nd(x1)={x1,x2,x4,x5},Nd(x2)={x2,x5},Nd(x3)=Nd(x6)={x2,x3,x5,x6},Nd(x4)={x4,x5},Nd(x5)={x5},Nd(x7)=Nd(x8)={x4,x5,x7,x8};Ne(x1)=Ne(x2)=Ne(x4)=Ne(x7)={x1,x2,x4,x5,x7},Ne(x3)=Ne(x8)={x2,x3,x8},Ne(x5)={x1,x2,x5},Ne(x6)={x2,x3,x6,x8};Nf(x1)=Nf(x2)=Nf(x4)={x1,x2,x3,x4,x5},Nf(x3)=Nf(x5)={x3,x5},Nf(x6)=Nf(x7)=Nf(x8)={x3,x5,x6,x7,x8}.We further calculate intersection of all elements containing xifor all parameters, denoted NA(x1), which is the minimal description of trainee pilot xiunder the evaluation parameter system. There are as follows:NA(x1)={x1,x2},NA(x2)={x2},NA(x3)={x3},NA(x4)={x4,x5},NA(x5)={x5},NA(x6)={x6},NA(x7)={x7},NA(x8)={x8}.Now, we need to find the indispensable parameter(s) from there parameters. On the basic of NA(xi), we construct the discernibility matrix by Definition 5.14. The discernibility matrix M(U, σA) of (U, σA) is presented as follows:∅{a,d}∅{a,d,e}{e,d∧f}∅{c}{a∧b,c,d}{a,c,d,e,b∧f}∅{c}{c}{c,e}{a,d,e,f}∅{a,c,d,e,f}{a∧b,b∧d,c,e,f}{c,b∧e}{a,d,e,f}{e}∅{b,c,d,f}{b,c,d,f}{a,b,c,d,e}{b∧c,c∧d,f}{c∧a,c∧b,c∧d,c∧e,c∧f}{a,b∧c,d,e}∅{b,c,d,e,f}{b,c,d,e,f}{b,c,d,a∧f}{a∧b,a∧d,b∧c,c∧d,e,f}{c∧b,c∧d,c∧f,e}{d,b∧a,b∧c}{e}∅Thus we can obtain a discernibility function by Definition 5.20 and calculate the minimal disjunctive normal form, which is mathematically expressed as follows:f(U,σA)=⋀{⋁cij|1≤j≤i≤8,cij≠∅}=(a∨d)∧(a∨d∨e)∧c∧(a∨c∨d∨e∨f)∧(b∨c∨d∨f)∧(b∨c∨d∨e∨f)∧(e∨(d∧f))∨((a∧b)∨c∨d)∧(c∨e∨f∨(a∧b)∨(b∧d))∧(a∨c∨d∨e∨(b∧f))∧(c∨e)∧(c∨(b∧e))∧(a∨b∨c∨d∨e)∧(b∨c∨d∨(a∧f))∧(a∨d∨e∨f)∧((b∧c)∨(c∧d)∨f)∧((a∧b)∨(a∧d)∨(b∧c)∨(c∧d)∨e∨f)∧e∧(c∧(a∨b∨d∨e∨f))∧(e∨c∧(b∨d∨f))∧(a∨d∨e∨(b∧c))∧(d∨b∧(a∨c))=c∧e∧(a∨d)∧(d∨(b∧a)∨(b∧c))∧(c∨e∨f∨(a∧b)∨(b∧d))∧((b∧c)∨(c∧d)∨f)∧(a∧b)∨(a∧d)∨(b∧c)∨(c∧d)∨e∨f)=(c∧d∧e)∨(a∧b∧c∧e).Therefore, we can obtain all reductions and the core of parameters by Theorem 5.21:Red(σA)={{c,d,e},{a,b,c,e}},Core(σA)={c,e}.From the above analysis, we get two simplified evaluation methods by parameter set {driving ability, academic performance, strain capacity} and {psychological quality, communicative ability, driving ability, strain capacity}, and apparently one's driving ability and strain capacity are the necessary parameters for training evaluation of trainee pilot. This plays a simplified and important role in the process of evaluation. If the eight trainee pilots are training samples, we get two different kinds of the applicable evaluation methods for other input samples: {driving ability, academic performance, strain capacity} and {psychological quality, communicative ability, driving ability, strain capacity}. So, it is showed that applying parameter reduction of a soft covering to simply the training evaluation system of trainee pilot is effective and feasible, and thus we save much time and labor force in a sense.

@&#CONCLUSIONS@&#
In this paper, we introduced soft coverings, investigated their parameter reductions by means of knowledge on the attribute reduction for covering information systems and proposed an algorithm (see Fig. 2). Parameter reductions of a soft covering may play an important role in some knowledge discovery problems. For further work, on one hand, we will concentrate on its applications in knowledge discovery. On the other hand, we will develop the proposed approach to deal with imprecise and uncertain information, and explore the proposed approach how to carry out the fusion of multi-source information.