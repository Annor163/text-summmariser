@&#MAIN-TITLE@&#
Generalized moment-independent importance measures based on Minkowski distance

@&#HIGHLIGHTS@&#
The paper focuses on the moment-independent importance measures in reliability and safety engineering.The paper presents a general formulation of moment-independent importance measures based on Minkowski distance.The properties of the generalized importance measures are studied.

@&#KEYPHRASES@&#
Moment-independent importance measures,Minkowski distance,Probability density function,Cumulative distribution function,Quantile,

@&#ABSTRACT@&#
Importance measures have been widely studied and applied in reliability and safety engineering. This paper presents a general formulation of moment-independent importance measures and several commonly discussed importance measures are unified based on Minkowski distance (MD). Moment-independent importance measures can be categorized into three classes of MD importance measures, i.e. probability density function based MD importance measure, cumulative distribution function based MD importance measure and quantile based MD importance measure. Some properties of the proposed MD importance measures are investigated. Several new importance measures are also derived as special cases of the generalized MD importance measures and illustrated with some case studies.

@&#INTRODUCTION@&#
Importance measures are very useful in ranking components in complex systems, especially when further improvement decisions have to be made. It is closely related to sensitivity analysis (Castillo, Mínguez, & Castillo, 2008; Helton, Johnson, Sallaberry, & Storlie, 2006; Kleijnen, 2005; Kleijnen & Helton, 1999; Saltelli, Chan, & Scott, 2000; Xie, 1987; Xie & Bergman, 1992). With the information of the relative importance of the parameters, one can quickly identify the improvement for the system performance instead of directly searching for a global optimal solution (Borgonovo, 2010; Kuo & Zhu, 2012a). The importance measures in reliability analysis can be classified into structure importance measure, reliability importance measure and lifetime importance measure (Kuo & Zhu, 2012b). A variety of importance measures have been proposed for different systems and different purposes, such as the traditional Birnbaum measure (Birnbaum, 1968) and Barlow–Proschan measure (Barlow & Proschan, 1975; Eryilmaz, 2013), the joint importance of components (Gao, Cui, & Li, 2007; Hong, Koo, & Lie, 2002), the importance measures for multi-state systems (Levitin, Podofillini, & Zio, 2003; Si, Cai, Sun, & Zhang, 2010; Si, Dui, Zhao, Zhang, & Sun, 2012) as well as the more recently importance measures for Markovian systems (Do Van, Barros, & Bérenguer, 2008, 2009, 2010) or for semi-Markov systems (Distefano, Longo, & Trivedi, 2012; Hellmich & Berg, 2013). On the other hand, the importance measures can be considered as “local” or “global” from the viewpoint of sensitivity analysis. The global sensitivity analysis techniques, including non-parametric techniques (Storlie & Helton, 2008; Storlie, Reich, Helton, Swiler, & Sallaberry, 2013; Storlie, Swiler, Helton, & Sallaberry, 2009), variance-based techniques (Iman, 1987; Li, Lu, & Zhou, 2011; Sobol, 2001, 2003; Zhou, Lu, Li, Feng, & Wang, 2013) and moment-independent techniques (Borgonovo, 2007), provide an attractive perspective in identifying the influence of the inputs on the output.The moment-independent importance measures, which consider the impact of a certain parameter on the distribution of the output and bear the merits of model-free and moment-independent, have been proposed recently and received considerable attention. Borgonovo (2007) introduced a probability density function (PDF)-based importance measure, whereas Liu and Homma (2010) proposed a similar importance measure but in terms of the cumulative distribution function (CDF). In order to carry out the sensitivity analysis of the parameter inside the distribution function, Cui, Lu, and Wang (2012) suggested a CDF-based importance measure utilizingL2the distance between the original CDF and the conditional CDF instead ofL1distance.In fact, all these importance measures weigh the importance of the parameter in terms of the distance between the conditional distribution function and the original distribution function of the output, where the distance can beL1distance orL2distance. So it is natural to extend the measure into a more general case by Lpdistance, i.e. Minkowski distance. On the other hand, since the distribution of the output can be characterized by its PDF, CDF or quantile function, it is reasonable to define importance measures on any one of these functions. With these considerations, three classes of generalized Minkowski distance based (MD) importance measures, i.e. PDF-based MD importance measure, CDF-based MD importance measure and quantile-based MD importance measure are proposed. The generalized MD importance measures have considerable flexibility and the existing moment-independent importance measures can be seen as their special cases. The properties of the generalized MD importance measures are studied and some new and promising moment-independent importance measures are derived from the generalized MD importance measures.The remainder of this paper is organized as follows. In Section 2, three unified MD importance measures are proposed. In Section 3, properties of the proposed MD importance measures are investigated. In Section 4, some new importance measures derived from the generalized MD importance measures are discussed and illustrated with case studies. Conclusions are given in the end.Minkowski distance is derived from the well-known Minkowski inequality. In general, the Minkowski distance of order p could be defined as:(1)d(g1,g2)=∫X|g1(x)-g2(x)|pdx1p,p⩾1,where g1(x) and g2(x) are functions of x andXis the range of the integration. For instance, ifXis an index set,X={1,2,…,n}, and g1(x) and g2(x) are real, then Eq. (1) can be rewritten as(2)d(g1,g2)=∑i=1ng1i-g2ip1p,which is the distance between two pointsg11,g12,…,g1nandg21,g22,…,g2nin Rn. Alternatively, ifX=[a,b]and g1(x) and g2(x) are Lpintegrable, i.e.∫ab|g1(x)|pdx<∞and∫ab|g2(x)|pdx<∞, then(3)d(g1,g2)=∫ab|g1(x)-g2(x)|pdx1/p,is the Lpdistance between functions g1(x) and g2(x).The Minkowski distance reduces to the rectilinear distance and Euclid distance when the order p equals to 1 and 2, respectively. As p approaches positive infinity, the Chebyshev distance is obtained:(4)limp→∞∫X|g1(x)-g2(x)|pdx1/p=supx∈X∣g1(x)-g2(x)∣.In this paper, we focus on the model Y=h(X), whereX=[X1,X2,…,Xn] is the random model inputs which can be described by the continuous PDF fX(·), and h(·) is a deterministic continuous scalar function. The uncertainty inXtransfers through function h(·), which results in that the model output Y is also a random variable. As mentioned, the moment-independent importance measures concentrate on the discrimination between the original distribution function and the conditional distribution function of the output, where the original distribution function is simply the distribution function of Y and the conditional distribution function is the distribution function of Y conditioned on that Xiis fixed to a certain value xi0, i.e. Y∣Xi=xi0.Minkowski distance can be used to measure the difference between the original distribution and the conditional distribution to generalize the existing moment-independent importance measures in a unified form. Since the distribution of the output can be characterized by its PDF, CDF or its quantile function, three classes of generalized MD importance measures are proposed based on different distribution functions.The PDF of a random variable characterizes the relative likelihood for this random variable to take on a certain value. For a random input Xi, if its uncertainty is removed, say Xi=xi0, then the shape of the conditional PDFfY|Xi=xi0would differ from the original PDF fY(y). For any value of y, the likelihood that Y takes y is different and the Minkowski distance∫Y|fY(y)-fY|Xi=xi0(y)∣pdy1/pover the output spaceYmeasures to what extent the difference would be. Since we do not know the true value of Xi, it is natural to use the expected distance to represent the shape change of the PDF due to Xi. So the PDF-based MD importance measure could be defined as follows.Definition 1Suppose∫YfY(y)-fY|Xi=x(y)pdy1/p<∞holds for p⩾1 andx∈Xi, then the PDF-based MD importance measure of parameter Xiof order p is defined as:(5)Iipdf(p)=EXi∫Y|fY(y)-fY|Xi(y)|pdy1/p=∫Xi∫Y|fY(y)-fY|Xi=x(y)|pdy1/pfXi(x)dx,whereXiis the domain of random variable Xi,fXi(x)is the marginal PDF of XiandfY|Xi=x(y)is the conditional PDF of Y given Xi=x.Specifically,Iipdf(∞)is defined as:(6)Iipdf(∞)=EXisupy∈Y|fY(y)-fY|Xi(y)|=∫Xisupy∈Y|fY(y)-fY|Xi=x(y)|fXi(x)dx.Remark 1Since the Minkowski distance∫Y|fY(y)-fY|Xi(y)|pdy1/pis actually a random variable, one can also use its quantile, e.g. the lower quartile or the upper quartile, to represent the importance of Xi.The variance of∫Y|fY(y)-fY|Xi(y)|pdy1/preflects the variation of the Minkowski distance between fY(y) andfY|Xi(y)due to the uncertainty in Xi, which can give an intuition of the range of the difference between fY(y) andfY|Xi(y). For a small variance, one only needs to consider whether it is necessary to remove the uncertainty in Xi, while a large variance requires the analyst to consider not only the necessity to remove the uncertainty in Xi, but also the extent that the uncertainty in Xishould be reduced (since distance between fY(y) andfY|Xi=x(y)varies a lot for different x). Therefore, it can also serve as an importance indicator.Borgonovo (2007) proposed a moment-independent importance measure δias follows(7)δi=12EXi∫-∞∞|fY(y)-fY|Xi(y)|dy.Clearly, this isIipdf(1)except for a normalization constant.With the notations unchanged, if we substitute PDFs with CDFs in Definition 1, the CDF-based MD importance measure could be obtained.Definition 2Suppose∫Y|FY(y)-FY|Xi=x(y)|pdy1/p<∞holds for p⩾1 andx∈Xi, then the CDF-based MD importance measure of parameter Xiof order p is defined as:(8)Iicdf(p)=EXi∫Y|FY(y)-FY|Xi(y)|pdy1/p=∫Xi∫Y|FY(y)-FY|Xi=x(y)|pdy1/pfXi(x)dx,where FY(y) is the CDF of Y and FY∣Xi=x(y) is the conditional CDF of Y given Xi=x.Specifically,Iicdf(∞)is defined as:(9)Iicdf(∞)=EXisupy∈Y|FY(y)-FY|Xi(y)|=∫Xisupy∈Y|FY(y)-FY|Xi=x(y)|fXi(x)dx.Apparently, the importance measure introduced in (Liu & Homma, 2010)(10)Si(CDF)=EXi∫Y|FY(y)-FY|Xi(y)|dy|E(Y)|.isIicdf(1)except for a normalization constant, whereas the importance measure introduced in (Cui et al., 2012)(11)εi=EXi∫Y|FY(y)-FY|Xi(y)|2dycan be seen as a variant ofIicdf(2).The distance∫Y|FY(y)-FY|Xi(y)|pdy1/pmeasures the vertical shape shift of the conditional CDF from the original CDF. If we want to measure the shape shift horizontally, the quantile function (or the inverse distribution function) can be applied. Let GY(μ)=inf {y:FY(y)>μ} andGY|Xi=x(μ)=inf{y:FY|Xi=x(y)>μ}be the quantile functions of Y and Y∣Xi=x, respectively. Then the quantile-based MD importance measure can be defined as follows.Definition 3Suppose∫01|GY(μ)-GY|Xi=x(μ)|pdμ1/p<∞holds for p⩾1 andx∈Xi, then the quantile-based MD importance measure of parameter Xiof order p is defined as:(12)Iiquantile(p)=EXi∫01|GY(μ)-GY|Xi(μ)|pdμ1/p=∫Xi∫01GY(μ)-GY|Xi=x(μ)pdμ1/pfXi(x)dx.Specifically,Iiquantile(∞)is defined as:(13)Iiquantile(∞)=EXisupμ∈[0,1]|GY(μ)-GY|Xi(μ)|=∫Xisupμ∈[0,1]|GY(μ)-GY|Xi=x(μ)|fXi(x)dx.Remark 3Iiquantile(1) is identical toIicdf(1). In fact,∫Y|FY(y)-FY|Xi=x(y)|dymeasures the area bounded by FY(y) andFY|Xi=x(y)vertically, whereas∫01|GY(μ)-GY|Xi=x(μ)|dμmeasures it horizontally, which is illustrated in Fig. 1.In this section, we focus on the properties of the proposed MD importance measures and some analytical results are shown. They are useful for the applications of the proposed MD importance measures.Proposition 1Iiquantile(p)is monotonically increasing with p⩾1, i.e.Iiquantile(p1)⩽Iiquantile(p2)holds for any 1⩽p1<p2.Leth(p)=∫01|GY(μ)-GY|Xi(μ)|pdμ1/p. The derivative with respect to p is∂h(p)∂p=1p2∫01|GY(μ)-GY|Xi(μ)|pdμ1p-1∫01|GY(μ)-GY|Xi(μ)|pln|GY(μ)-GY|Xi(μ)|pdμ-∫01|GY(μ)-GY|Xi(μ)|pdμln∫01|GY(μ)-GY|Xi(μ)|pdμ.First note that1p2∫01|GY(μ)-GY|Xi(μ)|pdμ1p-1>0. Second, the function g(x)=x ln x is a convex function for x>0, sincedg(x)dx=lnx+1andd2g(x)dx2=1x>0. Then, according to Jensen’s inequality we haveg∫01|GY(μ)-GY|Xi(μ)|pdμ⩽∫01g(|GY(μ)-GY|Xi(μ)|p)dμ,i.e.∫01|GY(μ)-GY|Xi(μ)|pdμln∫01|GY(μ)-GY|Xi(μ)|pdμ⩽∫01|GY(μ)-GY|Xi(μ)|pln|GY(μ)-GY|Xi(μ)|pdμ.Therefore, the derivative∂h(p)∂p⩾0holds, from which one can immediately conclude thatIiquantile(p)is monotonically increasing. □One may wonder whether the similar proposition can be carried out for the PDF-based or CDF-based importance measures. Actually, it is not possible to simply extend the conclusion of Proposition 1 to the PDF-based importance measure or the CDF-based measure (the first case study in Section 4 can serve as a counterexample). However, forIipdf(p)andIicdf(p), we have the following proposition.Proposition 2If∫Yδ0[fY(y)]dy⩽1, whereδ0[fY(y)]=1forfY(y)>00,otherwisethenIipdf(p)andIicdf(p)are monotonically increasing for p>1.We only prove the proposition forIipdf(p)and the proof forIicdf(p)can be obtained similarly. LetC=∫Yδ0[fY(y)]dy, thenIipdf(p)can be rewritten asIipdf(p)=EXiC1p∫Y|fY(y)-fY|Xi(y)|pCdy1p.It can be proved that∫Y|fY(y)-fY|Xi(y)|pCdy1pis monotonically increasing with p with a similar trick as in the proof for Proposition 1, andC1pis monotonically increasing with p for C⩽1, so we can conclude thatC1p∫Y|fY(y)-fY|Xi(y)|pCdy1pis monotonically increasing with p, which directly leads to Proposition 2.□Iicdf(∞)<Iipdf(1).Letyx∗=argsupy∈Y|FY(y)-FY|Xi=x(y)|. Then we haveFYyx∗-FY|Xi=xyx∗=∫-∞yx∗fY(y)dy-∫-∞yx∗fY|Xi=x(y)dy⩽∫-∞yx∗|fY(y)-fY|Xi=x(y)|dy<∫Y|fY(y)-fY|Xi=x(y)|dy,which directly leads toEXisupy∈Y|FY(y)-FY|Xi(y)|<EXi∫Y|fY(y)-fY|Xi(y)|dy.i.e.Iicdf(∞)<Iipdf(1). □For two parameters Xiand Xjand p1<p2, ifIi∗(p1)<Ij∗(p1)Ii∗(p1)>Ij∗(p1)andIi∗(p2)>Ij∗(p2)Ii∗(p2)<Ij∗(p2), where “*” can be pdf, cdf or quantile, then there exists p0∈(p1,p2) such thatIi∗(p0)=Ij∗(p0).One only should note thatIi∗(p)is actually a continuous function of p and applying the mean value theorem, one can get the conclusion. □The most commonly used cases of the Minkowski distance are these with the order set to 1, 2 and ∞, which are rectilinear distance, Euclid distance and Chebyshev distance, respectively. Therefore, it is intuitive to use the MD importance measures with order 1, 2 or ∞ for practical applications. The MD importance measures with p=1, i.e.Iipdf(1),Iicdf(1)andIiquantile(1), which measure the area bounded by the original distribution function and the conditional distribution function, have clear geometric meaning. On the other hand, the MD importance measures with p=2, i.e.Iipdf(2),Iicdf(2)andIiquantile(2)can be easily calculated, as argued in (Li, Lu, Feng, & Wang, 2012), as well as admit further sensitivity analysis on the distribution parameters since differentiation is possible (Cui et al., 2012). The MD importance measures with p=∞, i.e.Iipdf(∞),Iicdf(∞), andIiquantile(∞), investigate the most different part between the original distribution and the conditional distribution, which can bring more intuitive information on the shape change (e.g. the upper bound of the shape change) of the distribution function. Therefore, they can be applied when one concerns to what extent the difference between the original distribution function and the conditional distribution function can be.Some new importance measures can be derived from the generalized MD importance measures, i.e.Iipdf(2),Iipdf(∞),Iicdf(∞),Iiquantile(2)andIiquantile(∞). We first study the application of the PDF-based measuresIipdf(2)andIipdf(∞)to the Ishigami function; then the CDF-basedIicdf(∞)and quantile-basedIiquantile(2)andIiquantile(∞)are illustrated with a fault tree model.We first study the PDF-based importance measuresIipdf(2)andIipdf(∞), and use the Ishigami function to illustrate their application (Borgonovo, 2007). The mathematical expression of the Ishigami function is:(14)Y=g(X)=sinX1+asin2X2+bX34sinX1,where Xiare assumed to be independently and uniformly distributed between −π and π, and a and b are respectively set to 5 and 0.1 as in (Borgonovo, 2007).To estimate importance measures of Xi, we first generate N=1000 samples with the Latin Hypercube Sampling (LHS) technique (Helton & Davis, 2003; McKay, Beckman, & Conover, 2000). The corresponding realization of Y can thus be calculated and its PDF, i.e. the original fY(y) can be estimated by the kernel density estimation. Then we apply the substituted column design method (Saltelli, 2002; Saltelli et al., 2010) to construct the conditional samples and estimate the conditional PDFfY|Xi(y)resorting to the kernel density estimation. A detailed procedure for estimatingfY|Xi(y)is available in, e.g. (Wei, Lu, & Yuan, 2012). The PDF-based importance measures can be obtained according to Eqs. (5) and (6).Iipdf(2)andIipdf(∞)for each Xiare listed in Table 1andIipdf(1)is also given for comparison. The ranking of each parameter is in the parentheses.From the second and the third columns of Table 1, it can be seen that X2 is the most important parameter, X1 is the second important parameter while X3 is the least important one. On the other hand, according toIipdf(∞)we know that X1 is the most important one. So MD importance measures with different orders can produce different parameters rankings and it should be careful when determining the most influential parameter. Though different rankings can be derived from different MD importance measures, combining these three measures, we can have a deeper insight about how each parameter affects the output if its uncertainty is removed.I1pdf(1)<I2pdf(1)andI1pdf(2)<I2pdf(2)indicate that on average the shape of the PDF of Y would change the most over the whole range if the uncertain in X2 is removed;I1pdf(∞)>I2pdf(∞)indicates that on average the PDF of Y is most likely to suffer great shift at certain points if the uncertain in X1 is removed. Of course, X3 is the least important parameter and requires least attention since all the three measures give the same ranking for it.Remark 4One may note thatIipdf(p)is decreasing with p in the above example, which reveals that the monotonically increasing property may not hold if∫Yδ0[fY(y)]dy>1.Different orders will bring different rankings, because different order p emphasizes different part of the distributional shift in Y. If p is large, the part wherefY|Xi(y)differs from fY(y) the most is emphasized, and in turn the part wherefY|Xi(y)is close to fY(y) is weakened. The opposite happens if p is small.We use a fault tree model in (Iman, 1987; Liu & Homma, 2010) to illustrate the importance measuresIicdf(∞),Iiquantile(2)andIiquantile(∞), where the Boolean expression of the fault tree is(15)Y=X1X3X5+X1X3X6+X1X4X5+X1X4X6+X2X3X4+X2X3X5+X2X4X5+X2X5X6+X2X4X7+X2X6X7.In the above expression, Y represents the occurrence frequency of the top event of the fault tree model. X1 and X2 are initial events and are expressed as the number of occurrences per year. X3,…,X7 are basic events and are expressed as failure probabilities. All the seven inputs are independent from each other and follow lognormal distributions. The corresponding mean values and error factors are listed in Table 2.Similar to the Ishigami case, we use Monte Carlo simulation to obtain the importance measures of Xi. N=1000 samples are generated by LHS technique to estimate the CDF or the quantile function by kernel density estimation. Different MD importance measures of each Xiare given in Tables 3 and 4. The rankings of the parameters are given in the parentheses.As can be seen from Table 3, all the three CDF-based MD importance measures give the same importance ranking for the inputs. It is thus reasonable to believe that X2 is the most important inputs while X3 is the least important inputs.Table 4 lists the quantile-based MD importance measures of Xiand the corresponding rankings. The second column is identical to that of Table 3, sinceIicdf(1)andIiquantile(1)are the same by definition. Inconsistent with the results of the other five importance measures, the importance rankings of X5 and X6 as well as the rankings of X1 and X7 are swapped according toIiquantile(∞). This is possible since the importance of X5 and X6 (X1 and X7) are fairly close to each other according to other five importance measures and different measures emphasize different aspects of the importance. Combining the results of these six importance measures, the ranking of the inputs should be: X2≻X6≻X5≻X4≻X7≻X1≻X3.In practice, people should choose a proper importance measure to accommodate the situation since different importance measures reflect different aspects of the influence on the output. Alternatively, people can calculate different MD importance measures and combine these results to achieve a balanced ranking. For instance, in this fault tree model, the CDF-based MD importance measures with order 1, 2 and infinity give the exactly the same ranking for the seven inputs, then we are convinced to believe that the ranking is reasonable.

@&#CONCLUSIONS@&#
This paper presented a general formulation of moment-independent importance measures used in system reliability analysis. It is noted that almost all the existing moment-independent importance measures are based on Minkowski distance (MD). We have thus generalized the definitions of the existing importance measures by Minkowski distance. Three classes of MD importance measures are proposed: PDF-based, CDF-based and quantile-based MD importance measure. Their properties are investigated and some new and promising moment-independent importance measures are further studied as special cases of the proposed MD importance measures.The proposed importance measures reveal the significance of the input by the expected Minkowski distance between the original distribution function and the conditional distribution function of the output, which compress less information at the begin (the distribution function is apparently more informative than the variance, for instance). So it is more sensitive in identifying the important factors. Indeed, the variance-based measures sometimes are not able to identify the important input (Plischke, Borgonovo, & Smith, 2012). Additionally, the proposed measures have little restriction on the distribution of the inputs or the output (unlike some statistical test-based measures, e.g., measures in Section 6.6 in (Helton et al., 2006)), which indicates that the measures proposed in this paper are more applicable in real applications.In practice, the joint effect resulting from the variable interaction might be of concern, and measures proposed in this paper can be readily extended to the multiple variable case. For instance, the joint PDF-based importance measure for{Xi1,…,Xir}can be defined asIi1,…,irpdf(p)=EXi1,…,Xir∫YfY(y)-fY|Xi1,…,Xir(y)pdy1p=∫Xi1,…,irfXi1,…,Xir(xi1,…,xir)∫Y|fY(y)fY|Xi1=xi1,…,Xir=xir(y)|pdy1pdxi1…dxir,which measures the joint influence of the uncertainty in group{Xi1,…,Xir}on the uncertainty of the output. The CDF-based and the quantile-based importance measures for multiple inputs can be defined as well and their properties would be a direction for further study.It is pointed out that MD importance measures with different order might lead to different rankings of parameters and suggestions for order selection are proposed. Specifically, it is suggested to combine MD importance measures with different orders to achieve a reasonable parameter ranking. On the other hand, further studies on the comparison between different MD importance measures should be carried out to give more detailed guidance on the selection of the proper importance measure.In addition, efficient calculation of the MD importance measures is urged. Generally speaking, the computational effort required by the moment-independent importance measures consists in two aspects: running the model to obtain samples to estimate the distribution function and calculating the Minkowski distance between the original distribution function and the conditional distribution function. The latter involves numerical integration techniques and its computational intensity is often negligible compared with the former, especially when the model is associated with the long-running computer codes. Studies towards efficiently estimating the moment-independent importance measures have been carried out, and techniques therein can also be used to estimate the MD importance measures (Castaings, Borgonovo, Morris, & Tarantola, 2012; Plischke et al., 2012). How to further reduce the required model runs to estimate the MD importance measures would be our future study.