@&#MAIN-TITLE@&#
Fast solver for some computational imaging problems: A regularized weighted least-squares approach

@&#HIGHLIGHTS@&#
Regularized weighted least-squares framework.Conditioning estimation and improvement.Fast solver for image interpolation, filtering and registration.Fast solver for gradient-vector flow estimation.

@&#KEYPHRASES@&#
Regularized weighted least-squares,Preconditioned conjugate gradient,Preconditioning,Condition number,

@&#ABSTRACT@&#
In this paper we propose to solve a range of computational imaging problems under a unified perspective of a regularized weighted least-squares (RWLS) framework. These problems include data smoothing and completion, edge-preserving filtering, gradient-vector flow estimation, and image registration. Although originally very different, they are special cases of the RWLS model using different data weightings and regularization penalties. Numerically, we propose a preconditioned conjugate gradient scheme which is particularly efficient in solving RWLS problems. We provide a detailed analysis of the system conditioning justifying our choice of the preconditioner that improves the convergence. This numerical solver, which is simple, scalable and parallelizable, is found to outperform most of the existing schemes for these imaging problems in terms of convergence rate.

@&#INTRODUCTION@&#
In this paper we propose to solve some classical imaging problems with a unified quadratic optimization perspective. These topics include data smoothing and completion, edge-preserving filtering, gradient-vector flow estimation, and image registration. We are particularly interested in high-performance numerical solvers for these problems, as they are widely used as building blocks of numerous applications in many domains such as computer vision and medical imaging [1].Concretely, we look at the framework of the regularized weighted least-squares (RWLS):(1)argminu:RD→RJ(u):=∫RDw(x)(u(x)−u0(x))2dx+γ2α∫RD|Lαu(x)|2dxHere,u0(x)∈Rrepresents the observed measurement at pointx∈RD, andu(x)the data to estimate.w(x)⩾0are non-negative weights, which can be considered as confidence levels of the measurements.Lαu:RD↦Rnrepresents some regularization operatorLαapplying a penalty on u. We will restrictLαto be a linear fractional differential operator of orderα>0, such as the gradient and the Laplacian. Further,γ>0is a trade-off parameter between the weighted data-fidelity term and the regularity-penalty term.In this work we will focus on the discrete RWLS problem, or the discrete counterpart of Eq. (1):(2)argminu∈RNJ(u):=‖W12(u−u0)‖2+γ2α‖Lαu‖2Here,u0∈RNis the vector of the measurements of length N. For multi-dimensional measurements, the data are assumed to be vectorized in the lexicographical order.W∈RN×Nstands for a diagonal weighting matrix with the weights on its diagonalWi,i=wi⩾0fori=0,…,N−1.Lαin the discrete setting will be a matrix representing the differential operator and we keep the same notation. It will be clear (see Section 4) that each of our aforementioned imaging problems fits Eq. (2) by choosing a particular set of weights W and a particular regularization operatorLα.Our main contribution here is proposing an efficient preconditioned conjugate gradient (PCG) scheme which solves RWLS, and hence the above imaging problems. We provide a detailed analysis of the system conditioning justifying our choice of the preconditioner that improves the convergence. Surprisingly, this simple solver is found to outperform most of the state-of-the-art numerical schemes proposed for those problems. In particular, the convergence rate of PCG is spectacular, with a gain up to an order of magnitude observed in some of our experiments. Additionally, the PCG has the advantages of being easily implementable, scalable and parallelizable.This paper is organized as follows. Section 2 describes in detail the RWLS framework, and the proposed PCG solver. Section 3 analyzes the choice of the preconditioner by showing its potential in reducing the condition number of the problem and hence improving the convergence rate. Then, Section 4 presents the different imaging problems revisited and solved by the RWLS approach. We show the superior performance of our method compared to various existing schemes. We also discuss an extension of the RWLS model in Section 5. Our conclusions are drawn in Section 6. Finally, mathematical details are deferred to the appendices.Let us use an example in the 1-dimensional (1D) RWLS to introduce our notations and present our main results. The setting can be easily extended to multi-dimensional cases (Section 2.5).Consider the following RWLS in the continuous setting where the regularization operator is the first derivative (i.e.,Lα=d/dxwithα=1):(3)argminuJ(u)=∫Rw(x)(u(x)−u0(x))2dx+γ2∫Ru′(x)2dxThis choice makes Eq. (3) a Dirichlet regularized regression problem. Its solution is the stationary point to the associated Euler–Lagrange equation:(4)w(x)u(x)−γ2u″(x)=w(x)u0(x),x∈RIn the discrete version, the operatorL1will be represented by a first-order finite-difference matrix. For example, letL1be the following circulant matrix (Eq. (5)), which corresponds to a filterg1=[1,−1]/h1with a periodic boundary condition.(5)L1:=1h1[−110⋯00−11⋯0⋮⋱⋱⋱⋮0⋯0−1110⋯0−1]Hereh1>0represents the finite-difference spacing.To solve Eq. (2), one sets the gradient ofJ(u)to zero, and obtain a linear system which is no more than the discrete counterpart of Eq. (4):(6)Au=b,whereA:=W+γ2L1⁎L1andb:=Wu0We usedL1⁎to denote the conjugate transpose ofL1. It follows that(−L1⁎L1)is a Hermitian matrix which represents a second-order differential filter [2]g2=[1,−2,1]/h12. In addition, A is Hermitian and semi-positive definite.L1is diagonalizable by the fast Fourier transform (FFT) matrix F and its k-th eigenvalue isλk=(e−jωk−1)/h1withωk:=2πk/N:L1=F⁎ΛF,Λ:=diag[λ0,λ1,…,λN−1]Due to the orthonormality of FFT, one hasF⁎F=Iwhere I is the identity matrix. Therefore A can be rewritten as:A=W+γ2F⁎Λ˜F,Λ˜:=|Λ|2:=Λ⁎ΛwhereΛ˜is the diagonal matrix of the eigenvaluesλ˜kofL1⁎L1which are given byλ˜k:=|λk|2=[2h1sin(ωk/2)]2.The FFT choice above is clearly due to the assumed periodic boundary condition. More generally, the Hermitian matrixL1⁎L1always possesses an orthonormal eigen-decomposition:L1⁎L1=B⁎|Λ|2B,|Λ|2:=diag[|λ0|2,|λ1|2,…,|λN−1|2]where B is some orthonormal matrix, and(|λk|2)k=0,…,N−1are the eigenvalues ofL1⁎L1written in the modulus form to emphasize their non-negative nature. In practice, the basis B will represent trigonometric transforms, i.e. FFT, DCT (discrete cosine transform), and DST (discrete sine transform), with a periodic, an even symmetric, and an odd-symmetric boundary conditions [2] respectively imposed on the matrixL1⁎L1.Consequently, for any orderα>0one can defineLα⁎Lαto be a fractional differential operator such that:(7)Lα⁎Lα:=B⁎|Λ|2αB,|Λ|2α:=diag[|λ0|2α,|λ1|2α,…,|λN−1|2α]We will keep noting the spectrum ofLα⁎Lαby(8)Λ˜:=|Λ|2α,λ˜k:=|λk|2αIn the subsequent presentation, we will concentrate on the periodic boundary condition (i.e.,B=F).Similar to Eq. (6), for an arbitraryα>0, the minimizer of Eq. (2) is the solution to the following linear system:(9)Au=b,whereA:=W+γ2αLα⁎Lαandb:=Wu0where A is Hermitian and semi-positive definite, and can be written as:(10)A=W+γ2αF⁎Λ˜FThe k-th eigenvalue ofLα⁎Lαis given byλ˜k=|λk|2α=[2h1sin(ωk/2)]2α. These definitions will be extended to multi-dimensional case in Section 2.5.We keep considering the 1D RWLS problem. If the weights are everywhere constant (sayW=w¯Ifor some constantw¯), A has an explicit inverse. The solution is given by a linear filtering:(11)u=A−1b=F⁎w¯(w¯I+γ2αΛ˜)−1Fu0In plain words, Eq. (11) signifies:(i)take the Fourier transform ofu0;weight the spectrum bySk:=w¯/(w¯+γ2αλ˜k)in a pointwise manner;take the inverse Fourier transform.Regarding the case of non-constant weights, A no longer has an explicit inverse in general. However some asymptotic analysis sheds light on the expected behavior of the solution.Suppose that the weights are nowhere zero,α=1and a sufficiently small γ such that one can consider the first-order approximation of the solution:(12)u=A−1b=(W+γ2L1⁎L1)−1Wu0=(I+γ2W−1L1⁎L1)−1u0≈(I−γ2W−1L1⁎L1)u0=u0−γ2W−1L1⁎L1u0It can be seen that Eq. (12) represents a step of diffusion onu0where the step length is controlled byγ2W−1. Clearly, a data point associated with a large weight has a small step length and will undergo little change, while a point with a small weight (or large step) will tend to be blurred out by the diffusion process.Generally we resort to a PCG scheme for iteratively solving the linear systemAu=b. Let us point out that A is strictly positive definite if the null space of W and that ofLαdo not intersect. This is the case in Eq. (10) as long as the weights are not all zero.Our preconditioner is formulated as(13)M:=(νI+γ2αF⁎Λ˜F)−1=F⁎HFwhere(14)H:=(νI+γ2αΛ˜)−1The scalar parameterν>0is tunable. Note that Eq. (13) can be interpreted as a filter with the spectrum defined by Eq. (14). The PCG iteratively solves the systemMAu=Mbwhich is equivalent toAu=bbut has a better convergence rate when M is properly chosen. The analysis and the choice of the preconditioner are detailed in Section 3.In practice, PCG method does not need to explicitly store the huge matrices A and M. Instead, given any vector x one only needs to implement the matrix–vector applicationsAxandMx. These operations only involve the FFT, and pointwise additions and multiplications in the Fourier and in the signal domains. Moreover, the eigenvaluesΛ˜can be pre-computed for a given regularization operator. As a consequence, the PCG solver runs very fast and is inherently scalable and parallelizable.There is no unique way of extending Eq. (2) to multi-dimensional situations. For any extension, we only need to care about the form ofLα⁎Lαin Eq. (9). In this paper, we chooseLα⁎Lαto be any discrete version of the (negative) fractional Laplacian of order α:(15)(−Δ)α:=(−∂2∂x12−∂2∂x22−⋯−∂2∂xD2)αwhere D is the dimension. Note that(−Δ)αshould be understood in the Fourier sense. It is associated with the spectrum|ωs|2αwhereωsis the spatial frequency in the continuous domain.The rationale of this choice relies on its consistency with two important regularization kinds, i.e., the continuous RWLS energy (Eq. (1)) where the regularization term is either set to: (a) the Dirichlet energy, or to (b) the thin-plate spline bending energy with a periodic boundary condition. In both cases, the Laplacian operator shows up in the associated Euler–Lagrange equations.For a D-dimension dataset of sizeN1×N2×⋯×ND, the eigenvalues ofLα⁎Lαare written in the multi-index form:(16)λ˜k=(∑d=1D|λkd(d)|2)α,k∈{(k1,k2,…,kD):kd=0,1,…,Nd−1}whereλkd(d)is thekd-th eigenvalue of the matrix representing the 1D first-order derivative along the dimension d, i.e.,∂/∂xd. Under the periodic boundary condition, one has(17)λ˜k=(∑d=1D[2hdsin(ωkd2)]2)α,ωkd:=2πkd/NdFor a positive-definite linear systemAu=b, the condition number of the matrix A is defined as the ratio between its maximum eigenvalueλmax(A)and its minimum oneλmin(A), i.e.,κ(A):=λmax(A)/λmin(A). It is known that the convergence of a conjugate gradient method is faster if the condition number is smaller (i.e., closer to 1) [3], or in other words, if the spectrum of A is more compact. In this sense,κ(A)can also be deemed as a measure of the spectral compactness. The maximal compactness is achieved by a scalar matrixA=cI(c>0)where one hasκ(A)=1.The purpose of a preconditioner is to improve the conditioning of a linear system. In our case the preconditioner M is written in the form of Eq. (13). In the subsequent sections, we make detailed analysis of the system conditioning before and after introducing the preconditioner M (Section 3.1), and provide suggestions of choosing the parameter ν (Section 3.2).Exact computation of the condition number turns out to be prohibitive for large data, as we have to evaluate the eigenvalues of a huge-size matrix. Alternatively, we will introduce another spectral compactness measure based on the coefficient of variation (CV) of the spectrum. Although different from the condition number, the CV is found to be directly related to a certain upper bound of the condition number. It is in this sense that the CV can be deemed as an estimator of the system conditioning. Moreover, this measure possesses an explicit expression that can be easily evaluated.Let us first establish some notations and preliminary results. Given an N by N matrix G with real eigenvalues, we define:(18)μ(G):=1Ntr(G)(19)σ2(G):=μ(G2)−μ2(G)(20)τ(G):=σ(G)μ(G)wheretr(G)denotes the trace of G. It can be seen that Eq. (18), Eq. (19) and Eq. (20) represent the average, the variance and the CV of the spectrum of G.We recall the form of the preconditioner in Eq. (13). The diagonal matrix H (Eq. (14)) contains on its diagonal the eigenvalues of M written as(21)ηk=(ν+γ2αλ˜k)−1,k=0,…,N−1We usew¯to denote the average of the weights, andσw2for the sample variance of the weights:(22)w¯:=1N∑i=0N−1wi(23)σw2:=1N∑i=0N−1(wi−w¯)2=1N∑i=0N−1wi2−w¯2We also define(24)Wˆ:=FWF⁎=[q0,q1,…,qN−1]whereqiis the i-th column vector. We useqj,ito denote the j-th element ofqi. Note that since W is diagonal,Wˆhas a periodic structure such that it can be written as a Kronecker product of circulant matrices.Eigenvalues of A are real positive, and so do those ofMAsince M is a symmetric positive definite matrix. We note for anym>0:(25)βm(A):={μ(A)+m⋅σ(A)μ(A)−m⋅σ(A)ifμ(A)>m⋅σ(A)+∞otherwiseβm(A)will become an upper bound ofκ(A)as m increases. As a result, one way to compare the system conditioning before and after preconditioning is to assessβm(A)andβm(MA)for a certain m. We do not bother to find the optimal m that results in the tightest bounds as we have the following relation:(26)∀A1,A2,τ(A1)⩾τ(A2)⟺βm(A1)⩾βm(A2)for anym>0Eq. (26) implies that comparingβm's and comparing τ's are equivalent since Eq. (26) holds for any m. Comparing the CV's no longer involves this parameter and consequently, to tell that a preconditioner M is potentially effective, one only needs to verify the relationτ(MA)<τ(A).Propositions 1 and 2 show the CV of A and that ofMA, respectively. Reader can find the proofs in the appendices.Proposition 1The CV of the matrixAis given byτ(A)=σ(A)μ(A)where(27)μ(A)=w¯+γ2αμ(Λ˜)(28)σ2(A)=σw2+γ4ασ2(Λ˜)The CV of the matrixMAis given byτ(MA)=σ(MA)μ(MA)where(29)μ(MA)=1+(w¯−ν)μ(H)(30)σ2(MA)=1N∑i=0N−1ηi∑j=0N−1ηj|qj,i|2+(w¯−ν)2σ2(H)−w¯2μ(H2)withH,ηiandqj,idefined in Eq.(14), Eq.(21)and Eq.(24)respectively.We also introduce an upper bound ofσ2(MA)which is noted asσ˜2(MA)as precised in Proposition 3. The CV computed from this upper bound is written asτ˜(MA):=σ˜(MA)/μ(MA).Proposition 3We have the relationσ˜2(MA)⩾σ2(MA)where(31)σ˜2(MA):=σw2μ(H2)+(w¯−ν)2σ2(H)The purpose of introducing this approximation is to simplify Eq. (30) by avoiding computing the high-order harmonicsqj,i. Clearly, any preconditioner M verifyingτ˜(MA)<τ(A)automatically satisfiesτ(MA)<τ(A). One can see that only the first and the second statistical moments of the weights are involved in the quantitiesτ(A)andτ˜(MA).In practice, we are more interested in the asymptotic behavior of the CV's for the case of large datasets. In Eq. (17), if we letNd→+∞and write(32)λ˜(ω):=(∑d=1D[2hdsin(ωd2)]2)α,ω∈A:={(ω1,ω2,…,ωD):ωd∈[0,2π)}Eq. (27) and Eq. (28) become respectively:(33)μ(A)=w¯+γ2α(2π)D∫Aλ˜(ω)dω(34)σ2(A)=σw2+γ4α[1(2π)D∫Aλ˜(ω)2dω−(1(2π)D∫Aλ˜(ω)dω)2]Likewise, Eq. (29) and Eq. (31) respectively tend to:(35)μ(MA)=1+w¯−ν(2π)D∫A(ν+γ2αλ˜(ω))−1dω(36)σ˜2(MA)=σw2+(w¯−ν)2(2π)D∫A(ν+γ2αλ˜(ω))−2dω−(w¯−ν)2(2π)2D[∫A(ν+γ2αλ˜(ω))−1dω]2To understand the behavior of our preconditioners, we compute the ratioRCV:=τ˜(MA)/τ(A)for the 2D case (derived from Eq. (33)–(36)) as a function ofν/w¯for different weight moments and for different values of γ and α. Without loss of generality, we suppose the weights to be normalized within[0,1]. The weight mean and variance are uniformly sampled in the areaA={(w¯,σw2):0<w¯⩽1and0⩽σw2⩽w¯(1−w¯)}. Note that the variance upper bound comes from the Bhatia–Davis inequality [4]. α and γ are sampled within[1,2]and[0.1,5]respectively. Each combination of a weight mean, a weight variance, a value of γ and a value of α produces a curve ofRCV(as a function ofν/w¯). We have about3×104combinations in total.Fig. 2shows some of them forα=1andγ=1, curves for the other cases being similar. One can see that the highest efficiency of the preconditioners (i.e., the valleys ofRCV) is achieved for ν typically ranging fromw¯up to several multiples ofw¯. Indeed, the optimal ν is found within[w¯,5w¯]for more than94%among all cases. For a given RWLS problem, ultimately one could use some line search methods in this range to find the optimal ν which minimizesτ˜(MA). Practically, we chooseν=w¯in all our experiments of Section 4. Out of all the combinations above, this value of ν makesτ˜(MA)strictly lower thanτ(A)(i.e.,RCV<1) for more than90%cases. In other words, the preconditioner M is effective in most of the time. We point out that this choice also makesM−1the best approximation to the matrix A in the least squares sense.Data smoothing with missing values refers to filling unobserved pixels with some estimates computed from the observed pixels. This problem is straightforwardly addressed by the RWLS (Eq. (2)) where the weights are typically binary by taking zero values for unobserved areas and values of 1 for the observed ones. This application is studied in detail by Garcia [5] where the issues of robustness and of choice of γ are emphasized. Numerically, the author proposed Keller's preconditioned gradient descent solver [6] for RWLS.Fig. 3compares the performances of Keller's approach and the PCG on a 2-dimensional (2D) image smoothing example. The original image (of size256×256) is composed of a mixture of Gaussian functions. The observed image is generated by first adding a Gaussian noise, followed by dropping off about 2/3 data points. The missing data are first picked out randomly, then cropped out by 4 squares of size50×50each. We setα=2andγ=1. An even symmetry boundary condition is assumed and the DCT is employed to be the diagonalization basis ofLα⁎Lαas in [5].Compared to the PCG result, the square masks are still partially visible in the estimate of Keller after 100 iterations. This shows that the PCG diffuses faster the observed information into the unfilled areas. The energy evolutions confirm that PCG converges much faster than Keller's iterations. The PCG is also more accurate: after 100 replications, the mean squared error (MSE) of the PCG estimates is evaluated to be 0.015, in comparison to 0.48 for Keller.We also measured the execution time of both methods for indicative purpose. To achieve the same MSE target of 0.2 in this example, our approach took 0.9 seconds in contrast to Keller's method which took 11.5 seconds. The time was measured on a PC with 3.33 GHz Intel Xeon® CPU. Both methods are implemented in Matlab®.Fig. 4shows a more realistic case where we try to restore an image with 1/3 randomly sampled data available. Here the underlying image contains both low-frequency information and discontinuities. As we have shown in Section 2.3, one can view the weights in RWLS as actors of boundary conditions in a diffusion process. The boundary conditions are not strictly imposed but handled in a soft way through these weights. Eq. (12) shows that pixels receiving large fidelity weights prevent the diffusion induced by the regularity term at those pixels. In this sense, RWLS is very flexible as the weights automatically manage boundary conditions of any spatial shapes.Here the observed pixels serve as our boundary condition which are associated with weightswi=1, and the unobserved pixels with zero weights. We setγ2=0.1. Therefore, our RWLS will interpolate the unobserved areas while keeping observed values almost unchanged. Note that this procedure is quite similar to the interpolation based on partial differential equations [7].In the purpose of regularizing image while preserving discontinuities, one may apply the RWLS framework by using large fidelity weights on the pixels with discontinuities and small ones for homogeneous regions. According to Eq. (12),γ2W−1plays a similar role as the diffusion-rate function in the Perona–Malik anisotropic diffusion model [8].Fig. 5shows a filtering example using RWLS usingγ=0.5andα=1. Our weights are defined as follows:wi:=1−exp(−|∇I(xi)|2/K2)The parameter K can be considered as a reference edge-saliency level above which the diffusion will be hampered. Parallely, homogeneous areas encircled by the salient edges will be associated with small weights closed to zero. It follows that the remaining Dirichlet regularity term in Eq. (2) favors an approximation with harmonic functions on those areas.11This is due to the fact that Dirichlet penalty leads to Laplace equation as Euler–Lagrange equation with harmonic functions as solutions.Consequently, this results in piecewise smooth images.The gradient-vector flow (GVF) [9] is widely used in deformable-model based applications in order to extend the attraction range of external forces (see [1] and references therein). GVF in 2D continuous setting consists in solving the following variational problem:(37)argminu:=[u1,u2]T∫Ωg(|∇v(x)|)|u(x)−∇v(x)|2dx+γ2∫Ω|∇u1(x)|2+|∇u2(x)|2dxHere Ω is the image domain,x=[x1,x2]T∈Ω,∇v(x)is the observed gradient field, andu(x):=[u1(x),u2(x)]Tis the sought extended field. g is an “edge” weighting function which is typically monotone increasing and smooth.It can be seen that the componentsu1andu2are decoupled in the optimization problem (37). Solving (37) is equivalent to minimizing foru1andu2independently:(38)argminul∫Ωg(|∇v(x)|)[ul(x)−∂v∂xl(x)]2dx+γ2∫Ω|∇ul(x)|2dxwithl=1or 2. The Euler–Lagrange equation of Eq. (38) is given byg(|∇v(x)|)(ul(x)−∂y∂xl(x))−γ2Δul(x)=0. Therefore, it is straightforward to convert Eq. (38) to our discrete RWLS framework of Eq. (2) by setting:wi:=g(|∇v(x[i])|),u0[i]:=∂v∂xl(x[i]),Lα⁎Lα=−Δ,α=1wherex[i]is the position of the i-th pixel.A vast number of schemes have been proposed to solve Eq. (37) such as the gradient-descent [9], operator-splitting based schemes [10], augmented Lagrangian method [11], and the alternating-direction methods [12]. Boukerroui [12] also provides a comparative study of the different numerical approaches.In our comparative study, we include the alternating direction explicit scheme (ADES) solver which is recommended in [12], and the augmented Lagrangian (AL) based approach [11]. For ADES we followed the advice in [12] by including a left-right domain flipping in our iterations.Our comparison is based on a synthetic image of a centered disk (Fig. 6(a)) as in [12]. The gradient of this image (i.e., ∇v), which is nonzero only in a local vicinity of the disk boundary, is used to construct the edge-weighting function:g(t):=1−e−t2We setγ2=1.5for all three methods. In ADES, the time step in the iteration scheme is set to 10 (see [12] for details).As [12], our goal is to compare the orientation accuracy of the estimated vector fields. To build the ground truth, we compute analytically the orientation at each pixel of the image with respect to the disk center. These orientations are color-coded for[−π,π)and shown in Fig. 6(c). The orientations ofu(x)derived from the different GVF estimates are demonstrated in Fig. 6(d), (e) and (f), as well as their absolute residual in Fig. 6(g), (h) and (i) for the three approaches at the end of 15 iterations.Visually, some bias for pixels far from the image center can be seen in ADES, and some inaccurate estimates in the AL method lie around the image corners. Generally, we found that more iterations are necessary for ADES to reduce the bias, and for AL to diffuse the local gradient information sufficiently far away to the image boundaries and corners. Finally, the PCG result looks the closest to the ground truth.Quantitatively, the evolutions of the root mean squared error (RMSE) of the orientations ofu(x)(measured in degrees) for the three methods are shown in Fig. 7. One can see that the PCG needs very few iterations to reduce most of the error. This best performance is followed by ADES, and then AL. At the end of the iterations, we haveRMSE=0.71degrees for PCG, 4.17 degrees for ADES, and 11.1 degrees for AL.In terms of execution time, in order to attain an RMSE less than 5 degrees, it took 17.4 seconds for ADES, 1.04 seconds for AL, and 0.16 seconds for PCG. The time was evaluated on the same PC as in Section 4.1.Registration between two 2D images consists in seeking a smooth deformation field that approximately maps an observed image to a reference. We are particularly interested in Demons registration algorithm [13], which is extensively employed in medical imaging [14,15] for its simplicity and fast performance.The deformation field is derived as a solution of a variational problem such that the field is the best tradeoff between an image similarity measure and a field regularity measure. Here we consider the following minimization problem using the sum of squared difference (SSD) as our similarity term:(39)argminuJ(u)=∑i(v(xi+ui)−v0(xi))2+γ2α‖Lαu‖2where v andv0are the observed image and the reference image respectively, and u the sought deformation field.We will cast the problem Eq. (39) into our RWLS framework using the approximation of [14]. For this purpose, we first adopt a first-order approximation of the SSD measure, or in other words the assumption of a small deformation u.(40)SDi:=(v(xi+ui)−v0(xi))2≈(∇v(xi)Tui+v(xi)−v0(xi))2=(v(xi)−v0(xi))2+2(v(xi)−v0(xi))∇v(xi)Tui+uiT∇v(xi)∇v(xi)TuiWe then approximate the Hessian tensor∇v(xi)∇v(xi)Tin Eq. (40) by the optimal scalar matrix in the least squares sense12Tr(∇v(xi)∇v(xi)T)I. Completing the squares in Eq. (40), the problem Eq. (39) is converted into:(41)argminu∑i|∇v(xi)|2‖ui−uˆi‖2+2γ2α‖Lαu‖2(42)withuˆi=2v0(xi)−v(xi)|∇v(xi)|2∇v(xi)It can be seen that Eq. (41) fits our RWLS framework with the weights given bywi:=|∇v(xi)|2. This dependency of the weights on the gradient is very intuitive: the deformation of the image is entirely encoded by the pixels having high gradients (i.e., discontinuities like contours and edges), while a small displacement of a pixel inside a homogeneous area is totally invisible.Let us also point out that the first order approximation in (40) is valid for small displacement u. In registrating two images with large deformation, we basically need to solve a sequence of problems (39) where in each step, v is replaced by the estimate from the previous warped result. Consequently, the final field is estimated as a successive composition of the small deformation fields.In parallel, Demons registration consists first in computing the field Eq. (43) then followed by a Gaussian smoothing on bothuˆand the cumulated composite deformation field [13,15]. Compared to Eq. (42), Eq. (43) possesses an additional term in the denominator for preventing instabilities due to small gradient values. Note that this is not a problem for the RWLS model as small gradients are weighted by small values in Eq. (41).(43)uˆi=v0(xi)−v(xi)|∇v(xi)|2+ϵ(v0(xi)−v(xi))2∇v(xi)Fig. 8shows an example comparing Demons method and the RWLS approach on a synthetic case where we register a disk onto a peanut shape. The top three rows correspond to Demons warping progression using three increasing smoothing Gaussian widths (σ=1,σ=3, andσ=6). The value of ϵ is set to 0.01. The last row demonstrates the warping results given by RWLS. The corresponding deformation field estimates are presented in Fig. 9. Fig. 9 clearly shows that Demon requires a large enough Gaussian width to guarantee the regularity of the warping (in our exampleσ⩾3). However, as the Gaussian width increases the displacement magnitudes shrink implying that many more warpings are needed and hence can be time consuming.Fig. 10presents the evolution of the registration errors (defined as the norm of the difference between the warped image and the reference) as a function of the number of warpings. For a fixed error target, considerably fewer warpings are required in our PCG-based approach. The error reduction is slow in Demon's iterations and eventually almost stagnating.In this section, we will briefly discuss a generalization of the RWLS framework and its potential in real applications. We will consider its extension by weighting both the data term and the regularization term. In addition, we will consider a general linear regularization operator L:(44)argminu:RD→RJ(u):=∫RDw(x)(u(x)−u0(x))2dx+γ∫RDv(x)|Lu(x)|2dxHere,v(x)⩾0are the weights applied pointwisely on the regularization penalty. It follows that the generalized discrete RWLS can be written as:(45)argminu∈RNJ(u):=‖W12(u−u0)‖2+γ‖V12Lu‖2where the diagonal matrixV∈RN×Nbears the weightsvi⩾0on its diagonal elements. The minimizer is the solution to the following linear system:(46)Au=b,whereA:=(W+γL⁎VL)andb:=Wu0Mimicking the standard RWLS case, we can solve the system by PCG with a preconditioner M given by(47)M:=(w¯I+γv¯L⁎L)−1wherev¯represents the average value of the weightsvi.The advantage of considering the extended RWLS framework is that, by properly designing the weights v, one may address other penalties such asL1-minimization by this quadratic optimization framework. This point has been exploited by several works in the literature. For example, Daubechies et al. [16] proposed a sparse solution solver in compressed-sensing applications by casting anL1-norm minimization into a sequence of weightedL2-norm minimization problems. In [17], a half-quadratic algorithm [18,19] was applied to solve the total-variation (TV) minimization. The method converts the TV-regularization term∫|∇u(x)|dxinto a weighted quadratic term through the weightsv(x):argminu,v>0Data-Term(u)+∫[v(x)|∇u(x)|2+14v(x)]dxAt each iterative step, the algorithm first minimizes u by fixing v, and then solves the weights v while fixing u. Note that the solution of v given u has a simple form:v(x)=1/(2|∇u(x)|). The updated u and v serve as initializations at the next iteration.The generalized RWLS model allows us to deal with even richer applications under this flexible framework. Here we show its potential on a toy example of texture restoration application in Fig. 11. We simulated a periodic textural pattern by setting random amplitudes on 4 DCT coefficients and zeroing the rest. This image is shown by Fig. 11(a). Then in the spatial domain, we added a white noise (PSNR=30) and randomly kept only5.6%of the samples. The sampling mask is presented in Fig. 11(b). Our goal is to estimate the underlying pattern given the heavily down-sampled image.This textural restoration problem can be formulated as:(48)argminu∫w(x)(u(x)−u0(x))2dx+γ∫|Lu(ω)|dωwhere L represents the DCT transform, andu0the observed noisy and down-sampled image. The weightsw(x)are binary which take value of 1 only at the positions of the spatial sampling. The penalty of theL1-norm onLupromotes a solution with sparse DCT coefficients. Using the same technique of half-quadratic algorithm, Eq. (48) is converted into Eq. (49) by introducing the weights v:(49)argminu,v>0∫w(x)(u(x)−u0(x))2dx+γ∫[v(ω)Lu(ω)2+14v(ω)]dωWhile fixing the weights v, Eq. (49) is a generalized RWLS. The solution of u can be obtained by our PCG schema. While fixing u, the weights v can be explicitly computed. Consequently, we alternatively minimize u and v at each iteration and use the results as initial points for the next iteration. Fig. 11(d) shows the restored texture pattern despite the heavily degraded observations.

@&#CONCLUSIONS@&#
