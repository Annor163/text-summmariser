@&#MAIN-TITLE@&#
Pathological voice detection and binary classification using MPEG-7 audio features

@&#HIGHLIGHTS@&#
MPEG-7 audio descriptors are used for voice pathology detection/classification.Fisher ratio is applied to find the most discriminative features.Using only top five features the detection accuracy reached over 99%.Those five features obtained over 93% accuracy for binary classification.Achieved the best detection rate so far in MEEI database (subset).

@&#KEYPHRASES@&#
MPEG-7 audio features,Dysphonia recognition,Binary pathology classification,Voice pathology detection,

@&#ABSTRACT@&#
ObjectivesA pathological voice detection and classification method based on MPEG-7 audio low-level features is proposed in this paper. MPEG-7 features are originally used for multimedia indexing, which includes both video and audio. Indexing is related to event detection, and as pathological voice is a separate event than normal voice, we show that MPEG-7 part-4 audio low-level features can do very well in detecting pathological voices, as well as binary classifying the pathologies.Patients and methodsThe experiments are done on a subset of sustained vowel (“AH”) recordings from healthy and voice pathological subjects, from the Massachusetts Eye and Ear Infirmary (MEEI) database. For classification, support vector machine (SVM) is applied. An optional feature selection method, namely, Fisher discrimination ratio is applied.ResultsThe proposed method with MPEG-7 audio features and SVM classification is evaluated on voice pathology detection, as well as binary pathologies classification. The proposed method is able to achieve an accuracy of 99.994% with a standard deviation of 0.0105% for detecting pathological voices and an accuracy up to 100% for binary pathologies classification.ConclusionMPEG-7 descriptors can reliably be used for automatic voice pathology detection and classification.

@&#INTRODUCTION@&#
Analysis of acoustic signals of human voice has many purposes and can be seen from different perspectives. From a technological point of view, there is a quick and huge growth for the need to store, code, transmit, recognize and synthesize voice signals. From a health science point of view, it was proved that the human health condition and the pathological status do affect the human voice [1]. If the vocal folds become inflamed, some structural lesion may develop on them or they become not functioning effectively, and hence, the speech production process differ from normal conditions. In cases with disordered voice, speech samples carry symptoms of disorder from their origin. As a result, any abnormality in larynx most likely will affect the quality of voice signal and its characteristics. Some of the most spreading disorders are vocal fold paralysis, vocal fold nodules, adductor spasmodic dysphonia, keratosis and others [2–4]. Many techniques like stroboscopy, laryngoscopy, and endoscopy are widely used by physicians to diagnose voice disorders, aiming to diagnose at early stages of occurrence before they lead to critical conditions. However, these techniques can cause discomfort to the patients due to their invasive nature and they are costly as well. Developing an automated method would save time, cost, and ease the inspection and lead to patients comfort during inspection. The automated methods are increasingly used for the screening of voice disorder to assist the physician. For example, such a tool could be used by otorhinolaryngology surgeons or anesthesiologists to document a healthy voice or to disclose the hoarseness, requiring a more detailed examination by a voice pathologist prior to the operation [29].Voice perturbation and quality measures, such as jitter and shimmer, depend on accurate extraction of fundamental frequency and the amplitude of various waveform types. The extraction method directly affects the accuracy of the measure, particularly if several waveform types (with or without formant structure) are under construction and if noise and modulation are present in the signal [5]. Lieberman proposed one of the first acoustic voice parameters in pathological voice analysis in 1961 [6]. In [7], a method was developed for short-time jitter and the AUC (area under curve) reached 94.82%. Mel-frequency cepstral coefficients (MFCC) are popular features in speech analysis, and have been used also in voice pathology detection. In [8], detection of pathological voice was done by means of Gaussian mixture models and MFCC complemented by frame energy together with their derivatives. The method was applied on a subset samples from the Massachusetts Eye and Ear Infirmary (MEEI) database [12], and a detection accuracy of 94.07% was achieved. In [9], modulation spectral method was suggested to identify pathological voices, and this method was applied on the same subset used in [8], and a detection accuracy of 94.1% was achieved. In [10], a new method which is based on extracting eleven features using nonlinear analysis of time series plus MFCC was introduced; the samples which were used in the experiment are the same samples used in [8], and a detection accuracy of 98.23% was obtained. In [23], another method was developed for detecting voice disorder and for classifying the disorders by generating adaptive wavelets features and applying them to the voice signals from the MEEI database. Many experiments were done and the obtained accuracy for binary pathologies classification rates varied from 80.51% up to 99%. Kaleem et al. used empirical mode decomposition (EMD) for voice pathology detection using sustained vowel and continuous speech, and reported 95.8% detection accuracy [24]. EMD can decompose speech portions into intrinsic mode functions, which can be used for further temporal and spectral analysis. Lee et al. combined vocal tract related features such as the dynamics of formants, and vocal source related features such as fundamental frequency, shimmer, jitter, and harmonics-to-noise ratio (HNR) to improve the accuracy of voice pathology detection [25]. Hariharan et al. investigated four different classifiers, least-square support vector machine, k-means nearest neighborhood, probabilistic neural network, and classification and regression tree, for telemonitoring of vocal fold pathology [26]. They concluded that the least-square support vector machine performed better than the other three classifiers.In this paper we explain and evaluate detection and classification of pathological voices using MPEG-7 low-level audio features. In MPEG-7 part 4, the audio framework contains low-level tools designed to provide a basis for construction of higher-level audio applications [11]. These features are extracted from a subset of the MEEI database [12] with sustained vowel samples. Support vector machines (SVM) are used as a classifier and results are expressed in terms of accuracy, sensitivity, specificity and AUC. The contribution of the paper is to evaluate the MPEG-7 audio low-level features, which were tested in many other detection applications, on voice pathology detection and classification. We also identify the most prominent features of MPEG-7 that can significantly improve voice pathology detection or classification performance.In the proposed method, MPEG-7 audio low level features were extracted from sustained-vowel input signals. An optional feature selection module based on Fisher discrimination ratio (FDR) was applied on the extracted descriptors. SVM classifier is used for detection and classification. Fig. 1shows a general block diagram of the proposed method.MPEG-7 Part 4 audio features can be applied to all forms of audio content. It has restriction neither on the encoding format or medium of the audio, nor on the audio content whether it is with or without music, speech, sound effects or others. The MPEG-7 features are originally proposed for multimedia indexing, which contains both video and audio parts. Since the invention of MPEG-7 features, they have been used in many applications such as speaker recognition, environment recognition [13], audio sports event detection [14], musical instrument classification [15], musical onset detection [16], and some other audio analysis cases. For example, in [27], the authors studied and presented some features for automatic asthma wheezes recognition and one of these features was Audio Spectral Envelope (ASE) which is a part of the MPEG-7 audio features. They conducted experiment by selecting the most relevant features in automatic asthma wheezes recognition to reduce the algorithm computation complexity, and ASE was one of the two shortlisted features among 12 features. Wellhausen and Crysandt presented an audio segmentation technique using MPEG-7 low level audio descriptors as a source of metadata [30]. Ontology-based semantic indexing and retrieval of audiovisual content using MPEG-7 was introduced in [31]. Several MPEG-7 audio features such as ASE, Audio Spectrum Flatness (ASF), Audio Spectrum Centroid (ASC), Audio Spectrum Spread (ASS) were used to detect abnormal situations in audio recording and an accuracy of 91.94% was reported in [32]. A tool for metadata descriptions of surgical video in accordance with the MPEG-7 standard was developed in [33]. Also, MPEG-7 standard and features were incorporated in gastrointestinal-motility monitoring and video assisted bioacoustics application in abdominal sounds pattern analysis [34,35].From the above discussion, we find that though MPEG-7 features are used in different audio analysis cases including bioacoustics, it has never been tested on pathology detection and classification. One of the main objectives of this study is to investigate the feasibility of using the MPEG-7 audio features in an automated voice pathology detection and classification system.The MPEG-7 audio features that we extract are low-level features, which are of two types: scalar and vector. There are 45 features in total; four vector-type features that include ASE (3 features), ASF (22 features), Audio Spectrum Basis (2 features) and Audio Spectrum Projection (2 features). The others are scalar-type features that include Audio Waveform (2 features; Min and Max), Audio Power, ASC, ASS, Audio Harmonicity (2 features; Harmonic Ratio and Upper Limit of Harmonicity), Audio Fundamental Frequency (2 features; Raw and Weight), Log Attack Time, Temporal Centroid, Spectral Centroid, Harmonic Spectral Centroid, Harmonic Spectral Deviation, Harmonic Spectral Spread (HSS) and Harmonic Spectral Variation [11].After obtaining the MPEG-7 features, feature selection in the form of FDR is used to find the most relevant features for detection of pathologies. FDR for each feature (denoted i) is calculated using Eq. (1).(1)FDR(i)=(μ1,i−μ2,i)2(σ1,i2+σ2,i2)where μ1,iand μ2,irepresent the means for classes normal and pathological, respectively, while σ1,iand σ2,irepresent the variances for normal and pathological classes, respectively. A high FDR value refers to a high discriminative power of a certain feature. For a given feature, if the distance between the means of the two classes is high and the intra-class variances are low, the FDR will be high, indicating a good discrimination capability of that feature.In the experiments on MEEI database, five features are constantly found having their FDR>1; these are ASC, ASS, HSS and two features from ASF (9th and 10th bands). These highly discriminative features are described below:•ASC describes the center of gravity of the log-frequency power spectrum and is calculated using Eq. (2), where piis the power associated with frequency fi.ASS describes the second moment of the log-frequency power spectrum and is calculated using Eq. (3).HSS is computed as the average over the sound segment duration of the instantaneous Harmonic Spectral Spread, which is in turn computed as the amplitude weighted standard deviation of the harmonic peaks of the spectrum, normalized by the instantaneous Harmonic Spectral Centroid;ASF describes the flatness properties of the spectrum of an audio signal within a given number of frequency bands and is calculated using Eq. (4), where N is the number of coefficient within a sub-band and cnis the nth spectral power coefficient of the sub-band [11,17].SVM is widely used for data classification and it is known for its high prediction capabilities in many signal processing applications [28]. SVM objective is to find the optimal hyper plane that separates the two classes while maximizing the margin between separating boundary and the closest samples to it (support vector). Given a training set of instance-label pairs (xi, yi), i=1, …, l, where xi∈Rnand yi∈{1, −1}, the optimal hyper plane is defined asf(x)=w¯.x¯+b=0, wherew¯and b are obtained by solving the following optimization problem:Minimize12w¯Tw¯+C∑i=1lεi.Subjecttoyi(w¯Tϕ(xi)+b)≥1−εiandεi≥0.Here, the training vectors xiare mapped into a higher dimensional space by the functionϕ to make the feature vectors of the two classes linearly separable. SVM finds a linear separating hyper plane with the maximal margin in this higher dimensional space. C>0 is the penalty parameter of the error termɛ. It can be noted that for a voice pathology detection task, yi=1 for a pathological sample and yi=−1 for a normal sample.The kernel function that maps the input space into a higher dimensional space can be represented as K(xi, xj)≡ϕ(xi)Tϕ(xj).Many kernels such as linear, polynomial, radial basis function (RBF), sigmoid, etc. can be used in SVM. As RBF gives the best results in many types of detection algorithms, in our current study, we use RBF, which is defined as follows.K(xi,xj)=exp(−γ||xi−xj||2),γ>0Where, ||.||2 is the Euclidean distance between the two feature vectors, and γ is the kernel parameter.The optimal combination of C and γ is accomplished in the experiments by performing loose and fine grid search for these two parameters.In [18], it is mentioned that “the main benefit of using standard corpora is that it allows researchers to compare performance of different techniques on common data, thus making it easier to determine which approaches are most promising to pursue. In addition, standard corpora also can be used to measure current state-of-the-art performance in research areas for particular tasks and highlight deficiencies that require further research”. Thus, the database used in the experiments is the one developed by the MEEI Voice and Speech Labs [12]. The MEEI database contains sustained vowel/AH/recordings by 53 normal speakers with duration of around 3s and by 657 pathological speakers, with a wide variety of diseases, with duration of around 1 second. Since we are interested in vocal pathologies, we used sustained vowel recordings only to ensure that the vocal folds remain in motion during the entire utterance [8]. We used the same subset of the MEEI database which was used in [8–10,19], denoted as MEEIsubset; this subset has sustained vowel/AH/recordings for 53 normal and 173 pathological speakers. The criteria in selecting this subset took into consideration a wide variety of voice disorders and a uniformly distributed gender and age between both normal and pathological classes [19]. A statistical description of the MEEIsubset is shown in Table 1. We chose this subset because we could compare the proposed method with other methods using the same voice files [8–10]. The MEEI database has been thoroughly tested in numerous research works since its development and it is the most widespread and available of all the voice quality databases [20].All the sound files in the MEEIsubset have a sampling frequency of either 50kHz or 25kHz. Therefore, all the files having a sampling frequency of 50kHz are down-sampled to half the original sampling frequency so that all the files in the MEEIsubset have the same sampling frequency (which is 25kHz). This downsampling ensures that all the files have 25K samples per second, and maximum frequency of each file is 12.5kHz. We extracted the MPEG-7 audio features for each file in the MEEIsubset using TU-Berlin MPEG-7 audio analyzer [21]. All the 45 features were considered and their normalized FDR values are shown in a bar graph in descending order in Fig. 2. The least three contributed features, which are Log Attack Time, Temporal Centroid and Spectral Centroid, were removed for further consideration. In the remaining of this paper, therefore ‘all features’ refers to 42 features. The parameters set to extract MPEG-7 audio features were as follows: hop size (frame shift)=10ms; frame size (analysis window)=20ms; low edge (lower edge of the first filter)=250Hz, high edge (higher edge of the last filter; the maximum frequency content of the signal, which is half of the sampling frequency)=12.5kHz, resolution (width of each band)=4 Octave/band, low limit (lower limit of fundamental frequency)=40Hz, high limit (higher limit of fundamental frequency)=500Hz, Mode=Instantaneous (frame level analysis); these values are found to be the optimum in our experiments.The classification was performed using SVM. The RBF kernel was chosen for SVM as it is more general than other kernels (especially linear one) and usually it produces better accuracy and has less restriction than other kernels. For the evaluation, we adopted a ten-fold cross validation approach [36]. In the ten-fold cross-validation, the normal files and the pathological files’ features are randomly divided into ten equal groups each. In each iteration, nine groups each from the normal and the pathological are used for training, while the remaining are for testing. Therefore, at the end of ten iterations, all the ten groups are tested. There is no overlapping between the training set and the testing set in any iteration. Feature selection and the optimization of the SVM parameters were accomplished with the training set. Grid search was conducted to obtain the optimal parameters values for C and γ (in our experiments, the average optimum is found with C=2 and γ=0.177). The final accuracy was obtained by averaging the ten accuracies of the folds. We used LIBSVM, which is a library for SVM [22].Two different types of experiments were conducted, (i) detection of pathology voices in the MEEIsubset, and (ii) classification of the pathologies. For pathologies classification, a full pair-wise classification (six experiments) was performed on four types of diseases, vocal nodules, vocal fold polyp, keratosis and adductor. In addition, paralysis and non-paralysis (having other pathologies) classification was conducted as the seventh experiment. Pathologies classification was conducted on exactly the same samples and the same permutations as done in [9]. Here, the objective is to classify among two diseases each time to further develop a hierarchical classification (not developed in this paper). As the MEEIsubset has some samples containing two or more pathologies at the same time, the samples which are shared in any of the seven experiments were excluded during the classification. Full detail of each classification experiment including the file names and the number of files for each class per experiment is listed in Appendix A.Results for pathology detection and classification are expressed in terms of accuracy, sensitivity (Sn: the likelihood that an event will be detected given that it is present), specificity (Sp: the likelihood that the absence of an event will be detected given that it is absent) and area under the Receiver Operating Characteristic (ROC) curve, called AUC. For utterance based decision (per recording), if more than 90% of the frames in an utterance are correctly classified, the utterance is correctly classified. As the MEEIsubset contains 173 recordings of pathological and 53 recordings of normal speakers, the number of pathological is almost three times the number of normal speakers while the duration of normal recordings is almost three times (3s) the pathological ones (1s); therefore, the total number of frames for both classes are almost close to each other.The results of pathology detection using the proposed MPEG-7 based method are shown in Table 2. The best performance was with all the features while as can be seen most of the contribution is by the five features explained earlier (Section 2.2). With all features (total 42 features, as described before), the proposed method achieves average accuracy of 99.994% with 0.011 standard deviation (confidence interval 95%, α=0.05), and AUC=0.999. The accuracies of seven selected features (FDR≥0.8; Section 2.2 plus ASF8th and ASF13th), five selected features (FDR≥1; Section 2.2), and three selected features (FDR≥1.2; ASC, ASS, ASF10th) are 99.412%, 99.4%, and 92.535%, respectively (confidence interval 95%). It can be noteworthy that the frame based accuracy of the proposed method with the top five features is 98.87%±0.003. Fig. 3shows the ROC curves for pathology detection using the top five (utterance based and frame based) and the top three features. Table 3shows a comparison of our proposed method with three other methods [8–10] on the same subset of MEEI database (MEEIsubset); we can clearly see that our proposed method outperforms the other methods in terms of accuracy. Fig. 4shows a 3D scatter plot of the values for the top 3 features (ASC vs. ASS vs. ASF 10th), where each marker represents the average value per file in the MEEIsubset (total of 226=173+53). We can see that the normal samples can be visually separated from pathological ones; this can give an idea on how much these MPEG-7 features discriminate normal from pathological voices.Results for pathology classification among the four diseases, vocal nodules, vocal fold polyp, keratosis and adductor, together with paralysis and non-paralysis classification are shown in Table 4. With all features, we achieve an accuracy of 100% for all diseases combinations except for “nodules vs. keratosis” and “paralysis vs. non-paralysis”, where accuracies of 99.97% and 98.34% are achieved, respectively. The last column of the table shows AUC obtained in method [9]. As we see, the proposed method even with top five features has a competitive AUC compared to AUC in [9]. Fig. 5shows the top five features’ distributions along time (for 99 frames) for random samples of the four diseases and the normal voice. We can see no intersection among all the samples in case of ASC, which has the highest FDR; other four distributions are also non-overlapping between the diseases for most of the frames. This strongly indicates the discrimination capability of MPEG-7 features between normal and pathological diseases and between the pathologies themselves. Table 5gives p values obtained by performing unpaired t-tests of the null hypothesis for the top 5 features (α=0.05). In the test, null hypothesis has failed for all the cases; we can see in the table that the p values are close to zero in all the cases, which mean these features are statistically significant to discriminate the pathologies.

@&#CONCLUSIONS@&#
