@&#MAIN-TITLE@&#
Multi-kernel multi-criteria optimization classifier with fuzzification and penalty factors for predicting biological activity

@&#HIGHLIGHTS@&#
A novel MK–MCOC–FP approach is proposed for predicting active compounds in bioassay.Multi-kernel method reduces dimensionality and gains the interpretable classifier.A fuzzification method using class median minimizes the effect of outliers.The class-imbalanced penalty factors tune overfitting and underfitting.Our proposed classifier obtains better performance in flexibility and accuracy.

@&#KEYPHRASES@&#
Multi-kernel learning,Multi-criteria optimization,Fuzzification,Class-imbalanced learning,Classification,Bioassay,

@&#ABSTRACT@&#
Nowadays it is important to develop effective computational methods for accurately identifying and predicting biological activity in the virtual screening of bioassay data so as to speed up the process of drug development. Among these methods, multi-criteria optimization classifier (MCOC) is a classifier which can find a trade-off between the overlapping degree of different classes and the total distance from input points to the decision hyperplane. The former should be minimized while the latter should be maximized. Then a decision function is derived from training data and this function is subsequently used to predict the class label of an unseen instance. However, due to outliers, anomalies, highly imbalanced classes, high dimension, nonlinear separability and other uncertainties in data, MCOC and other methods often give the poor predictive performance. In this paper, we introduce a new fuzzy contribution to each input point based on class median, by defining the new row and column kernel functions the linear combination of different feature kernels to replace the single kernel function in the kernel-induced feature space and penalty factors to imbalanced classes, thus a novel multi-kernel multi-criteria optimization classifier with fuzzification and penalty factors (MK–MCOC–FP) is proposed and the effects of the aforementioned problems are significantly reduced. The experimental results of predicting active compounds in the virtual screening and comparison with linear and quadratic MCOCs, support vector machines (SVM), fuzzy SVM and neural network, the conclusions show that MK–MCOC–FP evidently increased the ability of resisting noise interference, the predictive accuracy of highly class-imbalanced bioassay data, the separation of active compounds and inactive compounds, the interpretability of importance or contributions of different features to classification, the efficiency of classification with feature selection or dimensionality reduction for high-dimensional data, and the generalization of predicting the biological activity of new compounds.

@&#INTRODUCTION@&#
We know that in the process of drug discovery and development a large amount of time and efforts are devoted to the primary-screening and the confirmatory-screening for extracting the relevant compounds from the bioassays. For high-throughput screening (HTS) a large number of compounds are screened against a biological target to test whether the compound is capable of binding to the target, if the compound binds then it is an active for that target otherwise it is an inactive one. In order to increase the efficiency and accuracy of HTS, virtual screening (VS) can be employed by using some effective computational methods [43]. These methods mainly include protein-based approaches, ligand-based approaches and some data mining approaches [36,44,22]. However, owing to the challenges of the growing complexity of bioassay data, for instance, the higher dimensionality, the highly imbalanced class and the massive data, these methods often give the poor predictive performance, such as the high false positive rates, the low classification accuracy, the poor generalization of predicting the bioactivity of new compounds, the weak interpretability and the overfitting majority of inactive compounds. Recently, more and more data mining and machine learning techniques are used to aid the prediction and selection of active compounds in VS, for example, naïve Bayes classifier, support vector machine (SVM), decision tree, random forest, and so on.SVM based on optimization method and statistical learning theory is recently applied to prediction of bioactivity [43]. At the same time, the SVM classifier has been extensively utilized to a variety of applications because of its better generalization than some traditional data mining methods [49,9,32,42,12]. The main idea of the SVM classifier is to partition instances into different classes by finding a separating hyperplane that maximizes the margin between two supporting hyperplanes of two classes and minimizes the misclassification simultaneously. For the linearly separable case, the separating hyperplane is built in the input space. For the nonlinearly separable case, kernel techniques are used to map input points from the input space into a feature space, and the separating hyperplane is positioned in the new feature space.However, in many real-world applications SVM is very sensitive to noise, outliers and anomalies in data set so that the separating hyperplane severely deviates from the right position and direction [5,6,51]. Thus several methods have been proposed to solve the problem by introducing a proper fuzzy membership function to the SVM model [45,31,48,41,25,47,18,23,3,14]. Additionally, in real world applications, it is very common that one class is more important than others, and the class distribution is imbalanced, which results in a rapidly degenerating classification precision and accuracy for instances from the minority class. In order to deal effectively with the class imbalance problem, some penalty techniques based on cost-sensitive learning and other methods are used [46,52,53,27,38,10].Multi-criteria optimization classifier (MCOC) is another optimization-based method which can be used to solve the classification problems in data mining. The main idea of MCOC is to find a trade-off between the overlapping degree of input points belonging to different classes and the total distance from input points to their decision hyperplane. The first criterion should be minimized while the second criterion should be maximized. Then a linear MCOC model based on the comprise solution was proposed and applied to the credit card portfolio management and risk analysis [39,40]. A multiple phase fuzzy linear MCOC approach was provided and used for the behavior analysis of credit cardholders [20]. Subsequently, a linearly penalized MCOC was presented for improving the catch rate of the bad credit cardholders [29]. A quadratic MCOC in data mining was given and used to carry out credit analysis [28]. In order to increase accuracy and efficiency of classification, an appropriate fuzzy membership function is introduced to MCOC, that is to say, the objective functions and the constraints are transformed into the corresponding fuzzy decision sets, then the new MCOC with fuzzy parameters is constructed so as to improve the generalization in the unseen data [55]. For the nonlinearly separable case, a kernel-based MCOC was given just like the use of the kernel trick in the SVM classifier [54].Besides, in many practical applications, owing to noise, outliers and anomalies, the highly imbalanced class distribution, and the nonlinear separability within a data set, MCOC will degenerate into an inefficient, instable, and inaccurate classifier when it is trained and tested with these data. To improve the predictive performance of MCOC, a novel MCOC using fuzzification, kernel and penalty factors was proposed and used for predicting protein–protein interaction hot spots, where a class mean-based fuzzy membership function was introduced to MCOC and associated with each input point in the input space, a kernel function was used to map input points into a high-dimensional feature space, and the unequal penalty factors are added to the input points of imbalanced classes [56]. At the same time, multi-criteria optimization classifier with kernel, fuzzification and penalty factors was put forward for credit risk evaluation. Different from the kernel trick in the former classifier, in this classifier a kernel function was firstly introduced, and then the fuzzy membership degree of each input point was calculated in kernel-induced feature space [57]. In a word, the results of the real world applications have shown that the two types of classifiers remarkably increased the predictive performance of classification. Therefore, the MCOC models and algorithms are gradually developing as a new alternative method for solving classification, regression and other problems in data mining and machine learning.Although the improved MCOC has ability to discover noise in data, deal effectively with the nonlinearly separable case and overcome the case of overfitting majority-class instances for class imbalance, the interpretability of MCOC is weakened due to introduction of the single kernel function. Recently, in practice we also found that MCOC is still subjected to the effects of outliers and anomalies, and the classifier lacks of ability to efficiently process the highly dimensional data and obtain the better interpretability by means of feature selection or dimensionality reduction.To this end, we reformulate the MCOC model and redesign the corresponding algorithm by introducing a new fuzzy membership function to each input point where class mean is not used but class median, substituting a linear combination of different feature kernels for the single kernel function, and integrating unequal penalty factors into highly imbalanced classes. Thus, the proposed MK–MCOC–FP approach can improved the overall performance of the original MCOC in stability, efficiency, separability and interpretability, that is to say, the aforementioned effects of outliers, anomalies, high dimension, class imbalance and nonlinearly separable problems can be reduced significantly.Therefore, the main contribution of this paper can be summarized as follows: firstly, by using new fuzzification method based on class median the effects of noise and anomalies are removed to the greatest extent such that the stability and flexibility of MCOC are improved considerably. On the other hand, by defining new row and column kernels of different features in high-dimensional data and employing the new sparsification method based on multi-kernel learning the important features are selected and used for classification such that the interpretability and efficiency of MCOC are enhanced remarkably. In addition, by applying the cost-sensitive penalty factors to the highly class-imbalanced data the proposed MK–MCOC–FP method is used to identify active compounds in bioassay and achieves the better predictive performance than that of other classifiers as shown in the experimental results and comparative analysis.The rest of this paper is organized as follows: Section 2 describes basic principles of MCOC. Then the new MK–MCOC–FP approach and the corresponding algorithm are illustrated in Section 3. The experimental results of predicting biological activity and comparison analysis are demonstrated in Section 4. Finally, discussions and conclusion will be given in Sections 5 and 6 respectively.In this section the MCOCs are presented in detail. For a two-class classification problem, suppose a training setT1={(x1,y1),…,(xn,yn)}is given, each input pointxi(xi∈Rd)belongs to either of two classes with a labelyi∈{-1,1},dis the dimensionality of the input space, and n is the sample size. In order to separate different classes some researchers have chosen the two measures: the overlapping degree deviating from the separating hyperplane and the total distance departing from the decision hyperplane [15]. In the first case an input point is in the wrong side of the hyperplane and misclassified, while in the second case it is in the right side and correctly classified. Subsequently, Glover considered the above two factors in the classification models simultaneously [16].Letαi(αi⩾0)be the distance where an input pointxideviates from the separating hyperplane, and the sum (called as the overlapping degree) of the distanceαishould be minimized. We have(1)minimize∑i=1nαisubject toyi(wTxi-b)⩾-αi,αi⩾0,i=1,…,n.where the input pointxiis given training data, the weight vectorwand the term b are unrestricted variables. It is obvious that the input pointxiis correctly classified ifαi=0. Otherwise, the input pointxiis misclassified.Similarly, letβi(βi⩾0)be the distance where the input pointxideparts from decision hyperplane, then the sum (called as the total distance) ofβishould be maximized, and we get(2)maximize∑i=1nβisubject toyi(wTxi-b)⩽βi,βi⩾0,i=1,…,n.And the input pointxiis misclassified ifβi=0. Otherwise, the input pointxiis correctly classified.If we take the two measures in the above Eqs. (1) and (2) into account simultaneously, MCOC is defined as(3)minimize∑i=1nαiandmaximize∑i=1nβisubject toyi(wTxi-b)=βi-αi,αi⩾0,βi⩾0,i=1,…,n.Let C be a penalty factor of the first objective function∑i=1nαi, Eq. (3) can be rewritten as a new MCOC with the hybrid objective function with respect toαiandβi. Thus the linear MCOC is denoted as(4)minimizeC∑i=1nαi-∑i=1nβisubject toyi(wTxi-b)=βi-αi,αi⩾0,βi⩾0,i=1,…,n.If the regularization term of the weight vectorw=(w1,…,wd)T(w∈Rd)and the quadratic function of the classification error sum with respect toαiare added to the above objective function, a quadratic MCOC is obtained as below(5)minimize(1/2)w22+∑i=1nαi2+C∑i=1nαi-∑i=1nβisubject toyi(wTxi-b)=βi-αi,αi⩾0,βi⩾0,i=1,…,n.According to the solutions of Eqs. (4) and (5), we can calculate the scalarb(b∈R)and the weight vectorwof the separating hyperplane which is defined as(6)wTx=b.For a new input pointxthe decision function of Eqs. (4) and (5), which is used to predict the class label of the input pointx, can be expressed as(7)f(x)=sign(wTx-b).It should be pointed out that the above classifiers in Eqs. (4) and (5) can only be used to solve the linearly separable problems. For the nonlinearly separable case, a kernel function can be introduced to the linear and quadratic MCOCs, which will be illustrated in the forthcoming Section 3 for the linear MCOC. For the quadratic MCOC, we can add a kernel function to the classifier by solving the corresponding dual problem of Eq. (5).Besides, for the linear MCOC in Eq. (4), the compromise solution approach has been used to improve its classification performance in business practices [39,40]. With its obvious advantages in simplicity, high efficiency and interpretability, MCOC has to solve practical problems in recent years. However, MCOC has the significant limitations of unstability, poor generalization and insufficient flexibility, especially for outliers, anomalies, class imbalance, nonlinear separability, high dimension and other uncertainties in data. Therefore, it is necessary that we use fuzzification, sparsification and nonlinear techniques to rebuild MCOC so as to remarkably increase its robustness, predictive accuracy and efficiency in solving classification problem.In this section a new fuzzification method, the theory of multiple kernel learning and the highly class-imbalanced case are firstly described, then these approaches are introduced to MCOC, and then the proposed MK–MCOC–FP approach and the corresponding algorithm are given in detail. Besides, a simulated data set is used to initially evaluate the classification performance of MK–MCOC–FP in the end.For classification problems the nonlinearly separable data is very common in many practical applications, the kernel trick is often utilized to address this problem. That is to say, the original input space was transformed into a higher dimensional feature space by using an appropriate basic functionϕ(x)with respect to the input pointxand the data set can be separated linearly in the new feature space [7,49,19]. Thus the dot product(ϕ(x)Tϕ(y))of the basic function is replaced with the single kernel functionK(x,y)=(ϕ(x)Tϕ(y))of input pointsxandyin the original input space without explicitly computing the value of the basic function. Besides the following kernel functions are often chosen: (i) the linear kernelK(x,y)=xTy. (ii) the radial basic function (RBF) kernelK(x,y)=exp(-x-y22/2σ2)(σ>0).Recently, in order to deal effectively with the high-dimensional data, the multi-kernel learning is proposed by combining multiple kernel functions of different features or variables instead of using a single one [33,58,17,35,50,34]. According to the combination way of different feature kernels, multi-kernel learning method is mainly classified into three types: (i) the linear combination method is very popular one and expressed as the linear function of different feature kernels with respect to the corresponding kernel weights. (ii) the nonlinear combination approach is implemented by using nonlinear functions of different feature kernels and their weights. (iii) data-dependent combination is directly learnt from data to gain weights of the feature kernels.By means of multi-kernel learning we can get the contribution or importance of each feature to classification based on the kernel weights of different features. In other words, the greater the kernel weight of a feature is, the more important it shows. Otherwise, it is a less important one. Therefore, according to the kernel weights of different features we can give a reasonable interpretation for the classification problem in practical applications. Moreover, feature selection or dimensionality reduction can be carried out so that the higher efficiency of classification can be obtained based on the reduced data set [4]. In this paper, we use the above linear and RBF kernel functions to obtain the linear combination of different feature kernels in our experiments.The fuzzification method based on class means aforementioned in the fuzzy SVM classifier has gained the good effect of classification in practice. However, owing to noise, outliers and anomalies in data, mean may deviate from its real value. To this end the median is a better alternative to mean, and it lies in the middle of a list numbers and is immune to the above abnormal values. We consider the class median as a representative point instead of the class mean, and the fuzzy membership degreeti(0⩽ti⩽1,i=1,…,n)of each input pointxican be calculated from the distance between the input pointxiand its class median.Thus an input point belonging to different classes may be the three cases: (i) the input pointxibelongs definitely to one class withti=1. (ii) the input pointxidoes not belong to one class with determination ifti=0. (iii) the input pointxibelongs to one class with the fuzzy membership degreeti(0<ti<1). At this point for a predefined thresholdτ(τ>0), ifti⩾τ, the input pointxiis considered as an important part of modeling the classification problem. Otherwise, it is less important one or noise and can be discarded from classifier [31,57]. In this paper, we use the above fuzzification method so as to further improve the predictive performance of our proposed classifier.We know that the class-imbalanced problem is also very common in data mining and machine learning, many studies show that in the case a classifier tends to overfit the instances of majority-class and underfit the instances of minority-class [1,24,8,52,27,53,5,6,21,56,26,37,30]. Generally speaking, some methods are proposed to solve the problem and they can be divided as two categories: (i) data preprocessing methods, for instance, resampling methods mainly include random and focused undersampling or oversampling methods and synthetic data generation methods. Ensemble methods include the random sampling, bootstrapping, Adacost and SMOTEBoost, in these methods data set is partitioned into multiple subsets such that each of these subsets has a similar number of examples as the minority-class data set, then majority voting method is used to determine the class of input points. (ii) integrated methods, for example, cost-sensitive learning, the unequal penalty factors are added to different classes so that the cost of misclassifying minority-class instances cost more than majority-class instances. Margin calibration, the method uses a margin compensation to refine the biased decision boundary for the highly class-imbalanced learning. Other methods mainly include one class learning, active learning, kernel modification methods, soft computing methods and hybrid methods, and so on. In our paper, we employed the cost-sensitive learning to correct of decision boundary and build a new classifier with the class-imbalanced penalty factors.Based on the above description of class-imbalanced learning, for the cost-sensitive method, letC1(C1>0)be the misclassification cost or penalty factor of the negative instances (yi=-1). Similarly, letC2(C2>0)be the misclassification cost of the positive instances (yi=1). Thus we can rewrite Eq. (4) as(8)minimizeC1∑yi=-1αi+C2∑yi=1αi-∑i=1nβisubject toyi(wTxi-b)=βi-αi,αi⩾0,βi⩾0,i=1,…,n.whereC1andC2(C1>0,C2>0)are the cost-sensitive penalty factors of different classes for the misclassification points. Obviously, ifC1⩽C2, the cost of misclassified points of the negative class is equal and less than that of misclassified points of the positive class. Otherwise, the cost of the former is greater than that of the latter.In light of the fuzzification method in fuzzy SVM, the fuzzy membership degree is defined as the ratio between the distance from an input point to its class representative point and the class maximum distance, and here the class median is regarded as the representative point. For the given data setT1, we can get a new data setT2={(x1,y1),…,(xn′,yn)}after each variable is sorted in ascending order.For a binary classification problem, letφ¯yibe the median of classyi(yi∈{-1,1})and it can be denoted as(9)φ¯yi=x12(Nyi+1)′,ifNyiis odd,12x12Nyi′+x12Nyi+1′,ifNyiis even.whereNyiis the absolute value∑iyiof the sum of classyi.The radius of classyiis the maximum distance from all input pointsxito their class median and it is denoted as(10)ryi=maxxi-φ¯yi2.Let the fuzzy membership degreetiof each input pointxibe the linear function of the medianφ¯yiand the radiusryiof classyi, we have(11)ti=1-xi-φ¯yi2ryi+δwhereδ(δ>0)is a sufficiently small constant and used to avoid dividing by zero.Thus a new training setT3={(x1,y1,t1),…,(xn,yn,tn)}with a fuzzy membership valueti(τ<ti⩽1)is obtained, where the input pointxi∈Rdis partitioned by target variableyi∈{-1,1}and the parameterτ(τ>0) is a user-defined threshold. Based on the above new training setT4, the classifier in Eq. (8) can be rewritten as(12)minimizeC1∑yi=-1tiαi+C2∑yi=1tiαi-∑i=1nβisubject toyi(wTxi-b)=βi-αi,αi⩾0,βi⩾0,i=1,…,n.where the user-specified parametersC1andC2are the penalty factors for imbalanced classes. As aforementioned, the parameterαi(αi⩾0)is a measure of the misclassified input point in Eq. (12), so the termtiαiis a new error measure with a weight. Thus, the effects of noises, outliers and anomalies in data are reduced remarkably and the stability of the classifier in Eq. (12) is improved considerably.For the nonlinearly separable case, we suppose thatϕ(x)is a basic function mapping input data into a higher dimensional feature space. In theory we can obtain the new data setT4={(ϕ(x1),y1,t1),…,(ϕ(xn),yn,tn)}. According to the basic idea of the kernel method, given the data setT4, the weight vectorwcan be denoted as the linear combination ofϕ(xj)andyjwith respect to the nonnegative coefficientλj(λj⩾0), that is(13)w=∑j=1nλjyjϕ(xj).Then incorporating the above weight vectorwin Eq. (13) into Eq. (12), we have(14)minimizeC1∑yi=-1tiαi+C2∑yi=1tiαi-∑i=1nβisubject toyi∑j=1nλjyjϕ(xj)Tϕ(xi)-b=βi-αi,αi⩾0,βi⩾0,i=1,…,n,0⩽λj⩽C1,foryj=-1,0⩽λj⩽C2,foryj=1,j=1,…,n.In the case the dot product(ϕ(xj)Tϕ(xi))of basic function is replaced by the kernel functionK(xj,xi), we get(15)minimizeC1∑yi=-1tiαi+C2∑yi=1tiαi-∑i=1nβisubject toyi∑j=1nλjyjK(xj,xi)-b=βi-αi,αi⩾0,βi⩾0,i=1,…,n,0⩽λj⩽C1,foryj=-1,0⩽λj⩽C2,foryj=1,j=1,…,n.Thus, introducing a new fuzzy membership degree to each input point can further reduce the effects of noise, outliers, and anomalies in data. The class-imbalanced penalty factors are used to trade off overfitting majority-class and underfitting minority-class. At the same time, kernel function can transform a nonlinearly separable case into a linearly separable one.However, in practice we found that the interpretability and sparsification of the classifier in Eq. (15) for different features are weakened because the single kernel function is introduced to the above classification model. Thus, according to the basic idea of multi-kernel learning the novel row and column kernels are defined and integrated into the classifier in Eq. (15) respectively. For the linear kernel case, given the data setT3, the row kernel vectorki,j(ki,j∈Rd)of any two input pointsxiandxj(i,j=1,…,n)corresponding to two rows(xi1,…,xid)and(xj1,…,xjd)in the data set respectively is defined as(16)kij=xixj=(xi1xj1,…,xidxjd)T,i,j=1,…,nwhere the elementximxjm(m=1,…,d)is the linear kernel value of the mth feature. If the feature weight vectorμ=(μ1,…,μd)T(μ∈Rd)is provided, then the weighted row kernel valueKijr(Kijr∈R)of the two input pointsxiandxjis expressed as the dot product of the row kernelki,jwith respect to the feature weight vectorμ, which is denoted as(17)Kijr=kijTμ=∑m=1dμmximxjm,i,j=1,…,n,m=1,…,d.Then the row kernel matrixKr(Kr∈Rn×n)of different features has the form(18)Kr=K11r…K1nr⋮⋱⋮Kn1r⋯Knnr=k11Tμ…k1nTμ⋮⋱⋮kn1Tμ⋯knnTμ=∑m=1dμmx1mx1m…∑m=1dμmx1mxnm⋮⋱⋮∑m=1dμmxnmx1m⋯∑m=1dμmxnmxnm.For the nonlinear kernel case, that is, given the data setT4, the row kernel vectorki,jof any two input pointsxiandxjis redefined as(19)kij=(ϕ(xi1)ϕ(xj1),…,ϕ(xid)ϕ(xjd))T,i,j=1,…,nwhere the row kernel valueϕ(xim)ϕ(xjm)can be further replaced by the feature kernel functionk(xim,xjm)with respect to the mth feature (m=1,…,d). Thus, we have(20)kij=(k(xi1,xj1),…,k(xid,xjd))T,i,j=1,…,n.Similarly, once the feature weight vectorμis given, the weighted row kernel valueKijrof the two input pointsxiandxjwith respect to the feature weight vectorμis computed by(21)Kijr=kijTμ=∑m=1dμmk(xim,xjm),i,j=1,…,n,m=1,…,d.Obviously, the row kernel matrixKrin Eq. (21) of multiple feature kernels has the ultimate formula as below(22)Kr=∑m=1dμmk(x1m,x1m)…∑m=1dμmk(x1m,xnm)⋮⋱⋮∑m=1dμmk(xnm,x1m)⋯∑m=1dμmk(xnm,xnm).Besides, the linear row kernel in Eq. (18) is actually the special case of the nonlinear row kernel in Eq. (22), that is, for the linear kernel we have the result ofk(xim,xjm)=ximxjmfor any two input pointsxiandxjwith respect to the mth feature.As far as the column kernel is concerned, for the linear kernel case, given a data setT3, the column vectorfm(fm∈Rn)of the mth feature is defined as(23)fm=(x1m,…,xnm)T,m=1,…,d.If the coefficient vectorλ=(λ1,…,λn)Tis given, the weighted column kernel vectorKmc(Kmc∈Rn)is calculated as the weighted sum of the linear kernelfmfmTof the mth feature with respect to the product of the coefficient vectorλ(λ∈Rn)and class labely(y∈Rn), that is the column kernel vectorKmcis derived from(24)Kmc=(fmfmT)λy=(x1m,…,xnm)T(x1m,…,xnm)(λ1y1,…,λnyn)T=x1mx1m…x1mxnm⋮⋱⋮xnmx1m⋯xnmxnmλ1y1⋮λnyn=∑l=1nλlylxlmx1m⋮∑l=1nλlylxlmxnm,m=1,…,d.Then the column kernel matrixKc(Kc∈Rn×d)of different features has form(25)Kc=K1c,…,Kdc=∑l=1nλlylxl1x11…∑l=1nλlylxldx1d⋮⋱⋮∑l=1nλlylxl1xn1⋯∑l=1nλlylxldxnd.For the nonlinear kernel, given the data setT4, the column vectorfmof the mth feature is redefined as(26)fm=(ϕ(x1m),…,ϕ(xnm))T,m=1,…,d.Besides, given the data setT4, their nonlinear kernelfmfmTis computed the productϕ(xim)ϕ(xjm)of the mapping functions of two input pointsxiandxj(i,j=1,…,n)with respect to the mth feature. Moreover, the above product can be replaced with the kernel functionk(xim,xjm)of the mth feature, we have(27)fmfmT=(ϕ(x1m),…,ϕ(xnm))T(ϕ(x1m),…,ϕ(xnm))=k(x1m,x1m)…k(x1m,xnm)⋮⋱⋮k(xnm,x1m)⋯k(xnm,xnm),m=1,…,d.Integrating the nonlinear kernel in Eq. (27) into the column kernel vectorKmcin Eq. (24), the column kernel vectorKmchas form(28)Kmc=∑l=1nλlylk(xlm,x1m),…,∑l=1nλlylk(xlm,xnm)T,m=1,…,d.Similarly, if the coefficient vectorλand class labelyare provided, the column kernel matrixKcof multiple features has the final form(29)Kc=∑l=1nλlylk(xl1,x11)…∑l=1nλlylk(xld,x1d)⋮⋱⋮∑l=1nλlylk(xl1,xn1)⋯∑l=1nλlylk(xld,xnd).Apparently the linear column kernel in Eq. (25) is the special case of nonlinear column kernel in Eq. (29) if the equalityk(xim,xjm)=ximxjmholds. Moreover, the feature kernelk(xim,xjm)in Eqs. (22) and (29) can be chosen as the common kernel functions, for instance, the aforementioned linear and RBF kernels in Section 3.1. That is to say, for the linear kernel of the mth feature we have the form(30)k(xim,xjm)=ximxjm,i,j=1,…,n,m=1,…,d.Similarly for the RBF kernel of the mth feature we have(31)k(xim,xjm)=exp-||xim-xjm||222σ2(σ>0),i,j=1,…,n,m=1,…,d.Based on the row kernel formula in Eq. (22), the single kernelK(xj,xi)in Eq. (15) is replaced by the row kernel matrixKrin Eq. (22) of multiple feature kernels. Therefore, the first phase model of the MK–MCOC–FP method in Eq. (15) can be rewritten as(32)minimizeC1∑yi=-1tiαi+C2∑yi=1tiαi-∑i=1nβisubject toyi∑j=1nλjyjKijr-b=βi-αi,0⩽λj⩽C1,foryj=-1,0⩽λj⩽C2,foryj=1,αi⩾0,βi⩾0,i,j=1,…,n,m=1,…,d.whereKijris the ith row and jth column element in the row kernel matrixKr.In order to solve the linear optimization problem for classification in Eq. (32), we need to firstly fix the kernel weightsμin the row kernel matrixKr, that is to say, the initial value of kernel weights should be assigned, for instance,μ(0)=(1/d,…,1/d)Tor(1,…,1)T. After solving the problem in Eq. (32), the new coefficient vectorλ=(λ1,…,λn)Tcan be obtained from its solution.Based on the new coefficient vectorλfrom the solution of Eq. (32), by substituting the linear combination of the column kernel matrixKcin Eq. (29) with respect to the kernel weight vectorμfor the weighted sum of the single kernelK(xj,xi)in Eq. (15) with respect to the coefficient vectorλand class labely, we can construct the second phase model of the MK–MCOC–FP approach using multiple feature kernels, thus the classifier is rewritten as(33)minimizeC1∑yi=-1tiαi+C2∑yi=1tiαi-∑i=1nβisubject toyi∑m=1dμmKmc-b=βi-αi,∑m=1dμm⩽S,μm⩾0,m=1,…,d,αi⩾0,βi⩾0,S>0,i=1,…,n.whereKmc(m=1,…,d) is the mth column element in the column kernel matrixKc. Besides, the kernel weightsμm(m=1,…,d) and the boundary b are the unknown variables. AndS(S>0)is a sparsification constant as a user-specified parameter, which is used to shrink the size of dimensionality of feature space and perform feature selection.Thus we can get a new kernel weight vectorμ(p)=(μ1p,…,μdp)Tby solving the linear optimization problem for classification in Eq. (33), wherep(p=1,2,…)is the iterative times. For the above two-phase MK–MCOC–FP model, we can replace the original kernel weight vectorμ(0)with the newμ(p)in the first model in Eq. (32), so the new coefficientsλjcan be gained by solving the classification problem in Eq. (32) and incorporated them into the second model in Eq. (33). We can get a new kernel weight vectorμ(p+1)again. Therefore, the above two-phase MK–MCOC–FP method is actually an iterative process and it will terminate until the following stopping criterion is satisfied(34)μ(p+1)-μ(p)2<ε.whereε(ε>0)is a smaller constant which is predefined by user.When the above stopping criterion is satisfied, the new coefficientsλjand kernel weightsμ(p)obtained in thepthiteration are considered optimal. According to the definition of the weight vectorw(see Eq. (13)) and the separating hyperplanewTϕ(xi)=b, for all input pointsxiwhich satisfyαi=0orβi>0, the boundarybcan be calculated byb=∑j=1nλjyj∑m=1dμmk(xj,m,xi,m), then an average b ofbis taken. Therefore, the decision function for an unknown input pointxis rewritten as(35)f(x)=sign(wTϕ(x)-b)=sign∑j=1nλjyj∑m=1dμmk(xj,m,x·,m)-b.Besides, once the optimal kernel weightμ∗=(μ1∗,…,μd∗)Tis gained, the contribution or importance of each variable to classification can be given. Thus the interpretability of the MK–MCOC–FP approach is remarkably increased. Besides, letρ(ρ>0)is a user-defined constant or threshold, ifμm∗⩾ρ, we can conclude that the mth feature is important for MK–MCOC–FP and should be hold. Otherwise, it is less important one and can be removed. Then a new reduced data setT6={(x1,y1,t1),…,(xn,yn,tn)}is obtained, for each input pointxi(xi∈Rq)qis the dimensionality of the new input space and we haveq⩽dfor the high-dimensional data. Based on the reduced data set, if necessary, the two-phase MK–MCOC–FP model and other classifiers can be trained and tested again efficiently. Thus, by integrating the above feature selection or dimensionality reduction with the classification model in Eq. (15), the efficiency of the MK–MCOC–FP method is enhanced significantly.Following the above illustrations, the overall process of the two-phase MK–MCOC–FP algorithm can be summarized into the seven steps as below.Step 1:Computing the fuzzy membership degreetifor each input pointxi(i=1,2,…,n)with training set (see Eq.(11)):ti=1-xi-φ¯yi2/(ryi+δ), where the class medianφ¯yi, the class radiusryiare given in Eqs.(9) and (10) respectively, andδ(δ>0)is a sufficiently small constant.Initializing the kernel weights asμ(0)=(1/d,…,1/d)T, constructing the row kernel matrixKrin Eq.(22) and the column kernel matrixKcin Eq.(29) respectively.Solving the first phase model in Eq.(32) and getting the optimal solutionλj(j=1,…,n)of MK–MCOC–FP on training set.Solving the second phase model in Eq.(33) and gaining the optimal solutionμ(p)=(μ1p,…,μdp)T(p=1,2,…)of MK–MCOC–FP with training set based on the new coefficientsλjobtained in step 3.Replacingμ(0)with the new kernel weightsμ(p)and repeating the above step 3 and step 4 until the stopping criterionμ(p+1)-μ(p)2<ε(see Eq.(34)) is satisfied, whereε(ε>0)is a smaller constant.Using the above optimal coefficientsλjand kernel weightsμ∗=(μ1∗,…,μd∗)T, for an unknown samplexthe decision function in Eq. (35) is constructed and used to predict its class label. And the classification accuracy and the contribution or importance of each feature can be respectively computed and reported.According to the optimal kernel weightsμ∗and the given threshold ρ, whereρ(ρ>0)is a sufficiently small constant, ifμm∗⩾ρ(m=1,…,d), the mth feature is important for MK–MCOC–FP and should be hold. Otherwise, it is less important one and can be removed. The reduced data set can be obtained by feature selection or dimensionality reduction. If necessary, the obtained classifier and other classifiers can be trained and tested with the reduced data set and the corresponding measures of predictive performance can be calculated and reported respectively.In this section a simulated dataset is designed and characterized by outliers, anomalies, class-imbalance and nonlinear separability so as to test the new MK–MCOC–FP approach. In this paper, since the linear MCOC and quadratic MCOC are only fit for the linearly or approximately separable case, SVM, fuzzy SVM, and MK–MCOC–FP with the RBF kernels are tested on the data set. Besides, the data set is used for binary classification with 237 points for negative class and 74 points for positive class as shown by diamonds and crosses in Fig. 1.In Fig. 1 the separating hyperplanes are found by SVM and fuzzy SVM with the RBF kernels forC=5000andσ=0.1respectively. At the same time, MK–MCOC–FP with the RBF kernel generates a better separating boundary forC1=1000,C2=5000andσ=0.05, which shows that MK–MCOC–FP can achieve an excellent accuracy of classification for the uncertain data.In this section we use MK–MCOC–FP and other classifiers for predicting active compounds in the virtual screening of bioassay data, including data sets, experiment design, feature selection and importance analysis, experimental results and comparison analysis in the following subsections.The following are the descriptions of data sets used in our experiments. Bioassay data in VS are sourced from the online UCI Repository of Machine Learning Databases (http://archive.ics.uci.edu/ml/datasets/). We select 2 primary screening data sets and 4 confirmatory screening data sets to test our proposed MK–MCOC–FP approach and conduct comparison with other classifiers. And they have been partitioned into training sets and the dependent test sets respectively. Before these data sets are used to train classifiers, they are normalized with the min–max standardization method for numerical attributes.AID362 gives the results of a primary screening bioassay for Formylpeptide Receptor Ligand Binding University from the New Mexico Center for Molecular Discovery. AID439 is a confirmatory screen of AID373 which is a primary screen for endothelial differentiation, sphingolipid G-protein-coupled receptors 3. AID644 is a confirmatory screen of AID604 which is Rho kinase 2 inhibitors. AID721 and AID 1284 are two confirmatory screens of AID746 which is Mitogen-activated protein kinase. And the above four primary screens are sourced from the Scripps Research Institute Molecular Screening Center. AID1608 is a different type of screening assay that was used to identify compounds that prevent HttQ103-induced cell death from National Institute of Neurological Disorders and Stroke Approved Drug Program.Each data set has many conditional attributes and one class attribute which label a compound as active and inactive one. Table 1illustrated the characteristics of different data sets, including the number of inactive compounds (negatives) and active compounds (positives) in training sets and the independent test set respectively, the number of nominal attributes and numerical attributes, and a ratio of inactive compounds to active compounds in bioassay data sets.Table 1 has shown that the six data sets are characterized by high dimension, highly imbalanced class distribution and mixed data types. Besides, owing to some inevitable errors in computing the values of different features, these datasets may contain potential noise, outliers and anomalies.In our experiments, the six data sets have been divided into training sets and corresponding test sets. Each test set is straightly used for the independent test set. For each training set of AID362, AID439, AID644, AID721, AID1284 and AID1608, the stratified 5-fold cross-validation (CV) method is employed to train MCOC, SVM, fuzzy SVM, neural network and MK–MCOC–FP with the training subsets, and each fold contains roughly the same proportions of different classes so as to evaluate different classifiers in a more reliable manner. According to the best predictive performance of these classifiers on the validation subsets, the optimal parameters are found. Then the averages of predictive performance of different classifiers with the independent test set can be calculated and reported. At the same time, the comparison analysis of these classifiers is implemented based on the predictive performance with the same test set.Following the description of MK–MCOC–FP in Section 3, the classifier has not only the ability to solve the classification but also to carry out feature selection or dimensionality reduction. That is to say, by using the stratified 5-fold CV method the classifier with the best predictive performance on the validation subsets is selected. According to the given the minimum importance threshold and the optimal kernel weights, the minimal feature subset of each bioassay data set can be obtained.In the total process of training different classifiers the optimal parameters of MCOC, SVM, fuzzy SVM and MK–MCOC–FP are selected from the predefined sets by grid search method. These parametric sets are defined as: the penalty factor C for MCOC, SVM and fuzzy SVM, the penalty factorsC1andC2for MK–MCOC–FP from the set {1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 50000, 100000}, the bandwidth σ from the set {0.001, 0.01, 0.1, 0.2, 0.5, 1, 2, 5, 10, 100, 1000} for the RBF kernel. Besides, feed forward neural networks are trained on the six bioassay data sets and then they are tested on the independent test sets, where the topology of neural network are designed as a multi-layer structure, including input layer, one to three hidden layers with the number of neurons ranging between 10 and 50, and one output layer with two neurons. Apparently, the optimal network topology dependents on the best predictive performance by using grid search method.For each of the aforementioned classifiers an iterative process on the predefined parametric set is used to find the best parameter, the stratified 5-fold CV method is applied in each iterative process. It should be pointed out that for the stratified 5-fold CV approach the best parameter of a classifier is determined by the best average of predictive performance on 5 validation subsets, therefore there are 5 classifiers corresponding to 5 training subsets. Then selected classifiers with the best parameter value are respectively used for prediction on the independent test set, and the average of their predictive performance can be computed and gained in the end.Besides, in our experiments six accuracy measures are used to evaluate the predictive performance of the above classifiers. They are total accuracy, sensitivity, specificity, F1 score, MCC and AUC. And these measures are respectively defined as(i)Total accuracy (the total classification accuracy):(36)Total accuracy=TP+TNTP+FN+TN+FP.Sensitivity (the identification rate of the active compounds, also called as Recall):(37)Sensitivity=TPTP+FN.Specificity (the identification rate of the inactive compounds):(38)Specificity=TNTN+FP.F1 Score (the mixed measure of sensitivity and precision):(39)F1Score=2SN∗PRSN+PR=2TP2TP+FN+FP.Matthew’s Correlation Coefficient (the adjusted impacts of imbalanced data set, MCC):(40)MCC=TP∗TN-FP∗FN(TP+FN)(TP+FP)(TN+FP)(TN+FN).Finally, all of the experiments are carried out using Matlab 8.1. The linear programming problem of linear MCOC and MK–MCOC–FP, and the convex quadratic programming problem of quadratic MCOC, SVM, fuzzy SVM and neural network, are solved by utilizing Matlab optimal and neural network tools respectively.In our experiments for the MK–MCOC–FP approach the stopping criterion is set asε=1e-1so as to get the optimal kernel weights, and the predefined threshold is chosen asρ=1e-4to carry out feature selection or dimensionality reduction for the optimal attribute subsets. Because for each training set the sum of different kernel weights is equal or lesser than 1 (S=1), the importance or contribution of each feature to classification is a proportion of its weight to the above sum. By using the stratified 5-fold CV method, MK–MCOC–FP with the best predictive accuracy on the independent test set is selected. Thus, for the selected MK–MCOC–FP with the linear and RBF kernels the results of feature selection and the interpretation of attribution contribution or importance are shown in Figs. 2 and 3respectively.As the results shown in Figs. 1 and 2, for MK–MCOC–FP with the linear kernel, 14, 6, 17, 9, 3 and 18 attributes are selected from the training sets of AID362, AID439, AID644, AID721, AID1284 and AID1608 respectively. For MK–MCOC–FP with the RBF kernel, 8, 5, 7, 6, 2, and 5 attributes are selected from the training sets of the same bioassay data sets. Moreover, for each data set the sum of importance of selected features is very close to 1, it indicates that the selected attribute subset can be considered as a representative of the original attribute set. And the importance or contribution of each feature to classification is also exhibited in figures of the corresponding data set. Besides, for the above data sets, the corresponding test sets should be reduced to the same number of attributes as the training sets. Finally, we find that the number of selected features from each training set using MK–MCOC–FP with the RBF kernel is obviously less than that of obtained features from the same training set using MK–MCOC–FP with the linear kernel.For the above each bioassay data set, firstly by using the stratified 5-fold CV method the MCOC, SVM, fuzzy SVM and MK–MCOC–FP models are trained on the training subsets and validated on the corresponding validation subsets, then the different classifiers are obtained and tested on the independent test set. Thus, the averages of the best predictive performance of these classifiers are respectively computed and reported in Tables 2–7as below.As the experimental results illustrated in Tables 2–7 respectively, we find that for active compounds in bioassay the predictive performance (sensitivity) of our proposed MK–MCOC–FP approach outperforms that of other methods. That is to say, the MK–MCOC–FP method has the best identification rate of active compounds, and F1 score, MCC and AUC measures also show our proposed classifier has a good prediction for the biological activity. Due to the majority of inactive compounds in bioassay, the specificity values of some classifiers are slightly better than that of our classifier on the AID439 and AID644 data sets. Besides, we also see that the catch rate of the MK–MCOC–FP method is almost immune to the influence of high class-imbalance while other methods often heavily bias towards the majority-class instances and gain the poor predictive performance for the minority-class instances. Finally, it is obvious that for the total predictive performance the MK–MCOC–FP with the RBF kernel is slightly better than that of the linear kernel.Similarly, by combining grid search and the stratified 5-fold CV method neural networks with different topological structures are trained on the training subsets and tested on the independent test sets. The average of the best predictive performance are calculated and reported in Table 8as below.Compared with the predictive performance in Table 2–7, we see that neural network generally has the better identification rate of inactive compounds (specification) than other classifiers. Due to the majority of inactive compounds and the minority of active compounds in data sets the neural network methods gives the poor sensitivity prediction as the evaluation results shown in Table 8. However, by using feature sparsification or selection and the class-imbalanced penalty factors MK–MCOC–FP approach has the better identification rate of active compounds than that of neural network and F1 score, MCC and AUC also verified the analytic results.In order to compare predictive performance of different classifiers, statistical tests are often used on the same data sets [2]. The study shows non-parametric tests are safer and stronger than parametric tests without the assumption of normal distributions [11]. The Wilcoxon signed-ranks test (WSR-Test) is a non-parametrically statistical test and often used to rank the differences in performances of two classifiers. To this end, here we employ the WSR-Test method to compare the classification performance of MK–MCOC–FP with that of MCOC, SVM and FSVM. In order to compare the predictive performance of two classifiers, the statistical values of total accuracy, sensitivity, specificity, F1 score, MCC and AUC measures are calculated with the six bioassay data sets respectively.For the WSR-Test method, letdibe the difference between the performance measures of the two classifiers on theithindependent test set. The differences are ranked according to their absolute values and average ranks are assigned in case of ties. LetR+be the sum of ranks for the independent test set on which the first classifier outperformed the second, and letR-be the sum of ranks for the opposite. Rank ofdi=0are split averagely among the sums. We haveR+=∑di>0rank(di)+(1/2)∑di=0rank(di),(41)R-=∑di<0rank(di)+(1/2)∑di=0rank(di).Let t be the smaller of the above sumsR+andR-, that is, we havet=min(R+,R-). Therefore, the z statistic of the WSR-Test method is distributed approximately normally, which can be denoted as(42)z=t-(1/4)K(K+1)(1/24)K(K+1)(2K+1).whereK=6. With the level of significance 0.05, the null hypothesis should be rejected ifz<-1.96, and then we can draw a conclusion that the difference between the two classifiers is significant statistically. Thus the statistics of the WSR-Test are calculated with the six bioassay data sets and shown in the following Tables 9.The results of nonparametric statistical test demonstrated in Table 9 show that the MK–MCOC–FP models achieve the statistically significantly better sensitivity than that of other methods for predictive active compounds in bioassay. And the results of the WSR-Test are almost consistent with the experimental results of predictive performance of different classifiers shown in Section 4.4. In other words, our proposed MK–MCOC–FP totally outperforms linear MCOC, quadratic MCOC, SVM and fuzzy SVM with the linear kernel on the sensitivity, F1 score, MCC and AUC. However, for the total accuracy and specificity measures we find that there is no statistically significant difference between MK–MCOC–FP and other classifiers, and the results show that our proposed method can accurately predict active compounds in bioassay without degrading the total accuracy and the identification rate of inactive compounds.

@&#CONCLUSIONS@&#
