@&#MAIN-TITLE@&#
A new training scheme for neural networks and application in non-linear channel equalization

@&#HIGHLIGHTS@&#
A Recently proposed DSO trained ANN used for the problem of channel equalization.This paper introduced a novel strategy for equalization of nonlinear channels using this DSO trained neural network.Proposed method of channel equalization performs better than contemporary equalization methods used in the literature.

@&#KEYPHRASES@&#
Artificial Neural Network,Channel equalization,Directed search optimization,

@&#ABSTRACT@&#
This paper deals with the problem of equalization of channels in a digital communication system. In the literature, artificial neural network (ANN) has been increasingly used for the said problem. However, traditional methods of ANN training fall short of desired performance in the problem of equalization. In this paper, we propose a recently proposed training method for ANN for the problem. This training uses directed search optimization (DSO) as a trainer to neural networks. Then, we apply the same to the problem of nonlinear channel equalization and in that way, this paper introduces a novel strategy for equalization of nonlinear channels. Proposed method of channel equalization performs better than contemporary equalization methods used in the literature, as evident from extensive simulation results presented in this paper.

@&#INTRODUCTION@&#
A channel equalizer recovers digital information from digital communication channels. The use of ANN for channel equalization is an area of increased demand in last three decades [1–4]. Artificial neural network models are more preferred over others for equalization because of their capable performance in handling non-linear problems. However, ANNs trained with popular back propagation (BP) algorithm are associated with three distinct limitations, i.e., (1) easily trapped in local minima for nonlinear classification problems, (2) slow convergence speed and (3) convergence greatly depends on choice of momentum, learning rate and weights. This tempted the research to search for alternative training methods for ANN. In last few years, nature inspired algorithms attempted to take this required position.Genetic algorithm (GA) [5–7] and particle swarm optimization (PSO) [8–10] are also used for training ANN based channel equalizers [11,12]. This inspires the researchers for use of other nature inspired algorithms. In an attempt, Patra et al. [13] used differential evolution (DE) trained trigonometric FLANN in channel equalization.Nurhan Karaboga [14] identified two limitations in GA, i.e., lack of good local search ability and premature convergence. Van den Bergh [15] shows that the particle solutions in the PSO have the limitations of searching only in a finite space and may also fall into local minima. Sensitivity in selection of controlling parameters remains as a limitation of DE [16]. Looking at these limitations (of GA, PSO and DE), this paper chosen to use recently proposed DSO trained ANN [17,18] for equalization.The organization of the paper is as follows: Section 2 discusses the problem of channel equalization in the system model. Section 3 reproduces DSO training to ANN equalizer. Performance of the proposed equalizer is studied in Section 4. Summary of the paper is outlined in Section 5.Equalization of nonlinear channel is illustrated through a base-band model of digital communication systems as shown in Fig. 1.For a binary transmitted sequence, which is denoted as x(k) for the kth time instance, A linear channel popularly modelled as a FIR filter. The channel with output,y1(k), at time instant k is represented as:(1)y1(k)=∑i=0N−1hix(k−i)Here,hi(i=0,1,…,N−1)are the channel tap values and N is the length of the FIR channel. Nonlinear distortions introduced in the channel during the process of transmission are shown by a separate block ‘NL’. A popular nonlinear function is:(2)y(k)=F(y1(k))=y1(k)+by1(k)3Here, b is a constant value. Hence, the output of the nonlinear channel is:(3)y(k)=∑i=0N−1hix(k−i)+b∑i=0N−1hix(k−i)3The output of the channel y(k) is corrupted by noise, η(k), which is modelled in this paper, as an additive white Gaussian noise process with a zero mean and varianceσ2. The corrupted signal r(k) is received at the receiver, i.e., input to the equalizer, is given by:(4)r(k)=y(k)+η(k)Equalizer cancels the effects of channel distortion and noise and recovers transmitted symbol,x(k−δ), with a-priory received signal knowledge, while the channel inherited by the transmission delay, δ.The desired output signal denoted as d(k) is defined by:(5)d(k)=x(k−δ)The equalization process is treated as a classification problem (2)–(5), where equalizer input spacex(k)=[x(k),x(k−1),…,x(k−N+1)]Tis partitioned by the equalizer into two distinct regions (for binary transmitted symbols,x(k−δ)).The optimal solution of the nonlinear classification method is the Bays solution. The decision function for the optimal Bays solution is:(6)fbay(x(k))=∑j=1nβjexp−x(k)−cj2σ2For binary transmitted symbols:(7)βj=+1cj∈Cd(+1)−1cj∈Cd(−1)Here,Cd(+1)/Cd(−1)is the set of channel states, cjbelongs to the transmitted symbol,x(k−δ)=±1andσ2is the noise variance.In Fig. 1, the block “Equalizer” is a neural network. number of layers and number of neurons in each layer (except input layer) for this ANN are decided by optimized values and combination of these values using DSO. In input layer, number of neuron is kept same as N, i.e., number of channel taps.The response of the equalizer is:(8)fANN(x(k))=∑j=1nwjexp−x(k)−tj2αjHere, tjand αjare respectively: the vector centres and the spreads of the neurons in the hidden layer (s). The vector wjrepresents the connecting weights.The output of the ANN equalizer of Eq. (1) implements the nonlinear function of Eq. (2) when the centres of neurons of the hidden layer tjare same as the channel states cjand the connecting weights are regulated properly.The ANN decision function, i.e., output of equalizer, is:(9)xˆ(k−δ)=+1fANN(x(k))≥0−1elsewhereHence, the difference between the equalizer output (i.e.,sa(k)=xˆ(k−δ)) and desired signal (i.e.,sd(k)=x(k−δ)) is termed as error, e(k), and used for updating the equalizer weights. Two parameters, mean square error (MSE) and bit error rate (BER), are popularly taken as performance index.In this paper, we concentrate on cancellation of effects of channel distortion and noise and we do not use any specific modulation scheme.For ease in reading, DSO [18] as proposed by Dexuan Zou reproduced in this section adopted by its application to channel equalization. The solution vectors form the population. The algorithm stops when it reaches a predefined maximum number of iterations (K).•All six parameters of the algorithm are initialized in step 1.These parameters are population size (PS); K; forward probability (pα); forward coefficient (α); backward coefficient (β) and genetic mutation probability (pm).The initial population is generated in step 2.To generate the initial population, a uniform distribution is chosen within the range of[xiL,xiU](i=1,2,…,N), as:(10)Pop=x11x21⋯xN1x12x22⋯xN2⋮⋮⋱⋮x1PSx2PS⋯xNPSHere,xijis the ith(i=1,2,…,N)component of the jth(j=1,2,…,PS)solution vector.Non-optimal solution vectors are updated in step 3 using the procedure is as illustrated in Fig. 2.In the figure, jgrepresents the global best solution vector, and r is a random number in the range [0,1]. Here,xij(k)andxij(k+1)respectively represent the ith component of the jth position vector in the kth iteration and its updated component. In kth iteration,xijg(k)represents the corresponding global best component. The lower and the upper bound of the same component respectively represented byxiLandxiU.Position is updated as shown in Fig. 3. In the figure,xij(k)is located at P, andxijg(k)is located at Q. xvis located at V, which is on the forward extension of line segment PQ. The space between P and V is termed as the forward region. Similarly, xsis located at S that is on the extension of line segment PQ in backward direction, the corresponding space between P and S is termed as backward region. For better convergence DSO adopts updating strategies like that of PSO algorithm, i.e., to follow the successful counterparts. But, DSO adopts a different strategy for updating the position. In this updating strategy,xij(k)followsxijg(k), and hence the main region of search is the forward region that is in fact a region nearxijg(k). Updating ofxij(k)is determined bypα: the forward region will be the right choice ifpαis satisfied, otherwise, we will get back to the backward region. The backward region is an additional region used in slowing down the convergence speed of the DSO to avoid the premature convergence. We definestepij(k)=xijg(k)−xij(k)as an adaptive step. Initially, the population is irregular in the solution space that is beneficial for the global search ability of the DSO; in final part of the algorithm, updating of the position makes the most of the solution vectors to become close to each other.In DSO, the adaptive steps are kept small for better local search; in other words,stepij(k)adjusted dynamically to keep a balance between the global and the local search.The diversity of individuals increased by genetic mutation operation used in step 3 in order to improve the performance of DSO avoiding premature convergence by falling to the local minima.•The stopping criterion is checked in final step, i.e. step 4.The algorithm runs till it satisfies maximum number of iterations K, otherwise, it repeats the step 3.The proposed channel equalization method uses a DSO trained ANN. ANN used in channel equalization have been explained in Section 2, and DSO explained in Section 3.1. In this section we introduce DSO trained ANN for the problem of equalization.Above mentioned training process mimics the “teaching process” in an organization, i.e. administrator, teacher and student. In an institution, the administrator formulates the guidelines for teaching process. The teacher then teaches a student abiding these guidelines. In the present context, ANN plays two possible roles, that of an administrator and a student, while DSO discharges the function of a teacher. The detailed training process of ANN using DSO is illustrated through a flow chart as shown in Fig. 4.First, ANN acts as an administrator that formulates guidelines for DSO (teacher). Then the teacher, DSO, teaches the ANN for particular applications, channel equalization in this paper.For performance evaluations, two parameters, mean square error (MSE) and bit error rate (BER), were used as index for performance measurement. For the purpose of comparison, we have reproduced standard versions of GA and PSO. For the simulations parameters of GA, PSO and DSO are outlined in Table 1. Control parameters for DSO are chosen same as used by Zou [18] except for population size which is chosen as 50 for comparison with GA and PSO.Simulations were conducted for binary signals and three different popular channels with following transfer functions were studied.(11)H(z)=0.26+0.93z−1+0.26z−2(12)H(z)=0.304+0.9029z−1+0.304z−2(13)H(z)=0.341+0.876z−1+0.341z−2Type of nonlinearity considered for all three channels is as explained in Eq. (2).For comparison purpose, we have reproduced following four equalizers.•BP trained ANN-equalizers of [19] (in the figures marked as BP-ANN)GA and PSO trained ANN-equalizers of [11] (in the figures marked as GA-ANN and PSO-ANN respectively)DE trained ANN-equalizers of [13] (in the figures marked as DE-ANN)For evaluation of MSE, signal to noise ratio (SNR) is kept fixed at 10dB. MSE plots for channels (11)–(13) are plotted in Figs. 5–7respectively.Similarly, Figs. 8–10respectively show the corresponding BER plots.It is observed from the figures that:•Channel 11: MSE performance of proposed DSO-ANN is similar up-to 300 iterations DE-ANN, after which DSO-ANN performs better than DE-ANN. However, DSO-ANN out performs other three ANN equalizers right from the beginning. Similarly, BER performance of DSO-ANN is better than other ANN-equalizers and becomes less than 10−5 at SNR of 10dB.Channel 12: MSE performance of proposed DSO-ANN is comparable with DE-ANN, with DSO-ANN performing slightly better. After 600 iterations, PSO-ANN also performs well. In BER comparison up-to SNR of 2dB, performance of DE-ANN, PSO-ANN and DSO-ANN are comparable, with DSO-ANN slightly better than the other two. However, after SNR of 2dB, DSO-ANN shows better performance than other ANN-equalizers and becomes less than 10−5 at SNR of 10dB.Channel 13: MSE performance of proposed DSO-ANN is better than other ANN-equalizers and becomes less than −20dB after 900 iterations. Also BER comparison shows, DSO-ANN shows better performance than other ANN-equalizers and becomes less than 10−5 at SNR of 16dB.This paper introduced a novel method for channel equalization using ANN. This uses a recently proposed DSO trained ANN.Three different channels with similar nonlinearities were studied for validation of superior performance of proposed equalizers. Comparisons were made with other ANN equalizers. ANN equalizers trained with back propagation, genetic algorithm, particle swarm optimization and differential evolution was considered for comparison. It was observed that, proposed method of channel equalization performs better than other ANN equalizers both in terms of mean square error and bit error rate. This clearly shows that proposed method of ANN training using DSO is superior as compared to use of BP, GA and PSO.The results proved that the proposed DSO trained ANN equalizer has better convergence abilities because of the use of updating strategy of DSO. Use of genetic mutation, as evidenced by the results, also illustrates the better capability of proposed equalizer escaping from the local optimum.Different channels with similar nonlinearities were studied in this paper. The works can be further enhanced to accommodate other kinds of nonlinearities and may be reflected in some of our future works. Similarly, this paper chosen only three popular channels for the simulations and hence the works can be extended for other popular channels models.ANN training using other nature inspired algorithms like artificial bee colony algorithm etc., are also be a potential direction for future research.

@&#CONCLUSIONS@&#
