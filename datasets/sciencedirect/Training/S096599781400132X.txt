@&#MAIN-TITLE@&#
On rotationally invariant continuous-parameter genetic algorithms

@&#HIGHLIGHTS@&#
We show that the standard CPGA is rotationally variant.We then construct a rotationally invariant CPGA.We ensure diversity using a modified mutation scheme.We also ensure diversity by adding a self-scaling random vector.

@&#KEYPHRASES@&#
Continuous parameter genetic algorithm,Rotational variance,Crossover,Mutation,

@&#ABSTRACT@&#
We examine the rotational (in)variance of the continuous-parameter genetic algorithm (CPGA). We show that a standard CPGA, using blend crossover and standard mutation, is rotationally variant.To construct a rotationally invariant CPGA it is possible to modify the crossover operation to be rotationally invariant. This however results in a loss of diversity. Hence we introduce diversity in two ways: firstly using a modified mutation scheme, and secondly by adding a self-scaling random vector with a standard normal distribution, sampled uniformly from the surface of a n-dimensional unit sphere to the offspring vector. This formulation is strictly invariant, albeit in a stochastic sense only.We compare the three formulations in terms of numerical efficiency for a modest set of test problems; the intention not being the contribution of yet another competitive and/or superior CPGA variant, but rather to present formulations that are both diverse and invariant, in the hope that this will stimulate additional future contributions, since rotational invariance in general is a desirable, salient feature for an optimization algorithm.

@&#INTRODUCTION@&#
To set the scene, we depart with a verbatim setting from Wilke et al. [1]: Consider the scale and frame in which an optimization problem is defined. Fig. 1depicts two reference framesxandxˆ, related by a scale factor s, translation by a vectort, and rotation by a proper orthogonal matrixQ, i.e.xˆ=t+sQx. A given function is expressed in these reference frames asf(xˆ)=f(t+sQx). An alternative but equivalent interpretation of Fig. 1 is that f andfˆare two distinct functions described in the same reference framex, i.e.f(x)=fˆ(t+sQx).Let us now consider ‘invariance’ in the context of optimization algorithms. ‘Scale invariance’ implies algorithm performance that is independent of uniform scaling of all variables. ‘Frame invariance’ on the other hand implies algorithm performance that is independent of frame translation and rotation.Frame indifference (or objectivity), is well known in classical mechanics [2], where physical laws dictate that this principle must hold. However, no corresponding law in optimization theory requires that an optimization algorithm must be frame invariant. It might therefore seem that frame indifference is merely an aesthetic requirement.However, arguments in favor of frame invariant optimization algorithms do exist. Due to the lack of a physical justification, the arguments are necessarily based on a user’s perspective. If a particular optimization algorithm is frame dependent, it follows that there exists a specific choice of reference frame in which the problem can be solved easier (i.e. requiring less iterations) or better (i.e. achieving a lower cost function value) as compared to some other reference frame. In general, the performance difference in different reference frames cannot be quantified, and depends amongst others on the optimization problem and algorithm specifics. Since a priori knowledge of the optimal reference frame for a particular problem is seldom available, this places an additional burden on the analyst, which now has to consider solving the problem in a number of reference frames. (An exception being the specialized class of separable functions.) If the algorithm’s frame dependency is severe, the analyst requires some external procedure to take the algorithm’s frame dependency into account. A conceptual procedure is to recast the optimization problem to simultaneously solve for the reference frame and solution, but this renders the problem ill-posed.An alternative phrasing of the above argument is as follows: A frame dependent algorithm implies a bias towards some particular reference frame (or frames). This in turn implies a bias towards some subclass of problems. If a problem is well suited to be solved in a particular reference frame, a frame dependent algorithm that is aligned with this specific reference frame will perform well. This however implies that frame dependent optimization algorithms are specialist algorithms, tuned to perform well on a special subclass of problems. In contrast, frame invariant optimization algorithms are general algorithms, applicable to a larger class of problems.But, how do we know if a problem is well suited to be solved in a particular reference frame? In general, we do not. Therefore, the choice of a frame dependent algorithm to solve a particular problem makes the tacit assumption that the problem’s reference frame is well matched with the algorithm’s reference frame bias. If the tacit assumption holds, good performance is expected, since the correct assumption implies additional problem information. Again, an example of practical relevance is the class of separable or decomposable optimization problems, where an n-dimensional problem is simply the sum of n 1-dimensional problems. An algorithm that independently searches along the coordinate axes, (which renders such an algorithm frame dependent), exploits this specific function characteristic. It is therefore expected that such an algorithm will be superior to its frame invariant counterpart.To summarize, frame invariance of optimization algorithms is not a strict requirement. Frame invariance does however provide a useful classification of optimization algorithms. A frame invariant algorithm requires less a priori knowledge (or tacit assumptions) of the optimization problem, as compared to a frame variant algorithm. This implies that a frame invariant algorithm will have either inferior or superior performance as compared to a frame variant algorithm, depending on the validity of the assumed information. In addition, frame invariance is desirable as it increases the predictive power of numerical performance results [3].Even though frame invariance is not required, some classes of optimization algorithms do satisfy this principle. In classical gradient based optimization [4], the gradient vector (or some conjugate direction to the gradient), indicates some direction of improvement, even if this direction is not optimal. The gradient vector of any scalar function satisfies the transformation rules for changing both scale and reference frame. This (usually) renders classical gradient based optimization algorithms scale and reference frame invariant.However, in so-called modern (stochastic) optimization procedures, like the CPGA, few algorithms satisfy the requirement of reference frame invariance. In this study we introduce two rotationally invariant CPGAs for which we show competing performance against the standard variant CPGA on a test set in the unrotated and an arbitrarily rotated reference frame.Our paper is constructed as follows: we present the concept of rotational invariance in Section 2. In Section 3, we give the formal mathematical optimization problem. We then present the continuous-parameter genetic algorithm used in this study in Section 4. We present our first invariant continuous parameter genetic algorithm in Section 5.1 and the second in Section 5.2. In Section 6, we present numerical results for some popular test functions, in both the unrotated and arbitrary rotated reference frames. Finally, we conclude our study in Section 7.

@&#CONCLUSIONS@&#
