@&#MAIN-TITLE@&#
Low-discrepancy sequence initialized particle swarm optimization algorithm with high-order nonlinear time-varying inertia weight

@&#HIGHLIGHTS@&#
Initial population particles are generated by the Halton sequence to fill the search space efficiently.1/π2 order nonlinear function is selected to adjust the time-varying inertia weight.Two acceleration coefficients are set to be constants.Performance of the proposed method is tested by a set of well-known benchmark optimization problems.

@&#KEYPHRASES@&#
Low-discrepancy sequence,Particle swarm optimization,1/π,2 order nonlinear function,Nonlinearly decreasing inertial weight,Constant acceleration coefficients,

@&#ABSTRACT@&#
Particle swarm optimization (PSO) is a population-based stochastic optimization algorithm motivated by intelligent collective behavior of some animals such as flocks of birds or schools of fish. The most important features of the PSO are easy implementation and few adjustable parameters. A novel PSO method called LHNPSO, with low-discrepancy sequence initialized particles and high-order (1/π2) nonlinear time-varying inertia weight and constant acceleration coefficients, is proposed in this paper. The initial population particles are generated by using the Halton sequence to fill the search space efficiently. Nonlinear functions with orders varied within big ranges are employed to adjust the inertial weight, cognitive and social parameters. Based on the sensitivity analysis of PSO performance to the changes of the orders of these nonlinear functions, 1/π2 order nonlinear function is selected to adjust the time-varying inertia weight and the two acceleration coefficients are set to be constants. A set of well-known benchmark optimization problems is then used to investigate the performance of the proposed LHNPSO algorithm and facilitate the comparison with other three types of PSO algorithms. The results show that the easily implemented LHNPSO can converge faster and give a much more accurate final solution for a variety of benchmark test functions.

@&#INTRODUCTION@&#
With the inspiration motived by the research results on the modeling and the simulations of the behavior reflected in flocks of birds, Kennedy and Eberhart proposed the particle swarm optimization (PSO) algorithm [1]. The algorithm is a stochastic population-based method and is regarded as a global search strategy. In the PSO, particles move through the problem space with a specified velocity in search of optimal solution. Each particle maintains a memory which helps it keep the track of its previous best position and the global best position. The most important advantages of PSO are that it is easy to implement and has few parameters to adjust [2]. Commonly, the adjustable parameters are inertial weight, cognitive and social parameters.PSO algorithm has attracted considerable attention and has been used in many research areas over the past decades [3–11]. Generally, convergence speed and ability to find global optima are basic criteria for assessing the performance of an optimization method. A large number of improved PSO algorithms [12–22] have been proposed to achieve the two goals, that is, faster convergence speed and avoiding premature or local optima.The convergence speed of a PSO algorithm is dependent on the inertia weight and two acceleration coefficients. Random [23], linear time-varying [24], nonlinear time-varying [25,26] and adaptive strategies [27–29] have been used to adjust the inertia weight to improve the convergence speed. A review and summary of various inertia weight modification mechanisms reported in literature can be found in reference [2]. Nonlinear time-varying and adaptive inertia weights normally have better performance than others. Moreover, the principle and implementation of nonlinearly decreasing inertia weights are simpler than those of adaptive ones. Currently, third and less order nonlinear functions have been investigated extensively for adjusting inertia weight, however, the performance of higher order nonlinear time-varying weight needs to be further studied.The concept for initializing the particles of most PSO algorithms is same as that for generating random numbers in the traditional Monte-Carlo simulation method which could be regarded as the original stochastic optimization method. In contrast to traditional Monte-Carlo methods using pseudo random numbers, the quasi-Monte Carlo method produces deterministic sequences of well-chosen points that provide the best-possible spread in the change ranges of variables [30]. These deterministic sequences are often referred to as low-discrepancy sequences filling the sample area efficiently and uniformly [31], which have been successfully used to solve globally optimal problems [30–33].The major objective of this paper is to propose an improved PSO algorithm to benefit by the advantages of high-order nonlinear time-varying inertia weight and low-discrepancy sequence. This paper is organized as follows. Section 2 reviews the classical PSO and some commonly accepted improved version of PSO. In Section 3, different-order nonlinear functions, varying from 1/18 to 9, are tested to adjust the inertia weight and two acceleration coefficients, and the corresponding parametric studies on the orders are also implemented. The improved PSO with adjusted coefficients are further test with different population sizes. Section 4 presents an improved PSO method with low-discrepancy sequence initialized particles and high-order (1/π2) nonlinear dynamic varying inertia weight (LHNPSO), and experimental results by using well-known benchmark test functions. Section 5 gives a brief conclusion about this study.PSO algorithm starts with a population of particles randomly initialized in the search space. Each particle represents a potential solution. The algorithm searches the optimal solution by moving the positions of particles in the search space. The position and velocity of the ith particle are represented by n-dimensional vector xi=(xi1, xi2, …, xin) andvi=(vi1,vi2,…,vin), respectively. The particle moves its current position toward the global optimum based on two items, that is, the best position encountered by this particle and the best position visited by the whole swarm. The velocity and position of the particle are updated according to the following formulations(1)vi(k+1)=wvi(k)+c1r1∘(xiPb−xi(k))+c2r2∘(xGb−xi(k))(2)xi(k+1)=xi(k)+vi(k+1)where k is the iteration number,vi(k)denotes the velocity of the ith particle at the kth iteration, xi(k) represents the position vector of the ith particle at the kth iteration, vectorxiPbis the best position visited by the ith particle, vector xGbis the global best location found by the whole swarm until the current iteration.wis the inertia weight controlling the influence of the previous velocity on the current one. In this paper, the maximum velocity is 20% of the search range divided by the size of particles. c1 is the cognitive parameter, and c2 is the social parameter. The two acceleration coefficients and represent dependent settings which indicate the degree of confidence in the best solution found by the individual particle and by the entire swam, respectively. r1 and r2 are two random vectors consist of random numbers uniformly distributed in the interval [0, 1]. The symbol ∘ in Eq. (1) denotes the Hadamard product.The global convergence of a PSO algorithm is dependent on the degree of local/global exploration controlled by the two acceleration coefficients, meanwhile, the relative rate of convergence is affected by the inertia weight parameter. Research results have shown that for a fixed/constant inertia value there is a significant reduction in the algorithm convergent rate. In the earlier optimization stage, a large inertia weight is required in order to search the design space thoroughly. When the most promising areas of the design space have been discovered and the convergence rate starts to slow down, the inertia weight should be reduced, in order for the particles’ momentum to decrease allowing them to concentrate in the best design areas. Therefore, Shi and Eberhart [34] proposed an improved PSO algorithm with a linear time-dependent value of the inertia weight (LPSO) to accomplish the aforementioned strategy(3)w(k+1)=wmax−wmax−wminkmax·kwhere k is the iteration number staring from iteration zero; kmax is the maximum number of allowable iteration;wmaxandwminare the maximum and minimum values of the inertia weight, respectively. Usually the value of the inertia weight varies between 0.4 and 0.9.Similarly, the PSO algorithm with linearly decreasing inertia weight and time-varying acceleration coefficients (LPSO-TVAC) has been proposed by Ratnaweera et al. [35]. In the early stage, a large c1 and small c2 allow the particles to move around the whole search space instead of moving toward the population best. In the latter stage, a small c1 and a large c2 allow the particles to converge into the global optimum. c1 and c2 are computed by(4)c1(k+1)=c1i−c1i−c1fkmax·k(5)c2(k+1)=c2i−c2i−c2fkmax·kwhere c1i, c1f, c2iand c2fare initial and final values of the cognitive and social parameters, respectively. The best reported results were achieved when c1i=c2f=2.5 and c1f=c2i=0.5 [20].Although numerous improved PSO algorithms have been presented and applied in many research areas, solving optimization problems with high accuracy and rapid convergence speed is still an important task. Both LPSO and LPSO-TVAC achieve better convergence and accuracy than classical PSO, and also keep the developed formulations structurally simple and easily understandable, making these variants become handy tools to solve optimization problems in a wide range of subjects. In other words, these two variants preserve one of the major merits of PSO, easy implementation. Because of this, in this paper, we manage to further improve PSO based on the ideas from LPSO and LPSO-TVAC, which will be explained at a greater extent in the next section. PSO, LPSO and LPSO-TVAC are all adopted as the reference methods.In contrast to linear time-varying inertia weight and acceleration coefficients, in this study, nonlinear functions are proposed to adjust them. Non-linear high-order function has advantage to update PSO parameters. For instance, a faster reduction of the inertia weight can be achieved in the early stage, which will increase the rate of convergence. In the surrounding area of the optimum, the reduction of the inertia weight becomes slower, which will be helpful for capturing the solution.(6)w(k+1)=wmax−(wmax−wmin)kkmaxα(7)c1(k+1)=c1i−(c1i−c1f)kkmaxβ(8)c2(k+1)=c2i−(c2i−c2f)kkmaxγActually, constant and linear time-varying parameters are special cases of what are described by the above formulations. Inertia weight, cognitive and social parameters are constants when α=β=γ=0, and they are linearly time-varying parameters when α=β=γ=1.To investigate the influences produced by α, β and γ on the performance of PSO, the values taken for them are listed in Table 1. In literature, commonly, a set of well-known nonlinear benchmark functions is used to test the performance of a new particle swarm optimization algorithm. The functions, search space of variables and their optimal values are given in Table 2. Sphere, Rotated hyper-ellipsoid and Ackley functions are used to assess the effectiveness of different values of α, β and γ. Sphere is an unimodal optimization problem. Rotated hyper-ellipsoid is an unimodal function with rotation. Ackley is a multimodal problem which is hard to optimize. These functions are tested in 20 dimensions; the population size for all tests is 10 and kmax equals to 1000. The mean fitness value of the best particle found by 30 independent runs for the three functions are given in Tables 3–5.Table 3 shows that the different values of α can result in quite different optimal solutions. For Sphere function, when the value of α changes within the range from 1/π2 to 1/18, 1000 iterations can yield very good solutions. Similarly, better solutions of Rotated hyper-ellipsoid can be found when α varies from 1/9 to 1/19. The best solution of Ackley can be obtained when α equals to 1/π2. Generally, the optimal solutions generated from 1000 iterations for these three test functions are quite good considering the dimensions of variables and the sizes of particles when α is 1/π2. Overall, taking into account the solutions resulted from 500 iterations, which can reflect the convergence rate more or less, PSO with nonlinearly decreasing inertia weight adjusted by using the value 1/π2 for α has excellent performance.From Tables 4 and 5, it can be easily observed that there are no big differences between the best solutions for all the three functions even when the values of β or γ are obviously different, in another word, the performance of a PSO algorithm is just slightly dependent on the changes of cognitive and social parameters during the iteration process. The main reason is that the effects of the values of these two parameters on the velocity of the particles, as shown in Eq. (1), are dependent on the values of random numbers which are uniformly distributed in the interval [0, 1]. It is the stochastic characteristics of PSO algorithms that the random numbers can change the velocities and positions of particles randomly.In Table 3, the mean best solution for Sphere, Rotated hyper-ellipsoid and Ackley corresponding to α=1/π2 is respectively 4.43E−5, 4.69E−8 and 0.002556 when the iterations are 1000. The best optimum for these three benchmark optimization problems is respectively, 7.444044, 365.9437 and 1.736648 in Table 4, and 2.209919, 32.02607 and 1.12324 in Table 5, under the same iterations. Comparing Tables 4 and 5 and with Table 3, it can be concluded that the change of α produces much greater effect on the PSO performance than β and γ. PSO performance is much more sensitive to the order of nonlinear function which is used to change the inertia weight. When a proper order of the nonlinear function for inertia weight is determined, for example, α takes 1/π2, the contributions made by the time-varying cognitive and social parameters can be ignored. Therefore, the two acceleration coefficients can be treated as constants.Based on the above conclusion, the performance of our improved PSO with α=1/π2,wmax=0.9,wmin=0.4, β=γ=0 and c1i=c1f=c2i=c2f=2.0 are further tested when different population sizes (10, 30 and 50) are adopted. The worst, mean and best fitness values found by 30 runs versus iterations for Sphere, Rotated hyper-ellipsoid and Ackley functions are shown in Figs. 1–9. The comparison of mean fitness convergence speed between different population sizes for these three test functions can be seen in Figs. 10–12. We sample the data at every 10 iterations to 400 iterations and every 50 iterations afterwards to plot these figures. In order to clearly demonstrate the convergence trend, all Figs. 1–12 only capture the most important parts of the whole convergence diagrams.All above figures explicitly illustrate that haphazard increasing population size may not improve the convergence speed when the population size has reached a certain number (that is 10 here). Drastic improvement cannot be observed from the results for Sphere and Rotated hyper-ellipsoid functions. These two functions are continuous, convex and unimodal, so our improved PSO does not require many particles to capture the global optimal. Sometimes, for Ackley function, a larger population size may hinder the convergence and accuracy of PSO. Although it has been concluded that a larger population size should be more effective on functions with more numerous and dramatic local optima, the authors also pointed out that PSO using the star sociometry sometimes converges quickly to a suboptimal point in the early stage instead of thoroughly exploring the whole space [36]. In order to further investigate this phenomenon, Ackley function in 2 dimensions is tested again when population sizes 10, 30 and 50 are adopted. Our improved PSO is able to converge to the global optimal with different population sizes, shown in Fig. 13. It can be therefore concluded that it has higher risk to stuck to a suboptimal region with a larger population size adopted when only the star sociometry is used. The population size from 25 to 60 for Ackley function in 30 dimensions is recommended in [37], but they used dynamic sociometry which is different with this paper. In the future, the stability of the presented PSO algorithm can be further improved by using dynamic topology.Here, we propose a low-discrepancy sequence initialized particle swarm optimization algorithm with high-order nonlinear time-varying inertia weight (LHNPSO). The initial population of particles is generated by using the Halton sequence to cover the search space efficiently. It has to be noted, based on our experience, low-discrepancy sequence initialization will not greatly improve the performance of our proposed method on these benchmark test functions, but low-discrepancy sampling based methods have been widely adopted to solve various real world problems and the increased computational burden can be neglected [33,38,39]; therefore, keeping it is able to ensure the wide implementation of LHNPSO. In addition, inertial weight is varied according to the nonlinear function given in Eq. (8) with α=1/π2,wmax=0.9andwmin=0.4. As the contributions made by the changes of the two acceleration coefficients are not significant, the two acceleration coefficients, cognitive parameter and social parameter are taken as constant values, that is, β=γ=0 and c1i=c1f=c2i=c2f=2.0.To evaluate the performance of the proposed LHNPSO algorithm, the classical PSO, LPSO and LPSO-TVAC are used for comparisons. In the classical PSO, α=β=γ=0,wmax=wmin=0.9and c1i=c1f=c2i=c2f=2.0. In the LPSO, α=1,wmax=0.9,wmin=0.4and c1i=c1f=c2i=c2f=2.0. In the LPSO-TVAC, α=β=γ=1,wmax=0.9,wmin=0.4, c1i=c2f=2.5 and c1f=c2i=0.5. Except the two-dimensional functions Easom and Brain, other benchmark problems are tested in 20 dimensions. The population size for all tests is 10. The mean fitness values of the best particle found by 30 runs for the ten functions are given in Table 6, while the iterations are 1000. To illustrate the convergence speed, the mean fitness values of the best particle found by 30 independent runs versus the iterations for functions Sphere, Schwefel's, Rotated hyper-ellipsoid, Rosenbrock and Ackley are shown in Figs. 14–18. Similar to Figs. 1–12, Figs. 14–18 also capture the most bottom parts of the whole convergence diagrams.Table 6 shows that all the four PSO algorithms can find very good solutions for Brain function which is a simple two dimensional problem. PSO, LPSO and the proposed LHNPSO can get much more accurate results for Easom function than LPSO-TVAC using 1000 iterations. It should be noted that the time-varying acceleration coefficients may not improve the PSO performance for some optimization problems as the similar phenomenal can be found in Fig. 17 for function Rosenbrock. As what is explained in Section 3, the contributions of the time-varying cognitive and social parameters are largely affected by the random numbers. For other eight benchmark test functions, LHNPSO generates significantly better solutions than PSO, LPSO and LPSO-TVAC.Figs. 14–18 show that LHNPSO converges much faster for all the five functions than the classical PSO, LPSO and LPSO-TVAC. LPSO-TVAC does show the worst performance for function Rosenbrock. It is clear that LHNPSO converges quickly to the global/real optima for all the five benchmark problems, however, the classical PSO, LPSO and LPSO-TVAC hardly hit the optima, which exhibits obvious prematurity and may fall into the local optima. The proposed LHNPSO can effectively prevent the premature convergence and significantly improve the convergence rate and accuracy. In summary, for all the test functions, the proposed LHNPSO shows a better performance than the classical PSO, LPSO and LPSO-TVAC.

@&#CONCLUSIONS@&#
A new variant of PSO is proposed which employs the low-discrepancy sequence and decreasing inertia weight adjusted by high-order (1/π2) nonlinear function. The performance of LHNPSO algorithm is tested by a set of well-known benchmark test functions and the results show that this method can converge faster and give a much more accurate final solution in comparison with PSO, LPSO and LPSO-TVAC algorithms. Moreover, the proposed LHNPSO is very easily to be implemented. From this study, it can also be obtained that the performance of PSO algorithm is not significantly affected by the time-varying cognitive and social parameters. Sometimes, changing acceleration coefficients even impairs PSO performance. In this paper, only the unconstrained optimization problems are used to test the performance of LHNPSO. In the future, LHNPSO will be further developed to solve more complex problems.