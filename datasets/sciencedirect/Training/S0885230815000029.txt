@&#MAIN-TITLE@&#
A hybrid approach to dialogue management based on probabilistic rules

@&#HIGHLIGHTS@&#
We present a new, hybrid modelling framework for dialogue management based on probabilistic rules.The probabilistic rules function as high-level templates for the generation of a directed graphical model.The rule parameters may be estimated from dialogue data via Bayesian inference.The OpenDial toolkit allows system designers to develop dialogue systems using probabilistic rules.User evaluation in a HRI domain shows that the approach outperforms traditional hand-crafted and statistical models.

@&#KEYPHRASES@&#
Spoken dialogue systems,Dialogue management,Probabilistic graphical models,Bayesian inference,Human–robot interaction,

@&#ABSTRACT@&#
We present a new modelling framework for dialogue management based on the concept of probabilistic rules. Probabilistic rules are defined as structured mappings between logical conditions and probabilistic effects. They function as high-level templates for probabilistic graphical models and may include unknown parameters whose values are estimated from data using Bayesian inference. Thanks to their use of logical abstractions, probabilistic rules are able to encode the probability and utility models employed in dialogue management in a compact and human-readable form. As a consequence, they can reduce the amount of dialogue data required for parameter estimation and allow system designers to directly incorporate their expert domain knowledge into the dialogue models.Empirical results of a user evaluation in a human–robot interaction task with 37 participants show that a dialogue manager structured with probabilistic rules outperforms both purely hand-crafted and purely statistical methods on a range of subjective and objective quality metrics. The framework is implemented in a software toolkit called OpenDial, which can be used to develop various types of dialogue systems based on probabilistic rules.

@&#INTRODUCTION@&#
The design of dialogue strategies is a challenging task in the development of spoken dialogue systems (SDS). The selection of system actions is often grounded in a complex dialogue state encompassing a variety of factors such as the dialogue history, the user goals and preferences, the external context and the task to perform. In addition, spoken dialogue is also riddled with uncertainties arising from speech recognition errors, ambiguous inputs, partially observable environments, and unpredictable dialogue dynamics. These difficulties are particularly striking in the case of human–robot interaction (HRI). By their very definition, human–robot interactions take place in a physical, situated environment that must be captured and monitored by the robotic agent. They must also typically deal with high levels of noise and uncertainty caused by e.g. imperfect sensors and actuators. The robot's tracking of the current dialogue state is therefore bound to remain partial and error-prone.Two families of dialogue management approaches have been historically developed to address these issues. The first family relies on hand-crafted strategies, ranging from finite-state automata to more complex inference procedures based on formal logic and classical planning. These strategies provide principled techniques for the interpretation and generation of dialogue moves on the basis of the dialogue participants’ mental states (including their shared knowledge). Dialogue is then framed as a collaborative activity in which the interlocutors work together to coordinate their actions, maintain a shared conversational context, resolve open issues and satisfy social obligations (Allen et al., 2000; Larsson, 2002; Jokinen, 2009). Such approaches can yield detailed analyses of various dialogue behaviours, but they generally assume complete observability of the dialogue state and provide only a limited account of errors and uncertainties. In addition, the knowledge bases from which the system's decisions are derived must be completely specified in advance by domain experts. Their deployment in practical applications is thus non-trivial.The second family relies on statistical modelling techniques (Levin et al., 2000; Roy et al., 2000; Young et al., 2010; Rieser and Lemon, 2011). The dialogue is here represented as a stochastic control process – often a Markov decision process (MDP) or a Partially observable Markov decision process (POMDP) – and the optimal dialogue strategy is the one that maximises the system's long-term expected utility. These probabilistic models offer an explicit account for the various uncertainties that can arise during the interaction. They also allow the dialogue strategies to be optimised in a data-driven manner instead of relying on hand-crafted mechanisms, making it easier to adapt to new environments or users. However, these probabilistic models typically depend on large amounts of training data to estimate their parameters – a requirement that is hard to satisfy for most dialogue domains. This shortage of relevant datasets is especially critical in human–robot interactions, given the high costs of collecting and annotating dialogue data for these dialogue domains.This article presents a hybrid approach to dialogue management that seeks to combine the benefits of hand-crafted and statistical techniques in a single framework. As in previous work on POMDPs models for dialogue management, the approach represents the dialogue state as a Bayesian network that is regularly updated with new observations and employed to derive the system's actions. However, the domain models are no longer expressed with traditional factored representations but are instead structured via probabilistic rules. As explained in the next pages, the rules can be viewed as high-level templates for probabilistic graphical models. The use of probabilistic rules provides an efficient abstraction layer that allows the system designer to capture the domain models in a concise and human-readable form.The present article is structured as follows. Section 2 reviews the key principles of dialogue management, focusing in particular on MDP- and POMDP-based approaches. Section 3 outlines the formalism of probabilistic rules and their instantiation as nodes of a graphical model. Section 4 describes how the parameters of probabilistic rules can be estimated via Bayesian inference, using either Wizard-of-Oz data (supervised learning) or user interactions (reinforcement learning). Section 5 presents the OpenDial toolkit, a domain-independent dialogue toolkit that allows dialogue developers to construct dialogue systems using probabilistic rules. Section 6 describes a user evaluation of this modelling approach in a human–robot interaction domain. Section 7 contrasts the framework with related work. Finally, Section 8 concludes the article and reviews future research directions.The general architecture of a spoken dialogue system is depicted in Fig. 1. The user speech signals are first processed by the speech recogniser, resulting in a list of recognition hypothesesu˜u, where each hypothesis is associated to a particular probability or confidence score.11Throughout this article, we follow the convention of denoting user-specific variables with the subscript u and machine-specific variables with the subscript m.Dialogue understanding then maps these hypotheses into high-level semantic representations of the dialogue act expressed by the user. These dialogue acts are also expressed as a lista˜uof semantic hypotheses together with their respective probabilities. Dialogue management is then in charge of selecting the best system action to perform given the current conversational context. The dialogue manager outputs a particular action am(which can be void, i.e. leading to no action). If the selected action relates to a communicative act, language generation is triggered to find its best linguistic realisation, denoted um. Finally, the constructed system utterance is sent to a speech synthesiser in order to generate the corresponding audio signal.Dialogue management serves a double function within this processing pipeline, illustrated by the two boxes in Fig. 1. The first function is to maintain a representation of the current dialogue state. The dialogue state is a representation of what is known of the current conversational situation (from the system's point of view) and is often factored in several variables related to the dialogue history, the external context, and the tasks to perform. This dialogue state is regularly updated on the basis of new observations (in the form of e.g. new user utterances or changes in the external context). The second function of dialogue management is to decide which actions to undertake in a particular dialogue state. Dialogue management is therefore essentially a problem of decision-making under uncertainty: the system is provided with multiple observations (which may be partial or erroneous) about the current state of the interaction and must find the best action to execute in this state. We describe in the next section how such decision problems can be formalised in statistical terms.Statistical approaches to dialogue management typically represent interactions as a Markov decision process (MDP)〈S,A,T,R〉whereSdenotes the state space,Athe action space, T the transition function that encodes the probability P(s′|s, a) of reaching state s′ after executing action a in state s, and R the reward function that expresses the reward value associated with the execution of action a in state s. The state space corresponds to the set of possible dialogue states, while the transition function captures the internal dynamics of the conversation, indicating how the dialogue state is expected to change as a result of the system actions. Finally, the reward function R represents the particular objectives and costs of the application domain.Given a particular MDP, the goal is to find a policyπ:S→Athat maps each state to the best action to execute at that state. The best action is defined as the action that maximises the expected return for the agent, which corresponds to the expected long-term accumulation of rewards from the current state up to a given horizon. As the transition model is usually unknown, various methods have been devised to automatically extract this optimal policy from experience via reinforcement learning. Due to the high number of cycles necessary to converge onto a near-optimal policy, interactions with real users are often impractical. Instead, most approaches have relied on the construction of a user simulator able to generate unlimited numbers of interactions on the basis of which the dialogue system can optimise its policy. The user simulator can either be designed by experts or “bootstrapped” from dialogue data extracted from recordings of human–human dialogues or Wizard-of-Oz experiments (Pietquin, 2008; Frampton and Lemon, 2009; Rieser and Lemon, 2011). The user simulator allows the agent to explore millions of dialogue trajectories on a scale that would be impossible to achieve with real users. Simulated interactions, however, run the risk of deviating from actual user behaviours. They are also more difficult to apply in human–robot interaction (and other types of situated dialogue domains) due to the need to capture the physical environment in addition to the user.A limitation faced by MDP approaches is the assumption that the dialogue state is fully observable, making it difficult to account for uncertainties arising from e.g speech recognition errors. A solution is to extend the MDP framework by viewing the state as a hidden variable that is indirectly inferred from observations. Such an extension gives rise to a Partially Observable Markov decision process (POMDP). POMDPs are formally defined as tuples〈S,A,T,R,O,Z〉. As in a classical MDP,Srepresents the state space,Athe action space, T the transition probability between states, and R the reward function. However, the actual state is no longer directly observable. Instead, the process is associated with an observation spaceOthat expresses the set of possible observations that can be perceived by the system (for instance, the N-best lists of user dialogue acts). The function Z defines the probability P(o|s) of observing o in state s.In the POMDP setting, the system's knowledge at a given time is represented by the belief state, which is a probability distribution P(s) over all possible states and is often factored as a Bayesian network (Bui et al., 2009; Thomson and Young, 2010). The belief state is continuously updated as new observations become available. Given a dialogue system with current belief state b that executes a system action a followed by observation o, the updated belief b′ is directly derived from Bayes’ rule:(1)b′(s)=P(s′|a,o)=ηP(o|s′)∑s∈SP(s′|s,a)b(s)where η is a normalisation factor. The observation o can for instance correspond to the N-best listau˜of dialogue act hypotheses. A POMDP policy is then defined as a function mapping each possible belief state to its optimal action. As for MDP-based methods, POMDP approaches often derive the dialogue policy from interactions with a user simulator (Young et al., 2010; Daubigney et al., 2012), although some recent approaches also explored the use of direct interactions (Gašić et al., 2011, 2013). The optimisation process required to extract POMDP policies is, however, considerably more complex than for MDPs, as the belief state space is continuous and high-dimensional. Approximation techniques are therefore necessary in order to extract dialogue policies of reasonable quality, such as grid-based discretisations (Young et al., 2010), linear function approximation (Thomson and Young, 2010; Daubigney et al., 2012) or non-parametric methods based on Gaussian processes (Gašić et al., 2013).One major bottleneck in statistical approaches to dialogue management is the size of the parameter space. The framework presented in this article seeks to reduce the numbers of parameters by taking advantage of expert knowledge about the dialogue domain. More precisely, the framework rests on the idea of representing the transition and utility models of a dialogue POMDP in terms of probabilistic rules. These rules are practically defined as if...then...else constructions that map logical conditions to probabilistic effects, based on the following skeleton:∀x,if(condition1holds)thenDistribution1overpossibleeffectselseif(condition2holds)thenDistribution2overpossibleeffects...elseDistributionnoverpossibleeffectsEach if...then branch specifies both a condition (expressed as a logical formula) and an associated distribution over possible effects. The if...then...else construction is read in sequential order, as in programming languages, until a satisfied condition is found, which causes the activation of the corresponding probabilistic effects. The conditions and effects of the rule may include underspecified variables, denoted x, which are universally quantified on top of the rule.22The variables x are variables in the sense of first-order logic and should not be confused with the random variables of the probabilistic model.The mapping between conditions and effects specified by the rule is in this case duplicated for every possible assignment (grounding) of the underspecified variables, allowing the system designer to abstract over particular aspects of the domain and express the dialogue models with a small number of rules.In line with previous work on POMDP approaches to dialogue management, the dialogue state is represented as a Bayesian network composed of state variables representing various aspects of the current context. The probabilistic rules are then applied at runtime to update this dialogue state on the basis of new observations and select the system actions. As we will see, this process is performed by instantiating the rules as latent nodes in the graphical model.We first present how such rules can be used to express probability distributions, and then show how to generalise the formalism to utility functions. We shall use the notion of probabilistic rules as an umbrella term that covers all types of rules, while probability rules will only refer to rules expressing probability distributions, and utility rules to rules expressing utility functions.A probability rule is formally expressed as an ordered list of branches [br1, …, brn] in which bridenotes the i-th branch of the if...then...else. Each branch briis a pair 〈ci, P(Ei)〉 where ciis a logical condition and P(Ei) is a categorical probability distribution over a set of mutually exclusive effects. The random variable Eirepresents the effect of the rule at branch i and has a set of possible valuesVal(Ei)={ei,1,…,ei,mi}, where miis the number of effects defined for the branch. Each effect ei,jhas a corresponding probability denoted θi,j. These probabilities must satisfy the standard probability axioms.33In other words, the probability values must satisfy to 0≤θi,j≤1 for all values i, j and∑j=1miθi,j=1for every branch of a given rule.Given these elements, a probability rule reads as such:(2)∀x,if(c1)thenP(E1=e1,1)=θ1,1…P(E1=e1,m1)=θ1,m1else if(c2)thenP(E2=e2,1)=θ2,1…P(E2=e2,m2)=θ2,m2…elseP(En=en,1)=θn,1…P(En=en,mn)=θn,mnThe conditions c1, …, cnare expressed as logical formulae over a subset of variables included in the Bayesian network representing the dialogue state. The logical formulae can be constructed using the usual operators from predicate logic (conjunctions, disjunctions, and negations) as well as various binary relations (equality, inequalities, set membership, etc.). The terms of the logical formulae may also include underspecified (i.e. free) variables which are universally quantified on the top of the rule.44Logical quantifiers are only allowed at the top level of a rule, and cannot therefore appear in the inner part of a rule condition or effect. This limitation is introduced to ensure that conditions can be checked efficiently, under real-time constraints. Similar restrictions can be found in planning formalisms (McDermott et al., 1998).The variables employed in the conditions define the input variables of the rule. An example of a rule condition is (au=x∧am=AskRepeat), which is satisfied when the variable auequals an underspecified value x and the variable amequals the value AskRepeat. This particular condition contains two input variables, auand am. Rule conditions can be arbitrarily complex and include nested formulae.The conditions offer a compact partitioning of the state space. Without such a partitioning, a rule ranging over the input variables I1, …, Ikwould need to enumerate Val(I1)×…×Val(Ik) possible conditions. The reliance on the if…then…else structure reduces this number to n partitions, where n corresponds to the number of branches of the rule and is usually small. Partitioning is closely related to the notions of state abstraction and state aggregation in planning and reinforcement learning (Li et al., 2006). State abstraction/aggregation corresponds to mapping a large state space into a more compact, abstract space (corresponding here to the rule conditions).Associated to each condition ciis a collection of mutually exclusive effectsei,1,…,ei,mi. Each effect ei,jrepresents a specific assignment of values for a set of variables called the output variables of the rule. We adopt the convention of denoting output variables with a prime to distinguish them from the input variables. An example of an effect is{au′=x}, which is an assignment of the variable auto the value x. Effects can be void (that is, represent no assignment) and can also express assignments covering more than one output variable. When no terminating else branch is explicitly specified at the end of a rule, a final else associated with a void effect is implicitly assumed to ensure that the partitioning is exhaustive.Each effect ei,jis assigned a probability θi,j=P(Ei=ei,j). The probabilities can be either fixed by hand by the system designer or estimated empirically (as explained in Section 4). For convenience, if the sum of all effect probabilities is lower than 1, we assume that the remaining probability mass is assigned to a void effect.As already noted, the probabilistic rules allow specific elements x inside the conditions and effects of a rule to be underspecified. Based on this quantification mechanism, the rules can cover large portions of the state space in a compact manner. One advantage of this representation is that it allows for powerful forms of parameter sharing, since the effect probabilities θi,jare made independent of the various instantiations of the variables x (see below for an example).(r1)∀x,if(au=x∧am=AskRepeat)thenP(au′=x)=0.9Rule r1 offers a prediction on the next user dialogue actau′based on the last dialogue act auand system's action a. has two input variables: auand amand one output variable:au′. The rule stipulates that if the system asks the user to repeat the last utterance x, the user is expected to comply and repeat x with probability 0.9. A void effect is implicitly associated with the remaining probability mass (0.1 in this example). The universal quantification allows us to express that this probability is independent of the particular dialogue act x uttered by the user.(r2)∀x,if(au=RequestAction(x)∧am=Do(x))thenP(au′=Confirm)=0.2else if(au≠RequestAction(x)∧am=Do(x))thenP(au′=Disconfirm)=0.5P(au′=RequestAction(Stop))=0.3The rule r2 distinguishes between two cases. If the user requests a particular action x to be performed and the system executes it, the user is expected to utter a positive confirmation feedback with probability 0.2. Else, if the system performs an action x that was uncalled for, the user is expected to utter a disconfirmation feedback with probability 0.5 or ask the system to stop with probability 0.3.Probability rules can be used to provide a compact encoding for the transition model of the given dialogue domain. As in previous POMDP approaches to dialogue management, this transition model is applied to update the Bayesian network representing the current (belief) dialogue state upon the reception of new observations or the execution of system actions. To this end, the probability rules are converted at runtime into standard probability distributions. This is done by instantiating each rule as a latent node that connects the rule's input variables (i.e. the variables that are mentioned in the rule conditions) to the rule's output variables (i.e. the variables that are mentioned in the rule effects). This latent node represents a random variable on the possible effects of the rule. Fig. 2illustrates this instantiation on a dialogue state comprising two variables: au(last dialogue act from the user) and am(last dialogue act from the system). The two rules r1 and r2 are applied onto this dialogue state, leading to the creation of two rule nodes and a common output variableau′(predicted user dialogue act for the next time step).The conditional probability distribution of a rule node r given its input variables I1, …, Ikis formally defined as:(3)P(r=e|I1=i1,…,Ik=ik)=P(Ei=e)wherei=min({i:ciissatisfiedwithI1=i1∧…∧Ik=ik})A condition ciis said to be satisfied iff the conditional assignment logically entails that the condition is true, that is: (I1=i1∧…∧Ik=ik)⊢ci. If the rule contains universally quantified variables, the possible groundings are first extracted on the basis of the rule inputs and instantiated inside the conditions and effects of the rule. The rule conditions are checked in sequential order until one condition is satisfied. Since the last condition cncorresponds to the final else branch and is therefore trivially true, there will always be at least one satisfied condition.For example, the conditional probability distribution P(r2|au=SayHi, am=Do(TurnLeft)) is a categorical distribution with three values:{au′=Disconfirm}with probability 0.5,{au′=RequestAction(Stop)}with probability 0.3, and the void effect {·} with probability 0.2.Output variables (such asau′in the example) are conditionally dependent on all rule nodes that refer to them in their effects. Their conditional distribution is a direct reflection of the combination of effects specified in the rule nodes that are parents of the output variable. Let X′ denote an arbitrary output variable with parent rules r1, …rn. Given a particular assignment of effects for these rules, the conditional distribution is expressed as:(4)P(X′=x|r1=e1,…,rn=en)=Numberofassignments{X′=x}ine1,…,enNumberofassignmentsforX′ine1,…,enifatleastoneeffecteiisnotvoid1(x=None)otherwisewhere 1 is the indicator function. In other words, the distribution for the variable X′ follows from the effects of the parent nodes. The distributionP(au′|r1={au′=SayHi},r2={·})in Fig. 2 has for instance a single value SayHi with probability 1. If the effects include conflicting assignments, the distribution is spread uniformly over the alternative values. If all effects are void, the output variable is set to a default None.As shown in Fig. 2, the main difference between traditional transition models and probability rules is the introduction of an abstraction layer (the rule nodes) that mediates between the input variables (state at time t) and output variables (state at time t+1). The introduction of this abstraction layer allows the system designer to decompose a complex transition model into smaller parts, each part being captured by a distinct probability rule.In addition to tracking the current dialogue state, the dialogue manager must also perform action selection, i.e. find the action with the highest expected utility in the current state. The formalism outlined in the previous section can also be used to express utility functions with only minor notational changes.Formally, a utility rule is an ordered list [br1, …, brn], where each branch briis a pair 〈ci, Ui〉, ciis a condition and Uian associated utility table over possible assignments of decision variables. The utility table Uispecifies a set of possible decisionsdi,1,…,di,mi. Each decision di,jhas a particular utility value denoted θi,j. Utility rules can be expressed in the following manner:(5)∀x,if(c1)thenU1(d1,1)=θ1,1…U1(d1,m1)=θ1,m1else if(c2)thenU2(d2,1)=θ2,1…U2(d2,m2)=θ2,m2…elseUn(dn,1)=θn,1…Un(dn,mn)=θn,mnA utility rule associates utility values to system decisions. As for probability rules, the conditions ciare defined as logical formulae over a set of input variables. The decisions di,jare assignments of specific values to decision variables, and the corresponding utilities θi,jare arbitrary real numbers – which, contrary to probabilities, may be positive or negative.(r3)∀x,if(au=RequestAction(x))thenU(am′=Do(x))=5elseU(am′=Do(x))=−5Rule r3 states that the utility of the system actionam′=Do(x)will be 5 if the action x is indeed requested for by the user, and −5 if the action was not requested.(r4)∀x,if(au=RequestAction(x)∨au=Ask(x))thenU(am′=AskRepeat)=1Rule r4 states the utility of the clarification requestam′=AskRepeatis set to 1, provided the last user utterance is a request or a question. The underspecified variable x enables us to specify that this utility is independent of the particular user request or question.Utility rules are instantiated in the dialogue state according to a procedure similar to probability rules. Each utility rule is translated into a utility node that is dependent both on the input and decision variables for the rule. Fig. 3illustrates this process.The utility function associated with each rule can be straightforwardly derived from the definition of the rule. Formally, the utility function generated by a rule r with input variables I1, …, Ikand decision variable D′ is defined as:(6)Ur(D′=d|I1=i1,…,Ik=ik)=Ui(D′=d)wherei=min({i:ciissatisfiedwithI1=i1∧…∧Ik=ik})In other words, the utility of the action D′=d is equal to its value in the utility table Ui, where i corresponds to the first branch in the rule whose condition is satisfied. If no utility is explicitly specified for D′=d in the utility table, the default value is zero. As is conventionally assumed in graphical models, the total utility for a particular action is defined as the sum of utilities resulting from each utility node. As an illustration, if we assume that the variable auof Fig. 3 is a categorical distribution with two values RequestAction(Stop) with probability 0.7 and Other with probability 0.3, the expected utility of actionam′=Do(Stop)will be equal to 5×0.7−5×0.3=2.The examples of probabilistic rules shown in the previous section relied on probabilities and utilities fixed by hand. In practical dialogue domains, such parameters are often difficult to determine manually, due to the inherent unpredictability that characterise spoken dialogue. They are therefore best estimated empirically from actual dialogue data. We developed two alternative methods for parameter estimation: a supervised learning approach based on Wizard-of-Oz data, and a reinforcement learning approach from real or simulated interactions. Both methods assume that the structure of probabilistic rules (the mapping between conditions and effects) is provided by the system designer, while the rule parameters (the effect probabilities and utilities) are determined via statistical estimation. This “division of labour” between the human designer and the learning algorithm is motivated by the fact that system designers often have a good grasp of the domain structure and relations between variables but are typically unable to quantify the precise probability of an effect or utility of an action.Bayesian inference is employed in both cases to estimate the best values for the rule parameters on the basis of observed dialogue data. The key principle in Bayesian learning is to associate each parameter with a prior distribution over its range of possible values, and gradually refine this distribution as data points are processed. For probability rules, we have seen in Eq. (2) that each branch i of a given rule is associated with a set of effect probabilitiesθi={θi,1,…,θi,mi}, where micorresponds to the number of effects specified in the branch. If these probabilities θiare unknown, we can define a prior distribution P(θi) over their (continuous) range of values. Similarly, for utility rules, each unknown utility θiis associated to a prior distribution P(θi). These random variables are instantiated in the graphical model as distinct nodes connected to their corresponding rules via outgoing edges. This process is illustrated in Fig. 4.Various types of priors can be used to encode these parameter distributions. For probability rules, unknown probabilities can be represented by Dirichlet priors. Since Dirichlet distributions are the conjugate priors of multinomial and categorical distributions (Koller and Friedman, 2009), they constitute a convenient choice to express the parameters of probability rules (recall from Section 3.1 that the rule effects are represented as categorical distributions). Formally, a Dirichlet distribution is a continuous, multivariate probability distribution encoded with a set of hyper-parameters α1, …αk, where k is the dimensionality of the Dirichlet. For each rule branch i containing mialternative effects with unknown probabilities, a Dirichlet distribution of dimensionality miwill be constructed. The α counts for the Dirichlet prior can be used to reflect the designer's initial assumptions regarding the relative frequency of each effect. Fig. 5shows a range of Dirichlet priors for the probability valueθ1,1r1used in the rule r1.For utility rules, uniform or Gaussian distributions are similarly applied to express the priors over utility parameters. Each utility is associated with its own prior distribution (contrary to probabilities, utilities are independent from one another and do not need to sum up to 1).On the basis of these parameter priors, the goal of the estimation process is to calculate their posterior distributions given the available data. This calculation is performed through standard techniques for (exact or approximate) probabilistic inference on directed graphical models. We describe below two distinct parameter estimation methods, respectively based on supervised and reinforcement learning.The most straightforward parameter estimation method relies on supervised learning from examples of expert behaviour collected via Wizard-of-Oz experiments. Wizard-of-Oz interactions are interactions between human users and a dialogue system which is remotely controlled by a human expert operating “behind the curtains”. They constitute a simple and efficient method to collect realistic conversational behaviour in the absence of a fully implemented or optimised system. We represent Wizard-of-Oz interactions as a sequence of state–action pairsD={〈Bi,ai〉:1≤i≤n}, whereBicorresponds to the dialogue state (encoded as a Bayesian network) at time i, and aiis the associated action performed by the wizard.Given a collection of Wizard-of-Oz examplesDand a set of probabilistic rules with unknown parametersθ, the learning goal is to estimate the posterior distributionP(θ|D)over the rule parameters given the observed data. Using Bayes’ rule and assuming the examples inDare independent and identically distributed (i.i.d.), this posterior distribution can be decomposed as:(7)P(θ|D)=ηP(θ)∏〈Bi,ai〉∈DP(ai|Bi;θ)whereP(ai|Bi;θ)represents the likelihood of the wizard selecting the action aiin the dialogue stateBiunder a particular assignment of values for the rule parametersθ. In order to define this likelihood distribution, we shall assume that the wizard is a (mostly) rational agent and will tend to select actions that are deemed most useful in their respective state. In other words, the utilityU(ai|Bi;θ)is expected to be ranked highly relative to the utility of other possible actions. Formally, we express the likelihood of observing the wizard executing action aiin a dialogue stateBigiven the parametersθas a geometric distribution:(8)P(ai|Bi;θ)=ηpifaiistheactionwithhighestutilityinU(a|Bi;θ)η(1−p)pifaiistheactionwithsecond-highestutilityinU(a|Bi;θ)…=η(1−p)x−1pwherexistherankofaiinU(a|Bi;θ)The η factor in the above distribution is a normalisation factor, while the probability p represents the learner confidence in the rationality of the wizard. The probability p indirectly defines the learning rate of the estimation process: the higher the probability, the faster the learner will converge to a policy that imitates the wizard action (but at the cost of a higher vulnerability to the occasional errors and inconsistencies on the part of the wizard).In practice, the Bayesian learning algorithm operates by traversing the state–action pairs one by one, in a single pass, and updating the posterior parameter distributions after each pair:P(θ(i+1))=ηP(θ(i))P(ai|Bi;θ(i)). As more Wizard-of-Oz examples are processed, the parameter distributions gradually narrow down their spread and converge to the values that best imitate the conversational behaviour of the wizard. Given the complexity of the resulting probabilistic models (which include both discrete and continuous distributions as well as partially observed data), sampling techniques such as likelihood weighting are employed to approximate the posteriors (Fung and Chang, 1989). It is worth noting that in the above approach, the utility of a given action a represent the total, long-term utility for the agent of executing a in a particular state. It corresponds therefore to the notion of Q-value in the reinforcement learning literature (or equivalently, to a reward value with a planning horizon limited to the present step). The idea of directly estimating utility values from examples of expert behaviour can also be found in imitation or demonstration-based learning [see e.g. Billard et al., 2008].Supervised learning techniques are hampered by the necessity to collect Wizard-of-Oz data for the domain. An alternative to supervised learning is to let the dialogue system learn the best conversational behaviour via trial and error from its own interaction experience – that is, through reinforcement learning (RL) – without relying on the provision of external examples. In Lison (2013), we outlined a Bayesian reinforcement learning approach to the estimation of rule parameters. As in the supervised learning case, the internal models of the domain (i.e. the transition and reward models) are expressed through probabilistic rules and associated with a number of parameters. In the course of repeated interactions with a real or simulated user, the agent gradually refines the spread of these parameter distributions. These parameters are then subsequently employed at runtime to plan the optimal action to perform, taking into account every possible source of uncertainty (including state uncertainty, stochastic action effects, and uncertainty over the parameter values). Experiments with a user simulator demonstrated that a transition model structured by probabilistic rules was able to converge to a high-quality dialogue policy (measured in terms of average return per episode) much faster than a traditional factored model. The interested reader is invited to consult Lison (2013, 2014) for more details.

@&#CONCLUSIONS@&#
