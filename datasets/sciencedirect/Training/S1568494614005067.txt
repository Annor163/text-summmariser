@&#MAIN-TITLE@&#
An improved attribute reduction scheme with covering based rough sets

@&#HIGHLIGHTS@&#
A simpler approach to attribute reduction based on discernibility matrix is presented with covering based rough sets.Some important properties of attribute reduction with covering based rough sets are improved.The computational complexity of the improved reduction approach is relatively reduced.A new algorithm to attribute reduction in decision tables is presented in a different strategy of identifying objects.

@&#KEYPHRASES@&#
Covering based rough set,Attribute reduction,Discernibility matrix,

@&#ABSTRACT@&#
Attribute reduction is viewed as an important preprocessing step for pattern recognition and data mining. Most of researches are focused on attribute reduction by using rough sets. Recently, Tsang et al. discussed attribute reduction with covering rough sets in the paper (Tsang et al., 2008), where an approach based on discernibility matrix was presented to compute all attribute reducts. In this paper, we provide a new method for constructing simpler discernibility matrix with covering based rough sets, and improve some characterizations of attribute reduction provided by Tsang et al. It is proved that the improved discernibility matrix is equivalent to the old one, but the computational complexity of discernibility matrix is relatively reduced. Then we further study attribute reduction in decision tables based on a different strategy of identifying objects. Finally, the proposed reduction method is compared with some existing feature selection methods by numerical experiments and the experimental results show that the proposed reduction method is efficient and effective.

@&#INTRODUCTION@&#
Attribute reduction and feature selection have become one of the important steps for machine learning tasks. Classical rough set theory [21] is a mathematical tool for handling data sets with imprecision and uncertainty. It can be employed to study attribute reduction and feature selection in information systems [2,4,6–8,10–17]. Equivalence relations are the mathematical basis for the rough set theory. On the basis of equivalence relations, samples of a universe can be partitioned into exclusive equivalence classes, which form basic information granules. By using basic information granules one can approximate arbitrary subset of the universe. The main idea of rough sets is to remove redundant information in data and to make correct decision or classification. Rough set theory has attracted wide attention in both the theory and its applications [9–19,23–26,29–39].However, equivalence relation in classical rough set theory is limited in practice, as it only deals with discrete variables. There are large amount of continuous data in real-life applications. For example, one can be faced with a lot of numerical data in performance analysis and equipment condition monitoring and diagnosis in power systems [22]. When dealing with such numerical attributes by using classical rough sets, numerical attributes are often discretized into symbol-type attributes as a pretreatment [20]. This type of conversion can bring information loss, thus affecting the accuracy of extracted rules [7]. In order to solve this problem, scholars have proposed a series of extensions of the rough set model [1,3,5,18,24,27,31] and presented some important feature selection criterions [4,6–8,10–16,23,26]. For example, Jensen and Shen [7] presented fuzzy-rough reduct algorithm based on Max-Dependency criterion. Pradipta Maji proposed maximum relevance and maximum significance criterion of feature selection based on fuzzy and rough sets [10–16,23]. These generalized methods of rough sets have been applied successfully to feature selection of continuous data [7,10–16]. Another important extension of rough sets is covering based rough sets. In Ref. [24], Pomykala first introduced the concepts of covering lower and upper approximation operators in rough approximation space. This innovation is based on the substitution of indiscernibility relations by coverings. Afterward, many authors investigated properties of covering approximation operators [19,28,32,37–39]. However, few people employ covering based rough sets to make research on attribute reduction. In [28] a pioneering work related to attribute reduction with covering based rough sets was conducted, where the authors constructed discernibility matrix, analyzed its some important properties, and developed an approach to compute all the reducts. Their main idea of attribute reduction is that each object is associated with a neighborhood, and then the identification between any two objects is performed by distinguishing the relationships of their neighborhoods. A major drawback of the strategy is that the constructed formula for computing discernibility matrix is very complicated, and so cannot be easily applied to practice.From the viewpoint of classical rough sets, two objects can be distinguished if one object does not belong to the neighborhood of another one. Since covering based rough sets are an extension of classical rough sets, we can use the similar strategy to distinguish objects with different decision values. In this paper, we present a simple approach to distinguish two objects using covering base rough sets and reconstruct discernibility matrix of attribute reduction with a simpler formula. The reconstruction is consistent to the viewpoint of identifying objects in classical rough sets. Compared with the approach in [20], the computational complexity of the improved approach is relatively lower. We improve some important properties of attribute reduction proposed in [28]. Furthermore, we study attribute reduction of decision tables based on the improved technique of identifying objects. Finally, experimental results show that the improved approach is feasible and valid.The remainder of this paper is structured as follows. In Section 2, we recall and define some basic notions related to covering based rough sets. In Section 3, we reconstruct the discernible approach to attribute reduction with covering based rough sets and then develop an approach to attribute reduction in decision tables. The improved method in this paper is compared with the old one [28] and some existing feature selection methods by some numerical experiments in Section 4. Section 5 concludes the paper with a summary.Attribute reduction is an important application field of rough set theory. However, in real world there are lots of data sets that cannot be handled well by classical rough sets. In light of this, similarity relation rough sets [27], dominance rough sets [5], and even neighborhood rough sets [6,31] were developed one by one. All these models induce coverings of a universe, instead of partitions, and thus can be categorized into covering rough sets, which are more general than classical rough sets and can handle more complex tasks.Granulating information in data sets is the basis of rough set theory. The granulated information forms elementary information granules to approximately describe arbitrary concepts in approximation spaces. Covering rough set theory employs the notion of coverings to granulate information in data sets.Definition 2.1[24] Let U be a nonempty and finite set of objects, whereU={x1,x2,…,xn}is called a universe of discourse.C={K1,K2,…,Km}is a family of nonempty subsets ofUand⋃i=1mKi=U. We say C is a covering of U, Kiis a covering element, and the ordered pair (U, C) is a covering approximation space.[28] Suppose U is a finite universe andC={K1,K2,…,Km}is a covering of U. For any x∈U, letC(x)=∩{Kj∈C:x∈Kj}, thenCov(C)={C(x):x∈U}is also a covering ofU,we call it the induced covering of CC(x) is the minimal descriptive subset containing x. This means C(x) cannot be written as the union of other elements inCov(C). Thus C(x) can be seen as the information granule of x with respect to C andCov(C)can be viewed as a set of information granules. These information granules are minimal covering elements associated with objects. For any x, y∈U,y∈C(x)if and only ifC(y)⊆C(x). So ify∈C(x)andx∈C(y), thenC(x)=C(y). The relationships between information granules have the following properties.(1)Reflexivity:∀x∈U,x∈C(x).Anti-symmetry: ify∈C(x)andx∈C(y), thenC(x)=C(y).Transitivity:∀x,y,z∈U, ifx∈C(y)andy∈C(z), thenx∈C(z).In classification and regression learning, we are usually confronted with the task of approximating some concepts with provided knowledge. With information granules in covering approximation spaces, any concepts can be approximated.Definition 2.3[28] Let (U, C) be a covering approximation space.X⊆Uis an arbitrary subset of the universe. The covering lower and upper approximations of X are defined asC_(X)={x:C(x)⊆X},C¯(X)={x:C(x)∩X≠∅}.[28] Suppose U is a finite universe andΔ={Ci:i=1,2,…,m}is a family of coverings of U. For anyx∈U, letΔ(x)=∩{Ci(x)∈Cov(Ci):i=1,2,…,m},thenCov(Δ)={Δ(x):x∈U}is also a covering of U, we call it the induced covering of Δ.Clearly,Δ(x)is the intersection of all the covering elements including x in all coverings, and so the minimal descriptive set containing x inCov(Δ).Similarly,Δ(x)can be viewed as the information granule of x with respect to Δ andCov(Δ)can be viewed as a set of information granules with respect to Δ If every covering in Δ is a partition, thenCov(Δ)is also a partition andΔ(x)is the equivalence class containing x. Each information granule inCov(Δ)cannot be written as the union of other granules. For anyx,y∈U,y∈Δ(x)if and only ifΔ(y)⊆Δ(x). So ify∈Δ(x)andx∈Δ(y), thenΔ(x)=Δ(y). The relationships between information granules inCov(Δ)also have such properties as reflexivity, anti-symmetry and transitivity. LetX⊆U, the lower and upper approximations of X with respect to Δ are defined as follows:Δ_(X)={x∈U:Δ(x)⊆X},Δ¯(X)={x∈U:Δ(x)∩X≠∅}.Definition 2.5Let U be a universe andΔ={Ci:i=1,2,…,m}a family of coverings on U. Then (U, Δ) is called a covering information system; Δ is called a conditional covering (attribute) set.Let (U, Δ) be a covering information system andCi∈Δ.Ciis called superfluous in Δ ifCov(Δ−{Ci})=Cov(Δ),i.e.,(Δ−{Ci})(x)=Δ(x)for anyx∈U. Otherwise,Ciis called indispensable in Δ. For any subsetP⊆Δ. P is called a reduct of Δ if each element in P is indispensable in P andCov(P)=Cov(Δ). The collection of all indispensable elements in Δ is called the core of Δ, denoted asCore(Δ).Definitions 2.5 and 2.6 are natural extensions of the corresponding concepts in classical rough set theory by substituting equivalence relations with coverings. It can be seen from the two definitions that the purpose for reducing conditional covering set is to find a minimal covering subset that keeps original information granularity invariant.In this section, we first develop some theorems to describe discernibility between objects. Then, we reconstruct the discernibility matrix of information systems based on covering and improve some characterizations of basic properties of attribute reduction. Finally, we examine how to find a relative reduct from a given decision table.LetU={x1,x2,…,xn}be a universe,Δ={C1,C2,…,Cm}be a family of coverings of U. For anyxi,xj∈U, ifxj∉Ck(xi), then we sayxiandxjcan be distinguished byCk. This statement accords to the corresponding views in classical rough sets.Proposition 3.1LetΔ={Ci:i=1,2,…,m}be a family of coverings on U,P⊆Δ. ThenCov(P)=Cov(Δ)if and only ifΔ(x)=P(x)for allx∈U. □The proposition presents an equivalence condition to judge whether two coverings are equal and shows the fact that two coverings are equal if and only if their induced granularities are equal.Theorem 3.2LetΔ={C1,C2,…,Cm}be a family of coverings of U. ThenCiis an indispensable covering if and only if there existx,y∈U, such thaty∉Δ(x)andy∈{Δ−{Ci}}(x).Straightforward. □The above theorem implies that an indispensable covering can be characterized by the discernibility between objects. That is to say, Ciis an indispensable covering if and only if there existx,y∈U, such that y not belonging to the neighborhood of x with respect to Δ implies y belonging to the neighborhood of x with respect toΔ−{Ci}. This implies that Ciis a sole covering that can distinguish the two objects.Theorem 3.3LetΔ={C1,C2,…,Cn}be a family of coverings on U. For allx,y∈U,y∉Δ(x)if and only if there is at least a coveringCi∈Δsuch thaty∉Ci(x).Straightforward. □This theorem shows that if two objects can be distinguished under the granularity level ofΔ(x), then there is at least a coveringCi∈Δsuch that the two objects are also distinguished under the granularity level ofCi(x), and vice versa. This means that if two objects are not in the same original information granule, i.e., one object is not in the neighborhood of another one with respect to Δ, we can find at least an attribute (covering) to distinguish them.Theorem 3.4(Judgment theorem of attribute reduction). LetΔ={C1,C2,…,Cn}be a family of coverings on U andP⊆Δ. ThenCov(P)=Cov(Δ)if and only if for allx,y∈U, ify∉Δ(x), theny∉P(x).Follows immediately from Proposition 3.1. □As mentioned above, the objective of attribute reduction is to find out minimal subsets of conditional covering set Δ that keeps invariant the minimal descriptive set of every object with respect to Δ. This theorem shows attribute reduction must keep invariant the original discernibility between any two objects. If two objects are distinguished under the original granularity level, they have to be distinguished under lower level of granularity induced by the candidate-attribute set of reducts. In order to search for all the reducts, we define discernibility matrix according to Theorems 3.3 and 3.4 as follows.Definition 3.1Let (U, Δ) be a covering information system. SupposeU={x1,x2,…,xn}, we denote byM(U,Δ)an n×n matrix(cij),called the discernibility matrix of (U, Δ) and defined ascij=C∈Δ:xj∉C(xi)forxi,xj∈U. Clearly, we havecii=∅for anyxi∈U.As an attribute and its induced covering are uniquely determined by each other, the discernibility matrix gives the description of all the attribute subsets that can distinguish any two objects. If two objects do not belong to one information granule at original level of granularity, then they must be distinguished by some attributes. This idea is consistent with the viewpoint in classical rough sets. Besides, covering and classical rough sets bear some formal resemblance between discernibility matrices. Thus the proposed approach is a generalization of classical rough sets.In [28] the reduction method based on discernibility matrix was also proposed to compute all the reducts. For the sake of comparison, let us review the definition of discernibility matrix introduced there. In following, the subset sigh “⊂” is only used for a strict inclusion between two sets.Definition 3.2[28]. LetU={x1,x2,…,xn}. ByM(U,Δ={C1,C2,…,Cm})we denote a n×n matrix(wij),called the discernibility matrix of (U, Δ) such that forxi,xj∈U,The above definition shows two objects are distinguished by employing the relationships between their neighborhoods. This strategy of identifying objects leads to the high computational complexity of discernibility matrix. Definition 3.1 employs different idea to identify objects. That is, two objects can be distinguished if one object does not belong to the neighborhood of another one. It is observed that the computational complexity of discernibility matrix in Definition 3.1 is relatively lower. Let us analyze the reasons. In Definition 3.2, for any two objects with different neighborhoods, we need at least m2 comparisons of their neighborhoods to distinguish them. As the computational complexity of n×n matrix isO(n2),the computational complexity of discernibility matrix in Definition 3.2 isO(m2⋅n2), while in Definition 3.1, we need m comparisons for the two objects. Hence, the computational complexity of discernibility matrix in Definition 3.1 isO(m⋅n2), where n and m are the numbers of samples and attributes, respectively. This justifies that our proposed method is simpler than the method in [28]. In fact, the two types of discernibility matrices are equivalent. Next, we present the proof of their equivalence.Theorem 3.5The discernibility matrices in Definitions 3.1 and 3.2 are equivalent.It is easily seen that the discernibility matrix given in Definition 3.2 is symmetric and the one introduced in Definition 3.1 is not symmetric. In order to prove the equivalence of the two discernibility matrices, we need to verifycij∪cji=wijfor anyxi,xj∈U.IfΔ(xi)=Δ(xj),we know from Definition 3.2 thatwij=∅, and at the same time,xj∈Δ(xi)andxi∈Δ(xj)by the reflexivity ofΔxiandΔxj.This implies thatxj∈Cxiandxi∈Cxjfor anyC∈Δ.According to Definition 3.1, we have thatcij=∅andcji=∅.It follows thatcij∪cji=wij.Suppose thatΔxi⊂ΔxjorΔxj⊂Δxi, next, we provecij∪cji=wij.IfΔxi⊂Δxj,it follows from the reflexivity of information granules thatxi∈Δxj. Assume thatxj∈Δxi, by the transitivity of information granules we haveΔxj⊆Δxi. It is a contradiction toΔxi⊂Δxj. Thusxj∉Δxi. SinceΔxi⊂Δxj, we have thatCxi⊆Cxjfor anyC∈Δ.LetC0∈cij, it follows from Definition 3.1 thatxj∉C0xi, which implies thatC0xj⊆C0xiis not true. This, together with the factCxi⊆Cxjfor anyC∈Δ, means thatC0xi⊂C0xj. From Definition 3.2 we getC0∈wij, which impliescij⊆wij.IfΔxj⊂Δxi,similarly, we have thatxj∈Δxi,xi∉ΔxjandCxj⊆Cxifor anyC∈Δ. LetC′∈cji, it is similar to get thatC′xj⊂C′xi. From Definition 3.2 we getC′∈wij, which impliescij⊆wij. Hence,cij∪cji⊆wij.On the other hand, letC∈wij,by Definition 3.2 we knowCxi⊂CxjifΔxi⊂Δxj, orCxj⊂CxiifΔxj⊂Δxi. The statementCxi⊂CxjorCxj⊂Cximeansxj∉Cxiorxi∉Cxj, respectively. It follows from Definition 3.1 thatC∈cijorC∈cji. Hence,cij⊇wij, as desired.Suppose thatΔxi⊄Δxj∧Δxj⊄Δxi, let us provecij∪cji=wij.To provecij∪cji⊆wij, we provecij⊆wijin the following. It is similar for the proof of the casecji⊆wij.LetCs∈cij, by Definition 3.1 we havexj∉Csxi, and soCsxj⊄Csxiby the transitivity. There are the following two possible cases for Cs.(i)Csxi⊄Csxj;(ii)Csxi⊆Csxj.From the first case and the factCsxj⊄Csxi, we getCsxi⊄Csxj∧Csxj⊄Csxi. It follows from Definition 3.2 thatCs∈wij. If Csmeets the second case, we haveCsxi⊆Csxj∧Csxj⊄Csxi. By the factΔxi⊄Δxj, there must exist another coveringCt∈Δsuch thatCtxi⊄Ctxj. Thus we getCtxi⊄Ctxj∧Csxj⊄Csxi. This implies by Definition 3.2 thatCs∧Ct∈wij. So we getcij⊆wij. Similarly, we can getcij⊆wij. It follows thatcij∪cji⊆wij.On the other hand, letC∈wij, it follows from Definition 3.2 that C meetsCxi⊄Cxj∧Cxj⊄Cxi. This meansxj∉Cxiandxi∉Cxjby the transitivity. By Definition 3.1 we getC∈cijandC∈cji. LetCs∧Ct∈wij, then by Definition 3.2 we can getCtxi⊄Ctxj∧Csxj⊄Csxi, which implies thatxi∉Ctxjandxj∉Csxiby the reflexivity and transitivity. This by Definition 3.1 means thatCt∈cjiandCs∈cij. So we getwij⊆cij∪cji, as desired. □The above theorem shows the two types of discernibility matrices are equivalent to each other. This means that we can get the same results if they are employed to compute reducts of identical data sets. But the proposed formula for computing discernibility matrix in Definition 3.1 is simpler.The following theorem is used to study the properties of the improved discernibility matrix.Theorem 3.6LetM(U,Δ)=(cij)be the discernibility matrix of(U,Δ)andΔ={C1,C2,…,Cm}. Then the following statements hold:Cl∈cij⇔xj∉Clxi;cii=∅;i≤1,2,…,n;cij⊆cit∪ctj,(i≠j,i,j,t≤1,2,…,n).(1) and (2) Straightforward.LetC∈cij, thenxj∉Cxi. Suppose thatxt∈Cxiandxj∈Cxt, thenCxt⊆Cxiby the transitivity, and soxj∈Cxi. This is equivalent to that ifxj∉Cxi, thenxt∉Cxiorxj∉Cxt, which implies that ifC∈cij, thenC∈citorC∈ctj. That is,cij⊆cit∪ctj. □This theorem illustrates that the properties of discernibility matrix are determined by the properties of coverings. If Δ is a family of equivalence relations,M(U,Δ)is the discernibility matrix of the corresponding information system in Pawlak's rough set theory.In [28], Proposition 4.5 presents a sufficient and necessary condition for characterization of the core attributes. For the sake of comparison, let us review the result.Theorem 3.7[28].Core(Δ)={C∈Δ:cij={C}∨cij={C∧Ct},t=1,2,…,k}for some i and j.This statement indicates that the core attributes come from either a singleton attribute set or some two-element discernible sets. This is not consistent to the classical viewpoint that the core attributes come from singleton discernible sets. Next, we improve it based on the discernibility matrix in Definition 3.1 as follows.Core(Δ)={C∈Δ:cij={C},i,j≤n}.SupposeC∈Core(Δ), thenCovΔ≠CovΔ−C.By Proposition 3.1 and Theorem 3.2, it follows that there exist xi, xy∈U such thatxj∉Δxi, butxj∈Δ−Cxi. Obviously, there is onlyC∈Δsatisfyingxj∉Cxi. By Definition 3.1,cij={C}. HenceCore(Δ)⊆{C∈Δ:cij={C},i,j≤n}. Conversely, ifcij={C}forxi,xj∈U, thenC∈Core(Δ)by Theorem 3.2. HenceCore(Δ)⊇{C∈Δ:cij={C},i,j≤n}. ThereforeCore(Δ)={C∈Δ:cij={C},i,j≤n}. □LetP⊆Δ, thenCov(P)=Cov(Δ)if and only ifP∩cij≠∅for anycij≠∅, (i,j=1,2,…,n).⇒ Assume that∃i0,j0≤n,ci0j0≠∅, butP∩ci0j0=∅. Therefore, for allC∈Pwe havexj0∈Cxi0, which impliesxj0∈Pxi0. SinceCov(P)=Cov(Δ), by Proposition 3.1 we havexj0∈Δxi0. Thusxj0∈Cxi0for anyC∈Δ, which impliesci0j0=∅. This is a contradiction to the assumption. SoP∩cij≠∅for anycij≠∅.⇐ For anyxi,xj∈U, if they can be distinguished by some attributes, that is,cij≠∅. We assume without loss of generality thatC∈cij. We can getxj∉Cxi, and soxj∉Δxiby the relationship betweenCxiandΔxi. On the other hand, ifxj∉Δxi, then there must be a attribute C, such thatxj∉Cxi. From Definition 3.1, we getC∈cij. This means thatcij≠∅is equivalent toxj∉Δxi. LetP∩cij≠∅, assume thatC′∈P∩cij, thenxj∉C′xiandC′∈P, which impliesxj∉Pxiby the relationship betweenC′xiandPxi.That is to say, for anyx,y∈U, if they satisfyy∉Δx, then we must gety∉Px. It follows from Theorem 3.4 thatCov(P)=Cov(Δ). □LetΔ={C1,C2,…,Cn}be a family of coverings on U.f(U,Δ)is a function on(U,Δ)and defined asf(U,Δ)=∧{∨(cij)},(i,j≤n,cij≠∅), where∨(cij)represents the disjunction operation among elements incij. By using the function, we can compute all the reducts of covering information systems. The following example is employed to compare our idea with that in [28].Example 3.1Let consider an example of house evaluation problem provided in [28]. For a detailed introduction to the example, the reader can refer to the reference. Suppose U={x1, x2, …, x9} is a set of nine houses, E={price; color; structure; surrounding} is a set of attributes. For each of the four attributes, we can get a covering of U, denoted by C1, C2, C3, C4, respectively. The four coverings are listed as follows:LetΔ=C1,C2,C3,C4, thenC1x1=C1x4=C1x7=x1,x2,x4,x5,x7,x8,C1x2=C1x5=C1x8=x2,x5,x8,C1x3=C1x6=C1x9=x2,x3,x5,x6,x8,x9.C2x1=C2x2=C2x7=x1,x2,x3,x4,x5,x6,C2x4=C2x5=C2x6=x4,x5,x6,C2x7=C2x8=C2x9=x4,x5,x6,x7,x8,x9.C3x1=C3x2=C3x7=x1,x2,x3,C3x4=C3x5=C3x6=x4,x5,x6,x7,x8,x9,C3x7=C3x8=C3x9=x7,x8,x9.C4x1=x1,x2,x4,x5,C4x2=x2,x5,C4x3=x2,x3,x5,x6,C4x4=x4,x5,C4x5=x5,C4x6=x5,x6,C4x7=x4,x5,x7,x8,C4x8=x5,x8,C4x9=x5,x6,x8,x9.The discernibility matrix of(U,Δ)is as follows:∅∅{C1,C4}{C3}{C3}{C1,C3,C4}{C1,C2,C3,C4}{C2,C3,C4}{C1,C2,C3,C4}{C1,C4}∅{C1,C4}{C1,C3,C4}{C3}{C1,C3,C4}{C1,C2,C3,C4}{C2,C3,C4}{C1,C2,C3,C4}{C1,C4}∅∅{C1,C3,C4}{C3}{C3}{C1,C2,C3,C4}{C2,C3,C4}{C2,C3,C4}{C2,C3,C4}{C2,C3,C4}{C1,C2,C3,C4}∅∅{C1,C4}{C2,C4}{C2,C4}{C1,C2,C4}{C2,C3,C4}{C2,C3,C4}{C1,C2,C3,C4}{C1,C4}∅{C1,C4}{C2,C4}{C2,C4}{C1,C2,C4}{C1,C2,C3,C4}{C2,C3,C4}{C2,C3,C4}{C1,C4}∅∅{C1,C2,C4}{C2,C4}{C2,C4}{C2,C3}{C2,C3}{C1,C2,C3}{C3}{C3}{C1,C3}∅∅{C1,C4}{C1,C2,C3,C4}{C2,C3,C4}{C1,C2,C3,C4}{C1,C3,C4}{C3}{C1,C3,C4}{C1,C4}∅{C1,C4}{C1,C2,C3,C4}{C2,C3,C4}{C2,C3,C4}{C1,C3,C4}{C3}{C3}{C1,C4}∅∅andf=(U,Δ)=∧{∨(cij):1≤j<i≤9,cij≠∅}=(C1∨C4)∧C3∧(C1∨C3∨C4)∧(C1∨C2∨C3∨C4)∧(C2∨C3∨C4)∧C2∨C4∧C1∨C2∨C4∧C2∨C3∧C1∨C2∨C3∧C1∨C3=(C1∨C4)∧C3∧(C2∨C4)=(C1∧C2)∨C4∧C3=(C1∧C2∧C3)∨C4∧C3SoRed(Δ)={C3,C4},{C1,C2,C3}andCore(Δ)={C3}.□The results are the same as ones in Ref. [28]. But the computational complexity is lower. So we can say that the proposed approach is simpler to compute all the reducts than the old one.Decision tables include two types of form: consistent and inconsistent decision tables. From the viewpoint of covering based rough sets, each object has a neighborhood in which all objects have the same decision as itself in consistent decision table. While in inconsistent decision tables this situation does not always exist. That is to say, there are also some objects in a neighborhood whose decisions are not consistent. Based on this observation, a consistent decision table can be seen as a special case of inconsistent decision tables. As follows, we examine how to find a relative reduct of a decision table with covering based rough sets.LetΔ={Ci:i=1,2,…,m}be a family of coverings of U,D=dbe a decision attribute. The decision function is defined asd:U→Vd,d(x)∈Vd, whereVd=1,2,…,ris the decision domain of d. The decision function d divides the universe U into equivalence classesU/D=X1,X2,…,Xr. Then the triplet(U,Δ,D)is called a covering decision system. LetP⊆Δ, the positive domain ofXii≤rwith respect to P can be computed byPosP(Xi)=P_(Xi)and the positive domain of D with respect to P is defined asPosP(D)=∪X∈U/DP_(X).ForCi∈Δ, ifPosΔ(D)=PosΔ−{Ci}(D), thenCiis called superfluous with respect to D in Δ, otherwise,Ciis called indispensable in Δ. Let P be a subset of Δ, if each element in P is indispensable in P andPosP(D)=PosΔ(D),then P is called a reduct of Δ relative to D. It is easy to observe that a covering decision system is consistent if and only ifPosΔ(D)=U.In the following discussion, we always supposePosΔ(D)≠∅.SincePosP(D)=PosΔ(D)is equivalent toPosP(X)=PosΔ(X)for anyX∈U/D, we only need to keep invariant the positive region of each decision class after reducing some superfluous attributes. Based on this idea, we construct the discernibility matrix of a covering decision table as follows.Definition 3.3Let(U,Δ,D={d})be a covering decision system whereU={x1,x2,…,xn},Δ={Ci:i=1,2,…,m}, andU/D=X1,X2,…,Xr. We denote an n×n matrix(cij)byM(U,Δ,D), called the discernibility matrix of(U,Δ,D), such that ifxi,xj∈Usatisfy:xi∈POSΔ(Xk),k∈{1,2,…,r},xj∈U−Δ_(Xk),thencij=C∈Δ:xj∉Cxi.otherwisecij=∅.Similar to a covering information system, we can show the following properties.Theorem 3.10Let(U,Δ,D)be a covering decision system andP⊆Δ. ThenPosP(D)=PosΔ(D)if and only ifP∩cij≠∅for anycij≠∅.⇒ Assume that there existi0,j0≤n, such thatci0j0≠∅butP∩ci0j0=∅. By Definition 3.3 we havexj0∈Cxi0for anyC∈P, which impliesxj0∈Pxi0. SincePosP(D)=PosΔ(D), it follows thatPosP(X)=PosΔ(X), namely,P_(X)=Δ_(X)for anyX∈U/D. Byci0j0≠∅, it follows from Definition 3.3 that there existsXk∈U/D, such thatxi0∈PosΔ(Xk)andxj0∈U−Δ_(Xk). This, together with the factPosP(Xk)=PosΔ(Xk)andPosP(Xk)=P_(Xk), implies thatxi0∈P_(Xk)andxj0∈U−Δ_(Xk). BecauseP_(Xk)=Δ_(Xk), we havexj0∉Pxi0, this is a contradiction.⇐ Letxi∈PosΔ(D), then by the definition ofPosΔ(D)there existsXk∈U/Dsuch thatxi∈PosΔ(Xk). It follows from Definition 3.3 thatcij=C∈Δ:xj∉Cxifor anyxj∈U−Δ_(Xk). Becausexi∈PosΔ(Xk)is equivalent toΔxi⊆Xk, we have thatxi∈Δ_(Xk)by the definition ofΔ_(Xk). Hence,xj∉Δxi, which impliescij≠∅.IfP∩cij≠∅,we assume without loss of generality thatC′∈P∩cij. Thenxj∉C′xi,and soxj∉Pxiby the relationship betweenC′xiandPxi. By the arbitrariness ofxj, we can getPxi⊆Δ_(Xk)⊆Xk. By the definition ofP_(Xk), we knowxi∈P_(Xk),that is,xi∈PosP(Xk). Hence,xi∈PosP(D). It follows thatPosΔ(D)⊆PosP(D). SincePosP(D)⊆PosΔ(D)is always true, we can getPosP(D)=PosΔ(D). □Let(U,Δ,D)be a covering decision system whereU={x1,x2,…,xn}andΔ={Ci:i=1,2,…,m}. The corresponding Boolean variableCi¯(i≤n)is defined for each coveringCi∈Δ.f(U,Δ,D)is a function on(U,Δ,D)and defined asf(U,Δ,D)C1¯,C2¯,⋯,Cn¯=∧∨(cij),i,j≤n,cij≠∅.Thenf(U,Δ,D)is a Boolean function of(U,Δ,D), called a discernibility function or discernibility formula of(U,Δ,D), where∨(cij)represents the disjunction operation among elements in Cij. By applying disjunctive and conjunctive operations on the discernibility function, we can get the minimal disjunctive normal form off(U,Δ,D)to compute all the reducts.Example 3.2Table 1 shows petroleum exploration data set, where ‘No’ stands for number of wells, ‘c1′ stands for average amplitude, ‘c2′ stands for semi-attenuation time, ‘c3′ stands for instantaneous frequency, ‘c4′ stands for Root Mean Square (RMS) amplitude, ‘D’ stands for well type, ‘1′ stands for petroleum well, ‘2′ stands for dry well.In the following, we apply covering based rough sets to reduce superfluous attributes from the conditional attributes and get all reducts of the data set. Since all attributes are numerical or real-valued, there are ordered relationships between any two samples. We can induce a dominance relation by sorting all samples according to their values for each attribute. Let R1, R2, R3 and R4 are the dominance relations induced by c1, c2, c3 and c4, respectively.(1)For attribute c1, we have the order relation x3>x1>x4>x2>x5>x7>x6. ThenR1sx1=x1,x4,x2,x5,x7,x6,R1sx2=x2,x5,x7,x6,R1sx3=x3,x1,x4,x2,x5,x7,x6,R1sx4=x4,x2,x5,x7,x6,R1sx5=x5,x7,x6,R1sx6=x6,R1sx7=x7,x6.For attributec2, we have the order relation x6>x5>x4>x1>x2>x7>x3. ThenR2sx1=x1,x2,x7,x3,R2sx2=x2,x7,x3,R2sx3=x3,R2sx4=x4,x1,x2,x7,x3,R2sx5=x5,x4,x1,x2,x7,x3,R2sx6=x6,x5,x4,x1,x2,x7,x3,R2sx7=x7,x3.For attributec3, we have the order relation x3>x6>x5>x7>x1>x2>x4. ThenR3sx1=x1,x2,x4,R3sx2=x2,x4,R3sx3=x3,x6,x5,x7,x1,x2,x4,R3sx4=x4,R3sx5=x5,x7,x1,x2,x4,R3sx6=x6,x5,x7,x1,x2,x4,R3sx7=x7,x1,x2,x4.For attributec4, we have the order relation x3>x1>x2>x4>x5>x7>x6. ThenR4sx1=x1,x2,x4,x5,x7,x6,R4sx2=x2,x4,x5,x7,x6,R4sx3=x3,x1,x2,x4,x5,x7,x6,R4sx4=x4,x5,x7,x6,R4sx5=x5,x7,x6,R4sx6=x6,R4sx7=x7,x6.LetCi=R1sxk|k=1,2,…,7,i=1,2,3,4. ThenΔ=C1,C2,C3,C4is a family of coverings ofU.ThusΔx1=R1sx1∩R2sx1∩R3sx1∩R4sx1=x1,x2.Similarly, we getΔx2=x2,Δx3=x3,Δx4=x4,Δx5=x5,x7,Δx6=x6,Δx7=x7.Decision attribute D is categorical. It partitions all samples into two equivalence classesD1,D2,whereD1=x1,x2,x3andD2=x4,x5,x6,x7.HenceΔ_D1=x1,x2,x3,Δ_D2=x4,x5,x6,x7,andPOSΔ(D)=∪X∈U/DΔ_(X)=Δ_D1∪Δ_D2=x1,x2,x3,x4,x5,x6,x7.This means that the decisions of all samples in the data table are consistent with respect to Δ.By Definition 3.2 we get the discernibility matrix as follows:∅∅∅c2c2,c3c2c3∅∅∅c1,c2c2,c3c2,c3c3∅∅∅c2c2c2c2c1,c3,c4c3,c4c1,c3,c4∅∅∅∅c1,c4c1,c4c1,c3,c4∅∅∅∅c1,c4c1,c4c1,c3,c4∅∅∅∅c1,c2,c4c1,c2,c4c1,c3,c4∅∅∅∅andfU,c1,c2,c3,c4,D=∧∨cij:1≤i≤j≤7=c2∧c3∧c2∨c3∧c3∨c4∧c1∨c4∧c1∨c2∨c4∧c1∨c3∨c4=c1∧c2∧c3∨c2∧c3∧c4.So all the reducts of this decision table arec1,c2,c3andc2,c3,c4. We can see that the attributes “semi-attenuation time (c2)” and “instantaneous frequency (c3)” are very important for classification of sample wells.In Section 3 we present a method for finding out all the reducts of conditional attributes. But the time complexity is NP-complete [7]. In the real world, it is enough to solve a real problem by approximating a minimal reduct. Hence, in following experiments we employ a heuristic algorithm to find a suboptimal reduct by using discernibility matrix.Heuristic algorithm:Step1:Input a decision table (U, A, D) and a threshold to compute discernibility matrix.Step2:Red←,A←all conditional attributes. // Red is the pools to contain the selected features.Step3:∀i,j, ifcij=1, thenRed←cij;A←A−cij,andcij=∅. For anycij≠∅,ifcij∩Red≠∅,thencij=∅.Step4:DoFor allcijsatisfyingA∩cij≠∅, find the attribute a in A with highest frequency of occurrence in current discernibility matrix andRed←Red∪a,A←A−a.For anycij≠∅,ifcij∩Red≠∅, thencij=∅.Until allcij=∅.Step5:Output Red.According to Definition 3.1, for any two samples with different neighborhoods, we need at most m times of comparisons to distinguish them. Hence, the computational complexity of discernibility matrix isO(m⋅n2),where n and m are the numbers of samples and attributes, respectively. The search for the core attributes from n×n discernibility matrix, which is carried out in step 3, hasO(n2)time complexity. The loop in step 4 is executed at most m times, in each loop the selection of attributes with highest frequency of occurrence takes less than n2 times. Hence, the total complexity to execute the loop isO(m⋅n2). In effect, the overall computational complexity of the heuristic algorithm based on discernibility matrix isO(m⋅n2).In order to evaluate the performance of the method in this paper, we compare the proposed heuristic algorithm with both the method in [28] and maximum relevance-maximum significance (MRMS) algorithms in [10–16]. The algorithms in Ref. [28] and this paper are simply denoted as CVR and IMCVR, respectively. MRMS algorithms based on classical rough sets and neighborhood rough sets are denoted as FMRMS, NMRMS, respectively. All the algorithms are performed in Matlab 2007 and run in the hardware environment with Pentium (R) Core 2, CPU E5200, 2.50GHz and 3.0GB RAM.We employ three classical classifiers to measure and compare the performance of different algorithms. Three classical classifiers are support vector machine (SVM), K-nearest neighbor rule (K-NN, K=3) and C4.5 decision tree rule. Ten data sets are used in the experimental analysis. Sonar, Glass, Wine, Wdbc, Iris, Wpbc, and Segmentation are selected from UCI Machine Learning Repository, and Colon Tumor, Leukemia (ALL vs. AML) and Lung Cancer are downloaded at Keng Ridge Bio-medical Data Set Repository [40]. The information of these data sets is in detail described in Tables 1–3. To compute the classification accuracy of different classifiers, both 10-fold cross-validation and training-testing are used. 10-fold cross-validation is conducted on Sonar, Glass, Wine, Wdbc, Iris, Wpbc and Colon Tumor data sets, while training-testing is used on Leukemia and Lung Cancer and Segmentation.Since the classical rough set considers only categorical data, the fuzzy c-means clustering (FCM) technique is employed to discretize numerical data before reduction with FMRMS method. The numeric attributes are discretized into four intervals. In MRMS, CVR and IMCVR algorithms, there is a parameter ɛ that is used to control the size of the neighborhood of an object; it has a great impact on different data sets. In FMRMS, NMRMS algorithms, there exists an additional parameter β that controls the proportion of relevance and significance between attributes. For FMRMS algorithm, the parameter β is set to 0.5. For NMRMS algorithm, we set the value of β to vary from 0 to 1 with a step of 0.125 in order to obtain the best classification accuracy. The parameter ɛ is set to vary from 0 to 1 with a step of 0.025. All the results reported in the following tables are presented at highest classification accuracy.Tables 4–8present the comparative performance of different attribute reduction methods using 10-fold cross-validation. From Table 4, these reduction methods can effectively reduce attributes, while NMRMS algorithm needs a pre-set number of selected features as a parameter to input for Sonar and Iris data sets and FMRMS algorithm does it for Colon. Otherwise, they cannot find out a suitable feature subset due to the lack of a good stopping criterion. Table 4 also shows that the numbers of selected attributes with CVR and IMCVR reduction methods are comparable. Table 5 shows that CVR algorithm needs more time to reduce data sets than other algorithms. The more samples and attributes there are in a data set, the greater the difference of required time will be. The standard deviations of running time with CVR are also greatest for all seven data sets. Both FMRMS and NMRMS algorithms take less time than IMCVR, and FMRMS algorithm takes the least. However, because NMRMS algorithm has two parameters, it actually requires more time to search a subset of optimal features for each data set.Tables 6–8 show that the classification accuracies based on CVR and IMCVR methods are roughly the same. Theoretically, CVR and IMCVR should produce the same classification accuracies according to Theorem 3.5. The nuance of classification accuracies for each data shows that each of data sets has multiple groups of features that have the same discriminating ability. Now, let us compare the classification accuracies of the SVM, 3-NN and C4.5 based on FMRMS, NMRMS and IMCVR algorithms. From the results reported in Tables 6–8, it can be seen that NMRMS and IMCVR methods produce significantly higher classification accuracies than FMRMS in most of the cases. Out of 21 cases of 10-fold cross-validation, the IMCVR method achieves highest classification accuracy in 9 cases, the NMRMS method obtain it in 8 cases, while NMRMS only attains it in 2 cases. This result shows that discretization causes information loss. It can be also observed that there are no significant differences of classification accuracy between NMRMS and IMCVR methods.Tables 9 and 10compare the performance of the proposed IMCVR algorithm with FMRMS and NMRMS algorithms using training-testing. From the results reported in Table 9, the three algorithms can effectively reduce attributes of data sets, while NMRMS algorithm needs a pre-set number of selected features as a parameter to input for Leukemia and Lung tumor data sets, and FMRMS algorithm does it for Leukemia because they have no good stopping criterion. Table 9 also shows that IMCVR algorithm needs less time to reduce data sets than FMRMS and NMRMS algorithms. In fact, NMRMS algorithm needs more time in searching for a reduct due to having two parameters.Table 10 shows that NMRMS method provides slightly higher classification accuracies than IMCVR method. Out of 9 cases of training-testing, NMRMS method achieves highest classification accuracy in 5 cases, IMCVR method obtains it in 2 cases, and FMRMS only attains it in 1 case. It is obviously observed that there are no significant differences of classification accuracy between NMRMS and IMCVR methods.From the above experimental analysis, we can conclude the following results.(1)The performance of FMRMS method is lower than NMRMS and IMCVR methods because of information loss caused by discretization.Although NMRMS method provides slightly higher classification accuracies than IMCVR method on Leukemia data set, there are no significant differences of classification accuracy on other data sets. The accuracy difference of the two methods on Leukemia is perhaps related to the inner information structures of Leukemia. What information structure of a dataset is more suitable to NMRMS or IMCVR? This problem is very interesting and will be addressed in future.In fact, NMRMS method requires more time for searching an optimal subset of features than IMCVR method due to its having two parameters. Furthermore, NMRMS method needs a pre-set number of features as a parameter to input on some data sets.Although the improved attribute reduction method can effectively deal with data sets with medium-sized numbers of samples, the computational complexity of discernibility matrix increases quadratically with the numbers of samples. How to reduce the computational complexity of discernibility matrix for data sets with large numbers of samples will be our future research directions.

@&#CONCLUSIONS@&#
