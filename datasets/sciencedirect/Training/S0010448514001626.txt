@&#MAIN-TITLE@&#
Perception-driven adaptive compression of static triangle meshes

@&#HIGHLIGHTS@&#
New adaptive compression method based on shape perception.Suitable for both single rate and progressive encoding.Works with any kind of perceptual error metric.Improvement with respect to most commonly used approaches.

@&#KEYPHRASES@&#
Mesh compression,Shape perception,Progressive decoding,

@&#ABSTRACT@&#
Mesh compression is an important task in geometry processing. It exploits geometric coherence of the data to reduce the amount of space needed to store a surface mesh. Most techniques perform compression employing a uniform data quantization over the whole surface. Research in shape perception, however, suggests that there are parts of the mesh that are visually more relevant than others. We present a novel technique that performs an adaptive compression of a static mesh, using the largest part of the bit budget on the relevant vertices while saving space on encoding the less significant ones. Our technique can be easily adapted to work with any perception-based error metric. The experiments show that our adaptive approach is at least comparable with other state-of-the-art techniques, while in some cases it provides a significant reduction of the bitrate of up to 15%. Additionally, our approach provides much faster decoding times than comparable perception-motivated compression algorithms.

@&#INTRODUCTION@&#
Mesh compression algorithms deal with the problem of efficiently storing surface meshes, which mainly consist of triangles. Usually a certain loss in the precision of the data is allowed in order to achieve a smaller size of the compressed data. The task at hand is to encode the given mesh with smallest possible distortion, that is, to represent the input triangle meshMwith a shortest possible sequence of bits, from which a reconstructionM̄can be decoded, while the differenced(M,M̄)is lower than some user-defined constant. Usually, mesh connectivity and mesh geometry are encoded separately. The connectivity encoding is lossless and efficient algorithms have been proposed that work near the theoretical performance limit  [1].Mesh compression plays the key role in several industries, such as computer aided design, in film and gaming industry, e-marketing and many others. The specifics of a particular application area play a key role in designing a compression algorithm. In general, there are two main scenarios where mesh compression is required: storage of surface models for further computer-aided processing, and storage of triangle meshes for viewing by human observers. These two scenarios pose radically different requirements on the desired properties of the algorithm.When storing meshes for further processing, it is desirable to keep the precision loss at a minimum, since the following steps may emphasize some compression artefacts that are on the verge of being visible at the current stage. The allowed precision loss in this scenario is justified and guided by the discrepancy between the precision of acquisition (scanner accuracy) and representation (length of mantissa of the used floating point representation of coordinates). In this scenario, it is possible to express the loss, that is, the functiond(M,M̄), in terms of vertex displacements, and it is desirable that both encoding and decoding are performed very fast, since storing occurs equally frequently as decoding during the usual workflow.In this paper, we are going to address the second scenario, that is, storing meshes before the final delivery to a consumer, which is a human observer. This is for example the case in e-commerce (displaying a 3D model in a webshop) or gaming (storing a 3D model on a distribution medium) applications. This scenario has the following key properties:•The decompressed mesh is required to be perceptually close to the original, that is, the allowed precision loss is justified and guided by the discrepancy between representation precision and the level of perceptible difference. Research in perceptual 3D metrics shows that visual similarity of 3D shapes correlates only weakly with vertex dislocations, and thus it must be measured using different approaches.The computational complexity of the decompression is of much higher importance than the computational complexity of the compression. Similarly to video encoding, longer processing times for encoding are acceptable, as long as the decoding can be done in real time.For some applications, such as e-commerce, it might be important to provide the user with the possibility to view a partially decoded mesh and the possibility to stop the decoding before the whole mesh is transmitted.Out of these properties, the need for a perceptual metric is probably the one that has the largest influence on the design of a compression algorithm. Recently, a number of researchers[2–4] have focused on the problem of estimating the visual effects of mesh processing, including mesh simplification, watermarking, and compression. Metrics have been proposed that correlate well with the results of user studies in which human observers were asked to evaluate the amount of distortion in 3D meshes. Interesting phenomena have been identified and documented, such as the masking effect (increased sensitivity of the observers to distortion in smooth areas of the mesh) and generally higher sensitivity of the human visual system to local, relative changes in vertex positions, contrasting with the relatively low sensitivity to global changes in absolute vertex positions.Mesh compression is usually applied in combination with mesh simplification, which reduces the number of vertices where possible. Consequently, the input data in most practical compression tasks exhibit a rather high variability in terms of sampling density and local smoothness. These factors contribute immensely to the local perceptibility of compression artefacts. For example, dislocating a vertex in a densely sampled area of the mesh can lead to a local flip of triangle normals, which is immediately visible to any human observer. On the other hand, dislocating a different vertex, this time in a sparsely sampled area of the mesh, by the same amount, may be completely imperceivable (see Fig. 1). This is the main motivation for our proposed algorithm.Recent experiments with perceptual metrics also indicate that even the traditional metrics based on vertex dislocations are relevant for perception in some situations. Typically, when multiple objects interact by touching (shaking hands, feet touching floor), a too large absolute dislocation becomes observable and is perceived as an artefact. Our secondary motivation is therefore to design the algorithm so that the root-mean-squared error is reduced as well.In this paper, we propose a triangle mesh compression algorithm with the following key features:•Adaptability to any error metric of choice, investing bits only in areas where refinement is needed in order to reduce the distortion. In particular, adaptability to perceptually motivated metrics has been implemented and tested.Improved performance in traditional absolute metrics with respect to other perception-motivated algorithms.Fast decoding procedure, which only involves local operations on the triangle mesh. This allows application of our approach for larger meshes, where the decoding computational complexity makes competing algorithms impractical.Progressive decoding support.Our algorithm builds on the parallelogram-based prediction strategy, but only uses this approach to build a coarse reconstruction of the mesh. Subsequently, an error metric of choice is used to determine where refinement bits are required in order to suppress perceivable artefacts. Our procedure is able to exploit the spatial coherence of mesh vertices by using the efficient parallelogram prediction for high-order bits, and at the same time it is able to distribute the bit budget efficiently across parts of the mesh that have different properties.

@&#CONCLUSIONS@&#
We presented an algorithm that allows compressing a triangle mesh in a format oriented at progressive transmission, combining aspects of both single-rate and progressive paradigms. It can be easily adapted to work with any kind of metric, achieving considerable improvements for perception-based error metrics. The experiments show that our technique represents a reasonable trade-off between the visual error and the absolute vertex displacement. It outperforms the traditional and still widely used parallelogram predictor in terms of perception-related error, with an improvement of up to 15% in terms of data rate. In some cases, our algorithm is outperformed by the Laplacian encoding of the mesh, yet at the same time it provides other features that make it more attractive:•The decompression speed of our approach is usually more than 10 times faster, which is an important property for practical applications. Since the time needed for the decompression is linear in number of mesh vertices, while the decompression in Laplacian encoding involves solving a large system of linear equations, it is likely that the performance gap will become even larger for more detailed meshes.Thanks to the octree spatial partitioning, our technique provides considerably improved performance in terms of absolute vertex displacements, which is of importance in practical situations where multiple objects interact with each other.Our algorithm is able to provide progressive transmission and decoding of a mesh, as demonstrated in Fig. 14.One of the main limitations of our technique is related to the high cost of the encoding process, especially when compared to other techniques. However, we believe that this should not represent a problem, since the decoding process is comparable to the other state-of-the-art methods and the encoding part of the process can be performed in a batch process on the encoder side.In the future we plan to extend our technique in order to work with encoding approaches based on differential coordinates. In these techniques it is not possible to locally refine the precision in vertex reconstruction, due to the nature of the problem. We plan to include the adaptive quantization in the setting by introducing different levels of quantization in the process, aiming at improving the compression rate without significantly affecting the visual error. Other future work aims at reducing the data rate by developing both a depth predictor, exploiting the correlation between vertices, error metric, and octree depth, and a refinement-bits-predictor, since the arithmetic encoding does not usually bring any significant gain.