@&#MAIN-TITLE@&#
Exact approaches for solving robust prize-collecting Steiner tree problems

@&#HIGHLIGHTS@&#
The paper deals with the Robust PCStT, addressing interval uncertainty on its data.Different mathematical programming formulations are proposed.Branch-and-cut Algorithms are designed and extensively tested.Recent theoretical results are adapted for the Robust PCStT and some of it variants.

@&#KEYPHRASES@&#
Prize collecting Steiner trees,Robust optimization,Interval uncertainty,Mixed integer programming,Branch-and-cut,

@&#ABSTRACT@&#
In the Prize-Collecting Steiner Tree Problem (PCStT) we are given a set of customers with potential revenues and a set of possible links connecting these customers with fixed installation costs. The goal is to decide which customers to connect into a tree structure so that the sum of the link costs plus the revenues of the customers that are left out is minimized. The problem, as well as some of its variants, is used to model a wide range of applications in telecommunications, gas distribution networks, protein–protein interaction networks, or image segmentation.In many applications it is unrealistic to assume that the revenues or the installation costs are known in advance. In this paper we consider the well-known Bertsimas and Sim (B&S) robust optimization approach, in which the input parameters are subject to interval uncertainty, and the level of robustness is controlled by introducing a control parameter, which represents the perception of the decision maker regarding the number of uncertain elements that will present an adverse behavior.We propose branch-and-cut approaches to solve the robust counterparts of the PCStT and the Budget Constraint variant and provide an extensive computational study on a set of benchmark instances that are adapted from the deterministic PCStT inputs. We show how the Price of Robustness influences the cost of the solutions and the algorithmic performance.Finally, we adapt our recent theoretical results regarding algorithms for a general class of B&S robust optimization problems for the robust PCStT and its budget and quota constrained variants.

@&#INTRODUCTION@&#
When defining an expansion plan of a fiber optic network in a given area and for a given planning horizon, a telecommunication company needs to decide to which subset of customers a service should be provided. Thereby, two elements need to be taken into account: potential gains in revenue (that will be referred to as prizes) of each customer, and infrastructure costs needed to connect them. This problem can be formulated as a network optimization problem called the Prize-Collecting Steiner Tree Problem (PCStT). In this paper we will focus on the PCStT and the Budget and Quota constrained variants, under data uncertainty assumption.When facing strategic decisions modeled by the PCStT, companies should consider the presence of uncertainty in problem parameters as an inevitable feature of the decision-making process. In our particular case, customer revenues and connection costs are uncertain parameters since they are affected by many external economic or even social factors. Consequently, uncertainty in both groups of parameters (or at least one of them) should be part of any decision model in order to obtain reliable and robust solutions from the economic point of view.In our models, robustness can be seen as a guarantee of protection against data uncertainty. This guarantee is provided by the use of the Bertsimas and Sim (B&S) Robust Optimization (RO) approach, see [11], which entails the adoption of protection functions that are included in the objective function and/or constraints. Protection functions depend on both, the uncertainty present in the problem’s input parameters and the intuition of the decision maker. These protection functions are all in all what determines the Price of Robustness, see [12], which can be defined as the worsening of the economic performance of the solutions while ensuring higher level of robustness in presence of higher levels of uncertainty. The resulting model will be called robust counterpart of the original deterministic problem.The PCStT arises as an important problem in Network Optimization from both the algorithmic and practical points of view (cf. Section 2). Therefore, we believe that studying the robust counterpart of the PCStT will help in solving and better understanding not only the robust PCStT itself, but also other related problems in the area of robust network optimization.In this paper we propose several RO variants of the PCStT and establish some connections between them. As main contribution, we propose three different strategies to exactly solve the Robust PCStT (RPCStT). These exact algorithms are all based on Branch-and-Cut techniques and the differences among them are implied by the underlying mathematical programming formulations and the different cutting-plane techniques. An extensive analysis of computational results is carried out in order to assess the performance of the proposed algorithms and their dependence on the problem parameters, and the nature and characteristics of the obtained solutions. This analysis concerns a qualitative study of the solutions in terms of the Price of Robustness and an interpretation and assessment of the different algorithmic performances. To complement this analysis, we also consider a budget-constrained variant of the PCStT and adapt the developed algorithms to solve its robust counterpart.Structure of the paper. In Section 2, the PCStT is formally defined, a review of the main literature is presented, two important variants of the problem, i.e., Budget and Quota PCStT, are defined, and an integer programming formulation is provided. In Section 3, motivations and alternatives to consider parameter uncertainty are presented with an emphasis on the B&S robust optimization model. Subsequently, different Mixed Integer Programming (MIP) formulations for the robust counterpart of the PCStT and the Budget and Quota Constraint variants are presented. Branch-and-Cut algorithms are presented in Section 4. In Section 5 we present and analyze the computational results obtained for different sets of benchmark instances for the robust counterparts of the PCStT and its budget-constrained variant. In Section 6, our recent theoretical results regarding algorithms for a general class of B&S robust optimization problems, see [3], are adapted for the robust PCStT and its variants. Finally, concluding remarks and paths for future work are presented in Section 7.The term Prize Collecting was used for the first time by Balas, see [7], in the context of the traveling salesman problem. However, it was in [13] where the PCStT has been introduced. It is worth to mention that in [38], Segev studied for first time the closely related Steiner tree problem with node weights. A formal definition of the PCStT can be given as follows.Given is an undirected graph G=(V, E) with n=∣V∣, m=∣E∣, edge costsce∈R>0for all e∈E, and node prizespv∈R⩾0for all v∈V. The PCStT consists of finding a tree T=(VT, ET) of G, that minimizes the function(1)f(T)=∑e∈ETce+∑v∈V⧹VTpv.For a feasible solution T, function (1) corresponds to the sum of the costs ceof the edges in the tree, e∈ET, plus the sum of the prizes pvof the nodes that are not spanned by the tree, v∈V⧹VT; this definition of the PCStT is known as the Goemans and Williamson PCStT (GW-PCStT) [13]. In the context of the expansion of fiber optic networks mentioned above, graph G=(V, E) is the potential network for which we want to find an expansion plan, so edges e∈E are the possible links with construction costs ceand nodes v∈V represent customers or street intersections with potential revenues pv>0 or pv=0, respectively. ByVpi>0(n′=|Vpi>0|) we will denote the set of potential customers and byVpi=0, the set of potential Steiner nodes.The PCStT can be also defined as the problem of finding a tree T that minimizes(2)fNW(T)=∑e∈ETce-∑v∈VTpv.Function (2) corresponds to the minimization version of the Net-Worth PCStT (NW-PCStT) which was introduced in [27]. Although functions (1) and (2) are equivalent in the sense that both produce the same optimal solutions, they are not equivalent regarding approximation algorithms, see [27].Approximation algorithms for the GW-PCStT are discussed in [13,20,27] and recently in [4]. Heuristic procedures are implemented in [14,29] and [37]. The first published work on polyhedral studies for the PCStT is [35], where a cutting plane algorithm is proposed. The cuts are efficiently generated when a violation of a generalized subtour elimination constraint (GSEC) is verified. In [34], a branch-and-cut algorithm based on a directed cut-set MIP formulation has been designed and implemented. Several state-of-the-art methods are combined and pre-processing techniques are used. The proposed procedure has significantly improved the algorithm presented in [35]. The same set of benchmark instances has been solved by two orders of magnitude smaller running times. Optimal solutions have also been achieved for large-scale real-world instances concerning the design of optical fiber networks. Another important algorithmic efforts for the PCStT and some of its variants have been presented in [14,21,22] and [23].In [34] an application of the problem is approached for the first time; the exact algorithm developed in the paper is used to solve real world instances for the design of fiber optic networks of a German city where an existing subnetwork needed to be augmented in order to serve new customers in the most profitable way. Over the last few years various other applications have been studied in which the PCStT has shown to play a crucial role in the modeling process. These problems arise from very different industrial and scientific contexts, showing the potential and versatility of the PCStT as a modeling tool.Relevant applications of the PCStT are found in Bioinformatics in the context of protein–protein interaction networks (PPINs). In [17,5,25] and [6] the PCStT is applied to network optimization problems arising in the analysis of PPIN for different datasets of biological processes. The PCStT is used to model an “inference problem” in order to find, or rather “to infer”, functional modules in PPIN. These networks represent signal pathways (constructed by edges) between proteins or protein complexes (represented by nodes). These biological networks are modeled as a graph G=(V, E), where edge costs cerepresent the confidence of interaction between the source and the target of the given edge e, and node prizes pvcorresponds to the differential expression of node v in the network for a given biological process. In [32], where a survey of models and algorithms for cellular response networks is provided, the PCStT and the algorithm studied in [17] (which is based on the exact approach developed in [34]) are presented as state-of-the-art tools for the detection of response networks in the context of analysis of gene expressions. Recently in [24], the author emphasizes the quality of the results obtained using the PCStT model compared with other modeling and algorithmic approaches for the analysis of signaling networks carried out over different gene databases.The design of a leakage detection system using the PCStT is performed in [36]. The problem consists of finding the optimal location of detectors in an urban water distribution network so that, given a budget constraint, a desired coverage is provided. The instance considered in the paper corresponds to the urban water distribution network of the city of Lausanne, Switzerland.In [42], the PCStT is used to efficiently detect region-based objects in the context of image recognition. Nodes v represent superpixels and edges e connect pairs of superpixels that share a boundary. Node prizes pvrepresent the contribution of the superpixel to the classifier score, and edge costs are a measure of the probability of two superpixels to belong to the same element. The objective is to find a best-scoring subregion identifying the most likely region of the object of interest. It is important to remark that in [17] and [42] the equivalence between the PCStT and the Maximum-weight connected subgraph problem (MWCS) is exploited to model the particular problem. For more details see [26].To characterize the set of feasible solutions for the PCStT, i.e., subtrees of G, we consider a directed graph model and use connectivity inequalities to guarantee connectivity of the solution.We transform the graph G=(V, E) into the directed graph GSA=(VSA, ASA). The vertex set VSA=V∪{r} contains the nodes of the input graph G and an artificial root vertex r. The arc set ASAis defined asASA={(r,i)|i∈Vpi>0}∪A, where A={(i, j), (j, i)∣e={i, j}∈E}. A subgraph TSAof GSAthat forms a directed tree rooted at r such that for each node i in TSAthere is a directed path between r and i, is called a Steiner arborescence and is a feasible solution of the problem in case there is only one outgoing arc from r. We will use the following notation: A set of vertices R⊂VSAand its complementR¯=VSA⧹R,R≠∅, induce two directed cuts:δ+(R)={(i,j)|i∈R,j∈R¯}andδ-(R)={(i,j)|i∈R¯,j∈R}. Let zij, ∀(i, j)∈A, be a binary variable such that zij=1 if arc (i, j) belongs to a feasible arborescence TSAand zij=0 otherwise. Let yi, ∀i∈V, be a binary variable such that yi=1 if node i belongs to TSAand yi=0 otherwise. The set of constraints that characterizes the set of feasible solutions of PCStT is given by:(3)∑(j,i)∈δ-(i)zji=yi∀i∈VSA⧹{r},(4)∑(i,j)∈δ-(R)zij⩾yk,k∈R,∀R⊆VSA⧹{r},R≠∅,(5)∑(r,i)∈δ+(r)zri=1.Let xe, ∀e∈E, be a binary variable such that xe=1 if edge e belongs to a feasible subtree T (induced by TSA) and xe=0 otherwise. The connection between x and z variables is given by(6)xe=zij+zji∀e={i,j}∈E.The corresponding set of feasible solutions satisfying these inequalities is given as:T={(x,y)∈{0,1}|E|+|V||(x,y,z)satisfies(3)–(6)andz∈{0,1}|ASA|}.Constraints (4), also known as cut or connectivity inequalities, are the directed counterpart of undirected GSECs used in [35]. They ensure that there is a directed path from the root r to each customer k such that yk=1. In-degree constraints (3) guarantee that the in-degree of each vertex of the tree is equal to one. The root-out-degree constraint (5) makes sure that the artificial root is connected to exactly one of the terminals. In addition, the following inequalities are used to initialize the MIP model:(7)zrj⩽1-yi,∀i<j,i,j∈Vpi>0,(8)∑(j,i)∈δ-(i)zji⩽∑(i,j)∈δ+(i)zij,∀i∈Vpi=0.Constraints (7), the so-called asymmetry constraints, ensure that for each feasible solution the customer vertex adjacent to the root is the one with the smallest index. Inequalities (8) are the flow-balance constraints, originally introduced for the Steiner tree problem (see [31]). Constraints (7) cut off symmetric solutions, while constraints (8) improve the quality of lower bounds of the Linear Programming (LP) relaxation of the MIP model.In the remainder, let T=(VT, ET) denote the tree induced by a pair (x, y), such that ET={e∣xe=1} and VT={v∣yv=1}. For simplicity of notation we state that T≡(x, y).Two well-known variants of the PCStT are the Budget Constrained PCStT (B-PCStT) and the Quota Constrained PCStT (Q-PCStT), which are presented for the first time in [27], where also approximation algorithms and computational studies have been provided.Given a cost budgetB,B∈R⩾0, representing the maximum total cost allowed for the construction of the solution, the B-PCStT is defined as(9)fB∗(T)=minT∈T∑v∈V⧹VTpv∑e∈ETce⩽B.Given a prize quotaQ,Q∈R>0, representing the maximum total prize allowed to be left out of a solution (or the total prize allowed to be lost), the Q-PCStT is defined as(10)fQ∗(T)=minT∈T∑e∈ETce∑v∈V⧹VTpv⩽Q.Problem (9) and (10) are natural extensions of the problem that appear in the bi-objective optimization framework. There are two conflicting goals, namely, minimization of the cost and maximization of the profit, and typically, one can solve these problems in iterative frameworks by e.g., the weighted sum approach or ∊-constrained based approaches (see, e.g., [18]).In this paper we consider decision-making environments with a lack of complete knowledge about the uncertain state of data and instead of dealing with probabilistic uncertainty (as in stochastic optimization, see e.g., [41]) we actually deal with deterministic uncertainty[11]. In contrast to probabilistic models, that treat the input parameters as random variables, in the deterministic uncertainty models we assume that the input parameters belong to a known deterministic set. This is in the core of many real world applications and it is the motivation supporting the robust optimization approaches, where the essential objective is to find solutions that will have a reasonably good performance (of optimality and/or feasibility) for all possible realizations of the parameter values.In the last 20years several RO models have been proposed, corresponding to different motivations and conceptual definitions; for a deep and extensive study on the RO we refer the reader to [10]. In our opinion there are three main characteristics that define the differences among RO models: (1) The nature of the input data; whether the data belong to e.g., an ellipsoidal set or polyhedral set, a closed interval, or a set of discrete scenarios; (2) If robustness is considered with respect to the value of the objective function (robust solution), to the feasibility of the solution (robust model) or both; (3) The definition of reasonably good performance of a solution, which is what determines the main features of the model.In this paper we consider the RO concept by Bertsimas and Sim (B&S) defined in [11] and [12]. This model is considered as one of the most important references in the field of RO. Regarding the first characteristic mentioned above, this approach tackles interval uncertainty. Regarding robustness, the B&S model allows to find solutions that are robust in terms of optimality and/or feasibility of the solution. The definition of what is a reasonably good performance of a solution is given by the protection against a pre-defined number of parameters that might be subject to uncertainty.In this paper we consider interval uncertainty, which means that associated with each input parameter there is a closed interval with its lower and upper bounds. Formally, in the case of the PCStT, an intervalce-,ce+, such that0<ce-⩽ce+, is associated with each edge e∈E, and an intervalpv-,pv+, such that0⩽pv-⩽pv+is associated with each customerv∈Vpi>0. To simplify the notation, we will define0⩽pv-⩽pv+for all nodes v∈V, wherepv-=pv+=0for potential Steiner nodesv∈Vpi=0. Since we consider deterministic uncertainty, each input parameter can take any value from the corresponding interval without any specific (or known) behavior and independently of the values taken by the other parameters. The lower interval valuesce-andpv-will be referred to as nominal values, i.e., they are the values to be considered if the deterministic PCStT is solved. Deviations from the nominal values are defined as:de=ce+-ce-, for all e∈E anddv=pv+-pv-, for all v∈V. In the following we will present two ways to derive mathematical programming formulations for the robust counterpart of the PCStT and its variants.The PCStT under interval uncertainty has been considered before in [1]. The authors used an alternative RO model based on a Risk/Cost trade-off concept defined in [15] and provided polynomial time algorithms for solving both the PCStT and its robust counterpart on 2-trees. In this context, our work is complementary since we consider a different RO model and we provide a more general algorithmic framework focusing on graphs with general structure. The PCStT under interval uncertainty with the B&S RO model has been introduced in our preliminary work [2]. In that work, one of the three approaches studied in this paper has been computationally tested; however, only for the robust version of the PCStT and on a subset of the instances that are considered here.Suppose that a decision maker wants to solve the PCStT in which the input parameters, edge costs and node prizes, are subject to interval uncertainty. In many practical applications it is unlikely that all of edge costs and/or node prizes will present an uncertain behavior at the same time. Therefore, we assume that only a subset of input data is subject to uncertainty, while the remaining parameters are fixed to their nominal values. More precisely, the decision maker may assume that only ΓEedges and ΓVnodes (ΓE∈[0, m] and ΓV∈[0, n′]) will be subject to uncertainty, although she/he does not know exactly which they are. Without loss of generality, we will assume that the values of ΓEand ΓVare integral.The essence of the model is to find a solution that is “robust” considering all scenarios in which ΓEedges and ΓVnodes present an adverse behavior. If ΓE=0 and ΓV=0, then uncertainty is ignored and the problem to solve is nothing but the nominal problem, whereas if ΓE=m and ΓV=n′, i.e., full uncertainty is assumed, the most conservative robust solution is sought.Considering the general mathematical programming formulation for combinatorial optimization problems with interval uncertainty presented in [11], the B&S RPCStT can be formulated as(11)ROPT(ΓE,ΓV)=minT∈T∑e∈ETce-+βE∗(ΓE)+∑v∈V⧹VTpv-+βV∗(ΓV),whereβE∗(ΓE)=max∑e∈E∼∩ETde|∀E∼⊆E,|E∼|⩽ΓEandβV∗(ΓV)=max∑v∈V∼∩{V⧹VT}dv|∀V∼⊆V,|V∼|⩽ΓV.These last two functions are the so-called protection functions and they provide robustness to the solutions in terms of protection of optimality in presence of a given level of data uncertainty, represented by ΓEand ΓV.An optimal solution for (11) can be interpreted as the one that minimizes the total nominal cost plus the cost of the maximal ΓEdeviations in the cost of the edges of the solution plus the maximal ΓVdeviations in the prizes of the nodes that are not spanned by the solution. If ΓE=m and ΓV=n′, the solution will obviously correspond to the optimal (worst-case) deterministic solution in which all edge costs and node prizes will be set to their upper bounds. The flexibility provided by ΓEand ΓVis the main advantage of the model from the practical point of view, because it allows the decision maker to include her/his preferences in order to control the level of conservatism of the solutions.Formulation Based on Compact Robust Constraints: To find a mixed integer programming formulation for (11), it is necessary to rewrite protection functionsβE∗(ΓE)andβV∗(ΓV)using auxiliary variables ue∈[0, 1], ∀e∈E and uv∈[0, 1], ∀v∈V, which represent the portion of the corresponding deviation, deand dvrespectively, included into the protection function. We thus obtain(12)βE∗(ΓE)=max∑e∈ETdeue|ue∈[0,1]∀e∈E,∑e∈Eue⩽ΓEand(13)βV∗(ΓV)=max∑v∈V⧹VTdvuv|uv∈[0,1]∀v∈V,∑v∈Vuv⩽ΓV.When considering (12) and (13) it is clear that the objective function of (11) contains two non-linear nested maximization problems. To overcome this, one can use strong duality. Let T∗≡(x∗, y∗) be an optimal tree for (11). Objective functions of problems (12) and (13) can be written as∑e∈Edexe∗ueand∑v∈Vdv(1-yv∗)uv, respectively. By strong duality (see, e.g. [11]), we have:(14)βE∗(ΓE)=minθΓE+∑e∈Ehe|he+θ⩾dexe∗andhe⩾0∀e∈Eθ⩾0and(15)βV∗(ΓV)=minλΓV+∑v∈Vkv|kv+λ⩾dv1-yv∗andkv⩾0∀v∈Vλ⩾0,respectively. Combining (11), (14) and (15), we can formulate the B&S RPCStT as the following Mixed Integer Programming (MIP) model:(16)ROPT(ΓE,ΓV)=min∑e∈Ece-xe+θΓE+∑e∈Ehe+∑v∈Vpv-(1-yv)+λΓV+∑v∈Vkvs.t.(17)he+θ⩾dexe∀e∈E(18)kv+λ⩾dv(1-yv),∀v∈V,(19)he⩾0∀e∈Ekv⩾0∀v∈Vandθ,λ⩾0(20)(x,y)∈T.In this model, variables he, kv, θ and λ are called “robust variables”, while constraints (17) and (18) are called “compact robust-constraints” as their number is linear in m and n.Formulation Based on Robustness Cuts: One can also use Benders decomposition to project out robust variables from the previous formulation. Since every solution(x,y)∈Tis feasible for the robust counterpart of the problem, only Benders optimality cuts will be needed to describe the robustness of an optimal solution. These optimality cuts are given by constraints (22) and (23) below:(21)ROPT(ΓE,ΓV)=min∑e∈Ece-xe+Θ+∑v∈Vpv-(1-yv)+Λ,(22)s.t.Θ⩾∑e∈Sdexe∀S⊆E|S|⩽ΓE,(23)Λ⩾∑v∈Rdv(1-yv)∀R⊆V|R|⩽ΓV,(24)Θ,Λ⩾0,(25)(x,y)∈T.In this model, additional variables Θ and Λ and constraints (22) and (23) allow to model the two nested maximization problemsβE∗(ΓE)andβV∗(ΓV), respectively. Constraints (22) and (23) are called “robustness cuts”. In this model we enforce robustness by working directly on the space of variables (x, y) at the expense of adding an exponential number of robustness constraints. In Section 4, we will show that these constraints can be separated in polynomial time. In Section 5 we will provide a computational study comparing the practical performance of the compact robust constraints versus these robustness cuts. In [19], the authors have proposed to use robustness cuts for modeling robust linear optimization problems with uncertainty in the constraint parameters.It is known that for the deterministic case the connection between f(T) and fNW(T) is given asfNW(T)=f(T)-∑v∈Vpv,i.e., the two formulations of deterministic GW-PCStT and NW-PCStT find the same solution because the sum of node revenues is constant. However, when node revenues are subject to interval uncertainty and a B&S robust solution is sought, this sum is not constant anymore. In this case, the robust counterpart of the NW-PCStT is essentially solving a different problem. To better understand this difference, assume for a moment that edge costs are deterministic. Recall now that in the robust counterpart of the GW-PCStT, nominal values for node revenues are set to conservative lower bounds and, therefore ROPT corresponds to a potential increase of revenues, which a decision maker can miss. On the other hand, conservative setting for the node revenues in the robust NW-PCStT case is to assume the values are set to their upper bounds,pv+, for all v∈V.By following the same ideas presented above for the GW-PCStT, the B&S Robust counterpart of the NW-PCStT is defined as:(26)ROPTNW(ΓE,ΓV)=min(x,y)∈T∑e∈Ece-xe+βE∗(ΓE)-∑v∈Vpv+yv-ηV∗(ΓV),whereηV∗(ΓV)=max∑v∈Vdvuv∑v∈Vuv⩽ΓVuv∈[0,1]∀v∈V.In other words, when assuming deterministic edge costs, ROPTNWcorresponds to a potential decrease of revenues, that the decision maker can experience. It can be easily seen from (26) that larger values of ΓVwill increase the total value of the solution (i.e., decrease the total revenue) as it is expected in this RO model. A MIP formulation can be obtained accordingly by following the same procedure explained for the GW-PCStT.Despite the fact that these two robust formulations essentially model different problems, the next result shows that in particular cases the two formulations are the same.Observation 1For a fixed value ofΓ∼E∈[0,m], and ΓV∈{0, n′}, the robust counterparts of the GW-PCStT and of the NW-PCStT are equivalent, i.e., they produce identical optimal subtrees. The following connection exists between the corresponding objective values:ROPTNW(Γ∼E,0)=ROPT(Γ∼E,n′)-∑v∈Vpv+andROPTNW(Γ∼E,n′)=ROPT(Γ∼E,0)-∑v∈Vpv-.In the case of both the GW-PCStT and the NW-PCStT, uncertainty is present only in the coefficients of the objective function, which means that their robust counterparts provide protection with respect to the optimality of the solutions. However, in the case of the B-PCStT and if the Q-PCStT, the presence of uncertainty in edge costs and in node prizes affects not only their corresponding objective functions but also their budget and quota constraints, respectively. Therefore, for a given level of uncertainty, the robust counterpart of these problems should not only provide protection in terms of optimality but also in terms of feasibility.Adopting the ideas presented in the previous sections, the Robust B&S Budget Constrained PCStT (B-PCStT), is defined as:ROPTB=min(x,y)∈T∑v∈Vpv-(1-yv)+βV∗(ΓV)∑e∈Ece-xe+βE∗(ΓE)⩽B.According to the previous section, for a given descriptionTof the deterministic problem, one can consider four possible ways to derive a valid MIP model for this robust counterpart of the problem. The objective function can be modeled using compact or Benders robust constraints. But also the budget constraint can be modeled using one or the other variant. To model the budget constraint using Benders reformulation, we will need to insert the following family of inequalities into the MIP:(27)∑e∈Ece-xe+∑e∈Sdexe⩽B∀S⊆E,|S|⩽ΓE.These cuts are similar to (22) (see also [19]).Similarly, the Robust B&S Quota Constrained PCStT (Q-PCStT), is defined as:ROPTQ=min(x,y)∈T∑e∈Ece-xe+βE∗(ΓE)∑v∈Vpv-(1-yv)+βV∗(ΓV)⩽Qand again one can consider four ways of deriving a MIP model for this problem.The MIP formulations considered throughout this paper cannot be solved directly, even for small instances, since there is an exponential number of connectivity constraints of type (4) and, in addition, if Benders cuts are used to model the protection functions, there is also an exponential number of robustness cuts to be considered. Consequently, more sophisticated and specific techniques should be designed and implemented to solve these models.In this section we propose three ways to develop a branch-and-cut (B&C) algorithm for solving the robust PCStT and its budget and quota constrained variants. We will explain the main ideas for solving the RPCStT, and a similar scheme needs to be applied in order to solve the B-RPCStT or the Q-RPCStT.B&C with Compact Robust Constraints (Compact): In this approach, we are solving the MIP model in which the deterministic model (3)–(8) is extended by a compact set of auxiliary variables and constraints (17)–(19) that model the protection functions (cf. Section 3.2). In this approach, only connectivity constraints will be separated within a B&C framework. The separation algorithm is an adaptation of the exact approach presented in [34]. The MIP initially contains all variables and the constraints (3), (5)–(8). In addition, we explicitly insert the subtour elimination constraints of size 2:xij+xji⩽yi∀i∈VSA⧹{r}j∈δ+(i)to avoid too frequent calls of the maximum flow procedure. The connectivity constraints are separated within the B&C framework by means of the maximum flow algorithm given in [16]. This separation randomly selects a terminali∈Vpi>0, calculates the maximum flow between an artificial root and i and inserts the corresponding (4), if violated. Instead of adding a single violated cut per iteration, we use nested, back-flow and minimum cardinality cuts to add as many violated cuts as possible (see [31] for details). We restrict the number of inserted cuts within each separation callback to 25.B&C with Separation of Robustness Cuts (R-Cuts): In this approach, we consider the MIP model in which protection functions are modeled by means of robustness cuts of type (22) and (23). We initialize the model using only the following bounds for Θ and Λ variables:Θ⩽∑e∈SΓE∗deandΛ⩽∑e∈SΓV∗dv,whereSΓE∗(SΓV∗)is the subset of edges (nodes) containing ΓE(ΓV) edges (nodes) with largest deviations. The correctness of the bounds comes from the fact that both Θ and Λ accumulate the deviations of the nominal costs for the solution edges and for the nodes left out of the solution, respectively.The separation problem for robustness cuts of type (22) is as follows: given the current LP solution(xˆ,yˆ,Θ^,Λ^), find a set S⊆E such that ∣S∣⩽ΓEand∑e∈Sdexˆeis a maximum. Assume that a subset of edges S∗ satisfies these properties. If∑e∈S∗dexˆe>Θ^, the current LP solution violates constraint (22) and hence we insert the cutΘ⩾∑e∈S∗dexeinto the model. To determine the set S∗, we associate with each edge e∈E a weightwe=dexˆe. The separation problem consists in finding the subset of edges of size ΓEwith the maximum weight, which can be done in O(∣E∣) time (see also [19]). This idea was first implemented in [19] in the context of robust optimization for linear and integer programming under uncertainty. The authors report a remarkable improvement in the running times when using these robustness cuts in the formulations and separation framework instead of a compact formulation.Robustness cuts are added on the fly, within the B&C framework, i.e., we are not waiting to find an LP-solution that satisfies all the connectivity cuts. Instead, within one separation callback, we insert all the violated connectivity cuts detected plus the (one or two) robustness cuts associated with (22) and (23).B&C with Separation of Robust Compact Constraints (C-Cuts): We have observed that not all of compact constraints associated with a protection function are tight in an optimal solution. On the other hand, when the number of nodes and/or edges increases, the size of the compact block of constraints associated withβV∗(ΓV)orβE∗(ΓE)may become a bottleneck of the implementation. Therefore, instead of inserting all these constraints at once, we propose to separate them within a B&C framework. We start with an LP model in which there are no constraints associated with robust variables, except the following ones:∑e∈Ehe+θ⩽∑e∈SΓE∗deand∑v∈Vkv+λ⩽∑v∈SΓV∗dvandθ⩽dΓEandλ⩽dΓVwhereSΓE∗(SΓV∗) has been described above,dΓEis the ΓEth largest edge cost deviation anddΓVthe ΓVth largest node prize deviation (see Lemma 1 in [3]).The separation of constraints (17) can be stated as follows: given an LP-solution(xˆ,yˆ,hˆ,kˆ,θˆ,λˆ), find a setE^⊆Eof maximum cardinality for whichhˆe+θˆ<dexˆe∀e∈E^and insert the corresponding constraints of type (17). Of course, the separation of constraints (17) and (18) can be performed in O(∣E∣) andO(|Vpi>0|)time, respectively.Within the B&C framework we first separate all the connectivity constraints (4), and once we find an optimal LP solution, we find a subset of violated compact robust constraints, and insert all of them at once into the current LP.Benchmark Instances. In our computational experiments four sets of benchmark instances have been tested: C, D, K and P. These instances have been used in most of the papers discussing algorithm design for the PCStT [35,34,37]. Instances of group P were introduced in [27] – they are unstructured and designed to have constant node degree and a constant prize/cost ratio. Group K are randomly generated geometric graphs designed to have a structure similar to street maps [27]. Groups C and D were presented in [14]. These two groups of instances are derived from the instances of the Steiner tree problem provided in the OR-Library [8].Groups C and D are composed by 40 instances each with 500 and 1000 nodes, respectively, and the number of edges goes from 625 to 12,500 and from 1250 to 25,000, respectively. Group P is composed by 11 instances with 100, 200 and 400 nodes and the number of edges goes from 300 to 1185. Finally, group K is formed by 23 instances with 100, 200 and 400 nodes and the number of edges goes from 344 to 1493. For more details on the description of instances see the first four columns of Table 9 (see Online Supplement).Given an original instance Prob for the deterministic PCStT, the corresponding robust instance, named Prob-α-β, (α∈[0, 1] and β∈[0, 1]) is derived as follows: the number of nodes and edges are left unchanged. Lower limits,ce-andpv-, for intervals defining edge costs and node prizes are set to the corresponding deterministic values ceand pv, i.e.,ce-=ce∀e∈E andpv-=pv∀v∈V. The upper limit of edge costs,ce+, is set to (1+α)ce∀e∈E. Similarly, the upper limit of node prizes,pv+, is set to (1+β)pv∀v∈V. Parameters α and β allow to control the width of the corresponding intervals and, consequently, the level of uncertainty of the problems. For most of our experiments we consider (α=0.05, β=0.05) (unless mentioned otherwise), which means that both edge costs and node prizes present a deviation equal to the 5% of their corresponding nominal values. In preliminary experiments we also considered deviations of 1% and 2.5%, however, these instances did not allow to clearly show the impact of considering higher levels of uncertainty on both the solution structure and the algorithm performance. A deviation of 5% is in the middle of the values considered in most of the literature which range from 1% up to 10% (see [11,12] and [19], among other papers). Almost the same criterion to generate interval data instances is also used in [9,30,33], and [39].Machine and Implementation. All the experiments were performed on a Intel Core2 Quad 2.33GHz machine with 3.25GB RAM, where each run was performed on a single processor. The Branch-and-cut algorithms were implemented using CPLEX 12.2 and Concert Technology. All CPLEX parameters were set to their default values, except the following ones: (i) Branching: we set the highest branching priorities to variablesyv,v∈Vpi>0; (ii) Emphasis: this parameter was set to optimality. (iii) Maximum Running Time was set to 500seconds.In the following tables and figures, the running times are expressed in CPU seconds.Reduction Tests Reduction tests for the deterministic PCStT have been implemented in [14,35,34] and [40]. It has been demonstrated that the utilization of some of these preprocessing procedures can lead to remarkable improvements of the algorithmic performance. For our interval data instances we have adapted one of these reduction tests, which is described in the following.Robust Least-cost Test. Let SPij(ΓE) be the cost of the B&S robust shortest path between a pair of nodes i and j calculated for ΓEin G. If there is an edge e connecting i and j such thatSPij(ΓE)⩽ce-, then edge e can be eliminated from G.Since the calculation of SPij(ΓE) requires O(m) shortest path calculations (cf. [11]), in our implementation we have used only a weaker variant of this test in which SPij(ΓE) is replaced by SPij(∣E∣). Although somehow conservative, this reduction criterion provides a unique reduced graph valid for any value of ΓE<∣E∣ when solving the RPCStT or any of its variants. For larger instances, the reduced graphs have less than 50% of the original number of edges. It is important to observe that applying this test requires only a few seconds even for large instances. It turned out that the other robust reduction tests cannot be easily derived from their deterministic counterparts – an illustrative example is a degree two test on a potential Steiner node. After merging two edges and two intervals into one, we basically obtain a new edge whose interval contains an extra break point that is needed to model 0, 1 or 2 deviations from the nominal edge costs.As mentioned above, the Price of Robustness corresponds to the increase of the cost of a robust solution with respect to the nominal cost when increasing the level of robustness, i.e., when increasing the values of ΓEand ΓV. For each group of instances, we report in Table 1the minimum, mean and maximum values, computed over all the instances of the corresponding setting and group, of the relative increase of the cost of the solutions, ΔROPT(%), for different combinations of ΓEand ΓV. For each instance and setting, ΔROPT(%) is defined as (ROPT−OPT)∗100/OPT, where ROPT and OPT are the corresponding optimal values1In case that none of the exact approaches was able to find an ROPT optimal solution within the specified time limit, we used the best known upper bound to calculate ΔROPT(%), which is a good approximation considering the quality of the gaps.1of the robust and of the nominal solution, respectively. For each instance, we consider 16 settings obtained by combining the 16 pairs of ΓV, ΓE∈ {0, 5, 20, 50}. Since we chose (α=0.05, β=0.05) for generating the instances, we would expect ΔROPT(%) to be always not greater than 5%. The difference between 5% and Δ ROPT(%) can be seen as the level of protection provided by the robust model and the chosen values of ΓEand ΓV. From the information reported in Table 1, two main observations can be made: (i) the B&S model seems to provide more protection against uncertainty to groups C and D than to groups K and P, and (ii) in the case of groups C, D and P, parameter ΓEhas a stronger impact on the price of robustness than ΓV, while in the case of group K, parameter ΓVis the one with a stronger influence on the price of robustness.Both observations can be explained by considering the relation between the particular values of ΓEand ΓVand the size (i.e., the number of edges and nodes) of the obtained solutions, whose statistics are given in the last 6 columns of Table 9 (see Online Supplement). In the case of C, D and P instances, the average number of edges in the solutions is almost always greater than the chosen values of ΓE, which means that in many cases the cost of some edges in the solution will remain within the corresponding lower limit. This explains why, for a given ΓV, the average value of ΔROPT(%) does not reach 5% even when ΓE=50. When comparing the average number of all customers and the average number of customers connected by the solutions for groups C and D (see Table 9 in the Online Supplement), it can be easily seen that many customers are taken into the solution. This means that the number of non-connected customers, i.e., those nodes whose prizes and deviations are added in the objective function, is generally smaller than ΓV=50. This explains why, for a given value of ΓE, a variation of ΓVdoes not strongly increase the value of ΔROPT(%). In the case of group P, particularly for instances P400.{0--4}, the ratio between the connected versus non-connected customers might be a little bit smaller than in the case of C and D, which explains why Δ ROPT(%) can be as high as 4.44% for the maximum values of ΓEand ΓV.In contrast to what happens for C, D and P groups, in the case of instances of group K, most of the solutions are on average relatively small, which explains why the mean value of Δ ROPT(%) can reach almost 5% for large values of ΓEand ΓV. The particular Euclidean geometric topology of these instances might also give hints to understand these results; nodes are ”locally connected” within a neighborhood, so despite the increase in the prize of non-connected nodes these are not reached because there are no direct connections between a given component and these attractive nodes, which increases the overall cost of the solution.To look deeper into the impact of ΓEand ΓVon the structure of the solutions, Fig. 1show two optimal solutions obtained for the instance K400.4--0.05--0.05 for ΓE=0, ΓV=50 and for ΓE=50, ΓV=0, respectively. The ROPT value of the first solution is 403,036 while that of the second is 393,919, which represents a relative difference of only 2.3% although the structure of the solutions are quite different; just as a reference, the value of ROPT for ΓE=0 and ΓV=0 is 389,451. These two figures put in evidence the capability of the B&S model to produce very different robust solutions for different levels of conservatism, and, at the same time, to provide a guarantee of protection in terms of the relative increase of the solution cost. This important feature of the model offers the possibility to choose a solution according to the perception of the uncertain state of the decision-making environment.As mentioned before, to solve the RPCStT we used three different B&C strategies: Compact, R-Cuts and C-Cuts. The performance of these different approaches depends not only on the instance group, and the size of the instances therein, but also on the particular selection of the parameters ΓEand ΓV.Fig. 2a shows the cumulative percentage of instances of group C solved to optimality within a given time ranging from to 0 to 500seconds. We compare the three different approaches for 16 settings across all values of ΓE, ΓV∈{0, 5, 20, 50} and across all 40 instances of group C. From this figure we conclude that Compact seems to be the best approach for this group since a larger percentage of instances can be solved within smaller running times than those of the other two approaches. However, we also observe that C-Cuts behaves similarly. To solve 90% of the instances, Compact requires less than 30seconds, C-Cuts slightly more than 30seconds, and R-Cuts more than 400seconds. To solve an extra 5% of instances, Compact requires about 300seconds, while both C-Cuts and R-Cuts reach the time limit (500seconds). Overall, R-Cuts presents a performance clearly worse than that of C-Cuts, and C-Cuts is slightly outperformed by Compact.To complement the previous analysis, Fig. 2b shows the cumulative percentage of instances solved by Compact considering four different combinations of ΓEand ΓV, for the 40 instances of group C. We can see that for the nominal case (ΓE=0 and ΓV=0), Compact can solve to optimality all the instances within just a few seconds. However, when increasing the values of ΓEand ΓV, the running times begin to increase quickly, and even for ΓE=5 and ΓV=5 there are a few instances that cannot be solved to optimality within 500seconds. Further increasing of the values of ΓEand ΓVproduces a severe deterioration of the algorithmic performance. For example, when taking ΓE=50 and ΓV=50, almost 15% of the instances cannot be solved to optimality within the given time limit. Hence, this is another aspect of the price of robustness: obtaining more robust solutions, in terms that they provide more protection against uncertainty, requires willingness to accept higher running times to calculate the optimal solutions.Tables 2–5provide more detailed statistics for the four groups of instances and the three algorithmic approaches. On the left hand side, for each of the approaches, we report the number of instances that are solved to optimality. On the center, statistics on the running times are reported considering only those instances that can be solved to optimality within 500seconds by all three approaches. On the right hand side, we provide statistics for the remaining problems (i.e., for those that cannot be solved to optimality by at least one of the approaches). For each approach, we report statistics on the final gap (calculated with respect to the corresponding lower bound) over these problems. These statistics indicate that, for the three approaches and across the four family of instances, there is a relatively small number of cases (given by a particular combination of ΓEand ΓV) that are intractable by the used algorithms. Although optimality is not always verified (especially by R-Cuts), the quality of the solutions obtained when reaching the time limit is remarkably good, as it can be seen from the statistics on the final gaps. The values of the median and the average of the gaps in Tables 2–5 indicate that the chosen formulations and approaches guarantee that solutions of a good quality can be obtained within a reasonable running time, in case that are not proven to be optimal. This observation complements the analysis of Fig. 2b.Further information about algorithmic performances is presented in the Online Supplement (Tables 10–13). The evolution, over time, of the gap between lower and upper bounds in the B&C tree for a subset of the most difficult instances is also analyzed in the Online Supplement.The overall superiority of Compact might be explained by the fact that from the beginning of the optimization process the underlying LP contains complete information regarding the robustness of the solution. Although at the root node we obtain tight bounds even if we consider R-Cuts or C-Cuts, after starting the branching process, a large sequence of re-optimizations (each time that a set of cuts is inserted we need to solve the underlying LP) deteriorates the optimization process entailing higher running times. In particular, in the case of R-Cuts, the convergence of the values of Θ and Λ becomes slower, i.e., more cuts have to be added and more branch-and-bound nodes have to be enumerated in order to reach optimal values. The combination of these two elements is responsible for the poor performance of this approach with respect to the others. A similar observation is pointed out in [19] when analyzing the performances of the compact formulation and robustness cuts to solve generic MIP problems.As mentioned before, robust instances were created from original instances using (α=0.05, β=0.05). In order to provide a more complete analysis of the robust model and the proposed approaches, we have also generated instances considering three additional combinations taken from α, β∈{0.05, 0.10}. For these experiments, we have considered groups C and K. We first present results regarding the Price of Robustness and then results regarding the performance of the proposed approaches.Price of Robustness. It is clear that if the interval width is increased (by augmenting α and/or β), the presence of uncertainty also increases; therefore, the price of robustness paid for a given level of uncertainty will be greater.In Table 6, similar to Table 1, we report statistics of the relative increase of the objective function value (ΔROPT(%)), when solving the RPCStT on instances of group C, for different values of ΓEand ΓVand considering the four resulting combinations of α and β. As expected, the value of ΔROPT(%) increases when increasing the values of α and β. In the four cases, one can recognize a common pattern: the value of ΓEis more responsible for the increase of ROPT than ΓV. As explained before for the (α=0.05, β=0.05) case, this is mainly due to the relation between the particular values of ΓEand ΓVand the size (number of edges and nodes) of the corresponding solutions; on average, the produced solutions have a quite similar size regardless of the values of α and β (cf. Table 14 in the Online Supplement). Roughly speaking, the solutions are on average comprised by 100 edges and a few nodes with positive prize are left out of the tree, which means that increasing α (uncertainty on the edges) has more impact on the solution cost than increasing β (uncertainty on the nodes). On the contrary, for instances of group K, the value of β, along with the value of ΓV, has more influence on ΔROPT(%) (cf. Table 15 in the Online Supplement); this can be concluded by taking into account the same arguments presented before for the (α=0.05, β=0.05) case.In summary, we can see that the effect produced on the price of robustness by different values of α and β follows a common pattern determined by the ratio between the size of the produced solutions and the corresponding values of ΓEand ΓV.Algorithmic Performance. In Table 7, similar to Tables 2–5, we report statistics for group C with ΓE, ΓV∈{0, 5, 20, 50} and four combinations of α and β. The first observation is that the proposed approaches behave quite similarly for the four pairs of α and β values: the number of instances solved to optimality is similar in each case, the running times are comparable and also the attained gaps are alike. The second observation is that regardless of the values of α and β, R-Cuts is the approach with the poorest performance. Regarding the other two algorithms, both are quite effective for all pairs (α, β), but looking at the number of instances solved to optimality we see that C-Cuts slightly outperforms Compact. Also in the case of group K (cf. Table 16 in the Online Supplement), α and β only slightly influence the algorithmic performance of the considered approaches, in which case, Compact marginally beats C-Cuts.More details regarding the influence of α and β on the algorithmic performance of C-Cuts are shown in Fig. 3a and b, where the cumulative percentage of solved instances within a given running time (that goes from 0 up to the time limit of 500seconds) is shown for group C and K, respectively, for the four combinations of α and β. In Fig. 3a one can see that the four curves are quite close to each other, reinforcing the conclusions obtained from Table 7 regarding the independence of the algorithms with respect to α and β when solving instances of group C. In the case of K instances, in Fig. 3b we see that when increasing the values of α and β some outliers appear and very few problems, 4 out of 386 in the case of (α=0.10, β=0.05) and 5 out of 386 in the case of (α=0.10, β=0.10), cannot be solved to optimality within the time limit of 500seconds (among these nine problems, gaps of at most 0.5% are reached). Equivalent conclusions can be drawn for Compact when analyzing the reported results in Fig. 8a and b in the Online Supplement. Hence, α and β have both a very limited influence on the algorithmic performance for the considered instances.In order to complement the analysis of the computational results presented for the RPCStT, we developed a similar experimental framework for the robust counterpart of the B-PCStT which, as we mentioned before, is an important variant of the PCStT. Because of the restriction on the length of the paper, we only present results obtained for group P and for eleven instances of group K (K100.{0--10}) considering (α=0.05, β=0.05).As part of the Robust B-PCStT model, it is necessary to provide a given budget B, which represents the maximum allowed sum of the edge costs, considering uncertainty, that the decision maker is willing to pay. Since different instances, even within the same group, have different cost structures, a given value of B might not be suitable for all of them, so it is necessary to establish a fair criterion to define appropriate values of B. In order to do so, we set the budget to be a percentage of a potential maximum robust budget value Bmax, associated with each particular instance. If the input graph is connected, Bmax represents the cost of the optimal robust Steiner tree in which all the customers are connected and the cost of at most ΓEedges is allowed to deviate from its nominal value. If the input graph is not connected, Bmax is the cost of the robust Steiner sub-tree connecting as many customers as possible. To calculate the value of Bmax, we set the node prizes to a big-M-value and ΓEto 50, and use one of the algorithms for the RPCStT proposed before. The selected value of ΓE=50 ensures feasibility for all the other values of ΓEas long as they are not greater than 50, which is the maximum value we consider for this parameter in our experiments. We note that it was necessary to set the node prizes to a big-M-value, instead of simply adding the constraints yv=1∀v∈Vpv-⩾0into the MIP model, because the considered instances are not necessarily connected.B&C Variants. Since there are more alternatives to formulate the Robust B-PCStT as a MIP, there are also more alternatives to design a B&C algorithm. Besides the separation of the connectivity inequalities, we have considered four alternatives to manage the different types of robust constraints: (i) B&C using the compact robust constraints of type (17) and (18) (Compact); (ii) B&C with separation of the robustness cuts of type (27) and type (23) including variable Λ in the objective function (R-Cuts); (iii) B&C with separation of the robust compact constraints of type (17) and (18) (C-Cuts); (iv) B&C with separation of the robustness cuts of type (27) but including all the compact constraints of type (18) (R-Cuts+Compact).In Fig. 4the value of ROPTBis reported for different values of the budget B and for four different combinations for ΓEand ΓVfor instance K100.10--0.05--0.05. As expected, and independently of the values of ΓEand ΓV, there is a monotone decrease of the value of the objective function (recall that this is the sum of the prizes of the nodes that are not connected) when increasing the value of the available budget. When considering a particular value of B, we observe that the differences of ROPTB, among different values of ΓEand ΓV, do not present a clear pattern as in the case of the RPCStT. This can be explained by the fact that ΓEis not included in the objective function but in the budget constraint, so it has an indirect influence on the objective function value. For example, when considering a budget given by 25% of Bmax, the four considered combinations produce significantly different values of ROPTB; while for a budget given by 90% of Bmax, the four values of ROPTBare almost the same. Another characteristic that we can observe, is that for tight budgets (0–20%) the value of ΓVhas more impact on the model than ΓE, while for large budgets (80–100%) it is just the opposite.As this was the case for the RPCStT, the latter behaviors are related to the size of the corresponding optimal solution and to its interaction with the problem parameters B, ΓEand ΓV. For a tight budget, an optimal solution is made up of only a few edges and many customer nodes are left unconnected, which explains why increasing the value of ΓVstrongly increases the value of the objective function, while increasing ΓEbarely produces changes since only a few edges can be taken into account. On the other hand, for a large budget, most of the customers are connected and an increase of ΓVmight not significantly affect the value of ROPTB, but increasing ΓEwill indeed strongly influence the value of ROPTBbecause the budget feasibility will enforce a solution of a smaller cardinality, i.e., it will be necessary to “disconnect” some customers and consequently the value of ROPTBwill be increased. An example that illustrates these dependencies is shown in Fig. 14 in the Online Supplement.Fig. 5a shows the cumulative percentage of instances of group P solved to optimality within time t with a time limit of 500 seconds, comparing the four approaches described above. In a more detailed plot (see Fig. 5b), only Compact and C-Cuts are compared. We observe that Compact and C-Cuts are substantially better than the other two approaches (which are both based on the utilization of robustness cuts). For example, to solve 95% percent of the instances of group P, Compact needs less than 20seconds, C-Cuts less than 30seconds, while the other two approaches need almost 150seconds to solve the same percentage of instances. Moreover, in a small number of cases (less than 2%), R-Cuts and R-Cuts+Compact reach the time limit without being able to find optimal solutions within the given time limit.More details about the running times needed to solve the instances, as well as the statistics on the gaps for those instances where at least one of the approaches failed to find an optimal solution, are reported in Table 8. For group P, Compact is the best in terms of average running times. However, C-Cuts has a similar performance and provides better minimum and median running times, but a few outliers (see Fig. 5b) deteriorate the overall statistics. The same table shows that, in 14 out of 1056 cases, R-Cuts and R-Cuts+Compact do not solve all the instances to optimality, but provide very small final gaps.Comparing the statistics for the RPCStT (see Table 5) with the results presented in Table 8 for the Robust B-PCStT, we may conclude that the Robust B-PCStT is a considerably more complex problem. With an inclusion of a budget constraint the search for an optimal solution becomes a more difficult numeric task. The influence of the budget level on the algorithmic performance is shown in Fig. 6, where the average running times over all the instances of group P are displayed for different budget levels and different values of ΓEand ΓV. Our first observation is that, independently of the values of the budget, increasing values of ΓEand ΓVdirectly influence the running times as it was the case of the RPCStT (see Fig. 2). However, budgets levels set between [0%, 25%] or [75%, 100%] entail a better algorithmic performance than those taken from [25%, 75%], and the influence of ΓEand ΓVis more accentuated in the latter case.These relations between the running times and the budget levels can be explained by the way how different values of B reduce the space of feasible solutions. Tight budgets, let us say [0%, 25%], strongly limit the set of feasible solutions, i.e., they usually correspond to small trees connecting a few customer nodes. Therefore, and considering that most of the P instances are sparse graphs (cf. Table 9 in the Online Supplement), the optimal solutions can be quickly obtained. On the other hand, optimal solutions for large budgets, as those defined by [75%, 100%], will be usually comprised by almost all the customer nodes; hence, solutions will be similar to the robust Steiner tree connecting those customers, which explains the decrease of the running times. On the contrary, for B chosen from [25%, 75%] of Bmax, the combinatorial nature of the problem seems to have more influence on the algorithmic performance and there are more solutions, probably each of them with a very different structure, that might verify the optimality. Consequently, the computational effort to find an optimal solution is greater as illustrated in Fig. 6.A similar analysis of the results obtained for instances K100.{0--10} (cf. Fig. 13 and Table 17 in the Online Supplement) lead us to conclude that C-Cuts is the best approach, both in terms of average and median running times. Once more, the robustness cuts based approaches do not seem to be competitive although they solve to optimality all instances within the given time limit. Consequently, we may say that both Compact and C-Cuts are the most effective approaches for solving the robust B-PCStT for the considered instances.Although in this paper we presented MIP-based exact approaches for solving the robust counterparts of the PCStT and its variants, it is also possible to solve them by successively solving a finite number of classical instances of the corresponding problem. The next corollaries are derived from the more general results presented in [3]. To apply the results below, we assume that the customers and the edges are sorted in non-increasing order with respect to their deviations, i.e., d1⩾d2⩾d3… and the last deviationsdn′+1(for the customers) and dm+1 (for the edges) are set to zero.Lemma 1Given ΓE∈{0, …, m} and a given ΓV∈{0, …, n′}, the B&S Robust Counterpart of the GW-PCStT can be solved by solving (n′−ΓV+2)(m−ΓE+2) nominal problemsROPT(ΓE,ΓV)=mina∈{ΓE,…,m+1}b∈{ΓV,…,n′+1}Ga,b,where for a∈{ΓE, …, m+1} and b∈{ΓV, …, n′+1}:Ga,b=ΓEda+ΓVdb+min(x,y)∈T∑e∈Ece-xe+∑e=1a(de-da)xe+∑v∈Vpv-(1-yv)+∑v=1b(dv-db)(1-yv).Given ΓE∈{0, …, m} and ΓV∈{0, …, n′}, the Robust B&S B-PCStT can be solved by solving (n′−ΓV+2)(m−ΓE+2) nominal problems(28)ROPTB(ΓE,ΓV)=mina∈{ΓE,…,m+1}b∈{ΓV,…,n′+1}GBa,b,where for a∈{ΓE, …, m+1} and b∈{ΓV, …, n′+1}Ga,b=ΓVdb+min(x,y)∈T∑v∈Vpv-(1-yv)+∑v=1b(dv-db)(1-yv)∑e∈Ece-xe+∑e=1a(de-da)xe+ΓEda⩽B.Given ΓE∈{0, …, m} and ΓV∈{0, …, n′}, the Robust B&S Q-PCStT can be solved by solving (m−ΓE+2)(n′−ΓV+2) nominal problems(29)ROPTQΓE,ΓV=mina∈{ΓE,…,m+1}b∈{ΓV,…,n′+1}GQa,b,where for a∈{ΓE, …, m+1} and b∈{ΓV, …, n′+1}Ga,b=ΓEda+min(x,y)∈T∑e∈Ece-xe+∑e=1a(de-da)xe∑v∈Vpv-(1-yv)+∑v=1b(dv-db)(1-yv)+ΓVdb⩽Q.Lemmas 1–3 are particularly useful when polynomial-time algorithms are available for graphs with some special structures (see, e.g., Corollary 1). We want to point out that for the case of general graphs, where the deterministic counterparts are NP-Hard, branch-and-cut algorithms like the ones presented in this paper remain a preferable option.Corollary 1If the input graph is a tree, a series–parallel graph or a 2-tree, the robust counterpart of the PCStT can be solved in O(∣V∣3) time.The deterministic PCStT can be solved in O(∣V∣) time on trees (see [28]). A series–parallel graph can be completed in linear time into a 2-tree. Ref. [1] have shown that the PCStT can be solved in O(∣V∣) time on 2-trees. We complete the proof by combining these results with the result of Lemma 1.□

@&#CONCLUSIONS@&#
In this paper we studied the PCStT and its budget and quota constrained variants assuming interval uncertainty associated with their input parameters. To include and handle this uncertainty we considered the B&S robust optimization (RO) approach, formulating the robust counterpart of the problems by means of different mixed integer programming formulations. Specific branch-and-cut algorithms were implemented to solve these problems. The algorithms were tested on a set of benchmark instances generated from state-of-the-art instances of the deterministic version of the problem. The obtained computational results suggest that: (1) the RO model allows to produce different robust solutions for different levels of conservatism. These solutions provide a protection in terms of the relatively small increase of the solution cost in presence of an increased uncertainty. This important feature of the model offers to the decision maker more flexibility to choose a solution according to her/his perception of the uncertain state of the decision-making environment. (2) The algorithmic performance strongly depends on the model parameters, ΓEand ΓV(and B in the case of the Robust B-PCStT). There is a strong correlation between the size of the optimal solution and the corresponding values for B, ΓEand ΓV. (3) Among three possibilities to deal with robustness in a MIP model, the addition of a compact set of constraints right at the beginning of the Branch-and-Bound process, outperforms the remaining two (cutting planes) approaches. This can be explained by the fact that from the beginning of the optimization process the underlying LP contains complete information regarding the robustness of the solution, which allows CPLEX to exploit its powerful preprocessing, heuristics and MIP algorithms, while this is not possible for the cutting plane approaches.As possible directions for future work, it would be interesting to develop algorithms for 2-trees (or, graphs with a bounded tree-width, in general) that improve the trivial running times obtained by running O(∣V∣∣E∣) iterations of the deterministic problem. In addition, a strategy combining the results described in Section 6 and the utilization of further polyhedral techniques might improve the results we obtained in terms of algorithmic performance.