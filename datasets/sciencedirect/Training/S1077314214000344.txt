@&#MAIN-TITLE@&#
Semantic video scene segmentation and transfer

@&#HIGHLIGHTS@&#
New semi-supervised method to semantically segment a scene based on video activity.Learned functional categories are used to segment different scenes (scene transfer).We introduce new trajectory features useful for semantic segmentation.Only a small subset of relevant features leads to high classification accuracy.Proposed method achieves state-of-the-art scene classification and transfer results.

@&#KEYPHRASES@&#
Video analysis,Semi-supervised learning,Scene segmentation,Functional modeling,

@&#ABSTRACT@&#
In this paper we present a new approach to semantically segment a scene based on video activity and to transfer the semantic categories to other, different scenarios. In the proposed approach, a user annotates a few scenes by labeling each area with a functional category such as background, entry/exit, walking path, interest point. For each area, we calculate features derived from object tracks computed in real-time on hours of video. The characteristics of each functional area learned in the labeled training sequences are then used to classify regions in different scenarios. We demonstrate the proposed approach on several hours of three different indoor scenes, where we achieve state-of-the-art classification results.

@&#INTRODUCTION@&#
Semantic scene segmentation is the task of labeling areas of a scene according to their functional use. Modern surveillance and ambient intelligence systems increasingly exploit knowledge about the functional usage of the environment to improve their performance and provide more advanced functionalities. In this work, we present a new semantic scene segmentation solution targeted to a low cost, flexible system used in indoor environments with ceiling-mounted cameras. This scenario imposes a number challenging requirements that drive the main design choices of the proposed approach: limited cost of sensors, processing units and network infrastructure; compliance with privacy regulations; ease of deployment. To satisfy these requirements, the system uses low-cost cameras mounted on the ceiling, facing towards the floor and equipped with a wide angle lens, so that each node can cover as much surface as possible. Besides, no video streaming and centralized storage is allowed for both economic and privacy reasons. This implies that the video analysis algorithm has to work in real-time on a simple (embedded) platform. Finally, to simplify deployment, the system should limit or possibly avoid tuning of parameters for each installation.An overview of the method proposed in this work is shown in Fig. 1. First, we propose to adopt a simple and flexible real-time tracker and a series of post-processing steps to mitigate common tracking errors. The resulting trajectories are used to build features that are employed in a machine learning framework to classify areas into functional categories. A user annotates one or more scenes by labeling each area with a functional category such as background, entry/exit, walking path, interest point. For each area, we compute statistics of features derived from object tracks computed in real-time on hours of video, and select the most discriminative ones. The characteristics of each functional area learned in the labeled training sequences are then used to classify regions into functional categories both in the same scene or in different scenes.One major contribution of this paper is the thorough analysis and selection of a large number of tracking features with different level of complexity and abstraction. We will show how feature selection is crucial to succeed in the scene categorization task. Another important contribution is the introduction of a semi-supervised approach to semantic video scene classification. To the best of our knowledge, such an approach has not been proposed before. We will demonstrate it on several hours of three different indoor scenes, where we achieve state-of-the-art results.The remainder of the paper is organized as follows: in Section 2 we contextualize our proposed approach considering the most recent advancements in the field. In Section 3 we detail the proposed approach; the results and conclusions are presented in Section 4 and Section 5 respectively.

@&#CONCLUSIONS@&#
We have presented a novel approach to semantic scene segmentation, exploiting trajectories computed in real time by a low complexity tracker. The proposed method is capable of dealing with the tracking inaccuracies of the video tracker. The different stages of the proposed methodologies are thoroughly investigated, showing the effect of individual steps. Of particular importance is the analysis of the feature set combination, which demonstrates how the use of only a small subset of features can lead to high accuracy. The quality of the results shows how the possibility of achieving scene transfer is not only attractive, but feasible. Our approach allows to exploit one-time annotations applied to training environments, to learn the required segmentation models, and deploy the system in previously unseen environments. The proposed approach is demonstrated on three different indoor scenarios, where state-of-the-art classification and scene transfer results are achieved. Furthermore, as the method is training based, it would allow to exploit more powerful machine learning methods once the size of the training sets would become sufficiently large. For example, techniques such as AdaBoost or Random Forest, which have been successfully employed in numerous applications in recent years, would be interesting options to investigate in the proposed system.A relevant open question is the influence of the tracking or motion feature extraction systems on the final scene segmentation results. To the best of our knowledge, this topic has not been addressed systematically in literature. We can expect that different motion features, ranging from simple optical flow up to the most advanced object trackers, will provide slightly different scene segmentation results. However, it is difficult to predict what is the combination of motion features and clustering techniques that can provide the best segmentation performances with the least overall complexity. We believe this is an interesting research question that we plan to address in future work.