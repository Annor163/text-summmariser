@&#MAIN-TITLE@&#
Named entity recognition with multiple segment representations

@&#HIGHLIGHTS@&#
Different segmentation representations (SRs) cause little difference in performance.Different SRs result in quite different outputs.Incorporation of different SRs is beneficial to NER task.We proposed a new feature generation method that uses multiple SRs.The proposed method improves the performance and the stability of NER.

@&#KEYPHRASES@&#
Named entity recognition,Machine learning,Conditional random fields,Feature engineering,

@&#ABSTRACT@&#
Named entity recognition (NER) is mostly formalized as a sequence labeling problem in which segments of named entities are represented by label sequences. Although a considerable effort has been made to investigate sophisticated features that encode textual characteristics of named entities (e.g. PEOPLE, LOCATION, etc.), little attention has been paid to segment representations (SRs) for multi-token named entities (e.g. the IOB2 notation). In this paper, we investigate the effects of different SRs on NER tasks, and propose a feature generation method using multiple SRs. The proposed method allows a model to exploit not only highly discriminative features of complex SRs but also robust features of simple SRs against the data sparseness problem. Since it incorporates different SRs as feature functions of Conditional Random Fields (CRFs), we can use the well-established procedure for training. In addition, the tagging speed of a model integrating multiple SRs can be accelerated equivalent to that of a model using only the most complex SR of the integrated model. Experimental results demonstrate that incorporating multiple SRs into a single model improves the performance and the stability of NER. We also provide the detailed analysis of the results.

@&#INTRODUCTION@&#
Named Entity Recognition (NER) aims to identify meaningful segments in input text and categorize them into pre-defined semantic classes such as the names of people, locations and organizations. This is an important task because its performance directly affects the quality of many succeeding natural language processing (NLP) applications such as information extraction, machine translation and question answering. NER has been mostly formalized as a sequence labeling problem that performs the recognition of segments and the classification of their semantic classes simultaneously by assigning a label to each token of an input text.While many researchers have focused on developing features that capture textual cues of named entities, there are only a few studies (Leaman & Gonzalez, 2008; Ratinov & Roth, 2009) that examined the effects of different segment representations (SRs) such as the IOB2 and the IOBES notations. This issue has been extensively discussed for a different NLP task, word segmentation (WS). In this task, complex SRs consisting of four to six segment labels have been proposed based on linguistic intuitions (Xue, 2003) and statistical evidence from corpora (Zhao, Huang, Li, & Lu, 2006) and shown to be more effective than the simple BI SR.1The BI SR identifies characters at the Beginning and Isnide of words.1However, complex SRs are not always beneficial, especially when the size of training data is small, because they can result in undesirably sparse feature space. In NER, the data-sparseness problem is an important issue because only a small portion of training data is named entities. Therefore, the use of a complex SR, which may better explain the characteristics of target segments than a simple SR, may not be much effective or even can bring performance degradation.In this paper, we present a feature generation method that creates an expanded feature space with multiple SRs. The expanded feature space allows a model to exploit highly discriminative features of complex SRs while alleviating the data-sparseness problem by incorporating features of simple SRs. Furthermore, our method incorporates different SRs as feature functions of Conditional Random Fields (CRFs), so we can use the well-established procedure for training. We also show that the tagging speed of a proposed model using multiple SRs can be boosted up as fast as that of the model using only the most complex SR of the proposed model. The proposed method is evaluated on the two NER tasks: the BioCreative 2 gene mention recognition task (Smith, 2008) and the CoNLL 2003 NER shared task (Tjong Kim Sang & De Meulder, 2003). The experimental results demonstrate that the proposed method contributes to the improvement of NER performance.The next section investigates several SRs developed for various NLP tasks, and explains a hierarchical relation among them that is the key concept to our proposed method. In Section 3, we shows the effect of different SRs on NER and analyze the results in two ways. This analysis motivates the necessity of using multiple SRs for NER. Section 4 describes the proposed feature generation method that creates an expanded feature space with multiple SRs. We also show how to speed up the tagging speed of a model using the proposed method. In Section 5, we present the experimental results and the detailed analysis. Finally, Section 6 summarizes the contribution of our research and future work.SRs are necessary for sequence labeling tasks that involve segmentation as a sub-task. This section introduces SRs used in various NLP tasks and presents a hierarchical relation among these SRs that will become the basis of our proposed method.Several SRs have been developed for and adopted to various NLP tasks such as NER (Ratinov & Roth, 2009), WS (Xue, 2003; Zhao et al., 2006) and shallow parsing (SP) (Kudo & Matsumoto, 2001; Tjong Kim Sang & Veenstra, 1999). Table 1presents the definition of some of these SRs. Each SR in the SR type column consists of segment labels in the Segment Labels column. The Examples column presents a few example label sequences of named entities, chunks and words with respect to the target tasks. We would like to note that the O label of the SRs in the NER and the SP tasks denotes a token that does not belong to any target segments. In WS, however, the O label is not necessary because every character of an input sentence is a part of a word.In NER, the IOB2 and the IOBES SRs have been used most frequently. The IOB2 SR distinguishes tokens at the Beginning, the Inside and the Outside of named entities. On the other hand, the IOBES SR identifies tokens at the Beginning, the Inside and the End of multi-token named entities, tokens of Single token named entities and tokens of the Outside of named entities. In SP, the IOB2 and the IOBES SRs work in the same manner as in NER. The IOE2 SR uses the E label to differentiate the end tokens of chunks instead of the B label of the IOB2 SR. The IOB1 and the IOE1 SRs are basically equivalent to the IO SR that uses the I label to denote tokens of chunks and the O label to indicate tokens outside chunks. However, the IO SR cannot distinguish the boundary of two consecutive chunks of a same type. To overcome this problem, the IOB1 SR assigns B* label to the token at the beginning of the second chunk, whereas the IOE1 SR gives the E* label to the token at the end of the first chunk. Lastly, in WS, the BI SR identifies the beginning and the inside of words, the BIS SR deals with single character words separately by assigning the S label to these words and the BIES SR uses the E label for the end characters of words. In addition, the BB2IES assigns the B2 label to the second characters of words consisting of more than two characters, whereas the BB2B3IES gives the B2 and the B3 labels to the second and third characters of words comprised of more than three characters.Table 2shows a sample text annotated with the seven SRs which will be used in this work. In addition to the IOB2 and the IOBES SRs that have been commonly used in NER, we also use the IOE2 SR to investigate whether it is better to distinguish the beginning or the end of named entities. The IO SR is adopted as the simplest SR that actually does not perform any segmentation. Because two named entities are not likely to appear consecutively, we can recognize named entities as a sequence of tokens that have a same label. The BI, the IE and the BIES SRs, to the best of our knowledge, were proposed for WS and have not been used for NER. We apply these SR to NER by regarding the O label as a semantic class and augmenting it with the remaining segment labels. This application is based on the observation that tokens appearing around named entities are not random words. In this example, for instance, the left round bracket appears between the full name of a gene and its abbreviation and the right round bracket occurs after the abbreviated gene name. Therefore, it is worth differentiating these tokens from the others by assigning separate labels.Conceptually, only two segment labels are necessary (e.g. B-gene and I-gene for gene names) to distinguish segment boundaries unambiguously. However, many words tend to appear at specific positions not at random places. For example, the names of location often end with the words such as “Street”, “Road” and “Avenue” and the names of companies are frequently followed by the phrases such as “Corporation” and “Co., Ltd.” Therefore, complex SRs that can capture these characteristics of target segments are able to create a more informative feature space than simple SRs. Xue (2003) articulated that choosing a suitable SR is a task-specific problem that depends on the characteristics of segments and the size of available training data.Segment labels of a complex SR often denote more specific positions than those of a simple SR. While every pair of any SRs can be inter-convertible if enough context information (segment labels of neighboring tokens) is provided, some of them are deterministically mappable by looking at only current labels. For example, to convert the IOBES SR to the IOB2 SR, we can simply map the B and the S labels of the IOBES SR to the B label of the IOB2 SR, the I and the E labels to the I label. Fig. 1shows the hierarchical relation among the seven SRs used in the previous example in Table 2. In this figure, a complex SR can be deterministically mapped to a simple SR if they are connected by directed arrow(s). Table 3shows how to map the segment labels of the BIES SR to those of simpler six SRs.The existing sequence labeling framework using the Viterbi algorithm assumes the Markov property for computational tractability. Therefore, it is impossible to use arbitrary context information for mapping segment labels of one SR to those of another SR. However, we can avoid this problem by considering only a subset of SRs that can be deterministically mapped from one SR to another SR as shown in Fig. 1. For example, when we use the IOBES SR, we can utilize the features created from not only this SR but also the other SRs which can be deterministically mapped from it (e.g. IOB2, IOE2 and IO).To investigate the effects of different SRs on NER, we performed a preliminary experiment on the BioCreative 2 gene mention recognition (BC2GMR) task (Smith, 2008). For the experiment, we trained seven models with seven different SRs (IO, IOB2, IOE2, BI, IE, IOBES and BIES), but with the same textual cues.2These textual cues are often called features. However, we use the term feature to indicate the combination between a textual cue and a label.2Among these SRs, the BI, the IE and the BIES SRs were originally designed for the WS task and do not use the O label. We assumed a sequence of continuous O labeled tokens as a kind of special named entities, namely O-class named entity, and gave them separate O labels to apply these SRs to the NER tasks. For example, the BI SR uses the B-O and I-O labels instead of the O label.For machine learning, we implemented a linear-chain CRFs with the L-BFGS algorithm.3http://www.chokkan.org/software/liblbfgs/.3Lafferty, McCallum, and Pereira (2001). defines a linear chain CRFs as a distribution:p(y|x)=1Z(x)exp∑t=1T∑k=1Kλkfk(yt-1,yt,x)where x=〈x1,x2,…,xT〉 is an input token sequence, y=〈y1,y2,…,yT〉 is an output label sequence for x, Z(x) is a normalization factor over all label sequences, T is the length of the input and output sequences, K is the number of features, fkis a feature and λkis a feature weight for the fk.In a linear-chain CRFs, fkis either a transition feature or a state feature. For example, a transition feature4A transition feature is a combination of previous and current labels. An input token sequence is not used for transition features in the current implementation.4fi, which represents the transition from the B-gene label to the E-gene label of the IOBES SR, can be defined asfi(yt-1,yt,x)=1((yt-1=B-gene)∧(yt=E-gene))0(otherwise)and a state feature5A state feature is a combination of a current label and a textual cue created from a sequence of input tokens within a context window.5fj, which indicates that the current state is E-gene and its corresponding input token is “protein”, can be defined asfj(yt-1,yt,x)=1((yt=E-gene)∧xt=(“protein”))0(otherwise).Training a linear chain CRFs model is equivalent to find a set of feature weights which maximize a model log-likelihood for a given training data. However, it is often necessary to use regularization to avoid overfitting. We use the following model log-likelihood formula (Sutton & McCallum, 2007). The last term is for regularization.l(θ)=∑i=1N∑t=1T∑k=1Kλkfkyt-1(i),yt(i),x(i)-∑i=1NlogZ(x(i))-C∑k=1Kλk2The parameter C determines the strength of regularization and it can be chosen by using development data. A smaller C value will result in a model that fits training data better than a bigger C value, while it is more likely to be overfitting. In the preliminary experiment, we reserved the last 10% of the original training data as the development data for tuning the C value. We examined ten C values6These C values are 2−5, 2−4, 2−3, 2−2, 2−1, 20, 21, 22, 23, and 24.6for each model and used the best performing C value for evaluation on the test data.We used features generated from input tokens, lemmas, POS-tags, chunk-tags and gazetteer matching results. The detailed explanation of the feature set is in Section 5.The seven models are evaluated in standard performance measures: precision, recall and F1-score. As shown in Table 4, precision tends to improve as the number of labels increases. On the other hand, recall does not exhibit such a clear tendency where the IOE2 and IOBES models achieve the higher recall than other models. If we follow the conventional approach, the BIES SR, which has not been used for NER, will be most suitable for this corpus.Although the evaluation in standard performance measures demonstrated that the BIES SR is most suitable for this corpus, we found that the tagging results of these seven models are quite varied. Table 5shows how the tagging results change when the SR alters from the simplest one (IO) to the most complex one (BIES) in terms of true positive (TP), false negative (FN), true negative (TN) and false positive (FP). Since the BIES model clearly outperforms the IO model, we anticipate that the BIES model will produce more correct tagging results. The BIES model actually corrects 372 false negatives and 381 false positives of the IO model. However, surprisingly, it introduces new 254 false negatives and 235 false positives which are non-negligible amount of errors.This analysis suggests that different SRs can produce feature spaces which are complementary to each other; and using multiple SRs is highly likely to improve NER performance. In the following section, we explain how to integrate multiple SRs into a CRF-based NER model.This section presents a feature generation method which incorporates multiple SRs into a single CRF-based NER model. An expanded feature space created with the proposed method allows a model to exploit both high discriminative power of complex SRs and robustness of simple SRs against the data sparseness problem.In Section 4.1, we explain the mapping relation of the SRs, and design four groups of SRs for the proposed method. Section 4.2 describes a modified linear chain CRFs model which can automatically generate and evaluate features of multiple SRs. In Section 4.3, we show that a simple model computation after training makes the tagging speed of a proposed model using multiple SRs as fast as the conventional model using the most complex SR of the proposed model.In Section 2.2, we presented a hierarchical relation among seven SRs that can be deterministically mappable and explained how to exploit multiple SRs without violating the Markov property. We call the most complex SR among all SRs used for a model as a main SR, and the other SRs as additional SRs. A conventional NER model can be interpreted as a model using only a main SR. For the experiment, we selected two most popular SRs, IOB2 and IOBES, and the most complex one, BIES, as the main SRs. As additional SRs, we basically use all deterministically mappable SRs to show the maximum effect of the proposed method. Three groups of SRs are shown in Table 6and their names are marked with ‘+’ symbol. In addition, we trained a model using only the BIES and the IO SRs, which are the most complex and the simplest SRs. This will minimize the increase of the total number of features, while allowing the model exploit complementary feature information of SRs in very different granularities.In Section 3, we briefly introduced a linear chain CRFs. To enable a model to use features generated from multiple SRs, we define a set of feature sets, Γ={Fl}, where Flis a set of features generated from the l SR. Then, we re-define a model asp(y|x)=1Z(x)exp∑t=1T∑Fl∈Γ∑f∈Flλff(yt-1,yt,x)where f is a feature of a feature set Flof the SR l, and λfis a feature weight for the feature f. This modified CRFs model can use features generated from multiple SRs.However, we need to remind that a label sequence y belongs to the main SR. Therefore, it cannot directly evaluate the features of additional SRs. For example, a model, which uses the IOBES as its main SR and the IOB2 as its additional SR, may have a transition featurefi′∈FIOB2as below. (To avoid confusions, we explicitly use the name of the SR as superscript to which a label belongs.)fi′yt-1IOBES,ytIOBES,x=1yt-1IOBES=B-geneIOB2∧ytIOBES=I-geneIOB20(otherwise)This feature cannot be directly evaluated because the input argument labels (yt−1 and yt) are of the main SR (IOBES) while the feature is of an additional SR (IOB2).To solve this problem, we define a label conversion function, gl(y) which converts a label y of the main SR into a label y′ of the SR l. Then the transition feature above can be re-defined asfi′yt-1IOBES,ytIOBES,x=1gIOB2(yt-1IOBES)=B-geneIOB2∧gIOB2ytIOBES=I-geneIOB20(otherwise).The same modification applies to state features. For example, a state featurefj′∈FIOB2can be re-defined asfj′yt-1IOBES,ytIOBES,x=1xt=(“protein”)∧gIOB2(ytIOBES)=I-geneIOB20(otherwise).For gl(y), we use a deterministic conversion function that works as explained in Section 4.1. This mapping function allows us to use well-established algorithms for training a model.A models using the proposed method generates more features and it inevitably slows down training speed. However, we can speed up the tagging speed of this model as fast as the model using only the main SR. The proposed method uses a deterministic label mapping function. It means that we know what kinds of features of additional SRs will be triggered for every feature of the main SR. By calculating the sum of feature weights that always appear together in advance and using it as the new weights for the main SR, the model can work as if it uses only the main SR. The model size and tagging speed will be identical to the model actually trained with the main SR only.

@&#CONCLUSIONS@&#
