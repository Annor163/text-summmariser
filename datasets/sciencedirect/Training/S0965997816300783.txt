@&#MAIN-TITLE@&#
Hybrid parallelization of the total FETI solver

@&#HIGHLIGHTS@&#
Hybrid parallelization of the Finite Element Tearing and Interconnecting method.Performance comparison of the hybrid parallelization to MPI-only parallelization.TFETI implementation for better utilization of the multi-core computer cluster.

@&#KEYPHRASES@&#
ESPRESO,Total FETI,Hybrid parallelization,MPI,Cilk++,

@&#ABSTRACT@&#
This paper describes our new hybrid parallelization of the Finite Element Tearing and Interconnecting (FETI) method for the multi-socket and multi-core computer cluster. This is an essential step in our development of the Hybrid FETI solver were small number of neighboring subdomains is aggregated into clusters and each cluster is processed by a single compute node.In our previous work we have implemented FETI solver using MPI parallelization into our ESPRESO solver. The proposed hybrid implementation provides better utilization of resources of modern HPC machines using advanced shared memory runtime systems such as Cilk++ runtime. Cilk++ is an alternative to OpenMP which is used by ESPRESO for shared memory parallelization.We have compared the performance of the hybrid parallelization to MPI-only parallelization. The results show that we have reduced both solver runtime and memory utilization. This allows a solver to use a larger number of smaller sub-domains and in order to solve larger problems using a limited number of compute nodes. This feature is essential for users with smaller computer clusters.In addition, we have evaluated this approach with large-scale benchmarks of size up to 1.3 billion of unknowns to show that the hybrid parallelization also reduces runtime of the FETI solver for these types of problems.

@&#INTRODUCTION@&#
The goal of this paper is to describe the Hybrid parallelization of FETI method based on our variant of the Finite Element Tearing and Interconnecting (FETI) type domain decomposition method called Total FETI (TFETI) [5]. The original FETI method, also called the FETI-1 method, was originally introduced for the numerical solution of large linear systems arising in linearized engineering problems by Farhat and Roux [1]. In the FETI methods, a body is decomposed into several non-overlapping subdomains and the continuity between the subdomains is enforced by Lagrange multipliers. Using the theory of duality, a smaller and relatively well conditioned dual problem can be derived and efficiently solved by a suitable variant of the conjugate gradient algorithm.The original FETI algorithm, where only the favorable distribution of the spectrum of the dual Schur complement matrix [9] was considered, was efficient only for a small number of subdomains. So it was later extended by introducing a natural coarse problem [10,11], whose solution was implemented by auxiliary projectors so that the resulting algorithm became, in a sense, optimal [10,11]. Even if there are several efficient coarse problem parallelization strategies [12], still there are size limitations of the coarse problem. Thus several hybrid (multilevel) methods were proposed [13,14].The key idea is to aggregate a small number of neighboring subdomains into the clusters, which naturally results in the smaller coarse problem. In our Hybrid Total FETI [15], the aggregation of subdomains into the clusters is enforced again by Lagrange multipliers. Hybrid FETI method uses two level decomposition: the problem is decomposed into clusters (the first level), then the clusters are decomposed into subdomains (the second level).In the TFETI method [5], also the Dirichlet boundary conditions are enforced by Lagrange multipliers. Hence all subdomain stiffness matrices are singular with apriori known kernels which is a great advantage in the numerical solution. With known kernel basis we can effectively regularize the stiffness matrix [6] and use any standard Cholesky type decomposition method for nonsingular matrices.This paper is an extended version of the paper [2] presented at the Fourth International Conference on Parallel, Distributed, Grid and Cloud Computing for Engineering. Further evaluation of the hybrid parallelization in the ESPRESO FETI Solver for large scale problems, up to 1.3 billion of unknowns, is described in Section 4.3. In this section we also identify under what conditions the main bottleneck of the FETI method, the coarse problem, becomes unacceptable. We also present our current effort, which deals with reducing the coarse problem size using the Hybrid FETI method in Section 5.The FETI-1 method [5] is based on the decomposition of the spatial domain into non-overlapping subdomains that are glued by Lagrange multipliers, enforcing arising equality constraints by special projectors. The original FETI-1 method assumes that the boundary subdomains inherit the Dirichlet conditions from the original problem. This means, that subdomains touching Dirichlet boundary are fixed while others are kept floating; in linear algebra speech, corresponding subdomain stiffness matrices are non-singular and singular, respectively. The basic idea of our Total-FETI (TFETI) [5] is to keep all the subdomains floating and enforce the Dirichlet boundary conditions by means of a constraint matrix and Lagrange multipliers, similarly to the gluing conditions along subdomain interfaces. This simplifies the implementation of the stiffness matrix pseudoinverse. The key point is that kernels of subdomain stiffness matrices are known a priori, have the same dimension and can be formed without any computation from mesh data. Furthermore, each local stiffness matrix can be regularized cheaply, and the inverse of the resulting nonsingular matrix is at the same time a pseudoinverse of the original singular one [6–8].Let Np, Nd, Nn, Ncdenote the primal dimension, the dual dimension, the null space dimension and the number of cores available for our computation. Primal dimension means the number of all DOF including those arising from duplication on the interfaces. Dual dimension is the total number of all equality and inequality constraints. Let us consider a partitioning of the global domain Ω into NSsubdomainsΩs,s=1,…,NS(NS≥ Nc). Subdomain stiffness matrixKsand the subdomain nodal load vectorfscorrespond to each subdomain Ωs.Rsshall be a matrix whose the columns of which span the nullspace (kernel) ofKs. LetBsbe a signed boolean matrix defining connectivity of the subdomain s with neighbour subdomains. It also enforces Dirichlet boundary conditions when TFETI is used. They constitute global objects(1)K=diag(K1,⋯,KNS)∈RNp×Np,R=diag(R1,⋯,RNS)∈RNp×Nn,B=[B1,⋯,BNS]∈RNd×Np,f=[(f1)T,⋯,(fNS)T]T∈RNp×1.Note that columns ofRalso span the kernel ofK.Let us apply the duality theory to the primal problem(2)min12uTKu−uTfs.t.Bu=oand let us establish the following notationF=BK†BT,G=−RTBT,d=BK†f,e=−RTf,whereK† denotes a pseudoinverse ofK, satisfyingKK†K=K,andGis a so-called natural coarse space matrix. We obtain a new QP(3)min12λTFλ−λTds.t.Gλ=e.Furthermore, equality constraintsGλ=ecan be homogenized toGμ=oby splittingλintoμ+λ˜whereλ˜satisfiesGλ˜=e. This impliesμ∈ KerG. The vectorλ˜can be chosen as the least square solution of the equality constraints, i.e.λ˜=GT(GGT)−1e. We substituteλ=μ+λ˜,minimize overμ(terms withoutμcan be omitted) and addλ˜toμ.Finally, equality constraintsGμ=ocan be enforced by the orthogonal projectorP=I−Qonto the null space ofG, whereQ=GT(GGT)−1Gis the orthogonal projector onto the image space ofGT(i.e.ImQ=ImGTandImP=KerG). The final problem reads(4)PFμ=Pd˜,whered˜=d−Fλ˜. Note we call the action of(GGT)−1the coarse problem of FETI.Lemma 1The matrixPFis symmetric positive definite on KerG.Reader can find the proof of Lemma 1 in [10]. Thanks to the Lemma 1 the problem (4) may be solved efficiently by the PCG (Preconditioned Conjugate Gradients) algorithm. The conjugate gradient method is a good choice thanks to the classical estimate by Farhat, Mandel, and Roux [10] of the spectral condition number:(5)κ(PFP|ImP)≤CHh.This section briefly summarizes FETI preconditioners introduced in [3]. The extension for cases with discretization heterogenities can be found in [4]. The subdomain’s stiffness matrix K(s) can be divided into four blocks(6)K(s)=[Kii(s)Kib(s)Kbi(s)Kbb(s)],according to indexes i (internal unknowns) and b (boundary unknowns). The Dirichlet preconditioner reads(7)FD−1=∑s=1NsB(s)[000Sbb(s)](B(s))T,whereSbb(s)is the Schur complement of the blockKii(s)with respect to the subdomain stiffness matrixK(s),(8)Sbb(s)=Kbb(s)−(Kib(s))T(Kii(s))−1Kib(s).The Dirichlet preconditioner is numerically scalable. The second ’lumped’ preconditioner lumps the Dirichlet operator on the substucture interface unknowns(9)FL−1=∑s=1NsB(s)[000Kbb(s)](B(s))T.The preconditionerFL−1is not numerically scalable, but it is more economical than the Dirichlet preconditioner. The matrixB(s) may not have a full column rank as redundant constrains are allowed. To enable the use of the Dirichlet or lumped preconditioner, a scaling matrixA(s) has to be introduced. The matrixA(s) is a diagonal square matrix, its size equals the number of dual variables, and its diagonal entries are given by the weight vectorw. After the decomposition, each node on the cut is split into at least two nodes. In such case,w(i)=1/2for each of its associated degrees of freedom i. Generally, if the number of subdomains associated with a node is m, the corresponding entry of the weight vector isw(i)=1/m. The modified forms of the Dirichlet and lumped preconditioners are(10)FD−1=∑s=1NsA(s)B(s)[000Sbb(s)](B(s))TA(s)FL−1=∑s=1NsA(s)B(s)[000Kbb(s)](B(s))TA(s).Obviously, these preconditioners have almost the same computational complexity as the basic ones (7), (9). The modified conjugate gradient algorithm with the preconditioner and projector (PCGP) reads as follows. By the symbollF−1¯we denote any FETI preconditioner. In our case, it is actually substituted byFL−1orFD−1from (10).Algorithm 1Linear solver based on the TFETI methodThe ESPRESO Total FETI solver is implemented in C++. A significant part of the development effort was devoted to writing a C++ wrapper for (1) the selected sparse and dense BLAS routines and (2) the sparse direct solvers (MKL version of PARDISO direct solver) of the Intel MKL library. By a simple modification of this wrapper, we can add support for additional direct solvers as needed.Since the solver development is mainly focused on the current and future multi and many core architectures, in particular the Intel MIC architecture, the Intel MKL library is the only external tool used by the solver. In addition, to be able to port the solver to Intel Xeon Phi (in both native and offload mode) the Intel compiler is used for compilation and Intel MPI is used as message passing library.The hybrid parallelization inside the ESPRESO solver is designed to fit the two-level decomposition used by the Hybrid FETI Method. This method decomposes the problem into clusters (the first level), then the clusters are decomposed into subdomains (the second level). In the case of the Total FETI method the problem is decomposed into subdomains only, but subdomains are processed in groups. In this paper we focus on the Total FETI method only.In our implementation this decomposition is mapped to parallel hardware in a following way. Clusters (for HFETI) or groups of subdomains (for TFETI) are mapped to compute nodes of a supercomputer, therefore a parallelization model for distributed memory is used - in our case the message passing (MPI). Subdomains inside a clusters or groups are mapped to CPU cores of the particular compute node, therefore a shared memory model is used for the second level.Current implementation allows to process multiple clusters/groups by a single compute node, but single cluster/group cannot be processed by more than one node, as this is a general limitation of the shared memory parallelization.There are two major parts of the solver that affects its parallel performance and scalability: (1) communication layer (described in Section 3.1) and (2) the inter-node processing routines for shared memory (described in Section 3.2).The first part deals with the optimization of the communication overhead caused mainly by gluing matrixBmultiplication and application of the projector (includes multiplication with matrixGand the application of the coarse problem). Having a fully optimized communication layer is essential for scalability. The second part is a set of routines developed for efficient parallel processing of multiple subdomains in shared memory.The hybrid parallelization is well suited for multi-socket and multi-core compute nodes. This is a hardware configuration used by most of today’s supercomputers.The first level of parallelization is designed for parallel processing of the groups of sub-domains. Each group is assigned to a single node but if necessary multiple groups can be processed per node. As mentioned earlier multiple nodes cannot work on a single group. The distributed memory parallelization is done using MPI. In particular, we are using MPI standard 3.0 (implemented in the Intel MPI 5.0) because the communication hiding techniques implemented in our FETI communication layer require the non-blocking collective operations.The essential part of this parallelization is the development of efficient communication layer. This layer is identical, whether the Total FETI solver runs single or multiple domains per group. It uses novel communication avoiding and hiding techniques for the main iterative solver. In particular, we have implemented: (1) the Pipelined Conjuagent Gradient (PipeCG) iterative solver - hides communication cost of the global dot products in CG behind the local matrix vector multiplications; (2) the coarse problem solver using distributed inverse matrix - merges two global communication operations (Gather and Scatter) into one (AllGather) and parallelizes the coarse problem processing; and (3) the optimized version of global gluing matrix multiplication (matrixBfor FETI) - implemented as a stencil communication which is fully scalable.The stencil communication for a simple decomposition into four sub-domains is shown in Fig. 1where the Lagrange Multipliers (LMs) that connect different neighboring subdomains are depicted in different colors. In every iteration when LMs are updated an exchange is performed between the neighboring sub-domains to finish the update. This affinity also controls the distribution of the data for the main distributed iterative solver, which in our case iterates over local LMs only. In our implementation each MPI process modifies only those elements of the vectors that match the LMs associated with all domains in its respective group.We call this operation the vector compression. In the pre-processing stage the local part of the gluing matrixBis also compressed using the same approach (in this case it means that all the empty rows are removed from the matrix) so that we can directly use sparse matrix vector multiplication on the compressed vectors.The second level of parallelization is designed for parallel processing of sub-domains in a group. Our implementation enables oversubscription of CPU cores in a way that each core can process multiple sub-domains. This means that the number of subdomains processed per compute node is not limited by its hardware configuration. If the number of sub-domains per group is less than the number of cores, then multiple MPI processes per node must be executed in order to utilize all the CPU cores.The shared memory parallelization is implemented using Intel Cilk++. We have chosen the Cilk++ due to its advanced support for C++ language. In particular, we are taking advantage of the functionality that allows us to create custom parallel reduction operations on top of the C++ objects, which in our case, are sparse matrices.We have evaluated the implementation on the solution of the following 3D linear elasticity problems: (1) Synthetic 3D-cube benchmark and (2) Real-world engine benchmark (both described in the following sections). The performance evaluation was carried out on Anselm supercomputer located at IT4Innovations in the Czech Republic. The machine has the following parameters:•IT4Innovation’s Anselm–up to  3300 coresnon-blocking cluster of 209 nodes each with:*2x 8-core Intel Sandy Bridge E5-2665 (Sandy Bridge), 2.4GHz and 64GB of RAMInfiniBand QDR network - 40 Gbit/s inter-node bandwidthThe first benchmark is a 2.5 million DOF model of a car engine depicted in Fig. 2. Using this benchmark we have evaluated the behavior of the communication layer during a strong scaling test. We have run the benchmark decomposed into 32, 64, 128, 256, 512, and 1024 subdomains in the TFETI mode only on Anselm supercomputer.Fig. 3shows how the two optimization techniques implemented in the communication layer and the application of the simple lumped preconditioner help the scalability and solver performance in terms of single iteration time. Note that all these tests ran with 8 MPI processes per node. As we do not have a preconditioned pipelined CG algorithm implemented yet, we have evaluated two most efficient combinations: (1) regular CG with the lumped preconditioner and (2) pipelined CG without preconditioner, and the effect of using a distributed inverse matrix of the coarse problem. As expected, preconditioned regular CG has slower iterations starting from 32 subdomains but more importantly starting from 256 subdomains GGTINV starts making the difference. This effect becomes dominant for decomposition into 512 subdomains and essential in the case of 1024 subdomains as preconditioned regular CG with GGTINV is faster than the pipelined CG without proconditioning. Reader should note, that in both cases using GGTINV keeps the scaling superlinear up to 1024 subdomains.The advantage of using the lumped preconditioner is shown in Fig. 4where, on average, the number of iterations is reduced by 60% - 70%. When these numbers are combined with the per iteration time, we get the entire solution time, shown in Fig. 5. In this figure the significant iteration reduction achieved using the lumped preconditioner is seen from very beginning but gets eliminated by iteration time for decompositions into 512 and 1024 subdomains where using GGTINV becomes the most significant aspect of the entire solution time. Again, the most important information is that we are able to achieve the superlinear scaling for the entire solution up to 1024 subdomain problem decomposition using simple preconditioner.The second benchmark is a linear elasticity problem in a three-dimensional domain. The domain depicted in Fig. 6has a shape of a cube with the length of the edge 1m. We considered fixed steel box deformed only by its own weight. We prescribe Dirichlet boundary conditionux=uy=uz=0on the left wall. All other walls are free. The material constants are defined by the Young modulusE=2.1×105[MPa], the Poisson ratioν=0.3.This test is focused on solving the largest possible problem using limited hardware resources. The following measurements were performed on 8 nodes of the Anselm cluster, each with 94 GB of RAM. The evaluation criteria are: (1) memory usage and (2) overall processing time including preprocessing and solver runtime.Limited memory size dictates the maximum sub-domain size, as large domains produce large L and U factors. Therefore the sub-domain sizes must remain reasonable.Another limiting factor is the amount of MPI processes that can run on a certain number of nodes and CPU cores. 8 compute nodes, each with 16 cores, can run up to 128 MPI processes without over subscribing the CPU cores with multiple MPI processes, which is not recommended.The solution to the limiting number of concurrently running MPI processes is the proposed hybrid parallelization. This allows the solver to run a single MPI process per node and to use Cilk++ runtime for shared memory parallelization. Cilk++ runtime will execute sub-domain processing routines in a way that all CPU cores will be utilized but will not be forced to run multiple routines at the same time, which leads to unnecessary context switching.The results of these tests are shown in Fig. 7. We have run the experiments for 2 problem sizes 24,361,803 DOF and 41,992,563 DOF both executed on the identical hardware (8 nodes of Anselm supercomputer). For each problem size, three scenarios are evaluated:•MPI only parallelization with large subdomains (first column in Fig. 7)–1 subdomain per MPI processnumber of MPI processes identical to total number of CPU coressubdomain size as required by the problem sizeoptimal hybrid parallelization (second column in Fig. 7)–domain size less than 30,000 DOF (to have L and U factors of reasonable size)1 MPI process per nodenumber of subdomains per MPI process as required by the problem sizeMPI only parallelization with oversubsription of CPU cores by MPI processes (third column in Fig. 7)–domain size less than 30,000 DOF1 subdomain per MPI processnumber of MPI processes as required by the problem sizeIn terms of the preprocessing time, the most efficient configuration is the one with a small number of large subdomains. But the factorization time, which is significantly higher in this case, increases the overall processing time and makes this method the least efficient. In case of the two configurations with smaller subdomains, the solver runtime is similar and the main difference is in the preprocessing time. This is significantly lower in the case of hybrid parallelization and makes it the most efficient configuration in terms of the processing time.Another key factor is the memory utilization. As expected, the configuration using large subdomains has the highest memory utilization due to the size of the factors. Both remaining configurations have similar utilization during the solver runtime, but there is a peak utilization during the construction of the coarse problem and calculation of its distributed inverse matrix (GGtINV). The coarse problem is constructed on all MPI processes and therefore, if a high number of MPI processes are running per node, the peak utilization is very high and the solver runs out of memory. This problem arose for the larger problem size and configuration with 1728 groups/MPI processes (216 MPI processes per node). Therefore, in this case, we have disabled the use of GGtINV which allows the solver to create the coarse problem only on the root MPI process. Please note that this slows down the solver performance.This problem is eliminated by the hybrid parallelization, as it allows us to run single MPI process per node, and therefore only one coarse problem is constructed per node.In this section, an evaluation of the hybrid parallelization in the ESPRESO FETI Solver for large problems is presented. All tests are performed on the 3D cube benchmark described in Section 4.2. The following three configurations are evaluated:•MPI only parallelization with large subdomains (Fig. 8)–1 subdomain per MPI process; 24 MPI processes per node - equal to the number of CPU coressubdomain size 177,957 DOFnumber of compute nodes: 2, 9, 31, 56, 115, 205 and 348 nodescoarse problem size - up to 50,112 for 348 compute nodesHybrid parallelization with medium domains (Fig. 9)–64 subdomains per node; 1 MPI process per nodedomain size less than 46,875 DOFnumber of compute nodes: 1, 8, 27, 64, 125, 216 and 343 nodescoarse problem size - up to 131,172 for 348 nodesHybrid parallelization with medium domains (Fig. 10)–729 subdomains per node; 1 MPI process per nodedomain size less than 6591 DOFnumber of compute nodes: 1, 8, 27. For 64 nodes the coarse problem processing ran out of memorycoarse problem size - up to 279,936 for 54 nodesIn the case of the FETI solver, decreasing the domain size reduces the factorization time of the stiffness matrices (in the figures “K regularization and factorization”). For the three presented cases, the time spent by factorization is reduced from  33 seconds (177,957 DOF subdomains) to  16 seconds (46,875 DOF subdomains) and ultimately to  5 seconds (6,591 DOF subdomains). Please note that in all cases the identical hardware configuration works with the problem of the identical size, only decomposition scheme (number and size of subdomains) is different.On the other hand, increasing the number of subdomains, the coarse problem increases as well which leads to exponential growth of the coarse problem processing time (in figures this is represented with the light blue bar as “Setup FETI solve - preprocessing”). This behavior can be observed in all experiments (Figs. 8, 9 and 10, but it is the most obvious in Fig. 10. Here the 27 nodes were assembling and processing the coarse problem of the size 118,098 for  57 seconds. The large problem on 64 nodes and with coarse problem of the size 279,939 was memory demanding and therefore the solver ran out of memory. In Fig. 9 the coarse problem of the size 131,712 unknowns is processed in  24 seconds due to our parallel algorithm which assembles the GGTmatrix in distribute fashion. But since the factorization of entire GGTis done on each node, its time is not reduced.From the overall solver runtime for 1.3 billion DOF test, it can be seen that the decomposition into smaller domains is more efficient in multiple aspects. This is possible only thanks to hybrid parallelization. As the smaller domains have better condition number, the number of iterations is also smaller. In addition, the single iterations time is shorter for smaller domains, therefore the FETI solver itself is more efficient. In terms of the preprocessing, the amount of time saved by the factorization is significant. The only slower part is the coarse problem preprocessing, but only by  5 seconds. To sum up, the solver runtime is reduced from  85 to  68 seconds.It is our intention to further reduce the preprocessing time and to avoid the main bottleneck of FETI, which is the coarse problem. The first result of this effort is shown in Fig. 11. Here it can be observed that we managed to flatten the FETI preprocessing time (light blue bar). The penalty is in the form of the Hybrid FETI preprocessing. This, however, is an embarrassingly parallel operation, the processing time of which remains constant for growing number of compute nodes used for processing. A short description of this approach is in the following section.Introduced implementation of ESPRESO parallel solver demonstrates the robustness of the FETI algorithm. Although FETI itself enables large scale problems with hundreds of millions of unknowns to be solved, it has some limitations. It can be the size of the coarse problem GGT. In linear elasticity it is equal to all of independent rigid body modes (RBM) of all subdomains (one subdomain contributes with 6 RBM). Due to factorization or orthogonalization process its size is limited according to the computer architecture. In [15], the Hybrid Total FETI method allowing the control of the size of the coarse problem GGTby clusters is introduced. The main idea is based on the creation of clusters. The number of clusters can be established in advance according to the specific requirements. Then each cluster can be decomposed into smaller subdomains, which are glued together on ’corner’ nodes via special and ’small’ set of Lagrange multipliersλ0 calculated exactly in each iteration. Practically it means that together with primal variables u also the setλ0 is eliminated, therefore one cluster will contribute to the whole size of GGTonly with 6 RBM, just like one subdomain in the common FETI method. The implementation of Hybrid FETI method to ESPRESO library is in progress.

@&#CONCLUSIONS@&#
