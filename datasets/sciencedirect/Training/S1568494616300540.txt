@&#MAIN-TITLE@&#
Differential evolution with guiding archive for global numerical optimization

@&#HIGHLIGHTS@&#
This study presents a DE framework with guiding archive to help DE escape from the situation of stagnation.The proposed framework is general and can be applied to most DEs.The proposed framework is applied to six original DE algorithms, as well as two advanced DE variants.The proposed framework is evaluated experimentally on 28 benchmark functions and 8 real-world application problems.

@&#KEYPHRASES@&#
Differential evolution,Stagnation,Guiding archive,Global numerical optimization,

@&#ABSTRACT@&#
Differential evolution (DE) is a simple, yet efficient, population-based global evolutionary algorithm. DE may suffer from stagnation. This study presents a DE framework with guiding archive (GAR-DE) to help DE escape from the situation of stagnation. The proposed framework constructs a guiding archive and executes stagnation detection at each iteration. Guiding archive is composed of a certain number of relatively high-quality solutions. These solutions are collected in terms of fitness as well as diversity. If a stagnated individual is detected, the proposed framework selects a solution from guiding archive to replace the base vector in mutation operator. In this way, more promising solutions are provided to guide the evolution and effectively help DE escape from the situation of stagnation. The proposed framework is applied to six original DE algorithms, as well as two advanced DE variants. Experimental results on 28 benchmark functions and 8 real-world application problems show that the proposed framework can enhance the performance of most DE algorithms studied.

@&#INTRODUCTION@&#
Differential evolution (DE) is a simple, yet efficient and population-based global evolutionary algorithm (EA) [1,2]. Recently, DE has drawn the attention of many researchers and has been successfully applied to diverse fields [3,4].DE, however, may gradually stop generating better solutions even though the population has not converged to a fixed point. This situation is commonly referred to as stagnation [5,6]. The salient feature of DE lies in its mutation mechanism that distinguishes it from other EAs. In mutation operators of DE, base vectors can be treated as some leading individuals to explore the search space. The mutation operators can be considered to be searching at the neighborhood of these base vectors. It is observed, on the one hand, base vectors in most of DE are selected randomly, which does not fully utilize the fitness information. Thus, all vectors are equally likely to be selected as base vectors without selective pressure at all [3]. On the other hand, the diversity information is always ignored. Some base vectors may belong to the same region, resulting in a repeated search in this region. Hence, the fitness and diversity information of the population is not fully exploited in the design of DE.This study proposes an alternative to the uniform random selection of base vectors during mutation operator in DE. A guiding archive is maintained and stagnation detection is introduced during the evolution process. The solutions with good fitness and diversity can be stored in guiding archive. When a stagnated individual is detected, a solution in guiding archive is selected to replace the base vector during mutation. In this way, more promising solutions are provided to guide the evolution and effectively help DE escape from the situation of stagnation. DE with guiding archive is named as GAR-DE framework. In order to evaluate the effectiveness of GAR-DE, the proposed framework is applied to original DE algorithms, as well as two advanced DE variants. Experimental results on 28 benchmark functions in CEC 2013 and 8 real-world application problems show that GAR-DE is able to enhance the performance of most DEs.The remainder of this paper is organized as follows. Section 2 briefly introduces DE and reviews the related works. Section 3 presents GAR-DE framework. In Section 4, experimental results are reported. Finally, this paper concludes in Section 5.For a single-objective optimization problem(1)Minimizef(X),where X=(x1, x2, …, xd, …, xD) ∈RDis a vector in the D-dimensional decision (variable) space (solution space), and the feasible solution space is xd∈[Ld, Ud], where Ldand Udrepresent the lower and upper bound of parameter of the dth dimension, respectively. DE evolves a population of NP candidate individuals (solutions) [1]. Each individual i is denoted as Xi,G=[xi,1,G, xi,2,G, …, xi,D,G], where i=1, 2, …, NP; NP is the population size, and G is the current generation. It has three main operators: mutation, crossover and selection. Fig. 1(a) shows the flowchart of original DE including three main operators.This operator generates a mutant vector Vi,Gwith respect to each individual Xi,G(named as target vector). The various mutation strategies is generally described as DE/x/y/z, where DE stands for differential evolution, x represents a string denoting the base vector to be perturbed. For example, x is rand means a randomly chosen based vector; best means the individual with the best fitness in the current population; y is the number of difference vectors considered for perturbation of x, and z stands for the type of crossover being used (exp: exponential; bin: binomial). Six frequently used mutation strategies are [1,3,4]:(1) DE/rand/1(2)Vi,G=Xi1,G+F·(Xi2,G−Xi3,G).(2) DE/rand/2(3)Vi,G=Xi1,G+F·(Xi2,G−Xi3,G)+F·(Xi4,G−Xi5,G).(3) DE/best/1(4)Vi,G=Xbest,G+F·(Xi1,G−Xi2,G).(4) DE/best/2(5)Vi,G=Xbest,G+F·(Xi1,G−Xi2,G)+F·(Xi3,G−Xi4,G).(5) DE/current-to-best/1(6)Vi,G=Xi,G+F·(Xbest,G−Xi,G)+F·(Xi1,G−Xi2,G).(6) DE/rand-to-best/1(7)Vi,G=Xi,G+F·(Xbest,G−Xi1,G)+F·(Xi2,G−Xi3,G).The indices i1, i2, i3, i4 and i5 are mutually different random indices chosen from the set {1, 2, …, NP}\{i}. Xbest,Gis the best vector with the best fitness (i.e., lowest objective function value for a minimization problem) in the population at generation G.In the mutation DE/rand/1 defined by Eq. (2), the first-term at the right hand of the equation,Xi1,G, is called base vector, and(Xi2,G−Xi3,G)is called difference vector. F∈(0, 1) is a positive scaling factor. The process is illustrated on a 2-D parameter space (showing constant cost contours of an arbitrary objective function) in Fig. 2, where the scaled difference vector is added to the based vectorXi1,G, hence the mutant vector Vi,Gis obtained.From the different mutation strategies (Eqs. (2)–(7)), we can find that there are two types of mutation operators in terms of number of difference vector involved: DE/rand/1 and DE/best/1 include one difference vector, while the remaining mutation operators include two difference vectors. Further, there are diverse methods for the selection of parents (i.e., individuals in base and difference vectors). In general, we can distinguish between mutation operators that promote exploration (called explorative strategies) and operators that promote exploitation (called exploitative strategies). Operators that incorporate the best individual (see DE/best/1, DE/best/2, DE/current-to-best/1, DE/rand-to-best/1) favor exploitation since the mutant individuals are attracted around the current best individual. DE/rand-to-best/1 has a stronger exploration capability by introducing more perturbation with the random individual. In contrast, operators that incorporate randomly selected individuals (see DE/rand/1, DE/rand/2) enhance the exploration ability since a high degree of random variability is introduced.This operator is applied to each pair of target vector Xi,d,Gand mutant vector Vi,d,Gto generate a trial vector Ui,G=[ui,1,G, ui,2,G, …, ui,D,G]. There are two basic crossover operators, binomial and exponential crossover. The most widely used is the binomial crossover as follows:(8)ui,d,G=vi,d,G,ifrandd(0,1)≤CRord=drandxi,d,G,otherwise,where randd(0, 1) is a uniform random number in (0,1) for dth dimension, CR∈(0, 1) is a predefined crossover control parameter, and drand∈(1, D) is an integer randomly chosen from 1 to D.This operator uses a one-to-one greedy scheme to select the better one between target vector Xi,Gand trial vector Ui,Gto survive in the next generation as follows:(9)Xi,G+1=Ui,G,iff(Ui,G)<f(Xi,G)Xi,G,otherwise.The selection operator in Eq. (9) is called a successful update if the trial vector Ui,Gis better than the parent Xi,G. The corresponding solution is called successful solution.

@&#CONCLUSIONS@&#
In this study, a new DE framework called GAR-DE has been presented. GAR-DE introduces stagnation detection and a guiding archive. The solutions with better fitness and diversity are stored in guiding archive. The base vector in mutation operator is selected from guiding archive when a stagnation individual is detected. The proposed framework provides more promising solutions to guide evolution and effectively helps DE escape from the situation of stagnation. The proposed framework has been applied to six original DE algorithms, as well as two advanced DE variants. Experimental results on benchmark functions and real-world application problems show that the proposed framework can effectively enhance the performance of most of the DE algorithms studied.As a new DE framework, there are still many interesting issues for further studying in GAR-DE. Firstly, the performance of GAR-DE can be further improved by introducing some methods to avoid premature convergence. Secondly, the way of using archive can be further improved. Finally, GAR-DE should be further tested on the learning based problems of CEC2015 [60] proposed recently.