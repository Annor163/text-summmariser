@&#MAIN-TITLE@&#
RSILC: Rotation- and Scale-Invariant, Line-based Color-aware descriptor

@&#HIGHLIGHTS@&#
A new rotation- and scale-invariant line-based color-aware descriptor is introduced.The descriptor captures both local (texture, color) and global (inter-line spatial) information.Experiments show that proposed descriptor is robust to rotation, scale, and illumination.The descriptor is compared to the well-known descriptors.The proposed descriptor is more accurate on matching line-rich objects such as faces.

@&#KEYPHRASES@&#
Image descriptor,Local features,Spatial features,Rotation invariance,Scale invariance,Color aware,

@&#ABSTRACT@&#
Modern appearance-based object recognition systems typically involve feature/descriptor extraction and matching stages. The extracted descriptors are expected to be robust to illumination changes and to reasonable (rigid or affine) image/object transformations. Some descriptors work well for general object matching, but gray-scale key-point-based methods may be suboptimal for matching line-rich color scenes/objects such as cars, buildings, and faces. We present a Rotation- and Scale-Invariant, Line-based Color-aware descriptor (RSILC), which allows matching of objects/scenes in terms of their key-lines, line-region properties, and line spatial arrangements. An important special application is face matching, since face characteristics are best captured by lines/curves. We tested RSILC performance on publicly available datasets and compared it with other descriptors. Our experiments show that RSILC is more accurate in line-rich object description than other well-known generic object descriptors.

@&#INTRODUCTION@&#
Feature matching is an essential component of many modern computer vision applications, including near-duplicate detection [1], stereo correspondence [2], 3D modeling [3], image stitching [4], as well as face alignment and matching [5–7]. Scene and object matching methods in digital images can be roughly divided in the following major groups by the density of the image features they extract and use:densedescriptor methods [8–10] tend to use all image information and often assume that all pixels in the image are equally important. Hence, they may be computationally expensive and require a high degree of correlation between the probe and gallery images. Typically, such methods are not very accurate given large variations in object pose, scale, and illumination.descriptor methods use non-dense image features (e.g., edges [11]) and/or various key-spots [12–14]. They are relatively robust to variations in pose, size, orientation, and lighting of the query image with respect to the objects in the gallery. They provide a sparse representation for objects and high-speed matching on the key locations that need to be automatically determined, which calls for some sort of a saliency map [15].This implicit methodology division prompted some well-performing hybrid techniques that include features from both categories and typically fuse them in a weighted features ensemble [16–18], optimized for a particular application [19].In content-based image retrieval, object detection, and face recognition (FR) in an unconstrained environment (typically outside of the studio), sparse descriptors are generally preferred because they are often more robust to deformations and lighting variations than dense feature methods. Key-spot-based matching involves detecting the key-spots, building local descriptors for each key-spot, and finding an aggregate distance of best matches. For pose- and lighting-robust matching, the descriptors should be robust to geometrical variations such as translation, rotation, scaling (and if possible to affine/projective transformations and photometric variations such as illumination direction, intensity, colors, and highlights. The selection of a robust key-spot (e.g., a point, a line, or a corner) depends on the image collection and the application.In generic object matching, the coarse features from key-points may be adequate to find suitable matches. However, in line-rich scenes, some dominant lines on objects may provide more stable and discriminative features than key-points. Such line-rich scenes and objects are omnipresent and can be natural (e.g., landscapes, plants, animals, humans) or artificial (e.g., cities, cars, house exteriors and interiors, office spaces). Stable (but not necessarily rigid) characteristic lines in them can be used as good key-spot candidates, promising a more stable object matching ability than key-points can.Human face matching/recognition (FM/FR) is an important special case of object matching that has been an active research and development area in academia and industry because of the wide variety of real-world applications, such as surveillance, visual authentication, human–machine interface, criminal identification, and commercial applications. It identifies individuals from face images or video sequences using computer vision and machine learning algorithms. The general procedure for the appearance-based face image retrieval (FIR) systems consists of detecting the faces, extracting the facial information, and comparing a query face descriptor with those in a database [7,20–24].To deal with the variation in face appearance (e.g., unknown head poses, unexpected facial expressions, and unpredictable lighting), the extracted features are expected to be robust to illumination changes, distortion, and scaling. One can certainly use key-point-based descriptors (e.g., SIFT [12], SURF [13], ORB [14], but because faces are line-rich objects, it may be beneficial to introduce the notion of key-lines and their descriptors for better matching.According to psychophysics and neuroscience studies, line-rich features, such as face outline, eyes, mouth, and hair, are most important for perceiving and remembering faces [25] by humans. Another study has investigated the importance of facial features for automatic face recognition [26] by extracting facial landmarks. Experimental results indicate that these facial features are indeed important for face identification. Several other studies [6,11,27] showed that the most informative face characteristics appear to come from the face lines that can model face features in a very intuitive, human-perceptible form.Consider the face images shown in Fig. 1.a–b. The prominent characteristic parts are marked by lines/curves, whose local regions and their spatial arrangements on a face can be used for robust matching. The human-perceptible important face lines overlap very well with the machine-computed edge maps of the faces on the CalTech set [28], whose cumulative distribution is also shown. The lines are mostly located on the prominent face characteristics (landmarks such as eye, mouth, nose, and face shape), which are the discriminative locations of a face (Fig. 1.c.). All these studies and illustrations indicate that lines with their descriptors can provide more stable recognition features for face matching.We propose a general-purpose key-line descriptor that is color aware, invariant to rotation/scale, and is robust to illumination changes. To increase the discriminative power of the descriptor, we combined color/texture information of the local regions and added the relational information of the other key-lines. We tested our descriptor matching power on publicly available datasets containing unconstrained images of faces and general objects. We compared the new descriptor to well-known descriptors (in the same test-bed system), and our experimental results show that the RSILC descriptor is robust to rotation, scale, reflection and illumination and produces more accurate matches in face and object retrieval applications.Many different techniques for modeling local image regions have been developed. Scale-Invariant Feature Transform (SIFT) [12,29] is one of the most robust key-point descriptors among the local feature descriptors with respect to different geometrical changes [30]. It detects notable and stable key-points for images at different resolutions and produces scale- and rotation-invariant descriptors for robust matching. Several papers have been published on SIFT-based face recognition [31–33]. Although SIFT originally was designed for gray-scale images, there are several extensions to make use of the color information in the descriptors [34–36]. One of the successful attempts is colored SIFT (CSIFT) [36] which embeds the color information through the gradient of color invariants instead of using gray-scale gradients as in conventional SIFT. Defining the descriptor in color space makes the descriptor more robust with respect to color variations.Another well-known key-point descriptor is Speeded Up Robust Features (SURF), which provides a quicker way to detect key-points and compute descriptors that are rotation and scale invariant as well as robust to illumination changes [37]. SURF is less accurate than SIFT, but it has been successfully used in many practical applications, including face/components matching [38,39].The mentioned key-point descriptors (e.g. SIFT and SURF), being robust to various affine transformations and lighting, are widely used for object detection and recognition. However, they typically contain information that is local to their key-points, which prompts some false-positive correspondences when performing many-to-many matches. This problem could be remedied by considering key-point spatial relationships, (e.g., having each descriptor record other key-points' azimuth angles much like shape context [40]), hence capturing not only local context of each key-spot but also their global spatial relationships. Knowing the usefulness of spatial relations in image understanding [41,42], several state-of-the-art methods have been reported together with the use of application-dependent local features. For example, the authors in [43,44] integrate spatial distribution of key-points by using shape context with texture features for food classification. In a similar fashion, spatial relations between the visual primitives (such as circles and corners) are integrated with statistical shape features for graphics recognition [45].The Pyramid of Histograms of Orientation Gradients (PHOG) descriptor represents an image by its local shape and the spatial layout of shape information [46]. The local shape is modeled by a histogram of edge orientations. The spatial layout is represented by tiling the image into the regions at multiple resolutions. The final descriptor vector is the concatenation of histograms at each resolution. The descriptor is robust to scaling as long as the object position and orientation remain the same, but it is rotation dependent and color-blind.As an alternative to key-points, another important set of features for object matching can be collected from edges, which provide the advantages of a lesser demand on the storage space and a lower sensitivity to illumination changes. Gao and Leung [11] describe a face recognition method using line edge maps (LEMs). The system extracts the lines from the edge map of face images and compares their similarity using the Hausdorff distance. LEM produces fast and reliable matching on aligned faces but does not use the region around the lines, which contains intensity information that helps to discriminate the lines and reduce mismatches. Gao and Qi [47] extend the LEM approach by considering corner points along the edge lines and show their method's robustness to scale as well as the superior one-image-per-person retrieval capability compared to the eigenfaces [48]. Deboeverie et al. [6] combined the curve edge map with the relative positions and intensity information around the curves. This system uses the orientation of the main axis of the curve segments for the first match. Then, it considers the histograms of inner and outer sides of the curve and relative positions of curve segments to refine the matching stage. However, this method lacks color information for the local regions.Liu et al. [16] propose SIFT flow to align an image to its nearest neighbors in a photo gallery. This hybrid method matches densely sampled, pixel-wise SIFT features between two images while preserving (sparse) spatial discontinuities, matching a query object located at different parts of the scene. Experiments show that the proposed approach robustly aligns complex scene pairs containing significant spatial differences. The applications include single image motion field prediction/synthesis via object transfer, satellite image registration, and face recognition.Motivated by the discriminative ability of (sparse) shape information and (dense) local patterns in object recognition, Nguyen et al. [17] propose a window-based (hybrid) object descriptor that integrates both cues: contour templates representing object shape are used to derive a set of key-points at which local appearance features are extracted. The key-points are located via template matching that uses both spatial and orientation information. An object descriptor is formed by concatenating the non-redundant local binary pattern (NR-LBP) features from all key-points to encode the shape as well as the appearance of the object. The experimental results suggest that the proposed descriptor can effectively describe non-rigid articulated objects and improve the detection rate compared to other state-of-the-art object descriptors.Satpathy et al. [18] propose two sets of edge-texture features for object recognition: Discriminative Robust Local Binary Pattern (DRLBP) and Ternary Pattern (DRLTP). These hybrid features fuse edge (sparse) and texture (dense) information in a single representation, and they appear to be robust to image variations caused by intensity inversion and discriminative to the image structures within the histogram block, as demonstrated by experiments on seven datasets.Some recent papers present an interesting approach to line-rich scene matching [49,50]. This approach works with gray-scale images and uses rotation- and scale-invariant key-lines and log-polar descriptors. The authors described their promising methodology, giving a few examples with line-rich scenes but showing no large-scale experiments. The approach is quite similar to ours. Therefore, we highlight the major differences, as RSILC: 1) uses color information in the chroma bands of YCbCrcolor space, 2) uses local gradients histogram from the equalized intensity band, and 3) uses spatial information across all key-lines to make descriptor matching more accurate.Computing our key-line descriptor involves: 1) extracting key-lines by applying oriented line filters to the image edge maps, and 2) sampling the line direction oriented circular histograms around each key-line, including both local (texture, color) and global (inter-line spatial) information.To extract key-lines from the studied image, we use convolution of line filters with an edge image, as reported in [49,50]. Formally, the whole process is composed of constructing line filters in different orientations and their convolution with an edge image. A set of line filters in a normal Gaussian 1D distribution are defined, which are oriented in different directions in the range [0, π). In generic form, we can express such a set asF=Fθ,θk=kπ6andk=0,1,…,5, where θkrepresents an orientation value associated with a specific filter F. In Fig. 2, line filters that are oriented in six different directions are shown. These line filters Fθare then convolved with an edge image IEobtained via Canny edge detector [51]. The convolution results in a set of key-lines from each line filter. Considering a setFof line filters, the complete setLof extracted key-lines can be expressed as(1)L=Fθ*IE∀θ∈[0,π)=<lineθ,l>l=1,…,L,where each extracted key-line carries its orientation value θ (that is associated with its line filter) and the total number of key-lines L that varies from one image to another. Note that small lines can be considered as noise and omitted. Fig. 3shows key-lines that are extracted from a face image.In this section, we describe how key-line features are computed to include both local and relational features. Local features are computed independently while relational features take other key-lines into account. We model each key-line with a circle of radius r. Fig. 4shows the circles corresponding to key-lines from face images with different poses. Because the size of the radius uses the length of the key-line, circle sizes vary. Given a complete circle, we sub-divide it into equal sectorssi∈S. For each sector, local and relational features are computed and concatenated to build a key-line descriptor.Each key-line is represented by gradient and color features that characterize the local texture and intensity distribution in every sector of the associated circle. For intensity information, YCbCrcolor space is used, which is robust to illumination changes and perceptually uniform compared to other color spaces [52]. The normalized Y-band is used to compute the gradient direction and gradient magnitude at each pixel. The color information is calculated from each color band (CB) separately. For each sector si, feature histograms are computed as follows,(2)hsi∇I=hist|∇I|sip,∀p∈si,hsiψ∇I=histψ∇Isip,∀p∈si,hsiCB=histIp,∀p∈si,where hist(★) is the histogram function and populates the values ★ into bins;∇Isip=∂Ip∂x2+∂Ip∂y2is the gradient magnitude;ψ∇Isip=atan∂Ip∂y/∂Ip∂xis the gradient direction;∂Ip∂★is the gradient component in ★ direction; and p={x,y} is the image pixel. To maintain the rotation invariance ofhsiψ∇I, the gradient angles are normalized with respect to the key-line orientation before the histogram computation. With the three separate histograms for color bands viz. Y, Cb, and Cr, the complete feature histogram in any particular sector can be expressed by concatenating them,Hlocsi=hsi∇Ihsiψ∇IhsiYhsiCbhsiCr. Considering all sectors in the specified circle for any key-line l, the complete local feature histogram can be expressed by concatenating them,(3)Flocl=Hlocs1Hlocs2…HlocsN.Such a concatenation of local feature histograms is more discriminant than the feature histogram computed from the whole circle at once [49,50].To satisfy the rotation-invariant property, we follow counterclockwise histogram concatenation, which starts from the reference key-line orientation defined in Eq. (1). Fig. 5shows a graphical illustration of rotation-invariance property of the descriptor. In this example, we have eight sectors where concatenation starts from the orientation angle of that particular reference key-line. Starting from the orientation angle of the key-line provides dynamic change in the sector indexing. As a consequence, the feature vector after concatenation remains the same for all key-lines in Fig. 5.a. In Section 3.3, real-world examples are considered to confirm the rotation-invariant property.Alone, local features for each key-line do not offer information about spatial organization of the remaining key-lines. This may cause some false matches. As an example, a key-line associated with the right eye of the query image could be matched with the key-line associated with the left eye of the database image. Therefore, relational information is incorporated to the local feature histograms in each sector. Consider a sector s of a reference key-line l. We define two features to compute the relationships between the key-lines: orientation angle differences between l and the other key-lines l′∈si, and the presence of key-lines in that sector siwith respect to l.The orientation angle difference Δθ between l and l′ can be computed as Δθ=|θ−θ′|∀l′∈s. Based on this, a histogram can be computed as(4)hsiΔθ=histθ−θ′).where hist(★) is the histogram function and populates the values ★ into bins; the number of bins is defined askπ6and k=[0,…,5].Additionally, the presence of key-lines is computed by taking the proportion of key-lines l′ that are visible from l in si. To make the descriptor more discriminant, we further sub-divide siinto annular sectors a∈siand compute the histogram,(5)hsi#l′=hist#l′)∀l′∈si,where hist(★) is the histogram function and populates the values ★ into bins; the number of bins is {aj}j=1,…,4 and the size of the annular sector depends on the radius r of the circle belonging to the reference key-line.In both relational features, histograms are normalized by using the total number of key-lines that appear in that sector. To graphically illustrate the relational feature histograms, we refer to Fig. 6. In one particular sector si, both histograms are concatenated to build the relational histogramHrelsi=hsiΔθhsi#l′. Considering all sectors for a single key-line l, the complete relational feature can be computed by concatenating all histograms,(6)Frell=Hrels1Hrels2…HrelsN.Like the local histograms, to satisfy the property of rotation invariance, concatenation follows exactly similar sector indexing.RSILC descriptor is designed to be rotation and scale invariant. To show these properties, we take example images as shown in Fig. 7. Example key-line locations are shown on straight, rotated, and re-scaled faces. For illustration purposes, we use a single key-line to build an RSILC descriptor (cf. Section 3.2). Histograms (both local and relational) from all sectors inside the circle are concatenated in a counterclockwise fashion, where the starting point is indexed by the reference key-line orientation angle. To maintain the scale invariance property, the reference key-line is modeled with a circle (cf. Fig. 5). The radius of circle is computed by using key-line length that varies with image size. Fig. 8shows the corresponding 1D feature vector of example images. The overlapping of feature vectors (histogram distributions) from the mouth regions of up-right and rotated faces as well as from the eye regions of up-right and scaled faces show that RSILC is rotation and scale invariant.Following our feature description in Sections 3.2.1 and 3.2.2, a line-rich image I can be described with a descriptor of line features(7)dsc=Fll=1,…,Lwhere Fl=[Flocl,Frell], as in Eqs. (3) and (6), and L is the number of key-lines in the descriptor. To match descriptors from two different images dsc1 and dsc2, a symmetric distance function is defined as the average norm of left and right best match scores vectors (BMSV) mij(8)distdsc1,dsc2=m12||+||m21/2(9)mij=minl=1,…,LjdiffFik,Fjlk=1,…,Li(10)diffFik,Fjl=||Fik−Fjl||where i,j∈{1,2}, mijare the BMSV and Li=1,2 is the number of individual line features in each descriptor. Note that one has to compute two BMSV (left and right) to satisfy the distance symmetry requirement. In our experiments, we found that in some applications (e.g., matching faces), it may be sufficient to compute just one half of the match (e.g., left BMSV) without much loss in the accuracy. This makes our distance non-symmetric but cuts matching time by approximately half. These time savings can be used for ensuring RSILC invariance to image reflection (a feature that most key-point descriptors lack, e.g., SIFT and SURF) by redefining(11)diffF1k,F2l=minF1k−F2l||,||F1k−F^2l(12)F^=Fi=n2+1,…,n,Fj=1,…,n2whereF^is a half-swapped feature vector (of even length n) computed to ensure the mirror flip matching invariance of the key-line descriptor; this effectively rotates the histogram sectors by the angle of π about the center of the key-line. One can skip the extra half-swap histogram match if the key-line orientation span is extended to the [0,2π] range (e.g., by computing the major intensity gradient direction at the line center), then diff can again be computed as in Eq. (10).Although there certainly may be other descriptor matching methods, ours is consistent with most matching methods in the literature. Given a relatively small number of key-lines (compared to number of key-points) in a typical image and a good discriminative power of RSILC, we did not need to remove any outliers or perform descriptors cross-check.

@&#CONCLUSIONS@&#
We introduced a new Rotation- and Scale-Invariant Line-based Color-aware descriptor (RSILC), which detects image key-lines and their circular regions, combining both local (intensity, color, gradient histograms) and global (line inter-positioning) information. It models the line-rich objects in a more intuitive and economical way than the popular key-point descriptors do. Moreover, RSILC has a better matching efficiency, since it produces more discriminative candidates to match in line-rich regions of interest. Aside from the mentioned features, RSILC is also mirror-flip invariant via symmetric key-line descriptor matching, something that other key-spot descriptors we tested cannot claim.We performed a thorough investigation of the descriptor matching performance by conducting the following image and face retrieval experiments involving (i) generic line-rich objects and faces (an important special case of line-rich objects), (ii) controlled illumination variation using several direct lights, (iii) geometric transformation, including rotation, scale, and mirror-flip transformations. We compared our descriptor matching performance with well-known descriptors in the same test-bed system. The results indicated that overall RSILC is more accurate for line-rich object matching than the key-point descriptors. We successfully integrated RSILC in the real-world face image retrieval system FaceMatch (FM) and observed that RSILC is more accurate than any of the FM individual descriptors.Our future work includes experiments with alternative edge detection methods (e.g., key-line filtering directly after the Sobel operator), various optimizations to RSILC computation and matching (e.g., reducing the individual descriptor size) utilizing multiple processing cores, and pushing batch matching to GPU. RSILC can be further expanded to piece-wise polynomial and curve-based region matching, which should expand the possible image matching application pool. Although RSILC produced good results in our out-of-plane rotation experiments, it was not designed to be invariant to affine or projective transforms, hence a natural extension would involve the study and introduction of such an invariance.