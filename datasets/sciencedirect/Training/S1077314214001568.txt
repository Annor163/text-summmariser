@&#MAIN-TITLE@&#
Gesture recognition corpora and tools: A scripted ground truthing method

@&#HIGHLIGHTS@&#
We present a review of the tools and corpora for gesture recognition.We introduce a framework supporting acquisition and management of corpora.We introduce a facilitating method for ground truthing corpora during acquisition.

@&#KEYPHRASES@&#
Human–computer interaction,Framework,Gesture recognition,Dataset,Corpora,Ground truth,

@&#ABSTRACT@&#
This article presents a framework supporting rapid prototyping of multimodal applications, the creation and management of datasets and the quantitative evaluation of classification algorithms for the specific context of gesture recognition. A review of the available corpora for gesture recognition highlights their main features and characteristics. The central part of the article describes a novel method that facilitates the cumbersome task of corpora creation. The developed method supports automatic ground truthing of the data during the acquisition of subjects by enabling automatic labeling and temporal segmentation of gestures through scripted scenarios. The temporal errors generated by the proposed method are quantified and their impact on the performances of recognition algorithm are evaluated and discussed. The proposed solution offers an efficient approach to reduce the time required to ground truth corpora for natural gestures in the context of close human–computer interaction.

@&#INTRODUCTION@&#
These last years, the field of human gesture and activity recognition has been evolving rapidly due to the research and development in novel sensors for human action, activity and gesture recognition. These new sensors can be split in three types: vision (color, depth or heat), position (inertial motion units, global positioning system, or motion capture) and physiological (temperature, heart rate or electromyography). The advances in technology allowed engineers to produce smaller, more efficient and cheaper sensors and the possibility to embed them in wearable devices such as necklaces, watches, and controllers. These new sensors offer interesting exploration paths for research but also complexify the quantitative comparisons of methods, algorithms and sensors.We identified three linked issues hindering research in the domain of natural gesture recognition. The recognition of gesture performed in the air by a human is often only considered as a subdomain of action and activity recognition and may confuse researchers, the lack of standards and common structure amongst corpora restraint valid quantitative comparisons of methods and the increasing complexity and cost of creating multi-purposes corpora may become a problem for researchers.The first issue concerns the confusion between research domains. Three main paths of exploration can be distinguished: human action and activity recognition, human surveillance and human gesture recognition. These three areas of research share many common aspects and are often confused. Action and activity recognition focuses on recognizing high-level actions or activities performed by humans such as walking, hiking, cycling, eating, lying in a couch, and working or preparing a meal. The result of the recognition is mainly applied to monitor or generate statistics about users, contextualize interaction or automatize the environment [1]. Human surveillance focuses on recognizing activity, actions or situations that may indicate an undesired behavior such as shoplifting, dangerous situations, potential threats to persons or simply to facilitate everyday life such as ambient-assisted living [2]. Gesture recognition slightly differs from the two latter, which may be seen as a single field of research with different purposes. Gesture recognition focuses on recognizing gestures performed by a human in order to control or interact with devices; the notion of intention is important. The recognized gestures can be waving a hand, pointing at a device, opening a hand, touching both hands, clapping, sign languages or any gesture performed in the environment. Those air-gestures are often considered as a more natural way to interact with our surrounding environment than physical controllers or buttons [3]. The confusion amongst these three fields complexifies research in gesture recognition; these areas share many common terms and aspects, sometimes with different meaning and are easily mixed in the literature. Furthermore, gesture recognition also has specificities that are not considered; specifically datasets and evaluation metrics are often shared amongst research domains despite their many differences. We believe that dedicated guidelines should be developed specifically for the domain of gesture recognition.The second issue concerns the lack of standards and common structure in gesture recognition datasets. This situation probably originates from the previously mentioned weak differentiation from action and activity recognition, which encourages the reuse of generic tools, guidelines and frameworks and from the fact that datasets are often initially produced only for usages internal to a laboratory. The availability of a dedicated framework supporting the complete chain of operations for developing applications and corpora for gesture recognition should hopefully lead the researchers to share common standards and structures. Some existing frameworks have already been developed to handle multimodal inputs in the generic context of activity, action and gesture recognition. However, these frameworks mainly focus on rapid prototyping functionalities to facilitate the creation of small applications, proof-of-concepts or demonstrators with only basic knowledge of programming. They have notably been used for artistic purposes [4–6]. The need of corpora and related tools for developing and evaluating algorithms is crucial for fields relying on machine learning algorithms. However, none of the reviewed framework has been developed to support facilitated corpora creation in the context of gesture recognition.The third issue concerns the availability of corpora. In order to train, optimize and evaluate supervised algorithms, researchers have two options: use existing publicly available corpora or create their own. Corpora consist in raw or processed sensor data and their corresponding ground truth. The ground truth, also called labeling or annotation, refers to precise description (textual or equivalent) describing what is in the data or what should be recognized from the data at a specific instant. Creating a true all purposes ground truth would then imply a complete description of explicit and implicit information contained in every frame of the data, which is practically not feasible. Generally, only three types of information are considered for the ground truths in gesture recognition corpora: the name of the gesture, its temporal segmentation and the spatial segmentation of specific body-parts. Therefore the use of existing dataset is not always possible due to missing ground truth or potential specificities of datasets and algorithms. The creation of a corpus is a time-consuming and costly task. The sole acquisition of sensor data for a corpus is already a time-consuming task; it requires recording multiple subjects in different controlled conditions potentially with multiple synchronized sensors. Once the acquisitions completed, the ground truthing of the data is usually performed by an expert human annotator spending numerous hours analyzing the data, frame by frame, and labeling names of gestures, temporal start and end of gestures, spatial position of body-parts, etc. Several programs are already available to facilitate this ground truthing process but only few valid automatic or semi-automatic solutions are applicable to gesture datasets. These limitations often hinder the number and the quality of the datasets produced. This situation notably limits the quantity of elements labeled during the ground truthing process, as each additional label implies additional manual work.In this article, we present a framework supporting the rapid prototyping of applications, the creation and management of datasets and the quantitative evaluation of algorithms for gesture recognition. The central part of the paper proposes a novel method that has been developed to automatize the acquisition of ground truth when creating new corpora in the context of gesture recognition for human–computer interaction. Concretely, our contributions are:•A review of the available frameworks, tools and corpora for gesture recognition.A framework supporting rapid prototyping, the creation and management of datasets and the evaluation of algorithms in the context of multimodal gesture recognition.A method facilitating the ground truthing of data when creating new datasets.The article proceeds with a discussion of related work in Section 2. Then our “Framework for the Evaluation and Optimization of Gesture Acquisition and Recognition Methods” (FEOGARM) is presented, illustrated with two practical examples of applications and discussed in Section 3. In the central part of the article, Section 4, our novel ground truthing method is presented, followed by an evaluation of its accuracy and potential impact on the recognition rate for machine learning algorithms and discussed. In Section 5, we provide a general conclusion for the article, a conclusion for each section and we point toward potential future works.This section focuses on a literature review for the three topics addressed in this article. We first clarify what is referred to when using the term gesture recognition and we present a brief review of the recent advances in the field. In the first subsection, we review the frameworks used in literature for the rapid prototyping of gesture recognition applications and we detail three of the most popular ones. Then we review the available corpora for gesture recognition and highlight their main characteristics in the second subsection. Finally, we describe the tools and methods used for ground truthing and illustrate them with practical examples from the literature.The term gesture recognition is often incorrectly referred as being similar to human action and activity recognition in literature. In this article, gesture recognition precisely refers to a subset of the human activity and action recognition field [7] and can be defined as the process by which specific gestures, intentionally performed by a user, are recognized and interpreted by a machine. Natural gestures refer to expressive and meaningful body movements involving physical motion of the fingers, hands, arms, head, face or body with the intent of conveying information [8]. It can be summarized as an ergonomic body-command performed with the intent of interacting with an automatic system. Gesture recognition has initially reached most of its popularity based on devices measuring acceleration or inertia such as the Wii controller [9] and then some success with video processing techniques based on cameras [10], and time-of-flight cameras [11].The apparition of commercially available cheap and efficient RGB-D cameras has given a new impulse on vision-based techniques for gesture recognition [12,13]. The use of multimodal inputs has been studied for a long time in the context of human–computer interaction, such as the famous “Put-That-There” example from 1980 [14]; it consists in a human–computer interaction system based on multiple communication channels such as speech, gesture, and writing [15]. Multimodal or multi-sensors systems for human computer interaction have largely evolved and have driven to new research paths thanks to the recent advances in sensors size, price and availability. Multimodal research also drives the research toward contextual and opportunistic systems for the selection and use of available sets of sensors to interact with the environment [16] or toward methods to improve recognition by fusing multiple types of sensors or modalities [15]. This article focuses on the domain of multimodal gesture recognition performed in mid-air in the context of close-human computer interaction.Several frameworks have been developed to handle and facilitate the development and evaluation of multimodal applications. In [17], most frameworks applicable for pervasive research have been reviewed and classified into design and evaluation frameworks. These frameworks, also called rapid prototyping toolkits, facilitate the development of the entire design process: from early concepts based on low fidelity prototypes to the deployment of high-fidelity prototypes for practical tests and evaluations [17]. An issue with many of these earlier frameworks is that they have been developed mainly for end-users and have limited expressive power [18]. Most frameworks share common properties and technical features such as distributed processing, synchronization, modularity and rapid prototyping. It is worth noting that most frameworks used in research for gesture recognition are extensions or reuse of frameworks initially developed for activity or emotion recognition or for pervasive computing.Best-known frameworks include Context Recognition Network Toolbox (CRNT) [4], EyesWeb XMI [5] and Social Signal Interpretation (SSI) [6]. CRNT has been developed to facilitate the rapid construction of multimodal context recognition systems and their rapid deployment in targeted environment [4]. The framework provides a set of tools and modules for the connection of devices (drivers), the processing of data and some pre-implemented algorithms and machine learning systems. It has notably been used to acquire the dataset of the Opportunity Challenge where multiple sensors distributed in the environment and on a user were synchronously recorded in an everyday life scenario [16]. EyesWeb XMI has been developed as a toolbox and visual interface for processing full-body human movements and interaction in the context of emotions and non-verbal expression recognition [19]. A collection of software modules has been developed and released publicly: the “EyesWeb Expressive Gesture Processing Library”. This collection implements models and modules to handle the processing of data for motion, space and trajectory analysis. The main research directions for the framework include analysis and classifications of expressive actions in musical signals and human movements, real-time generation of visual and audio content based on results of data analysis and user interactions. The framework has been widely used in dance and artistic fields due to the simplicity of its visual “drag&drop” user interface [5]. SSI has been developed to complement existing tools by providing support for the development of online recognition systems in the context of multiple sensors. It has been specifically tuned for machine learning pipelines and the acquisition of data [6]. The main research directions for SSI are the analysis of physiological signals in real-time; notably expressivity in user speech and affective recognition from video. The framework has later been extended to cover a larger area of research and to facilitate the manual annotation of recorded data through a specific interface [20]. Our proposed framework bears much resemblance with SSI but it has been specifically designed for gesture recognition and for supporting the creation and management of datasets in the context of multiple sensors and/or modalities.Most of the research performed in the field of gesture recognition relies on corpora to develop, to train, to optimize and to evaluate algorithms and systems. In the context of gesture recognition, a corpus consists in a machine-readable collection of labeled files containing data such as video, text or audio which characterize the gestures. The files are labeled with meta-data describing the gestures temporally and/or spatially and potentially also describing the context of the data acquisition and information about the subject; this meta-data is referred as ground truth. In this article and in the machine learning literature, corpora and datasets refer to the same concept and can be freely interchanged. Developers either create their custom datasets or reuse datasets created by other research groups when possible. The advantages of using an existing dataset are not negligible: beyond the time and cost spared by not acquiring the dataset, it also provides means to quantitatively compare the results on common material (benchmark platform).A few surveys have been published for the field of human action and activity recognition and some of them specifically on the available datasets; however the sub-domain of gesture recognition has been explicitly omitted from those surveys of datasets for comprehensibility [7]. Recently, a startup called ARB Labs has tried to initiate a commercial transition in the field by selling its corpus of gestures to companies and researchers [21]. Note that the KUG database already attempted a first step in this direction in 2006 [22]. A recent and comprehensive survey reviews the video datasets available for human action and activity recognition [7] and another article listed the few datasets available for multimodal activity recognition [23]; however a comprehensive survey specifically dedicated to corpora for the field of gesture recognition was still missing. A few recent research papers have been focusing on theoretical and practical considerations required in order to produce quality datasets for research. In [24], they identified three requirements: a natural gesture set, a gesture set containing enough instances of each class and an analysis of the recording conditions and their potential effects. In [25], they studied the impact of using particular semiotic modalities such as text, image or video during subjects instructions on the quality of the acquired gestures as training data for machine learning algorithms. The study highlighted the need to have a trade-off between coverage and correctness of the motion of the acquired gestures for optimal learning performances of the algorithms.A review of the most recent and/or popular datasets for gesture recognition is shown in Table 1. It resumes their main characteristics: sensors, recording conditions, involved body-parts, ground truthing methods and material, applicability and availability. The table illustrates the increase in the number of datasets being released these last years, mainly due to the novel sensors but also due to the growing number of potential fields of application. By analyzing the corpora listed in Table 1, several considerations can be postulated. A rapid change of the main vision sensors used in the field is clearly visible: from color cameras and webcams to depth cameras. Three main types of datasets can be distinguished according to the “body-parts” visible from the sensors: hand(s) only for hand gesture recognition with only the hand(s) and sometimes part of the arm(s) being visible from the sensor [26–30], upper-body where the subjects use their arm(s) and hand(s) to interact [31–36] and full-body where the whole body is tracked and potentially used for interaction [25,37,38]. Note that upper-body and sometimes full-body may also involve tracking the head and the face of the subject. The intended domain of applicability of each dataset also varies depending on the type of ground truth data available, recording conditions and the chosen set of gestures. The datasets are mainly produced for the training and the evaluation of recognition algorithms and therefore include at least the name of each gesture as ground truth. Some datasets also include temporal segmentation and can be used for gesture spotting [31,33–35,37,38], meaning that it allows researchers to train and evaluate algorithms to automatically detect the start and end of gestures. They can also contain 2D or 3D spatial position of body-parts in the ground truth and therefore may be used to train and to evaluate tracking algorithms [27,31,33,35]; the hands and head are the body-parts most commonly tracked. A few datasets also contain varying or cluttered background, varying light conditions or additional subjects wandering in the background in order to test reliability of algorithms to perturbations and real conditions [29,38]. Some datasets also focus on specific types of gestures such as sign language [28,31,34], gaming gestures [37], iconic/deictic gestures [36] or hand pose [29].It is also interesting to observe the ratio “number of instances per class” which largely varies between datasets; generally, the more classes collected the less number of instances per class. For training and evaluations of algorithms, a high ratio is generally required to obtain valid results or to generalize them [24]. Another interesting piece of information indicates how ground truth data has been acquired, a characteristic which is generally not clearly explained; because most corpora are annotated manually by experts after the recordings. The different ground truthing methods are detailed in Section 2.3. In the above list of datasets, seven have been annotated manually, one automatically, one semi-automatically, one has been annotated by users during data acquisition and for five datasets the method has not been reported although they probably have been manually annotated. Note that the only semi-automatically annotated dataset provides only the name of the gesture as ground truth, which corresponds to the minimal ground truth information. In this article, we present a novel automatic scripted ground truthing approach which has been used to annotate the name and the temporal segmentation of the gestures occurrences contained in the ChAirGest dataset [32]. This approach is described in details in Section 4.Various software and approaches have been developed to facilitate the ground truthing task. Most of them have been initially developed in the context of voice and video processing and then reused or extended for human action, activity and gesture recognition. This section reviews some of the software, user interfaces and tools that have been applied for ground truthing and then lists and reviews the existing approaches developed to facilitate ground truthing and illustrate each of them with examples of application from research fields close to gesture recognition.Several software have been developed in order to facilitate the manual ground truthing of large corpora containing multimodal and/or multi-sensors data.ANVIL is a platform-independent free research tool initially developed to annotate audiovisual data containing multimodal dialogue [41]. Since then it has been widely used and extended amongst the research community for various research fields. ANVIL defines a specific annotation format which enables to hierarchize the information in multiple tracks and to create several depths of annotation such as words spoken, head movement, hand movement, and gesture name. Such a coding system also provides the possibility to add new levels of annotations on pre-existing files. Several studies have been carried to obtain an intuitive interface in order to simplify the labeling process by providing multimodal functionalities. ANVIL has recently been extended to visualize motion capture data using a 3D skeleton representation [42].ELAN is a platform-independent professional linguistic annotation tool [43]. It has been designed to create and manage text annotations for audio and video files with a strong focus on annotation of language. Similarly to ANVIL, annotations are grouped in layers that can have several relationships between each other: independent, aligned or embedded. The development of the software focused on providing a precise time accuracy of the labeling through specific tools and interfaces.ViPER is an open-source framework that enables ground truth annotation of video data through a visual interface and also provides systems to evaluate algorithms performances [44]. The toolkit mainly offers two distinct programs: ViPER-GT consists in a graphical user interface (GUI) developed in Java for video annotation focusing on spatial and temporal labeling and ViPER-PE is a command-line performances evaluation tool offering various performance metrics to compare algorithms. SSI is an open-source C++ framework for multimodal signal processing in real-time [20]. It has been developed for human behavior recognition in multimodal contexts and handles most of the steps required for the production of a corpus: processing, synchronization and recording of sensor signals using specific blocks, modules and pipelines which can be easily defined in XML files. A specific user interface has also been developed to visualize and annotate the recorded multimodal data offline.Recently, VATIC has been developed as an open platform for monetized crowdsourcing of video ground truthing using Mechanical Turk systems [45]. It provides tools to facilitate the development of web-based labeling interfaces in order to take advantage of crowdsourcing possibilities. Finally our framework is being developed to handle all the processes involved when developing multimodal applications [46], including specific tools for facilitating the acquisition of multimodal corpora, as further described in Sections 3 and 4.In literature, five main approaches can be distinguished to produce the ground truth annotations for videos and multimodal datasets: semi-automatic, crowdsourced, user-annotated and automatic. The following paragraphs describe each approach and illustrate them with practical examples.The first approach is manual ground truthing; it is the most used method in research, particularly for small project-specific datasets but also for larger datasets. In most cases, manual ground truthing is performed offline by one or more experts spending hours annotating the videos, generally frame by frame, using custom tools or existing framework [47]. Manual ground truthing may also be partially done online during the data acquisition process by observers annotating the events in real-time [48]. Researchers often release partially labeled datasets due to the time required for the manual ground truthing of all the data [49]. Researchers have also worked at making this process faster using better interfaces or lighter representations of the data such as 3D skeleton representation instead of video [42]. In gesture and activity recognition, the manual segmentation often implies splitting videos and/or data in small files where each file contains a single occurrence of an event to recognize; this file segmentation simplifies the ground truthing and the data management processes [35]. The disadvantage of such method is that it removes the possibility to train or evaluate spotting algorithms due to the absence of temporal segmentation. Many datasets have been manually ground-truthed although the information was not reported in related publications.The second approach is the semi-automatic ground truthing which consists in partially labeling the data using specific algorithms and then requesting an expert to correct, validate or extend the results of the algorithms. This method has been used for face land marking and sign language recognition where annotators were only requested to validate the resulting output of the algorithms [50,51]. In the context of object tracking and recognition in videos, one semi-automatic method consists in providing facilitating tools to the annotators for automatically extracting the contours of objects in each frame [52]. A different approach in the context of multiple subjects interacting for human–robot interaction consists in several steps of annotations [53]. The first step consists in using scripted scenarios to partially label the data during the recordings; the second step uses algorithms to automatically augment the labeling with spatial positions of actors and with a textual translation of their speech; finally, in the last step, human annotators validate the automatic labeling of algorithms and also manually augment the labeling with the actions performed by the subjects.The third approach consists in crowdsourcing the manual annotation of the datasets. This approach has been popularized these last years with the apparition of crowdsourcing marketplaces on internet such as Amazon Mechanical Turk [54]. On these marketplaces, human workers can be hired to complete a specific task online. This approach is still young and the advantages and disadvantages of using non-expert and untrained annotators have only been partially studied [55]. A recent study showed that using this approach was applicable and efficient for segmenting and labeling activities in videos sequences using specific filtering methods to identify and remove non-serious workers [56]. The interface and requirements to produce good quality annotations for complex videos with dense and closely cropped labels have been studied in Vondrick et al. [45], where they demonstrated that crowdsourcing only should not be used for annotation of videos and that computers should assist humans. Furthermore they have shown the importance of the design of a clear annotation protocol to obtain high quality labeling in the context of crowdsourcing.The fourth approach consists in user annotations and relies on the subjects to annotate autonomously their data at the time of acquisition. This is only possible in particular contexts and highly relies on the good will of the users. This approach has been applied for activity recognition in real-life using smartphones as input sensors: in Kawaguchi et al. [57], they developed an application where the users or researchers can autonomously annotate accelerometer data in real-time on their smartphone while the activity is being performed. The produced data can then be submitted online. Another project developed the same approach for static object/scene recognition from 3D depth data using a Kinect™ sensor: users can download a specific software enabling the acquisition and labeling of the data acquired by a 3D depth sensor and then submit their scans using a dedicated website [58]. Another classical user annotation approach consists in the user pressing a button in real-time to indicate the start and end of the gestures that he performs. However, from our experience, user annotation tends to produce many errors: subjects often label the class incorrectly or simply forget to press the button to indicate start and stop of gestures. Correcting the errors manually offline may also be a time-consuming task.The fifth and more challenging approach consists in methods enabling the fully automatic ground truthing of the data. Several methods have been developed to automatically annotate the data: creation of known data through programs or scripts, using devices with high precision to produce ground truth data concurrently with less reliable sensors or using high confidence algorithms to produce reliable ground truth data. In [59], they use computer animation to produce scripted 3D realistic scenes in order to obtain pre-labeled video surveillance datasets. In [60], they use a specific program to produce realistic hand images from multiple view-points to obtain a dataset for hand pose recognition. Using scripted-scenarios is also an interesting method; scripts can be produced by the researchers [61] or researchers can reuse existing video sequences where scripts are already available such as for movies. In [62], they use sequences from Hollywood movies and parse the scripts of the movies to produce the ground truth. Another approach consists in using precise motion capture systems to produce reliable tracking data as in Sigal et al. [63]; this method has often been used for action and activity recognition but requires expansive and constraining setups. The last approach consists in relying on algorithms to automatically label or to extend the labeling of data; it has notably been applied for enhancing sign language annotations based on state-of-the-art automatic sign language recognition algorithms [64] or for video annotations based on speech and object recognition [65]. Note that most of these automatic ground truthing algorithms rely on pre-existing labeled datasets for their training and high confidence algorithms.The FEOGARM framework has been built in order to provide tools and methods allowing developers to handle the complete chain of operations when developing a gesture-based multimodal application: from designing and building corpora to the evaluation of the performances of the developed algorithms while maintaining functionalities for rapid prototyping of applications. The aim is to provide a single modular and reusable framework to facilitate the development, training, testing and deployment of application or prototypes for gesture recognition. The added value of the framework is that it has also been built to support the creation and management of corpora and to facilitate the quantitative evaluation of gesture recognition algorithms. The general goal is to simplify the acquisition and processing of data and the comparison of algorithms in the context of multimodal and/or multi-sensors gesture recognition.Fig. 1briefly resumes the generic chain of operations required when building a new gesture-based prototype application. The first line refers to the conceptualization of an application: several sensors are considered, a specific vocabulary of gestures is chosen, several algorithms are envisioned, evaluation measures are defined and potentially the final intended application is specified. The second line represents the practical operations that have to be developed to build a prototype application usually based on live trials and reduced dataset. These operations are supported by most frameworks reviewed in this article, including FEOGARM. Finally the third line represents the extended practical operations supported by FEOGARM where more emphasis has been put on the corpus thus facilitating the comparison and optimization of algorithms in the next phases.The FEOGARM framework, similarly to other rapid prototyping frameworks, facilitates the work of the developers by providing tools and modules to rapidly produce a working prototype application. The added value of our framework is that it also supports specific tools and modules to handle the recording, management and ground truthing of a dataset, data visualization and analysis modules and methods to compare different algorithms or sets of sensors. One of the main difference is that most frameworks focus on the rapid development of prototype applications in order to demonstrate the feasibility of a concept while our approach aims at producing applications where multiple algorithms or combination of sensors can be tested and quantitatively compared on one or more datasets in order to produce an optimal experience in the final application and we therefore claim that our framework can handle the whole chain of operations.The framework has been developed in C++/C# and exhibits the standard pipelines, modules and distributed architecture shared amongst most frameworks for multimodal applications [17]. FEOGARM has been built upon ARAMIS, a framework developed in our research group and intended for contextual hybrid gesture multimodal interaction [66]. The FEOGARM framework has been developed for experienced developers; therefore it does not provide a simplified user interface to prototype applications but enables full customization and modification of every modules and drivers through well-documented code and examples.For each application, a new project must be created: a specific class defines the global structure by specifying the modules and their interconnections as well as potential options. As illustrated in Fig. 2, the framework supports several drivers, processing, recording and evaluation modules and simple graphical visualization interfaces for sensor data or algorithm performances. It also provides specific graphical user interfaces for data producers and for data consumers: the data producer graphical user interface facilitates the process of acquiring ground-truthed corpora through a presentation style interface which is further described in Section 4. The consumer graphical user interface provides tools to manage and visualize the data of the corpora; the visualizer notably provides an interface to visualize the data streams and the ground truth synchronously and specific tools to analyze the data contained within the corpus. To handle recording and replaying of multimodal corpora containing multiple sensors, some internal processing is performed during acquisition to temporally tag the data streams recorded from each distinct sensor and then an automatic resynchronization of the data is performed when reloading the data. These mechanisms ensure temporal synchronization even if the sensors have different frame rates. Furthermore the process is transparent for the consumers. A current limitation of this solution is that the corpus must have been recorded by the framework in order to be correctly replayed with our system.The framework requires the implementation of specific drivers for each new sensor. All drivers are based on a common structure and only a few methods that are required to obtain the data from a specific sensor must be modified. The processing modules allow developers to record, to replay or to visualize the data streams from the sensors. They also enable to automatically process the data in real-time: features extraction, filters and segmentation modules. It is worth noting that additional modules can easily be coded and then reused. Classifiers modules correspond to machine learning algorithms which can be trained, used in real-time and evaluated within the framework. Currently several types of Hidden Markov Models (HMM), Support Vector Machine (SVM) and Neural Networks (NN) have been implemented. They are based on the open-source Accord.NET framework [67]. The FEOGARM framework is currently only available internally and should be released as an open-source project in 2014.A Natural Pointing System has been developed based on previous work and implemented using the FEOGARM framework [3]. The developed system facilitates the interaction between people with mobility impairments and their surrounding environments; two different paradigms have been evaluated; interaction through a touch interface using a smartphone and through natural pointing gestures. The implementation of the natural pointing detection system [68] in the application has been facilitated by the rapid prototyping features of our framework. It enabled to easily interconnect the different devices required for the experiment and to rapidly perform the evaluation procedure in a laboratory setting.The ChAirGest corpus and the related open challenge have been respectively acquired and managed using the FEOGARM framework [32]. The steps in order to produce the corpus have been largely facilitated through the use of the framework. The first step was to design the corpus by choosing the desired physical setup, the set of sensors and the set of gestures for the corpus. Based on these choices, a specific application has been implemented interconnecting the drivers, recording modules and the visual producer interface. Once all the pieces connected, preliminary tests have been performed to assess the usability of the application and to improve it. As further described in Section 4, a first recording of each unique gesture and their manual labeling allowed us to subsequently record and label automatically all the gestures performed by the subjects during acquisition. Note that the handling of the subject information, folder grouping of the acquired files and all the metadata were also managed by the framework. Finally the corpus visualization and management tools allowed us to check and clean the raw data rapidly and then to convert it into several lighter formats for release and distribution purposes. The data has been released in the context of an open challenge.1https://project.eia-fr.ch/chairgest/.1Several tools and an API have been released along with the data in order to allow the participants of the challenge to visualize, to access and to process the raw data of the corpus. Finally the evaluation modules have been used to quantify the results of the contestant of the open challenge and to provide quantitative feedbacks.

@&#CONCLUSIONS@&#
