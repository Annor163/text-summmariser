@&#MAIN-TITLE@&#
Speaker identification using vowels features through a combined method of formants, wavelets, and neural network classifiers

@&#HIGHLIGHTS@&#
This paper proposes a new method for speaker feature extraction based on Formants, Wavelet Entropy and Neural Networks denoted as FWENN.In the first stage, five formants and seven Shannon entropy wavelet packets are extracted from the speakers’ signals as the speaker feature vector.In the second stage, these 12 feature extraction coefficients are used as inputs to feed-forward neural networks.In contrast to conventional speaker identification methods that extract features from sentences (or words), the proposed method extracts the features from vowels.Advantages of using vowels include the ability to identify speakers when only partially-recorded words are available. This may be useful for deaf-mute persons.

@&#KEYPHRASES@&#
Speaker verification and identification,Wavelet packet,Neural networks,Formants,

@&#ABSTRACT@&#
This paper proposes a new method for speaker feature extraction based on Formants, Wavelet Entropy and Neural Networks denoted as FWENN. In the first stage, five formants and seven Shannon entropy wavelet packet are extracted from the speakers’ signals as the speaker feature vector. In the second stage, these 12 feature extraction coefficients are used as inputs to feed-forward neural networks. Probabilistic neural network is also proposed for comparison. In contrast to conventional speaker recognition methods that extract features from sentences (or words), the proposed method extracts the features from vowels. Advantages of using vowels include the ability to recognize speakers when only partially-recorded words are available. This may be useful for deaf-mute persons or when the recordings are damaged. Experimental results show that the proposed method succeeds in the speaker verification and identification tasks with high classification rate. This is accomplished with minimum amount of information, using only 12 coefficient features (i.e. vector length) and only one vowel signal, which is the major contribution of this work. The results are further compared to well-known classical algorithms for speaker recognition and are found to be superior.

@&#INTRODUCTION@&#
Speech processing applications include speech recognition and speaker identification. Speaker identification system is a technology with a potentially large market due to broad applications that range from automation using operator-assisted service to speech-to-text aiding system [1,2].In general, a speaker identification system can be implemented by observing the voiced/unvoiced components or by analyzing the speech energy distribution. Such systems can be divided into two main steps: feature extraction and speaker classification [3]. Several digital signal processing methods have been used by researchers: Linear Predictive Coding (LPC) technique [1], Mel Frequency Cepstral Coefficients (MFCC) [4], Discrete Wavelet Transform (DWT) [5] and Wavelet Packet Transform (WPT) [3].Due to its success in analyzing non-stationary signals, DWT has become a powerful alternative to the Fourier methods in many speech/speaker identification applications. The main advantage of wavelets is its optimal time–frequency resolution in all frequency ranges. This is a result of varying the window size for different frequencies: wide for slow frequencies and narrow for fast frequencies [1,6,7].Previous studies showed that using Wavelet Packet (WP) entropy as features in recognition tasks is effective. In [27], a method to calculate the wavelet norm entropy value in digital modulation recognition was proposed. In [36] a combination of genetic algorithm and WPT was presented and the energy features were determined from a group of WP coefficients. The work was applied in bio-medic where the results were used for pathological classification and evaluation. Energy indexes of WP were proposed for speaker identification [3] and sure entropy was considered terminal node signal waveforms obtained from DWT [1] and applied to speaker identification. Others, [28], used features extraction methods based on a combination of three entropy types (sure, logarithmic energy and norm).Formant can be described as a function of the supralaryngeal vocal tract. The air in the oral and nasal cavities vibrates at a range of frequencies as a response to the vibratory movement of the vocal folds and air passing through the glottis. These resonant frequencies are affected by the size/shape of the vocal tract and by the tongue and lip positions [9]. Vocal tract resonances are often studied in terms of vowel formant frequencies. Because the male vocal tract is about 15% longer than the female vocal tract, men's speech signals have lower formant frequencies than women's [10]. During voiced speech, resonant frequencies of the vocal track are recognized as formants with valuable features for both automatic speech recognition and speech synthesis [11].The use of formants for 1-D and 2-D continuous motion control created a new vocal interface that allowed people, especially individuals with motor impairments, to interact with computer-based devices [8,49].Researchers used different methods for formant tracking that included: Linear Predictive Coding spectral analysis [12], hidden Markov model based methods [13,60], nonlinear predictors [14], and Kalman filtering framework [15].Artificial Neural Network (ANN) models have been effectively used for speaker classification [38,53,57,59]. Researchers used radial basis function networks [17,18,54] and developed ANN-based techniques using a cascade neural network [16] for speaker verification. Others compared ANN with second order statistical techniques for speaker verification [19]. Committee neural networks to improve the reliability of ANN based classification systems were developed [21,22] and the use of these committee networks for text-dependent speaker verification was addressed [20]. Support Vector Machines (SVM), a special case of Tikhonov regularization that belongs to the general linear classifier family, have been used for speaker recognition [46–47].Researchers used a combination of MFCC and parametric feature-sets’ algorithms to improve the accuracy of speaker recognition systems in adverse environments. Some studies, such as [39], emphasized their work on text-dependent speaker identification, which deals with feature extraction by means of LPC coefficients. A Gaussian shaped filter was used for calculating MFCC and IMFCC instead of typical triangular shaped bins. A system using four transform techniques was suggested in [40,41]. The feature vectors were the row mean of the transforms for different groupings. Experiments were performed on Discrete Fourier Transform (DFT), Discrete Cosine Transform, Discrete Sine Transform and Walsh Transform. All these methods showed an accuracy of more than 80% for the different groupings considered. However, the results showed that Discrete Sine Transform had the best performance. In [42] IMFCC, which covers high frequency, was used to improve speaker recognition rate.Researchers investigated fundamental and formant frequencies for a speaker recognition task [50]. It was concluded from a detailed comparison that the long-term formant distributions contributed to the rejection of the suspect. Grigoras continued this study to calculate likelihood ratios based on the density estimation of formant frequencies on distinct vowel phonemes ([a], [e], [i], [o]) [51]. Rose [52] suggested the comparison of vowel phonemes by likelihood ratio computation, and recognition of human speech phonemes by fuzzy method was proposed in [58].In our study, vowels are used for speaker recognition. And because the formants are recommended in case of vowels [51,52], they are studied here in detail. In order to enhance the recognition results, WP entropy is utilized. The reason behind WP entropy is to extract additional features over different band passes of frequency by Shannon entropy.This paper presents a new method for speaker identification that uses formants and wavelet packet entropy within a feed-forward neural network. The objective was to develop the method using partially-recorded speech signals only. The major contribution of this research is the development of an accurate speaker identification method that uses simple computations with minimum amount of information. The developed method is capable of dealing with vowels as the only input from the speech signal. This method might be used for forensic and criminal investigation as well as for deaf-mute speakers’ recognition.The paper is organized as follows: Section 1 describes the general structure of proposed method. Section 2 discusses feature's extraction using Power Spectrum Density and Wavelets while Section 3 describes the neural networks used in this work. Section 5 provides the experimental results and Section 6 concludes the paper.The method developed and proposed in this work, Formant and Wave Entropy within Neural Networks (FWENN), is explained in this section. The proposed method is based on several steps (as shown as a flow chart in Fig. 1) and can be divided into four stages: recording and filtering the speech signals, extracting features, classification, and speaker retrieval.The emphasis here will be on the second and third stages: extracting features and classification [37]. Extracting features of the speech signals were performed using two techniques: formants using Power Spectrum Density (PSD) and entropies using WP. These concepts will be explained in Section 3. Classification was done using neural networks and will be explained in Section 4.Periodic excitation is seen in the spectrum of certain sounds, especially vowels. The speech organs form certain shapes to produce the vowel sound and therefore regions of resonance and anti-resonance are formed in the vocal tract. Location of these resonances in the frequency spectrum depends on the form and shape of the vocal tract. Since the physical structure of the speech organs is a characteristic of each speaker, differences among speakers can also be found in the position of their formant frequencies. These resonances affect the overall spectrum shape and are referred to as formants. A few of these formant frequencies can be sampled at an appropriate rate and used for speaker recognition. These features are normally used in combination with other features. Unlike our previous works that investigated DWT for feature extraction [24,25], in this work, the formants are used with the WP entropy to identify the speakers.This paper proposes to use formants and WP parameters as inputs to ANN for speaker classification. Therefore, it is necessary to introduce the two concepts: feature extraction by formants and WP algorithms. The discussion of these concepts will be limited to their use in speaker identification.Formants are the spectral peaks of the sound spectrum of vowels or the acoustic resonances of the human vocal tract. In a wide band spectrogram they show up as black bars. In a small band spectrogram the fundamental frequency and the harmonics are visible as well. The sound production can be modeled as a time varying linear system (having these resonances), that is excited by a sequence of impulses. Using a linear system of order+n and taking the inverse, the model spectrum of the linear system can be created from the speech signal. Using a small order can result in noisy resonances while using a large order can introduce artificial harmonics: For small n, the resonances are smeared, while for large n, some of the peaks are actually not formants but harmonics. An order n corresponding to about 1ms seems to be a good choice resulting in five formants.The first five vocal resonant frequencies, i.e. formants (F1, F2, F3, F4, F5), during voiced-speech are distinguishable for each person and therefore are proposed as the speaker features. For voiced-speech, the glottis signal is periodic with a fundamental frequency (i.e. pitch, F0). Variations of the pitch during the duration of the utterance provide the contour, which can be used as a feature for speech recognition. The speech utterance is normalized and the contour is determined. The vector that contains the average values of pitch of all segments is thereafter used as a feature for speaker recognition. The pitch might be sufficient for the speaker identification, but is usually assisted by the formants for the best speaker recognition [26].The filtered speech signal can be used as an input to a power spectrum algorithm in order to identify the first five formants. These formants can be used as the unique features for the speaker. The PSD algorithm can be used to identify the formants in two steps: First, the PSD is estimated using the Yule–Walker Auto-Regressive (AR) method. Then, the local maxima are identified.Power spectrum can be found by taking the Fourier Transform of the Autocorrelation Function (ACF)(1)Px(ejω)=∑n=−∞∞rx(n)e−jωnwhere rx(n) is the ACF of the signal x(n). The autocorrelation values are estimated from finite data record, x(n) for 0≤n≤N−1, and is defined as(2)rˆx(k)=1N∑n=0N−1−kx(n+k)x*(n)k=0,1,…,pEq. (1) is one estimate of the PSD, but has some disadvantages that include excessive variance estimation. A better estimate is the Yule–Walker method.The Yule–Walker method estimates the PSD of the input using the Yule–Walker AR method. The concept is to minimize the forward prediction error by fitting an AR model to the windowed input data. This method is also called autocorrelation method [30]. The PSD is estimated using the following(3)Px(ejω)=|b(0)|21+∑k=1pa(k)e−jωk2The parameters a(k) and b(0) can found from the autocorrelation estimates and will be described next.The AR model is described in the equation(4)rˆx(n)=−∑k=1pa(k)rˆx(n−k)Which can be expanded into matrix form as(5)rˆx(0)rˆx(1)⋯rˆx(p−1)rˆx(1)rˆx(0)⋯rˆx(p−2)⋯⋯⋯⋯rˆx(p−1)rˆx(p−2)⋯rˆx(n0)a(1)a(2)⋯a(p)=−rˆx(1)rˆx(2)⋯rˆx(p)This formulation is defined as the Yule–Walker equations and the Levinson–Durbin recursion are used to solve the equations in order to obtain the AR parameters: a(1), …, a(p). On the other hand, the parameter b(0) is calculated using(6)|bˆ(0)|=rˆx(0)+∑k=1pa(k)rˆx(k)The above parameters, b(0) and a(k), can now be substituted to estimate the PSD.In the context of this paper, Arabic vowels were used for speaker identification. However, the proposed method can be applied to other languages. Figs. 2 and 3illustrate the applied results for the first five formants (spectrum peaks) of Arabic vowels for speakers’ models discrimination. Fig. 2 illustrates the formants of Arabic vowels for two speakers. For each speaker, four speech signals with Arabic vowel ي sounds /e/ were recorded (see Appendix A). Note that the spectrum features for each speaker are similar while there are clear differences in magnitudes and indexes between the two speakers. Fig. 3 illustrates the formants of two Arabic vowels:sounds /a/ and Arabic vowel ي (i.e. vowel-independent). Here, the overlap between the two speaker spectrum features is much larger because different vowels are used (i.e. vowel-independent case). This is a limitation due to the use of different vowels for each speaker.Frequency information, specifically the indexes of local maxima, contains the distinguishable speaker features. Table 1shows the formants calculations for five speakers. Note that the indexes vary among different speakers, but are consistent for each speaker.A general case of the wavelet decomposition is the WP method. The mother wavelet function is defined by(7)ψa,b(t)=ψt−bawhere a and b are the scale and shift parameters, respectively. By varying a and b, the mother wavelet is scaled and translated. The wavelet transform is obtained by the inner product of the data functionx(t)and the mother waveletψ(t)(8)WψX(a,b)=1a∫−∞+∞x(t)∗ψt−badtThe WP uses a recursive binary tree, as shown in Fig. 4, for the recursive decomposition of the data. A pair of low-pass and high-pass filters, denoted ash[n]andg[n], respectively, are used to generate two sequences, with the purpose of capturing different frequency sub-band features of the original signal. The two wavelet orthogonal bases generated from a previous node are defined as:(9)ψj+12p(k)=∑n=−∞∞h[n]ψjp(k−2jn)(10)ψj+12p(k)=∑n=−∞∞g[n]ψjp(k−2jn)whereψ[n]is the wavelet function, while j and p are the number of decomposition levels and the number of nodes in the previous level, respectively [3,44]. In this study, WPT is applied at the feature extraction stage, but the large amount of data might cause some difficulties. Therefore, a better representation for the speech features is needed and is explained next.For a given orthogonal wavelet function, a library of WP bases is generated. Each of these bases proposes a particular way of coding signals, maintaining global energy and reconstructing exact features. The WP is used to extract extra features to guarantee higher recognition rate. In this work, WPT is used at the feature extraction stage, but this data is not suitable for classifier due to a great amount of data length. Therefore, there is a need to find a better demonstration for the speech features.The WP features’ extraction method can be summarized as follows:•Decompose the vowel signal at WP depth of level two with Daubechies type and calculate the Shannon entropy for each sub-signal. The WP extracts additional features to the Shannon entropy and therefore enhances the recognition rate.Calculate Shannon entropy for all seven nodes of wavelet packet using the equation(11)E(x)=−∑ixi2log(xi2)where x is the signal under consideration andxiare the signal coefficients that form the orthonormal basis. These seven entropies (in addition to the five formants) will be used to identify different speakers.Two different classification approaches were used in the experimental investigation: feed-forward neural networks and probabilistic neural networks.Feed-forward networks are usually composed of multi-layer nodes (Fig. 5). The direction of the data goes only in one-way (i.e. forward). Consider a three-layer neural network which receives inputs x1, x2, …, xm, processes it to the hidden layer and then to the output layer to give the outputs y1, y2, …, yn.The connecting arrows between the nodes have weights (the network variables). These weights are: vjh(connects node input i with hidden node h) and whj(connects node input h with hidden node j). The outputs of the hidden and output layers at each pattern, k, are given by(12)zh(k)=f∑i=1Mvihxi(k)h=1,…,H(13)yj(k)=g∑h=1Hwhjzh(k)j=1,…,Nwhere f and g are the activation function. The most commonly used activation functions are the sigmoidal and hyper-tangent.Let the desired outputs be d1, d2, …, dn. The learning objective is to determine the weight values that minimize the difference between the desired and network outputs for all patterns. Let the error criterion be defined as follows:(14)SSE=1K∑k=1K∑j=1N(ej(k))2=1K∑k=1K∑j=1N(yj(k)−dj(k))2where k refers to the pattern number and j refers to the output node number. The weights are updated recursively(15)whj(iter+1)=whj(iter)+s(iter)(16)vih(iter+1)=vih(iter)+s(iter)Here s(iter) is the search direction at the specified iteration. An efficient update is the use of Levenberg–Marquardt method to find the search direction. This is provided next for the output weights wjh[30](17)E(k)=∑j=1N(yj(k)−dj(k))2(18)J=∂E(k)∂wih(19)s(iter)=−(JTJ+λI)−1(JTE(k))where J is the Jacobian matrix and I is the identity matrix. The parameter λ is updated for each search step: increased if the algorithm is divergent and decreased if the algorithm is convergent. There are two main advantages of using this parameter: enforce descending function values in the optimization sequence and increase the numerical stability of the algorithm.The derivative of the error w.r.t., the hidden weights, vih, involves more steps: The target values at the hidden layer are not available and a chain rule is used to approximate the hidden error. This algorithm is called backpropagation[29].A major application for neural networks is pattern classification. In this paper, the formant and wavelet information presented in the two previous sections were used as input/output data for the neural network for classification. The total number of inputs used was 12 (five formants and seven entropies).Probabilistic neural networks are implementations of statistical algorithms and are generally used as classifiers. These networks are unsupervised feed-forward networks with four layers: input, pattern, summation, and output. A probabilistic function, such as the Gaussian, is used for each pattern node. The network weights are updated according to the input patterns. The patterns are then classified using the nearest-neighborhood function according to the Gaussian classifiers. The mean and variance for each node function can also be updated during training to minimize the distance between the patterns and their closest classifiers. More theoretical details of such networks can be found in [31].

@&#CONCLUSIONS@&#
