@&#MAIN-TITLE@&#
Man vs. machine: Investigating the effects of adversarial system use on end-user behavior in automated deception detection interviews

@&#HIGHLIGHTS@&#
We present adversarial systems as a novel/growing area of IS research.Knowledge of a deception system's operations increases countermeasure use.Presenting deceivers with relevant stimuli increases countermeasure use.Truth tellers use countermeasures when aware of the system's functionality.An extensive set of novel countermeasures is identified.

@&#KEYPHRASES@&#
Deception,Credibility assessment,Adversarial system,Countermeasures,Mandatory technology adoption,Concealed information test (CIT),

@&#ABSTRACT@&#
Deception is an inevitable component of human interaction. Researchers and practitioners are developing information systems to aid in the detection of deceptive communication. Information systems are typically adopted by end users to aid in completing a goal or objective (e.g., increasing the efficiency of a business process). However, end-user interactions with deception detection systems (adversarial systems) are unique because the goals of the system and the user are orthogonal. Prior work investigating systems-based deception detection has focused on the identification of reliable deception indicators. This research extends extant work by looking at how users of deception detection systems alter their behavior in response to the presence of guilty knowledge, relevant stimuli, and system knowledge. An analysis of data collected during two laboratory experiments reveals that guilty knowledge, relevant stimuli, and system knowledge all lead to increased use of countermeasures. The implications and limitations of this research are discussed and avenues for future research are outlined.

@&#INTRODUCTION@&#
A vital consideration of information systems research is the growing use of mandatory systems. These systems have the capacity to measure user behavior without the express consent or instigation of the user. Traditional technology adoption research has been conducted from the perspective that use is voluntary and focused on a reward or positive outcome for the user. Primarily, this research has focused on systems interaction contexts in which users want to use the system to help them accomplish certain tasks, or make them more effective in their work [1–3]. Prior efforts have focused on user perceptions of system usefulness, ease of use, job relevance, image, output quality, computer self-efficacy, perception of external control, computer playfulness, enjoyment, and usability [4]. Most system interactions today are of this type—voluntary and reward-focused [5,6]. While some research has looked at the involuntary adoption of systems, the outcome was still focused on task effectiveness and the ability of the system to improve overall organizational effectiveness [7,8]. However, many of these factors are not relevant to interactions with systems in which the interaction is compulsory (e.g., a full-body scanner at an airport), and could result in a punitive outcome for the user (e.g., being detained at the airport). Systems of this nature, hereafter referred to as adversarial systems, introduce a new context of research where users are placed in situations in which they must interact with the system, have no control over the data that are collected, and could be subject to a punitive outcome (see Table 1for definitions of key terms used in this paper).Deception detection is one context in which a user and a system may be working in opposition [9]. In this context, the human–computer interaction principle of a system supporting the user—or the system and user complementing one another [13]—is violated. Traditional computer-aided deception detection often includes the use of a polygraph device coupled with accompanying sensors to aid in determining the veracity of a person's statements. A polygraph device requires the direct measurement of a person's heart rate, skin conductance, respiration, and blood pressure by a trained polygraph examiner [14]. This process is expensive, obtrusive, and not easily scalable to a large number of interactions. A growing body of information systems research addresses the development of computing devices that will permit deception detection to be automated, unobtrusive, cost effective, and potentially more accurate and scientifically valid [15–20]. A system capable of conducting automated deception detection interviews has the potential to be utilized in any number of government or organizational contexts and applications. These include employment screening and the identification of insider security threats, a key concern of information security researchers [21–25]. Despite recent progress in the development of deception detection systems, several elements of users' interactions with such systems have yet to be investigated. Specifically, three key areas related to user behavior with adversarial systems that information systems researchers have yet to address include: (1) the impact of guilty knowledge, (2) the impact of relevant stimuli being presented during the interaction and (3) the impact of increasing a user's knowledge about the system.First, deceptive users working to avoid detection by a deception detection system would perceive the system to be adversarial. Accordingly, users would likely attempt to mitigate the system's effectiveness by altering their behavior to appear innocent. Actions taken to mitigate the effectiveness of a detection system are called countermeasures. The practice of using countermeasures to appear innocent has been witnessed and studied extensively in polygraph examinations [26]. Extant work on countermeasures has been limited to the investigation of (a) countermeasures employed against the polygraph [27], and (b) the impact of traditional polygraph countermeasures on newly developed information systems designed to detect deception [9]. Most deception indicators targeted by new deception systems are different from those targeted by the polygraph; accordingly, researchers must explore novel ways in which users will manipulate their behavior to appear truthful.Second, all forms of deception detection interactions require the selection of questions or stimuli that will elicit deception indicators from users. Even the most valid deception interaction formats can be difficult to administer due to limitations in selecting relevant questions/stimuli to be used during the interaction [28]. Research is needed to explore how a lack of relevant stimuli during a deception detection interaction will influence countermeasure use.Third, deception researchers developing new systems often conduct studies in which participants have no concept of the purpose of the system or any concept of its operations [15–17]. This limits ecological validity as real-world users—especially those with a vested interest in deceiving the system—would have a substantial amount of knowledge about the functionality of a real-world system when it is deployed for use. We see this currently with the polygraph, with widely available resources teaching how to “beat” a polygraph examination. Understanding how increased system knowledge will affect countermeasure use warrants further investigation.This research investigates variations in behavior that occur when users interact with an adversarial deception detection system. These variations are manifested as countermeasures. The use of countermeasures is predicted to vary in response to the following three manipulations: (1) the presence or lack of guilty knowledge in system users, (2) the system's inclusion or omission of relevant stimuli during the interaction, and (3) the user being aware or unaware of the capabilities/functionality of the system. This research contributes to existing knowledge by demonstrating that there are substantial differences in the way users interact with adversarial deception detection systems based on a presence or lack of: guilty knowledge, relevant stimuli, and system knowledge. The remainder of the paper is organized as follows. First, we discuss relevant literature. Next, we specify hypotheses based on relevant theory. Third, we outline the methodology used for two data collections. Fourth, we provide an analysis of the data and discuss the implications of our work. Finally, we present limitations and avenues for future research.

@&#CONCLUSIONS@&#
