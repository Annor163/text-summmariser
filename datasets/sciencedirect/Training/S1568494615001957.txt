@&#MAIN-TITLE@&#
Distributed HS-ARTMAP and its forecasting model for electricity load

@&#HIGHLIGHTS@&#
This paper proposes a new network named dHS-ARTMAP based on ARTMAP.The method reduces proliferation problems, retains stable memories, and enjoys noise tolerance.dHS-ARTMAP model achieves excellent results compared with other models.

@&#KEYPHRASES@&#
Distributed hyper-spherical ARTMAP (dHS-ARTMAP) model,Electricity load,Forecast,

@&#ABSTRACT@&#
Intelligence technology develops quickly to predict and respond to the actions of electric power users to maintain a reliable and secure electricity infrastructure. This paper proposed a new on-line training network called distributed hyper-spherical ARTMAP (dHS-ARTMAP) to forecast the electricity load. The new model constructs a more compact network structure and largely decreases the proliferation problem that Fuzzy ARTMAP models usually encounter. Experiments of short-term electricity load forecasting are made with the data from Queensland, Australia. Results are compared with other methods. The effectiveness of the dHS-ARTMAP network proves itself a promising alternative to put into practical use.

@&#INTRODUCTION@&#
As a booming developing field of applied science concerned with green energy resources, the smart grid employs advanced products and services together with intelligent monitoring, control, communication, and self-healing technologies to maintain a reliable and secure electricity infrastructure. One of its significant functions is to predict and intelligently respond to the behavior and actions of users concerned with energy, especially electric power [1].Electricity load prediction plays an important role for power plant operators, power suppliers and traders, and energy managers in decision making. But the overall load connected to the power grid varies significantly over time. Considering this characteristic of unstableness, some spare generators are put on a dissipative standby mode to respond to a rapid increase in power consumption, along with inevitable energy waste. Therefore, short-term electricity load forecasting is essential in arranging the plan of purchasing electricity and establishing the way of operation for the dispatching and planning department. Technologies matches electricity production with demand, making generators (which can start, stop and operate efficiently at chosen load, independently of the others) suitable for base load and peaking power generation [2]. Using mathematical forecasting model, it is possible to predict how many standby generators need to work, to reach a certain failure rate, to accommodate intermittent generation and storage, to acquire higher efficiency of the system. Various techniques have been proposed to make prediction of short-term electricity load. The main models include statistical models such as time-series approaches, ARMA model, Kalman filtering, Grey model, etc. [3–5]. These approaches are overshadowed by neural networks and neuro-fuzzy models in that intelligence technology is able to learn complex and nonlinear relationships between load and influence factors that statistical methods can hardly handle [6,7]. And many models are created to predict electricity load using neural intelligent techniques like hierarchical neural model, Anfis predictor, and new computational intelligence scheme [8–10], etc. Martínez-Álvarez et al. presented a new approach to find outliers and forecast the behavior of time series based on similarity of pattern sequences with the electricity-related data from Australia [11,12]. But these methods belong to off-line applications and do not have sufficient adaptive capability to accommodate time-varying dynamic effects. To overcome such defect, some approaches are developed to support on-line learning. For instance, Angelov et al. proposed some evolving schemes based on the Takagi-Sugeno fuzzy model (eTS) for control and classification applications [13,14]. They construct the cluster structures according to a potential measurement and update the consequent linear parameters by least squares estimate (LSE) algorithm. Problems are that the spreads and centers of the newly generated clusters cannot be updated properly.The ART (adaptive resonance theory) neural networks was first introduced as a self-organizing, parallel clustering and unsupervised pattern recognition machine [15]. Such structure converges faster and learns stably. The patterns which have already been trained wont be lost when new patterns come [16,17]. Fuzzy adaptive resonance theory map (Fuzzy ARTMAP) makes it possible to perform learning tasks with real-valued patterns and realize the mapping function [18,19]. But ART-based networks often suffer from the category proliferation problem under noisy conditions. The Gaussian ARTMAP was proposed to deal with this problem [20]. Mu-Chun Su described a simplified structure called the hyper-spherical ARTMAP (HS-ARTMAP), which is a synthesis of an RBF-network-like module and an ART-like module [21]. This network uses the hyper-sphere basis function to accommodate the problem of desired outputs maintaining constant values in certain regions that the Gaussian basis functions may encounter. It performs well for the function approximation application in highly noisy conditions. Different from traditional ART family, the distributed ARTMAP introduced by Carpenter does not adopt the winner-takes-all strategy to maintain stability [22]. There are also some combined models, the fuzzy ART and ARTMAP neural network could reduce the processing time in load forecasting with the help of fuzzy technique [23]. Cai et al. combined distributed ART network and hyper-spherical ARTMAP neural network to reduce failure rate for load forecast [24].In this paper, we develop a new network named the distributed hyper-spherical ARTMAP network (dHS-ARTMAP) to improve the HS-ARTMAP network. The HS-ARTMAP network has the incremental learning capability, also can perform well for approximation applications in highly noisy conditions. However, a large portion of neurons are produced to complete the category match. That is to say, the structure of the HS-ARTMAP is complex and redundant, which tends to cause proliferation problem. The dHS-ARTMAP model aims at solving this problem and acquiring high accuracy of the short-term electricity load forecasting. This specific network is modified in the critical map field of the algorithm by complicating the winner-takes-all strategy, we call it distributed match. In the cluster structure, contributions to the results come from more categories than a single one, and the number of such categories was reduced one by one until the output reached the desired precision during the distributed match process. If there was only one category that had not been disabled at last, the forecasting process turned to the mode used in the general HS-ARTMAP network where the winner-takes-all strategy was executed. The dHS-ARTMAP not only guarantees stable memories even with fast on-line learning, but also contains memory compression and noise tolerance capabilities. Most important, the structure of the network is more compact, reducing the proliferation problem to a large extent. Observed from experiments, the redundant structure shrinks and the prediction application is more stable in highly noisy condition.This paper is organized as follows. A detailed introduction of the dHS-ARTMAP network is presented in Section 2, including its configuration and training process. In Section 3, an experiment of electricity load forecasting are described and forecasting performance is evaluated. Finally, the main contributions of this paper and the future research are in Section 4.Taking into consideration the properties and shortcomings of the fuzzy ARTMAP, Mu-Chun Su introduced the HS-ARTMAP neural network which is incrementally constructed and capable of learning new information without forgetting old knowledge. We propose the dHS-ARTMAP that improves the map field of the HS-ARTMAP to reduce the proliferation problem and raise the accuracy of prediction. The configuration of the dHS-ARTMAP synthesizes an RBF-network-like module and an ART-like module.An RBF network is made up of a competitive layer and a category layer. A transforming function map ℜnto ℜ through(1)y=∑j=1Jvjφj(x,cj,rj),where x=(x1, …, xn)Tis an n-dimensional data pattern,vjis the connection weight between the hidden nodes j and the output node, J is the number of hidden nodes, φj(x, cj, rj) is RBF, cjand rjare the adjustable parameters of the RBF, and y is the output of the network. The generally used Gaussian basis functions may not perform well when the desired outputs have abrupt changes or constant values in certain regions. Thus the hyper-spherical basis function is used to handle the constant-values problem. This function is defined as(2)φj(x,cj,rj)=gmax−gj(x)α+(gmax−rj),where(3)gmax=∑i=1n(Mi−mi),(4)gj(x)=rjif||x−cj||≤rj,x−cjif|x−cj||>rj,where Miand miare the maximum value and the minimum value of the ith input pattern. α is a small parameter defined to select the hyper-sphere with the smallest rjwhen the data pattern simultaneously falls inside several hyper-spheres. Respectively, cjand rjare regarded as the center and the radius of a hyper-sphere, HSj, defined by the equation ||x−cj||=rjin the training procedure.In the RBF-network-like module, the hyper-spherical basis function is used as the transforming function in the competitive layer. If a neuron's output is the maximum of all, it is said that this neuron wins the competition. During the training process, the category layer holds the labels according to the winner. Corresponding to the winners (the kth neuron in the ART-like module and the jth neuron in the competitive layer), the connection weight,wj,k, between the jth node in the competitive layer and the node, Ck, in the category layer is set to be unity. Otherwise, there would be no connection between these two nodes. In this process, the connection between the neuron in the category layer and the competitive layer is a one-to-many relationship, and the connection stays unchanged after it is built. These connection weight vectors are then used to compute the final output of the network in the weighted summation module to make a prediction.The ART-like module aims at grouping the desired outputs into discrete clusters incrementally according to a pre-specified similarity threshold. It is similar to the traditional ART network, but simplifies the ART network to a large degree. In the ART-like module, each output is associated with an m-dimensional connection weight vector, which stands for a cluster center. The following part describes how the ART-like module works. When the first pattern, d1, is presented to the network, a neuron is generated and its connection weight vector, λ1, is initialized to be d1. In other cases, when the ith input pattern, di, comes, the winning neuron or category is chosen by the minimum distance criterion [21]:(5)k*=argmink=1...K||di−λk||,where K is the number of current neurons. If there is more than one active category, the first choice is the one with the smallest index. If the following condition is satisfied:(6)|||di−λk*|<θd,where θdis a pre-specified similarity threshold called the vigilance parameter, and represents the amount of the mean square error expected for a well-trained network. Then, the pattern, di, can be assigned to the k*th neuron and its connection weight vector updated as follows [21]:(7)λk*=Nk*Nk*+1λk*+1Nk*+1di,whereNk*represents the current number of patterns assigned to the k*th neuron. This equation is a recursive form to compute the mean vector fromNk*+1patterns. The value ofNk*is also need to be updated(Nk*=Nk*+1). If the condition is not satisfied, new neuron is generated to include the desired output pattern. Then the connection weight vector of the new neuron, λK+1, can be identified as di, and the number of neurons is raised by one (K=K+1).The structure of HS-ARTMAP network is shown in Fig. 1. It was derived from the Fuzzy-ARTMAP: an RBF-like module takes the place of ARTa, an ART-like module takes the place of ARTb, and a weighted summation module takes the place of the map field. The map field omits the match tracking process, which, instead, calculates an estimated output from more than one neuron in the weighted summation module. During supervised learning, an input pattern x=(x1, …, xn)Tis presented to the competitive layer of the RBF-network-like module, while the desired output pattern d=(d1, d2, …, dn)Tis presented to the ART-like module. After the input patterns are processed through the competitive layer, neurons of this layer will compete with each other, and the winner will respect the input pattern. At the same time, the ART-like module groups the desired output patterns into clusters incrementally according to a pre-specified similarity parameter. The current desired output pattern is assigned to a single cluster, namely choosing the winner. Then, the winner in the ART-module and the winner in the competitive layer lead the connection weight, connecting the winner in the competitive layer and the winner node in the category layer to unity.The schematic process of the training algorithm is summarized as follows.Step 1: Initialize the baseline vigilance ρb, the radius rb, parameters Δp and α. Initialize the current neuron number index (in the competitive layer) J to be zero.Step 2: Present a new input pattern x=(x1, …, xn)Tinto the RBF network-like module and a corresponding desired output pattern d into the ART-like module respectively. The neuron Ckwins the competition in the ART-like module.Step 3: If all the neurons are uncommitted in the competitive layer, go to Step 7 to generate a new neuron to respond to the new input pattern x. Or else, calculate the outputs of the committed neurons by the basis function in Eq. (2).Step 4: Through competition, find the node j* with the largest basis function output,(8)j*=argmaxj=1...Jφj(x),where J is the current number of neurons in the competitive layer. The j* neuron becomes active. Check whether the resonance criterion is satisfied:(9)φj*(x)≥ρb.If not, go to Step 7. Otherwise go to Step 5.Step 5: Check cases.Case 1||x−cj||≤rj*andwj*,k=1.Go to Step 2 for a new input pattern since the winners in two modules match each other. Here ||·|| is defined as:(10)x=∑i=1nxi21/2.Case 2||x−cj||≤rj*andwj*,k≠1.Letρj*=1in that the winning neuron does not match Ck. Go to Step 7 to generate a new neuron.Case 3||x−cj||>rj*andwj*,k=1, andρj*=1.Disable the winning neuron. Go to Step 4 and search for another winner.Case 4||x−cj||>rj*andwj*,k=1, andρj*≠1.Go to Step 6 and update the weights of the winning neuron.Case 5||x−cj||>rj*andwj*,k≠1.Slightly adjust the value of the vigilance parameter related to the j* neuron [21]:(11)ρj*=1Λ(ρj*+Δρ).Disable the winning neuron. Go to Step 4 and search for another winner.Step 6: Expanding the hyper-sphere according to the weights of the winner [21] to update the winner's weights:(12)cj*,i=xi+rj*+||x−cj*||2rj*(cj*,i−xi).For 1≤i≤n,(13)rj*=rj*+||x−cj*||2.Go to Step 2.Step 7: Generate a new neuron. Initialize the parameters of the new neuron as Eq. (14) through Eq. (18)[21]:(14)ρJ+1=ρb,(15)cJ+1,i=xi,For 1≤i≤n,(16)rJ+1=rb,(17)wJ+1,i=1ifi=k,0ifi≠k,(18)J=J+1.Learn the subsequent pattern as in Step 2.For convenience, we put the flowchart here so that readers can compare with that of dHS-ARTMAP (Fig. 2).In general fuzzy ART, among those that pass the match test, only one category is selected as the corresponding result following the winner-takes-all rule. It requires an excess of neurons to correspond to a pattern, which lead to a complex network. Nevertheless, in most cases of real-time system, an input pattern is related to some categories rather than one category. In other words, from the level of classification, groups which things belong to are often ambiguous. One group can cover some parts of the characteristics, but some characteristics may be obvious in other groups. So the first phase of this predictor is to find groups related to the current data most. Then the idea from the distributed ARTMAP was borrowed to construct a training algorithm turning from the multi-category responding process to the winner-takes-all process in the map field. In this way, unnecessary nodes are omitted. Meanwhile, the output would be closer to the real value. The configuration of the dHS-ARTMAP network is the same as the HS-ARTMAP network, but the training algorithm in the map field was modified. When an input pattern was presented, some categories were selected if they passed the match test (which means they relate to the current data to a certain level), and a prediction was made according to these categories with the well-trained network. Since high precision was the principal focus of forecasting, the desired precision was set as a vigilance parameter in the match tracking process of the distributed training. If the precision achieved the expected value, parameters were adjusted. In other cases, selected categories with the least impact were disabled one after another until there was only one category left. Only at this time, the common HS-ARTMAP training algorithm was adopted. This process is defined as the winner-takes-all training process.At the very beginning of the training algorithm, no committed neuron exists in the competitive layer. Neurons are added into the network incrementally with the increase of input training patterns for the correct predictions. The schematic process of the training algorithm of dHS-ARTMAP is summarized as follows.Step 1: Initialize the baseline vigilance ρb, the radius rb, and vigilance parameterP˜(desired precision). Specify the values of the parameters Δρ and α. Initialize the current neuron number index (the competitive layer) J to be zero.Step 2: Present a new input pattern x into the RBF network-like module and a corresponding desired output pattern d into the ART-like module respectively. The neuron Ck(of which the value is the actual value) wins the competition in the ART-like module. The actual value is denoted by act.Step 3: If all neurons are uncommitted in the competitive layer, then generate a new neuron as Eq. (14) through Eq. (18) to respond to the new input pattern x. Then go to Step 2. Otherwise, calculate the outputs of the committed neurons in the competitive layer using basis function Eq. (2). Let(19)φ¯=mean{φj},j=1,2,…,Jand(20)ρ=ρ0⋅φ¯,where ρ0 is an adaptive constant. Choose nodes with outputs passing the match test as the candidate categories. A candidate set is defined as(21)Scand={i:φi>ρ}.Let s denotes the number of candidates in this set.Step 4: (Distributed training process) make a prediction according to all the candidates with the equation(22)forecast=∑j∈Scand∑k∈SARTwj,kφj(x)∑j∈Scandφj(x),where(23)SART={k:wj,k=1andj∈Scand}.Then calculate the precision of the prediction value:(24)pre=forecast−actact.Case 1pre≤P˜.Go to Step 5 to update the weights.Case 2pre>P˜and s>1.Disable the minφ neuron in the candidates set to get higher forecast accuracy, where(25)minφ=argminj∈Scand{φj}.Update the candidates set by deleting the minφ candidate, and s=s−1. Then go to Step 3.Case 3pre>P˜and s=1.Let j* denote the rest of the candidates. This case indicates that the training algorithm failed in the distributed training process and algorithm should turn to the winner-takes-all process. Here j* is the active node with the largest basis function output in Step 4 in the HS-ARTMAP training algorithm. The next procedures refer to the HS-ARTMAP training algorithm.Step 5: Choose candidates that are subject to(26)||x−cj*||≤rj*Update the candidates’ weights by expanding the hyper-spheres(27)cj,i=xi+φj(x)∑k∈Scandφk(x)⋅rj+||x−cj||2rj(cj,i−xi),for 1≤i≤n, j∈Scand(28)rj=xi+φj(x)∑k∈Scandφk(x)⋅rj*+||x−cj||2,Then go to Step 2.The flowchart of this training algorithm is presented in Fig. 3.In this part, the testing mode is depicted to testify the training results. In the fuzzy ARTMAP system, the network's output is determined only by the winner in the competitive layer. Here, neurons (more than one) that have higher responses to the input pattern, contribute to the network's outputs through the weighted summation module. The network computes the output as(29)forecast=∑j∈Scand∑k∈SARTwj,kφj(x)∑j∈Scandφj(x),where(30)Scand={j:φj>ρ},(31)SART={k:wj,k=1andj∈Scand},The approximation accuracy depends on the value of the parameter ρ: the larger ρ is, the less the neurons responsible for the outputs are. This method lowers the sensitivity of the network under noisy conditions. Besides, this equation implies that contributions to the forecasting result are proportionate to the RBF outputs.For the purpose of assessing the effectiveness of the proposed model, the mean square error (MSE), the mean absolute error (MAE), and the mean absolute percentage error (MAPE) are used for model evaluation and model comparison. If Wrealis the actual observation value and Wforecastis result of the model, then the errors are defined as(32)e=Wreal−Wforecast,(33)MSE=1n∑i=1nei2,(34)MAE=1n∑i=1n|ei|,(35)MAPE=1n∑i=1nWreal−WforecastWreal⋅100%,where n is the total number of days in which the forecast is made in the test. Smaller values of these measures indicate more accurate forecasting results.In order to ensure the statistical significance of the results comparison, as Diebold and Mariano suggested, we took tests of the null hypothesis of equal forecast accuracy of competing forecasts respectively through methods including the Sign Test and Wilcoxon's Signed-Rank Test [25].Consider two forecasts,{yˆit}t=1Tand{yˆjt}t=1T, of the time series{yt}t=1T. Let the associated forecast errors be{eit}t=1Tand{ejt}t=1T. We allow the time-t loss associated with a forecast (say i) to be an arbitrary function of the realization and prediction,g(yt,yˆit). The null hypothesis of equal forecast accuracy for two forecasts is E[g(eit)]=E[g(ejt)] or E[dt]=0, where dt=E[g(eit)−g(ejt)] is the loss differential.The null hypothesis is a zero median loss differential: med(g(eit)−g(ejt))=0. Assuming that loss differential series is iid (and we shall relax that assumption shortly), the number of positive loss-differential observations in a sample of size T has the binomial distribution with parameters T and 1/2 under the null hypothesis. The test statistic is therefore simply(36)S1=∑t=1TI+(dt)where(37)I+(dt)=1ifdt>0,0otherwise.Significance may be assessed using a table of the cumulative binomial distribution. In large samples, the studentized version of the sign test statistic is standard normal:(38)S1a=S1−0.5T0.25T∼N(0,1)We assume for the moment that the loss differential series is iid. The test statistic is(39)S2=∑t=1TI+(dt)Rank(|dt|),the sum of the ranks of the absolute values of the positive observations. Its studentized version is asymptotically standard normal,(40)S2a=S2−(T(T+1)/4)(T(T+1)(2T+1)/24)∼N(0,1).In this section, two experiments of multiple-step-ahead short-term electricity load forecasting are carried out using the new proposed method with data series from Queensland, Australia. The two series of electricity load data were collected every half hour in the first 2 months of 1999 and 2000 respectively. There are 48 load values (one period) every day.For every experiment, the data set is partitioned into a training set and a testing set. During the forecasting stage, the inputs include historical data only. The formula of each input vector (on the training or forecasting) is(41)Wy(d)=(Wy(d−1),…,Wy(d−p))where Wy(d−p) stands for the electricity load value of the pth half hour before the time to be predicted. The number of points to be forecast is 48, which is the load value of the next day from 0:00 AM to 23:30 PM.According to the model, the value of p determines forecasting results to a large degree. Allowing for the real situation, the results corresponding to different p values (ranging from 1 to 50) are tested through simulation with the data of 1999. Initial parameters of the neural network are presented in Table 1. The experiment is made by using MATLAB R2010a. Forecasting results are listed in Table 2. In Table 2, it is clear that the forecasting results differ from each other when p changes. For this reason, choosing a proper value for parameters is very necessary and important. New model achieves relatively higher precision when the value of p is between 5 and 17. Here we choose p=12.For the first experiment, with the data set of 1999, we compare our model with the HS-ARTMAP model and the BP model. Forecasting results are shown in Table 3. Table 4displays the detailed forecasting errors of each model. Fig. 4gives a more noticeable observation. It can be observed that the MAPE, MSE and MAE out of the dHS-ARTMAP are the smallest of all three methods. Fig. 4 suggests that the modified model has exceptional agreement with the actual load value. What is more, it can be seen from Table 4 that the number of neurons in the hidden layer in the dHS-ARTMAP model is 2158 less than that in the HS-ARTMAP model. In other words, this growing structure creates a sufficient and economical number of or hidden units in the training process. For this reason, it can be concluded that the dHS-ARTMAP avoids category proliferation in the complex system.Another experiment is conducted with data of 2000. The forecasting values of the three methods are presented in Table 5, the error comparison in Table 6. Fig. 5depicts the performance by image. In this set of results, although some forecasting points are not fitting as well as those of the HS-ARTMAP network, the forecasting results of the dHS-ARTMAP network are fairly well at other points. It is evident that the error rates are smaller than those of the HS-ARTMAP network and the BP model. The number of the hidden units reduces significantly from 2820 to 662. The structure of the dHS-ARTMAP network is much more simple and compact than that of the HS-ARTMAP network. On the whole, these conclusions verify the high precision and efficiency of the dHS-ARTMAP model.Then, we use the Sign Test and Wilcoxon's Signed-Rank Test to identify the statistical significance of competing forecasts. Test results are shown in Table 7, in which ‘1’ represents ‘accepting the hypothesis’ and ‘0’ ‘refusing’. Thus, for the sample at hand, we do not reject at conventional levels the hypothesis of equal expected error, i.e. the dHS-ARTMAP model is not a statistically significantly worse predictor of the electricity load forecast than the HS-ARTMAP. And it performs significantly better than the BP model.In sum, the dHS-ARTMAP performs better than the HS-ARTMAP model and the BP model in load prediction. Besides, this on-line training model not only maintains stable memories under highly noisy condition, but also has compact structure which decreases the proliferation problem.

@&#CONCLUSIONS@&#
On account of the essential part which electricity load prediction plays for power plant operators, power suppliers and traders, and energy managers, a new on-line training neural network, the distributed hyper-spherical ARTMAP (dHS-ARTMAP), was proposed to provide more reliable forecasting results in this paper. In the new model, distributed match process in the map field makes it possible to largely compress the structure of the network so that to get succinct and compact hidden layer, and solve category proliferation problem that the HS-ARTMAP model and Fuzzy ARTMAP networks usually encounter. In the forecasting application, our model presented an excellent performance for the data of Queensland compared with the HS-ARTMAP model and the BP model. It can quickly capture the system's dynamic behavior and track the system's characteristics accurately under noisy condition.The dHS-ARTMAP model can be regarded as a reliable forecasting tool in large systems dealing with electricity load. This system may also benefit the studies of diverse fields such as new energy resources, etc. More work is under way to extend the proposed model for predictions of complex systems in the future.Future work is focused on dependence of parameter p on different data sets to improve the efficiency of distributed match process.