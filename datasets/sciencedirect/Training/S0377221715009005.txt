@&#MAIN-TITLE@&#
CVaR (superquantile) norm: Stochastic case

@&#HIGHLIGHTS@&#
CVaR(|X|) is a norm in the space of random variables.CVaR norm as a Measure of Error is related to a Regular Risk Quadrangle.Dual norm for CVaR norm is the maximum of L-1 and scaled L-infinity norms.Trimmed L1-norm is an analog of L-p for p < 1.Linear regression problems were solved by minimizing CVaR norm of residuals.

@&#KEYPHRASES@&#
CVaR norm,L-p norm,Superquantile,Risk quadrangle,Linear regression,

@&#ABSTRACT@&#
The concept of Conditional Value-at-Risk (CVaR) is used in various applications in uncertain environment. This paper introduces CVaR (superquantile) norm for a random variable, which is by definition CVaR of absolute value of this random variable. It is proved that CVaR norm is indeed a norm in the space of random variables. CVaR norm is defined in two variations: scaled and non-scaled. L-1 and L-infinity norms are limiting cases of the CVaR norm. In continuous case, scaled CVaR norm is a conditional expectation of the random variable. A similar representation of CVaR norm is valid for discrete random variables. Several properties for scaled and non-scaled CVaR norm, as a function of confidence level, were proved. Dual norm for CVaR norm is proved to be the maximum of L-1 and scaled L-infinity norms. CVaR norm, as a Measure of Error, is related to a Regular Risk Quadrangle. Trimmed L1-norm, which is a non-convex extension for CVaR norm, is introduced analogously to function L-p for p < 1. Linear regression problems were solved by minimizing CVaR norm of regression residuals.

@&#INTRODUCTION@&#
The concept of Conditional Value-at-Risk (CVaR) is widely used in risk management and various applications in uncertain environment. This paper introduces a concept of CVaR norm in the space of random variables. CVaR norm inRnwas introduced in (Pavlikov & Uryasev, 2014) and developed in (Gotoh & Uryasev, 2013), and is a particular case of general error measures introduced and developed by Rockafellar, Uryasev, and Zabarankin (2008). The term “superquantile”, free from dependence on financial terminology, can be used as a neutral alternative name for “CVaR”, like it was done in Rockafellar and Royset (2010); Rockafellar and Uryasev (2013). For the similar reason the alternative name “superquantile norm” is proposed to replace “CVaR norm” when desired. For the sake of consistency within the paper and with the earlier study (Pavlikov & Uryasev, 2014), this paper will use mostly the “CVaR norm” term.This section provides a short introduction in the CVaR norm inRnand shows the relation with the CVaR norm in the space of random variables. This paper is motivated by applications of norms in optimization. We consider norms inRnand in the space of random variables. We use symbols x and xifor a vector and an ith vector component inRn,i.e.x=(x1,…,xn). We use symbol X for a random variable.lpnorms are broadly used inRn,and Lpnorms are considered in the space of random variables. For p ∈ [1, ∞], the norms lpand Lpare defined as follows11Note that the classic definition for lpnorm islp(x)=(∑i=1n|xi|p)1/pdoes not satisfy inequality lp(x) ≤ lq(x) for p < q. This paper uses an equivalent scaled version of this normlp(x)=(1n∑i=1n|xi|p)1/p,which satisfies that inequality. Lpnorm is commonly defined asLp(f)=∥f∥p≡(∫S|f|pdμ)1/p,where S is a considered space. It is known (e.g. Brezis, 2010, page 118) that for ‖ · ‖pand ‖ · ‖qnorms inequality∥f∥p≤μ(S)1p−1q∥f∥qholds for 1 ≤ p ≤ q ≤ ∞, where S is a considered space and μ(S) is the measure of the space S. When S is a probability space,μ(S)=1and inequality Lp(X) ≤ Lq(X) holds for 1 ≤ p ≤ q ≤ ∞, whereLp(X)=(E|X|p)1/p.:lp(x)=(1n∑i=1n|xi|p)1/p,Lp(X)=(E|X|p)1/p,where E is the expectation sign. The most popular cases arep=1,2,∞,i.e.,•l1(x)=1n∑i=1n|xi|,L1(X)=E|X|;l∞(x)=maxi=1,…,n|xi|,L∞(X)=sup|X|;l2(x)=(1n∑i=1nxi2)1/2,L2(X)=(EX2)1/2.It is known that l1(x) ≤ l2(x) ≤ l∞(x) and L1(X) ≤ L2(X) ≤ L∞(X), which follow from lp(x) ≤ lq(x) and Lp(X) ≤ Lq(X) for p < q, see (e.g. Brezis, 2010, page 118).The other family of norms, CVaR norm forRnwas defined in Pavlikov and Uryasev (2014) and studied in Gotoh and Uryasev (2013). According to Pavlikov and Uryasev (2014, Definition 3), the non-scaled CVaR norm with parameter α, or ⟨⟨x⟩⟩α, is defined onx∈Rnas a sum of absolute values of biggestn(1−α)components. Ifn(1−α)is not an integer, then ⟨⟨x⟩⟩αis defined as a weighted average of two norms〈〈x〉〉α1and〈〈x〉〉α2for closest values α1, α2 such thatn(1−α1)andn(1−α2)are integers.A similar norm, called D-norm, was introduced in Bertsimas, Pachamanova, and Sim (2004, Section 3) in a different way. D-norm is defined as a maximum of a sum of weighted absolute values of vector components. The maximization is performed over all sets of indexes for components in the sum, with a constraint on cardinality. Forα∈[0,n−1n],the CVaR norm ⟨⟨x⟩⟩αcoincides with the D-norm |||x|||pwith parameter p defined byp=n(1−α),see Pavlikov and Uryasev (2014, Proposition 3.4). (Bertsimas et al., 2004, Proposition 2) find a dual norm to D-norm; this result was generalized with Item 2 of Proposition 2.1 of this paper for the stochastic case.Both CVaR norm and D-norm can be viewed as important special cases of Ordered Weighting Averaging (OWA) operators, see Merigó and Yager (2013); Torra and Narukawa (2007); Yager (2010). A subfamily of OWA operators with monotonically non-increasing weights, when implied to the absolute values of the vector, were formalized as norms in Yager (2010). The worst-case averages, corresponding to CVaR, were also studied, e.g., in Ogryczak and Zawadzki (2002); Romeijn, Ahuja, Dempsey, and Kumar (2005).The paper (Pavlikov & Uryasev, 2014, Definition 1) has also defined scaled CVaR norm〈〈x〉〉αS. Scaled version calculates average value of components instead of sum:n(1−α)〈〈x〉〉αS=〈〈x〉〉α. This paper defines (scaled) CVaR norm of a random variable X as an expectation of |X| in its right(1−α)-tail. It can be shown that proposed norm is a generalization of〈〈x〉〉αSin a following way. Consider mappingX(x):Rn→L1(Ω)from Eucledian space of dimension n to the space of L1-finite random variables. Denotex=(x1,…,xn). Let X(x) be discretely distributed over atomsx1,…,xnwith equal probabilities1n. Then it is easy to see that〈〈x〉〉αS=CVaRα(|X(x)|)≡〈〈X(x)〉〉αS,see Pavlikov and Uryasev (2014, Definition 2).This paper also defines non-scaled CVaR norm〈〈X〉〉α=(1−α)〈〈X〉〉αS,which corresponds to ⟨⟨x⟩⟩αfrom Pavlikov and Uryasev (2014). Non-scaled version has attractive properties with respect to parameter α, see Items 6, 7 from Section 2.Risk Quadrangle considers riskR(X),deviationD(X),regretV(X),errorE(X)and statisticS(X),related with a set of equations, called The General Relationships, see Rockafellar and Uryasev (2013, Diagram 3). If a functional satisfies a corresponding set of axioms, it is called regular, see Rockafellar and Uryasev (2013, Section 3). It can be proved that ifR(X)is a coherent and regular Measure of Risk, thenR(|X|)is both a norm and a regular Measure of Error, however, this proof is beyond the scope of this paper. This paper proves that ⟨⟨X⟩⟩αis a regular Measure of Error and finds the corresponding functionsR(X),D(X),V(X)andS(X)in risk quadrangle related to the Measure of ErrorE(X)=〈〈X〉〉α,see Section 2.Item 3 from Proposition 2.1 can be viewed as a stochastic generalization of Hall and Tymoczko (2012, Lemma 1). Paper (Hall & Tymoczko, 2012) considers functions Σj(x) on nonnegative orthantR+n,corresponding toR+nreduction of special cases of CVaR norm. Paper (Hall & Tymoczko, 2012) relies on majorization theory, see (Marshall, Olkin, & Arnold, 2011), which is generalized for the stochastic case with a concept of stochastic dominance, see Ogryczak and Ruszczynski (2002); Dentcheva and Ruszczynski (2003, e.g.).This paper considers also non-convex functions closely related to CVaR norm. In deterministic case, by definition, CVaR norm is the average of biggest(1−α)nabsolute values of components of a vector. The trimmed L1-norm is also known as a trimmed sum of absolute deviations estimator for LTA regression, see (Hawkins & Olive, 1999; Bassett Jr, 1991; Hössjer, 1994, e.g.). L1-based LTA regression is introduced similarly to the more common L2-based Least-Trimmed-Squares (LTS) regression, see Zabarankin and Uryasev (2014b, e.g.). Trimmed L1-norm is denoted here by tα, it is the average of smallest αn absolute values of components of a vector. Calculation formulas and mathematical properties for tα(x) in Eucledian space and Tα(X) in the space of random variables are considered in Section 3. Trimmed L1-norm is also related to the sparse optimization, similar to functions lpfor 0 < p < 1, see Ge, Jiang, and Ye (2011). Note that the constraint on trimmed L1-norm directly specifies sparsity of the solution vector (see Item 6 of Section 3), compared to “indirect” sparsity specification with lpfunction. This paper also provides an illustration for tαin Euclidean space.Paper (Krzemienowski, 2009, Definition 2) defines conditional average CAVG function. Both average quantile and CVaR are subfamilies of CAVG family, therefore, both〈〈X〉〉αSand Tα(X) are subfamilies of CAVGβ, γ(|X|) function family. Unfortunately, these functions are not convex or concave in general, and are out of the scope of this study, although robust regression applications based on these functions is a promising research direction.The paper is organized as follows. Section 2 gives a formal definition of CVaR norm in stochastic case and enlists various mathematical of CVaR norm, including that it is indeed a norm and a regular measure of error. CVaR norm is a parametric family of norms with respect to the confidence parameter α, properties of CVaR norm as a function of α are proved. Dual representation of the CVaR norm is derived, and a dual norm to the CVaR norm is defined. A short introduction to the concept of Risk Quadrangle is given. We derive the quadrangle related to the CVaR norm as a measure of error and we prove that this quadrangle is regular. Section 3 defines the trimmed L1-norm, both inRnand in the space of random variables, and enlists several basic properties. The trimmed L1-norm is an extension of CVaR norm, but it is not actually a norm. Section 4 illustrates properties of CVaR norm with a case study. Section 5 provides concluding remarks and acknowledgements.This section gives a formal definition of CVaR norm in stochastic case and proves various properties of the norm. Let us denote[x]+=max{0,x},[x]−=max{0,−x}. Consider cumulative distribution functionFX(x)=P(X≤x). If, for a probability level α ∈ (0, 1), there is a unique x such thatFX(x)=α,then this x is called the α-quantile qα(X). In general, however, the value x is not unique, or may not even exist. There are two boundary values:qα+(X)=inf{x|FX(x)>α},qα−(X)=sup{x|FX(x)<α}.We will call by the quantile the entire interval between the two boundary values,(1)qα(X)=[qα−(X),qα+(X)].We will use notation∫qp(X)dp≡∫qp−(X)dp,which is reasonable since∫qp+(X)dp=∫qp−(X)dp.The CVaR norm is defined as follows.Definition 1Let X be a random variable with E|X| < ∞. Then CVaR (superquantile) norm of X with parameter α ∈ [0, 1] is defined by〈〈X〉〉αS=CVaRα(|X|)=q¯α(|X|).Following the logic of (Pavlikov & Uryasev, 2014),〈〈X〉〉αSis called scaled CVaR norm, while Definition 2 introduces ⟨⟨X⟩⟩α, corresponding to non-scaled CVaR norm forRnin (Pavlikov & Uryasev, 2014, Definition 3). By default we call by CVaR norm the function〈〈X〉〉αS. The second version of the norm, non-scaled CVaR norm, is defined as follows.Definition 2Let X be a random variable with E|X| < ∞. Then non-scaled CVaR (superquantile) norm of X with parameter α ∈ [0, 1) is defined as follows:〈〈X〉〉α=(1−α)〈〈X〉〉αS.Note that by continuity〈〈X〉〉1≡limα→1〈〈X〉〉α=0. That is, forα=1,the function ⟨⟨X⟩⟩αis not a norm.Recall some properties of CVaR (e.g. Rockafellar & Uryasev, 2013, page 20), see also (Rockafellar & Uryasev, 2000; 2002):•Positive homogeneity:(2)CVaRα(λX)=λCVaRα(X),forλ>0.Subadditivity:(3)CVaRα(X+Y)≤CVaRα(X)+CVaRα(Y).Monotonicity:(4)CVaRα(X)≤CVaRα(Y),forX≤Y.Properties and representations of CVaR (superquantile) norm1.〈〈X〉〉0S=L1(X),〈〈X〉〉1S=L∞(X).(Follows from Definition 1).〈〈X〉〉αSis a norm onL1(Ω)space of random variables.(Norm positive homogeneity and convexity follow from the positive homogeneity and convexity of the CVaR and the absolute value ‖ · ‖, whereasCVaRα(|X|)=0⇔X≡0is obvious.)The representations(5)〈〈X〉〉αS=minc{c+11−αE[|X|−c]+}.(6)〈〈X〉〉αS=11−α∫α1qp(|X|)dp.(7)〈〈X〉〉αS=E(|X|||X|>qα(|X|)),ifXisacontinuousrandomvariable,follow directly from CVaR calculation formulas, see (e.g. Rockafellar & Uryasev, 2000, formulas (3),(5)), (e.g. Rockafellar & Uryasev, 2002, Definitions 3,4, Theorem 10). Also,(8)〈〈X〉〉αS=CVaR(1+α)/2(Y),whereY={X,withprobability12;−X,withprobability12;see the proof in Appendix A.Let X be a discrete random variable, i.e., it takes values{xi}i=1Nwith positive probabilities{pi}i=1N,where∑i=1Npi=1andN∈N∪∞(N also can be ∞). Let us denote by{|x|(i)}i=1Nan ordered sequence{|xi|}i=1N,i.e.,|x|(i)≤|x|(i+1).22Note that ordered sequence {|x|(i)} may not exist for some sets {xi}. As a counterexample, considerxi=2−i,fori=1,…,∞.We also denote by{|p|(i)}i=1Na corresponding to the{|x|(i)}i=1Nsequence of probabilities from the{pi}i=1N. Then(a)forα=1,〈〈X〉〉1S={|x|(N),forN<∞;limi→∞|x|(i),forN=∞;forαj=∑i=1j|p|(i)and j < N,j∈Z+≡N∪{0},〈〈X〉〉αjS={11−αj∑i=j+1N|x|(i)|p|(i),forN<∞;11−αj∑i=j+1∞|x|(i)|p|(i),forN=∞;forαj<α<αj+1,〈〈X〉〉αS=(1−λ)1−αj1−α〈〈X〉〉αjS+λ1−αj+11−α〈〈X〉〉αj+1S,whereλ=α−αjαj+1−αj.See the proof in Appendix A.〈〈X〉〉αSis a continuous increasing function of α.(Follows from the fact that CVaR is a continuous increasing function of α, see Rockafellar and Uryasev (2002, Proposition 13).)⟨⟨X⟩⟩αis a concave and decreasing function of α.(The integral∫α1qp(|X|)dpis a convex function of α as shown in Ogryczak and Ruszczynski (2002, page 6), and〈〈X〉〉α=E|X|−∫α1qp(|X|)dp.)⟨⟨X⟩⟩αand(1−α)CVaRα(X)are piecewise-linear functions of α for a discretely distributed random variable X.(Since quantile function for discretely distributed random variable is a step function, then its integral is piecewise-linear, and〈〈X〉〉α=E|X|−∫α1qp(|X|)dp.)The norm〈〈X〉〉αSgenerates a Banach space for α ∈ [0, 1].(From the definition of Banach space33A Banach space is a vector spaceXoverR,which is equipped with a norm ‖ · ‖ and which is complete with respect to that norm. By definition, completeness means that for every Cauchy sequence{xn}n=1∞inX(i.e., for every ε > 0 exists N such that∥xm−xn∥<ɛfor all m, n > N), there exists an element x inXsuch thatlimn→∞xn=x,i.e.,limn→∞∥xn−x∥=0.follows that sinceL1(X)=E|X|generates44We say that norm L generates space(XL,L),whereXL={X|L(X)<∞}.a Banach space andE|X|≤〈〈X〉〉αS≤11−αE|X|,then for α ∈ [0, 1) CVaR norm also generates a Banach space, and〈〈X〉〉αS=L∞(X)forα=1.)E(X)=〈〈X〉〉αSis a regular measure of error.See the proof in Appendix A.ForX∈L1(Ω)and the norm〈〈·〉〉αS,the following statements hold:1.〈〈X〉〉αS=supY∈YEXY,whereY={Y||Y|≤11−α,E|Y|≤1}={Y|EXY≤〈〈X〉〉αS}is a closed convex set.The norm〈〈Y〉〉αS*=max{E|Y|,(1−α)sup|Y|}is dual to the norm〈〈X〉〉αSfor α ∈ (0, 1).Problemmind∈R〈〈X−d〉〉αShas the following solution:argmind〈〈X−d〉〉αS=12(q(1−α)/2(X)+q(1+α)/2(X)),mind〈〈X−d〉〉αS=11−α(1+α2CVaR(1−α)/2(X)+1−α2CVaR(1+α)/2(X)−EX).1.Papers (Rockafellar, Uryasev, & Zabarankin, 2006), (Rockafellar & Uryasev, 2013, (6.9)) proved thatCVaRα(X)=supQ∈QEXQ,whereQ={Q|0≤Q≤11−α,EQ=1}.DenoteY={Y||Y|≤11−α,E|Y|≤1}andY*={Y|EXY≤〈〈X〉〉αS}. Then〈〈X〉〉αS=supQ∈QE|X|Q≤supY∈YEXY≤supY∈YE|X∥Y|≤supQ∈QE|X|Q.The first inequality holds since for anyQ∈QexistsY=(I(X>0)−I(X<0))Q∈Ysuch that|X|Q=XY. Hence,〈〈X〉〉αS=supY∈YEXY. Note that〈〈X〉〉αS=supY∈Y*EXY. Then,Y*is a closed convex hull ofY.Yis closed convex as an intersection of two closed convex sets{Y||Y|≤11−α}and {Y|E|Y| ≤ 1.}. That is,Y*=Y.Item 1 implies thatY={Y||Y|≤11−α,E|Y|≤1}is a unit ball for the dual norm〈〈Y〉〉αS*=supX≠0EXY〈〈X〉〉αS. Then, the unit sphere〈〈Y〉〉αS*=1for the dual norm is the set{Y|sup|Y|=11−α,E|Y|≤1}∪{Y|sup|Y|≤11−α,E|Y|=1}.Therefore, the dual norm equals〈〈Y〉〉αS*=max{E|Y|,(1−α)sup|Y|}.Note that for c ≥ 0 and arbitraryx,d∈R,we have[|x−d|−c]+={d−c−x,forx≤d−c;0,forx∈[d−c,d+c];≡[d−c−x]++[x−(d+c)]+=x−(d+c),forx≥d+c.=[x−(d−c)]++d−c−x+[x−(d+c)]+.Letc1=d−candc2=d+c,then this relationship yields(9)(1−α)c+[|x−d|−c]+=(1−α)c2−c12+[x−c1]++c1−x+[x−c2]+=1+α2c1+[x−c1]++1−α2c2+[x−c2]+−x.An optimal value c in〈〈X〉〉αS=minc∈R{c+(1−α)−1E[|X|−c]+}is the quantile qα(|X|) and hence nonnegative. Thus, c can be restricted in this optimization problem to be nonnegative, and, applying (9),mind∈R〈〈X−d〉〉αS=minc≥0,d∈R{c+(1−α)−1E[|X−d|−c]+}=11−αminc1,c2∈R{1+α2c1+E[X−c1]++1−α2c2+E[X−c2]+−EX}=11−α(1+α2CVaR(1−α)/2(X)+1−α2CVaR(1+α)/2(X)−EX),where optimal c1 and c2 are determined byq(1−α)/2(X)andq(1+α)/2(X),respectively, and yield optimald=12(q(1−α)/2(X)+q(1+α)/2(X)).□Risk Quadrangle (Rockafellar & Uryasev, 2013) relates riskR(X),deviationD(X),regretV(X),errorE(X)and statisticS(X)with the following equations (Rockafellar & Uryasev, 2013, Diagram 3):(10)V(X)=EX+E(X),R(X)=EX+D(X),(11)D(X)=minC{E(X−C)},R(X)=minC{C+V(X−C)},(12)S(X)=argminC{E(X−C)}=argminC{C+V(X−C)}.Following the paper (Rockafellar & Uryasev, 2013, Section 3), we consider theL2(Ω)space of random variables with finite second moment, EX2 < ∞, which implies finite first moment, E|X| < ∞. The natural (“strong”) convergence inL2(Ω)of a sequence of random variables Xkto a random variable X is characterized as follows:L2-limk→∞Xk=X⇔limk→∞L2(Xk−X)=0.The functionalFis closed if for anyC∈Rthe set{X|F(X)≤C}is closed with respect toL2-convergence. The functionalFis convex ifF(λX+(1−λ)Y)≤λF(X)+(1−λ)F(Y)for all X, Y and λ ∈ (0, 1).Measure of errorE(X)is regular if: 1)E(X)∈[0,∞]; 2)E(X)is closed convex; 3)E(0)=0; 4)E(X)>0for any X ≠ 0; 5)E(X)≥ψ(EX)with a convex function ψ on(−∞,∞)havingψ(0)=0but ψ(t) > 0 for t ≠ 0. Definitions for regular measures of risk, deviation and regret are available in Rockafellar and Uryasev (2013, Section 3). The quadrangle(R,D,E,V,S)is regular if Eqs. (10)–(12) hold and if alsoR(X)is a regular measure of risk,D(X)is a regular measure of deviation,V(X)is a regular measure of regret, andE(X)is a regular measure of error.(Rockafellar & Uryasev, 2013, Quadrangle Theorem) implies that if Eqs. (10)–(12) hold for functionsR,D,E,V,S,and if alsoE(X)is a regular measure of error, then(R,D,E,V,S)is a regular quadrangle. Since〈〈X〉〉αSis a regular measure of error, then ⟨⟨X⟩⟩αis a regular measure of error, and the quadrangle, related to CVaR norm as a measure of error, is regular. IfE(X)=〈〈X〉〉αand Eqs. (10)–(12) hold, then the corresponding measure of risk and statistic are calculated from Item 3 of Proposition 2.1, and the whole corresponding quadrangle is presented below.Proposition 2.2CVaR (superquantile) Norm QuadrangleFor α ∈ [0, 1) the error measureE(X)=〈〈X〉〉αis related to the following regular quadrangle:S(X)=12(q(1−α)/2(X)+q(1+α)/2(X)),R(X)=1−α2CVaR(1+α)/2(X)+1+α2CVaR(1−α)/2(X),D(X)=1−α2CVaR(1+α)/2(X−EX)+1+α2CVaR(1−α)/2(X−EX),V(X)=〈〈X〉〉α+EX,E(X)=〈〈X〉〉α.CVaR norm quadrangle is similar to the Mixed-Quantile-Based quadrangle, see Rockafellar and Uryasev (2013), for(13)α1=(1+α)/2,α2=(1−α)/2,λ1=(1−α)/2,λ2=(1+α)/2.Fork=1,2defineEαk(X)=E[αk1−αkX++X−],Vαk(X)=11−αkEX+.With parameters from (13) we obtain the Mixed-Quantile-Based quadrangle with the same risk and deviation as in CVaR norm quadrangle, but with different statistic, error and regret:S(X)=1−α2q(1+α)/2(X)+1+α2q(1−α)/2(X),V(X)=minB1,B2{λ1Vα1(X−B1)+λ2Vα2(X−B2)|λ1B1+λ2B2=0},E(X)=minB1,B2{λ1Eα1(X−B1)+λ2Eα2(X−B2)|λ1B1+λ2B2=0}.Suppose one is optimizing measure of error over some parametric family X(θ):(14)minθEi(X(θ)),wherei=1for error from CVaR norm quadrangle, andi=2for error from Mixed-Quantile-Based quadrangle. Assume thatX(θ)=θ0+Y(θ˜),whereθ=(θ0,θ˜),and θ0 is a free parameter. Defineθi*=argminθEi(X(θ)). Thenθ˜1*=θ˜2*=argminθ˜D(Y(θ˜))=θ˜*. Therefore,Y(θ˜1*)=Y(θ˜2*)and two optimal pointsX(θ1*)andX(θ2*)for problems (14) can be obtained from each other by adding a constant shiftX(θ1*)=(θ0*)1+Y(θ˜*),X(θ2*)=(θ0*)2+Y(θ˜*),X(θ1*)−X(θ2*)=(θ0*)1−(θ0*)2.Paper (Ge et al., 2011) considers a class of functions defined similar to Lpnorms, but for p ∈ [0, 1). These functions are not norms and they are concave for some regions of the space they are defined66For p ∈ [0, 1) there is lp(x) inRnand Lp(X) in the space of random variables. Concavity holds, for example, for region x ≥ 0 inRnand for region X ≥ 0 in the space of random variables.. Such functions are used in optimization problems to achieve a sparse solution vector. We will define a similar functions in terms of CVaR concept.Further we define trimmed L1-norm. Contrary to CVaR norm, this function takes average over smallest α-fraction of absolute values |X| of a random variable X. The word “norm” here is potentially deceptive, since it corresponds to L1 and not the resulting function itself: trimmed L1-norm is not actually a norm. The term “trimmed” is widely used in robust regression, when the average of a few smallest regression residuals is minimized. Before averaging, residuals are usually transformed with some functionϕ:R→[0,∞). The case ofϕ(x)=x2is the most commonly used and corresponds to the least trimmed squares regression (LTS), see Rousseeuw (1984); Rousseeuw and Van Driessen (1999); Zabarankin and Uryasev (2014b). The case ofϕ(x)=|x|corresponds to the least trimmed sum of absolute deviations (LTA) regression, see Bassett Jr (1991); Hawkins and Olive (1999); Hössjer (1994), it also corresponds to the trimmed L1-norm function here, see below. The general case for arbitrary function ϕ was described in Neykov, Čížek, Filzmoser, and Neytchev (2012, formula (3)) and formalized for random variables via average quantile function in Zabarankin and Uryasev (2014b, problem (6.5.9)).One possible way to define trimmed L1-norm, or Tα, is as follows.Definition 3Let X be a random variable with E|X| < ∞. Trimmed L1-norm Tα(X) for α ∈ [0, 1] is defined byTα(X)=−CVaR1−α(−|X|).Below we provide some mathematical properties for trimmed L1-norm. These properties mostly follow from the ones presented in Section 2.Properties and representations of trimmed L1-norm1.Forα=0,trimmed L1-normTα(X)=inf|X|. For α ∈ (0, 1] trimmed L1-norm can be calculated using one of the formulas below:(15)Tα(X)=1α(E|X|−(1−α)CVaRα(|X|)),(16)Tα(X)=1α∫0αqp(|X|)dp,(17)Tα(X)=maxc{c−1αE[|X|−c]−}.0≤Tα(X)≤L1(X)=E|X|,Tα(λX)=|λ|Tα(X),if XY ≥ 0, thenTα(λX+(1−λ)Y)≥λTα(X)+(1−λ)Tα(Y),Tα(0)=0,however, there exists X ≠ 0 such thatTα(X)=0,Tα(X)≤0⇔Tα(X)=0implies thatP(X=0)≥α.Tα(X) is a continuous non-decreasing function w.r.t. α.αTα(X) is a convex non-decreasing function w.r.t. α.Formula (15) follows from Pflug (2000, Proposition 2 (iii)), whenY=|X|is taken. Tα(X) can be interpreted as an expectation of |X| in left α-tail. Note that Tα(X) is then the average quantile of the random variable |X|, hence, formula (16) holds, see Zabarankin and Uryasev (2014a, formula (1.4.1)). Formula (17) follows from Rockafellar et al. (2006, formula (5)).For p ∈ (0, 1) the following inequality holds Lp(X) ≤ L1(X), whereLp(X)=(E|X|p)1/p. Since xpis a concave function for 0 < p < 1, using Jensen’s inequality we have E|X|p≤ (E|X|)p, therefore, (E|X|p)1/p≤ E|X|, and Item 2 shows that for trimmed L1-norm. Items 2–4 follow from the fact that−1α∫0αqp(X)dpis a coherent, expectation bounded risk measure, see Rockafellar et al. (2006, Example 3). Item 5 follows from example ofX=0with probability 0.5 andX=1with probability 0.5. Then, for α ∈ [0, 0.5] functionTα(X)=0. Item 6 follows from formula (16). Note that Item 6 can be used in optimization problem settings for a constraint Tα(X) ≤ 0 to achieve a given sparsity of a solution variable vector, or as max α subject to Tα(X) ≤ 0 to maximize the sparsity of a solution variable vector.Item 4 implies that Tα(X) is a concave function for X ≥ 0. Notice that this property cannot be strengthened to concavity in the whole space of random variables. Consider a function g(X) such that g(X) ≥ 0,g(0)=0andg(X)¬≡0. Assume that g(X) is concave in the space of random variables. Sinceg(X)¬≡0,then there exists X such that g(X) > 0. Theng(X)+g(−X)>0=g(0)=g(X−X),which implies that g(X) is not a concave function.Considerx=(x1,…,xn)∈Rn. Let X(x) be a discretely distributed random variable taking valuesx1,…,xnwith equal probabilities1n. Then the trimmed L1-norm onRnis defined astα(X)=Tα(X). Properties of tα(x) are similar to properties of the Tα(X) and follow directly from Definition 3 and enlisted properties. Forx∈Rn,trimmed L1-norm is calculated as follows:tα(x)=1α(l1(x)−(1−α)〈〈x〉〉αS),tαj(x)=(|x|(1)+⋯+|x|(j))/j,forα=αj=j/n,t0(x)=mini|xi|,forα=0,t1(x)=l1(x),forα=1.Fig. 1shows level-sets of〈〈x〉〉αSand tα(x) inR2for different values of α. The function tα( · ) is a natural extension of〈〈·〉〉αS. When α varies from 0 to 1, the function tα(x) changes from mini|xi| tol1(x)=1n∑i=1n|xi|,and the function〈〈x〉〉αSchanges from l1(x) to maxi|xi|.We illustrate CVaR norm quadrangle, see Proposition 2.2, with the following case study. The case study results, data, and codes are posted at this link77http://www.ise.ufl.edu/uryasev/research/testproblems/advanced-statistics/cvar-norm-regression/.We considered a linear regression problem with CVaR norm error. Let X be a n × d design matrix, where n is a number of observations, d is a number of explanatory variables. Lety∈Rnbe a vector of observations on the dependent variable. Lete∈Rnbe a vector of ones. We denote byX˜=[e,X]an extended design matrix including additional unit column. We considered a linear regression:y^=X˜a,wherea∈Rd+1is a vector of parameters. To solve this regression problem we minimized CVaR norm of vector of residualsy−y^:(18)mina∈Rd+1〈〈y−X˜a〉〉α.It is desirable to use CVaR norm in regression when we want to control directly large absolute values of residuals. We are indifferent to the sign of the residual. We just do not want to have large absolute values, but are tolerant to small absolute values. Similar purpose can be achieved by minimizing Lpnorm, but in this case we do not control directly some specific percentage of largest outcomes. We can directly specify the percentage of largest absolute residuals with CVaR norm, e.g., 10% of largest outcomes. We also want to mention that the percentile regression (Koenker & Bassett Jr, 1978) with Koenker and Bassett function is quite close to CVaR norm regression. However, percentile regression is concentrated on large outcomes in one tail, while CVaR norm regression pays attention to large outcomes without identifying the sign of the residual.Similar type of error has been considered earlier in OR literature. For instance, Zabarankin and Uryasev (2014b, formula (6.5.9)) considered so called “average alfa-quantile” error minimization applied to the transformed residual. In this problem, the average is taken over the left tail of the distribution. Such approach corresponds to trimmed error measures and to robust regression, it produces regression which is stable to outliers. By selecting the absolute value as a transformation function in “average alfa-quantile” error we are coming to trimmed L1-norm minimization, or to LTA regression. Here we consider averaging over the right tail of distribution, which leads to CVaR, or superquantile, functions. Similarly, by selecting the absolute value as a transformation function in CVaR we are coming to CVaR norm minimization. CVaR norm regression is not stable to outliers, moreover, it is a “pessimistic” estimator focused is on a fraction of the most “problematic” observations. Aside from potential benefits of the pessimistic approach, problem (18) is a convex problem and can be solved precisely and efficiently.We consider the dataset from the case study “Estimation of CVaR through Explanatory Factors with Mixed Quantile Regression”88http://www.ise.ufl.edu/uryasev/research/testproblems/financial_engineering/estimation-of-cvar-through-explanatory-factors-with-mixed-quantile-regression/. The data contains returns of the Fidelity Magellan Fund as a dependent variable. Russell Value Index (RUJ), Russell 1000 Value Index (RLV), Russell 2000 Growth Index (RUO) and Russell 1000 Growth Index (RLG) are taken as independent variables. Data include 1264 observations. Solving Time on a PC with 2.83GHz is 0.01 s.The CVaR norm is minimized with Portfolio Safeguard (AORDA, 2009) software package. Confidence level α in CVaR norm equalsα=0.9. We minimized CVaR instead of CVaR norm, according to calculation formula (8). Denotey¯=[y;−y]∈R2nandX¯=[X˜;−X˜]∈R2n×d. Formula (8) implies〈〈y−X˜a〉〉αS=CVaR(1+α)/2(y¯−X¯a).Then, problem (18) is equivalently stated as followsmina∈Rd+1CVaR(1+α)/2(y¯−X¯a).Optimization results for this problem are in Table 1.CVaR norm quadrangle is a regular quadrangle, see Proposition 2.2. According to Rockafellar and Uryasev (2013, Regression Theorem), first stated as the Error-Shaping Decomposition of Regression Theorem (Rockafellar et al., 2008, Theorem 3.2, page 722), the intercept, obtained in regression, equals to the Statistic of a modified residuals. In CVaR norm quadrangle, statistic equalsS(X)=(q(1+α)/2(X)+q(1−α)/2(X))/2. Denote the optimal vector of parameters obtained in regression bya*=[c*,b*],wherec*∈Ris an optimal intercept. According to the Regression Theorem,c*∈S(y−Xb*)(we write ∈ because, in general, quantile qp(X) is an interval, see (1), thereforeS(X)is also an interval). At the optimal point,c*=−0.002,q0.05−(y−Xb*)=−0.013,q0.95−(y−Xb*)=0.009. Therefore,S(y−Xb*)≈(q0.05−(y−Xb*)+q0.95−(y−Xb*))/2=(−0.013+0.009)/2=−0.002=c*.That is, numerical experiment confirms theoretical results for CVaR norm quadrangle, namely statisticsS(y−Xb*)=12(q(1−α)/2(y−Xb*)+q(1+α)/2(y−Xb*))including the optimal intercept, c*.In the following case study we consider L1-norm as an out-of-sample criterion. For the in-sample criteria we consider elements of the CVaR norm parametric family. L1-norm is an element of this family with the corresponding parameter value 0. We vary value of the parameter between 0 and 1 for in-sample learning to optimize L1-norm of residuals in out-of-sample. We show that using CVaR norms with parameter value bigger than 0 can lead, for small training samples, to better out-of-sample performance than direct in-sample minimization of L1-norm.We illustrate CVaR norm regression with a controlled numerical experiment. In this case study we set true law asy(x)=xfor x ∈ [0, 1]. We pick sample points(x1,…,x11)=(0,0.1,…,0.9,1). Then we simulate dependent variable asyi=xi+ɛi,where error terms εi∝Laplace(0, 0.5) are distributed according to the Laplace distribution (double exponential distribution, probability density functionf(x;μ,b)=12bexp(−|x−μ|b)), see Fig. 2.L1-norm of regression residuals is chosen as the out-of-sample error criterion. Similar to exponential distribution, Laplace distribution has heavy tails, which makes L1-based regression a reasonable choice. Moreover, minimization of L1-norm of residuals is equivalent to likelihood maximization for the case of Laplace-distributed error. Since the true distribution is known, there is no need to divide sample into training and testing subsamples to measure error of the estimated model. Lety^idenote model estimation for the data point xi. First, “expected error” is calculated asEE=1n∑i=1nEɛ|y(xi)+ɛ−y^i|,wheren=11and each expectation is taken for ε∝Laplace(0, 0.5). Also, since the true law is know, then “true error” is calculated asTE=1n∑i=1n|y(xi)−y^i|.As in Section 4.1, the problem (18) is solved. L1-norm minimization is a special case of (18) whenα=0. The goal of the case study is to test whether “pessimistic” estimation provided by CVaR norm minimization with α > 0 can achieve better out-of-sample quality of estimators, measured by EE and TE, than direct L1-norm minimization withα=0. To smooth the results obtained with the randomly generated sample, number of different samples generatedN=1000,each sample sizen=11. For each sample, the problem (18) is solved for valuesαk=(k−1)/(K−1)fork=1,…,K,K=21(that is, values of α are(0,0.05,0.01,…,1)). For sample j, where 1 ≤ j ≤ N, and parameter αk, regression estimator is found, and errorsEEjkandTEjkare calculated. Fig. 3 shows dependence on α for average error and standard deviation of error, scaled to the average error forα=0and the standard deviation of error forα=0correspondingly. These values are calculated for the two types of error we consider: “expected error” EE, and “true error” TE. Minimal average EE is achieved forα=0.15and is 1% lower than average EE forα=0. Despite the modest drop in average error, CVaR norm model is more stable than L1-norm model: standard deviation of EE is 10% lower forα=0.15than forα=0. Average TE is minimized by the same valueα=0.15and is 5% lower than forα=0; standard deviation of TE is 8% lower forα=0.15than forα=0.This case study showed that even if CVaR norm is not an out-of-sample criterion itself, as it was assumed in Section 4.1, the minimization of CVaR norm can be more advantageous than direct optimization of the out-of-sample criterion.

@&#CONCLUSIONS@&#
