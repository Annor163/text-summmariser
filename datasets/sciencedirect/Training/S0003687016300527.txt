@&#MAIN-TITLE@&#
Why we love or hate our cars: A qualitative approach to the development of a quantitative user experience survey

@&#HIGHLIGHTS@&#
We present a novel way of developing questionnaires for measuring user experience.Items were generated using users' natural and domain-specific language.The survey is sensitive to real-life experiences.Results were highly reliable to measure drivers' appraisals of their cars.

@&#KEYPHRASES@&#
User experience,Design and emotion,Survey development,Appraisal theory,Car industry,

@&#ABSTRACT@&#
This paper presents a more ecologically valid way of developing theory-based item questionnaires for measuring user experience. In this novel approach, items were generated using natural and domain-specific language of the research population, what seems to have made the survey much more sensitive to real experiences than theory-based ones. The approach was applied in a survey that measured car experience. Ten in-depth interviews were conducted with drivers inside their cars. The resulting transcripts were analysed with the aim of capturing their natural utterances for expressing their car experience. This analysis resulted in 71 categories of answers. For each category, one sentence was selected to serve as a survey-item. In an online platform, 538 respondents answered the survey. Data reliability, tested with Cronbach alpha index, was 0.94, suggesting a survey with highly reliable results to measure drivers' appraisals of their cars.

@&#INTRODUCTION@&#
Interest in measuring user experience has progressively increased along with a growing interest in experience-driven design. Initially, design researchers borrowed instruments developed in the social sciences (for reviews, see Laurans et al., 2009; Poels and Dewitte, 2006; Desmet et al., 2016) to undertake these measurements. Later, domain-dedicated instruments were developed. These instruments, such as scales that measure subtle and mixed user emotions (Desmet, 2003), affective responses to interactive products (Hassenzahl et al., 2010), or user experience over time (Karapanos et al., 2009), proved to be much more sensitive to the particular characteristics of the user experience itself. As a result, over the last decade, an overwhelming amount and diversity of user experience measurement instruments have become available. In response to this proliferation, several inventories have been organized and frameworks proposed with the aim of providing a comprehensive overview (e.g., Obrist et al., 2009; Vermeeren et al., 2010; ENGAGE, 2006; Höök, 2008).Besides the attempt to provide an overview, various authors have also made an effort to increase awareness regarding the added value of applying experience measures in design processes (Väänänen-Vainio-Mattila et al., 2008; Law et al., 2014). These studies have revealed some pertinent theoretical, methodological, and practical issues for the measurement of user experience in the context of design processes. One recurrent issue is related to whether user experience is best measured with qualitative or quantitative methods. This question seems to refer back to the classical distinction between reductionism and holism at the same time that it appears to divide the user experience research community in two (Law, 2011). One of the main advantages of qualitative data, which tends to be rich and detailed, is that it can inform about the causes of certain experiences and offer relevant insights for both envisioning design opportunities and formulating design requirements. In contrast, one of the main advantages of quantitative data, which tends to be simple and precise, is that it can provide an objective basis for critical decisions on design and developmental issues. Such objectivity may come in handy when trying to attract investments and convince stakeholders about the effectiveness of design decisions. In addition, quantitative data can be helpful in testing and improving user experience theories, which, in their turn, may provide universal principles applicable to a multitude of design contexts. In practice, many researchers choose to combine qualitative and quantitative approaches as, for instance, when using emotion measurement in combination with open interviews (see Desmet and Schifferstein, 2012).Within the wide variety of approaches to assess user experience, the traditional questionnaire is still currently the most often used in the context of design and design research, both for obtaining quantitative and qualitative data (Bargas-Avila and Hornbæk, 2011). Surveys are easy to develop and administer; they do not require sophisticated instruments or software; and they can be easily tailored to the research challenge at hand. Item generation is typically theory-based, requiring researchers to translate theoretical factors or variables into survey items. Two often reported concerns about using such theory-based questionnaires relate to their face and ecological validity, which again seem to refer back to the reductionism versus holism debate (Law et al., 2014). Design researchers tend to be sceptical about the degree to which surveys are able to uncover actual ‘real’ feelings and responses. In addition, they tend to question if theory-based surveys are sufficiently sensitive to the richness and variety of people's responses to stimuli in real-life settings. Acknowledging these two issues, we propose a novel way of developing theory-based item questionnaires for measuring user experience. The key difference from traditional questionnaires is that natural and domain-specific language of the research population is actually used in the generation of the items. Our proposition, therefore, is that both face and ecological validity may be increased with the use of questionnaires that are, not only based on underlying theoretical factors, but also formulated in a natural language sensitive to real experiences of real people in real usage contexts.In the specific case of this paper, the attempt of applying a natural-language approach to item-development takes place within the context of a car experience survey. The reason for choosing car design is that vehicles usually evoke strong emotions and rich user experiences (Desmet et al., 2000; Kamp, 2012; Hiemstra-van Mastrigt et al., 2015; Franz et al., 2012). Moreover, stakeholders in the automotive industry typically require quantitative measures to justify and evaluate experience-driven design initiatives (Saucken et al., 2014). In the paper, we first briefly introduce the theoretical basis for the experience survey. Then, we present the three steps undertaken for developing the natural-language based questionnaire. Next, we report on the application of the survey and its results. Finally, in the Discussion Section, we bring the paper to an end, exploring some challenges and future research possibilities.The theoretical basis for our approach to survey development is appraisal theory, one of the most commonly used theories (either implicitly or explicitly) for understanding emotional responses both in standard emotion research (Frijda, 1993) and in design research (Desmet and Hekkert, 2002). An appraisal is a sense-evaluation of the ‘relational meaning’ of a stimulus event, which determines the perceived relevance of the event to one's well-being (Frijda, 1986; Lazarus, 1991). Events that are appraised as contributing to one's well-being evoke pleasant emotions, and those that are appraised as threatening or harming one's well-being evoke unpleasant emotions. Because appraisals mediate events, user goals, and emotions, they may trigger insights regarding the relationships among these variables (Desmet and Hekkert, 2002). Experience questionnaires are usually developed on the basis of sets of distinct appraisal components, where each one relates to a particular ‘relational issue’ of a stimulus event (Roseman, 2001; Scherer, 2001). Reviewing a series of appraisal theories, Demir et al. (2009) selected seven main appraisal components that are relevant for user experience in human–product interactions. These components are presented in Table 1along with their respective key relational issue.Demir et al. (2009) argued that these seven appraisal components (Table 1) facilitate a systematic and fine-grained analysis of emotions. At the same time, the authors acknowledged that these components are too abstract to be useful for design purposes and, thus, advised that they should be operationalized according to the particular design domain of interest. In our natural-language approach to developing experience questionnaires, this operationalisation was mediated by domain-relevant interviews with real users rather than directly done by the researchers themselves (as it is the common practice). The next section reports on the three steps involved in this procedure in more details.The first step was to conduct in-depth interviews to operationalize the appraisal components presented in Table 1 within the context of car experience, using natural and domain-relevant language. Respondents were recruited by e-mail. In this mail, they were asked to indicate to what extent they agreed with the sentences “I love my car” and “I hate my car”, using a five-point Likert scale (ranging from completely agree to completely disagree).Ten respondents were selected from 50 e-mails sent: 5 females and 5 males; 18–54 years old; 3 undergraduates, 4 professionals, and 3 graduate students; all Brazilians. Five completely agreed that they loved their cars, and five completely agreed that they hated their cars. Even though the makeup of the two groups was different, as seen in Table 2, these respondents were selected to ensure that the subsequent interviews would generate emotional exclamations, that is, evoke naturalistic sentences from the drivers when expressing appraisals in relation to their cars.An interview-guiding list based on the seven appraisal components was developed to be discussed with the selected respondents. The interviews were conducted individually and took between 45 and 75 min. Time and place were determined by the respondents. For reasons of ecological validity, the interviews were conducted in the respondents' car; first while they just sat in the driver's seat and, after, while they were driving. The interviews were conducted while the car was parked. During the driving, the interviewers refrained from speaking and only registered spontaneous verbalizations. Interviews were recorded in video and transcribed afterwards.All transcribed material was analysed according to content analysis (see Neuendorf, 2002). In line with general practice in this field, interviews were analysed until data saturation was reached, indicating that the ten respondents from the qualitative stage of this research were able to provide clear reasons to their appraisals. Categories of answers were identified, mainly by means of similarity. That is, similar sentences mentioned by different respondents were grouped together. Content analysis was initially developed separately by two researchers with more than ten years of experience with the technique. All transcriptions were read and generated a first draft of the categories, which was discussed on a meeting between them. In a collaborative process, the two researchers developed the final outline of categories.This procedure resulted in 71 categories of answers, each related to at least one of the seven appraisal components. From each category, one sentence was selected to serve as the basis for the questionnaire items. In this selection, the following five basic guidelines for formulating survey items were taken into consideration (see Hinkin, 1998): (1) the language used is simple, short and familiar to the group of respondents (e.g., “It's a dream car.” Sentences were selected from the interviews, using everyday language.); (2) each item assesses a single issue, avoiding combinations of issues (e.g., “It makes unbearable noises.”); (3) items do not represent more than one theoretical construct (e.g., “Its engine is quiet” is representative of the appraisal component intrinsic pleasantness.); (4) leading formulations (questions phrased in a way that suggest expected responses) are avoided (e.g., we have refrained from using sentences such as “It is the best model I could pay for”); (5) items that are expected to generate little variance are avoided (e.g., sentences revealing ideas with which users obviously totally agree were not included, such as “a great car should never fail”.).The second step was to prepare the 71 sentences collected for serving as survey items. The sentences were kept in the original language (Brazilian Portuguese) and were adapted as little as possible to preserve the drivers' genuine way of expressing themselves. Nevertheless, some small changes were made. Firstly, slang was eliminated. Secondly, some sentences were adapted to make sure that they were understandable when taken out of the original context of the conversation. Thirdly, emotional words (e.g., afraid and happy) were eliminated as much as possible since the focus of the questionnaire was not on particular emotions but on appraisals that evoke emotions. Regarding this last change, however, it is important to highlight that, whenever the omission of emotional words compromised the meaning of the sentences, they were kept in the original form. This kind of exception can be illustrated, for instance, by contrasting two connotatively different sentences included in the item pool: “It has already given me so many problems that I'm afraid of it” and “This car has just given me problems”. These two sentences would end up having very similar meanings, if the clause “that I'm afraid of it” had been omitted from the first sentence.The questionnaire was built in an online platform. It started with general questions about the respondents' demographics and cars. Next, respondents indicated their level of agreement with each one of the 71 sentences, using 5-point Likert-type scales (1 = “completely disagree”; 5 = “completely agree”). Sentences were randomized among respondents to control for order and tiredness effects.The third step was to authenticate both the content and the face validity of the questionnaire. For this survey, content validity referred to the extent to which it sufficiently represented all seven appraisal dimensions (Table 1). Content validity is typically judged by experienced (and recognized) subject matter experts. Here, the judge was a leading expert in experience design research (board member of the International Design and Emotion Society, editor of peer reviewed design research journals, and professor of design aesthetics). In a meeting with the authors, the judge discussed the relevance and form of all items, suggesting several minor modifications. While content validity refers to what the survey actually measures, face validity refers to what the survey appears to measure in the eyes of the respondent population, and the adequacy of the language and item presentation. Face validity was determined with 61 undergraduate students from a southern Brazilian university. They first answered the survey in a media lab, and then reflected on each item. This step resulted in some other additional minor modifications of some items; the final set of items is presented in Table 4.The final version of the natural-language questionnaire was tested in a study with 710 Brazilian respondents. Respondents were recruited by e-mail. The data of 538 respondents (75.8%) who completed the whole survey were included in the data analysis. Briefly, information about the respondents and their vehicles may be summarized as follows:•mean age: 31.2 (SD = 10.94);gender: 56.7% female and 43.3% male;level of education: 30.1% undergraduate students, 22.7% professionals holding a bachelor degree or equivalent, and 38.5% holders of a master degree;car brands: 16.2% Chevrolet, 15.8% Volkswagen; 14.9% Fiat, and 14.3% Ford;car condition at purchase: 65.2% new and 34.8% used;car use frequency: 76.9% on a daily basis;car users: 51.7% single users and 24.5% main users, but not the only ones.Respondents filled out the questionnaire individually, online, at a place and time of their convenience.

@&#CONCLUSIONS@&#
