@&#MAIN-TITLE@&#
Capturing relative motion and finding modes for action recognition in the wild

@&#HIGHLIGHTS@&#
To recognise actions solely on motion observed in the video.A quaternary representation of the interest point node test.Extensive experimental analysis throughout the approach.State of the art performance on “in the wild” datasets.Performance gains independent of the features or classification architecture.

@&#KEYPHRASES@&#
Action recognition,Relative motion descriptor,Mode Finding,Outlier Detection,RANSAC,

@&#ABSTRACT@&#
“Actions in the wild” is the term given to examples of human motion that are performed in natural settings, such as those harvested from movies [1] or Internet databases [2]. This paper presents an approach to the categorisation of such activity in video, which is based solely on the relative distribution of spatio-temporal interest points. Presenting the Relative Motion Descriptor, we show that the distribution of interest points alone (without explicitly encoding their neighbourhoods) effectively describes actions. Furthermore, given the huge variability of examples within action classes in natural settings, we propose to further improve recognition by automatically detecting outliers, and breaking complex action categories into multiple modes. This is achieved using a variant of Random Sampling Consensus (RANSAC), which identifies and separates the modes. We employ a novel reweighting scheme within the RANSAC procedure to iteratively reweight training examples, ensuring their inclusion in the final classification model. We demonstrate state-of-the-art performance on five human action datasets.

@&#INTRODUCTION@&#
Human action recognition from video has gained significant attention in the field of Computer Vision. The ability to automatically recognise actions is important because of potential applications in video indexing and search, sports analysis, activity monitoring for surveillance, assisted living purposes, etc. Excellent results have been obtained on simulated actions in simplified settings. However, natural actions in uncontrolled environments, such as movies and personal video collections have proven more difficult. The task is especially challenging due to variations in illumination, scale, camera motion, viewpoint, background, occlusion, action length, subject appearance and style.In order to adequately describe actions in the presence of the above variations, recent recognition approaches make use of a combination of feature types. They often combine shape and motion, or include contextual information, while others employ object detectors to bolster recognition confidence. This can cause problems as many of the challenges stated in the paragraph above are appearance based. We are motived to just use motion, inspired by Johansson’s [3] human perception experiments using Point Light Displays. These experiments show that it is possible to observe detailed properties of human actions and actors based entirely on the dynamics of their motion. Hence, given a set of point-lights that appear randomly placed to human observers, the introduction of their motion can clearly convey what action is being performed. This motivates our approach. This paper therefore investigates the sole use of dynamics for automatic action recognition in complex videos using interest point positions without their individual appearance information.An action can be defined as a collection of atomic events at various spatial and temporal regions of a video. Our method is based on the premise that those events are better defined by motion information, given that the appearance of subjects and the background are subject to change. It is assumed that the motion information provides a discriminative representation. A novel representation of actions in video is therefore proposed, which captures the relative distribution of motion-based interest points by encoding their local spatio-temporal configuration in an efficient manner. This results in an action descriptor, which, in vectorised form, can be learnt using a discriminative classifier. In contrast to other interest point distribution-based representations, which assign labels to points based on the appearance of their local spatio-temporal neighbourhoods, the proposed representation ignores such labels, and relies on the strengths of responses within less localised regions of the video.Furthermore, approaches to action recognition typically attempt to generalise over all examples from the training data. Given the amount of variability within actions classes in natural settings, it is considered unrealistic to assume that all aspects of variability can be modelled by a single classifier. This paper presents an approach which tackles variability in complex action examples by assuming the presence of noisy examples, which cause non-separability between classes; and by assuming that there exist multiple modes within the set of examples of any one class. This is necessary as there are no constraints placed on actions performed and captured in movies and personal videos. Fig. 1shows examples of the action categories, Get Out of Car and Hand Shake from the Hollywood2 dataset [1]. It is clear from these examples that, while the same semantic action is being performed, all examples appear radically different in appearance which leads to multiple modes within the same action. Despite these variations, the examples are given one semantic label, and analysis of these examples as a single group may limit classification performance.It can be seen in Fig. 1 that category labels can be broken into sets of different class subsets depending on the placement of the camera with respect to the car, or the individuals shaking hands, for Get Out of Car and Hand Shake respectively. Therefore, instead of considering all examples of an action category label as one class, we analyse examples to determine inherent modes or groups. An action class can, therefore, be partitioned based on these groups, significantly simplifying the training and classification task. We achieve this by employing a variant of the Random Sampling Consensus (RANSAC) algorithm [4] to training examples of a class. Once the modes of an action class are found, several sub classifiers are created, which are applied independently to unseen examples. Action categories can therefore be split into subsets of consistent modes, which cover variability of action, environment and viewpoint. While extensive works exist on local classification methods for object category recognition [5,6], human pose estimation [7], etc. [8], the assumption of multi-modality has not been explicitly applied to action recognition.The layout for the remainder of this paper is as follows: Section 2 discusses related research. In Sections 3 and 4, we present the Relative Motion Descriptor and Automatic Mode Finding methods respectively. We describe our experimental setup in Section 5 and present recognition results in Section 6. Finally, conclusions are presented in Section 7.

@&#CONCLUSIONS@&#
