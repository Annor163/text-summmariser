@&#MAIN-TITLE@&#
An ensemble neuro-fuzzy radial basis network with self-adaptive swarm based supervisor and negative correlation for modeling automotive engine coldstart hydrocarbon emissions: A soft solution to a crucial automotive problem

@&#HIGHLIGHTS@&#
Investigating the applicability of soft computational techniques for identifying the coldstart effect in SI engines.Proposing a novel bi-level learning technique to design the structure of neuro-fuzzy identifier.Extending the applications of negative correlation based ensemble identifiers to a really important engineering problem.

@&#KEYPHRASES@&#
Coldstart,Hydrocarbon emission,Automotive engine modeling,Neuro-fuzzy computing,Ensemble computing,Swarm intelligence,

@&#ABSTRACT@&#
In this paper, the authors propose a novel intelligent framework to identify the exhaust gas temperature (Texh) and the engine-out hydrocarbon emission (HCraw) during the coldstart operation of an automotive engine. These are two key variables affecting the cumulative tailpipe emissions (HCcum) over the coldstart phase, which is the number one emission-related problem for today's spark-ignited (SI) engine vehicles. The coldstart operation is regarded as a highly nonlinear, transient and uncertain phenomenon. The proposed identifier integrates different soft computational strategies, i.e. neuro-fuzzy computing, fuzzy controller, swarm intelligent computing, and ensemble network design, beneficial for capturing both uncertainty and nonlinearity of the problem at hand. Furthermore, concepts of negative correlation topology design and hierarchical pair competition based parallel training are extracted from literature to form a diverse and robust ensemble identifier. Training of each neuro-fuzzy sub-component in ensemble network is carried out using a hybrid learning scheme. One feature of the antecedent part of neuro-fuzzy system, i.e. number of linguistic terms for each variable, as well as characteristics of rules in rule base are adjusted using hierarchical fair competition-based parallel adaptive particle swarm optimization (HFC-APSO) and the rest of features, i.e. the shape of (membership functions) MFs and the consequent variables of each rule, are tuned using back-propagation (BP) and steepest descent techniques. As it was mentioned, the authors try to design an ensemble identifier with acceptable rate of generalization, robustness and accuracy. These features help them to tame the intuitive uncertainties associated with the rate of Texhand HCrawemission over the coldstart period. To do so, the potential characteristics of sub-components (solution domain of network design) are divided into a set of partitions and then HFC-APSO is utilized to explore/exploit each of those partitions. The exploration/exploitation rate of PSO (the core of HFC-APSO) is dynamically controlled by a fuzzy logic based controller. Hence, it is expected that HFC-APSO yields a set of accurate sub-identifiers with different operating characteristics. To further foster the diversity of the ensemble, negative correlation criterion is considered which obstructs the integration of identical sub-identifiers. The identification results demonstrate that the method is highly capable of providing an authentic model for estimation of Texhand HCrawemission during the coldstart period.

@&#INTRODUCTION@&#
The main goal of engineers and authorities of automobile industry is to reach a practical compromise among different operating aspects of automotive engines, i.e. fuel economy, drivability, cost of product, and emissions. However, today's stringent emission standards oblige the authorities of automotive industry to exert a remarkable effort on achieving low level tailpipe emissions in SI engines. Unburned hydrocarbons (HCs), carbon monoxide and nitrogen oxides are among the most important pollutant materials [1,2]. Considering the most recent engineering efforts for mitigating the emissions, it can be inferred that the regulations concerned with the amount of HCs is of highest importance. Significant reduction/control of emitted HCs is the most challenging problem with emissions from today's SI engines [3]. Therefore, it is really necessary to researchers to correctly analyze performance of SI engines to cope with undesired HC emissions. For a typical driving cycle, during the first couple minutes after coldstarting of an SI engine, known as the “coldstart period”, a considerable amount of HCs is emitted (about 80–90%). Therefore, improvements during the coldstart period have very critical impacts on reducing the total HC emissions to meet the requirements during the standard emission testing procedures, such as Federal Test Procedure (FTP) or Economic Commission for Europe (ECE) driving cycles. This fact motivated different researchers of automotive engineering (AE) society to analyze/model, control and optimize different aspects of coldstart phenomenon [4].The total amount of tailpipe-emitted HCs or HCcumis primarily a function of the engine-out hydrocarbon emission HCrawand the exhaust gas temperature Texhin the coldstart period. Henein et al. [5] developed a model for analyzing cycle-by-cycle HC emissions during the coldstart operation of a gasoline engine. Based on the results, they concluded that the main reason to undesirable HC emissions is that the catalytic convertor is not warmed up over the coldstart period. Initially, the engine-out hydrocarbon emissions HCrawand the tailpipe emissions HCcumare approximately the same, but once the catalytic convertor reaches the light-off temperature, it begins converting the combustion by-products, specifically unburned engine-out hydrocarbon emissions or HCraw, at a better rate with a conversion efficiency of about 50%. Therefore, engineers have been trying to find effective and practical strategies to reduce the time required for catalytic convertors to reach the light-off temperature. It was observed that Texhhas a remarkable contribution to providing required heat for catalyst light-off [1,5]. Hence, along with modeling HCraw, researchers of AE society have conducted a wide range of researches to analyze/model Texh, as the other significant factor affecting HCcum.Generally, the most concentration of AE researchers was on utilizing physics-based engine dynamic models to investigate the characteristics of various subsystems, including catalytic convertors, for coldstart analysis and control. Dobner [6] utilized both linear and nonlinear modeling formulations to provide a dynamic model for controlling the main characteristics of a given engine. In that work, the author mainly focused on modeling the throttle and the intake manifold dynamics of the engine. The results of that investigation were then extended by Moskwa and Hedrick [7] and Cho and Hedrick [8] to facilitate the real-time control of the key characteristics of the engine. Tunestal and Hedrick [9] used the net heat released data to estimate the air/fuel ratio of cylinders. Zavala et al. [10] developed a physics-based model to identify the fuel dynamics over the coldstart period. Sanketi et al. [11] used a hybrid modeling/controlling framework to reduce the amount of coldstart HC emissions in automotive engines. Sanketi et al. [12] developed an optimal controller via convex relaxation for reducing the amount of emitted HCs during coldstart. Shen et al. [13] developed a precise physical model of a catalyst with 13-step kinetics and 9-step oxygen storage mechanisms to investigate the performance of catalytic convertors during coldstart period. Chan and Hong [14] analyzed the chemical conversions of carbon monoxide and unburned hydrocarbons in the oxidation process of coldstart period by a precise heat transfer modeling. Fiengo et al. [15] proposed a new real-time optimal control technique for the warm-up phase of three-way catalyst of an SI engine. Zavala et al. [16] developed simple control-oriented models to predict both HCrawand Texhduring coldstart period.Together with implementations of physics-based models, numerical and semi-empirical techniques have been used for modeling and controlling of HCrawand Texhduring coldstart period. Jones et al. [17,18] investigated the applicability of semi-empirical techniques for real-time control of transient characteristics of a three-way catalyst. Koltsakis and Tsinoglou [19] used genetic algorithm (GA) to tune the parameters of a control-oriented model that aimed at identifying the thermal response of close-coupled catalysts during light-off. Soumelidis et al. [20] developed four different nonlinear dynamic models for three-way catalyst control and diagnosis. Gonatas and Stobart [21] used a black-box model to predict the emissions and oxygen storage over coldstart period. McNicol et al. [22] used an expert knowledge based Bayesian calibration algorithm for optimizing coldstart emissions. Recently, Azad et al. [1] proposed a systematic numerical procedure, including validation test, optimal control and sensitivity analysis, to determine the required accuracy of a model used for real-time optimal control of HC emissions during coldstart period. In another work, Azad et al. [23] proposed a sliding mode control with bounded inputs for Texhto reduce HC emissions during coldstart period.As it can be seen, in spite of fruitful and comprehensive researches, there are rare reports in literature addressing the applications of soft computational approaches to control, modeling and minimization of coldstart HC emissions. This is while soft computing and computational intelligence (CI) concepts proved their authenticity for handling design, optimization, power management and control problems of different types of automotive propulsions, such as electric vehicle (EV), hybrid electric vehicle (HEV) and plug-in hybrid electric vehicle (PHEV) powertrains. Yildiz et al. [24] used particle swarm optimization algorithm (PSO) for nonlinear constraint component sizing of PHEV powertrains. They observed that PSO can yield precise and efficient results as compared to conventional techniques. Jain et al. [25] proposed a multiobjective genetic algorithm (MOGA) for optimal powertrain component sizing of a fuel cell PHEV. Mozaffari et al. [26,27] used both single objective and multiobjective mutable smart bee algorithm (MSBA) to optimize the most crucial parameters of an irreversible Atkinson engine. Feldkamp et al. [28] used a recurrent neural network for energy management of a mild hybrid electric vehicle with an ultra-capacitor. Piccolo et al. [29] used genetic algorithm (GA) to efficiently manage the energy flow in HEVs. Shi et al. [30] developed a fuzzy controller for parallel HEVs. Mohebbi and Farrokhi [31] developed a neural controller for parallel HEVs. As it can be seen, soft computing has been vigorously applied to different problems in AE.Actually, the applications of soft computing techniques are not limited to automotive systems. That would not be an exaggeration if we contend that soft methods have been utilized for handling a wide spectrum of engineering problems in the fields of manufacturing [32,33], smart materials [34], aerospace [35], robotics [36], energy systems [37] and etc. The promising feed-back of above researches has instigated the authors to investigate the potentials of soft techniques for modeling the crucial variables of the coldstart phenomenon. Indeed, the aim of the current research is to elaborate the applicability of soft computing for identifying Texhand HCrawduring coldstart operation, which is a highly nonlinear, transient and uncertain situation.To this end, the authors take the advantages of a self-organized ensemble identifier. Ensemble identifiers proved their generalization, robustness and accuracy over various researches. It has been demonstrated that such tools are quite useful for modeling highly nonlinear engineering phenomena. In this study, a novel ensemble approach which integrates the concepts of negative selection [38], bi-level learning [39], and fuzzy controlled swarm based optimization [40,41] is presented. The main motivations behind the use of such an identification tool are:(1)Ensemble intelligent tools benefit from several characteristics such as robustness, accuracy, and generalization [42] which suit them to be used for identifying the transient and nonlinear rate of HCrawover coldstart period [1].As it will be shown in the next section, the database of coldstart problem consists of different informatics streams. In other words, the database is quite large, and thus it is very hard for a sole identifier to capture the entire knowledge required for the control or the optimization. To solve the obstacle associated with knowledge acquisition, ensemble identifier is used. Generally, ensemble identification/classification frame includes several members (sub-components) of different characteristics. By exerting a good policy, we can assign each of those sub-components to learn specific characteristics of data base. In this study, the concept of negative correlation is used to force the ensemble frame to select sub-components of different characteristics [43].Each subcomponent is a neuro-fuzzy identifier. As the physics of the problem is associated with uncertainties, the fuzzy computing is used to mitigate the un-desired effects of uncertainty [33].The contrived bi-level learning mechanism consists of both global and local optimizers. It is expected that the use of global optimizer (at hyper-level) increases the probability of obtaining global optimum characteristics for each sub-component [39].As it was mentioned, it is quite important for researchers of AE society to generate accurate and practical models for coldstart phenomenon. Over-fitting is one of the major flaws that obstructs the applicability of intelligent tools. To tackle such a deficiency, the authors take the advantages of ensemble identifier. Using several independent sub-components provides ensemble identifiers with higher probability to escape from over-fitting phenomenon [39,42].After providing a detailed algorithmic and analytic explanation, the method is used for modeling the key factors of coldstart phenomenon.The rest of the paper is organized as follows. Section 2 is devoted to the experimental procedure conducted by the authors and their colleagues in the Vehicle Dynamics and Control Laboratory at UC, Berkeley to measure Texhand HCrawover the coldstart period for an automotive engine. In Section 3, a detailed description of ensemble identifier is provided. Moreover, the steps required for implementing the bi-level learning and negative correlation component selection is presented. Results and discussions of the modeling procedure are given in Section 4. Finally, the paper is concluded in Section 5. In this section, the authors also try to discuss some potential work for future investigations.The experimental data for the current study were collected in several coldstart runs at UC, Berkeley's engine test facility. For the coldstart research activities, this facility houses a spark-ignited, multi-port fuel injection engine (2.4L, 4-cylinder, DOHC 16 valve) equipped with variable valve timing system and intake air valve control. The engine can generate up to 117kW at 5600rpm and produce a maximum torque of 220N.m at 4000rpm. An eddy current dynamometer is connected to the engine through a clutch and a 5-speed manual transmission. It can absorb up to 225kW and also supply motoring to the engine to a maximum torque of 156N.m in a range of 200–3400rpm. The speed and the torque of the dynamometer can be adjusted by a dyno-controller for different measurements and collecting experimental data. The engine installation in the test cell is shown in Fig. 1.A cooling pump separated from the main drive belt can be used to cool the engine quickly. In addition to the factory supplied sensors, the engine is equipped with a couple of broadband air/fuel ratio (AFR) sensors, intake manifold pressure sensor, FID hydrocarbon analyzer, thermocouples and an encoder on the crank shaft. All the sensors and the dyno-controller can communicate with a rapid control prototyping (RCP) unit that has performance characteristics similar to the production ECU.The RCP unit is employed to implement and evaluate the new engine controllers, which change the throttle, AFR, spark timing (Δ) and variable valve timing to achieve certain performance objectives. The throttle angle and Δ can also be manually controlled using potentiometers connected to the RCP unit. Furthermore, important variables, such as the engine speed (ϖe) and the catalyst temperature, can be monitored on a panel during the engine operation in the test cell.After a sensitivity analysis, it was observed that, AFR, Δ and ϖehave high influences on HCrawand Texh. Therefore, the authors decided to produce a reliable and robust model, which provides a map between AFR, Δ and ϖeas the inputs, and also, HCrawand Texhas the outputs.Here, we conduct three different experiments to elaborate on the authenticity of ensemble identifier. In the rest of the paper, these three independent experiments are known as Case 1, Case 2, and Case 3. As it is clear, each of these cases consists of two independent experimental stages to identify both Texhand HCraw. Figs. 2 and 3indicate the characteristics of measured inputs for predicting both Texhand HCraw.Ensemble networks have been widely used by researchers of CI community. Recently, such topologies have gained an increasing attention, and the areas of their applications encompass the design of neural systems, neuro-fuzzy systems and even natural-inspired metaheuristics [44]. The basic idea is to integrate a set of independent identifiers known as sub-components of ensemble. Generally, we would like to generate an ensemble paradigm with high rate of diversity. The main reason to design a diverse ensemble emanates in the pursuit of developing an intelligent system capable of mitigating the undesired effects of uncertainty and random noises. In spite of the promising characteristics of ensemble networks, in practice, they have some drawbacks such as high computational complexity, dependency to precision of sub-components and their calibration capabilities, and requiring an effective controlling algorithm (trigger of ensemble) to fuse the information of each sub-component. As it is obvious, without considering all aforementioned issues, one may not be able to design an effective ensemble framework.To design an efficient ensemble framework, some criteria and issues should be taken into account:(1)Selecting an appropriate learning strategy is among the most crucial aspects of designing an ensemble framework. It seems that several concerns should be fulfilled. As an example, to what extent, the learning strategy influences the generalization, accuracy and robustness of the ensemble framework? Is it rational to employ complex and hybrid learning techniques for training the ensemble networks? What are the main characteristics of an effective learning technique? Is there a direct relation between the use of hyper-level topology design and generalization of ensemble classifiers? Such questions persuade the researchers of CI community to extensively work on different learning aspects. It seems that using a hybrid learning technology, which fosters the use of both traditional and modern optimization approaches would be a logical answer to most of the abovementioned questions [39,42]. However, one should be aware that the characteristics of hybrid learning algorithms should be wisely selected to effectively reconcile the performance to the requirements of an ensemble network.As the ensemble network entails a set of sub-identifiers, its computational complexity remains an important concern. When designing ensemble architectures, one should seek for the most qualified topology design algorithms. In other words, the lower the computational complexity of the network, the better the quality of the topology design algorithm. However, the designer should always consider a logical trade of between computational complexity and system accuracy. As it is clear, these two features do not have a direct relation. Therefore, that would be necessary to design a computationally efficient yet accurate ensemble identifier.Which are the realms of applications of ensemble identifiers? Are these model suited for control problem? As it was mentioned in previous paragraph, the computational complexity of ensemble frameworks, including ensemble identifiers, is relatively high. Hence, they may not be best suited for online modeling, incremental learning [34] and controlling applications. However, as their accuracy and robustness is quite high, they can be applied to off-line identification as well as optimization problems. Furthermore, one can train ensemble identifiers in an off-line fashion and then use them for online monitoring issues. The CI society has been trying really hard to discern the best realms of applications of ensemble identifiers [45].There exist several theoretical and practical reports in literature trying to fulfill the abovementioned concerns. Furtuna et al. [39] utilized a multiobjective hyper-heuristic evolutionary algorithm to design a set of comparative ensemble topologies for system identification tasks. The obligation of hyper-heuristic was to design the structure of sub-components of ensemble. Thereafter, the quasi-Newton algorithm was applied to tune the parameters of each sub-component. Based on experimental results, they observed that integration of hyper-level topology design and low-level parameter tuning provides a strong learning scheme for ensemble design. In spite of successful performance of their method, it was observed that the devised strategy requires a very high computational time for ensemble network design. To decrease the computational burden, Mozaffari and Fathi [42] replaced the multiobjective optimizer with the great salmon run (TGSR) optimization algorithm. Besides, to find a set of competent ensemble networks, they introduce preference to topology design problem. Tsakonas and Gabrys [46] proposed an evolutionary framework for production of fuzzy rules, which in turn delegate an ensemble of predictors. Their method proved its applicability for automotive design of predictive models. For classification tasks, Scherer [47] proposed an ensemble of neuro-fuzzy systems. Each sub-component was based on logical-type inference systems as it has been proved that such inference mechanism yields highly reliable results for classification. Based on extensive numerical study, it was demonstrated that the ensemble technique can outperform other techniques or at least show comparative results. Ensemble frames proved their applicability for other machine learning tasks such as classification [48] and medical diagnosis [49]. Along with machine learning tasks, recently, the realms of applications of ensemble computing have been extended to modern optimization. Mallipeddi et al. [50] proposed an ensemble evolutionary programming framework for function optimization. The idea was to build up a set of populations and allocate a specific type of mutation for each of them. The results proved that the ensemble technique is capable to effectively handle a wide spectrum of optimization problems. For further elaboration, the authors conducted a rigor comparative study and realized that the integration of different mutation operators cannot be as effective as ensemble paradigm. Yu and Suganthan [44] developed an ensemble of niching algorithms in which a set of independent niching techniques were combined to form a more qualified optimizer. The results of experiments prove that the ensemble frame yields better results as compared to single niching techniques. Mozaffari et al. [51] proposed an ensemble version of mutable smart bee algorithm for optimizing the operating parameters of a large-scale power plant. To elaborate the authenticity of the technique, it was compared to a set of bee-inspired techniques as well as sole mutable smart bee algorithm. Based on obtained results, the authors concluded that the ensemble frame outperforms other rival optimization techniques for large-scale problems.As it can be seen, the versatility of ensemble approaches strongly attracts the attention of CI researchers, and every day we have new reports on applications of ensemble approaches especially for function approximation. Recently, Lee et al. [43] proposed a novel learning strategy to develop an ensemble network with high generalization capability. The salient asset of their strategy lies on the use of hierarchical fair competition strategy as well as novel sub-component selection constraint. To design ensemble neural network, they used hierarchical fair competition-based parallel genetic algorithm (HFC-PGA) to simultaneously design the structure and tune the weights of sub-identifiers, which were multi-layered feed-forward (MLFF) neural networks in their case. They reported that such a policy is highly promising since we can automatically design a set of sub-components with different types of characteristics.In the light of promising outcomes of that research, here, the authors are persuaded to follow a relatively identical strategy to design an ensemble identifier to model the hydrocarbon emission over coldstart period. However, there are some major differences between our strategy and that proposed in Ref. [43].(1)Instead of using GA, we foster the use of swarm-based (PSO) techniques. This is because they are quite simple, effective and fast. Besides, PSO is equipped with a fuzzy controller which guarantees the convergence of exploration in a predefined partition [40].In our ensemble frame, neuro-fuzzy radial basis network (NFRBN) is selected as the base component. As the identification of HCrawover the coldstart period is quite uncertain, we take the advantages of fuzzy computing. The fuzzy rules are beneficial for capturing the uncertainty of the database.Given the promising results reported in Refs. [39,42], a bi-level learning strategy is used for training each NFRBN. At hyper-level, HFC-APSO is utilized to determine the fundamental characteristic of fuzzy system and at lower-level, neural learning techniques i.e. BP and steepest descent algorithms are utilized to tune some antecedent parameters as well as rule consequents.In what follows the section, steps required for designing the proposed ensemble identifier are scrutinized.Radial basis networks (RBN) are type of generalized black-box approximators in which the activation function of each neuron is a basis functions (e.g. gaussian, cubic, thin-plate spline, multi-quadric and etc.). Steps required for training such networks is a bit different from classic feed-forward neural networks. Although estimation of connection weights can be considered as a regression problem, the characteristics of basis function should be determined using trial and error procedure. Through several researches, it has been demonstrated that when the operating parameters of RBN are adjusted properly, such networks are highly capable of learning the physics of a system [52]. Fig. 4indicates the topology of RBN.Let us consider the gaussian radial basis function as the activation function, the firing of each neuron in hidden layer is calculated as:(1)u(i)=exp−||X−Ci||22σt2The estimated output of the network is calculated as:(2)yo=∑i=1jωi⋅u(i)where i=1,…, j shows the number of neurons of hidden layer, i indicates the ith neuron of hidden layer, Ciis the center of basis function of ith neuron, and σidelegates the width of basis function of ith neuron.As it can be inferred, the architecture of RBN has many things in common with neuro-fuzzy inference systems. Besides, it seems that transforming the crisp input-output to fuzzy space mitigates the difficulties associated with learning process. This is because we can easily adapt the operating parameters using TSK type neuro-fuzzy learning concepts. Besides, the mapping between input-output spaces (product spaces) is done using fuzzy rules and fuzzy inference engine instead of using a full-rank connected complex network. Furthermore, adding the fuzzy computing concepts to the classic RBN is highly useful for handling uncertainties associated with practical engineering problems.Taking the abovementioned advantages into account, the authors intend to boost the performance of RBN by turning it to a fuzzified version (NFRBN). To do so, following criteria should be considered:(1)At input layer, gaussian membership functions (MFs) are used to fuzzify the input variables.At hidden layer, each hidden neuron represents a rule and therefore the resulted hidden layer is equal to rule base.The consequent part of each rule is a constant value.The overall output of the neuro-fuzzy system is obtained by weight average (WA) defuzzification mechanism.Fig. 5depicts the architecture of NFRBN identifier. Obviously, the structures of RBN and NFRBN are really similar.To provide the crisp data-base for NFRBN, all variables of the input vector need to be fuzzified. Let us consider that each input vector includes r variables as X=[x1x2, …, xr]T. The fuzzification is accomplished by transforming the input vectors from crisp values to Kifuzzy linguistic expressions, i.e. {A1(j), A2(j),…, AKi(j)}, where j=1, …, r. Here, all fuzzy linguistic terms are of gaussian shapes. Such a MF helps us reserving the properties of classic RBNs. Fig. 6indicates a schematic illustration of fuzzification procedure.After adjusting the antecedent parameters, the linguistic terms are trusted to neurons of hidden layer (i.e. elements of rule base) to develop a set of rules. In this paper, the characteristics of rules are determined at hyper-level optimization stage. Consider the each NFRBN uses R rules for inference. Hence, ith rule (i=1, …, m) in the rule base has the following form:(3)R(i):IFx1isAi1(1)AND…ANDxrisAir(r)THENy(i)=ω(i)here the consequent part of fuzzy rules is considered to be a constant value (ω). To decrease the complexity of computation and also augment the interpretability of fuzzy inference system, a complexity reduction algorithm should be contrived. Such a policy helps us to gradually decrease the number of rules by removing the inefficient and vague ones. This can be easily done by using a very simple pruning algorithm:(1) Compute the percentage of pattern concentration for each rule using the following equation:(4)n¯i=niN×100where i=1, …, R and nirepresents the number of training patterns coincided with Ailinguistic by a degree of firing greater than Θ (i.e. μi(x)≥Θ).(2) Sort the rules in a descending order with respect to their percentage of pattern concentration, and define a threshold value ζ.(3) Extract the eligible rules (R˜) with criterionn¯i≥ζ.It is worth mentioning that all of the above soft thresholds are determined at the hyper-level optimizer.At this stage, the degree of firing of each rule can be calculated as:(5)μi(x)=∏j=1rμij(i)(xj)whereμrj(i)(xj)=exp−xj−Crj2σrj2.After calculating the degree of firing as well as consequent parameters of each rule, the overall output is obtained using WA defuzzification technique:(6)yo=∑i=1mμi(x)∑i=1mμi(x)⋅ω(i)Training the sub-components of ensemble entails two different phases, i.e. hyper-level learning and low-level learning. The hyper-level learning deals with arranging the architecture of NFRBN while the low-level learning aims at adapting the operating parameters of identifier. Besides, instead of using a sole optimizer at hyper-level, the authors engage a set of independent optimizers in a parallel fashion. After termination of learning procedure, we expect to have a set of NFRBNs of different characteristics. This helps us to construct a diverse ensemble with acceptable rate of generalization. At the most promising condition, by dividing the solution domain of hyper-optimizer into N partitions, we can achieve N independent NFRBNs of different characteristics. Fig. 7indicates the schematic illustration of partitioning of hyper-level solution space. In practice, some of the partitions in solution domain have similar characteristics. Hence, number of independent NFRBNs is usually less than number of partitions. In this research, we use the idea of negative correlation to find those NFRBNs with different characteristics. This procedure will be scrutinized later in this section.As it was mentioned, the aim of hyper-level training is to design a set of NFRBN architectures as members of ensemble identifier. To this end, we conduct hierarchical parallel fair completion optimization [53]. The core optimizer is an adaptive version of PSO algorithm [40]. Since the solution space of optimization is divided into a limited number of partitions, we have to balance the exploration/exploitation of PSO so that it is predisposed to conduct an intensified exploitation. The reason to this issue is really clear. Suppose that the goal of optimization is to find a set of eligible solutions, and not the global one. As we know, this evokes the concept of niching optimization [44]. One strategy is to divide the solution space (in our case, the admissible range of constructive elements of NFRBN) into a set of sub-spaces (partitions), and then explore each of these partitions. Obviously, the obligation of optimizers is to find the best solutions in their assigned partition. However, to avoid blind (un-guided) exploration, that is also necessary for the optimizers to be informed about the best solutions in other partitions. To address such requirements, the authors supply following provisions:(1) To guarantee an effective and appropriate optimization, we adopt fuzzy adaptive PSO [40]. The fuzzy controller incrementally evolves the parameters of PSO so that we can have a desired balance between exploration and exploitation. Algorithm 1 explains the stepwise procedure required for implementing the adaptive PSO (APSO). Fig. 8a tries to graphically reveal the purpose of using a set of intensified optimizers (APSOs) for finding different solutions.Algorithm 1 Pseudo-code of APSO algorithm1:begin2:Define the initial parameters (number of particles (n), number of iterations (t), ω, υ, and ψ)3:Objective function f(X), X=(x1, …, xd)T4:Generate initial population of n particles Xi(i=1, 2, …, n)5:Define the inputs of controller (ω0 and normalized current best performance evaluation (NCBPE))6:Formulate the left triangle, triangle and right triangle according to Ref. [40]7:Define the stopping criteria8:while Stopping criteria are not met9:t=t+110:Compute the velocity of each particle11:Update the positions of particles12:Update the values of fuzzy controller inputs (ω (t) and NCBPE (t))13:Fed the inputs to the rule-base of fuzzy controller and calculate the required changes, Δω (t)14:end15:Visualize the values of over optimization procedure* It is worth noting that the value of NCBPE is calculated in a real-time fashion based on the best and worst obtained objective values of population(2) To guarantee the guided exploration within each partition, the concept of hierarchical parallel fair competition-based optimization is utilized. This trait enables us to provide a link (information chain) between each optimizer, and help them to think globally. At the end, we expect that each optimizer yields effective and independent characteristics for NFRBN. Fig. 8b provides a graphical vision to disclose the concept of using information chains between independent optimizers (this figure only shows a sample connection).As it was mentioned before, the main structural characteristics of NFRBN is verified by HFC-APSO. These characteristics are listed below:(1) Firstly, the algorithm determines the number of linguistic expressions for each of the input variables. In our case, the admissible range for number of MFs of each input is:(7)2≤MF≤5As it is clear, the value of each variable is integer. Although, the PSO algorithm is designed for continuous optimization, it can be used for integer programming as well. For example, by using the floor command in MATLAB, we can extract the integer value of each continuous number.(2) At the next step, number of rules and their detailed characteristics should be defined. The admissible range of rules directly depends on number of linguistic expressions for each variable. Considering Kilinguistic terms for each variable, the maximum number of rules is calculated as:(8)R=∏j=1rKijTherefore, the admissible range of rules in NFRBN is calculated as:(9)1≤R˜≤RAfter that, we need to verify the characteristics of each rule. This is done by a very simple encoding mechanism. Let us consider R decision variables with respect to R maximum number of rules. Now, we just need to fill each of these R decision variables. It is worth mentioning that there is no obligation to fill all of the R decision variables. In other words, if the number of rules is less than maximum potential of rules (R˜<R), we have a set of null variables within the particle. Along with an integer number which indicates the number of rules, we need to devise an encoding policy to verify the characteristics of each rule.Let us give an example to explain the encoding procedure. Consider we have three inputs (r=3) which are fuzzified using 2, 4, and 3 linguistic expressions, respectively (first step). Now, the maximum number of rules can be calculated as 2×4×3=24. Hence the admissible range of is in the range of1≤R˜≤24. Therefore, we can fill 24 variables in rule section of each particle. Suppose that PSO considers three rules for inference engine. So, we need to fill three of those decision variables and the rest remain null. Now, another type of encoding should be utilized to determine the characteristics of rule base. It is clear that the available linguistic expressions are as:{First variable: 1, 2}, {Second variable: 1, 2, 3, 4}, {Third variable: 1, 2, 3}Those three decision variables can be filled with any combination of the above numbers, e.g. 213, 243, 111. This combination yields the following rules:R(1):IFx1isA12(1)ANDx2isA11(2)ANDx3isA13(3)THENy(1)=g(1)R(2):IFx1isA22(1)ANDx2isA24(2)ANDx3isA23(3)THENy(2)=g(2)R(3):IFx1isA31(1)ANDx2isA31(2)ANDx3isA31(3)THENy(3)=g(3)(3) In the last step, the hyper-optimizer tries to find the optimum threshold values (Θ and ζ) of complexity reduction algorithm. These values are continuous and their admissible ranges are:(10)0.1≤Θ≤0.5,0.001<ζ<0.1Fig. 9indicates the structure of each particle.After explaining the structure particles in one APSO, we need to extend the optimization concept to a parallel frame in which a set of independent APSOs work in tandem to yield a set of optimum solutions. From machine learning point of view, such a policy helps us to maintain the diversity of the optimum NFRBN architectures. Generally, the provocation for parallelism of the hyper-optimizer is twofold. In one hand, this aids the optimizer to automatically search for a set of optimum solutions instead of a sole solution. It is worth noting that, even if a sole optimizer converges to the global solution (which is not possible for most of practical problems), inevitably, the other optimum potentials of the problem are neglected. This hinders the designer from a practical engineering judgment. On the other hand, by considering a set of optimizers and assigning each of them to a predefined partition, we automatically improve the quality of obtained solutions. As a conclusion, for multimodal optimization, using a hierarchical optimizer is beneficial in terms of both exploration and exploitation, and also provides us a diverse set of optimum solutions which includes the most qualified local solutions as well as the global one. Such conclusions were also obtained in previous research. According to Ref. [53], using hierarchical optimization concept (GA in their case) decreased the possibility of premature convergence and preserved the diversity of population.The concept of hierarchical competition-based optimization used in this research is a bit different from that introduced in Ref. [43]. The main difference lies in the use of swarm-based approaches (APSO in this study). The exploration/exploitation strategies pursued in APSO suits it to be embedded with HFC concept. As we know, particles in APSO are in interaction with local (archived memory) and global best optimums, and the type of random walk used in APSO guarantees such an interaction. According to HFC theory, optimization domain is divided into a set of partitions and independent optimizers are engaged to perform the optimization. Particularly, the main goal of multiple search mechanism is to protect the new spotted but promising particles from being neglected or even discarded. This is because, although such particles do not have the highest fitness, after some iteration, they may promote their fitness and consequently improve the quality of best solution (gbest). To embed the idea of HFC inside APSO, a simple yet effective strategy is considered. Firstly, we divide the solution domain into N partitions and assign independent APSO algorithms to explore/exploit each sub-region. After termination of optimization process, we sort all independent optimizers with respect to value of gbest so that the ranks of best and worst optimizers are 1 and N respectively. After that, a threshold criterion should be defined which enables independent optimizers of lower quality to interact with more qualified ones. In our algorithm, the best solution of an optimizer of lower rank (APSOL) can dispatch its gbest to a more qualified optimizer (APSOU) if and only if:(11)Bestfitness(APSOL)>Meanfitness(APSOU)The schematic illustration of the idea is depicted in Fig. 10. The detailed description of algorithmic steps required for implementing the concept is given in Algorithm 2.Algorithm 2 Pseudo-code of HFC-APSO1:begin2:Define the initial parameters (number of partitions (N), number of particles (n), number of iterations (t), ω, υ, and ψ)3:Divide the solution space to N parts so that each section includes a certain landscape from solution domain4:Objective function f(X), X=(x1, …, xd)T5:Assign an independent APSO for each partition6:Generate initial population of n particles Xi(i=1, 2, …, n) for each APSO7:Define a criterion for interaction among independent APSOs8:Define the stopping criteria9:Define the controlling parameters of APSOs as stated in Algorithm 110:Define number of iterations (t1) and sub-iterations (t2) for HFC-APSO and APSO respectively11:while Stopping criteria are not met12:t1=t1+113:Let all independent APSOs to conduct the optimization procedure for t2iterations (Algorithm 1)14:Save the final population of each APSO15:Calculate the mean and best objective values of all APSOs16:Sort all APSOs in a descending manner17:Check the interaction criterion between two successive APSOs18:for Each two successive APSOs19:if Interaction criterion between jth and kth APSOs is met20:Let the APSO of lower rank to share its best solution with APSO of higher rank21:end22:end23:Extract two of best particles from each APSO and concatenate them in a matrix24:Consider the matrix as the solutions submitted by HFC-APSO25:end26:Submit the final solutions of HFC-APSO27:Visualize the diversity among obtained particlesAs it is clear, such a hyper-level optimization approach provides us with a diverse set of sub-components of different properties. This consequently enables us to augment the robustness and generalization capability of ensemble identifier. The first stage of learning is the most important and crucial part of designing an ensemble identifier. This is because the architecture design of NFRBN is associated with several stochastic and uncertain features. The most important concerns are:(1)The desired complexity of the rule base is not unique.There is no compromise on optimum number of linguistics expressions for each input.It is not clear that for which combination of diverse sub-components, the ensemble identifier yields the best results.As it can be seen, the use of stochastic optimization techniques could be a wise choice for architecture design. This refers to essence of such methods which help us capturing the uncertainties of the problem, especially, those mentioned above.After the hyper-level optimization, two straightforward and deterministic strides should be taken to finalize the characteristics ensemble identifier.Once the architecture of NFRBN is determined via hyper-level optimizer, its structural parameters are adjusted using neuro-computing learning concepts, i.e. back propagation (BP) and steepest decent techniques. As it can be inferred, for low-level optimization, the authors prefer to use deterministic approaches. This is because of their speed, simplicity and reliability. Although the integration BP and steepest descent algorithms does not yield a global optimizer, its applicability has been acknowledged by various researchers. This is because such a learning technique is capable to yield a very desirable solution within a reasonable time. However, it is worth noting that to improve the quality of optimum obtained solution, the authors add a momentum term to optimization equation. This is explained later in this section. As it was mentioned before, in this research, the authors assign the optimization of shape of MFs as well as optimum values of consequent parameters to low-level trainer. Hence, we need to optimize the shape features of gaussian MFs (C and σ) and the consequent values of each rule (ω).Based on the error for each sample, we devise a propagation mechanism which intends to adjust the parameters of the fuzzy system to increase the prediction accuracy. The BP based steepest descent learning algorithm can be mathematically expressed as [54]:(12)Crj(i)(t+1)=γCCrj(i)(t)−ηC∂E∂Crj(i)(13)σrj(i)(t+1)=γσσrj(i)(t)−ησ∂E∂σrj(i)(14)ω(i)(t+1)=γCω(i)(t)−ηC∂E∂ω(i)where the derivative used in above equations is simplified as:(15)∂E∂Crj(i)=(yo−yd)ω(i)(t)∑i=1mμi(x)−∑i=1mω(i)(t)μi(x)∑i=1mμi(x)2μi(x)(1−μir(i))xr−Crj(i)(t)(σrj(i)(t))2(16)∂E∂σrj(i)=(yo−yd)ω(i)(t)∑i=1mμi(x)−∑i=1mω(i)(t)μi(x)∑i=1mμi(x)2μi(x)(1−μir(i))(xr−Crj(i)(t))2(σrj(i)(t))3(17)∂E∂ω(i)=(yo−yd)μl(x)∑i=1mμl(x)where η represents the learning rate, γ is the momentum, ydrepresents the desired output, yoindicates the prediction of NFRBN, i represents the ith rule, r indicates the input number, j shows the jth linguistic expression of rth input, and t delegates the tth epoch of BP learning.As it was mentioned, there are many techniques in literature addressing the ways for combining the components of an ensemble. Negative correlation is one of the most effective and applicable ones, which automatically guarantees the diversity of components in the ensemble. To the best knowledge of authors, a research by Liu et al. [55] is one the seminal works on effectiveness of negative correlation in field of ensemble computing. On that paper, evolutionary learning and negative correlation concepts were used to train independent neural networks. It was observed that the use negative correlation provides an ensemble in which each component learns different characteristics of training database. They concluded that such a trait in turn increases the generalization capability of ensemble. Such promising findings have persuaded different researchers of theoretical and engineering societies to take the advantages of negative correlation for designing ensemble identifiers or classifiers. Minku et al. [45] investigated the potentials of negative correlation and ensemble computation for incremental learning. Based on experiments, they concluded that the use of negative correlation can be useful for mitigating the effect of catastrophic forgetting as a major flaw of on-line learning techniques. Tang et al. [56] extended the concept of negative correlation incremental learning by conducting a much more rigor investigation into its different characteristics. Kim and Cho [48] used the concept of negative correlation to produce an ensemble classifier for DNA micro-array classification.Recently, Lee et al. [43] provided a mathematical proof which demonstrated how negative correlation can be used for designing a diverse ensemble classifier. In Appendix A, we indicate that a same approach can be used to prove the effectiveness of negative correlation for designing an ensemble identifier with good generalization capability. Based on what provided in Appendix A, one can easily understand that negative correlation augments the generalization of ensemble identifier as well. The goal is to retain minimum correlation between sub-components of ensemble.To do so, we need to define a metric which aids us realizing when we should stop adding components to ensemble. The stopping criterion relates to both accuracy and generalization of ensemble identifier. Let us consider that the metric is expressed as prediction error (Eˆ). Obviously, a higher accuracy of prediction affords a lower value of metric, and vice versa, the higher the correlation among components is, the lower the metric value tends to be. The new component (S+1) can be accepted as a member of ensemble if and only ifEˆ(S+1)<Eˆ(S). This implies that adding component (S+1) to ensemble improves its capabilities in terms of both accuracy and generalization. Algorithm 3 provides the algorithmic steps required for implementing selective ensemble identifier. Finally, based on what presented in this section, the stepwise algorithmic procedure for inspiring the ensemble technique is given in Algorithm 4.Algorithm 3 Pseudo-code of selecting NFRBNs for ensemble identifier1:begin2:Consider a set of potential NFRBNs (these identification networks are obtained using HFC-APSO)3:Calculate the prediction error of each NFRBN4:Calculate the negative correlation of each NFRBN with respect to other potential NFRBNs5:Determine the eligibility index for each NFRBN (Eq. (19))6:for All components7:if Eligibility index is larger than a predefined threshold8:Select the component as a member of ensemble: Ensemble=Ensemble ∪ New Component9:end10:if prediction error of new ensemble is lower than old ensemble11:Accept the concatenation of new components to the ensemble12:end13:end14:Archive the final ensemble identifierAlgorithm 4 Pseudo-code of designing the ensemble identifier1:begin2:Define the initial parameters and thresholds for all stage of ensemble designing3:////Hyper-level optimization4:Obtain a set of potential NFRBNs with different characteristics5:////Low-level optimization6:Optimize the parameters of all rough architectures obtained at hyper-level learning7:////Component selection based on negative correlation8:Obtain the eligible components9:Combine those components to form the ensemble model10:////Termination of designing procedure11:Archive the final ensemble identifier

@&#CONCLUSIONS@&#
