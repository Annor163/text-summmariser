@&#MAIN-TITLE@&#
Classification of gene expression data: A hubness-aware semi-supervised approach

@&#HIGHLIGHTS@&#
A semi-supervised hubness-aware classifier is proposed.The classifier is evaluated on publicly available real gene expression data.We made the implementation of hubness-aware machine learning techniques available in the PyHubs software package.

@&#KEYPHRASES@&#
Gene expression,Machine learning,Semi-supervised classification,High dimensionality,

@&#ABSTRACT@&#
Background and objectiveClassification of gene expression data is the common denominator of various biomedical recognition tasks. However, obtaining class labels for large training samples may be difficult or even impossible in many cases. Therefore, semi-supervised classification techniques are required as semi-supervised classifiers take advantage of unlabeled data.MethodsGene expression data is high-dimensional which gives rise to the phenomena known under the umbrella of the curse of dimensionality, one of its recently explored aspects being the presence of hubs or hubness for short. Therefore, hubness-aware classifiers have been developed recently, such as Naive Hubness-Bayesian k-Nearest Neighbor (NHBNN). In this paper, we propose a semi-supervised extension of NHBNN which follows the self-training schema. As one of the core components of self-training is the certainty score, we propose a new hubness-aware certainty score.ResultsWe performed experiments on publicly available gene expression data. These experiments show that the proposed classifier outperforms its competitors. We investigated the impact of each of the components (classification algorithm, semi-supervised technique, hubness-aware certainty score) separately and showed that each of these components are relevant to the performance of the proposed approach.ConclusionsOur results imply that our approach may increase classification accuracy and reduce computational costs (i.e., runtime). Based on the promising results presented in the paper, we envision that hubness-aware techniques will be used in various other biomedical machine learning tasks. In order to accelerate this process, we made an implementation of hubness-aware machine learning techniques publicly available in the PyHubs software package (http://www.biointelligence.hu/pyhubs) implemented in Python, one of the most popular programming languages of data science.the set of k-nearest neighbors of x.probability that x belongs to class C given its nearest neighborsthe probability of the event that x appears as one of the k-nearest neighbors of any labeled training instance belonging to class Cthe prior probability of the event that an instance belongs to class Chow many times x occurs as one of the k-nearest neighbors of labeled training instances belonging to class Chow many times x occurs as one of the k-nearest neighbors of other instances when consideringDlab∪{x}

@&#INTRODUCTION@&#
Various tissues are characterized by different gene expression patterns. Additionally, a number of diseases and disease subtypes may be associated with characteristic gene expression patterns. Therefore, recognition tasks related to gene expression data may contribute to the diagnosis of various diseases such as colon cancer, lymphoma, lung cancer and subtypes of breast cancer [9]. Due to the large amount of data (e.g., even if we consider just a single patient, expression levels of thousands of genes may be measured), such recognition tasks are typically solved by computers, and state-of-the-art solutions are based on machine learning.In case of supervised machine learning, a previously collected dataset (e.g., gene expression levels measured for a set of patients) together with evidence or indication (e.g., the presence, absence or subtype of a particular disease for each patient) is used to induce a decision model, called classifier. Once the classifier is induced, it will be able to solve the recognition task for new data instances (e.g., the classifier will be able to recognize the subtype of cancer for new patients). With training the classifier we refer to the induction of the model, while the data used to induce the model is called training data. If the data is associated with evidence, it is called labeled data, e.g., a labeled dataset may contain gene expression levels together with the information describing which patient has which subtype of cancer, in contrast, if only the gene expression levels are available without knowing the subtype or presence of the disease, the dataset is unlabeled. The value of the evidence is called label, e.g., if a patient has estrogen receptor positive (ER+) subtype of breast cancer, we say its label is “ER+” (at the technical level, labels are usually coded by integer numbers, such as 0 for “ER+” and 1 for “ER−”).The classification task is challenging for several reasons. Usually, the expression levels of several thousands of genes are measured, therefore, the data is high-dimensional which gives rise to the phenomena known under the umbrella of the curse of dimensionality[3]. While well-studied aspects of the curse are the sparsity and distance concentration, see e.g., [19], a recently explored aspect of the curse is the presence of hubs [14], i.e., instances that are similar to surprisingly many other instances. According to recent observations, the presence of hubs characterizes gene expression datasets [10,15]. A hub is said to be bad if its class label differs from the class labels of those instances that have this hub as one of their k-nearest neighbors. In the context of k-nearest neighbor classification, bad hubs were shown to be responsible for a surprisingly large portion of the total classification error.Recently, algorithms have been developed under the umbrella of hubness-aware data mining, see e.g., [5,12,13,15,22,25,26,20,21]. These algorithms try to recognize bad hubs and reduce their influence on classifications of unlabeled instances.It may be expensive (or even impossible in case of rare diseases) to collect large amount of labeled gene expression data, therefore, we have to account for the fact that only relatively few labeled instances are available which may not reflect the structure of the classes well enough. Therefore, while training the classifier, in addition to learning from labeled data, the classifier should be able to use unlabeled data too in order to discover the structure of the classes.In this paper we introduce a semi-supervised hubness-aware classifier, i.e., a classifier that uses both labeled and unlabeled data for training. In particular, our approach is an extension of the Naive Hubness Bayesian k-Nearest Neighbor, or NHBNN for short [23], which is one of the most promising hubness-aware classifiers. As we will show, straightforward incorporation of semi-supervised classification techniques with NHBNN leads to suboptimal results, therefore, we develop a hubness-aware inductive semi-supervised classification scheme. We propose to use our classifier for recognition tasks related to gene expression data. To our best knowledge, this paper is the first that studies hubness-aware semi-supervised classification of gene expression data.

@&#CONCLUSIONS@&#
