@&#MAIN-TITLE@&#
Robust estimation of the parameters of

@&#HIGHLIGHTS@&#
Introduced simple methods to estimate theg-and-hdistributional parameters.Proved consistency and asymptotic normality.Effective robust version introduced.Robust version used to obtain base distribution for outlier detection.Illustrated use of proposed methods to multiple fields.

@&#KEYPHRASES@&#
g,-and-,h,distribution,Indirect inference,Least squares,Maximum likelihood,Outlier detection,Quantiles,Robust,

@&#ABSTRACT@&#
Theg-and-hdistributional family is generated from a relatively simple transformation of the standard normal and can approximate a broad spectrum of distributions. Consequently, it is easy to use in simulation studies and has been applied in multiple areas, including risk management, stock return analysis and missing data imputation studies. A rapidly convergent quantile based least squares (QLS) estimation method to fit theg-and-hdistributional family parameters is proposed and then extended to a robust version. The robust version is then used as a more general outlier detection approach. Several properties of the QLS method are derived and comparisons made with competing methods through simulation. Real data examples of microarray and stock index data are used as illustrations.

@&#INTRODUCTION@&#
Theg-and-hdistributional family was introduced by Tukey (1977). Theg-and-hdistributed random variable is a monotone transformation of the standard normal variable. Thepth quantile ofg-and-hrandom variableTis defined as:(1)F−1(p)=TA,B,g,h(zp)=A+Begzp−1gehzp2/2,B>0,h≥0,g≠0,(1a)F−1(p)=TA,B,g,h(zp)=A+Bzpehzp2/2,B>0,h≥0,g=0,where,zpis thepth quantile of the standard normal distribution,A,B,gandhare parameters, withgcontrolling skewness andhelongation. This family and its extension have been studied by Martinez and Iglewicz (1984), Hoaglin (1985), Field and Genton (2006) and Rayner and MacGillivray (2002a,b), with the latter providing maximum likelihood estimates of the parameters.Theg-and-hdistributional family approximates a rich class of common univariate distributions which could accommodate different levels of skewness and elongation and can approximate a wide variety of commonly used distributional shapes, such as normal, lognormal, Weibull and exponential (Martinez and Iglewicz, 1984). It also approximates most of the Pearson family (Pearson, 1895) and many other distributions that do not have finite first four moments, such as Cauchy distribution (Martinez and Iglewicz, 1984). This distributional family has been used in a variety of more recent publications, including Badrinath and Chatterjee (1988, 1991), Dutta and Babbel (2002, 2005), Dutta and Perry (2007), Jiménez and Arunachalam (2011), Field (2004), He and Raghunathan (2006), Kafadar and Phang (2003), Mills (1995), Strandberg and Iglewicz (2013) and Walters and Ruscio (2009).Hoaglin (1985) described the Tukey (1977) parameter estimation approach, which uses a letter value based sequence of quantiles to estimate the parameters sequentially. In this work, the slightly modified version of the letter value based method (LV) is used with following steps: (1) Estimate parameterAby the sample median; (2) Selectpi=0.005,0.01,0.025,0.05,0.10, and 0.25 to calculategpi=−1zpilog(x1−pi−x0.5x0.5−xpi), wherexpis the observedpth sample quantile; then estimategby median of thegpi’s; (3) Regresslog(g(x1−p−x0.5)e−gzp−1)orlog(g(x0.5−xp)1−egzp)onzpi2/2depending ongpositive or negative to estimateBandh. By adjusting quantile selections, Hoaglin (1985) demonstrated that a quantile estimation method works well forgandhestimation, especially in exploratory data analysis. Rayner and MacGillivray (2002a) derived numerical maximum likelihood estimation (NMLE) method for the generalizedg–horg–kdistributions, which can be modified to provide reliable estimates for theg-and-hparameters. Rayner and MacGillivray (2002b) introduced weighted quantile-based estimation of this family, which is suitable for small sample sizes. Currently availableg–and–hparameter estimation methods are not resistant to a moderate amount of contaminated observations or outliers. Meanwhile, NMLE is a computational time consuming method when data sample size is large. One aim of this paper is to provide two versions of a newly proposed and relatively simple iterative approach for estimating theg-and-hparameters, with the second option being more robust and thus more resistant to a moderate proportion of contaminated observations. Some theoretical properties of these two estimating methods are derived and a simulation comparison performed. The simulation studies indicate that these proposed methods have potential for use in estimating theg-and-hparameters, independent of potential outlier detection uses.The second aim of this paper deals with applying theg-and-hdistributional family as a univariate outlier detection method. In general, the detection of outliers deals with deviations from a specific distribution, such as normal. Given a distribution, there are a number of methods for detecting outliers. For example, Barnett and Lewis (1994) discuss almost 50 procedures for detecting outliers from a normal distribution. Some outlier detection methods are based on robust estimators and work for a variety of distributions, once distributional choice is made. A more general approach is suggested by first estimating robustly theg-and-hparameters and then using this fittedg-and-hdistribution to detect outliers by utilizing a boxplot based approach first discussed in Hoaglin et al. (1986). The robustly estimatedg-and-hmethod also can be combined with the Benjamini and Hochberg (1995) based false discovery rate approach. The proposed two stage outlier detection approach, of first estimating theg-and-hparameters and then using this estimated distribution and boxplot or false discovery rate to identify outliers, will be investigated through a simulation study and illustrated through examples from several published data sets.The paper is organized as follows. Section  2 describes the proposed non-robust and robust quantile least squares (QLS) approaches and basic properties of the QLS estimator for the parameters of theg-and-hdistributional family. In Section  3, a simulation study is reported comparing the LV, NMLE, QLS and robust QLS (rQLS) methods for estimation of theg-and-hdistributional family parameters. Additionally, the estimates of the 95th and 99th percentiles are compared. The simulated random samples come from specific uncontaminated members of theg-and-hdistributional family plus an additional 5% of the observations from different distributions. This allows for the comparison of estimation methods plus studying the effect of outliers on the estimates. Section  4 summarizes the boxplot outlier detection procedure and results of the simulation study performed to investigate the use of rQLS estimated distributions for detection of outlier. Several real data sets are used in Section  5 to illustrateg-and-hparameter estimation and outlier detection. Conclusions are found in Section  6.LetXi,i=1,…,n, be i.i.d. from Tukeyg-and-hdistribution with parametersA,B,g,h,B>0,h>0and density functionfgh(x|A,B,g,h). Let0<p1<⋯<pm<1be known pre-specified numbers. For anyi=1,…,n, let us denote byzpithepith quantile ofN(0,1)and byξpithepith quantile ofg-and-hdistribution,ξpi=A+Bg−1{gzpi−1}exp(hzpi22),ifg≠0;ξpi=A+Bzpiexp(hzpi22),ifg=0.Letξˆpibe thepith sample quantile, which may be broadly defined as thenith order statisticx(ni)which satisfieslimn→∞nin=pi(Mosteller, 1946). In particular,ni=⌈npi⌉is used as the smallest integer not less thannpi. Let us denote the vector of unconstrained parameters byθ=[θ1,θ2,θ3,θ4], so thatA=θ1,B=exp(θ2),g=θ3,h=exp(θ4)and the parameter space isΘ=R4. Respectively, in this parameterization, the quantiles are(2)ξpi(θ)=θ1+eθ2θ3−1{exp(θ3zpi)−1}exp(eθ4zpi22),ifθ3=g≠0andξpi(θ|θ3=0)={θ1+exp(θ2)θ3−1{exp(θ3zpi)−1}exp(exp(θ4)zpi22)}, so that(2a)ξpi(θ|θ3=0)=θ1+eθ2zpiexp(eθ4zpi22).Further denote the vector of true quantiles byξm(θ)=[ξp1(θ),…,ξpm(θ)]and the vector of sample quantiles byξˆm=[ξˆp1,…,ξˆpm].The QLS estimatorθˆQLSofθis defined as(3)θˆQLS=argminθ{[ξˆm−ξm(θ)]T[ξˆm−ξm(θ)]}=argminθ∑i=1m{ξˆpi−ξpi(θ)}2.Theorem 1LetXi,i=1,…,n, be a random sample from Tukeyg-and-hdistributionfgh(x|θ0)with the true parameter vectorθ0=[θ10,θ20,θ30,θ40]∈Θ=R4such thatA=θ10,B=exp(θ20),g=θ30,h=exp(θ40). Under assumptions listed inAppendix  A, the QLS estimator defined in   (3)   is consistent for estimatingθ0and asn→∞n(θˆQLS−θ0)⟶DN4(0,(Σθ0TΣθ0)−1Σθ0TVθ0ξmΣθ0(Σθ0TΣθ0)−1);where[Σθ0]ij=∂ξpi(θ)∂θj|θ=θ0are given inAppendix  A   and matrixVθ0ξmhas the entries(4)[Vθ0ξm]jk=min(pj,pk)[1−max{pj,pk}]fgh(ξpj|θ0)fgh(ξpk|θ0).The proof is given inAppendix  A.Since the QLS estimator may be viewed as an indirect estimator (Gourieroux et al., 1993), its robust properties are inherited from the robust properties of the auxiliary parameter vector of sample quantilesξˆm(Genton and de Luna, 2000; Genton and Ronchetti, 2003). Theorem 1 in Genton and de Luna (2000) yields the influence function at the probability measureυdefining the contamination processFεIF(υ,θˆQLS,θ0)=limε→0+θˆQLS(Fε)−θˆQLS(Fgh)ε=(Σθ0TΣθ0)−1Σθ0TIF(υ,ξˆm,θ0),whereIF(υ,ξˆm,θ0)=[IF(υ,ξˆp1,θ0),…,IF(υ,ξˆpm,θ0)]is the influence function ofξˆm. For the Hampel’s original definition of influence function withυ=Δx(the measure with the point mass atx) andFε=(1−ε)Fgh+εΔx, the influence function ofξpiis given (Shao, 2003) byIF(x,ξˆpi,θ0)=[Fgh(ξpi|θ0)−I(−∞,ξpi](x)]/fgh(ξpk|θ0),which is bounded for any fixedp1>0andpm<1. Consequently, the corresponding influence function ofθˆQLSis bounded.Furthermore, since the breakdown point of the indirect estimator is the same as the breakdown point of the auxiliary estimator (Genton and de Luna, 2000), the asymptotic breakdown point ofθˆQLSis themin{p1,1−pm}.In the QLS estimating procedure, all information of the unknown parameters is captured through the sample quantileξˆpi. Therefore, the selection ofpiand sample quantileξˆpiwill determine the quality of QLS estimator. The following general rules are recommended for the selection:a.Both sides of median should have adequatepi’s;There should be somepi’s allocated within quarter range and in both sides of tails;Asymptotically, any consistent sample quantile estimatorξˆpiworks equivalently.The selection ofpihere is different from LV method, because the emphasis of LV is on exploratory data analysis and on tail behavior, whereas QLS method focuses on the behavior of majority of the observations. The impact of such selection will be further discussed in Section  3.In the proposed QLS estimator, quantiles serve as the auxiliary parameters (Gourieroux et al., 1993). Heggland and Frigessi (2004) showed that asymptotically, the highest efficiency is achieved when dimension of original and auxiliary parameter vectors are the same, e.g.m=4. However, the simulation results did not support this asymptotic result when sample sizes are between 100 and 10,000, which cover most practical situations. Genton and Ronchetti (2003) mention that the Akaike information criterion (AIC) or Bayesian information criterion (BIC) may be used to select the number of auxiliary parameters. In this paper, it is selected to minimize AIC given byAIC=nlog(SSEQLSn)+2(m+1)(Burnham and Anderson, 1998), whereSSEQLS=∑i=1n{ξˆpi−ξpi(θ)}2,pi=i−1/3n+1/3and4≤m≤20. Since extreme quantiles are easily impacted by outliers,m>20is not considered to improve robustness of the QLS estimator. Besides using the AIC for selection ofm, simulations results using various a priori chosenmare also provided in supplemental Table 1 (see Appendix B).Any effective non-linear optimization algorithm would work well when minimizing the non-linear function∑i=1m{ξˆpi−ξpi(θ)}2to compute the QLS estimator. In this paper, the optim function in R with option “Nelder–Mead” is used for the minimization. The Nelder–Mead (Nelder and Mead, 1965) algorithm is a “simplex” direct search method, which does not require derivatives, is easy to implement, and “has been consistently one of the most used and cited methods for unconstrained optimization” (Wright, 2012). In the Nelder–Mead minimization procedure, LV estimates ofθare used as the initial values.When data are contaminated with a fair number of outliers or observations from other distributions, these outliers will cause bias in estimating the parameters from the base distribution. Consequently, a robust estimation method is desired to fitg-and-hparameters well for the uncontaminated portion of the data. The QLS method is derived by minimizing the quantile least squares function (2) and (2a) with respect to parameter setθ. The robust version applies the QLS method to robustly estimated sample quantiles derived by trimming the outliers. This trimming procedure uses the Tukey biweight weighing function to trim the observations with weight 0.The robust QLS (rQLS) estimatorθˆrQLSis defined asθˆrQLS=argminθ{[ξˆmTR−ξm(θ)]T[ξˆmTR−ξm(θ)]}=argminθ∑i=1m{ξˆpiTR−ξpi(θ)}2,whereξˆmTR=[ξˆp1TR,…,ξˆpmTR]andξˆp1TR,…,ξˆpmTRare sample quantiles described in Section  2.1 but using the trimmed subset of order statistics. The trimming is performed using the Tukey biweight weights applied to the differences between the observed and predicted order statistics. More specifically letx(i),i=1,…,n, be theith order statistic of the sampleXi,i=1,…,n, from Tukeyg-and-hdistribution. The predictedith order statistic is defined as the quantileξpi(θ)of Tukeyg-and-hdistribution with parameter vectorθ, wherepi=i−1/3n+1/3. Then the weight corresponding tox(i)is given by(5)wi=w(x(i))={[1−(x(i)−ξpi(θ)c)2]2for|x(i)−ξpi(θ)|≤c0for|x(i)−ξpi(θ)|>c,wherecis a constant of the Tukey biweight function. The order statistics with non-zero weights defined by (5) are included in the trimmed subset of order statistics{x(ij),1≤ij≤n′}, wheren′is the number of observations with non-zero weights. This subset is then viewed as the full set of order statistics for the purpose of computing the sample quantilesξˆp1TR,…,ξˆpmTRfor0<p1<⋯<pm<1. When the constantcis appropriately chosen, the extreme data values will be down weighted to zero under the true parameterθ. Therefore the sample quantiles can be estimated robustly using trimmed order statistics with non-zero weights. For a given constantc, rQLS estimatorθˆrQLSis computed using the following iterative procedure:I.Estimate the Tukeyg-and-hparameters using a non-robust method, such as LV or QLS to obtainθˆrQLS(0).The following steps are repeated until convergence of estimates ofθ.For a current estimateθˆrQLS(i),i≥0, compute the weights for order statistics using (5) withθˆrQLS(i).Compute the sample quantilesξˆp1TR,…,ξˆpmTRusing computed weights.Obtain new estimatesθˆrQLS(i+1)using (4).Apply rQLS method using a few trial values for constantc; check the number of trimmed observations when the procedure converges. Findaandbsuch thatais the smallest constant resulting in<50%observations trimmed, andbis the smallest constant resulting in no observations trimmed. A large constant can be used here as a starting point and decreased untilaandbare found. The heavier the tail, the larger a starting point is needed. For samples with LV estimatedh≅0, 0.1, 0.4 and 1, starting values can be 5, 10, 30 and 1500 respectively. Hereaandbneed not be precise. For starting point 1500, the searching process can use 1500, 1400 and etc.Defineci=b/2−(i−1)×v, wherei=1,…,⌊b2−av⌋+1;vis the desired grading interval,10⌊log10(b/2)⌋−1is used here.Use rQLS procedure to compute the robust estimatorθˆrQLSusingc=c1.Repeat step 2 usingc=ci,i=1,…,⌊b2−av⌋+1until stopping criteria are met.Denotewithe weight of order statisticsx(i). Stop ifwi≥0.8,i=1,…,n;Otherwise: Stop if both (a) and (b) are met.(a)Denotes1=max(i,wi<0.7andi<n2)ands2=min(i,wi>0.8andi<n2); ifs1<s2andw1=min{wi,i=1,…,n2};Denotes3=max(i,wi>0.8andi>n2)ands4=min(i,wi<0.7andi>n2); ifs3<s4andwn=min{wi,i=n2,…,n}.Stop ifc<a.Simulations are used in this section to compare the accuracy ofg-and-hparameter estimation for four studied methods, LV, NMLE, QLS, and rQLS. Without loss of generality, it is assumed thatA=0andB=1and the notation(g,h)is used to specify consideredg-and-hdistributions. Simulations of six uncontaminatedg-and-hdistributions(0,0),(0,0.1),(0,0.4),(0.1,0),(0.4,0),(0.2,0.2)were carried out. The selection of these combinations covers the most commonly used distributions with moderate tails. For each parameter combination, 1000 standard normal random samples were generated with distinct seeds using “rnorm” function in R, and then transformed tog-and-hdistribution sample using formula (1) or (1a). In this simulation study, sample sizesn=100,n=1000andn=10,000were considered; Table 1shows comparison of results for uncontaminated samples fitted by LV, NMLE, QLS and rQLS forn=100andn=1000. Selected results forn=10,000fitted by rQLS are presented in Table 3as part of outlier detection study.In Table 1, for each(g,h)case, the upper entries represent the average biases between the estimated and true population parameters, while the entries below, in parenthesis, represent their standard errors. Thus, forn=100, estimatinggusing the LV method and the(0,0)case, standard normal, the averagebias=−0.0044and standarderror=0.0031. The third column for each procedure gives percent error for estimating the 95th and 99th percentiles placed above and below, respectively. The percentiles are computed as|xp−yp|xp×100%, wherexpis thepth quantile of underlying distribution, whileypis the estimated value based on the fittedg-and-hdistribution. For QLS,mis selected by minimizing AIC as described in Section  2.1;m=10is used for rQLS.From Table 1 it is clear that all procedures perform well in estimating the(g,h)parameters for uncontaminated samples, with NMLE generally having the lowest standard errors. Regarding the estimation of the 95th and 99th percentiles, QLS is competitive with NMLE, with both being superior to the LV method, which was used to obtain the starting values for the NMLE and QLS iterative procedures.When sample size increases from 100 to 1000, the patterns are quite similar, but with improved efficiency. Consider, as an example, the estimation of the(g,h)parameters for (0, 0). Then the(g,h)standard errors are (0.0029, 0.0012) for QLS andn=100, while they become (0.0009, 0.0003) forn=1000. Supplementary Table 3 (see Appendix B) indicates that the magnitude of bias and standard errors for estimation ofAandBby QLS and rQLS have similar pattern as the estimation ofgandh.To ensure one-to-one transformation,hwas chosen to be a non-negative number, but some commonly used distributions can be approximated usingg-and-hdistributions with negativeh(Martinez and Iglewicz, 1984). Whenhis negative, the distributions are not proper and any approximations to distributions can be only partial. The observations at the tail may not be fitted properly. Since QLS method does not use the observations at the very tail, like the LV method, the QLS can be modified to allow negative estimates ofhby using parameterizationθ=[θ1,θ2,θ3,θ4], so thatA=θ1,B=exp(θ2),g=θ3,h=θ4. The corresponding simulation results (supplemental Table 2, Appendix B) indicate that using this parameterization provides less biased estimates ofh=0as compared to using the QLS approach with assumptionh>0. This property makes QLS more flexible to fit some commonly used distributions such as Chi-squared distribution.To study the QLS and rQLS procedure in sample with moderate amount of outliers in high end tail, for each of the above uncontaminated cases described in Section  3.1, additional 5% random observations consisting ofN(μ,0.5), where the 0.5 represents the standard deviation, were added. The values forμare 5 for the (0, 0) case, 17.5 for the (0, 0.1) case, 742 for the (0, 0.4) case, 6.5 for the (0.1, 0) case, 16.5 for the (0.4, 0) case, and 105 for the (0.2, 0.2) case, respectively. Each of theseμrepresents the corresponding upper tail area about 2.9×10−7. In these simulations,m=10is used for both QLS and rQLS. The simulation results comparing all four methods are shown in Table 2. Some other simulations with outliers at both low end and high end were also explored; the results are similar to the simulations with high end outliers and not presented.Table 2 indicates considerably smaller average bias of rQLS estimates as compared to the other three methods. Similarly, the percent errors in estimating the 95th and 99th percentiles are considerably smaller than those based on the other three methods. This suggests that rQLS works well in estimating theg-and-hparameters when a moderate proportion of outlying observations are present. The other three methods treat the entire data set as representing the true distribution and fit the parameters accordingly.Table 2 also demonstrates the effects of the different emphasis of LV, NMLE, QLS and rQLS methods, especially when outliers exist. Since QLS and rQLS use a selected subset of data quantiles from the body of data, they are more resistant to the outliers in the tail; LV approach, using data quantiles from the quartiles out, with ever-decreasing ‘probability’ steps, fits the tail better; NMLE uses all observations to fit the distribution and works well when no outliers exist.The simulations were carried out also to explore the breakdown point of the rQLS method numerically. For a given random sample withn=10,000, different percentage of observations were moved to infinity (10,000 used here); then rQLS method was used to fit theg-and-hdistribution. The results from differentg-and-hdistributions suggest that the actual breakdown point is near 50%.Among the considered methods, the LV method requires the least computing time and the LV estimates were used as starting values for the other three procedures. When sample size is 1000, using a PC with Intel Core i3-330M dual-core processor and 4 GB RAM, the QLS approach requires on average about 0.1 s per sample, compared to about 2 min for NMLE. When sample size increases to 10,000, the running time for QLS and NMLE increases to 0.2 s and 15 min per sample, respectively. The NMLE computing time is approximately proportional to the sample size. This property makes QLS a better approach for large samples. The rQLS usually takes 5–10 times as long as the QLS method, depending on the amount and location of the contaminants.The detection of outliers has a long history and many practical applications. A detailed reference is Barnett and Lewis (1994) and a far shorter introduction given by Iglewicz and Hoaglin (1993). This work deals with univariate data. In general one assumes that the bulk of the sample comes from a specific distribution with the rest of the observations, if any, deviating sufficiently from these observations to be labeled as outliers. Often it is assumed that the base distribution is normal and then uses this distribution to identify outliers in the sample. Thus, Barnett and Lewis (1994) lists almost 50 procedures for identifying outliers from normal distribution, with a fair number of these procedures dealing with the identification of one or two outliers. Of interest here are procedures that have the ability to identify multiple outliers. The approach is to first use the proposed robust version, rQLS, to estimate theg-and-hparameters and then use this estimated distribution as the base distribution for identifying potential outliers. For this purpose the simple robust Banerjee and Iglewicz (2007) procedure will be used, although other methods can also be considered.For a given relatively large sampleX1,…,Xn, from a univariate distributionF(⋅), the boxplot outlier detection procedure (Hoaglin et al., 1986; Hoaglin and Iglewicz, 1987) defines the upper outlier cutoff for right skewed distribution as(6)Q3+k(Q3−M),and lower and upper outlier cutoff for symmetric distributions as(7)Q1−k(Q3−Q1)andQ3+k(Q3−Q1),respectively; whereMis the sample median andQ1andQ3are the first and third sample quartiles respectively.When sample size is sufficiently large, Banerjee and Iglewicz (2007) provided the solution fork,(8)k=F−1((1−α)1/n)−F−1(0.75)F−1(0.75)−F−1(0.5),(9)k=F−1((1−α/2)1/n)−F−1(0.75)F−1(0.75)−F−1(0.25),for right skewed and symmetric distribution respectively; whereF−1(p)is thepth quantile of a given distribution (Banerjee and Iglewicz, 2007). This boxplot based procedure will be here denoted by BP. For BP,α=0.05will be used. Once theg-and-hparametersA,B,gandhare estimated for a given sample, the constantkin (6) and (7) is easily calculated using (8), (9) and (1) to obtain the outlier cutoff.Table 3 reports simulation results for the same cases as in Tables 1 and 2, but withn=10,000, since the real data examples involve relatively large samples. Thus, the simulated data include 10,000 random observations for uncontaminated cases and 10,000 random uncontaminated observations plus 500 “upper outliers” for contaminated cases. A comparison is made between applications of boxplot detection procedure utilizing three possibilities of computing quantiles in (8): (a) Using the fittedg-and-hdistribution with rQLS parameter estimates; (b) using the normal distribution with parameter estimates from data normally distributed in (8); (c) Using the trueg-and-hdistribution. Table 3 shows the average number of regular observations labeled as outliers, the average number of contaminated observations labeled as outliers for contaminated cases, and the estimated probability that at least one regular observation in a sample is labeled as an outlier (some-outside rate per sample). In Table 3 the base distribution results can be viewed as ideal for purposes of making comparisons.A simulation study, summarized in Table 3 indicates that the number of real outliers (contaminated observations) identified by outlier labeling procedure based on rQLS estimatedg-and-hdistribution is comparable to that based on known distribution for all simulation scenarios. Meanwhile, theg-and-hmethod successfully controls the some-outside rate per sample to around 5% in studied scenarios, which is comparable to that based on using the known distribution. When the true distributions is not normal, the procedure based on normal distribution does well at detecting the real outliers, but at the cost of detecting a fair number of regular observations as outliers. This indicates the drawback of assuming that data comes from normal distribution with possibly some outliers that need to be detected, when the observations actually come from a distribution that differs considerably from the normal. Overall, Table 3 lends support for using the proposed outlier detection rule consisting of first fitting ag-and-hdistribution using rQLS to estimate parameters and then using formula (6) to identify outliers. This proposed approach works well for relatively large samples.The above outlier detection approach is based on control of the rather conservative familywise error rate, Hochberg and Tamhane (1987), where family is defined as “any collection of inferences for which it is meaningful to take into account some combined measure of error”. More recently, methods based on control of the false discovery rate (FDR), introduced by Benjamini and Hochberg (1995), have become popular, especially in identifying expressed genes in microarray experiments. FDR is designed to control the expected proportion of incorrectly rejected null hypotheses (type I errors). Among the large number of papers on this topic are Reiner et al. (2003), Qian and Huang (2005) and Kauffmann and Huber (2010). This procedure can be modified to use for outlier detection. The following steps are used in this paper: (1) For every observation, thez-score based on rQLS estimatedg-and-hparameters is calculated by solving forzpin formula (1); (2) Convert thez-score to originalp-value and adjust thesep-values using “p.adjust” function in R with “BH” option; (3) Declare the observations with adjustedp-value<0.01or 0.05 as outliers. For a brief study of the false discovery approach, the (0, 0.1) case from Table 3 was repeated by first estimating the(g,h)parameters using rQLS and then using the Benjamini and Hochberg (1995) procedure (Call this procedure BH). Then for the 500 contaminated observations, BH identifies all of them but at the cost of identifying, on average, 24.3 regular observations as outliers, versus 0.03 by BP. Realizing this,p-values<0.01, instead of 0.05 are frequently used with false discover rate procedure, see, e.g. Kauffmann and Huber (2010). Denote the 1% version as BH1 and the 5% version as BH5, respectively. If BH5 is now replaced by BH1, then again on average, all 500 contaminated observations were labeled as outliers, at a cost of identifying, on average, 4.7 regular observations as outliers. Both BH and BP outlier identification methods will be illustrated with three published data sets in Section  5. In summary, rQLS can be used to estimate ag-and-hdistribution and then effectively use this estimated distribution as the basis for identifying outliers either using the familywise or false discovery rate approaches.In this section three data sets are used to illustrate application of the proposed QLS and rQLS estimation procedures, two involving gene expression microarray data and the third stock market index data. All three data sets involve between 6000 and 18,000 observations. So, in that sense they can be viewed as large data sets. These data sets will be fitted by LV, NMLE, QLS and rQLS, using the bootstrap to estimate each estimated parameter’s bias, standard error, and confidence interval, plusp-value of Kolmogorov–Smirnov (KS) test for goodness of fit. These results are summarized in Table 4. Outlier detection using both the BP and BH procedures will be investigated.The first data set, called ApoAI here, is from Callow et al. (2000) who investigated the gene expression profile in HDL deficient mice with microarray technique. This study compared the effects of ApoAI deficiency on gene expression in the liver in order to investigate the molecular pathway of ApoAI in regulating HDL metabolism. The raw data contained the measurements of Cy5 and Cy3 signal from 6384 genes on 16 microarrays. The data set used here consists of the estimated mean differences in log ratio of Cy5 to Cy3 for each gene after normalization using LIMMA software package (Smyth, 2005).The second data set, called Swirl here, used zebrafish as a model to identify genes with differential expression in the Swirl mutant zebrafish compared to wild-type zebrafish. The data illustrated here comes from package LIMMA (Smyth, 2005). Briefly, their final data, consisting of 8448 observations, was transformed from raw data by normalization, linear model and empirical Bayes analysis. This univariate data was theoreticallytdistributed with 7 degrees freedom. An independent analysis indicates that atdistribution with 7 degrees of freedom does not fit so well, so(g,h)estimated distribution will be relied on.The third data set comes from Strandberg and Iglewicz (2013) and consists of 17,310 modified daily percent changes, in full five day weeks, of Dow Jones Industrial Average (DJIA) values from October 1, 1928 to December 18, 2009. Supplemental web figures are provided to illustrate the histogram of these three data sets (see Appendix B).The QLS, rQLS, LV and NMLE methods are used to estimate theg-and-hparameters for all three data sets. Bootstrapping was applied to re-sample from original data 1000 times with replacement to obtain the estimated bias, standard error and 95% confidence interval for each estimated parameter. Additionally, the Kolmogorov–Smirnov Goodness-of-fit test (KS test), here used as an exploratory tool, was performed for each estimated distribution. Table 4 summarizes the results. Additionally, Fig. 1providesQ–Qplots of NMLE, QLS quantiles versus sample quantiles, and rQLS estimated quantiles versus trimmed sample quantiles for DJIA and Swirl data.Q–Qplot for all examples with LV, NMLE, QLS and rQLS fitted quantiles are provided as web supplements (see Appendix B). For ApoAI data, all considered estimators resulted in very similarQ–Qplots (Supplemental Fig. 2, Appendix B). It was observed from Table 4 that all fourg-and-hestimated distributions fit nicely the ApoAI sample data, as seen from KS tests, but with slightly elevated NMLE estimate ofh. This occurs because NMLE method uses all observations to estimateg–hparameters; whereas LV, QLS and rQLS only use selected quantiles. Consequently, NMLE is not resistant to outliers. Furthermore, use of QLS and rQLS provide reasonable distributional fits for both the Swirl and DJIA cases, while neither LV nor NMLE were able to do so for the DJIA case and LV failed to provide a reasonable fit for the Swirl case. As seen from Table 4, these three data sets seem to consist of mixtures of mostly regular observations that are contaminated with outliers. This is especially true for the Swirl and DJIA cases, where both QLS and rQLS provided reasonably fitted distributions for the data sets. Note that the rQLS method fits majority of observations (about 95%) in the middle better than in the tails. Fig. 1 demonstrates that rQLS fits the trimmed sample well. Note that only 0.46% of the Swirl data and 0.83% of the DJIA data were actually trimmed for the purpose of computing the final rQLS estimates.Q–Qplots of QLS for both Swirl and DJIA data look worse than NMLE, because theQ–Qplot cannot reflect the robustness of QLS, as illustrated in supplemental Fig. 5 (see Appendix B).Both the BP and BH outlier identification procedures are used to identify lower and upper ends outliers for the three data sets. For BH approach both the 5% and 1% significance levels are used, while only 5% is used for the BP approach. Eight outliers are identified by both the BP and BH approaches for the ApoAI data with the observations, following removal of these eight outliers, fitting nicely a normal distribution. The number of outliers found for the other two data sets differ depending on method. For the Swirl data set, 14 outliers are found using BP, 22 using BH1 and 43 based on BH5; here the BP outliers are included in BH1 outliers. Similarly, the numbers of outliers found for the DJIA case are 21, 27, 52 for BP, BH1 and BH5, respectively. This time the BH1 and BP outliers are both subsets of BH5 outliers; BP identifies 4 lower end outliers (<−35.5%) corresponding to daily percent change during the Great Depression in 1929; BH1 only identified positive changes, despite BH5 identified both ends outliers. As illustrated for the Swirl and DJIA data sets, the number of outliers identified depends on whether BP or BH are used and whether 1% or 5% are used for BH.A quantile least square method (QLS) and a robust version, rQLS, are proposed to estimate theg-and-hparameters. Meanwhile, the same estimation approaches may be used to estimate parameters of any distribution with known quantile functions. Similar estimators may be also developed for any parametric distribution (with unknown quantile functions) using the framework of simulation-based indirect inference (Gourieroux and Monfort, 1996). The multivariate extension ofg-and-hdistribution was recently developed by Field and Genton (2006). They used the estimation procedure similar to QLS (based on empirical multivariate quantiles) for estimating multivariate versions ofgandhparameters after estimating location and scale parameters using the minimum covariance determinant (MCD) estimator (Rousseeuw, 1985), but did not address the asymptotic properties of the resulting estimators.The QLS is an efficient and rapidly convergent estimator of the Tukeyg-and-hdistributional parameters and rQLS provides a robust version that works well when contaminated observations are present. These two approaches are compared with two alternative procedures of LV and NMLE. The simulation results indicate that these two methods are competitive for data coming from a random sample, with both QLS and rQLS providing similar estimates of the parameters.The QLS and rQLS approaches also enable the robustly estimatedg-and-hdistribution as a basis for identifying outliers in the data. When data includes some contaminated observations, QLS does well at fitting the distribution of the entire sample, while rQLS does well at fitting the observations coming from the uncontaminated bulk of the data set. The rQLS method is further used to provide a more general version of an outlier identification procedure, by first fitting theg-and-hparameters and then using this fitted distribution as a means of identifying outliers. This more general outlier identification approach is investigated, through simulation, both for a familywise error rate method, BP, and the BH false discovery rate approach. The QLS and rQLS methods are further illustrated with three data sets, two involving microarray data where expressed genes are of interest, and one dealing with stock market index data. These provide additional evidence that the QLS and rQLS estimation methods can be effectively used to estimate theg-and-hparameters and rQLS can also be used as the basis for obtaining an estimated base distribution that can be successfully utilized to identify potential outliers.The proposed robust version of QLS may be viewed as a robust indirect estimator (Genton and de Luna, 2000; Genton and Ronchetti, 2003). However, the asymptotic properties of rQLS estimator do not follow immediately from the general theory of indirect inference because the consistency and asymptotic normality of the sample quantiles based on trimmed order statistics has not been established. Deriving such asymptotic distribution is far beyond the scope of this work and will be a subject of a separate investigation. As an alternative to using trimmed order statistics, it is possible to adaptM-quantiles (Breckling and Chambers, 1988) as a robust auxiliary parameter. However, theM-quantiles are monotoneM-estimators and may be expected to be less robust as compared to using redescending Tukey biweight. Also, the joint asymptotic distributions of the vector ofM-quantiles, which would enable derivation of asymptotic distribution of corresponding indirect estimator, has not been established either. For another potentially highly robust alternative to QLS, one may consider using the least median of squares (Rousseeuw, 1984) instead of the least sum of squares in (3) as an objective function to be minimized. This direction was not pursued here because computing such least median of squares estimator for large data sets is expected to be really challenging (e.g. Steele and Steiger, 1986; Stromberg, 1993).

@&#CONCLUSIONS@&#
