@&#MAIN-TITLE@&#
Variable assessment in latent class models

@&#HIGHLIGHTS@&#
Two measures are proposed for assessing continuous and discrete variables in LCA.Both measures are either in closed form or straightforward to compute.Both measures perform reasonably well compared to existing measures such LRT andFst.Both absolute and relative interpretations of one measure are possible.

@&#KEYPHRASES@&#
Latent class analysis,Variable selection,Mixed data type,Total variation,Posterior gradient,Cross entropy,Kolmogorov distance,

@&#ABSTRACT@&#
The latent class model provides an important platform for jointly modeling mixed-mode data—i.e., discrete and continuous data with various parametric distributions. Multiple mixed-mode variables are used to cluster subjects into latent classes. While the mixed-mode latent class analysis is a powerful tool for statisticians, few studies are focused on assessing the contribution of mixed-mode variables in discriminating latent classes. Novel measures are derived for assessing both absolute and relative impacts of mixed-mode variables in latent class analysis. Specifically, the expected posterior gradient and the Kolmogorov variation of the posterior distribution, as well as related properties are studied. Numerical results are presented to illustrate the measures.

@&#INTRODUCTION@&#
Heterogeneous data types have become commonplace in many sciences. In the medical sciences, clinical studies often collect data that are continuous (e.g., blood pressure), binary (whether or not the subject has diabetes), ordinal (severity level of a disease), categorical (medication used), and other types such as count and time-to-event. The identification of clinically meaningful phenotypes in the population using a heterogeneous data type is thus an important area of research.Everitt (1988, 1993) referred to heterogeneous data types as mixed-mode data in the context of latent class and mixture analysis, in which multiple data types are used as indicators for putting similar objects into groups (see also  Lawrence and Krzanowski, 1996; Vermunt and Magidson, 2002). The terms latent class model and mixture are used interchangeably. The idea here is to cluster a vector of mixed-mode responsesY=(Yi)for indicatorsi=1,…,mintoSdistinct latent classesZ=1,…,S. There are at least two general approaches for mixed-mode latent class analysis (MM-LCA). The first approach is to relate the manifested categorical response to an underlying multivariate Gaussian distribution such that continuous normal variables and categorical variables can be jointly modeled (Joreskog, 1973; Shi and Lee, 2000). As pointed out by Dunson (2003), the underlying Gaussian approach has limitations, one of which is that it cannot easily accommodate general data types such as counts. An alternative is to use the generalized linear mixed-model approach proposed by Sammel et al. (1997) and later extended by Moustaki and Knott (2000), Dunson (2003), Daniels and Normand (2006), Yang and Dunson (2010), and Cai et al. (2011). This approach can accommodate any mixture of outcomes from an exponential family. Under the assumption of conditional independence given latent classZ, the likelihood for an individual subject in an MM-LCA can be expressed as:(1)f(Y|θ)=∑z=1Sαz∏i=1mpiz(yi|θz),whereθcontains the vector of parametersθzfor each individual classz, which has a prior probabilityαz=p(Z=z). Within an exponential family framework, different link functions can be specified for the conditional distributionpizfor different data types.One question that arises from the generalized mixed-model approach for latent class analysis and latent variable in general is how the different types of data “impact” the likelihood. It is possible that one data type “overwhelms” another data type in the likelihood and becomes dominant in defining the structure of the latent class model. Because data values are not measured on the same scale, it is not easy to promptly assess the impact of a variable on the overall likelihood. This question is directly related to a second question: if only a limited number of mixed-mode indicators can be included in a latent class analysis, which variables should be selected for maximally “discriminating” between the classes? Interestingly, the latter question can also be reformulated as a variable-selection problem and solved by a search algorithm using criteria such as the BIC (Raftery and Dean, 2006; Dean and Raftery, 2010).Two measures are proposed for assessing a variable’s contribution to the classification of latent classes. In LCA class labels are not known a priori; the term classification here refers to the extent to which a variable contributes to discriminating the classes, or in the case the class label is known (e.g., in a simulation setting) the accuracy in retrieving class membership. The first measure, the expected posterior gradient (EPG), measures the absolute contribution of a variable to MM-LCA. The second measure, based on the Kolmogorov variation of the posterior distribution (KVP), can be interpreted in terms of the relative contribution of a variable by comparing classification accuracies with and without the variable in the MM-LCA. Interestingly, both measures can be related to the statistical distance between the prior distributionp(z)and the posterior distributionp(z|y). There are several advantages in using the EPG and KVP. First, they both have strong theoretical foundations, which will be described in the following two sections under the heading “Justification of measures”. Second, the measures can be universally applied to all kinds of mixed-mode data—continuous, discrete, and count data. Furthermore, computationally the two measures are straightforward to compute and closed form solutions are available for EPG. For the remainder of the paper, Section  2 describes the EPG measure, and the procedure of how the measure can be derived and used in practice. Section  3 describes KVP and specifically its relation to the total variation measure, which is commonly used in the image processing literature. In Section  4, two numerical examples of MM-LCA are provided to illustrate the proposed methods. A brief discussion is given in Section  5.Consider anS-class latent class model that includes both continuous and discrete random variables,Y=(Y1,…,Ym), with class-conditional distributions of normal, exponential, Gamma, Poisson, ordinal, or binomial distributions, given the latent random variableZ∈S,S={1,…,S}. Class-conditional independence is assumed among all the variables—i.e.,(2)p(Y1=y1,…,Ym=ym|Z=z)=∏i=1mp(Yi=yi|Z=z).Denote the posterior probabilityp(Z=z|Y1=y1,…,Ym=ym)byτz, the class-conditional probabilityp(Yi=yi|Z=z)byπyi|z, and letπy|z=∏i=1mπyi|z. These quantities are related by the Bayes formula:(3)τz=αzπy|zp(y),wherep(y)is the marginal probability of observing the outcome vector,y=(y1,…,ym), and(4)p(y)=∑z=1Sαzπy|z.The EPG measure for assessing the impact of variableyion the MM-LCA is denoted byBiand its definition is given by:(5)Bi=∑z∈Sαz|Ey(∂log(τz)∂yi)|.The idea behind EPG is that when a change in a specific variable has a strong impact on the posterior distribution, it would imply that the variable contains substantial information about the classification of latent class—i.e., the membership of subject given the response pattern. In other words, if class membership of a subject is sensitive to a change in the value of the variable, then the variable has a strong impact. Because the gradient oflog(τs)byyiis a function ofy, the expectation with respect toyin (5) is taken.A few examples of the termE(∂log(τz)/∂i), are presented in closed form for parametric distributions. Taking the logarithm of Eq. (3) and forming the expected partial derivative leads to:(6)E(∂log(τz)∂yi)=E(∂log(πyi|z)∂yi)−E(∂log(p(y))∂yi).And from (2) and (4),(7)∂p(y)∂yi=∑z∈Sαzπy|z∂log(πyi|z)∂yi.It is easy to derive the following result for the expected log posterior gradients using the identities (6) and (7). Details of the derivation are given in the supplementary file (see Appendix B).1.For a continuous variableYi, without loss of generality, centered at 0, and class-conditional normal distributions,N(μz,σz),(8)Ey(∂log(τz)∂yi)=μzσz2.For a continuous variableYiwith unconditional meanλ̄and class-conditional exponential distributions,Exp(λz),(9)Ey(∂log(τz)∂yi)=λ̄−λz.For a continuous variableYiand class-conditional gamma distributions,G(kz,θz),(10)Ey(∂log(τz)∂yi)=(kz−1)∑j=1Sαj(kj−1)θj−1θz.For a count variableYiwith unconditional meanλ̄and class-conditional Poisson distributions,Pois(λz),(11)Ey(△τzτz(yi+1))=1−λ̄λz,where△τz=τz(yi+1)−τz(yi), andτz(yi)=p(Z=z|Yi=yi,i=1,…,p).For a discrete variableYiwith class-conditional binomial distributions,B(n,πz),(12)Ey(△τzτz(yi+1))=1−ŌOz,whereŌ=∑z=1SαzOzandOz=πz/(1−πz).In the univariate mixture model ofSnormal distributions with equivariance, ifμ1=⋯=μS, thenμ1=⋯=μS=0, the degenerating case mentioned in Bauer and Curran (2003). In such a case, the continuous variable has zero expected contribution to the posterior measure. A similar degenerating case is seen in the Poisson distribution whenλ1=⋯=λS=λ̄, and in the Gamma distribution whenk1=⋯=kSandθ1=⋯=θS.From (12), it can be seen that if a binary variable has uniform conditional probabilities across all classes—i.e.,∀z,π1|z=c, then the EPG is zero. On the other hand, if at least one ofπ1|zis close to 0 or 1, the EPG, or the contribution to the reduction in entropy, would be large.Intriguingly, the EPG is related to the gradient of change of entropy from knowing the prior distribution of the latent class to knowing the distribution of the latent classes after observing the data. For example, if the prior distribution is uniform and the posterior distribution is highly skewed with probability mass concentrated on one particular class, then the change of entropy is large. More formally, denote the cross entropy between two distributionsq1andq2by(13)CH(q1,q2)=−∫q1(x)logq2(x)dx.Cross entropy is a statistical distance measure that is directly related to the Kullback–Leibler distanceDKLthrough the following equation:(14)DKL(q1∥q2)=H(q1)+CH(q1,q2),whereH(x)is the Shannon entropy. Specifically for the prior distribution ofZ,H(Z)=−∑z=1Sαzlogαz.The cross entropy for the distribution ofZand the distribution ofZgivenYis given by(15)CH(Z,Z|Y)=−∑z=1Sαzlogτz.The quantity∂CH(Z,Z|Y)/∂yisignifies the relative change of cross entropy between the prior distributionp(Z)and the posteriorp(z|y)after observingYwith respect to a change in the variableyi. Thus, the cross entropy gradient quantifies the impact of the variableyion the MM-LCA in terms of change in the classification distribution of the latent class variableZ.Using the Jensen inequality, it is easy to show that the EPG and cross entropy are related by the following inequality:(16)Ey(|∂CH(Z,Z|Y)∂yi|)≤Bi=∑z∈Sαz|Ey(∂log(τz)∂yi)|.Thus, the EPG is the upper bound of cross entropy gradient. This bound is important for variable assessment because if a variableyihas small EPG valueBi, then it is not possible for large cross entropy values to exist, and therefore the variable is deemed to have insignificant impact in terms of its discriminatory power for distinguishing between latent classes. In general, the further the class-conditional distribution ofyiis away from a uniform distribution, the more the contribution it provides toBi. Taking absolute value of gradients in (16) is necessary because given a univariate standard-Gaussian variable andσz=1forz∈S, without the absolute sign,Bican be zero,(17)Bi=∑z∈Sαzμzσz=0,even when class-conditional means,{μz,…,μS}, are very different.To illustrate the EPG, a small data set ofn=1000is simulated, using a 2-class mixture (S=2,Z=1,2, andp(Z=1)=p(Z=2)=0.5) in whichY1is a mixture ofN(−1,1)andN(1,1)respectively forZ=1andZ=2. Two additional variablesY2andY3were independently randomly sampled fromN(0,1)and added to the mix. Then an LCA software, Latent Gold, is used to estimate the 2-class model. ForY1, the class-conditional statistics wereμˆz=1=−0.99,σˆz=12=1.33, andμˆz=2=0.99,σˆz=22=0.74. Therefore, the estimatedB1=0.5×(|−0.99/1.33|+|0.99/0.74|)=1.04, as opposed to the population value of 1.0. ForY2andY3, the sample estimates of EPG were respectivelyB2=0.015andB3=0.002, showing that these two variables had little or no impact. In general, the EPG can be estimated by first fitting an MM-LCA to data, and then using the easy-to-compute analytical formulas to calculateBifor variable ranking and selection. In the above example,Y1would be ranked as the variable with the highest impact on the posterior distribution and retained in the model.The second measure for variable assessment, the Kolmogorov variation of posterior distribution, or KVP, is based on the statistical distance between the class-conditional distributions. Formally, the Kolmogorov distance (Ali and Silvey, 1966) between two distributions,fandg, is defined as(18)dKD(f,g)=∫|f(x)−g(x)|dx,and the measure of KVP is defined as:(19)Ci=maxs∈P(S)∑z∈sαzdKD(πyi|z+1,πyi|z),whereP(S)represents the set of all permutations of the class index setS, andπyi|S+1=πyi|1.The KVP can be linked to a concept of “classification accuracy” in MM-LCA. The idea behind KVP is that for a candidate variable to be included in an MM-LCA it needs to have a substantial impact on the “classification accuracy” of the latent classes, and one way to assess impact is to compare the “classification accuracies” of the MM-LCA with and without the specific variable in the model. If there is a significant reduction in classification accuracy without the variable in the model, then the variable is deemed important. Otherwise, the variable is considered not important and could be a candidate for removal in an iterative model assessment process. This section shows how the concept of classification accuracy is operationalized in terms of total variation and how KVP can be linked to total variation.Consider a generative latent class model in which each person is given a class membership and mixed-model variables are generated given class membership. Under such a scenario, a method that allows the accurate recovery of class membership is highly desirable. One measure of “classification accuracy” is the total variation (TV) of the posterior distribution, which is commonly used in the signal processing and compressive sensing literature (Chan and Shen, 2005; Osher et al., 2005). Total variation in the context of MM-LCA is defined as(20)TV(τ)=maxS∈P(S)∑z∈S|τz+1−τz|,whereP(S)is defined in (19). With this definition, the measureTV(τ)is order-invariant, and thus the measure does not change even when the class labels are switched. For operational convenience, it is assumedτS+1=τ1. Fig. 1shows the TVs of three example posterior distributions,τ={τz|z=1,…,4}, respectively with TV values of 0,1, and 2 (from left to right). From the classification perspective, the third example (TV=2) would be the most desirable. Briefly, higher TV value implies better discrimination between classes. It is interesting to note that the TV of any posterior distribution is bounded between 0 and 2, as demonstrated respectively by the first and the third examples in Fig. 1. The proof of the bounds are straightforward and not included here. It is important to note that from the definition of total variation, the measure TV is not dependent on knowing the actual or true class membership. In a sense, the TV can be thought of as a measure of (non)uniformity of the classification probabilities computed over all pairs of classes for an observed data point.Total variation as defined in (20) is a function of the observed outcomey. The expectation of total variation is a more useful summary for the purpose of assessing the overall impact of a variable. The expectation of TV is given by(21)Ey(TV(τ))=∫TV(τ)p(y)dy.The TV measure inherits two desirable features directly from its very definition: that it can be applied to mixed-mode data, and that it is order invariant, which means that one does need to be concerned about label switching when compiling comparison statistics for different models. Besides these two features, there are two less trivial properties of the TV measure that make it desirable for assessing mixed-mode variables. The first property concerns the lower bound of its value and states that using any variable for classification in an MM-LCA could not deteriorate classification accuracy compared to the classification based on the prior distribution. Specifically this non-negativity property is stated as:(22)Ey(TV(τ))−TV(α)≥0.The non-negativity property guarantees that any variable added to the MM-LCA cannot decrease the expected TV as compared to the TV of the prior distribution. In other words, one would avoid a potential awkward situation in which using a variable in the model could make the classification accuracy worse than simply using the prior distribution of the classes without knowledge of any manifest variable.The second property of the expected TV is related to a relative measure of impact—a comparison of the TVs of the posterior distributions with and without the given variable in a given model. Define, for a given variableyi,τ−=p(z|y−), wherey−denotes the remainder of the set of variables after deletingyi. Then under the condition that a monotonic relationship exists between the joint probability,p(z,y−), and the class-conditional probability,πyi|z–that is,πyi|z1≥πyi|z2ifp(z1,y−)≥p(z2,y−)–the following inequality stands, i.e.,(23)TV(τ)≥TV(τ−),—that is, the total variation of the posterior distribution after adding an extra variableyiwould be no less than the total variation of the posterior distribution withoutyi. The proofs for the two properties are included in the supplementary file (see Appendix B).The proposed measure KVP takes advantage of the properties of TV, and the connection between the two can be summarized by the following theorem, which states that the absolute value of the expected total variation difference between models with and without a given variable in an MM-LCA is bounded by a weighted sum of KVPs, where the weights are the priors of the latent classes.Theorem 1The expected difference ofTV(τ)andTV(τ−)is upper-bounded by the KVP:(24)|Ey(TV(τ))−Ey−(TV(τ−))|≤Ci,whereCiis given in   (19).Although not obvious,Cihas an upper bound as 2 and a lower bound 0 as well. The upper bound is achieved when∀z,dKD(πyi|z+1,πyi|z)=2. Unlike the EPGBi, closed form solutions ofCimay not be immediately available. However, simple numerical procedures such as finite difference, can be used for computingCi. The proof of Theorem 1 is given in an Appendix.Theorem 1 states that the change of the expected total variation of the posterior distribution between models with and without a variable, is upper-bounded by the weighted sum of the Kolmogorov distances between class-conditional distributions. This suggests that variables with largeCi’s are more desirable. If the increase of classification accuracy by a variable as measured by expected TV is upper-bounded by a small value, the variable would not be very useful. The reasoning is similar as that for using the upper bound of cross entropy in EPG; the difference here is that the upper bound for TV is used, and the two quantities–TV and KVP–share the same lower and upper bounds of variation. Their common bounds can be viewed as yet another desirable property because this implies thatCicould not be too far off from the total variation measure. To put it differently, it allows a tighter interpretation ofCiin terms of total variation.The following numerical examples used both simulated and real data for illustrating the proposed two measures of EPG and KVP. One challenge of evaluating the performance of a new procedure for LCA–or for unsupervised learning procedures in general–is that if real data are used, there is no known label for the latent classes and hence it is hard to evaluate classification accuracy. In the first example using simulated data, LCA parameters derived from a real data set of archeometry data are used to simulate a set of continuous and discrete variables. Therefore, the class labels for latent classes are known and can be used in evaluating the performance of the proposed measures. In the second example, real genetic data from several ethnic groups are used. The latent classes are based on the ethnic group, therefore the class labels are known as well.The first example used archeometry data from Moustaki and Papageorgiou (2005), in which 3 latent classes were estimated from 21 continuous normal variables, and 12 binary variables selected from the original 19 binary variables. These variables were measurements taken from archeological artifacts–such as floor vases–dated back to ancient Italy. The continuous measures provided chemical information whereas the binary variables were obtained from petrological analysis. The discrete and continuous data are simulated using the reported estimated parameters in Moustaki and Papageorgiou (2005) for a sample size of 1000 and used the data for analysis. Without refitting the original latent class model, the simulated data and the known model parameters are used to compute the posterior probabilities and the classification accuracy. The two defined measures for assessing variables were included in the analysis: the log EPG (log(Bi)), and the KVP (Ci). Scatterplots of the two measures are shown in Fig. 2, in which near-monotonic relationships are clearly seen between the two measures. However, the two measures appear to assess continuous and binary variables differently. This is further evidenced in the following analysis when both measures are compared to classification accuracy.For thenth subject, the classification iszn=maxzp(z|yn1,…,ynm), and the classification accuracyr(y)is the percent of correctly classified subjects, given the observed variables. In Fig. 3, the two measures log EPG and KVP are evaluated against the actual classification accuracy using each individual variable, i.e.,zn=maxzp(z|yni), andriis the percent of correctly classified subjects, givenyi. Because class membership is known in this simulated data set, the measurerican be considered an objective reference for performance. From Fig. 3,Cihas a clear and strong linear monotonic relationship withri, while the monotonic relationships is less clear inlog(Bi), even though the continuous and binary variables each exhibits a high degree of monotonicity. Based on the result in Fig. 3, the measureCiappeared to be more consistent with the objective measureriand subsequently was used to rank all the 33 continuous and binary variables. Table 1shows the values of the conditional probabilities across the three latent classes,Bi,Ci, and the rank order of the binary variables based onCi, whereas Table 2shows the conditional means,Bi,Ci, and the rank order of the continuous variables. The ordering of the variables in both tables are based onBi. The tables show that the top 5 ranked variables usingCiare continuous, and appear to dominate the binary variables for classification. The measure EPG, on the other hand, tends to report high values for binary variable and ranks four of the binary variables higher than all the continuous variables. One reason for this to arise is that the conditional probabilities for these binary variables are close to the boundary which lead to large odds ratios. As a result, the ratio for computing EPG in Eq. (12) gives exceedingly high EPG values.Finally, in this example, the top 10 variables that have the largest values ofCiare selected. In this case, the selected continuous variables were 12, 6, 9, 2, 9, 11, 4, and 8, and the selected binary variables were 13, and 14. Using only these ten variables, which are less than one third of the original number of variables, the resulting MM-LCA could achieve a classification accuracy of 99.71%.The data set for this example was extracted from Hapmap2 (Frazer et al., 2007), which is a second-generation version of the haplotype map of the human genome. A Hapmap2 data set of 210 subjects including three populations—central European, African, and Chinese/Japanese, with respective sample sizes of 60, 60 and 90 was acquired. For each subject, 1% of the original two million SNPs was randomly sampled to result in 20,293 SNPs, for each of which the major and minor alleles frequencies were counted. As the ethnicity of an individual was known, the three populations are treated as latent classes. Thus, the variable assessment question here is to select a subset of the SNPs to achieve a high level of classification accuracy.The EPG,Bi, and the KVP,Ci, are computed for each SNP and compared them with two commonly used statistics in population differentiation—the likelihood ratio (Goudet et al., 1996) and theFststatistic, which are defined as follows.The likelihood ratio for theith SNP is given by,(25)LRTi=2∑j=1S∑k=12NijkNijkEijk,whereSis the number of populations,Nijkis thekth allele count of theith SNP in thejth population, andpijkis the corresponding expected allele frequency.TheF-statisticFstis defined as,(26)Fst(i)=∑k=12∑j=1S(pijk−pik)2pik(1−pik),wherepijk=Nijk/nj,njis the number of subject in thejth population, andpik=∑j=1SNijk/∑j=1Snj, is the probability of observing thekth allele in theith SNP in all populations.Of note, theF-statistic is similar to Efron’s pseudoR-squared measure (Efron, 1978), and the LRT is similar to McFadden’s pseudoR-squared measure (McFadden, 1973).The relationships between the three statistics,Ci,F, and LRT, are shown in Fig. 4, wherein scatterplots between pairs of the three statistics are plotted, in which bothxandy-axis are in log scales. Overall, all three statistics are monotonically related to each other, andFstis more linearly related to LRT, while LRT andFstappear to have greater variations thanCi. The scatterplot betweenBiandCi(not shown) is also monotonic and approximately linear.The performances of four measures, i.e.,Fst, LRT,BiandCi, are compared in terms of classification accuracy. All 20,293 SNPs are first separately ranked according to each of the four measures. Starting with the top ranked SNP, the classification accuracy is calculated only using this SNP, and then the next highest ranked SNP is incrementally added. Classification accuracies were calculated for each measure as a function of the number of SNPs in the LCA. In a way similar to the first example, the classification accuracy was calculated for each set of chosen SNP- i.e., the posterior distribution of population classes for each subject is computed, and the total number of correctly classified subjects is counted. Fig. 5shows that the accuracy curves for all four measures limited to the first 50 SNPs. The curve forCirises fastest among the four, suggesting that if only a small number of SNPs was used for classification, thenCiwould be the most effective measure. Paradoxically, as more SNPs are included, the classification accuracies of all measures, to various extents, tend to fluctuate. When the number of SNPs is more than 20,Fstperforms the best, although its performance also seems to decrease after including 45 SNPs. The performance ofCiand LRT tend to converge, and forBiits performance is comparable to the other measures. One remark is that LRT is applicable to mixed-mode data whileFstonly applies to discrete data, so the above results are not necessarily generalizable to data with different modes.To compare how similar, or different, the several measures selected the respective most important SNPs, we calculated the number of overlapping top ten SNPs selected by each measure. We found that the top-10 SNPs selected by the measures substantially overlapped. For example, 9 out of 10 overlapped betweenFstand LRT, 7 out of 10 betweenCiand LRT, 6 out of 10 betweenCiandFst, 3 out of 10 betweenBiandFst, and 3 out of 10 betweenBiand LRT. Overall, compared toBi,Cihad a stronger overlap withFstand LRT.In summary, this example illustrates the practical relevance of the proposed measures for efficiently identifying among a large number of SNPs, a small number of SNPs that could be used for clustering and classification purpose.A summary of how EGP and KVP can be computed and used in practice is listed below.1.Collect all the available variables, which could be of mixed mode, and fit a latent class model.Use the class-conditional parameters estimated in the previous step to compute the proposed EPG and KVP measures.Select the most discriminating variables and eliminate the weak variables based on the measures.Re-run LCA using the selected variables. Using the classification accuracy of the full model as a reference, compute the classification rate of the reduced model, which assumes the same latent structure in the number of classes.Assess the adequacy of the reduced model. Repeat steps 1–4 if necessary until a satisfactory reduced set of variables is selected.

@&#CONCLUSIONS@&#
