@&#MAIN-TITLE@&#
Evolutionary wrapper approaches for training set selection as preprocessing mechanism for support vector machines: Experimental evaluation and support vector analysis

@&#HIGHLIGHTS@&#
Support vector machines (SVMs) are popular and accurate classifiers.We study if SVMs can be further improved using training set selection (TSS).We adjust wrapper TSS techniques for SVMs.Experimental evaluation shows that filter TSS techniques cannot improve the accuracy of SVMs.Experimental evaluation shows that evolutionary based wrapper TSS techniques significantly improve SVMs.

@&#KEYPHRASES@&#
Support vector machines,Training set selection,Data reduction,

@&#ABSTRACT@&#
One of the most powerful, popular and accurate classification techniques is support vector machines (SVMs). In this work, we want to evaluate whether the accuracy of SVMs can be further improved using training set selection (TSS), where only a subset of training instances is used to build the SVM model. By contrast to existing approaches, we focus on wrapper TSS techniques, where candidate subsets of training instances are evaluated using the SVM training accuracy. We consider five wrapper TSS strategies and show that those based on evolutionary approaches can significantly improve the accuracy of SVMs.

@&#INTRODUCTION@&#
In many real-world applications, datasets can contain noisy or wrong information. Even the best classifiers might not be able to deal with these datasets. Training Set Selection (TSS, [1–3]) is a good way to alleviate this problem. It is a preprocessing technique that only selects relevant instances before applying the classifier. The objective of TSS is twofold: on the one hand, the accuracy of the classifier can be improved, while on the other hand, the efficiency can be enhanced.TSS has mainly been investigated for the K Nearest Neighbor (KNN) classifier [4], in that context it is referred to as Prototype Selection (PS, [5]). There are two main groups of PS techniques. Wrapper techniques use the KNN classifier to evaluate entire candidate subsets of instances, while filter techniques do not make use of the KNN classifier or only use it to carry out partial evaluations.In this work we want to study if TSS techniques can also improve the accuracy of Support Vector Machines (SVMs). As wrapper PS techniques explicitly use the KNN classifier, they cannot be applied meaningfully to improve SVM classification. It is clear, however, that filter PS techniques can be directly applied to SVMs as they are less dependent on the KNN classifier. On the other hand, filter methods are in general less suited to improve the accuracy of a classifier.To the best of our knowledge, only two filter TSS techniques have been proposed to specifically improve SVMs. In [6], the Multi-Class Instance Selection (MCIS) method is proposed, which selects instances near the boundary between one and the other classes of datasets. This method focuses on reduction of the dataset to improve the efficiency of the SVMs. Another approach is presented in [7], where only training instances that are likely to become support vectors are selected. This Sparsifying Neural Gas (SNG) algorithm was developed to improve the efficiency of the SVM while maintaining or slightly improving the accuracy.Unfortunately, these filter TSS techniques are unable to improve the accuracy of the SVMs. Therefore we attempt to improve the accuracy of SVMs using wrapper approaches. We adapt the five most important wrapper TSS techniques by plugging the SVM into the TSS methods. The wrapper techniques we consider evaluate candidate subsets based on the so-called training accuracy, which is the accuracy obtained when building the classifier at hand based on the candidate subset and using this model to classify the entire training data. In our case we use SVMs to calculate the training accuracy, and as a result subsets with a high training accuracy will be well-suited for SVM classification.The remainder of this paper is organized as follows: In Section 2 we provide the necessary background on SVMs. In Section 3 we present existing filter TSS techniques and in Section 4 we present the design of the wrapper TSS techniques for SVMs. Then, we set up an experimental framework to evaluate the approaches’ performance in Section 5. By means of an experimental evaluation on 43 real-life datasets, we show that wrapper TSS techniques can indeed significantly improve SVM classification. Evolutionary approaches, and the Generational Genetic Algorithm (GGA,[8,9]) in particular, seem to be especially well-suited for our purpose. In order to get more insight into the operation of the evolutionary wrappers, we provide a more detailed analysis for the latter, investigating the effect of TSS on the SVM's support vectors, and illustrating their behavior graphically on a two-dimensional artificial dataset. Finally, we conclude in Section 6.In this subsection we provide a general background on SVMs to make the paper self-contained. We denote instances by their feature vectors x. For now, we consider two-class problems, the class of an instance is either −1 or 1. At the end of this section we discuss the multi-class case.The most basic form of SVMs are separating hyperplanes, where one aims to separate the two classes linearly by a hyperplane. The hyperplane can be represented by a linear function f(x) that is optimized such that the distance from the hyperplane to the closest instances from both classes is maximal. To classify a new instance t, the value f(t) is calculated. When f(t)>0, t is classified to class 1 and else to the negative class −1.In practice, the data is often not linearly separable, which led to the introduction of support vector classifiers, which allow for overlap between the classes. The idea is to find a hyperplane that maximizes the margin between the two classes, but allows for some data points to fall on the wrong side of the margin. Of course the number of misclassified training points is bounded. It can be shown that the resulting hyperplane is a linear combination of a set of instances, these lie on the classification margin and are called support vectors.To allow for even more flexibility, kernel-based SVMs were introduced. Before constructing the separating hyperplane, the feature space is enlarged using a function h such that for two feature vectors x1 and x2(1)h(x1)th(x2)=K(x1,x2)where K is a kernel function. A well-known example is the Radial Basis Function (RBF) kernel (x1, x2 feature vectors):(2)K(x1,x2)=exp(−||x1−x2||22σ2).The separating hyperplanes are represented by a function f that takes values in (−∞, ∞). However, it is more useful to obtain probabilities. Therefore, a sigmoid model can be used to calculate the probabilities P(y=1|f). This scaling, referred to as Platt's scaling [10], can also be seen as training the model to find a better threshold: instead of using the standard 0 as threshold to classify test instances, we can train the model based on the class probabilities to find a better threshold.The discussed methods apply to two-class problems. A traditional approach to handle multi-class problems is pairwise coupling [11–13], where the multi-class problem is decomposed in all possible two-class problems and the majority voting principle is applied. For instance, when there are K classes, for each pair of classes i and j with i, j≤K and i≠j, the binary SVM is constructed. A new instance is classified by all classifiers, and each class gets a vote if the new instance is classified to that class. The class with the highest number of votes is the final class returned for that instance. Another approach is the so-called one-versus-all technique. In this case, K training datasets are considered, where in each dataset one class is the positive class and the remaining classes form the negative class. The SVM is trained on each of these training datasets and the target instance t is classified by each SVM. Each SVM returns a probability value p expressing the confidence that t should be classified to the positive class. Finally, t is classified to the class for which this probability is maximal.In [5], a comprehensive overview of TSS techniques is provided. Apart from the already mentioned distinction between wrapper and filter approaches, TSS techniques can also be categorized as edition, condensation or hybrid methods: while edition (or editing) methods remove noisy instances in order to increase classifier accuracy, condensation methods compute a training set consistent subset, removing superfluous instances that will not affect the classification accuracy of the training set. Finally, methods that eliminate both noisy and superfluous are called hybrid ones.As we are mainly interested in improving the accuracy of SVM classification, we only consider the nine editing filter techniques discussed in [5]. They are reviewed in Section 3.1.In addition to the nine TSS techniques from [5], which were originally developed to improve KNN, we also consider two filter TSS techniques that were specifically developed for SVMs. These are discussed in Section 3.2.A basic method is the Edited Nearest Neighbor (ENN, [14]) algorithm, which considers every instance in the training set and removes it whenever the nearest neighbor rule classifies it incorrectly using the remaining instances as training data. Many methods are derived from ENN, including:•ENN with Estimation of Probabilities of Threshold (ENNTh, [15]): proceeds like ENN, except that the removal criterion is based on probabilities.All-KNN ([16]): applies ENN for different numbers of neighbors and removes an instance whenever any of the ENN runs marked an instance for removal.Modified ENN (MENN, [17]): takes into account the fact that multiple instances can be at the same distance from the target instance.Nearest Centroid Neighborhood Edition (NCNEdit, [18]): is very similar to ENN, but uses an alternative definition to determine the neighbors of an element based on centroids.Multi-Edit [19]: randomly divides the training data in blocks, applies ENN to them and merges the resulting sets.We also consider three methods that are not derived from ENN:•Relative Neighborhood Graph (RNG, [20]): constructs a proximity graph and removes instances that are misclassified by the neighbors in the graph.Model Class Selection (MOCS, [21]): uses a feedback system to incorporate knowledge about the dataset in a tree-based classifier.Edited Normalized Radial Basis Function (ENRBF, [22]): calculates for each instance the probability that it belongs to each class. Instances for which the actual class does not correspond to the class with the highest probability are removed.The first SVM-specific technique is Multi-Class Instance Selection (MCIS, [6]), which can only be used in a one-versus-all setting. When the number of classes is K, the one-versus-all scheme considers K problems, where the i-th problem considers the i-th class as positive and the remaining classes as negative. For each of these problems, a subset of instances S is selected, and the SVM is trained on S instead of on the entire training set. The MCIS algorithm clusters only the positive class and then removes instances of the positive class that are close to the centers of the clusters and selects instances of the negative class that are closest to the centers of the clusters. In this way, instances near the boundary between the positive and negative class are selected.Another technique developed to improve SVMs is the Sparsifying Neural Gas (SNG, [7]) algorithm, which is restricted to two-class problems. The intention of SNG is to only select instances that will likely become support vectors in the final SVM classification. To this goal, a combination of learning vector quantization techniques and the growing neural gas algorithm is used.Note that neither MCIS nor SNG was developed to improve the accuracy of SVM classification. Instead, the algorithms aim to improve the efficiency of SVM maintaining a good accuracy rate. However, as MCIS and SNG were specifically developed for SVM classification we do include them in our experimental study to get a complete understanding of TSS for SVM.In this section, we present our approach to use wrapper TSS techniques for SVMs. Recall that wrapper TSS methods depend on the classifier used, in our case SVM. One component that all presented wrapper TSS techniques have in common is that they evaluate candidate subsets of instances based on their training accuracy. When the training set is given by X and subset S⊆X is a candidate subset of instances, its training accuracy, denoted by acc(S), is calculated as Algorithm 1 depicts.Algorithm 1Wrapper TSS for SVMConstruct the SVM based on the instances in SFor each instance in X (including instances in S)Classify it using the SVM obtained in the previous stepcount = number of correctly classified instances in XReturn count/|X| as training accuracy.Next, we plug this function acc into five wrapper TSS techniques. Below, we summarize the procedures that these techniques follow. In particular, Section 4.1 considers three evolutionary approaches, while two non-evolutionary approaches (one hybrid and one editing method) are reviewed in Section 4.2. For more technical details of each of the described methods, we refer to the corresponding papers.The main concept of the evolutionary methods we consider in this section is that they maintain a population of individuals, which are subsets of instances in the TSS case. The algorithms initialize the population randomly and then repeat the steps enumerated in Algorithm 2.Algorithm 2Cycle followed by evolutionary algorithmsRepeat until a specified number of Generations is reachedSelect the best individualsGenerate new individuals from the selected, using cross-over and mutationEvaluate the fitness of the new individualsSurvivor selection: replace the worst individuals in the populationThe iteration is stopped when a fixed number of evaluations is reached and the prototype subset in the final population with the best fitness is returned.The fitness of an individual S is based on the value acc(S) defined above on the one hand and on the reduction red(S) on the other hand, wherered(S)=|X|−|S||X|when X is the original training set. These two components are balanced as follows:(3)fitness(S)=αacc(S)+(1−α)red(S),with α∈[0, 1] a user-defined variable.In particular, we consider three evolutionary algorithms that follow the above general scheme, and whose particular characteristics are summarized below:•Generational Genetic Algorithm (GGA, [8,9]): GGA follows the general scheme of evolutionary algorithms. The population is initialized randomly. Parent selection happens stochastically, that is, individuals with a higher fitness have a higher chance of being selected, but also individuals with a low fitness value can be selected as parent. Once the parents are selected, parents are matched randomly and offspring is generated using two-point crossover. Mutation only happens with a small probability, and the probability of a 0 to 1 mutation is smaller than the probability of a 1 to 0 mutation in order to force the algorithm to obtain higher reduction rates. Survivor selection is done by selecting the entire generated offspring and adding the fittest individual from the previous population, this is also referred to as elitism.CHC evolutionary algorithm ([23,24]): by contrast to GGA, CHC only matches parents that differ enough in order to prevent incest: if the similarity between two parents is larger than a certain threshold, no crossover takes place. The threshold is dynamic, that is, if there are nearly no parent pairs left, the threshold can be increased. Survivor selection happens by merging the old population with the new one and selecting the fittest individuals amongst them. CHC does not use mutations to introduce variation but re-initializes the population if it converges, by selecting the fittest individual found so far and changing a fixed percentage of randomly selected genes from 1 to 0.Steady State Genetic Algorithm (SSGA, [23]): SSGA follows the same procedure as GGA, but instead of selecting multiple parents in each step, only two parents are selected to generate offspring, and they always replace the two worst individuals in the population.Random Mutation Hill Climbing (RMHC, [25]) is a hybrid TSS technique described in Algorithm 3.Algorithm 3Random Mutation Hill ClimbingInitialize a random subset of fixed number of instances SRepeat a predefined number of timesN = neighbor solution of S: replace one instance in S by one in X.If acc(N)≥acc(S),Replace S by NFinally, the Fuzzy Rough Prototype Selection (FRPS, [26]) algorithm is an editing wrapper algorithm based on fuzzy rough set theory [27] that proceeds as illustrated in Algorithm 4.Algorithm 4Fuzzy Rough Prototype SelectionFor all instancesMeasure the quality q based on fuzzy rough set theoryOrder and rename the instances such that q(i1)≥q(i2)≥…≥q(i|S|)Select the subset Siamong S1={i1}, S2={i1, i2}, S3={i1, i2, i3},…, S|S|={i1, i2, …, i|S|} for which acc(Si) is highest.The idea is that only high-quality instances should be selected; the threshold for the quality measure is determined using the training accuracy.In this section, we evaluate if TSS algorithms can improve the accuracy of SVM classification. We first discuss the experimental set-up of our evaluation in Section 5.1, the results are presented and discussed in Section 5.2.We use 43 datasets from the Keel11http://www.keel.es/.and UCI [28] dataset repository. The properties of these datasets are described in Table 1. The number of instances and attributes is limited, as larger datasets might need specialized distributed techniques for TSS [29–33] beyond the scope of this paper.In this study we use the Sequential Minimal Optimization (SMO, [34]) algorithm to construct the SVM, as it is one of the fastest and most regularly used optimization algorithms in the context of SVMs. It divides the optimization problem in several smaller problems and solves them analytically. We choose to use Platt's scaling [10] after building the SVM. We use the Radial Basis Function (RBF) kernel with σ=0.01 and set the cost parameter C=1. These parameters could be tuned, but as we want to study the net effect of TSS, we fix the parameters in our work. We use the pairwise coupling setting to handle multi-class problems, except for the MCIS algorithm which can only be used in combination with the one-versus-all strategy.The parameters of the TSS methods, as proposed in [5,26] are described in Table 2.We use a 10 fold cross validation procedure, that is, we divide the data in 10 folds and use each fold once as test data, and the remaining folds as train data. We apply the TSS technique to the 10 training datasets, this results in 10 subsets S. We build the SVM on each S and classify the instances in the corresponding test set using this SVM model.To contrast the algorithms among each other we use Friedman's aligned-ranks test [35–38]. This procedure calculates the average aligned-ranks of each algorithm, obtained by computing the difference between the performance of the algorithm and the mean performance of all algorithms for each data set. When significant differences are detected, we use Holm's post-hoc procedure [39] to test if the algorithm with the best Friedman aligned-rank significantly outperforms the others. We report the adjusted p-values, which represent the lowest level of significance of a hypothesis that results in rejection.

@&#CONCLUSIONS@&#
