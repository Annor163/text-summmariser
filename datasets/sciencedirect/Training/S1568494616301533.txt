@&#MAIN-TITLE@&#
Cooperation in the evolutionary iterated prisoner’s dilemma game with risk attitude adaptation

@&#HIGHLIGHTS@&#
A new IPD model of multiple agents with adaptive risk attitudes was built.A coevolutionary learning model was designed for a population of adaptive players.Aspirations of agents were determined based on historical or social comparisons.Agents sustain high-level cooperation under historical comparison in simulation.Social comparison makes agents become risk-seeking and uncooperative in the IPD.

@&#KEYPHRASES@&#
Evolutionary iterated prisoner’s dilemma game,Risk attitude,Historical comparison,Social comparison,Agent-based simulation,

@&#ABSTRACT@&#
The Iterated Prisoner’s Dilemma (IPD) game has been commonly used to investigate the cooperation among competitors. However, most previous studies on the IPD focused solely on maximizing players’ average payoffs without considering their risk preferences. By introducing the concept of income stream risk into the IPD game, this paper presents a novel evolutionary IPD model with agents seeking to balance between average payoffs and risks with respect to their own risk attitudes. We build a new IPD model of multiple agents, in which agents interact with one another in the pair-wise IPD game while adapting their risk attitudes according to their received payoffs. Agents become more risk averse after their payoffs exceed their aspirations, or become more risk seeking after their payoffs fall short of their aspirations. The aspiration levels of agents are determined based on their historical self-payoff information or the payoff information of the agent population. Simulations are conducted to investigate the emergence of cooperation under these two comparison methods. Results indicate that agents can sustain a highly cooperative equilibrium when they consider only their own historical payoffs as aspirations (called historical comparison) in adjusting their risk attitudes. This holds true even for the IPD with a short game encounter, for which cooperation was previously demonstrated difficult. However, when agents evaluate their payoffs in comparison with the population average payoff (called social comparison), those agents with payoffs below the population average tend to be dissatisfied with the game outcomes. This dissatisfaction will induce more risk-seeking behavior of agents in the IPD game, which will constitute a strong deterrent to the emergence of mutual cooperation in the population.

@&#INTRODUCTION@&#
Competition and cooperation are among the mostly noted paradoxical phenomena in the real world. For example, collaboration with competitors in business is common for modern firms [1], which is enforced by the rapid advance in information technology and the globalization of markets. Moreover, strategic alliances increase in various industries every year. Firms are hoping to improve their performances by collaboratively developing new technologies, obtaining critical resources, or attaining economies of scale [2]. Nonetheless, despite the benefit of mutual cooperation, firms still compete with one another for gaining sustained competitive advantages in the marketplace [3]. Then, a question arises as to how cooperation emerges among competitors when they still have strong incentives to compete. The iterated prisoner’s dilemma game (IPD) is a prominent paradigm to examine the essence of this problem.The Prisoner’s Dilemma (PD) is a bilateral game in which each player can either cooperate (C) or defect (D) [4,5]. A reward (R) will be given to both players if they cooperate with each other, whereas a punishment (P) will be enforced if they both defect. In the circumstance with one defecting but the other cooperating, the defector will obtain a tempting payoff (T) and the cooperator will receive a sucker’s punishment (S). Given the conditions T>R>P>S and 2R>T+S, a dilemma emerges. Although defection is the dominant strategy for any player, mutual cooperation yields the highest total payoff. The situation will become more complex if the PD is iterated over many rounds (IPD). Thus, the IPD is often referred to as a mixed-motivation game in which opportunities for cooperation and defection (or competition) coexist.The evolutionary IPD game has been widely employed by researchers to study the evolution of cooperative behavior among selfish individuals [4,6–8]. The researchers usually start with a system of multiple agents (players) engaging in recurring IPD interactions while adjusting their own strategies by adaptive learning. As the adaptation proceeds, the aggregation of recurring interactions among agents can generate complex collective behavior of the system [9,10], and the resulting cooperation (or defection) at equilibrium can be analyzed under different scenarios. The aim of this research is to verify what specific factors can influence the emergence of cooperation, and the observed results are used to explain real-world cooperative phenomena.Evolutionary algorithms have been widely adopted as an appropriate analog for modeling the agents’ adaptive learning in the evolutionary IPD game [11]. Agents are generally assumed to adapt their strategies by learning from the better-performing peers in the population. Such a learning mechanism echoes the social learning processes in human social interactions [12–14], in which individuals often draw upon others’ experiences to refine own strategies and enhance own behavioral practices [15]. Recently, with the rapid development in behavioral disciplines [16], in which personal traits are commonly treated as independent variables to explain human behavior, it is strongly suggested to design agents with explicit personal traits in light of behavioral studies through the agent-based modeling [17,18].Risk is a critical feature of individual activities, and risk attitude has been treated as an independent variable in explaining cooperation in the behavioral IPD experiments. Based on the principle that conditional cooperation supports a sub-game perfect equilibrium [19], Snijders and co-workers [20,21] argued that a player’ strategy in the IPD game was whether or not to unilaterally exploit the opponent who cooperated conditionally. Their studies showed by behavioral experiments that risk-averse players favored cooperation in such a scenario, whereas risk-seeking players tended to unilaterally defect. However, their formulation of risk involved only individual preferences on gambles between deterministic and variable prospects. No study had explicitly defined what risk was in the IPD game.Risk-averse players retain steady cooperation with their opponents because the cooperation could minimize the interaction uncertainty and yield stable payoffs for them in the IPD [20,21]. This observation is in accordance with the proposition that firms in the iterated Cournot competition, a realistic IPD situation, would like to coordinate with each other on their production outputs to enhance their income stream stability [22]. Motivated by these studies, we first defined risk as the standard deviation of the one-move payoffs for players in the IPD game [23]. This definition on risk is in line with the concept of “income stream risk” in economics and finance [24], in which risk is conceived as a manifestation of uncertainty or uncontrollability of outcomes [25].Based on this risk definition, we have re-examined the two-player IPD problem by considering both maximizing average payoff and minimizing risk for players [23]. Computational simulation did demonstrate that risk-averse players favored mutual cooperation in the IPD game [23]. Nonetheless, we focused only on the learning of the IPD for two agents of fixed risk attitudes, but did not consider the possibility of changes in their risk preferences over time. Therefore, we attempt to consider agents of adaptive risk attitudes in this paper by following the fact that risk preference is contextually dependent in human society [26,27].The evolutionary IPD game with pair-wise interactions of multiple agents is investigated, in which agents play the IPD game by adapting their risk attitudes according to relative performance feedback. Following the prospect theory [26] and empirical findings on human risk behavior [28,29], agents respond to poor performances by taking greater risk or satisfactory performances by avoiding risk (see Section 3.2 for more details). In particular, the poor or satisfactory performances are distinguished by agents in comparison with their own past performances (historical comparison) or with the population’s average performance (social comparison). Consequently, the evolution is characterized by the changes of risk preferences in the population, and the resulting outcome should be reexamined. The contributions of this study involve the following three respects:(1)A novel evolutionary IPD model is formulated with agents of adaptive risk attitudes, which is aligned with the trend of integrating human traits into the agent-based modeling. To our best knowledge, this work is the first attempt to study the adaptive risk behavior in the evolutionary IPD game, in which the effects of historical comparison and social comparison on the evolution of cooperation are investigated particularly.Based on the evolutionary model, extensive simulation studies are conducted to examine the effect of dynamic risk consideration on the evolution of cooperation. Results indicate that high levels of cooperation can emerge in the IPD even with short game encounters when agents consider only their past payoffs as reference points (historical comparison) to assess their performances for adjusting risk attitudes. The whole population can achieve sustainable high returns at low risk in this case. The speed by which agents change their risk attitudes is shown to have a significant impact on the cooperation.When agents employ the population’s average payoffs as reference points to appraise their performances (social comparison), we find that agents will become more dissatisfied on average. As a result, the evolution leads to a large proportion of highly risk-seeking agents, which deters mutual cooperation and brings low returns to the population. Thus, our findings present the evidence that uncooperative behavior can be perceived as a consequence of social comparison.The rest of this paper is organized as follows. Section 2 reviews some related literature on the IPD game. Section 3 presents the definition of risk and introduces the theoretical basis for the risk attitude adaptation. Section 4 formulates the proposed evolutionary IPD model with agents of adaptive risk attitudes. The simulation studies are reported in Section 5. Section 6 concludes this study with remarks for future work.Games are often employed to model socio-economic phenomena [30], and the evolutionary PD and IPD games have been extensively studied to understand the evolution of cooperation, which has been a considerable concern to evolutionary biologists [31], economists [32,33], social scientists [34], cognitive scientists [35], and even computational scientists [36].A classical evolutionary PD game involves a population of individuals who play the PD game against each other (called the well-mixed game setting). Because uncooperative individuals (defectors) always have higher average payoff (fitness) than cooperators, defection dominates cooperation in the population, driving cooperators to extinction under natural selection. This evolutionary outcome can be analytically derived in replicator dynamics, imitation dynamics, or selection-mutation dynamics [37]. Nonetheless, because cooperation is the beneficial choice to the whole population, a question arises as to what mechanisms can promote the cooperation in the evolutionary PD game.Ohtsuki et al. [31] considered an evolutionary PD game with individuals occupying vertices of a graph, in which the edges of the graph determined links between individuals in terms of game interaction. An individual only played the PD game with the connected ones in the population. Ohtsuki et al. [31] found that the graph-based interaction could allow cooperators to form clusters. They calculated the fixation probability that a single cooperator invaded a population of defectors under natural selection, and concluded that the fewer the connections among individuals, the easier the natural selection promoted cooperation. This mechanism was defined as network reciprocity [38]. Zhang et al. [39] extended the work of Ohtsuki et al. [31] by externally forcing some randomly chosen individuals to cooperate with a certain probability. Mathematical analyses proved that the cooperation could be enhanced with such an external constraint [39].Izquierdo et al. [40] further provided individuals with an option to disconnect current links in response to the game outcomes, i.e., conditional dissociation. When a link was broken, both individuals had to find new links with other peers. Izquierdo et al. adopted computer simulations and analytical approximation to analyze the population dynamics with conditional dissociation under the standard evolutionary forces (selection and mutation). It was concluded that conditional dissociation could induce a significant level of cooperation in the evolutionary PD game. In other words, conditional dissociation could provide an evolutionary basis for the emergence of cooperation through generating endogenous positive assortment between cooperators.Inspired by the notion of indirect reciprocity from the field of evolutionary biology [41], Stahl [42] developed an evolutionary PD model with individuals using Markov strategies, which mapped the reputation information of an individual and its opponent to the possible actions (i.e., C or D). Stahl [42] examined a class of reputation mechanisms, and summarized that, under the reputation mechanism entailing both punishment and penitence,11Cooperating with a good opponent when an individual was in the bad reputation state was penitence, which resulted in the restoration of the individual’s reputation to good. Defecting against a bad opponent when an individual was in the good reputation state was punishment, which did not result in the loss of good reputation [42].a strategy that cooperated with good opponents (with good reputation) and defected against the bad ones (with bad reputation) was a global attractor of replicator dynamics. Therefore, cooperation was steadily sustained, highlighting the important role of indirect reciprocity in the evolution of cooperative behavior [41]. The positive effect of the reputation mechanism on the cooperation has also been supported by behavioral experiments in [43].Real-world interactions always involve repeated encounters, such as the foraging competition between two species and the Cournot competition between two firms [44]. The IPD game indicates a more complex situation than the PD game. Fudenberg and Maskin [19] proved that cooperation could be self-enforcing by players if there were enough rounds in the IPD game.22The discount factorδapplied by Fudenberg and Maskin [19] could be considered as the probability of playing another PD round in the whole IPD game. Then, the average number of rounds could be given by1/(1−δ). Hence, “there were enough rounds” means “a sufficiently large discount factorδ” in [19].This kind of cooperation was termed as direct reciprocity [38]. In particular, their results depended on trigger strategies, which switched to relentless defection as long as opponents defected. It was this threat that forced cooperation in equilibrium in the IPD game, which was considered as conditional cooperation [20].Experimental studies on human behavior have confirmed that people are actually conditional cooperators [45,46]. Suzuki et al. [35] showed the neural and psychological bases of conditional cooperation and suggested that the cognitive inhibition of motivation to cooperate with defectors led to the conditional behavior. Axelrod [4] sponsored three IPD tournaments by inviting scholars from the fields of economics, mathematics, psychology, and social sciences to submit their strategies, and found that a conditional cooperative strategy, Tit for Tat, which cooperated in the first round, and then copied opponent’s last choice, won in all of the tournaments. Inspired by his work, researchers have developed various efficient IPD strategies with the cognitive capacity to perform conditional cooperation, using artificial neural networks [44] or incremental learning technologies [36].Theoretical studies on the evolutionary IPD game also supported conditional cooperative strategies [39,47]. For example, Moreira et al. [48] devised an evolutionary IPD game model in which individuals can be unconditional defectors or memory-constrained conditional cooperators. The conditional cooperators exhibited different memory capacities, which helped them remember opponents’ previous actions. It was theoretically verified that increasing the memory capacities could enable cooperation to emerge and sustain in populations of increasing sizes. While studies on reputation mechanism [42] were implicitly associated with individuals’ cognitive capacities, the work of Moreira et al. [48] focused on cognition explicitly. With the aim to obtain analytical solutions, individuals were often restricted to a small set of pre-determined strategies in the theoretical IPD models [39,47,48]. As a consequence, the theoretical models often lack the capability to demonstrate innovative behavior of individuals in the IPD game [37].The limitation of the theoretical models can be supplemented by multi-agent models [37], in which agents (individuals) are allowed to adopt novel strategies to play the IPD game through adaptive learning. Studies of multi-agent models are usually performed by simulations, which involves a variety of agents [17]. Multi-agent IPD models are more flexible to address the cooperation problem. For example, Fogel [6] studied the evolutionary IPD game with agents represented by finite-state machines under the well-mixed game setting. He found that cooperation (i.e., conditional cooperation) hardly emerged in the case with very short game encounters. Note that this result was in line with the argument in [19], which stated that cooperation could only be sustained in the IPD game with enough rounds.After the seminal work of Fogel [6], researchers have tended to examine the mechanisms promoting the evolution of cooperation in the short-encounter IPD games with the multi-agent models. Alkemade et al. [49] introduced the tag-mediated mechanism into the IPD game, following the theory of kin reciprocity from evolutionary biology [50]. Based on this mechanism, each agent carried an additional tag, and only those with similar ones were matched to play the IPD game. As similar to conditional dissociation, the tag-mediated mechanism allowed the positive assortment between cooperators, and thus ensured the benefits of mutual cooperation to outweigh the losses against defectors. In this case, cooperative behavior could be effectively promoted in the IPD with short game encounters [49].Ishibuchi and co-workers [7,51] examined the network reciprocity in the evolutionary IPD game by assuming agents to be distributed across square lattices. They concluded that fewer connections among agents (i.e., small interaction neighborhoods) could facilitate the evolution of cooperation. Furthermore, they also considered the spatial mechanism in the random pairing IPD game, in which each agent played against a randomly chosen neighbor in each round and thus the interaction length against the same opponent was very short. It was verified that cooperation could be even evolved in such a difficult case when the neighborhood structure was specified appropriately [51].Chong and Yao [52] developed agents with an additional ability to learn about reputations of opponents in the evolutionary IPD game. Agents selected strategies according to the opponents’ reputations, and high levels of cooperation could be steadily evolved in this case. This outcome was verified in all circumstances whatever the length of game was specified [52], demonstrating the successfulness of indirect reciprocity [41,42] in promoting the evolution of cooperative behavior in the IPD game.Agents were supposed to maximize their average payoffs in the IPD game in the above studies. In our previous research on the two-agent IPD game [23], we have demonstrated that risk aversion of agents could also enhance mutual cooperation in the IPD with short game encounter, and this result was also in accordance with the outcomes of behavioral experiments in [20,21], which stated that risk-averse individuals were more cooperative than the risk-seeking ones in the IPD game. In particular, the risk in [23] was defined as the standard deviation of the one-move payoffs for agents in the IPD game by following the concept of “income stream risk” in the field of economics and finance [24].However, we only considered games with agents of fixed risk attitudes in [23]. Since an individual’s risk preference strongly depends on the context in which it is situated [53,54], we attempt to extend our investigation into the IPD game with agents of adaptive risk attitudes. A multi-agent IPD model is considered, in which agents engage in pair-wise IPD interactions while adapting their risk attitudes based on prospect theory [26]. Our main purpose is to examine the novel effect of the dynamic risk consideration on the evolution of cooperation in the multi-agent IPD model, especially with a short-encounter game setting. Using prospect theory to study the behavior of agents was in aligned with the trend of constructing cognitively sophisticated agents with designs informed by psychological theories [17].Suppose that the one-move payoffs for a player in the IPD isp=(p1,p2,...,pl), where l is the length of the encounter andpkrepresents the payoff on the k-th move (k=1,2,…,l). Then, the income stream risk for the player can be measured as:(1)r=1l×∑k=1l(pk−p¯)2and the player’s utility can be expressed as follows:(2)u=p¯1+α×rwherep¯=∑k=1lpk/l, andαrepresents the player’s risk attitude. The value ofαis set as 0 if the player is completely risk-seeking. Otherwise,α>0. The more risk-averse a player is, the largerαbecomes [23].The utility function (2) involves the two most important factors for individual performance measures, namely, the average payoff and the risk [27], which thus creates a decision-making methodology for players to balance between average payoff and risk based on their own risk preferences in the IPD game. Risk-seeking players (with small values ofα) tend to ignore the risk of interaction. They will try to maximize their potential payoff in each move. Such an attempt may entice them to exploit their cooperative opponents, even though the exploitation will expose them to being retaliated, which often yields lower average payoffs for both players. On the contrary, risk-averse players (with large values ofα) are willing to maintain a steady cooperation with their opponents for reducing risk in the IPD game.According to prospect theory [26], an individual’s risk preference strongly depends on the decision-making context in which it is situated. Prospect theory assumes that individuals normally perceive outcomes (in the decision-making context) as gains or losses relative to some reference points, and they tend to be risk averse if the perceived outcome lies in the domain of gains, otherwise, they become risk seeking when the perceived outcome lies in the domain of losses. In the past decades, researchers have provided a large amount of evidences for the reversal change of risk preference in a variety of decision-making domains, including politics [55], financial markets [56], and organization management [29].We attempt to incorporate the above reversal change of risk preference into the evolutionary IPD game by allowing agents to adjust risk attitudes according to their game payoffs. Following previous research [28,53,54,57], the reference point used by an agent for evaluating the outcomes (i.e., payoffs) are defined as its aspiration level (see Section 4.3.1), which is further decided by its past payoff (historical comparison) and by the population average payoff (social comparison). Then, high-performance agents whose payoffs are above their target aspiration levels will increase their risk-aversion levels(α)in subsequent game iterations, whereas low-performance agents with payoffs below aspirations are more likely to decrease their risk-aversion levels. As a result, the dynamic risk attitude adaptation and its effect on the emergence of cooperation can be investigated in the evolutionary IPD game.The above risk attitude adaptation mechanism also has special meaning in the IPD game. First, our study defines risk as the standard deviation of one-move payoffs for agents in the IPD interactions, and thus an agent’s risk aversion level actually reflects how the agent evaluates the stability of its payoffs (profits) [24]. There exists a large amount of empirical evidence that individuals with performances below aspirations (in the loss situations) would like to initiate exploration of new practices [54]. Such an exploration, however, may directly give rise to a large fluctuation in the individuals’ performances (payoffs). In contrast, individuals with performances above aspirations (in the gain situations) will become more concerned with their steady development, i.e., their income stream stabilities [29,58]. Therefore, the risk attitude adaptation mechanism represents these concerns of individuals in the gain or loss situations.Second, as discussed in Section 3.1, individuals with higher risk aversion levels are more willing to maintain a steady cooperation with their opponents in the IPD game, and this proposition has been well verified by laboratory experiments [20,21], and also by computational experiments in our previous study [23]. Thus, the risk attitude adaptation mechanism embodies the fact that satisfied (or dissatisfied) individuals, with payoffs higher (or lower) than aspirations, will become more cooperative (or uncooperative), echoing well with previous research findings [59,60] that individuals experiencing positive (or negative) affect will become more cooperative (or uncooperative). The reason is that individuals tend to adopt a more mutual-rewarding (or self-serving) interpretation when they are making decisions in the domain of gains (or losses) [59,61]. A successful long-term cooperative relationship will surely rely on high levels of satisfaction of players in the IPD game.More detailed description of the evolutionary IPD model with multiple agents of adaptive risk attitudes is presented in the following section.Multi-agent models, with agents interacting and co-adapting in a population, have been used to address various real-world problems, such as manufacturing process organization [62] and natural resource allocation [63]. This section describes the proposed evolutionary IPD model with multiple agents of adaptive risk attitudes, which helps provide insights into the relationship of risk adaptation to cooperation. Fig. 1(a) shows the main process of the model. The evolution starts with a random initialization of a population of N agents, including the initialization of agents’ risk attitudes and strategies. Afterwards, in each generation, agents play the IPD game, and perform risk attitude adaptation and strategy modification subsequently. This evolution process iterates until the maximum generation is reached.Fig. 1(b) depicts the pseudocode of the evolutionary process. The risk attitude adaptation is implemented on the basis of the prospect theory [26,64], and is designed to be payoff-oriented. On the other hand, the strategy modification is performed with regard to agents’ utilities, integrating agents’ payoffs and risk attitudes. Evolutionary learning is applied for the strategy modification.The agent definition, interaction structure, and learning dynamics, which are fundamental to the behavior of a multi-agent model, are elaborated respectively as follows.With the initialization, each agent, indexed asi(i=1,2,...,N), is specified by a risk attitudeαi0and an IPD strategysi0(the “0” in the subscript denotes the initial generation). The value ofαi0determines the payoff-risk trade-off of the agent, and it is limited to the range of [0, 1].33The risk attitudes of agents were picked from{0,0.1,...,1.8}in our previous work [23]. But agents withα>0.8were observed to perform similarly well and were identified as highly risk-averse in the IPD game. Thus, we restrict the range of the risk attitudes of agents to be [0, 1] in this paper.The strategysi0specifies the agent’s actions in the IPD game, which is conditioned on the historical information of prior three moves [4,65]. The binary string representation of strategy is exploited [66]. Because each move has four possible outcomes (“D-D”, “D-C”, “C-D”, and “C-C”), a total of 43 possible histories exist. Thus, a string has 64 bits to store an action for each possible history, in which “1” represents “C” and “0” represents “D”. For example, bit position 15, with its binary code as “001110” (the decoded value starts at zero), specifies the action for the history with an agent playing “D-C-C” and the opponent playing “D-C-D” in the previous three moves. If its value is “0”, then the agent will defect; otherwise, the agent will cooperate.A history of previous three moves is randomly assigned for agents at the beginning of an IPD game [66]. This can help agents find strategies more flexible in different situations. No information is assumed about the length of a game in the binary string representation, which means that agents cannot figure out when the IPD interaction will come to an end. Thus, no “shadow of the future” exists [67].To filter out the interaction factors that might contribute to the maintenance of cooperation, such as kin-biased interaction [49] or spatial (or network) interaction [8,68], each agent is assumed to play the IPD game againstgnopponents randomly chosen from the population. Here, we apply a scheme for selecting thegnopponents for an agent to reduce the effect of randomness. In each generationt(t=0,1,2,...,G), when agentiselects its opponents, the remaining agents are sorted into a list of an ascending order with respect to their risk attitudesαjt(j=1,...,i−1,i+1,...,N). Then, those agents are sectioned intognequal groups, and an opponent is randomly selected from each group.The average payoff for agentiin the IPD game is computed as(3)P=it1gn∑j=1gnp¯(sit,Oppijt)(i=1,2,…,N)wherep¯(sit,Oppijt)represents the average payoff per move for agentito play against its opponent agentOppijt,j=1,2,...,gn.After the pair-wise interactions, all agents undergo risk attitude adaptation, which follows the tenets of prospect theory [26]. As described in Section 3.2, prospect theory suggests that an agent usually compares its payoff with a reference point, and becomes risk averse (or risk seeking) when the former is higher (or lower) than the latter. Hence, the key factor to apply prospect theory here is to determine the reference points used by agents to evaluate their game payoffs.Aspiration levels or goals are usually used by individuals as reference points to appraise own performances [26,28,53]. For example, firms normally evaluate their management achievements against their pre-determined goals. Thus, agents are set to appraise their payoffsPit(i=1,2,...,N)against pre-specified aspiration goalsGit(i=1,2,...,N)for changing their risk attitudes in the evolutionary IPD game. In particular, the agents’ aspiration goals are determined by their past payoffs (historical comparison) and the payoffs of peers in the population (social comparison).Individuals’ comparison with past performance (historical comparison) or comparison to relevant referents (social comparison) has been demonstrated to strongly influence the aspiration goals of individuals. On one hand, historical comparison can help individuals to evaluate their current states relative to the past, and thus to assess any progress or regress [54,57]. On the other hand, social peers stand as an external benchmark for individuals to make comparison [69,70]. Thus, social comparison is also an important source of aspiration. For example, people would like to pursue a relatively higher social rank for contentment and confidence [71]. Likewise, organizations often set goals to outperform competitors in the market competition [28,29].Major extant studies, including empirical studies [29,53,72] and agent-based simulation studies [73], use the average performance of peers as the standard for social comparison. In particular, the aspiration of an individual is usually defined as a linear combination of its own past performance (historical comparison) and the average performance of the reference peers (social comparison) [54,57]. Following these studies, we define an agent’s aspiration goal as the weighted sum of its own past payoff and the population average payoff in the evolutionary IPD game:(4)Git=(1−β)×Pi,t−1+β×P¯t(i=1,2,…,N)whereβ(0≤β≤1) denotes the proportion of the social comparison in the overall aspiration level, andP¯trepresents the population average payoff in the current generation. Here,P¯tis defined as the population median payoff rather than the mean payoff to avoid the effect of extreme cases [58].The adjustment of risk attitude of each agentiis formulated as follows44Risk attitude adaptation is absent in the initial generation ifβ<1because no historical payoffs exist.:(5)αi,t+1={αit×(1+rup)+N(0,αit×rup×v)αi,tαit×(1−rdown)+N(0,αit×rdown×v)Pit≥(1+γ)×Git(1−γ)×Git<Pit<(1+γ)×GitPit≤(1−γ)×GitFirst, ifPitis higher thanGitby a ratio larger thanγ(γ≥0), then agentiis considered to be satisfied with its performance, and it will increase its risk aversion level by a ratio ofrup. Second, ifPitis lower thanGitby a ratio greater thanγ, then agentiis regarded to be dissatisfied, and it will decrease its risk aversion level by a ratio ofrdown. Both of these two kinds of adjustments are perturbed by a Gaussian disturbance. If the Gaussian disturbance drives the risk attitude value outside the range of [0, 1], the attitude will be adjusted back to the closer boundary value. The parametervis assumed to be 0.2, which specifies the scale of the Gaussian disturbance. Moreover, since individuals are always loss averse, the impact of a loss on the risk attitude is always larger than that of an equivalent gain. This has been known as the rule of “losses loom larger than gains” in prospect theory [26]. Thus, the inequalityrup≤rdownis further required. Third, if the deviation ofPitfromGitis smaller than the ratio ofγ, then agent i is considered to be indifferent about the game outcome, and its risk attitude will remain unchanged.Based on Eq. (5), agents’ aspiration goals can exert a significant impact on the changes of their risk attitudes in the evolutionary IPD game. The incorporation of historical comparison or social comparison into agents’ aspirations is in line with previous literature, and it is demonstrated that historical comparison [53,54] or social comparison [29,71] do affect an individual’s risk behavior. Moreover, a direct examination can be conducted about the effects of the two comparison approaches on the evolution of cooperation in the evolutionary IPD game.Agents update their strategies based on a social learning approach [5,50,68] after the adjustment of risk attitude. The underlying mechanism is that agents tend to learn from those who are successful. Such a learning mechanism, as mentioned in Section 1, echoes the social learning processes in human societies [12–14], in which individuals are usually reliant on the successful peers’ experiences to refine their strategies [15].Here, successful agents are assumed to be the bestN/2agents with the highest average payoffs in the current population, whose strategies are denoted by{s1,best,s2,best,...,sN/2,best}. When agenti(i=1,2,...,N)modifies its strategy, two strategies are randomly selected from{s1,best,s2,best,...,sN/2,best}with probabilities proportional to the average payoffs of corresponding successful agents. Then, a candidate strategy (Cit) is generated with each of its bit value randomly picked from either of the two selected strategies (combination). Afterwards, each bit of the candidate strategy is subject to a bit-flip mutation with a probability of 0.05.55The mutation operator is applied for the exploration of innovative strategies for agents in the learning of the IPD game, and thus the mutation rate cannot be too low. Certainly, the mutation rate cannot be too high either, since high mutation will degenerate the evolutionary leaning into a random search [67]. The mutation rate of 0.05 used in this study is chosen based on the previous work in [36,74]. Experiments demonstrate that the agents’ cooperative or defective behaviors are robust in a wide range of mutation probabilities.Interpreted in the economic context, the above selection scheme reveals the principle that the better one performs, the more its strategy will be known and learned by others. The combination operation can be interpreted as the integration of different efficient strategies, and mutation means the attempt of innovation.The utility of the candidate strategyCitis calculated as:(6)U(Cit)=1gn∑j=1gnp¯(Cit,Oppijt)1+αi,t+1×r(Cit,Oppijt)wherer(Cit,Oppijt)represents the risk forCitto play againstOppijt,j=1,2,...,gn. The strategysitof agentiwill be renewed asCitif the following condition is satisfied:(7)U(Cit)≥U(sit)=1gn∑j=1gnp¯(sit,Oppijt)1+αi,t+1×r(sit,Oppijt)Otherwise,sitremains unchanged and is directly copied assi,t+1for agentiin the next generation.Based on Eq. (7), an agent will change its strategy as long as its utility can be increased. This strategy adaptation mechanism relaxes the rather strict cognitive assumption in classical game theory, which states that individuals are looking for the optimal response. The adaptation by social learning has been demonstrated to be more cost-effective and closer to real cognitive processes [75]. The strategies{s1,best,s2,best,...,sN/2,best}are unchanged in each generation, and thus the strategy updating is synchronous for agents. After the strategy modification process is accomplished for every agent, the strategy modification rate of the population is calculated as the ratio of the number of agents with successful strategy renewal to the total number of agents (N).The routines of game interaction, attitude adaptation, and strategy modification are repeated in each generation until the maximum generation G is reached. According to Equations (6) and (7), the risk attitudes of agents exert a significant impact on their strategic choices in terms of the payoff-risk tradeoff in the IPD game. Note that agents try to update their strategic behavior in each generation. Therefore, givenγ≥0, the evolutionary model manifests the intuition that the preferences of agents may adapt slower than their strategies. The parameterγcan be regarded as an indication of the sensitivity of agents’ risk attitudes with respect to their performances in the evolutionary IPD game. A smaller value ofγmeans that agents change their risk attitudes in a quicker response to the game outcomes, whereas a larger value ofγmeans that agents change their risk attitudes in a slower manner. These settings reflect the fact that individuals (or organizations) may respond to what happens swiftly or slowly.We are interested in the emergence of cooperation in the evolutionary IPD model and the way by which it is affected by parametersβ,γ,rup, andrdown. This model is analytically infeasible, and therefore we rely on computational simulations. The standard payoff values T=5, R=3, P=1, and S=0 are consistent with the parameterization in previous studies [4,51,65]. The encounter length l is fixed as 10. Thus, mutual cooperation is difficult to achieve in the IPD without risk consideration [6]. The population size is specified as N=200, and every agent is set to play againstgn=20opponents in each generationt(t=0,1,2,...,G). A maximum ofG=20,000generations are performed.A brief summary of parameter settings for the computational experiments is depicted in Table 1. We begin with the experiment of the evolutionary IPD game with no risk consideration (E0), in which agents’ risk attitudes are fixed as 0 in the evolution. Then, the experiment E1 examines the IPD game with risk consideration but without risk attitude adaptation. Risk attitudes of agents are independently and randomly drawn from the uniform interval of [0, 1] in the initial generation and then remain unchanged throughout the evolutionary process. The E0 and E1 serve as two baselines for subsequent experiments.The experiment E2 examines the dynamics of risk adaptation, in which risk attitudes of agents are independently and randomly initialized and then are adjusted in the evolutionary process. In particular, agents consider only their own historical payoffs as reference points in adjusting their risk attitudes. The value ofβis fixed to 0. The rates of changes of risk attitude are fixed asrup=rdown∈{0.2,0.4,0.6,0.8,1}. The parameterγis altered from 0 to 0.5 in a step of 0.05, resulting in a total of 55 different scenarios. The experiment E3 further investigates the circumstances in which agents also benchmark their payoffs against the population average payoffs in adapting their risk attitudes. All parameters are the same as in E2 except forβ, which is varied from 0.05 to 1. In the experiment E4, we fix the value ofrupas 0.4 and change the ratio ofrdown/rupfrom 1 to 2 in an increment of 0.2 to examine any changes in the evolutionary outcomes, following the principle of “losses loom larger than gains” in the prospect theory [26]. The parameterβis specified as 0 or 1 for the considerations of historical or social comparisons respectively.The payoff value of 2.6 is taken as a surrogate for the emergence of mutual cooperation in the IPD game [6], which is higher than the average payoff of 2.5 that agents can obtain from the move of “CD” or “DC”. Each experiment with a specific parameter setting is repeated for 20 independent runs, and the average results are reported.66Experiments show that the results averaged over 20 runs is not statistically different from (with 95% confidence level) those averaged over 30, 40, or 50 runs.Fig. 2illustrates the population average payoff by generation for E0. We find an initial drop in the population average payoff from 2.25 to about 1.7, indicating that the population tends to mutually defect in early generations. The reason is that the strategies of agents are initialized randomly and the best strategy is to always defect in such an environment. However, this condition last for only a few generations. From about the 30th generation, some strategies begin to reciprocate cooperation, and become proliferated in the population. This makes the population average payoff steadily rise to about 2.5 at about the 350th generation (Fig. 2), which, on the other hand, slowly declines to and converges to a value of about 2.25.The latter decline in the population average payoff can be attributed to the spite effect in the IPD game. A spiteful action is one that worsens the payoffs of others at no benefit or even at a cost to the actor [76,77]. The purpose of the action is to reduce the possibility of being outcompeted. Thus, although the population can achieve a certain level of cooperation after the initial drop in E0, the spite emerges afterwards due to the frequent adoption of defection by some agents, which can yield them higher, at least not lower, payoffs than their opponents in the IPD game. However, the defection bring the agents into lose-lose situations.77According to Vriend [78] and Bergin and Bernhardt [79], social learning from the better-performing peers may (but not necessarily) induce spiteful behavior among agents. Thus, we also conduct additional experiments in which agents select strategies of the other peers with equal probabilities in modifying their IPD strategies. The results are similar to those reported here. This also holds true for the following experiments with strong social comparison (Section 5.3), in which the evolution of high levels of cooperation also fails. Thus, the resulting spiteful (defective) behavior is not due to the social learning mechanism in the model.Fig. 3presents the population average payoff by generation in the experiment E1. It is observed that the population average payoff stabilizes at about 2.5 at the end of evolution, verifying that the cooperation level in the IPD is higher with risk consideration than without [23]. Fig. 4compares the percentages of moves “D-D”, “D-C” and “C-D”, and “C-C” performed by agents in E0 and E1. Since performance stabilizes much earlier, only the result in the first 10,000 generations is shown. Compared with E0, E1 is able to elicit the “C-C” interaction right from the start of the evolution, which is essential for the emergence of mutual cooperation in the IPD with short game durations [52]. This observation is fairly consistent in the subsequent experiments with dynamic risk consideration. On the other hand, although the percentage of “D-C” and “C-D” is higher than that of “D-D” at the beginning of E1, the latter exceeds the former from about the 1000th generation onwards. Although the payoff by alterations of cooperation and defection is greater than that by mutual defection, agents tend to defect for preventing themselves from being outperformed by others. Thus, the population average payoff cannot be maintained higher than 2.6 in E1 either.Further inspection on the percentages of moves played by agents with different risk attitudes indicates that agents perform similarly well in E1. Moreover, no exploitation exists among agents in their mutual interactions. That is, no agents can unilaterally defect (“D-C”) against their opponents without being punished (“C-D”). The reason is that social learning mechanism facilitates information exchange among agents [36], which helps deter the emergence of unilateral exploitation in the population. This observation also holds true for the following experiments with agents of adaptive risk attitudes.In this subsection, we examine the effect of dynamic risk consideration on the evolution of cooperation in the IPD game, in which agents’ aspirations are restricted to their past performances (E2).Fig. 5(a) illustrates the population average payoff for the 55 scenarios of E2. Results are averaged over 20,000 generations. Fig. 5(a) indicates that the parameterγhas an inverted U-shaped influence on the population average payoff. That is, moderate values ofγmaximize the population average payoff in E2. This observation is fairly consistent forrup=rdown∈{0.2,0.4,0.6,0.8,1.0}. In particular, the larger are the values ofrupandrdown, the higher is the value ofγ, at which the population average payoff attains its peak value. Fig. 5(b) depicts the corresponding population average risk attitude, and it indicates also an inverted U-shaped relationship betweenγand the population average risk attitude, which suggests a strongly positive correlation between risk aversion and cooperation in the IPD game (Fig. 5(c)).A closer inspection of the population average payoff by generation reveals that E2 with small value ofγexhibits a similar evolutionary behavior to that of E0.Fig. 6illustrates such a phenomenon withrup=rdown=0.4andγ=0, in which the population average payoff is observed to increase to exceed 2.5 after the initial drop, but it then decreases and stabilizes at about 2.25. Moreover, agents tend to be highly risk-seeking in this case (Fig. 7(a)). Second, E2 withγclose to 0.5 leads to a convergence of the population average payoff towards about 2.5, which is a similar game dynamic with E1. This is because few agents change their risk attitudes whenγis large enough (Fig. 7(c)). Third, ifγis chosen with the population average payoff attaining its peak value, agents will become highly risk-averse in the IPD game (Fig. 7(b)). In this case, the population average payoff will steadily increase with the generation counter after the initial drop, and finally stabilize above 2.6 at the end of evolution, indicating the successful formation of mutual cooperation (see the example withγ=0.15in Fig. 6).The above observations verify the fact that historical comparison promotes the evolution of cooperation among agents.88Tanabe and Masuda [80] also used historical comparison to define aspirations for individuals in the IPD game. However, different from our work, Tanabe and Masuda used the payoff of previous PD rounds to define the aspiration payoff of the current PD for an individual in the IPD game. They designed an on-line reinforcement learning mechanism in the IPD game, in which the propensity of playing the move C or D would be increased (or decreased) as long as the move brought an above-aspiration (or below-aspiration) payoff to an individual. Such a mechanism enabled the learning of cooperative behavior in the evolutionary IPD game. Tanabe and Masuda concluded their result as an affirmative example of the Baldwin effect, which states that learning accelerates the evolution to optimality.Nonetheless, an appropriate setting ofγis crucial for the learning of cooperative behavior. Since strategies are selected by agents regarding their risk preferences in the IPD game (Section 4.3.2), the parameterγcan have a direct impact on the effectiveness of the agents’ strategy modification. A too fast (γ≤0.05) or a too slow (γ≥0.4) adjustment of risk attitude are not anticipated, because they may make the strategy modification too fast or too slow to closely adapt to the changes of the game environment across generations. As summarized in Table 2, the rates of strategy modification of the population are generally smaller at the extremes of the range ofγbut larger in the middle. Thus, a too fast or a too slow adjustment of risk attitude will degrade the effectiveness of strategy learning of agents, which will certainly decrease their satisfactions with own performances and further decrease their willingness to reduce risk in the IPD game. As a result, high levels of cooperation become impossible in these scenarios.As detailed in Section 4.1, agents play the IPD game based on histories of previous three moves. To further explain the different outcomes in the evolutionary games with different speeds of attitude adaptation of agents, we turn to examine the average cooperation rates of agents under different histories of previous three moves. Table 3depicts the result for E2 withrup=rdown=0.4. Similar results can be obtained for other values ofrupandrdownas well. It is seen that agents are more cooperative under the moderate speeds of attitude adaptation than under the speeds that are too fast or too slow. In particular, in the scenarios withγranging from 0.15 to 0.25, in which the population tend to be highly risk-averse in the evolution (Fig. 5(b)), agents generally cooperate with a rate greater than 0.6 after the moves of “D-D” or “D-C”, except for the case of three “D-C”, in which opponents are so foolish to be exploited thrice in the previous three moves. Thus, the contrition for defection is prevalent among agents when they are highly risk-averse in the IPD. This contrition successfully counteracts the spite effect, and ultimately enhances the evolution of high levels of cooperation.The above observation can be further confirmed by comparing the percentages of the moves “D-D”, “D-C” and “C-D”, and “C-C” performed by different agents. Table 4illustrates the relevant results in E2 withrup=rdown=0.4andγ=0.15. Data are averaged over 20,000 generations.99Generations with the absence of agents with particular risk attitudes have been excluded from the averaging process for the corresponding agents. This operation also applies for Table 5.It is clearly shown that the percentage of the move “C-C” is consistently higher than that of “D-C” and “C-D”, which is consistently higher than that of “D-D” in the interactions between all agents, implying the successful inhibition of spiteful behavior in this circumstance. Nonetheless, this finding does not hold whenγis too small or too large. Moreover, compared with the result in E1, more “C-C” are performed between agents with0.2≤α≤1in this scenario. Thus, mutual cooperation become more valued when highly risk-seeking agents (0≤α<0.2) contribute only a small part of the population in the evolutionary game (Fig. 7(b)).Social comparison is a fundamental characteristic of human interactions, in which individuals often assess their own performances by comparing with others to ascertain the validity of their behavior [69]. Therefore, in following experiments, social comparison, in additional to historical comparison, is also employed as a component for agents to determine their satisfactions with their payoffs in the evolutionary IPD game (E3). The parameterβis varied from 0.05 to 1 to investigate the cooperation levels that evolve in the population.The experiment E3 withβ≤0.1exhibits similar result to that of E2, but larger values ofβlead to lower levels of cooperation of agents in E3. Fig. 8(a)–(c)  illustrates the population average payoff, population average risk attitude, and population average risk, respectively, for E3 withβ=0.5. Results are averaged over 20,000 generations. First, mutual cooperation is impossible in the evolutionary game when agents depend too much on social comparison in adapting their risk attitudes. This phenomenon verifies the fact that social comparison of one’s performance to others rewards defective behavior.1010In [75], Zschache also used social comparison to measure agents’ satisfaction in a network-based public good game (i.e., N-person PD game). The satisfaction (or dissatisfaction) of agents was related to strengthening (or weakening) their current behavior and existing relationships. He showed that cooperation could prevail under the social comparison in this network-based public good game, which differed from the result of our experiment E3. Note that the endogenous relationship adjustments played an important role in the evolution of cooperation in [75] and its effect on the evolutionary IPD game should be further investigated.Second, different from the case withβ=0, in which the value ofγhas an inverted U-shaped influence on the population average payoff, a U-shaped relationship is found betweenγand the population average payoff (also the population average risk attitude) whenβ=0.5. That is, a too fast or a too slow adjustment of risk attitude would have a negative impact on the spread of the defective behavior in the population when social comparison exists among agents. This outcome follows the same reason as the survival of cooperative behavior in E2, which is associated with the validity of strategy learning behavior of agents [81]. As shown by the example withrup=rdown=0.4in Fig. 9, an appropriate speed of attitude adaptation (γ=0.1) that assures the accuracy of strategy modification of agents will accelerate the evolution of defective behavior in the social comparison process. On the contrary, a too high (γ=0) or too low speed (γ=0.5) of attitude adaptation will prevent the defection. Thus, a certain level of cooperation can emerge in the population in these two cases, but none with population average payoff stabilizing higher than 2.6.Fig. 10(a) depicts the population average risk attitude (averaged over 20,000 generations) for E3 with different settings ofβ. The parametersrupandrdownare fixed as 0.4. Similar results can be obtained for other values ofrupandrdown. To illustrate the effect of attitude adaptation on the learning of the cooperative or the defective behavior, we compute the average results ofγ=0.1, 0.15, and 0.2 that indicate different speeds of attitude adaptation.It is observed that larger values ofβgive rise to lower population average risk attitudes in the evolutionary IPD game. Thus, agents tend to be more risk seeking when they base their risk attitude adaptation more on social comparison. Although agents performing better than their aspirations (containing the population averages) tend to increase their risk aversion levels, their opponents performing worse than aspirations (containing the population averages) become more risk seeking. Thus, mutual cooperation may fail in the population with strong social comparison, because it can only be established when both players are risk averse in the IPD game. With the increasing failure of cooperation, agents (including those previously performing better than their aspirations) will become more dissatisfied with the game outcomes, and the whole population will tend to be more risk seeking as the evolution proceeds. Therefore, the evolution of mutual cooperation becomes difficult in the evolutionary IPD game with largerβ(Fig. 10(b)).Table 5further reports the percentages of moves performed by agents in E3 withβ=0.5,rup=rdown=0.4, andγ=0.1. The percentage of the move “C-C” is smaller than 0.6 in the interactions between all agents, which is similar to the result in E0 (Fig. 4(a)). Moreover, the percentage of the move “D-D” is greater than that of “D-C” and “C-D” in the interactions between agents with0≤α<0.2, who make up most of the population in the evolution. Thus, social comparison makes agents tend to be aggressive, which fuels the spite effect in the IPD game. This outcome is fairly consistent forβranging from 0.2 to 1. In particular, the contrition for defection is lacking among agents. For example, in E3 withβ=1andrup=rdown=0.4andγ=0.15, agents generally cooperate with a rate lower than 0.3 after the move “D-D” or “D-C”, except for the case in which they cooperate with a rate of about 0.9 following three moves of “D-D” to escape from endless mutual defections (Table 6). Note that the rate of 0.3 is only half of that (0.6) in E2 withrup=rdown=0.4andγ=0.15as well (Table 3). These observations present an explicit explanation for the failure of high levels of cooperation under social comparison.Thus far, we have assumed that agents adjust their risk attitudes upwards and downwards by the same amount while experiencing gains and losses (rup=rdown). According to prospect theory [26], individuals are always loss averse, and thus they dislike a loss more than they like a gain of equivalent amount. Thus, in the following experiment, we fixrupas 0.4 and change the value ofrdown/rupfrom 1 to 2 in steps of 0.2 to examine any change of the cooperative outcome in the evolutionary IPD game (E4). The population average payoffs (averaged over 20,000 generations) are shown in Fig. 11.Comparable results are obtained for the six specifications of the ratiordown/rup. That is, an increasingγhas an inverted U-shaped influence on the evolution of cooperation in the evolutionary IPD game whenβ=0, but a U-shaped influence whenβ=1. In particular, a lager value ofrdown/rupleads to a larger value ofγat which the population average payoff attains its peak or valley values. Moreover, whenβ=0, the influence ofγon the population average payoff becomes relatively modest with larger values ofrdown/rup, which interferes with the evolution of cooperation. This observation suggests that the positive impact of risk consideration on mutual cooperation would be weakened when agents adjust their risk attitudes at a higher rate in the downward direction than in the upward one. Whenrdownis much larger thanrup, risk-averse agents constitute only a small proportion in the population, meaning that agents will be unable to maximize joint gains in their mutual interactions. Thus, the evolution of cooperation becomes difficult.

@&#CONCLUSIONS@&#
