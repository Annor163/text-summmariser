@&#MAIN-TITLE@&#
Online preference learning for adaptive dispatching of AGVs in an automated container terminal

@&#HIGHLIGHTS@&#
AGV dispatching can be done by using a policy recommending the best job assignment.The policy proposed in this paper is based on a pairwise preference function.The policy can be adapted by having the preference function updated through learning.The pairwise preference function is implemented as a neural network.

@&#KEYPHRASES@&#
Vehicle dispatching,Automated container terminal,Machine learning,Genetic algorithm,Artificial neural network,

@&#ABSTRACT@&#
This paper proposes an online preference learning algorithm named OnPL that can dynamically adapt the policy for dispatching AGVs to changing situations in an automated container terminal. The policy is based on a pairwise preference function that can be repeatedly applied to multiple candidate jobs to sort out the best one. An adaptation of the policy is therefore made by updating this preference function. After every dispatching decision, each of all the candidate jobs considered for the decision is evaluated by running a simulation of a short look-ahead horizon. The best job is then paired with each of the remaining jobs to make training examples of positive preferences, and the inversions of these pairs are each used to generate examples of negative preferences. These new training examples, together with some additional recent examples in the reserve pool, are used to relearn the preference function implemented by an artificial neural network. The experimental results show that OnPL can relearn its policy in real time, and can thus adapt to changing situations seamlessly. In comparison to OnPL, other methods cannot adapt well enough or are not applicable in real time owing to the very long computation time required.

@&#INTRODUCTION@&#
The automated guided vehicles (AGV) in automated container terminals transport containers between the quay cranes (QC) at the quayside and the automated stacking cranes (ASC) at the storage yard to support the discharging and loading operations. In a discharging operation, an inbound container is picked up from a vessel by a QC and is handed over to an AGV. The container is then delivered by the AGV to an ASC at a storage block in the yard, where it is stacked and stored until it is claimed for road transportation. The container move in a loading operation is in the opposite direction. First, an outbound container to be loaded onto a vessel is picked up by an ASC from the block where it resided. Next, it is handed over to an AGV and then delivered to the destination QC servicing the target vessel. To maximize the productivity of a terminal, both the discharging and loading operations should be carried out efficiently so that the turn-around time of each vessel is minimized. One of the key factors influencing the quayside productivity is the degree of synchronization between the AGVs and cranes, i.e., the QCs and ASCs. If the AGVs can collaborate with the cranes in such a way that the cranes do not need to wait for the AGVs, the loading and discharging operations can be performed seamlessly without a delay. Such collaboration is not easily achieved by simply increasing the number of AGVs because of the traffic congestion incurred, which results in a delivery delay. What is needed is a good dispatching scheme that allows the AGVs to be scheduled more efficiently.AGV dispatching can be initiated by either a vehicle or a job. For vehicle-initiated dispatching, the vehicle that just finished its previous assignment selects a job to be done. For job-initiated dispatching, the most urgent job to which no vehicle has yet been assigned selects a vehicle. In this paper, we propose a vehicle-initiated AGV dispatching method. Many previous works on AGV dispatching have aimed at the automation of manufacturing systems. A variety of methods have been proposed, ranging from the adoption of simple heuristic rules [1–3] to the use of a Markov decision process [4], fuzzy logic [5], and neural networks [6]. Owing to the relatively simple path layout and small number of AGVs in their manufacturing system environment, simple heuristics such as the nearest-workcenter-first rule and variations of the modified first-come-first-served rule have shown good performances. Although not many, there are some previous studies on the AGV dispatching problem in automated container terminals. These works propose not only the use of simple rule-based methods but also optimization methods based on mathematical programming [7–11].The problem we deal with in this paper for the dispatching of AGVs in an automated container terminal has two objectives. One is to maximize the QC productivity, and the other is to minimize the CO2 emissions. The reduction of CO2 emissions, which has received more attention recently owing to environmental concerns, can be achieved by reducing the empty-travel distances of the AGVs. Although reducing the empty-travel distance may enhance the efficiency of the AGV operation and thus contributes to a reduction in the QC delays, a very strong bias toward an empty-travel reduction leads to a sacrifice of services to the QCs, resulting in increased QC delays. By taking the reduction of empty travel as an explicit objective, we are prepared to trade QC productivity for a reduction of CO2 emissions. Note that simple rule-based methods show a limitation in achieving multiple objectives because of their simplistic decision-making process. Kim et al. [12] solved this problem by introducing a multi-criteria scoring function to evaluate and select candidate jobs. Their scoring function calculates the score of a candidate job through a weighted sum of the evaluations based on various criteria. Each criterion is designed to evaluate a job's status from the standpoint of either QC productivity or the empty-travel distance. Since the resulting score depends on the weights of these criteria, and thus a different weight vector brings about a different best candidate, the weight vector is viewed as a dispatching policy. Pursuing the best policy that works well under various conditions, Kim et al.’s method conducts a search in the policy space to find one that shows best average performance when simulated with a set of various training scenarios. However, the policies thus obtained fail to show the best performance when they are applied to scenarios that differ from what they were trained on. Moreover, their policy search demands hours of CPU time, making it impossible to adjust their policies to new situations in real time.Another aspect distinguishing our work from previous efforts is that our dispatching policy is intended to work in an uncertain and dynamic environment. The work progress in a container terminal often deviates from the expectation for unpredictable reasons. The time taken to load a container onto a vessel depends on the skill level of the QC operator and/or the weather conditions. The retrieval time of a container by an ASC from a storage block can be lengthened if some other containers are stacked on top of the target container. The travel time of an AGV depends not only on the travel distance but also on the traffic congestion it may face en route. Under these circumstances, the terminal situation varies continuously with time, and thus we virtually face an infinite number of different situations. It should be noted that dispatching policies based on simple heuristics or static optimization approaches will not easily achieve synchronization between the AGVs and cranes under this type of environment. However, the dispatching method proposed in this paper can adapt to changing situations by learning and adjusting the dispatching policy in real time.The AGV dispatching method proposed in this paper uses a preference function[13] that returns a real number as a preference value for a given pair of candidates. When k candidates are given, the best one can be sorted out by applying this pairwise preference function to every possible pair. Conflicts among different preferences, if any, can be resolved using the heuristics developed by Cohen et al. [13]. To dispatch AGVs to their appropriate delivery jobs, the preference function is represented by a set of attributes [6,12,14–16] whose values are obtained by evaluating the candidate jobs based on various criteria regarding the achievement of the two objectives described above. Some of these criteria include the urgency of the candidate job, the empty-travel distance to the target container, and the loaded travel distance required by the job. One of the key features of our approach is that, to adapt to the ever-changing situations, the preference function is relearned after every dispatching action. In preparation for the learning, each candidate job seen at the time of a decision is evaluated through a simulation of a short look-ahead horizon. A set of training examples is then generated from these candidate jobs. Each training example is a pair of jobs, in which the first is preferred to the second for a positive example, and vice versa for a negative example. To alleviate the effect of noisy examples that can cause preference conflicts, the examples are weighted differently depending on the degree of difference in preference between the pair of jobs in each example. How much better one job is than the other for a selection is revealed by the simulation result. The experimental results show that our online learning method can really make the dispatching adapt to changing situations. Other methods compared cannot adapt well enough or are not applicable in real time owing to the very long computation time demanded. Although the adaptability does not seem to be essential when the workloads of AGVs are not too heavy, it does make a difference, particularly when the workload is high but there is an insufficient number of AGVs available.The remainder of this paper is organized as follows. The next section describes the AGV dispatching problem in detail through an illustration of the layout of an automated container terminal. Section 3 reviews previous works on the AGV dispatching problem. Section 4 describes our online preference learning algorithm. Section 5 evaluates the performance of the proposed algorithm from a comparison with other algorithms through a series of experiments. Finally, Section 6 provides a summary and some concluding remarks.Fig. 1shows the layout of an automated container terminal, where AGVs move around in the apron area to deliver containers between the quay and stacking yard. The hinterland area is where the external trucks enter and exit to deliver containers to/from their inland destinations/sources. The stacking yard consists of many blocks of container stacks, in each of which a pair of ASCs are operated for container stacking and retrieval. In front of each block in the stacking yard are a few handover points (HP) where a container transfer between an AGV and ASC takes place. A container transfer between an external truck and an ASC occurs at the HPs at the back of each block. An HP also exists under the back-reach of each QC for a container transfer to/from an AGV. AGVs are not allowed to stay idle at any HP under the QC. They can either stay at an HP in front of a block or in the waiting area in the middle zone of the apron between the blocks and the quayside. Some of the AGVs shown in the vertical direction in the figure are in a waiting status.The loading and discharging of containers for a vessel are both carried out in predetermined sequences that are carefully preplanned by taking into account various constraints and conditions. Building a loading sequence is particularly complicated because the weights of the containers, their destination ports, and their stacking status in the yard should all be considered simultaneously. The heavier containers should be loaded before the lighter ones for the weight balance of the vessel. Those to farther destinations should be loaded before those to nearer to avoid rehandlings. Fig. 2(a) shows that the containers in a vessel are stacked in bays arranged in a longitudinal direction. QCs usually work starting from a bay, and move to the next bays in consecutive order to minimize gantry travel. The containers within a ship bay are stored in groups according to their destination ports, as illustrated by A, B, and C in Fig. 2(b). The loading and discharging plans are built under the constraint imposed by this stowage plan in the ship bays. Another aspect to take into account is that those containers stacked higher in the yard should be loaded before those stacked lower whenever possible to minimize the rehandling required during the retrieval from a yard block. Since a delay of any single job in a sequence can disturb the entire process, terminal operators try their best to make the loading operation go smoothly, which is critical for the productivity of the whole terminal. Any delay of a QC or ASC caused by the late arrival of an AGV should be avoided to maximize the productivity. Given sequences of container loading and discharging that are being undergone in parallel, the dispatching of AGVs to determine which delivery jobs to be done next and by which AGVs directly influences the productivity of the cranes and the operational cost of the AGVs.A delivery job of an AGV consists of four steps: empty travel to an HP for container reception, the reception (or pickup) of the container, loaded travel to the destination HP, and release (or drop-off) of the container. Suppose an AGV is assigned a loading job right after finishing its previous job. It first has to make an empty travel to one of the HPs of the block where the target container to be loaded is stored. It then waits there for the ASC of the block to retrieve and put down the container on top of it. After the container is put on, it starts a loaded travel to the HP under the QC that is supposed to load the container onto the target vessel. As soon as the container is picked up by the QC, the AGV moves to the waiting area for the next job assignment. If an idling AGV is assigned to a discharging job, it travels to the HP under the QC that is supposed to discharge the target container. After receiving the container from the QC, the loaded AGV travels to an HP of the block where the container is planned to be stacked. After the container on top is picked up by the ASC of the block, the AGV waits there for its next job assignment.After completing a job upon releasing a container to an ASC or QC, if the AGV receives another container from the same crane as its next delivery job, it can accomplish this job without the need for any empty travel. This so-called dual cycle operation contributes significantly to a reduction of the empty travel by a fleet of AGVs in operation. Many operators of modern container terminals try to build a loading and discharging plan that can maximize the dual cycle because an efficient use of the AGVs leads to a reduction of crane's waiting time for the AGVs. However, effort to reduce the empty travel can sometimes cause longer crane delays. As an example, suppose an ASC of a block needs to release an export container to an AGV for delivery to the QC for loading onto a vessel. If an AGV just freed from its job is far from the ASC, and there is another AGV at an HP of a neighbor block that is still busy with its current job, this second AGV may be a better candidate for reducing the empty-travel distance. However, if the expected arrival time of the second AGV to the ASC is later than that of the first AGV, the ASC has to suffer from a longer wait for the benefit of a shorter empty-travel distance of the AGV. A longer wait by the ASC eventually leads to a longer QC delay. The choice depends on the relative importance of the two objectives, i.e., enhancing the QC productivity or reducing the empty-travel distance of the AGVs.

@&#CONCLUSIONS@&#
