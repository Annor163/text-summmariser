@&#MAIN-TITLE@&#
RGB-D camera based wearable navigation system for the visually impaired

@&#HIGHLIGHTS@&#
A wearable RGB-D camera based indoor navigation system for the blind is proposed.Egomotion estimation is performed using both sparse features and dense point clouds.Probabilistic mapping and traversability analysis for path planning is presented.The system stores and reloads maps to expand coverage area of navigation.The system improves mobility performance complementing the white cane.

@&#KEYPHRASES@&#
Wearable navigation system,Visual SLAM,Assistive technologies for the visually impaired,

@&#ABSTRACT@&#
In this paper, a novel wearable RGB-D camera based indoor navigation system for the visually impaired is presented. The system guides the visually impaired user from one location to another location without a prior map or GPS information. Accurate real-time egomotion estimation, mapping, and path planning in the presence of obstacles are essential for such a system. We perform real-time 6-DOF egomotion estimation using sparse visual features, dense point clouds, and the ground plane to reduce drift from a head-mounted RGB-D camera. The system also builds 2D probabilistic occupancy grid map for efficient traversability analysis which is a basis for dynamic path planning and obstacle avoidance. The system can store and reload maps generated by the system while traveling and continually expand the coverage area of navigation. Next, the shortest path between the start location to destination is generated. The system generates a safe and efficient way point based on the traversability analysis result and the shortest path and updates the way point while a user is constantly moving. Appropriate cues are generated and delivered to a tactile feedback system to guide the visually impaired user to the way point. The proposed wearable system prototype is composed of multiple modules including a head-mounted RGB-D camera, standard laptop that runs a navigation software, smart phone user interface, and haptic feedback vest. The proposed system achieves real-time navigation performance at 28.6Hz in average on a laptop, and helps the visually impaired extends the range of their activities and improve the orientation and mobility performance in a cluttered environment. We have evaluated the performance of the proposed system in mapping and localization with blind-folded and the visually impaired subjects. The mobility experiment results show that navigation in indoor environments with the proposed system avoids collisions successfully and improves mobility performance of the user compared to conventional and state-of-the-art mobility aid devices.

@&#INTRODUCTION@&#
According to the 2014 World Health Organization, 285 million people reported experiencing vision loss. Visual perception plays a very important role in everyday life, and hence visual impairment adversely affects several daily activities such as ambulating familiar and unfamiliar environments. Researches have proved that vision loss severely lowers the mobility of the visually impaired (Golledge et al., 1997; Turano et al., 2004; West et al., 2002). As a result, more than 30% of the blind population do not ambulate autonomously outdoors (Clark-Carter et al., 1986). Visual impairment also increases the risk of unintentional injuries which often result in medical consequences (Legood et al., 2002; Manduchi and Kurniawan, 2011).Long canes and the guide dogs have been the most popular navigation aids among the blind. An interview in Manduchi and Kurniawan (2011) also indicated that 12% and 55% of the visually impaired interviewees have used the guide dog and the long cane as a primary mobility aid, respectively. However, the guide dog has very limited availabilities as only about 1500 individuals are estimated to graduate from a dog-guide user program (news service, 1995) every year. The relatively small coverage range and a reactive nature of the long cane still accompany a high risk of collision, because the visually impaired can avoid obstacles only when they make contact with obstacles. Furthermore, both aids do not provide directional information toward a destination.To overcome the limitations of the conventional navigation aids and improve the orientation and mobility performance of people with vision loss, researchers have proposed Electronic Travel Aids (ETA) utilizing various types of technologies. Over the past few decades, computer vision and mobile robotics technologies have advanced remarkably aided by improvements of vision sensors on measuring 3D structures in terms of an accuracy, price, power profile, and size. This noteworthy improvement in 3D sensing has driven various successful applications in autonomous navigation, manipulation, and human-robot interaction fields. Autonomous robotics navigation and human navigation are different in many respects including sensory systems, spatial representation, and behaviors. However, underlying problems in the two domains are quite similar (Werner et al., 1997). Therefore, we claim that it is very appropriate to transfer these advanced computer vision and autonomous robotics technologies into a wearable navigation system for the visually impaired to improve their mobility.In this paper, we present a novel head-mounted wearable RGB-D camera based navigation system for the blind built on Lee’s work (Lee and Medioni, 2014). The proposed system provides a visually impaired user with navigation aids to reach a destination in an environment where static and dynamic obstacles are present with the minimum amount of help which is a one time travel from one location to another location located on the same floor of a building given no GPS signals and prior map are given. The proposed system allows a blind user to label places of interest, estimates the pose of the blind user, builds incremental maps of traversed environments, and plan a safe and efficient way point to help a blind user navigate from one location to another location in traversed areas.The tactile interface device consisting of four micro-vibration motors is designed to help the blind user receive a cue generated by the algorithms at a reasonable cognitive load through a wireless network. Note is that the system is designed to complement the white cane, not to replace it. We believe the white cane is still good a safety mechanism for the blind and using the system in conjunction with the white cane will guide the blind more safely and naturally.Contributions of this paper are twofold. The first contribution is to propose a wearable navigation system that allows a blind user to label places of interest, builds an incremental map of the traversed environments over time, and helps a blind user to navigate among the labeled places. To the best of authors’ knowledge, this is the first system that performs a SLAM with user interactions allowing a blind user to reuse results of the SLAM algorithm efficiently to improve mobility and orientation of the blind. The system has extended a scope of navigation tasks of existing system (Pradeep et al., 2010) from directing users towards the nearest open space to navigating to a specific destination in a more robust way as presented in Section 4. The second contribution is that we validated and demonstrated the proposed system with both blind and blind-folded subjects. We present experimental results showing that the system improves the mobility performance of the blind subjects and is able to guide the blind subject to a designated goal in unknown environments without colliding to obstacles in a reasonable completion time.In the rest of this paper, we discuss challenges of wearable indoor navigation system for the visually impaired in Section 2, and related research in electronic travel aids devices to improve orientation and mobilities of the visually impaired in Section 3. Then overview of hardware and software components of the system are described in Section 4 followed by details of the navigation software between Section 5 and Section 7. In Section 8, experimental results are presented and we finish with conclusions and future work in Section 9.For a successful navigation, localization is an essential step. Localization is a process to find a relative locations and orientation of a system by analyzing measurements from attached sensors with respect to a global reference coordinate or landmarks. Unlike outdoor environments, such a reference from a global positioning system (GPS) is not readily available in indoor environments. Therefore, a main challenge in indoor localization method is how to define such landmarks.Visual SLAM in robotics and computer vision field has shown very successful performance recently. Visual SLAM approaches build an incremental map of traversed environments and localizes the system within the map simultaneously by detecting and matching visual features. One disadvantage of employing Visual SLAM approaches for real-time applications in large scale is that it is computationally very expensive as the size of features and map grows. Thus, a challenge to use the Visual SLAM approaches is to design an efficient data structure and process to meet the real-time requirement. The performance of Visual SLAM approaches is dependent on the quality of 2D / 3D inputs. It is well understood that a motion from a moving camera attached to a walking person’s head often causes jitter in images. Hence, maintaining accuracy and performing successful navigation tasks using limited hardware resources of wearable navigation system from a moving camera are also a very challenging problem.Another challenge for the system is to find the most adequate interface to guide the blind subjects based on a signal generated by the system. Speech is one of the most popular way for communication for a higher accuracy. Speech feedbacks usually last more than a second which often causes delays in cue deliveries and constant audio feedback has proved to have very high cognitive loads.

@&#CONCLUSIONS@&#
