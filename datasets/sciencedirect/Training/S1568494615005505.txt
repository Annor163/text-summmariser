@&#MAIN-TITLE@&#
Multi-objective optimization of shared nearest neighbor similarity for feature selection

@&#HIGHLIGHTS@&#
A new feature selection method is proposed based on sample similarity.The SNN distance is taken into consideration for sample similarity computation.Multi-objective optimizations is employed to arrive at a consensus solutions.Comparative study with related state-of-art methods are presented here.

@&#KEYPHRASES@&#
Nearest neighbor distance,Hubs,Multi-objective optimization,Sample similarity,Redundancy analysis,

@&#ABSTRACT@&#
A new unsupervised feature selection algorithm, based on the concept of shared nearest neighbor distance between pattern pairs, is developed. A multi-objective framework is employed for the preservation of sample similarity, along with dimensionality reduction of the feature space. A reduced set of samples, chosen to preserve sample similarity, serves to reduce the effect of outliers on the feature selection procedure while also decreasing computational complexity. Experimental results on six sets of publicly available data demonstrate the effectiveness of this feature selection strategy. Comparative study with related methods based on different evaluation indices have demonstrated the superiority of the proposed algorithm.

@&#INTRODUCTION@&#
Analysis of large data involves feature selection as an important stage. It is particularly used during preprocessing, for reducing dimensionality, removing irrelevant attributes, reducing storage requirements, increasing computational efficiency, and enhancing output comprehensibility. In the process it selects a minimum subset of features with cardinality d, from an original set of cardinality D (d<D), such that the feature space is optimally reduced according to certain predetermined evaluation criteria [18]. Feature selection has been widely applied to many fields such as text categorization [16], stock market analysis [19], wireless sensor network analysis [1], genomic analysis [3] and social media analysis [38,39].Search is an important issue in feature selection, encompassing its starting point, direction, and strategy [32]. A search over a dataset involving D features needs to traverse a feature space of 2Dpossibilities. Here D is the cardinality of the original feature space. Therefore, with larger values of D the cardinality of the feature space becomes huge. To deal with such situations, random search or related strategies are found to be useful. One also needs to evaluate the performance of the generated feature subsets [30,31].Feature selection can be supervised, semi-supervised or unsupervised, depending on the availability of class or label information of patterns. The algorithms are typically categorized as filter, wrapper and embedded models [27,32], based on whether (or not) the learning methodology is used to select the feature subset and the stage at which the selection is made. Filter methods rank or evaluate feature subsets based on information content and/or usage of intrinsic properties of data [27]. The wrapper methods assess the selected feature subsets according to their usefulness toward a given predictor or classifier. However selecting a good set of features is usually suboptimal for building a predictor, particularly in the presence of redundant variables. The embedded models use a learner with all features, and select the optimal set based on the structure of the learning algorithm [32]. Since finding the best feature subset is found to be intractable or NP-hard [2], therefore heuristic and non-deterministic strategies are often deemed practical.Supervised feature selection is mostly dependent in the existence of a labeled dataset. On the other hand, in the absence of class information, the unsupervised techniques use some intrinsic property of the data [33]. Here no external information like class label of a pattern is needed, and the selected feature subset is evaluated in terms of certain criteria. Related literature on some such evaluation measures include Category Utility score [11], Fisher's feature dependency measure [13,37], and entropy-based unsupervised feature ranking [8]. These algorithms select subset(s) of features, while preserving inherent characteristics of the data.Similarity measures based on distance are often sensitive to the dimensionality of the pattern space [6]. The relative contrast is found to decrease, with increase in dimensionality, over a broad range of data distribution and distance measures. This, in turn, reduces the discriminatory ability of the measures [22]. As an alternative, researchers have devised a simple and common secondary similarity measure involving shared nearest neighbor (SNN) information. The novelty this paper lies in selecting a subset of features by using this discriminative property of a secondary distance for preserving the neighborhood structure of a pattern, that exists in the original feature space, also in the reduced space.SNN has been used in the merging step of agglomerative clustering [17,24], for clustering high dimensional data sets [15,21], and in identifying outliers in subspaces of high dimensional data [28]. It is found to be less affected by the distance concentration effect that occurs in higher dimensions, and is more robust than primary distances while providing better separability in the presence of irrelevant and redundant features [22].A popular feature selection algorithm, based on nearest neighbor approach, is ReliefF [26,36]. Another well-known algorithm is SPEC [41], which ranks a feature based on its alignment to the leading eigenvectors of the pair-wise similarity matrix of samples. Thereby it helps to preserve the geometric structure of the data. However these algorithms handle each feature individually, while neglecting possible correlation between different sets of features. Zhao et al. [42] overcame this limitation by collectively evaluating sets of features to solve the combinatorial optimization formulation, using a sequential forward selection (SPFS-SFS) [42] approach.In this paper we employ the SNN-based distance for feature selection. It chooses a reduced set of features while preserving the pairwise sample similarity. A feature evaluation criterion is formulated in terms of the SNN distance, and is simultaneously optimized with the feature set cardinality in a multi-objective framework. We employ multi-objective genetic algorithm NSGA-II [10] as an optimization technique to traverse the search space and find a non-dominated set of features. NSGA-II is a randomized search, guided by the principle of evolution and natural genetics, with a large amount of implicit parallelism.The rest of the paper is organized as follows. In Section 2 we present an outline of multi-objective optimization and shared nearest neighbors followed by some basic measures for evaluating feature subspaces. The feature selection algorithm is introduced in Section 3. The experimental results and comparative study are described in Section 4, on various real datasets. Finally, Section 5 concludes the article.In this section we present the mathematical background related to the proposed shared nearest neighbor based feature selection algorithm. We begin with some concepts from multi-objective optimization and shared nearest neighbors, followed by a few feature subset evaluation indices.Multi-objective optimization [9] trades off between a vector of objective functionsF→(x→)=[F1(x→),F2(x→),…,FM(x→)], where M is the number of objectives andx→(∈RZ)is a vector of Z decision variables. Unlike single-objective optimization problems, this technique tries to optimize two or more conflicting characteristics represented by objective functions. Modeling this situation in a single objective format would amount to a heuristic determination of a number of parameters employed in expressing such a scalar-combination-type objective function. The multi-objective technique, on the other hand, is engaged with minimization or maximization of a vector of objectivesF→(x→)that can be the subject of a number of inequality constraints (pi) and/ or equality constraints (qk) or bounds. In other words, this methodology has(1)Minimize/MaximizeF→(x→)subjecttopi(x→)≤0,i=1,2,…,I;qk(x→)=0,k=1,2,…,K;xjL≤xj≤xjU,j=1,2,…,Z;where I and K are the inequality and equality constraints respectively. Each decision variable xjtakes a value within lower boundxjLand upper boundxjU, with the bounds composing a decision variable spaceD. Z denotes the number of components ofx→. The solution set ofx→that satisfies all (I+K) constraints and all 2Z variable bounds, constructs the feasible solution space Ω. As these objective functions are competing with each other, there is no unique solution to this technique. Instead, the concept of nondominance [9] (also called Pareto optimality [7]) must be used to characterize the objectives. The objective function space Λ is defined asΛ=f∈RM, wheref=F→(x→)x→∈Ω. A mapping from feasible solutions space into objective functions space, in two dimensions, is delineated in Fig. 1.The concept of optimality, behind the multi-objective optimization, handles a set of solutions. The conditions for a solution to be dominated with respect to the other solutions are listed here. A solutionx→(1)is said to dominate the other solutionx→(2)if the following two conditions are true [9]:1.The solutionx→(1)is no worse thanx→(2)in all M objectives, i.e.Ft(x→(1))▷Ft(x→(2))∀t=1,2,…,M.The solutionx→(1)is strictly better thanx→(2)in at least one of the M objectives, i.e.Ft¯(x→(1))◁Ft¯(x→(2))for at least onet¯∈{1,2,…,M}.Here we have used the operator ◁ between two solutions t1 and t2 as t1◁t2 to denote that solution t1 is better than solution t2 on a particular objective. If any of the above conditions is not satisfied, then the solutionx→(1)does not dominate the solutionx→(2). So, the solutionx→(1)andx→(2)constitute the Pareto optimal front of these objective functions. A typical Pareto-optimal front over two objective functions is depicted in Fig. 2. Here we simultaneously optimize the conflicting requirements of the multiple objective functions.Genetic algorithms may be used as a tool for multi-objective optimization. In this article we have used the Non-dominated Sorting Genetic Algorithm (NSGA-II) [10], that has been shown to converge to the global Pareto front while simultaneously maintaining the diversity of the population [10].A fundamental form of shared nearest-neighbor similarity measure is the ‘overlap’ [24]. Let a data set X consist of N=|X| sample points,s∈ℕ+and NNs(x)⊆X be the set of s-nearest-neighbors of x∈X as determined by some specified primary distance or similarity measure, viz. euclidean, city block, cosine distance. A primary similarity or distance measure is any function which determines a ranking of sample points relative to a query. It is not necessary for the data points to be represented as vectors. The query pertains to the s-nearest neighbors of a sample.The ‘overlap’ between sample points x and y is defined to be the intersection size SNNs(x, y)=|NNs(x)∩NNs(y)|. It is an alternative to conventional similarity, and is sometimes called a secondary similarity measure as it is based on the rankings induced by a specified primary similarity measure. The similarity measure, implemented here, is based on this ‘overlap’. It is similar to the cosine of the angle between the zero-one set membership vectors for NNs(x) and NNs(y), and is defined as [15,20](2)simcoss(x,y)=SNNs(x,y)s.Transforming to the distance form [22], we have(3)dacoss(x,y)=arccos(simcoss(x,y)).This distance is symmetric and satisfies the triangular inequality. Therefore, this distance is a metric [22]. There also exist other similar distance forms like linear inversion dinvs(x, y)=1−simcoss(x, y) and the logarithmic form dins(x, y)=−ln(simcoss(x, y)) [22]. However, these distances do not satisfy the triangular inequality property. All of these distance functions decrease monotonically with respect to the similarity value between the points x and y.The proposed algorithm, outlined in the following section, employs the dacoss(x, y) distance to evaluate feature subsets.Feature subsets can be evaluated in terms of sample similarity and redundancy. Two such criteria are the Jaccard score (JAC) and redundancy rate (RED).The JAC evaluates the proficiency of a selected feature subset in preserving pairwise sample similarity, and is computed as [42](4)JAC(MF,M,m)=1N∑i=1NNN(i,m,MF)∩NN(i,m,M)NN(i,m,MF)∪NN(i,m,M).HereMF=XFXFTis the similarity matrix computed over the selected feature setF(using the inner product),XFis the pattern set withFfeatures, M is the similarity matrix computed in the original feature space, NN(i, m, M) andNN(i,m,MF)denote the m-nearest neighbors of the ith sample according to M andMFrespectively. JAC measures the average overlapping of the neighborhoods specified byMFand M, with a higher score indicating a better preservation of sample similarity.The RED assesses the average linear correlation among all feature pairs over a subset ofFfeatures, and is measured as [42](5)RED(F)=1d(d−1)∑fi,fj∈F,i>jρi,j,where Pearson's correlation coefficient ρi,jis defined as [40](6)ρi,j=∑k=1N(x(k,i)−x¯(i))(x(k,j)−x¯(j))∑k=1N(x(k,i)−x¯(i))2∑k=1N(x(k,j)−x¯(j))2between feature pairs fiand fj, withx¯(i)=1/N*∑l=1N{x(k,i)}, and the cardinality ofFis d. A larger value of this measure indicates that more features are strongly correlated, and this implies that greater redundancy exists inF. Therefore a smaller value ofRED(F)corresponds to the selection of a better feature subset.The proposed feature selection method selects a small subset of features which preserves the pair-wise common natural grouping present in its s-size neighborhood in the original feature space while simultaneously reducing size of the feature set. Let PDMsbe the pairwise secondary distance matrix of N×N dimension, where N is the number of patterns in a data set. We have(7)PDMs(i,j)=dacoss(i,j).It is obvious from the definition that PDMsis symmetric and its principal diagonal elements are always zero. Hence the upper triangular part of matrix PDMscontains information about the pairwise common natural groupings of all N data points in the original feature space.Patterns which are closer to the mean of a data distribution are typically closer to all other patterns over any feature subset. It is shown [35] that this tendency gets amplified with higher dimensionality, such that patterns residing in the proximity of the mean appear closer to all other patterns as compared to the analogous situation in a reduced space. This tendency results in high-dimensional patterns, that are closer to the mean, having increased inclusion probability in the k-NN lists of other patterns. Such patterns are termed hubs and are the most relevant [35]. Real datasets are usually clustered, with patterns being organized into groups produced by a mixture of distributions. Therefore, hubs tend to be closer to their respective cluster centers. Our algorithm tries to include these special patterns by selecting those with lower values of dacoss. These are used as representative points of the datasets, since a lower value of dacossis indicative of a higher value of SNNs. Moreover, dacossis computed in the original high-dimensional space with a lower value implying an increase in the inclusion probability of the patterns that are closer to the cluster-means in the dataset.Algorithm 2 is used to generate a reduced set of patterns Xsel. As |Xsel|≪N, it reduces the computational complexity of the subsequent optimization process.PDMs1is computed employing s1-nearest neighbors on the sample set Xsel, with its dimension being Nsel×Nsel(Nsel=|Xsel|) where Nselis a user-defined parameter. It may be noted that the choice Nsel≪N serves to reduce the effect of outliers on the feature selection process.LetPDMs1fredu(i,j)be the pairwise secondary distance, with s1 nearest neighbors being evaluated on set Xselover fredusubset of features. The freduis a reduced subset of the original features, and is considered for evaluation. In a multi-objective framework the proposed algorithm simultaneously reduces the size of the feature subset while preserving the pairwise topological neighborhood information present in the s-size neighborhood in the original feature space. This neighborhood information is incorporated in terms of the objective function(8)F1=∑i=1,j>ii=Nsel−1,j=Nselabs(PDMs1(i,j)−PDMs1fredu(i,j)).The second objective function is the cardinality of the reduced feature set, and is expressed by minimizing(9)F2=|fredu|.We employ NSGA-II [10,5] for heuristically exploring this search space.A given feature subset is represented as a binary string, also called as chromosome, with a “0” or “1” in position k specifying the absence or presence of the kth feature in the set. The length of the chromosome is equal to the total number of available features in the data. It represents a prospective solution of the problem in hand. A population of such chromosomes is evaluated by optimizing the two objective functions, in a multi-objective framework, in order to enhance the fitness.Algorithm 1MFSSNNInput: Pattern set X, with N sample points and D features.Size of neighborhoods s and s1, cardinality of reduced set Nsel.Output: A feature subset ffinal.1: Construct pairwise dissimilarity matrix PDMsusing Eq. (7).2: Construct a set Xselof samples using Algorithm 2.3: CalculatePDMs1with s1-nearest neighbors on the set Xsel.4: Select feature subset(s) by simultaneously optimizing F1 [Eq. (8)] and F2 [Eq. (9)] in a multi-objective framework.Multi-objective GA proceeds to find a fit set of individuals (here, feature subsets) by reproducing new children chromosomes from older parents. In the process it employs the operators selection, crossover (where parts of two parent chromosomes are mixed to create an offspring) and mutation (where bit(s) of a single parent are randomly perturbed to create an offspring). Crossover probability pc(with scattered crossover function) and mutation probability pmare used. Crowded-comparison operator [10] is used for selection, with preference for solutions having higher non-domination rank and separated from each other based on crowded distance. The chromosomes associated with the non-dominated solutions with respect to the fitness functions are decoded to obtain the reduced feature subsets. The algorithm stops when the weighted average change in the fitness function value, over 50 generations, is less than the average change in value of the spread of the Pareto set [10].The multi-objective feature selection algorithm, based on Shared Nearest Neighbors (MFSSNN), is outlined as Algorithm 1.Algorithm 2A heuristic for constructing XselInput: The pair wise dissimilarity matrix PDMsand Nsel.Output: Reduced sample set Xsel1: Find the minimum entry (>0) of each row of PDMsand store as min_rowi, with i∈1, …, N−1.2: Sort min_rowiin ascending order along with indices.3: Select top Nselindex values of min_rowi.4: Generate sample set Xselwith these selected points.

@&#CONCLUSIONS@&#
In this article we have developed a new unsupervised feature selection algorithm which tries to preserve sample similarity in a reduced feature space based on the concept of shared nearest neighbor distance. It simultaneously reduces the cardinality of feature subsets in a multi-objective framework. The results demonstrate that the reduced feature subsets could not only preserve the predictive accuracy of the classifiers in the reduced feature space, but also improved a little in some of the cases with respect to the original feature space. The validation index indicated that sample similarity was also preserved in the reduced space, with the selected features having very little correlation amongst them. Comparative study with related algorithms like SPFS-SFS, ReliefF and SPEC demonstrated the suitability of our algorithm.Our algorithm tries to preserve pair-wise structural similarity through a set of representative points (termed hubs [35]), which lie closer to the cluster-means of a dataset. When these points get identified correctly, our algorithm performs better than many other related methods (as demonstrated in Fig. 5 and Table 1). However appropriate selection of this representative set is a bottleneck, and sometimes results in a collection of patterns other than hubs. This is an area which needs further investigations, and is a reason why our algorithm fails in some cases. We also plan to design other multi-objective frameworks, specific to this problem, in order to alleviate the situation. We shall further investigate our method on more datasets as well as on datasets which have no label information associated with them.