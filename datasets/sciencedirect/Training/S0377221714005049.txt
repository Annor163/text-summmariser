@&#MAIN-TITLE@&#
Concurrent multiresponse non-linear screening: Robust profiling of webpage performance

@&#HIGHLIGHTS@&#
A data-engineering analysis is developed for handling website performance.The method profiles multi-response multi-factorial orthogonal designs.We screen 3-response 4-factor performance data non-parametrically.The screening is extended for weighted conditions among responses.

@&#KEYPHRASES@&#
Robust screening,Non-linear profiling,Non-parametric screening,Non-linear orthogonal array,Webpage performance,

@&#ABSTRACT@&#
Profiling engineered data with robust mining methods continues attracting attention in knowledge engineering systems. The purpose of this article is to propose a simple technique that deals with non-linear multi-factorial multi-characteristic screening suitable for knowledge discovery studies. The method is designed to proactively seek and quantify significant information content in engineered mini-datasets. This is achieved by deploying replicated fractional-factorial sampling schemes. Compiled multi-response data are converted to a single master-response effectuated by a series of distribution-free transformations and multi-compressed data fusions. The resulting amalgamated master response is deciphered by non-linear multi-factorial stealth stochastics intended for saturated schemes. The stealth properties of our method target processing datasets which might be overwhelmed by a lack of knowledge about the nature of reference distributions at play. Stealth features are triggered to overcome restrictions regarding the data normality conformance, the effect sparsity assumption and the inherent collapse of the ‘unexplainable error’ connotation in saturated arrays. The technique is showcased by profiling four ordinary controlling factors that influence webpage content performance by collecting data from a commercial browser monitoring service on a large scale web host. The examined effects are: (1) the number of Cascading Style Sheets files, (2) the number of JavaScript files, (3) the number of Image files, and (4) the Domain Name System Aliasing. The webpage performance level was screened against three popular characteristics: (1) the time to first visual, (2) the total loading time, and (3) the customer satisfaction. Our robust multi-response data mining technique is elucidated for a ten-replicate run study dictated by an L9(34) orthogonal array scheme where any uncontrolled noise embedded contribution has not been necessarily excluded.

@&#INTRODUCTION@&#
Structured knowledge acquisition remains at the forefront as a key field of intense interest in information sciences as it is exhibited by current reviews on the subject (Liu, Li, Liu, & Chen, 2012; Mariscal, Marban, & Fernandez, 2010). Statistical engineering provides the inference capability for the knowledge gaining effort (Hastie, Tibshirani, & Friedman, 2009). It is statistical engineering that fuels innovation for products and services that succeed in sustaining global demand (Goh, 2010; Pantula, 2011). A fruitful strategy for imbuing cutting-edge information into designing competitive products while building insight in how to improve existing processes has been associated with the area of Design of Experiments (DOE) (Taguchi, Chowdhury, & Taguchi, 2000; Box, Hunter, & Hunter, 2005). DOE studies are generally geared towards screening projects. In harvesting information, DOE provides the profiling apparatus where screened effects undergo a fastidious statistical filtering (Bose, 1961). Classical DOE methodology requires structured data generation engineered through the use of fractional factorial designs (FFDs) followed by a data processing phase (Mukerjee & Wu, 2010). In this work we will spotlight a particular brand of FFDs which is favored by many researchers in a wealth of technological applications. Such FFDs are the ones that belong to the so called orthogonal array (OAs) family. It is noted that OAs constitute part of the standard engineering toolbox for robust on-line design in Taguchi methods (Taguchi, Chowdhury, & Wu, 2004). Accordingly, OAs are applied broadly in information research in diverse studies that range from improving computer vision and medical imaging processing, to enhancing spam-filter performance and software development, and from optimizing vapor deposition processes to screening milling operations and simulated annealing (Besseris, 2009a, 2010b, 2010c; Chang, 2008; Chen & Sun, 2000; Jung & Yum, 2011; Orfanakos & Besseris, 2010; Tansel, Gulmez, Demetgul, & Aykut, 2011). Customarily, the data analysis part in DOE is entrusted on mainstream multi-testing techniques such as the Analysis of Variance (ANOVA) or regression-based approaches related to the General Linear Modeling (GLM) principle (Ilzarbe, Alvarez, Viles, & Tanco, 2008). The OAs are utilized optimally in programming the data collection phase of a study only when they are saturated (Besseris, 2009b). OA saturation is synonymous to complete utilization of all available array columns for a selected OA sampling scheme. In the state of saturation, resource usage and research budgeted expenditures are minimized. Thus, from a data mining perspective, while maximum information is recovered from collecting datasets that have been planned on saturated OAs, the cost to gain that information is optimally suppressed at the same time. However, working with saturated OA schemes takes a toll on selecting a proper mainstream technique to handle the output data. This is because saturated OAs are not generally symbiotic with standard multi-factorial contrasting tools such as ANOVA and GLM. Both of these last two methods emerge as inoperative in saturated OA schemes because both approaches require information about the unexplainable error which is practically incalculable under saturation. As a result, inferences drawn with either ANOVA or GLM techniques are not poised to be considered in an objective manner. This phenomenon stems from the fact that at the saturation point, all available DFs become depleted due to their complete allocation to the tested effects, allowing no capacity to be distributed to the estimation of the unexplainable error (Besseris, 2012). Without the ability to pinpoint the contribution arising from the unexplainable error, the required F-test ratios in ANOVA and the standard errors in GLM cannot be decoded. In summary, saturation in OA sampling schemes simply equalizes the supply of data with the maximum demand for identifiable information but in the process removes the discovery capability from ANOVA and GLM methods. In spite of lacking any solid basis to sizing-up statistical significance, heuristic approaches exist to assist in interpreting experimental outcomes using the fitting components of ANOVA or GLM results. Response graphs and response tables typify a line of such diagnostics (Box et al., 2005; Taguchi et al., 2004). Squeezing out maximum information at the saturation point during an OA analysis has been a topic of heightened interest for the past half-century where more than thirty techniques have been devised to restore objectivity in the multi-factorial decision making (Besseris, 2009b). Most of the proposed work has been confined to the saturated-unreplicated case addressing the uni-response problem. Addressing the multi-response multi-factorial profiling case has been dealt with recently using nonparametrics which includes the additional option to prioritize the profiled traits for a group of unreplicated responses (Besseris, 2009c). A pure order-statistics single-response methodology has been proposed for a non-linear unreplicated-saturated OA scheme assorted with an illustrative application drawn from the area of improving the quality performance of an information technology system (Besseris, 2010a).The technique that is presented in this work is intended to extract simultaneous multi-response information induced from multi-factorial contrasting. The main aim of our proposal is to provide a robust tool capable of deciphering process or product behavior in pragmatic modern operational surroundings. As it will become more evident later in this report, the bulk of potential assumptions arising with such types of techniques is maintained rather lean in our approach. The main theme of our method is to stochastically screen for multi-effect status and non-linearity simultaneously. This technique becomes then a convenient ‘two-in-one’ (combo) profiler. Ordinary profilers test at two preselected operational endpoints taking the risk that in the examined range the response will be a monotonous increasing or decreasing function. This assumption might be valid unless the shape of the curve is concave or convex. If the latter occurs then predictions may be digressed. There are several advantages that are realized with this new approach. Our method integrates the robust data reduction of replicates with the super-ranking concept to downgrade the replicated multi-response problem to its corresponding unreplicated counterpart (Besseris, 2009c, 2010d). The overall method may claim to offer some stealth features because is operable without an assuring knowledge of the various distribution laws that ought to be tagged to the investigated responses. Theoretically, our method possesses a stochastic intelligence that ‘flies-by’ without being gaged against any statistical entity that might resemble to the concept of the unexplainable error. Nevertheless, the approach intimately conforms to leading edge tendencies in information technology to facilitate forecasting under unknown and unknowable disturbances (Cicerone, Di Stefano, Schachtebeck, & Schobel, 2012).The type of the sampling schemes that will be adopted in the developments that follow have been in accord to the non-linear OAs insomuch as they are regularly embraced in the recent information and knowledge engineering research (Chang, Chen, & Liao, 2010; Chang & Chen, 2011; Khaw, Lim, & Lim, 1995; Kim & Yum, 2004; Lin & Jules, 2006; Wang & Huang, 2007). Finally, our formalism is distinguishable in another practical aspect that of being indifferent to the so-called sparsity assumption (Besseris, 2009b). Sparsity is a binding notion for quite a few methods that aspire to process saturated OA datasets. In simple terms, this means that without the sparsity assumption several techniques that depend on it are rendered inoperable. Sparsity takes for granted that only a small number of the examined effects will turn out to be statistically significant. From a knowledge discovery standpoint, sparsity discounts in advance the key role of the examined effects. In other words, sparsity is naturally biased against the anticipated extent of influences on the investigated phenomenon. Henceforth, the sparsity postulation has been rescinded in our developments to the benefit of exploring the total content of information without any preset restrictions on how much information should be eventually extracted from the profiling endeavor.The measurement of web site performance has been linked directly with corporate overall performance for a wide spectrum of organizational types (Welling & White, 2006). Therefore, it is of paramount importance to acquire and further develop techniques that conveniently and efficiently metamorphose web usage data to propitious knowledge discovery (Liu, 2011; Raju & Satyanarayana, 2008). The ideal tactic to gain knowledge regarding web-page performance issues is to examine weblog transactions (Facca & Lanzi, 2005). Integrating web data which may emanate from multifarious streams, ranging from page-accessing to querying information, offers new grounds for exploratory work on web analytics (May & Lausen, 2004). Several grassroots techniques exist for data-mining website behavior, particularly when the webpage view behavior has been captured through well-planned queries (Mecca, Mendelzon, & Merialdo, 2002; Mena, 1999). Data preparation techniques that consolidate structured sampling and efficient collection schemes are deemed as highly desirable particularly when information is required for characterization of World Wide Web browsing patterns (Cooley, Mobasher, & Srivastava, 1999). Subsequently, the problem of screening and optimizing complex web-site properties has attracted modern data-processing treatments which indicate that there is a great anticipation for further research on this subject (Asllani & Lari, 2007). To assist in setting up screening studies, several web search quality measures have been proposed which lend themselves to efficient data mining (Sufyan Beg, 2005a; Sufyan Beg, 2005b; Sufyan Beg & Ahmad, 2007). Indicative examples for tracking web site enhancement may include consideration of quality measures that address the theme of information overload as well as the issue of tracing search depth capability (Lin, 2006).An effort is attempted to enhance the performance of web-page loading time in conjunction with the important input relayed from concurring customer satisfaction data. In this study the three characteristics to be measured are: (1) the time to first visual (TFV), (2) the total loading time (TLT) and (3) customer satisfaction (CS). TFV which is also known as “time to first paint” or “time to start render” is the time it takes for a viewer to identify the onset of a page content (Sterne, 2002; Croll & Power, 2009). TFV measures web page loading efficiency in the time span from which a web page URL has been keyed-into a browser window to the time the user detects the first visual element on the browser window. During this time the user is looking at a blank page. Viewers desire this time interval to be instantaneous if possible. Correspondingly, TLT (also known as “time to display” or “time to full screen”) readings were collected when the web page was completely loaded. TLT signifies the time interval from which a web page URL has been entered on the browser window to the time the HTML document has been fully displayed. Again, viewers demand that this time to be instantaneous. TFV and TLT are measured in ms, respectively. CS has been measured in a five-point Likert-type scale where the maximum satisfaction level is allotted to a value of 5.We screen the concurrent behavior of the above three responses by profiling the following four controlling factors which was thought that may influence directly the page-content transactions: (1) the number of Cascading Style Sheets files (CSS), (2) the number of JavaScript files (JS), (3) the number of Image files (Im) and (4) the Domain Name System aliasing (DNSA). Meanwhile, there was a lack of any succinct knowledge regarding whether the ensuing trends were to be monotonously increasing or decreasing with respect to each effect when contrasted against each response individually. Therefore, it was decided to select a sampling scheme that is known to accommodate scoping potential non-linear effects, if they were present. One scheme that allows testing for four factors simultaneously while tracking alternating slope tendencies is the L9(34) OA (Taguchi et al., 2004). To plan a dataset collection with an L9(34) OA, it requires setting-up trials adjusted at three distinct settings per investigated controlling factor. The selected experimental settings are listed in Table 1in terms of their corresponding controlling factors which are also expressed in coded form for convenience in referencing them. As a side note, it needs to be clarified here that for CSS and JS files, the zero count setting was achieved by inlining the code while converting Im files to base64 coding.Data collection was attained by the real browser monitoring tool of WatchMouse™ on transactions performed on the managed web-hosting by Liquid Web®. The web page tested in this research had typically about 5million visitations per month while the content size fluctuated in loads of about 3MB depending on the respective trial set-up. The step-by-step procedure for carrying out the web experiments as well the data manipulation of the supporting weblogs is detailed by Ziogas (2012). At the inception of this project, data from the scheduled transactions were gathered for ten consecutive days. That time frame was appointed arbitrarily and in the absence of any prior knowledge with respect to an anticipated degree of repeatability for the planned trials. Estimating statistically the sufficiency of the number of conducted replicates was to be determined a posteriori by inspecting all possible cross-correlations among the repetitive data. Therefore, exhibiting high correlation coefficients among replicates would provide a means of quantifying repeatability by establishing a satisfactory level of statistical significance. This last assumption is acceptable in the particular context of this research because we deal with characteristics that are not dynamic. Additional data would have been gathered, if they were needed, until repeatability was resolved in case where there was not a satisfying outcome from the initial dataset. The nine-run experimental recipe has been compiled in Table 2where increasing run numbers have been assigned to each trial combination for ease of reference. All data processing in this article has been carried out with the software package MINITAB 16.0. Exact runs-tests and correlations have been calculated through the statistical software SPSS 19.0.We describe the concurrent multi-response, multi-factorial screening of replicated trials. Data are engineered by a typical orthogonal array (OA) which is also selected to bridge over detection capabilities for non-linearity. A convenient choice for a generic three-level (setting) OA is considered suitable, and thus implemented, in treating simultaneously the potency of the examined effects along with curvature tendencies. Moreover, three-level arrangements are highly economical since data generation is maintained to an absolute minimum for detecting an underlying non-linearity. Technically, pure three-level OAs permit probing at two predefined operating endpoints and one intermediate setting for all investigated controlling factors in the collection scheme. An accessible way of constructing easily non-linear experimental designs is to adopt Taguchi-type OAs (Taguchi et al., 2004) of the Ln(3k) family (n=number of runs, k=number of tested effects). Therefore, Ln(3k) OAs are assumed in the developments that follow. For illustrational purposes, the L9(34) OA will be showcased particularly in more detail by the end of this outline since it suits directly the case study that follows in the next section.We consider any arbitrary Ln(3k) OA where the investigated factors are labeled as X1, X2, …, Xk. Then, their respective predetermined settings may be written simply as X1j, X2j, …, Xkj(j=1, 2, …, n). Furthermore, we consider examining a number of m characteristics that comprise of the responses Y1, Y2…Ym. The generated responses may be identified in terms of r-replicated run-entries symbolized as: y1jl, y2jl, …, ymjl(j=1, 2, …, n; l=1, 2, …, r). A comprehensive depiction of the relevant input/output DOE arrangement where factors and responses are positioned on the left- and right-hand side of the design, respectively, is shown in Fig. 1.Since the use of fractional factorial designs often reflects a situation where probing is to be constrained by finite sampling, the need for deploying practical and resistant measures with a substantial breakdown-point performance is apparent. In exploratory studies where information is extracted from minimal data volumes, suppression of breakdown point performance may become crucial in hindering robust decision-making. In our multi-response data reduction strategy, we will attempt first to compress the r-replicates for each of the m generalized responses down to merely two meaningful yet robust components for each response individually. One component will be based on the median (Med) measure which is a well-known robust estimator for capturing the central tendency of the r-replicates (Conover, 1999; Hoaglin, Mosteller, & Tukey, 2000). When data mining against unknown and possibly intermixed distributions, information may be obfuscated if the selected estimator yields a low break-down point. The median estimator possesses a known breakdown point of 50%, while the more commonly used alternative which is the mean measure may easily be upset since it is floored by a breakdown point of 0% (Hubert, Rousseeuw, & Van Aelst, 2008). For a particular response, q, and a given trial run, s, respectively, the relevant entries to undergo compression will be the set of {yqsl} for l=1, 2, …, r. To obtain a median estimation, the elements in {yqsl} are rank ordered to {y′qsl} for l=1, 2, …, r, where y′qs1⩽y′qs2⩽⋯⩽y′qsr. Then, we need to prepare the positioning point (p) formula (Hoaglin et al., 2000). If r is odd then p is defined as:(1)p=r+12withMedqs=yqsp′If r is even then two positioning points are needed, p and p′ defined as:(2)p=r2andp′=p+1withMedqs=(yqsp′+yqsp′′)/2The median operation in terms of the column arrangement provided in Fig. 1 for the qth characteristic is better now explained in Fig. 2. To complete the stochastic description of a replicate dataset, a robust estimator will be needed indicating the extent of dispersion for the inspected data. An appropriate choice to measure robustly the spread of the data is the interquartile range (IQR) (Hubert et al., 2008). The IQR provides a parametric estimate of the replicate dataset variation while maintaining a non-parametric nature in gaging the characteristic estimate. IQR inference is also computationally inexpensive offering another incentive to adopt it. Likewise to median measure properties, the IQR as a dispersion measure fends off more effectively low-quality data and other intrusions since its breakdown point is set at 25%. Thus, in contrast to the ordinary candidate for variance estimation, i.e. the standard deviation – which possesses a breakdown point of 0% – the IQR is received as a justifiably advantageous alternative. To compute an IQR estimation, we need to borrow the non-central location measures of the first (Q1) and third (Q3) quartiles. By definition then: IQR=Q3–Q1. Q1 and Q3 are the sample values where 25% of the observations are smaller and larger in magnitude, respectively. In order to obtain the positioning point formulae for Q1 (Q) and Q3 (Q′) we write:(3)Q=r+14andQ′=3(r+1)4(1)If Q, Q′ are integers then Q1=y′qsQand Q3=y′qsQ′.If Q, Q′ are not integers but half-way between two integers, then the mean value of the two integers will represent their magnitude.For any other case, Q, Q′ are rounded off to the closest integer.In Fig. 3, the IQR compression process is exhibited for the qth characteristic in terms of the DOE arrangement of Fig. 1. Up to this stage, an important step has been attained in compacting the r-replicated data for the m-characteristics in two distinct ‘co-responses’ reflecting location and dispersion tendencies in a more concrete manner. The original input/output DOE representation of Fig. 1 has now been summarized robustly in terms of the median and IQR estimators displayed in Fig. 4. At this point, it is imperative to check for possible correlations between the median and IQR columns within each characteristic individually. Only in the case that it turns out that they are both uncorrelated, we may proceed with including both entities in the non-parametric transformation step that follows. If the outcome is that they are strongly correlated then both measures essentially transmit the same information and thus one of them is redundant and may be omitted in any subsequent data processing. Since the generalized profiling framework we present in this work is not restricted by the normality hypothesis, the Kendall’s τbcorrelation coefficient will be employed to check the level of association between the two measures (Conover, 1999). Equally important is to check at this moment the degree of association across medians for all characteristics and to repeat this process with the IQR vectors. Again, the Kendall’s τbcorrelation coefficient may be used for this screening. The median response of a characteristic that will be found to correlate with that of some other characteristic(s) median response may be dropped since processing correlated characteristics only offer redundant information. The same argument holds for the correlation treatment across their corresponding IQR responses.Next, the data compressed representation of the responses in Fig. 4 receives a separate rank-ordering for the median and IQR vectors for each characteristic individually. This transformation is elucidated in Figs. 5 and 6, respectively. The relative non-parametric quantities become simply now RMijand RIQRij(i=1, 2, …, m; j=1, 2, …, n). This rank-ordering is useful for three reasons: (1) it alleviates the perplexity of dealing with possibly multiple data distributions, (2) it facilitates concurrent handling of median and IQR estimator processing within a characteristic, and (3) it homogenizes the way that data processing will be incurred among characteristics in order to eventually propel an overall synchronous screening that takes in account the behavior from all characteristics. Therefore, the stage is set to accumulate all non-parametric median contributions from all characteristics to a ‘total median’ response. To achieve this, we adopt the mechanism for preparing the super-rank response for unreplicated-saturated multi-response profiling studies (Besseris, 2009c, 2010c). Before offering a proposal of how to apply this concept for the present situation, we introduce the added feature of our method that of allowing weights to be inserted among different responses. The role of weights is to place an emphasis on some of the examined responses over the rest, in case such information is available before the concurrent profiling commences. The weights are denoted as wi(i=1, 2, …, m) where in general the following relationship holds among them:∑i=1mwi2=1The process of aggregating the squared and weighted RMijand RIQRijquantities is shown in Figs. 7 and 8, respectively, leading correspondingly to the two totaled responses properly denoted as RMTiand RIQRTi(i=1, 2, …, n). Both of the last two responses have accumulated the overall weighted central tendency and variation contributions in distinct separate accounts embodying simultaneously the information contained by all characteristics. However, there is an exception with respect to the viability of the operations shown in Figs. 7 and 8 that ought to be discussed here. It might be occurred from the previous step (Figs. 5 and 6) that some of the m median and/or IQR responses have been eliminated from further consideration as redundant. Ostensibly, a complication arises if the count of the remaining median and IQR responses that will enter the summation operations (Figs. 7 and 8) is not equal. Eventually, the RMT and RIQRT response entries may not be represented comparably in terms of a common basis for magnitude because one of the two will contain a larger number of summed elements. To remedy this in our formalism, we only need to normalize the intermediate quantities that participate in the summing operation (Figs. 7 and 8) by dividing the total sums with their corresponding number of elements that have been partaking in the summation. It is pointed that if the number of components in the summation operation remains equal after the elimination round for both types of responses, then the transactions would not require the normalization step. Therefore, the aggregation operations demonstrated in Figs. 7 and 8 hold as they are. Nevertheless, this time the upper limit index in the summation should be a number less than m. The RMT and RIQRT are already cast in uniform scale-less quantities that may be visualized as the squared elements of two n-dimensional vectors. Therefore, the RMT and RIQRT vectors may be added now to create the total contribution derived from location and dispersion effects concurrently from all engaging characteristics. The new vector entity that emanates from the joining of the two contributions is symbolized as Vi(i=1, 2, …, n) and its relationship with the RMT and RIQRT vectors is displayed as a matter of clarity in Fig. 9. The V-vector receives one final rank ordering that aids in collapsing the statistical processing part of this technique to the equivalent of saturated-unreplicated problem encountered in the non-linear OA non-parametric treatment case (Besseris, 2012). This last re-rank transformation creates the master-response, MRi(i=1, 2, …, n) which is shown in Fig. 10. The essence of this multi-transformational procedure outlined in this section was to facilitate the profiling process for the non-linear multi-response multi-factorial OA data where repetitive experiments have been collected as presented in the arrangement of Fig. 1. It comes to light that the information content of Fig. 1 may be compressed finally down to form a master vector, MR, which permits a great simplification of the initial problem to the arrangement shown in Fig. 11.We may now envisage solving the replicated multi-response profiling by reverting to the ordinary non-linear non-parametric treatment for unreplicated-saturated OAs.Before providing a brief review of how to profile unreplicated-saturated OA schemes, we adapt the non-parametric analysis part to exemplify specifically the standard L9(34) OA. This OA choice is congruent to the size of the design necessitated for the exhibited case study undertaken in the next section. Hence, the generalized arrangement derived in Fig. 11 may now be explicitly systematized for the L9(34) OA in Fig. 12. In Fig. 12, the four considered factors have been labeled as A, B, C and D, while the numbering within each column reflects each of the three coded factor settings without loss of any generality (Taguchi et al., 2000).Based on the MR-vector, the effects will be rank-summed in order to estimate the P-statistic (Besseris, 2010d). The Pi-statistic (i=1–3) has been developed to be a versatile measure in gaging the statistical significance for one-factor, two-factor, and multi-factor performances. In order to accrue the uni-, bi- and multi-factor rates, it is necessary first to compute the sums of squared master rank-sums (SSMRS). To proceed with the evaluation of the SSMRS, we provide its generalized form first for each effect l (l=1, 2, …, k):(4)SSMRSl=∑j=13∑ijl=1n/3MRij2We elaborate in more detail the development of equation 4 by adapting it specifically to the experimental arrangement of interest in our showcased study in the next section. In this situation, the SSMRS will be expanded for the four coded factors as derived in Fig. 12. The four respective quantities are denoted as SSMRSA, SSMRSBSSMRSC and SSMRSD, which are calculated in terms of the master-rank entries that are simply associated with the corresponding factor settings:(5)SSMRSA=(MR1+MR2+MR3)2+(MR4+MR5+MR6)2+(MR7+MR8+MR9)2(6)SSMRSB=(MR1+MR4+MR7)2+(MR2+MR5+MR8)2+(MR3+MR6+MR9)2(7)SSMRSC=(MR1+MR6+MR8)2+(MR2+MR4+MR9)2+(MR3+MR5+MR7)2(8)SSMRSD=(MR1+MR5+MR9)2+(MR2+MR6+MR7)2+(MR3+MR4+MR8)2To test for single-effect dominance we proceed by setting-up the four possible versions:(9)P1=SSMRSA(10)P1=SSMRSB(11)P1=SSMRSC(12)P1=SSMRSDNext, the two-effect dominance testing requires checking the following six combinations:(13)P2=SSMRSA+SSMRSB(14)P2=SSMRSA+SSMRSC(15)P2=SSMRSA+SSMRSD(16)P2=SSMRSB+SSMRSC(17)P2=SSMRSB+SSMRSD(18)P2=SSMRSC+SSMRSDFinally, for the three-way contrasting, the four derived combinations are:(19)P3=SSMRSA+SSMRSB+SSMRSC(20)P3=SSMRSA+SSMRSC+SSMRSD(21)P3=SSMRSB+SSMRSC+SSMRSD(22)P3=SSMRSA+SSMRSB+SSMRSDThe critical values for the respective group of Pi-statistic estimations are easily retrievable from the corresponding reference distributions listed separately for each group in Table I of the referenced work in Besseris (2010d).The data manipulation procedure is progressed without any explicit assumptions on the distributional make-up of either the collected replicate data series or trial-run trends among the executed OA recipes. As such our technique is inherently deft in tracking and harmonizing data from multifarious origins without any knowledge about their stochastic fingerprinting. Moreover, since the contrasting scheme is not dependent on the variation amassed from the inert effects, hence our proposal is also calibration-free. This allows searching naturally for influence superiority without decoupling nonlinearity from the scheme. Our solver does not envisage the data conversion phase as a pure location contrasting process. Instead, its algorithm is made agile enough such that not to preclude dispersion information from entering the statistical engine. No specific assumptions are declared for the potential noise intrusions in the replicate observations. The technique should remain operable even if undetected oddities have found a way to invade the recorded measurements. Any deviant contributions are automatically dealt with the innately deployed robust estimators that promptly shun irregularities as outliers or extreme points.

@&#CONCLUSIONS@&#
