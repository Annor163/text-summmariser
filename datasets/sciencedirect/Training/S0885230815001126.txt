@&#MAIN-TITLE@&#
Concept-to-Speech generation with knowledge sharing for acoustic modelling and utterance filtering

@&#HIGHLIGHTS@&#
We present two knowledge sharing approaches for CTS generation.Syntactic features replace prosody phrasing features in HMM-based speech synthesis.Acoustic features are used to filter synthetic utterances for one input concept.The HMM-based acoustic model yields comparable results without prosodic phrasing.Utterance filtering can remove inferior synthetic utterances for the input concept.

@&#KEYPHRASES@&#
Concept-to-Speech,Speech synthesis,Hidden Markov model,Natural language generation,

@&#ABSTRACT@&#
A Concept-to-Speech (CTS) system converts the conceptual representation of a sentence-to-be-spoken into speech. While some CTS systems consist of independently built text generation and Text-to-Speech (TTS) modules, the majority of the existing CTS systems enhance the connection between these two modules with a prosodic prediction module that utilizes linguistic knowledge from the text generator to predict prosodic features for TTS generation. However, knowledge embodied within the individual modules has the potential to be shared in more ways. This paper describes knowledge sharing for acoustic modelling and utterance filtering in a Mandarin CTS system. First, syntactic information generated by the text generator is propagated to a hidden Markov model (HMM) based acoustic model within the TTS module and replaces the symbolic prosodic phrasing features therein. Our experimental results show that this approach alleviates the local hard-decision problem in automatic prosodic phrasing for Mandarin CTS systems and achieves a comparable performance to the traditional approach without explicit prosodic phrasing. Second, the acoustic features of multiple synthetic utterances expressing the same input concept are utilized to evaluate the utterance candidates. With this ‘post-processing’ mechanism, our CTS system is able to filter out inferior synthetic utterances and find an acceptable candidate to express the input concept.

@&#INTRODUCTION@&#
The transfer of human language proficiency to machines has been explored for decades. One branch of exploration, the Concept-to-Speech (CTS) approach, endeavors to enable machines to produce speech based on abstract representations of the sentence to be spoken. Because humans articulate their ideas by translating them into syntactic, phonological and phonetic codes that guide the vocal tract articulator to produce acoustic waveforms, a similar process is to some extent adopted by CTS systems wherein the input concept undergoes syntactic and phonological processing and eventually drives the ‘articulator’ to synthesize speech. Another type of human language proficiency, reading text out loud, has also been transferred to machines. This technique is known as Text-to-Speech (TTS) synthesis. One obvious difference between CTS and TTS is that the CTS approach must convert conceptual representations into sentences before articulating them while TTS directly accepts a concrete sentence as input. Because conceptual representation is typically domain dependent, CTS is incorporated into a spoken dialogue system wherein conceptual representations can be defined and provided by front-end modules. For example, well-known CTS systems have been deployed in an inquiry system for a water-supply network (Young and Fallside, 1979) and a multimedia medical briefing system (McKeown and Pan, 1997).Although CTS may appear to be more complex than TTS due to the additional task of sentence composition, CTS can be practically implemented by concatenating a text generator with a TTS module, as shown in Fig. 1(a). This solution has been adopted by several dialogue systems that generate spoken responses (Litman et al., 1998; Rudnicky et al., 1999; Walker, 2000). However, this solution abandons the valuable linguistic knowledge that is available in the text generator, which cannot be fully retrieved by the error-prone text-analyzer in the TTS module.Finding improvements to knowledge sharing amongst the components of a CTS system is a vital issue in CTS research (Young and Fallside, 1979). The consensus among researchers seems to be that linguistic information such as discourse, semantic and syntactic knowledge permits finer control of prosody modelling in CTS systems. Existing CTS systems based on this idea typically incorporate external prosodic prediction modules to convert linguistic information into prosodic symbols. This idea is illustrated in Fig. 1(b). For example, the system in Pan (2002) utilizes various types of linguistic information and an instance-based learning algorithm to predict ToBI symbols (Silverman et al., 1992) for CTS generation in the medical briefing domain. Other CTS systems apply similar ideas, and their performance confirms that predicting prosodic symbols based on the knowledge from text generation is effective for CTS tasks (Danlos et al., 1986; Theune et al., 1997; Dorffner et al., 1990; Nakatani and Chu-Carroll, 2000; Fawcett, 1990; Teich et al., 1997; Takada et al., 2007; Schnell and Hoffmann, 2004; Hitzeman et al., 1998; Pan, 2002).Although the above strategy is informative, we wonder whether it represents the best way to address the CTS task, particularly for Mandarin. Initially, the pipeline structure in Fig. 1(b) only shares the linguistic information from text generation with the prosodic model or a prosodic phrasing model for Mandarin. We think that propagating the linguistic information as well as the prosodic features to the speech synthesizer is a worthy pursuit (Badino et al., 2012). However, because automatic prosodic phrasing for Mandarin is challenging, we doubt that the improvements produced by linguistic information in both automatic prosodic phrasing and acoustic modelling can prevent the unacceptable synthetic speech after an erroneous prosodic phrase boundary is predicted by the automatic prosodic phrasing component in the CTS pipeline. Thus, for our Mandarin CTS system, we replace the prosodic phrasing features with linguistic features given by text generation. These linguistic features are directly utilized to cluster the acoustic models for spectra, F0 and duration features without using prosodic phrasing features. Although these linguistic features are more ambiguous for predicting acoustic features than the hand-annotated prosodic features, experiments will demonstrate that the loss in the accuracy of the prediction is more acceptable than the degradation caused by the erroneously predicted prosodic features in the traditional approach. Additionally, our approach avoids the cost of annotating prosodic boundaries and building a prosodic phrasing module for the Mandarin CTS system.Another imperfection of the existing CTS systems is their pipeline structure wherein the text generator makes decisions regarding text planning and sentence realization without any consideration of the capability of the speech waveform generator. Due to the inevitable sparse distribution of acoustic data, the speech generator may fail to articulate certain words or accomplish the appropriate intonation in certain contexts. One solution is to back propagate the acoustic information to help the text generator avoid these pitfalls during sentence composition. However, acoustic information cannot be collected before the sentence is generated, or at least partially processed, by the speech generator. To address this ‘surface problem’ (Inui et al., 1992), we propose utterance filtering for the CTS generation: first, several synthetic utterances that express the same input concept are generated by the text generator and speech synthesizer in the CTS system; second, the candidates’ acoustic information is collected and propagated to the utterance filter in the CTS pipeline; lastly, synthetic utterance candidates with imperfect acoustic quality can be removed and a more suitable candidate can be used to reconstruct the output speech waveform. In this manner, the quality of the output speech is expected to be improved over conventional CTS systems.The structure of the proposed CTS method is shown in Fig. 1(c). Because this paper primarily addresses Mandarin CTS synthesis, Section 2 will introduce the baseline Mandarin CTS method, which incorporates text generation based on natural language generation (NLG), automatic prosodic phrasing and HMM-based parametric speech synthesis for Mandarin. In Section 3, the shortcomings of the baseline CTS method will be discussed. Next, the proposed method of knowledge sharing in acoustic modelling and utterance filtering will be introduced. Section 4 will detail the experiments that were performed on both the baseline and proposed methods. Section 5 presents the conclusions of the study.Although the proposed methods do not require domain-specific knowledge, the experiments in this paper are conducted on a CTS system built in a specific domain using these methods. After all, CTS systems are domain-specific as aforementioned. But the same methods can be used to construct CTS systems for other domains. If the domain-specific CTS system based on the proposed methods has to be utilized in another domain, additional ‘domain adaptation’ techniques such as the one proposed by Tsiakoulis et al. (2014) should be utilized. This topic is not covered in this paper.This paper focuses on Mandarin CTS systems. The baseline system follows the structure in Fig. 1(b) and will be introduced in this section; Section 2.1 will address NLG based on Komet-Peman MultiLingual (KPML) for Mandarin; Section 2.2 will discuss prosodic phrasing for Mandarin; Section 2.3 will discuss HMM-based parametric speech synthesis. Note that the domain-dependent aspects of CTS systems, primarily in NLG, will not be discussed in this paper.Rather than using a template-based text generator, the baseline system utilizes a technique from NLG to convert the input abstract representation of the concept into text. The typical structure of an NLG module consists of a text planner, sentence planner and surface realizer (Reiter and Dale, 2000). While the first two components organize the concept and prepare for composing the sentences in a domain-dependent way, the surface realizer converts the text specifications given by the front-end into a string of surface words without using domain-dependent knowledge. Because this paper focuses on the quality of the speech generated by CTS systems, the baseline and proposed system are initiated with a surface realizer and the input to the surface realizer is assumed to be the input to the entire CTS system.The NLG module in our Mandarin CTS system uses KPML (Bateman, 1996), a surface realizer based on systemic functional grammar (SFG) (Halliday and Matthiessen, 2004). According to SFG, language provides the resources for us to create meaning, represent knowledge and interact with one another. For example, we interact with other people via strategies such as asserting, doubting, etc. because our language provides us with the resources for expressing these speech functions. As one component of language, grammar provides the resources for creating meaning in the form of grammatical sentences (Matthiessen and Halliday, 2009). For example, if we assert or describe something in English, the grammar defines that we must compose the sentence with a subject. We generally choose different language resources to express the intended meaning and compose sentences following the grammatical constraints defined by the grammar. For the grammars defined by SFG, these resources are organized as choices. If we suppose that clauses can be either indicative (e.g., describing) or imperative (e.g., requesting), the grammar provides us with these two choices, as shown in Fig. 2(a), wherein one clause must be either indicative or imperative; if it is indicative, it must contain a subjective and finite element according to the grammatical constraints +Subject (insert a subject) and +Finite (insert a finite element). In SFG, each point of choice defines a System.11We use System in Serif font to refer to the grammar system in SFG.Each System reflects a minimal grammatical difference (Bateman, 1997). It also describes what choice a particular linguistic unit can choose, what the grammatical constraint for that choice is and when that choice should be chosen.Fig. 2(a) defines only one System. In English, an indicative clause can be either ‘declarative’ (they ran away.) or ‘interrogative’ (did they run away?). Thus, another System for the two choices will follow the indicative path shown in Fig. 2(a). Similarly, more delicate Systems follow the more general Systems. This interrelated set of Systems is called a System Network. These principles of SFG are implemented by KPML. In KPML, the System Network is manually encoded as expert knowledge that will be called for text generation. Note that the condition for choosing either term is not shown in Fig. 2(a).The input to KPML, which contains the semantic or other information for one potential clause, will guide KPML to make choices in the System Network. The input to KPML can be encoded in Sentence Plan Language (SPL) (Kasper, 1989). The SPL structure for the sentence ‘That balloon is rising’ is shown in Fig. 2(b). In this SPL, RISE, BALLOON and NOW specify the semantic content of the clause while THAT directly specifies the grammatical determiner that must be included in the clause. Initially, KPML begins with clause as the entry condition and then walks forward in the System Network under the guidance of SPL. For example, :tense NOW will guide KPML to choose the present tense in grammatical tense Systems. Although this SPL structure does not specify the choice in MOOD system explicitly, KPML will choose Indicative by default. In this paper, this mechanism is referred to as the default choice mechanism. When KPML reaches the end of the System Network, it can collect grammatical constraints and construct the surface text accordingly. In other words, KPML takes SPL as input and converts it into a surface sentence based on the grammatical constraints acquired from the System Network. The System Network is a language-specific resource that must be loaded prior to text generation. Due to space limitations, the lexical realization of concepts and other aspects of KPML cannot be introduced in this paper.To utilize KPML for Mandarin, we manually augmented a System Network based on the work in Yang and Bateman (2009). Besides, the lexicon of KPML is augmented with phonemic symbols so that the initial/final and tone sequences of a Mandarin sentence can be directly generated from KPML. Thus, our baseline Mandarin CTS system does not contain a text-analyzer.As introduced in Section 1, several existing CTS systems use manually compiled rules or statistical tools to predict prosodic symbols based on linguistic information from the text generator. In these CTS systems, typical prosodic symbols include the pitch accent and boundary tone defined in ToBI (Silverman et al., 1992). After being predicted by the prosodic module, these prosodic symbols are realized as the fundamental frequency (F0) or other acoustic parameters by the TTS modules of these CTS systems.Our baseline CTS system adopts an identical method for prosodic modelling; however, although linguists suggest that Mandarin prosody includes stress structure, pitch variation and prosodic structure (or rhythmic structure, prosodic hierarchy) (Li, 2002), the Mandarin CTS system described in this paper only covers prosodic structure, which is similar to typical Mandarin TTS systems. The consensus among researchers of Mandarin prosody is that one of the most pronounced features of Mandarin speech flow is the chunking and grouping of smaller prosodic units into larger units. At the bottom level, Mandarin syllables generally form polysyllabic chunks or prosodic words (PWs). Next, these chunks further establish the prosodic phrase (PP) and intonation phrase (IP) (Li, 2002; Tseng and Pin, 2004; Cao, 2000). An example of this prosodic structure is shown near the bottom of Fig. 3. Note that this prosodic structure assumes three tiers: the clause, prosodic phrases and prosodic words. Given that the prosodic structure has a fixed number of levels and each level has its own prosodic unit type, the prosodic structure can be equivalently represented by a sequence of the prosodic boundaries between every pair of adjacent syllables in the utterance. For example, the first three prosodic boundaries for the utterance in Fig. 3 are PW boundary, PW boundary and the boundary within PW.Prosodic phrasing, or predicting the prosodic structure, is essential for Mandarin speech synthesis because prosodic structure correlates not only with pitch variation but also with the pre-boundary lengthening in the utterance (Tseng and Tseng, 2002; Cao, 2000). Specifically, the PP boundary signifies the location of a silent pause within a Mandarin utterance. An incorrectly located PW or PP boundary may result in an unnatural synthetic utterance. More fundamentally, because the prosodic structure influences the listener's perception of the meaning of the synthetic utterance (Cao, 2000), an inappropriate prosodic boundary may make the utterance obscure.Researchers have proposed several methods for automatic prosodic phrasing in Mandarin speech synthesis. The studies by Cao and Zhu (2002) and Che et al. (2014) confirm that syntactic information can be used as input features for prosodic phrasing. Therefore, we utilize the syntactic information provided by KPML to predict prosodic boundary sequences in our baseline Mandarin CTS system. The syntactic information provided by KPML will be discussed in Section 3.1.2. In the baseline system, the sequence of prosodic boundaries is converted into the prosodic phrasing features listed in Section 2.3. These features are then added to contexts of the HMM-based acoustic model. The HMM-based parametric speech synthesizer then generates the acoustic parameters such as the F0 contour. Note that the prosodic phrase (PP) boundary explicitly specifies the location of a silent pause in the utterance. The HMM model for a silent pause is always inserted after a PP boundary so that a perceptible break can be generated by the HMM-based speech synthesizer.The TTS module of our CTS system adopts an HMM-based parametric speech synthesis framework (HTS) (Zen et al., 2009) due to its flexibility in incorporating various contextual features and relatively low requirements on the size of the training data corpus. The core of HTS consists of first modelling the distribution of the acoustic featuresOgiven the corresponding contextsWand then predicting the acoustic featuresofor the target contextw. These two steps are shown in (1) and (2) (Zen et al., 2009),(1)λˆ=argmaxλp(O|W;λ),(2)oˆ=argmaxop(o|w;λˆ).whereλis the model parameter set.For Mandarin speech synthesis, the basic units of the HMM-based acoustic model are the initial/final sounds of a Mandarin syllable (Sun, 2006). For simplicity, we will refer to these initials and finals sounds as Mandarin phones. For every acoustic model, the contextual featuresWinclude:•Segmental features:–The identities of the current and surrounding Mandarin phones;The tones of the current and surrounding Mandarin syllables;Prosodic phrasing features:–The number of syllables in the current prosodic word (PW), the number of PWs in current prosodic phrase (PP) and the number of PPs in current utterance;The distance between the current syllable and the head of the current PW, PP and utterance;The distance between the current syllable and the tail of the current PW, PP and utterance;The type of the prosodic boundary between the current and adjacent syllables.In the baseline system, the Gaussian distribution is used to describe the duration of the Mandarin phones accompanied by model contextW, and another set of multi-variate Gaussian distributions is used to model the durations of the HMM states of every phone. Before training the two sets of duration models, the duration of every phone and its associated HMM states can be acquired through force-alignment of the training data. These two sets of context-dependent Gaussian distributions can then be trained. During the synthesis phase, the duration of the HMM states is predicted by maximizing the joint probability of the duration of the phone and its HMM states (Wu and Wang, 2006). Note that the silent pause following a PP boundary is treated as a phonemic unit, and the duration of the silent pause is modelled in the same manner as the other phonemic units.The baseline Mandarin CTS system adopts the structure shown in Fig. 1(b). Although this structure is widely accepted in the CTS community, we wonder if the external prosodic phrasing model for Mandarin CTS might make full use of the linguistic knowledge from the text generator. What is more, we think the acoustic information offered by the acoustic model could also help formulate the content of the synthetic utterance. Thus, we propose a CTS method in which linguistic and acoustic knowledge can be propagated to downstream components so that linguistic and acoustic knowledge can be exploited by the system. This CTS method is illustrated in Fig. 4. The use of linguistic knowledge in acoustic modelling and acoustic knowledge in sentence content planning via utterance filtering will be discussed in Sections 3.1 and 3.2 respectively.

@&#CONCLUSIONS@&#
