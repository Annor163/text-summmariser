@&#MAIN-TITLE@&#
HMM-based expressive singing voice synthesis with singing style control and robust pitch modeling

@&#HIGHLIGHTS@&#
We propose singing style control for HMM-based expressive singing voice synthesis.Feature-space pitch adaptive training is proposed for robust pitch modeling.Robust vibrato modeling is proposed for unclear vibrato expressions.Experiments show the techniques improve the naturalness and similarity.Experiments show we can control singing style expressivity intuitively

@&#KEYPHRASES@&#
HMM-based singing voice synthesis,Singing style control,Multiple-regression HSMM,Pitch adaptive training,Vibrato modeling,

@&#ABSTRACT@&#
This paper proposes a singing style control technique based on multiple regression hidden semi-Markov models (MRHSMMs) for changing singing styles and their intensities appearing in synthetic singing voices. In the proposed technique, singing styles and their intensities are represented by low-dimensional vectors called style vectors and are modeled in accordance with the assumption that mean parameters of acoustic models are given as multiple regressions of the style vectors. In the synthesis process, we can weaken or emphasize the intensities of singing styles by setting a desired style vector. In addition, the idea of pitch adaptive training is extended to the case of the MRHSMM to improve the modeling accuracy of pitch associated with musical notes. A novel vibrato modeling technique is also presented to extract vibrato parameters from singing voices that sometimes have unclear vibrato expressions. Subjective evaluations show that we can intuitively control singing styles and their intensities while maintaining the naturalness of synthetic singing voices comparable to the conventional HSMM-based singing voice synthesis.

@&#INTRODUCTION@&#
Speech synthesis is the key technologies for human computer interaction (HCI) systems, and the interactive robot is one of the most typical and important applications to be realized in HCI systems. Recently, a humanoid robot named HRP-4C (Kaneko et al., 2009) was developed whose appearance is quite close to that of a human (Nakaoka et al., 2009). For such a state-of-the-art interactive robot, more advanced speech synthesis with rich para-linguistic and non-linguistic information, e.g., affections, emotions, speaking styles, and speaker characteristics, is indispensable. In addition, a function of synthesizing not only speech but also singing voice is desirable to achieve HCI systems that is capable of making the speech communication more diverse and rich like a human. This is because in our daily life there are a variety of music pieces including singing voices that are capable of relaxing or exciting us and some people communicate their feeling to others by singing. If an interactive robot has a function of singing voice synthesis with various singing styles, the application of the robot will expand not only to home entertainment but also to business showcase, exhibition, musical concert, and so on. Also in the education area, such an advanced and sophisticated interactive robot will give a good impact at the music class. These applications have a possibility of providing a new communication/interaction style between a human and a robot to our future life.Singing voice synthesis is becoming an attractive application for speech synthesis in these days, and several products such as VOCALOID (Kenmochi and Ohshita, 2007) have become popular in the entertainment industry, especially in Japan. In the singing voice synthesis, users can easily create singing voices of certain singers or characters by inputting arbitrary musical notes (or MIDI codes) and lyrics, and this provides composers with an assistance method for adding original singing voices to their compositions. Recently, singing voice synthesizers have been utilized not only for hobby use but also for professional music production, a singing robot, karaoke, and live music, which shows the potential capability for entertainment and amusement applications (Tachibana et al., 2010; Kenmochi, 2012).To develop a singing voice synthesis system, various approaches have been proposed (Cook, 1996; Rodet, 2002). The techniques based on speech production models, e.g., (Cook, 1993), and formant synthesis, e.g., (Sundberg, 2006), have an advantage that their model parameters have physical meanings and hence we are able to modify the synthetic singing voice by carefully controlling the parameters. However, the synthesis performance highly depends on the target voice, and the quality of output voice is not always satisfactory, which is the main drawback in singing voice synthesis. In terms of spectral reproducibility, concatenative synthesis, e.g, (Macon et al., 1997; Bonada et al., 2003; Kenmochi and Ohshita, 2007), outperforms the above techniques because recorded singing samples, such as diphones and sustained vowels, are directly used without physical modeling. A limitation of the concatenative synthesis is that singing samples, which are used as synthesis units, are recorded separately whereas we sing a song in a continuous manner beyond phonemes, syllables, and words. As a result, it is difficult to model the prosodic characteristics of singers and singing styles, and rule-based prosody generation is generally used. However, this heuristic approach is not always sufficient to synthesize singing voices with a wide variety of singing styles of various singers.Singing voice synthesis based on hidden Markov models (HMMs) (Sako et al., 2004; Saino et al., 2006) is an alternative approach that enables simultaneous modeling of spectral and prosodic characteristics of singers and singing styles from a continuous singing voice corpus. The basic framework of the HMM-based singing voice synthesis is the same as that of HMM-based speech synthesis (Yoshimura et al., 1999). Although the baseline quality of the HMM-based singing voice synthesis has been steadily improved by introducing several techniques such as time-lag modeling (Saino et al., 2006), frame-based vibrato modeling (Oura et al., 2010), and pitch adaptive training (Oura et al., 2012), studies for diversifying singing voices are rather limited (Saino et al., 2010). By contrast, a variety of techniques, e.g., speaker adaptation (Tamura et al., 2001), style interpolation (Tachibana et al., 2005), and style control (Nose et al., 2007), have been proposed in HMM-based speech synthesis research area for adding or controlling various speaker and style characteristics (Yamagishi et al., 2009; Nose and Kobayashi, 2011). However, few studies have so far applied these techniques to singing voice synthesis (Sung et al., 2011).In this study, we apply the style control technique of synthetic speech (Nose et al., 2007) to the HMM-based singing voice synthesis, which enables users to change the singing style expressivity in an intuitive and continuous manner.11Part of this work was presented at INTERSPEECH 2013 (Nose et al., 2013).In the proposed technique, multiple singing styles and their expressivity are represented by a low dimensional vector named a style vector and are simultaneously modeled using multiple-regression hidden semi-Markov models (MRHSMMs) (Niwase et al., 2005). The style vector is used as a explanatory variable of the MRHSMM where the mean parameter of each probability density function (pdf) is assumed to be given by a multiple regression of the style vector. In the model training, the parameters of MRHSMMs are estimated with training songs and the corresponding style vectors using maximum likelihood estimation with the EM algorithm. In the singing voice synthesis, we can control the singing style expressivity by changing the style vector.To improve the modeling accuracy of pitch in the case of a limited amount of training data, we extend the model-space pitch adaptive training (Oura et al., 2012) into the feature-space one for the MRHSMM. In the proposed modeling, the observation features of static log fundamental frequency (F0) values are normalized using the pitch of the corresponding note in the parameter re-estimation process by the EM algorithm. By using pitch adaptive training, we can generate better F0 trajectories that closely follow the original pitch of given notes. In the singing voice synthesis, the vibrato modeling is also important for natural sounding synthetic singing voices (Maher and Beauchamp, 1990). However, the singing voices sometimes have unclear vibrato expressions, making conventional vibrato modeling techniques such as (Oura et al., 2010) unsuitable. To alleviate this problem, we also propose a technique for the robust vibrato parameter extraction using zero-crossing and energy with a moving average filter. Through subjective evaluation tests, we show that the intuitive singing style control is well achieved while maintaining the naturalness of the synthetic voices.We first briefly outline the HMM-based singing voice synthesis that is the basis of this study. Most processes of model training and parameter generation are the same as those of the HMM-based speech synthesis. Specifically, we record singing voices of multiple songs as training data in advance. From the musical scores of these songs, the phonemes and their contextual information, e.g., pitch, duration, and position of preceding/current/succeeding note, are extracted and converted into context-dependent labels. Context-dependent HMMs are then trained from the training data using the EM algorithm and parameter tying by tree-based context clustering (Young et al., 1994). The spectral and F0 features are simultaneously modeled using multi-space probability distribution HMMs (Tokuda et al., 1999). To model the state duration of HMM appropriately, a hidden semi-Markov model (HSMM) (Zen et al., 2007) is generally used that has an explicit duration distribution in each state.In the synthesis phase, the input musical score is converted into context-dependent labels and spectral and prosodic feature trajectories are estimated from the corresponding context-dependent HMM sequence using the parameter generation algorithm (Tokuda et al., 1995). Finally, the singing voice waveform is synthesized from the generated spectral and prosodic parameters using vocoding methods such as cepstral vocoding with MLSA filter (Imai, 1983) and STRAIGHT (Kawahara et al., 1999).To model multiple singing styles and their intensities, we use multiple-regression HSMM (MRHSMM) (Nose et al., 2007). In the MRHSMM, mean parameters,μiand mi, of output and state-duration probability density functions (pdfs) of the i-th state are assumed to be given by a multiple regression of a low dimensional vector, i.e., a style vector,vas(1)μi=Hbiξ(2)mi=Hpiξ(3)ξ=[1,v1,v2,…,vL]⊤(4)=[1,v⊤]⊤where L is the dimension of the style vector, andvnis the intensity/expressivity of the n-th singing style.HbiandHpiare M×(L+1) and 1×(L+1) regression matrices, respectively, and M is the dimension of the feature vector. The space where the style vector is defined is called style space, and the style space is determined by the style vectors associated with training data. Fig. 1shows an example of one- and three-dimensional style spaces.The pdfs at state i are thus expressed as(5)bi(o)=N(o;Hbiξ,Σi)(6)pi(d)=N(d;Hpiξ,σi2)whereoandΣiare the observation vector and covariance matrix of the output pdf, respectively, and d andσi2are the state duration and variance of state-duration pdf, respectively.In the model training, different style vectors are set to the training singing voices in accordance with the singing style expressivity appearing in each voice. The optimum model parameter setλ* including regression matrices is estimated using the maximum likelihood criterion as follows:(7)λ*=argmaxλ∏k=1KP(o(k)|λ,v(k))whereo(k) andv(k)are the k-th singing voice and the corresponding style vector. We use style-independent variance parameters,Σiandσi2, estimated using all training data in the training of MRHSMMs, which is the same condition as our previous study (Nose et al., 2007). Details of the estimation formulas for regression matrices and variance parameters can be found in (Nose et al., 2007).Fig. 2shows the outline of the process of synthesizing a singing voice.In the synthesis phase, users give a style vector that corresponds to their intended singing style expressivity in the style space. Mean parameters of output and state-duration pdfs are calculated from the given style vector and regression matrices of trained MRHSMMs using Eqs. (1) and (2). As a result, an ordinary HSMM sequence is obtained, and singing voice parameters are then generated from the HSMM sequence using the speech parameter generation algorithm as described in Section 2. Users can continuously control the singing style and its expressivity appearing in the synthesized singing voice by changing the input style vector since the style vector can have continuous values.As described in Section 3, singing style expressivities can be expected to be controlled by using MRHSMMs when sufficient training data is available. However, the reproducibility of each acoustic feature strongly depends on the training data because the HMM-based singing voice synthesis is a corpus-based approach. As for the pitch feature, which is one of the most important features in singing voice synthesis, it is difficult to generate a desirable F0 contour that closely follows the notes when the pitch contexts of the training data have poor coverage. This is also a crucial problem in the singing style control. In the conventional MRHSMM-based style control of synthetic speech (Nose et al., 2007), there is no framework for avoiding the coverage problem of pitch contexts, and hence the robust pitch modeling technique is strongly required in the case of singing voice synthesis. For this purpose, we here propose a pitch modeling technique by extending the pitch adaptive training (Oura et al., 2012) into the training of MRHSMMs in a feature space.In the conventional pitch adaptive training of HMM/HSMM (Oura et al., 2012), the mean parameter μiof the pdf for the static feature of log F0 in the i-th state is given by a shift from an F0-normalized mean parameterμˆias(8)μi=μˆi+ciwhere ciis the log F0 value of the note corresponding to the i-th state. In this case,μˆirepresents not an absolute but a relative log F0 mean based on the note information, and Eq.(8) can be viewed as model-space normalization of the static log F0 feature. To simplify the extension of the pitch adaptive training to MRHSMM, instead we use the following feature-space normalization that is equivalent to Eq.(8) as(9)oˆt=ot−ciwhere otandoˆtare log F0 observations of voiced frames at time t before and after the F0 normalization, respectively. Note that the variance parameters do not vary since the transformation is only the shifting. Given the note sequencec(k) of the k-th singing voice, the optimum model parameter setλ* after the F0 normalization is estimated as follows:(10)λ*=argmaxλ∏k=1KP(o(k)|λ,v(k),c(k)).The estimation formula of regression matricesHˆbiat the i-th state for the static feature of the normalized log F0 is obtained using Eq.(9) as follows:(11)Hˆbi=∑k=1K∑t=1T(k)∑d=1tγtd(i)∑τ=t−d+1t(oτ(k)−ci)ξ(k)⊤·∑k=1K∑t=1T(k)∑d=1tγtd(i)·d·ξ(k)ξ(k)⊤−1whereoτ(k)is the log F0 value of the k-th observation sequenceo(k) at time t, and T(k) is the number of frames ofo(k).γtd(i)is the probability of being in state i at a period of time from t−d+1 to t giveno(k).v(k)is the style vector corresponding too(k), andξ(k)=[1,v(k)⊤]⊤. Compared to the case of the conventional model-space approach, the derivation of the pitch adaptive training for MRHSMM can be very simple by using the feature-space approach. Note that this approach is different from the data-level F0 normalization (Saino et al., 2010) because the normalization is conducted not in the data preparation phase but within the parameter estimation process. In the data-level F0 normalization, the alignment between feature vectors and musical notes are fixed in advance whereas the alignment is not fixed in the forward-backward step of the EM algorithm.In the parameter generation, first the normalized mean parameter of each state is calculated using Eq.(1). Then, the normalized mean parameter is transformed into the mean parameter of each state using the input note and Eq.(8). Finally, a log F0 contour is generated from the pdf sequence in an ordinary manner of the HMM-based singing voice synthesis.Another issue with pitch modeling in singing voice synthesis is a vibrato modeling problem. Vibrato is a kind of singing expression mainly consisting of a regular and pulsating change of pitch and is typically represented by a periodic F0 variation in singing voice synthesis. When the vibrato is not explicitly modeled in the HMM-based singing voice synthesis, the periodicity and phase characteristics of the vibrato can not be taken into account in the F0 model training, and the resultant F0 contour is highly flattened in long-tone segments and vibrato expression in the original singing voice is not reproduced. In this paper, we define segment as a phone unit. When the segment is a vowel and have long tone, the segment is called long-tone segment. To overcome this problem, a frame-based vibrato modeling technique was proposed (Oura et al., 2010). In this technique, the F0 contour is assumed to be a periodically time-variant sequence for vibrato segments, and vibrato parameters are calculated from the amplitude and the interval between two peaks of the sinusoidal F0 contour. The parameters are extended to frame-level features using interpolation. These features are added to the observation vectors and are modeled simultaneously by context-dependent HMMs.The above frame-based vibrato modeling technique is effective when the singer is professional and the vibrato expression is very clear. However, the vibrato expression highly depends on a target singer or a target singing style, and the vibrato expression is not always very clear.In Fig. 3(a) and (b) are F0 contours extracted from the same part of the same song used in (Oura et al., 2010) and from an adult-like style song used in the experiments in Section 5, respectively. Note that (a) and (b) were sung in different singing styles, i.e., classical and pop, respectively, by different singers. For comparison, a synthetic F0 contour generated using the training data of the adult-like style without vibrato modeling is also shown in (c). In the F0 contour of (a), we can see a clear vibrato expression in the long-tone segments. By contrast, the amplitude is much smaller for the vibrations of (b) than for those of (a), and the vibrato periodicity is not always clear. Other detailed samples of unclear vibrato in the different long-tone segments are shown in Fig. 4. It is apparent that the clear periodic F0 vibration can not be found in these samples and reliable vibrato parameters is difficult to be extracted using the conventional approach. When comparing (b) and (c), the F0 contour of the long-tone segments is highly flattened in the synthetic voice and the fluctuation appearing in the original voice has disappeared, which degrades the naturalness of the synthetic singing voice. Therefore, a vibrato modeling technique applicable to such an unclear vibrato expression is important to synthesize songs with various speakers and singing styles.We propose a vibrato modeling technique that can be used even when the vibrato expression is not very clear. From a preliminary analysis, we defined the long-tone segment as a segment that has a single vowel and its duration is more than 600ms. In each long-tone segment, the vibrato is modeled by a sine wave in which the fundamental period and the amplitude were assumed to be constant. The vibrato parameter vector consists of a vibrato rate parameter r in Hz and a vibrato amplitude parameter a in cent of a vibrato function g(t) for log F0. g(t) is given by(12)g(t)=a·C·sin(r·t)where C=log2/1200 is a constant for scaling between the values of log F0 and F0 in cent. In the actual log F0 contour f(t) of a long tone, the vibrato appears around an underlying gradual F0 movement. To obtain the vibrato patternfˆ(t)where the gradual movement is removed, we apply a moving average filter to f(t) and subtract the output from f(t). Then, the vibrato rate parameter is calculated from the zero-crossing rate nzoffˆ(t)as(13)r=πnzwhere the zero-crossing rates offˆ(t)and g(t) were assumed to be equal. In this study, we restrict the range of vibrato frequency from 5 to 8Hz on the basis of the previous study (Oura et al., 2010). By using a similar manner, the vibrato amplitude parameter a is given by(14)a=∑t=0T−1f(Tst)2∑t=0T−1sin2(πnzTst)where the energy between long-tone segments offˆ(t)and g(t) were assumed to be equal. T and Ts[s] are the length of the long tone and the frame period, respectively. Although a vibrato amplitude was restricted from 30 to 150 cent in (Oura et al., 2010), we set the lower limit to zero to take into account a smaller vibrato expression. Note that the vibrato parameters are not determined and not modeled in the not-long-tone segments in this study.A single set of vibrato rate and amplitude parameters is determined for a long-tone segment. We construct two-dimensional vibrato feature vectors by combining the rate and amplitude parameters. In the model training, vibrato parameter vectors are modeled using context-dependent single Gaussian pdfs. In the synthesis stage, first a log F0 contour is generated from HSMMs and long-tone segments are determined in accordance with phone durations. For the long-tone segments, vibrato is added by calculating a superposition of the generated log F0 contour and the vibrato function g(t) determined by the vibrato parameters. We use linear interpolation so that the resultant log F0 sequence smoothly changes in the first and last 50ms.

@&#CONCLUSIONS@&#
In this paper, we proposed a novel singing voice synthesis technique that enables users to control singing styles intuitively and continuously. The technique is based on MRHSMM, and feature-space pitch adaptive training was proposed for the MRHSMM training to model pitch precisely in the singing style control. We also proposed a robust vibrato modeling technique that can be used even for singing voices with unclear vibrato expressions. The subjective evaluation results showed that the naturalness of the synthetic singing voices with representative styles were comparable between the conventional style-dependent HSMM and the proposed MRHSMM. The proposed pitch adaptive training in the feature space can be applied easily to the MRHSMM and had the similar performance to the conventional pitch adaptive training in the model space for the HSMM. In addition, the proposed segment-based vibrato modeling was shown to be more effective on naturalness than the conventional frame-based vibrato modeling. Finally, we showed that users can control the style expressivity of synthetic singing voices with acceptable quality by changing the input style vector. Future work will focus on modeling and controlling local singing style variations that often appear in singing voices of professional singers. The proposed technique also needs to be evaluated with a larger amount of singing voice data including more singing style variations with multiple dimensional style vectors.