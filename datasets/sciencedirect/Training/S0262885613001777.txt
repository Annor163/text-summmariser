@&#MAIN-TITLE@&#
A talking profile to distinguish identical twins

@&#HIGHLIGHTS@&#
We prove that twins look alike, but they behave differently.A talking profile consisting of usual facial motions is proposed as a biometric.Our model counts more on abnormal action and gains the superior performance.Our experiments show that the talking profile is robust to some aging effect.

@&#KEYPHRASES@&#
Talking profile,Identical twins,Abnormal motions,EMRM,

@&#ABSTRACT@&#
Identical twins pose a great challenge to face recognition due to high similarities in their appearances. Motivated by the psychological findings that facial motion contains identity signatures and the observation that twins may look alike but behave differently, we develop a talking profile to use the identity signatures in the facial motion to distinguish between identical twins. The talking profile for a subject is defined as a collection of multiple types of usual face motions from the video. Given two talking profiles, we compute the similarities of the same type of face motion in both profiles and then perform the classification based on those similarities. To compute the similarity of each type of face motion, we give higher weights to more abnormal motions which are assumed to carry more identity signature information.Our approach, named Exceptional Motion Reporting Model (EMRM), is unrelated with appearance, and can handle realistic facial motion in human subjects, with no restrictions of speed of motion or video frame rate. We first conduct our experiments on a video database containing 39 pairs of twins. The experimental results demonstrate that identical twins can be distinguished better by the talking profiles over the traditional appearance based approach. Moreover, we collected a non-twin YouTube dataset with 99 subjects. The results on this dataset verified that the talking profile can be the potential biometric. We further conducted an experiment to test the robustness of talking profile to the time. Videos from 10 subjects which span across years or even decades in their lives are collected. The results indicated the robustness of talking profile to the aging process.

@&#INTRODUCTION@&#
The occurrence of twins has progressively increased in the past decades as twin birth rate has risen to 32.2 per 1000 birth with an average 3% growth per year since 1990 [1]. With the increase of twins, identical twins are becoming more common as well. This, in turn, is urging biometric identification systems to accurately distinguish between twin siblings. Although identical twins represent a minority (0.2% of the world's population), it is worth noting that they equal the whole population of countries like Portugal or Greece. Therefore failing to identify them is a significant hindrance for the success of biometric systems. Identical twins share the same DNA code and therefore they look extremely alike. Nevertheless, some biometrics depend not only on the genetic signature but also on the individual development in the womb. As a result, identical twins have some different biometrics such as fingerprint and retina. Several researchers have taken advantage of this fact and have shown promising results in automatic recognition systems that use these discriminating traits: fingerprint [2], palmprint [3], iris [4] and combinations of some of the above biometrics [5]. However, these biometrics require the cooperation of the subject. Thus, it is still desirable to identify twins by pure facial features, since they are non-intrusive, they do not require explicit cooperation of the subject and are widely available from photos or videos captured by ordinary camcorders. Unfortunately, the high similarity between identical twins' appearance is known to be a great challenge for face recognition systems. The performance of various face appearance based approaches on recognizing identical twins has recently been questioned [5,6]. They both confirmed the difficulties encountered by appearance based face recognition systems on twin databases, and strongly suggested the need for new ways to improve the performance of recognizing identical twins.In psychology, it has been demonstrated that the human visual system utilizes both appearance and facial motion for face recognition [7,8]. Appearance information provides the first route for face recognition, while the dynamic signature information embedded in facial motion are processed in the superior temporal sulcus and provide a secondary route for face recognition. This secondary route for face recognition, also known as the supplemental information hypothesis, is supported by many works both in psychology and computer vision [7,9].The traditional appearance based face recognition simulates the first route to recognize faces, but fails to effectively distinguish between identical twins. Considering both studies support that facial motion contains identity information, and the observation that twins may look alike but behave differently, we propose to use talking profiles which consists of multiple type of usual face motions to recognize twins. Our intention is to simulate the secondary route for face recognition. The flowchart of our approach, named Exceptional Motion Report Model (EMRM), is illustrated in Fig. 1.1)Given a video, the talking profile is extracted. The profile consists of 6 types of usual face motions, such as pose change and 2D in-plane movement. Each type of motion in the profile is represented as a sequence of local motions between two adjacent frames.From the computed two talking profiles, we compute the similarity of each pair of corresponding face motions of the same type from both talking profiles. Given a pair of corresponding face motions, since they may be unsynchronized, i.e. different frame rate or speed, we perform a motion alignment in advance. The alignment is achieved by minimizing the abnormality function. A Gaussian mixture model is employed to estimate the abnormality of each local motion. This abnormality scheme is inspired from [10] in psychology which proves that humans use exceptional motions to identify faces. This is also observed in real life that humans always use a person's peculiar head motion (e.g. tilt) rather than common head movement to aid recognition. After alignment, the similarity of this pair of corresponding face motions is computed as the weighted sum of local motion similarity.From the similarities of every pair of corresponding face motion sequences in the talking profiles, we perform a SVM classification on those similarities. Also, our model is set in verification mode, that is, to claim that the faces in two videos are genuine or imposters.We test our algorithm by conducting several experiments on a free talking video database with 39 pairs of identical twins. Our experimental results shows that compared with traditional appearance based approaches, the talking profile can be used to accurately distinguish between identical twins. We further apply talking profile on a free talking video database of 99 subjects from YouTube. Results from our second set of experiments are in agreement with the psychological findings that facial motion contains identity signatures and demonstrate that the talking profile has a potential to be used in biometrics. To test the robustness of talking profile against time, we further collected an cross-age video database with 10 subject whose videos are set across years and conducted the experiments on this database. The results indicated that the talking profile is relatively robust to the time change.The contributions of our work are four fold: 1) we show that talking profiles can be used to distinguish between identical twins. 2) We propose a novel EMRM to analyze facial motion in video, which also provides a general framework of using abnormality for recognition. 3) Our experiments on YouTube dataset support the psychological findings that facial motion does provide identity signatures. 4) Our experiments on cross-age video database demonstrate the robustness of talking profile against the aging process. We continue by introducing existing twin recognition and motion based face recognition works in Section 2. In Section 3, we describe the details of our model. We present our dataset and experiments in Section 4, concluding in Section 5.

@&#CONCLUSIONS@&#
