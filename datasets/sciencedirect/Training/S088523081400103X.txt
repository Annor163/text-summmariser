@&#MAIN-TITLE@&#
Translating without in-domain corpus: Machine translation post-editing with online learning techniques

@&#HIGHLIGHTS@&#
We present a method to customize machine translation systems when in-domain data is not available.For that we perform an online learning automatic post-editing from ready-to-use generic machine translation systems.The results show that the method is very effective on rule-based machine translation systems.On statistical machine translation systems the method performs well if no in-domain data was used in the training.Finally, if there is not enough repetition our method has limited use.

@&#KEYPHRASES@&#
Machine translation,Statistical machine translation,Interactive machine translation,Automatic post-editing,Online learning,

@&#ABSTRACT@&#
Globalization has dramatically increased the need of translating information from one language to another. Frequently, such translation needs should be satisfied under very tight time constraints. Machine translation (MT) techniques can constitute a solution to this overly complex problem. However, the documents to be translated in real scenarios are often limited to a specific domain, such as a particular type of medical or legal text. This situation seriously hinders the applicability of MT, since it is usually expensive to build a reliable translation system, no matter what technology is used, due to the linguistic resources that are required to build them, such as dictionaries, translation memories or parallel texts. In order to solve this problem, we propose the application of automatic post-editing in an online learning framework. Our proposed technique allows the human expert to translate in a specific domain by using a base translation system designed to work in a general domain whose output is corrected (or adapted to the specific domain) by means of an automatic post-editing module. This automatic post-editing module learns to make its corrections from user feedback in real time by means of online learning techniques. We have validated our system using different translation technologies to implement the base translation system, as well as several texts involving different domains and languages. In most cases, our results show significant improvements in terms of BLEU (up to 16 points) with respect to the baseline systems. The proposed technique works effectively when the n-grams of the document to be translated presents a certain rate of repetition, situation which is common according to the document-internal repetition property.

@&#INTRODUCTION@&#
Globalization has urged the need for high-quality translations with fast turn-around times. Examples of that are companies aiming to internationalize their businesses in order to discover new markets and gain competitive advantage, or transnational institutions that have legal requirements to produce documentation in multiple languages. Frequently, these documents need to be delivered with tight deadlines and, at the same time, clients are pushing to adjust prices. As a result, translation agencies and in-house translation departments have been compelled to adopt automated machine translation (MT) in an attempt to improve their translation pipelines (Dove et al., 2012). In that way, MT systems are used to produce drafts of the translations that later are post-edited by human translators in order to achieve the high-quality standards required by the industry.Historically, rule based machine translation (RBMT) systems have been used by companies to automate their translation needs (Silva, 2012). Nevertheless, RBMT systems are expensive to personalize, as expert linguists are needed to create bilingual dictionaries or specific rules (Bennett and Slocum, 1985; Isabelle et al., 2007). As a result, these systems are only available for a handful of European languages. On the contrary, statistical machine translation (SMT) systems are created in a more unattended manner by harvesting parallel segments from a collection of Bi-texts or translation memories (TM). The quality that SMT systems achieve is often better than that of RBMT systems (Béchara et al., 2012; Silva, 2012), at least for some language pairs, and provided that there is enough data. However, it is only recently that SMT systems are being effectively used to improve the productivity of human translators by means of building engines customized from the client's data. Unfortunately, clients seldom have previous parallel corpora from the same domain that can be used to train these customized engines, or to adapt the domain of a pre-existent one (Irvine et al., 2013). Additionally, training such engines may take hours, days, if not weeks of computation. On the other hand, RBMT systems can be used right out-of-the-box, and they can be enhanced with an automatic post-editing (APE) by an SMT system in a way that translators appreciate it more than either of both systems alone, regardless their BLEU scores (Béchara et al., 2012). That paper shows that, although automatic evaluation metrics favor the pure SMT system, human evaluators prefer the output provided by the statistically post-edited RBMT system.Thus, the premise of this work is based on a real case scenario: a human translator, probably a freelancer, is given a translation assignment with a tight turn-around time. Alas, our translator lacks the necessary linguistic resources such as TMs or parallel texts that would allow him or her to build an MT system (no matter which technology is used) adapted to the specific domain of the document. Under these circumstances, what are his or her alternatives?W/O RESOURCESThis is the traditional manual method, but it requires more time and effort. Note that, in this case, we are not considering the use of previously collected TMs neither TMs generated on the go. On the contrary, each sentence is supposed to be translated from an empty box, or filled up with the source text at most.Translating with a web-based translation application, and then post-edit its output. Nowadays, there are many free web-based translation applications which can achieve a translation quality enough for gisting and, even in some cases, the quality can be satisfactory. However, it can be insufficient for many domains of interest. Also, these web-based translation applications can present some confidentiality issues that should be considered, because all content uploaded will be employed to enrich their models. Moreover, some of them are not free when translating more than a given quantity of words.Translating with a RBMT system, and then post-editing its output. There are many RBMT translation systems, some of them free. Nevertheless, the output of RBMT systems is usually not tailored to the domain of the document being translated and fail to adapt to new domains (Isabelle et al., 2007), e.g., lexical choices may not be appropriate. Although APE may alleviate this problem, still parallel corpora is needed.If he or she is familiar with SMT, he or she can train an SMT model with unrelated corpora (remember that there are no available in-domain TMs, which is a frequent case). As in the RBMT case, these SMT translations will contain several mistakes due to the fact that, in this case, the training corpus is out-of-domain.Neither of these options is optimal since, as we have discussed above, MT customization is key to improve the translator's productivity. In this paper, we propose a technique to help the translator in this regard. We assume that the translator will adopt one of the different MT alternatives proposed previously as a draft for post-editing, none of which is customized to the document domain. APE can be specially useful under these circumstances since it can be used as a domain adaptation technique. Domain adaptation has received extensive attention from the SMT research community during the last years. However, this topic has typically been approached in scenarios where the set of training samples used to estimate the model parameters (both in and out-of-domain) are available beforehand, and the system does not get updated after the training stage has concluded.In this work the domain adaptation problem is tackled in a different way, specifically, as translation hypotheses are amended by the user, the system will learn from these corrections, using them to train APE models on-the-fly. In this way, the following system translation hypotheses automatically will apply past user's amendments. To achieve this effect, an on-line learning (OL) SMT system will be used to customize the output of the original system after each translated segment. Hence, APE is considered here as an online technique, where the user interacts with the system in order to correct the initial hypotheses. In addition to this, these corrections could be taken as new corpora to retrain the APE models. In an ideal APE scenario, these models should be updated for each new hypothesis given by the system, in order to minimize the user post-editing effort. This is why we propose APE as a natural application field of OL techniques.The rest of the paper is organized as follows. First, Section 2 introduces the techniques implemented in our system, which applies statistical machine translation (Section 2.1) to an automatic post-editing task (Section 2.2) in an online learning environment (Section 2.3). Then, Section 3 compares our proposal with some similar techniques appearing in the literature. Section 4 describes the proposed system, emphasizing the base translation systems (Section 4.1) and our OL approach (Section 4.2). Finally, we show some experiments in Section 5 that apply these techniques, employing corpora with different features (Section 5.1) and several base translation systems (Section 5.2). Finally, we discuss the results in Section 5.3, showing in Appendix A some plots with the dynamics of OL APE.In this section we describe the theoretical foundations of our proposal, where we apply online learning techniques to an automatic post-editing task based on statistical machine translation. Thus, we introduce the statistical approach to machine translation (Section 2.1) as well as two language technologies that can be built upon it: automatic post-editing (Section 2.2), and online learning (Section 2.3).Given a sentence f from a source languageFto be translated into a target sentence e of a target languageE, the fundamental equation of SMT (Brown et al., 1993) is the following:(1)eˆ=argmaxePr(e∣f)(2)=argmaxePr(f∣e)Pr(e)where Pr(f∣e) is approximated by a translation model that represents the correlation between the source and the target sentence and where Pr(e) is approximated by a language model representing the well-formedness of the candidate translation e.State-of-the-art statistical machine translation systems follow a log-linear approach (Och and Ney, 2002), where direct modelling of the posterior probability Pr(e∣f) of Eq. (1) is used. In this case, the decision rule is given by the expression:(3)eˆ=argmaxe∑m=1Mλmhm(e,f)where each hm(e, f) is a feature function representing a statistical model and λmits weight.Current most popular MT systems are based on the use of phrase-based models (Koehn et al., 2003) as translation models. The basic idea of phrase-based translation is to segment the source sentence into phrases, then to translate each source phrase into a target phrase, and finally to reorder the translated target phrases in order to compose the target sentence. If we summarize all the decisions made during the phrase-based translation process by means of the hidden variablea˜1K, we obtain the expression:(4)Pr(f∣e)=∑K,a˜1KPr(f˜1K,a˜1K∣e˜1K)where eacha˜k∈{1…K}denotes the index of the target phrasee˜that is aligned with the k-th source phrasef˜k, assuming a segmentation of length K.According to Eq. (4), and following a maximum approximation, the problem stated in Eq. (2) can be re-framed as:(5)eˆ≈argmaxe,a{p(e)·p(f,a∣e)}Following the log-linear approach stated in Eq. (3), Eq. (5) can be rewritten as:(6)eˆ=argmaxe,a∑m=1Mλmhm(e,a,f)which is the approach that we follow in this work.MT systems usually need a final revision step by a human post-editor, to assure a quality output. This can be a tedious task, where the post-editor will have to repeatedly correct the same mistakes, due to the systematic behavior of MT systems (Allen and Hogan, 2000; Carpuat and Simard, 2012).This correction process can be understood as a transformation from an input (the translation provided by the previous MT system, usually with errors), to an output (a text in the same language where those errors have been amended). Thus, post-editing could be considered as a translation between two languages. APE systems were proposed by Knight and Chander (1994) to try to automate as far as possible that final human revision phase. Some authors consider APE as a domain adaptation or customization technique (Isabelle et al., 2007; Diaz et al., 2008; Rubino et al., 2012).In a statistical APE system, SMT models are trained to correct the outputs of another MT system (Simard et al., 2007), which is often considered as a black box. In this way, the fundamental equation of SMT (Eq. (1)) would be applied from a sentence e′ of target language with errorsE′, which is the output of the previous MT system that need to be corrected, into a target sentence e of a target languageEwithout errors (hopefully, at least with fewer errors thanE′).(7)eˆ=argmaxePr(e|e′)Fig. 1shows a diagram of a statistical APE system. In the diagram, the source sentences are processed as input by a generic MT system, producing a set of translations. After that, each system translation is used to feed the statistical APE system, whose models have been initialized from parallel corpus that have also been translated with the same MT system. For a given system translation, the APE system produces an automatic post-edition that is corrected by the user to generate the final output.One key feature of the technique proposed in this paper is the application of online learning. Here we describe the concept of online learning, including a brief review of different works that apply this concept to SMT (Section 2.3.1), the specific formulation of the log-linear SMT model with online learning used in this paper (Section 2.3.2) as well as a brief description of how the models are extended from new training samples (Section 2.3.3).Online learning is a machine learning task that is structured in a series of trials, where each trial has three steps: (1) the learning algorithm receives an instance, (2) a label for the instance is predicted and (3) the true label for the instance is presented.Online learning fits nicely in MT post-editing tasks, since the output of MT systems is corrected and validated by expert translators. If an SMT system is used to generate the translations, then its statistical models can be modified from the newly generated training pairs. Fig. 2shows a diagram of an SMT system with OL.During the last years, there has been an increasing interest in developing techniques to adapt or train the features of a log-linear combination in online learning settings. As far as we know, the SMT system with OL proposed by Ortiz-Martínez et al. (2010) (the system used in this paper as it is explained in the following section) constitutes the first work that successfully applies OL to SMT, solving the technical limitations encountered in previous works without the need of introducing heuristic approximations. Such previous works on online SMT include the dynamic adaptation of an IMT system via cache-based model extensions proposed by Nepveu et al. (2004) and the statistical computer assisted translation scenario with online learning proposed by Cesa-Bianchi et al. (2008). In both cases, the proposed systems were heavily limited by their inability to extend the translation models due to technical limitations to efficiently incorporate new parameters in a principled way. The work presented by Hardt and Elming (2010) applies a cache-based strategy similar to that presented by Nepveu et al. (2004), where the translation model is extended by means of heuristic IBM4-based word alignment techniques. IBM-4 word alignment techniques are replaced by phrase alignment techniques to extend the translation model in Bertoldi et al. (2013), Wäeschle et al. (2013). An additional attempt to efficiently extend the translation model was proposed by Blain et al. (2012). Their proposal aligns the output of the decoder with the reference given by the user as a previous step to obtain the word alignments between the source and reference sentences that are necessary to extract new phrase pairs. The method used to align the system translation and the reference sentence is based on the edit distance algorithm since it is assumed that both sentences will be similar.Here we adopt the online learning techniques described by Ortiz-Martínez et al. (2010). In that work, the authors define an incrementally updateable SMT model for its application in the interactive machine translation framework. Such an SMT model is able to process new training samples one by one, with constant computational complexity (i.e. the complexity does not depend on the training samples that have been previously seen). Moreover, their proposed system has already been implemented in a certain number of SMT prototypes (Ortiz-Martínez et al., 2011; Alabau et al., 2014).The SMT system described by Ortiz-Martínez et al. (2010) uses a log-linear model to generate its translations. According to Eq. (6), we introduce a set of seven feature functions (from h1 to h7): a n-gram language model (h1), an inverse sentence-length model (h2), inverse and direct phrase-based models (h3 and h4 respectively), a target phrase-length model (h5), a source phrase-length model (h6), and a distortion model (h7). The details for each feature function are listed below:•n-gram language model (h1):h1(e)=log(∏i=1|e|+1p(ei∣ei−n+1i−1)),22|e| is the length of e, e0 denotes the begin-of-sentence symbol, e|e|+1 is the end-of-sentence symbol andeij≡ei...ej.h1 can be implemented by means of smoothed n-gram language models. Here we adopt an interpolated n-gram model with Kneser-Ney smoothing.source sentence-length model (h2): h2(f, e)=log(p(|f|∣|e|)), h2 can be implemented by means of a set of Gaussian distributions whose parameters are estimated for each source sentence length.inverse and direct phrase-based models (h3, h4):h3(e,a,f)=log(∏k=1Kp(f˜k∣e˜a˜k)), where h3 is implemented with an inverse phrase-based model. This phrase-based model is smoothed with an HMM-based alignment (Vogel et al., 1996) model by means of linear interpolation.Analogously h4 is defined as:h4(f,e,a)=log(∏k=1Kp(e˜a˜k∣f˜k))target phrase-length model (h5):h5(f,e,a)=log(∏k=1Kp(|e˜k|), this feature is modelled by means of a geometric distribution. The geometric distribution penalizes the length of the target phrases.source phrase-length model (h6):h6(f,e,a)=log(∏k=1Kp(|f˜k|∣|e˜a˜k|)), a geometric distribution can be used to model h6, such distribution penalizes the difference between the source and target phrase lengths.distortion model (h7):h7(a)=log(∏k=1Kp(a˜k∣a˜k−1)), again, this feature function can be modelled by means of a geometric distribution. Such distribution penalizes the re-orderings.In order to incrementally train the log-linear model, a set of sufficient statistics that can be incrementally updated should be maintained for each feature function. If the estimation of the statistical model does not require the use of the expectation–maximization (EM) algorithm (Dempster et al., 1977) (e.g. n-gram language models), then it is generally easy to incrementally update the model given a new training sample. By contrast, if the EM algorithm is required (e.g. word alignment models), the estimation procedure has to be modified, since the conventional EM algorithm is designed for its use in batch learning scenarios. For those models, the incremental version of the EM algorithm (Neal and Hinton, 1999) is applied. Incremental EM guarantees estimation convergence after each algorithm iteration in a similar way to conventional EM, but E and M steps are individually applied to each training sample.In this section we identify the sufficient statistics for the main components used in the log-linear combination described above. These components are the language model (feature h1) and the translation model (features h3 and h4). Source and target phrase-length models (features h5 and h6) and the distortion model (feature h7) are implemented by means of geometric distributions with fixed parameters and thus they do not require a complex treatment. Finally, since the sentence length model (feature h2) is implemented by means of gaussian distributions, well known incremental update rules using simple sufficient statistics can be found in the literature (see for instance Knuth (1981)).Sufficient statistics for the language model (h1). Since language models are implemented using interpolated Kneser-Ney smoothing, probabilities are generated according to the following equation:(8)p(ei∣ei−n+1i−1)=max{cX(ei−n+1i)−Dn,0}cX(ei−n+1i−1)+DncX(ei−n+1i−1)N1+(ei−n+1i−1•)·p(ei∣ei−n+2i−1)where Dn=(cn,1)/(cn,1+2cn,2) is a fixed discount (cn,1 and cn,2 are the number of n-grams with one and two counts respectively),N1+(ei−n+1i−1•)is the number of unique words that follows the historyei−n+1i−1andcX(ei−n+1i)is the count of the n-gramei−n+1i, where cX(·) can represent true counts cT(·) or modified counts cM(·) (see Chen and Goodman (1996) for more details).Under these circumstances, the list of incrementally updateable sufficient statistics for the language model will include ck,1, ck,2, N1+(·), cT(·) and cM(·). In this particular case, the EM algorithm is not required, greatly simplifying the update process for a new training sample (see Ortiz-Martínez et al. (2010) for more details).Sufficient statistics for the translation model (h3and h4). Features h3 and h4 are implemented by means of inverse and direct phrase models. Since phrase-based models are symmetric models, only an inverse phrase-based model is maintained. Inverse phrase model probabilities are obtained from the relative frequencies of a set of phrase pairs:(9)p(f˜∣e˜)=c(f˜,e˜)∑f˜′c(f˜′,e˜)According to Eq. (9), the set of sufficient statistics for the phrase models is composed of a set of phrase countsc(f˜,e˜), which following standard estimation techniques, can be extracted from word alignment matrices. More specifically, given a sentence pair and its corresponding word alignment matrix, only those phrase pairs that are consistent with such word alignment matrix are extracted (see Koehn et al. (2003) for more details). Because of this, we also need to maintain direct and inverse HMM-based alignment models. These models are not only useful for smoothing purposes (as it was explained in Section 2.3.2), but also for generating the word alignment matrices that are required to obtain the phrase counts. Since the estimation of HMM-based alignment models requires the use of the EM algorithm, here we need to replace conventional EM by its incremental counterpart, allowing us to modify the parameters of the models for each individual training pair (a more detailed description of the incremental estimation of HMM-based alignment models can be found in Ortiz-Martínez et al. (2010)).

@&#CONCLUSIONS@&#
In this work we have analyzed a real translation scenario, where a human expert needs to translate a document without having any similar translation memory, nor in-domain corpus to train SMT models. We have proposed the application of automatic post-editing in an online learning framework, similar to that shown in Simard and Foster (2013), but replacing the automatic post-editing module based on edit distance word alignments by a fully functional online SMT system (Ortiz-Martínez et al., 2010) that successfully removes any heuristic approximations introduced in previous works to update the model parameters.We have assessed our technique with three corpora, namely, EMEA, Xerox and i3media, covering different domains and pairs of languages. Additionally, in the reported experiments we have combined our automatic post-editing module with three different kinds of base systems, including RBMT, Web and SMT systems. According to the obtained results, for the EMEA and Xerox corpora our system was able to significantly outperform the results obtained by all of the base systems. More specifically, improvements of up to 16, 11 and 7 BLEU points were obtained for the RBMT, Web and SMT systems respectively.However, the proposed technique did not behave properly when facing our third corpus, i3media. As we have seen in our experiments, this is explained by the fact that the repetition rate of the n-grams appearing in the test set is lower than that observed for the other corpora used in the experimentation. From the results, we have proved the importance of n-gram repetitions to take advantage of OL techniques. This requirement has been previously suggested in other works (Simard and Foster, 2013). However, we think this is not an exclusive limitation of online learning but also of domain adaptation techniques in general (see Section 5.1 for a detailed explanation). On the other hand, in online learning scenarios, the document internal repetition property (Church and Gale, 1995) predicts a high probability of observing similar sentences composed of similar n-grams in a given document.