@&#MAIN-TITLE@&#
Ranking and comparing evolutionary algorithms with Hellinger-TOPSIS

@&#HIGHLIGHTS@&#
Ranking and comparison of algorithms performance in evolutionary computation is a challenging issue.An alternative novel application of Hellinger TOPSIS has been proposed to ranking and comparison of algorithms performance.Promising results show the feasibility of the approach in terms of effectiveness and easy implementation.

@&#KEYPHRASES@&#
Algorithms comparison,TOPSIS,Hellinger-TOPSIS,Hellinger distance,

@&#ABSTRACT@&#
When multiple algorithms are applied to multiple benchmarks as it is common in evolutionary computation, a typical issue rises, how can we rank the algorithms? It is a common practice in evolutionary computation to execute the algorithms several times and then the mean value and the standard deviation are calculated. In order to compare the algorithms performance it is very common to use statistical hypothesis tests. In this paper, we propose a novel alternative method based on the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) to support the performance comparisons. In this case, the alternatives are the algorithms and the criteria are the benchmarks. Since the standard TOPSIS is not able to handle the stochastic nature of evolutionary algorithms, we apply the Hellinger-TOPSIS, which uses the Hellinger distance, for algorithm comparisons. Case studies are used to illustrate the method for evolutionary algorithms but the approach is general. The simulation results show the feasibility of the Hellinger-TOPSIS to find out the ranking of algorithms under evaluation.

@&#INTRODUCTION@&#
Methods for solving multicriteria decision making (MCDM) have been widely used to select a finite number of alternatives generally characterized by multiple conflicting criteria (attributes). Great efforts and significant progress have been made to the development of several MCDM approaches to solve different types of real-world problems. One of these techniques known as technique for order preference by similarity to ideal solution (TOPSIS) developed by Hwang and Yoon [1] evaluates the performances of the alternatives through the similarity with the ideal solution. According to this technique, the best alternative would be the one that is closest to the positive-ideal solution and farthest from the negative-ideal solution. The positive-ideal solution is the one that maximizes the benefit criteria and minimizes the cost criteria. The negative-ideal solution maximizes the cost criteria and minimizes the benefit criteria.TOPSIS is widely used to treat real-world decision making problems and it has been generalized to deal with a variety of information types. For example, there are many adaptations of TOPSIS to deal with interval numbers [2,3], probability distributions [4], fuzzy information [5–9] intuitionistic fuzzy information [10], interval-valued intuitionistic fuzzy information [11,12], among many others. For a broad survey about the TOPSIS the interested reader shall refer to [13].A great difficulty in evolutionary computation is the comparison of algorithms. Usually, the algorithms are applied several times to multiple benchmarks. Then, the results are analyzed by means of statistical hypothesis tests [14,15]. The statistical tests can detect if there are differences between the performances of the algorithms. The problem is if there are differences, which algorithm is the best one? To use statistical tests in this step, it is necessary to make pairwise comparisons between the algorithms. Obviously, the number of tests required increases greatly with the number of algorithms being analyzed. This is problematic first because the tiresome work of comparing each pair of algorithms; secondly and more importantly, the probability of making an error increases. When multiple hypothesis tests are being performed, one can only choose between increasing the probability of Type 1 Error or losing test power [22].Recently, the TOPSIS method has also been extended to treat, in a direct way, data expressed as probability distributions by means of the Hellinger distance [16]. It means that the TOPSIS with Hellinger distance have opened a new possibility for ranking alternatives expressed in terms of probability distributions in the context of MCDM problems.Due to the stochastic nature of the evolutionary algorithms, in many cases the performance of the algorithms are expressed in terms of mean and standard deviation. However, since these quantities are usually random variables, a direct comparison of these values may not be as meaningful as desired. For this reason, the usual approach is to perform statistical hypothesis tests to compare the algorithms.We present a new approach to support the selection of the best algorithms by applying the Hellinger-TOPSIS and reducing the quantity of hypothesis tests to be performed. In addition, in most situations it is not necessary to know the exact distribution of the solution of an algorithm. By the Central Limit Theorem (CLT), we know thatX¯⟶DN(μ,σ2/r). Therefore, if the algorithm is performed r times with r sufficiently large, we can approximate the distribution of the sample mean by the Gaussian distribution. Then, the Hellinger-TOPSIS can be easily applied to provide a rank order of the algorithms in a very easy and direct way, using the Hellinger distance between two Gaussians distributions. In the context of algorithms comparison, the alternatives consist of multiple algorithms and the criteria are the benchmarks. Assuming the validity of CLT, the method is simplified and it gets easier to be applied.The aim of this work is to present a tool to aid in selecting the best algorithms when applied to multiple benchmarks. The rest of this article is organized as follows: Section 2 describes the TOPSIS. In Section 3, we present the Hellinger-TOPSIS to deal with decision matrix with ratings expressed in terms of probability distributions. In Section 4, two case studies are analyzed, where different versions of genetic algorithms are applied to a suite of benchmarks in order to illustrate the feasibility of the approach. In Section 5, conclusions and directions for future work are given.Let us consider the decision matrix D, which consists of alternatives and criteria, described bywhere A1, A2, …, Amare viable alternatives, and C1, C2, …, Cnare criteria, xijindicates the rating of the alternative Aiaccording to criteria Cj. The weight vector W is composed of the individual weightsW=(w1,w2…,wn)withwj(j=1,…,n)for each criterion Cjsatisfying∑j=1nwj=1. In general, the criteria are classified into two types: benefit and cost. The benefit criterion means that a higher value is better while for cost criterion is valid the opposite. The data of the decision matrix D come from different sources so, in general, it is necessary to normalize them in order to obtain a dimensionless matrix. The normalized value rijcan be calculated as(1)rij=xij∑i=1mxij2,withi=1,...,m;j=1,...,nor(2)rij=xijmaxxij,withi=1,...,m;j=1,...,nThe normalized decision matrixR=[rij]mxnrepresents the relative rating of the alternatives. After normalization, we calculate the weighted normalized decision matrixP=[pij]mxnwithi=1, …, m, andj=1, …, n by multiplying the normalized decision matrix by its associated weights. The weighted normalized value pijis calculated as:(3)pij=wjrijwithi=1,...,m,andj=1,...,n.The TOPSIS is described in the following steps [17]:Step 1: Identify the positive ideal solutions (PIS) A+ (benefits) and the negative ideal solutions (PIS) A− (costs) as follows:(4)A+=(p1+,p2+,…,pm+)wherepj+=(maxipij,j∈J1;minipij,j∈J2)(5)A−=(p1−,p2−,…,pm−)wherepj−=(minipij,j∈J1;maxipij,j∈J2)where J1 and J2 represent benefit and cost criteria, respectively.Step 2: Calculate the separation measures from the positive ideal solutions A+ (benefits) and the negative ideal solutions A− (costs) for each alternative Ai, respectively as follows:(6)di+=∑j=1n(pj+−pij)2withi=1,…,m.(7)di−=∑j=1n(pj−−pij)2withi=1,…,m.Step 3: Calculate the relative closeness coefficient ξifor each alternative Aiwith respect to positive ideal solution as given by(8)ξi=di−di++di−⋅Step 4: Rank the alternatives according to the relative closeness coefficients. The best alternatives are those that have higher value ξiand therefore should be chosen.The extension of TOPSIS to Hellinger-TOPSIS is described in the following.The decision matrix D consisting of alternatives and criteria is described bywhere A1, A2, …, Amare alternatives, C1, C2, …, Cnare criteria, xijindicates the rating of the alternative Aiwith respect to criterion Cjdescribed in terms of probability distributions. In the context of algorithms comparison, the alternatives consists of the algorithms and the criteria are the benchmark problems.In order to define a method that deals directly with information in the form of probability distributions, we need to answer two questions: given two probability distributions which one is higher/preferable? and how far are they from each other? Next, these issues have been addressed in [16] and here we provide some necessary definitions.Definition 1[18]Let f and g be two probability density functions (pdf). The Hellinger distance between f and g is given byThe Hellinger distance satisfies:1.0≤DH(f, g)≤1DH(f, g)=DH(g, f)Let f1 be the pdf of aN(μ1,σ12)and let f2 be the pdf of aN(μ2,σ22).Then, the Hellinger distance between f1 and f2 is calculated by(10)DH(f1,f2)=1−2σ1σ2σ12+σ22exp−14⋅(μ1−μ2)2σ12+σ22Definition 2[19]Letx˜1∼f1andx˜2∼f2be two random variables, where f1 and f2 are the respective distributions. Then the (stochastic) dominance degree of f1 over f2 is given byDf1≻f2=P(x˜1≥x˜2)−0.5P(x˜1=x˜2).Since the ratings are in terms of probability distribution, it is necessary to adapt the TOPSIS to process such data sets. In the standard TOPSIS, the weights are incorporated directly into the decision matrix and only then the distances are calculated. Since we are using distance measures, which are nonnegative numbers, there is no need to use the square and squared root functions. Another aspect is that for algorithms comparison, the criteria are the benchmarks and since all have the same importance, the weight of each criterion can be disregarded in the calculation of the distances. It is worth to remind that there is no need to normalization of the data since the Hellinger distance value lies in [0,1].Next, the Hellinger-TOPSIS (H-TOPSIS) using the Hellinger distance for comparison of algorithms performance is described in the following steps:Step 1: Calculate the PIS f+ and the NIS f− for each criterion.The PIS is determined byfj+∈(f1j,f2j,…,fmj)such thatDfj+≻fij≥Dfij≻f+∀i=1,…,mfor benefit criteria andDfj+≻fij≤Dfij≻f+∀i=1,…,mfor cost criteria.The NIS is determined byfj−∈(f1j,f2j,…,fmj)such thatDfj−≻fij≥Dfij≻f−∀i=1,…,mfor cost criteria and theDfj−≻fij≤Dfij≻fj−∀i=1,…,mfor benefit criteria.Since we are concerned with minimization problems, the smaller the value of the mean, the better. So, the issue of determination of the PIS and the NIS can be summarized as follows:PIS –fj+←fijwhereiis the index which corresponds tomini(μij)∀j=1,…,nand∀i=1,…,mandNIS –fj−←fijwhereiis the index which corresponds tomaxi(μij)∀j=1,…,nand∀i=1,…,mwith fij∼N(μij, σij) where μij, σijstands for mean and standard deviation, respectively.Step 2: Calculate the separation measures from the positive ideal solutions f+ and the negative ideal solutions f− for each alternative (algorithm), respectively, as follows:(11)di+=∑j=1nwjDH(fj+,fij)withi=1,…,m.(12)di−=∑j=1nwjDH(fj−,fij)withi=1,…,m.where the distanceDH(fj+,fij)andDH(fj−,fij)between two Gaussian distributions is calculated according to Definition 1. Since in case of algorithms comparison, the benchmarks have the same weights both equations turn to(13)di+=∑j=1nDH(fj+,fij)withi=1,…,m.(14)di−=∑j=1nDH(fj−,fij)withi=1,…,m.Step 3: Calculate the relative closeness coefficient ξifor each alternative Aiwith respect to positive ideal solution as given by(15)ξi=di−di++di−Step 4: Rank the alternatives according to the relative closeness coefficients. The best alternatives are those that have higher value ξiand therefore should be chosen because they are closer to the positive ideal solution.The idea here is to use the H-TOPSIS as a preprocessing step. The role of H-TOPSIS is to rank the algorithms such that one could use this rank as a filter to separate the best algorithms from the worst algorithms. Since the H-TOPSIS provides the rank order of the algorithms, another issue arises: how one can determine the group of the best algorithms? Would they be the first five? The first ten? There is not a precise way to determine how many algorithms should be considered as the best ones. One could determine a percentage of the algorithms being analyzed to compose the group of best algorithms, like 30–40% of the algorithms best ranked by the H-TOPSIS must be analyzed more formally with statistical tests. Another possibility is to consider a threshold δ for the closeness coefficient in such a way that only the algorithms with a closeness coefficient higher than δ would be considered good enough to be statistically analyzed. Differently, one could define a small percentage of the algorithms to be analyzed, like the three first ranked algorithms, and check if there are significant differences between their performances with a statistical test like Friedman's test. If there are differences between them, the pairwise comparison must be performed. If there is no difference, the algorithm in the next rank should be included in the analysis, which would be performed with the same logic. This approach is more formal than the first two, however it demands more statistical tests, which increases the probability of error. On the other hand, the first two approaches requires the specification of the percentage or threshold, which is subjective. In cases that a new algorithm is being proposed, one could guarantee that such algorithm be in the analysis. A possibility is to consider the proposed algorithm as a control treatment and then compare the best ranked algorithms with the new one (the control). We would like to highlight here that the rank provided by the H-TOPSIS should not be considered final and statistical analysis must be performed. The main reason for that is that the H-TOPSIS does not consider random fluctuation inherent to the estimators. The H-TOPSIS assume that the real values of the parameters are known, so there would be no randomness in these parameters, which is not the case when the parameters are estimated. Therefore, statistical analysis is essential, and the H-TOPSIS support such analysis by reducing drastically the number of pairwise comparisons to rank the alternatives.In order to illustrate the H-TOPSIS, let us consider the case where 3 different evolutionary algorithms (or different versions originated from the same algorithm), named A1, A2, and A3 for easy of notation. These algorithms are applied to solve optimization problems. There are in the literature established benchmark problems and without loss of generality we consider minimization problems but maximization problems can be transformed easily to its equivalent minimization problems. The 3 algorithms are applied to solve 4 benchmark problems. As a standard procedure in evolutionary computation, each of the algorithms is run a maximum number of generations (iterations) and the experiment is repeated for each algorithm A1, A2, A3 r=50 times for each of the benchmarks P1, P2, P3, and P4. So, in this way a statistic in terms of mean and standard deviation is calculated as shown in Table 1. Since, the Central Limit Theorem (CLT) is satisfied in this case, then the ratings of the matrix D can be modeled by Gaussian distributions and the Hellinger distance described by Eq. (10) can be used.For the H-TOPSIS, we first need to calculate the PIS and NIS for each criterion. In this case, it is easily obtained by using the step 1 of the algorithm applied to the data matrix in Table 1. Table 2presents the PIS and NIS for each criteria (Benchmarks). Once we calculated the PIS and NIS, one must calculate the Hellinger distance between the alternative ratings and the corresponding PIS and NIS. Next, one calculates thedi+anddi−and finally the closeness coefficient. The ξiresults are presented in Table 3. As we can notice, the final ranking obtained is A2≻A1≻A3. The best algorithm is A2, and the worst is A3. Fig. 1presents the box-plots of the error in terms of the mean for each algorithm. As can be noticed the rank according to H-TOPSIS is in agreement with the box-plot shown in Fig. 1.Next, we present two realistic case studies for comparing several different versions of genetic algorithms applied to benchmarks of constrained dynamic optimization.The G24 Benchmark Set of Dynamic Constrained Optimization Problems (DCOPs) was introduced by Nguyen [20] and Nguyen and Yao [21], which consists of a set of 18 benchmarks: (1) G24-u, (2) G24-1, (3) G24-f, (4) G24-uf, (5) G24-2, (6) G24-2u, (7) G24-3, (8) G24-3b, (9) G24-f, (10) G24-4, (11) G24-5, (12) G24-6a, (13) G24-6b, (14) G24-6c, (15) G24-6d, (16) G24-7, (17) G28-a, and (18) G24-8b.The algorithms established in the literature for dynamic optimization problems are 21 different versions of Genetic Algorithms (GA) presented by Nguyen [20] and Nguyen and Yao [21]: (1) GAnoElit, (2) RIGAnoElit, (3) HyperMnoElit, (4) GAelit, (5) RIGAelit, (6) HyperMelit, (7) GA+Repair, (8) GA+RepairwUPGwNR, (9) GA+RepairwUPGwRR, (10) GA+RepairwUPCwNRR, (11) dRepairGA, (12) dRepairRIGA, (13) dRepairHyperM, (14) dRepairGAOOR, (15) dRepairRIGAOOR, (16) dRepairHyperMOOR, (17) Genocop, (18) GenocopwUPGwNRR, (19) GenocopwUPGwRR, (20) GenocopwUPCwNRR, (21) dGenocop.Next, the 21 different versions of Genetic Algorithms have been applied to the 18 Benchmarks of dynamic constrained minimization problems after 50 runs [20,21]. The statistic in terms of mean and standard deviation (stdDev) is shown in Table 4.The problem now is to determine the best algorithms in terms of effectiveness among the 21 algorithms analyzed. As said previously, one can easily check if there are significant differences between the algorithms performances. This could be done by applying the Friedman's test [14,15,22]. In such case, each algorithm represents a treatment and each problem represents a block. The Friedman's test analyzes if there are differences between the algorithms performances. But the main question remains: which ones are the best algorithms? To answer this question one should make pairwise comparisons between the algorithms. Since we have 21 algorithms, 210 pairwise analysis should be carried out. This certainly will considerably inflate the probability of making a mistake, i.e., conclude that there is no difference when there is a difference or the opposite.As stated previously, the idea here is to use the H-TOPSIS as a preprocessing step. Applying the H-TOPSIS to the data in Table 4 provides the results fordi+di−, the closeness coefficient ξiand the ranking of the algorithms shown in Table 5. As we can notice, the best algorithm is A21, the second better algorithm is A20 and the worst is A1. Fig. 2presents the box-plots of the performance of each algorithm in the order of H-TOPSIS rank. In Fig. 3the box-plots are presented in order of the Friedman's rank.We can see in Fig. 2 that the rank order provided by the H-TOPSIS is consistent, as the rank goes up the error tends to increase. In few cases, like A5 and A7, where the H-TOPSIS preferred A5 over A7, it may be counter intuitive with relation to the box-plot. However, it must be highlighted that the box-plot does not consider the variance in each block, i.e., in each benchmark. It only considers the mean value obtained in each benchmark. The H-TOPSIS take into account the mean as well as the variance values for each benchmark. On the other hand, by ordering the box-plots by the Friedman's ranks, as presented in Fig. 3, the variation of the box-plots with relation to the ranks are much less smooth than the obtained by the rank of the H-TOPSIS. Many counter intuitive cases arises, for example, it is very hard to explain by the box-plot why A18 is preferred to A17 by the Friedman's rank. Also, the Friedman's rank does not consider the variance for each benchmark, so one could expect a better agreement between the Friedman's rank and the box-plots.We start our analysis by considering the first three ranked algorithms, i.e., A21, A20 and A17 (see Table 5 or Fig. 2). The Friedman's test provides a p-value of 0.2776, leading to the non-rejection of H0 in any reasonable significance level. Therefore, we conclude that there are no statistical differences between the performances of the first three algorithms. Then, we include in the analysis the A16 algorithm and the Friedman's test is still unable to detect any difference in performance. Following these steps, the first difference in performance detected by the Friedman's test occurs when A14, the algorithm in rank 8, is included in the analysis with p-value 0.0005. At this point, one could stop adding algorithms in the analysis. This preprocessing step reduced the number of algorithms from 21 down to 8 which leads to a reduction of pairwise comparisons from 210 down to 28. This reduction in the number of hypotheses tests obviously reduces the probability of making a mistake. Just to illustrate how significant the reduction of the probability of making a mistake may be, assume the tests are independent and that we fixing the probability of Type 1 Error in 1% for each test. Then, in the 210 tests, the probability of not committing a Type 1 Error in all tests would be about 12% while using the 28 tests it would be about 75%. One could argue that it is possible to correct the test significance to guarantee a smaller probability of committing the Type 1 Error, however, in such a case the probability of Type 2 Error would increase.Therefore, this analysis suggests that A21, A20, …, A12 are superior to the remaining algorithms. While the rank order of H-TOPSIS suggests the advantage of A21, which is an evidence of its superiority, the evidence is not sufficient to state by means of statistical tests. So, one could argue that the algorithms A21, A20, …, A12 present approximately the same quality and, in case that it is necessary to select one algorithm, one could use the rank provided by the H-TOPSIS for a supported choice. Now suppose that the algorithm A21 is a new algorithm being proposed. As said previously, one could treat such algorithm as a control. Then, make the pairwise comparisons as presented in Table 6, which shows by means of statistical test that the algorithm A21 is superior than the algorithms A14, A11 and A10, which have the ranks 8, 9 and 10 respectively.In the following, an additional evolutionary algorithm (named by us as a new alternative (A22), which is a gravitational search algorithm (GSA) developed by Pal et al. [23] have been proposed to minimize the 18 benchmarks and the results in terms of mean and standard deviations has been added to Table 4 and the new decision matrix is now given in Table 7.Applying the H-TOPSIS to the data in Table 7 provides the results for the closeness coefficient ξiand the rank of the algorithms shown in Table 8. As we can notice, the best algorithm is A22, the second better algorithm is A21 and the worst is A1. Fig. 4presents the box-plots of the performance of each algorithm in the order of the Friedman's rank.It is worth to mention that in this case study an additional alternative (A22) has been added to the decision matrix and the rank order for the best alternative changed from A21 to A22. It turns out that there is no evidence of occurrence of rank reversal, which is an important aspect in multicriteria decision making.In this work, we present the Hellinger-TOPSIS to compare the performances of the algorithms. The method is used as a support tool for the statistical analysis to rank the algorithms. In order to illustrate the method a simple example has been shown and a realistic case involving benchmarks of constrained dynamic optimization is presented. The results show the effectiveness of the method. In terms of computational burden, the H-TOPSIS consists of a very simple computation procedure, which shall encourage researcher/practitioner in different areas of knowledge to use it.It is important to note that, despite the preprocessing with the H-TOPSIS may be considered less formal than statistical tests, the TOPSIS is a well-established and reliable methodology. Also, the gain obtained by the preprocessing step, by reducing significantly the probability of making a mistake in the statistical analysis, is very attractive. In addition, one could argue that the values considered by the H-TOPSIS are subject to random fluctuation which could impact in the rank order. Indeed, this is true, but if the number of execution of the algorithms is sufficiently large, the sample mean will be stable enough to not cause any significant impact in the H-TOPSIS rank.

@&#CONCLUSIONS@&#
