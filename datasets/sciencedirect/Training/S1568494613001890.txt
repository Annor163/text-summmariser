@&#MAIN-TITLE@&#
Evolving spiking wavelet-neuro-fuzzy self-learning system

@&#HIGHLIGHTS@&#
Evolving system based on self-learning fuzzy spiking neural network is proposed.The adaptive wavelet activation-membership functions usage is extended.Complex clusters detection capability is achieved via evolving architecture.

@&#KEYPHRASES@&#
Computational intelligence,Hybrid evolving system,Multilayered spiking neural network,Self-learning,Control systems theory,Wavelet,Fuzzy clustering.,

@&#ABSTRACT@&#
The paper introduces several modifications to self-learning fuzzy spiking neural network that is used as a base for evolving system design. The adaptive wavelet activation-membership functions are utilized to improve and generalize receptive neuron activation functions and the temporal Hebbian learning algorithm. The proposed evolving spiking wavelet-neuro-fuzzy self-learning system retains native features of spiking neurons and reveals evolving systems’ capabilities in detecting overlapping clusters of irregular form.

@&#INTRODUCTION@&#
Self-learning spiking neural networks have appeared to be a powerful computational intelligence tool for fast and efficient data clustering [1–3]. Moreover, being more realistic models of biological neural systems than artificial neural networks of the previous generations [4,5], spiking neural networks present another step of computational intelligence paradigm evolving toward biologically plausible computing [6]. Hybrid intelligent systems designed on the basis of self-learning spiking neural networks and fuzzy clustering approaches were shown to be successfully applied for data clustering in the presence of overlapping classes [7–9].Another area where self-learning spiking neural networks and hybrid systems based on them can be successfully used is a hierarchical clustering. Previously, multilayered spiking neural network with feed-forward adjustable lateral connections in the hidden layers was suggested for solving tasks of such kind [1]. While the obtained results of hierarchical clustering produced by the network were sufficiently successful, approach to application of the lateral connections within spiking neurons layers seems to be rather unnatural. Such network requires two learning algorithms, namely, one that adjusts synaptic weights between layers for initial data partitioning and another one that adjusts weights of lateral connections for complex clusters detecting. Apparently, several learning algorithms usage increases learning time. Furthermore, the network layers number is set a priori and does not change through the network functioning. Architecture of multilayered self-learning spiking neural network that uses lateral connections for spikes transmitting only was introduced thereafter [10]. Capability of irregular form clusters detecting was achieved by improving biologically inspired learning algorithm that adjusted solely connections between layers. The network operation was considered from control theory point of view, in terms of Laplace transform. Such approach allowed of network architecture designing on a general technically plausible ground that provided researchers and engineers with a powerful framework for various hardware implementations of intelligent systems based on spiking neural networks.Though self-learning spiking neural networks can successfully perform various clustering tasks, they, like any technical tool, have certain limitations. The most important one is that some network architecture adjustable parameters lack for rules on their initial setup. The first and foremost issue here is how to determine number of spiking neurons layers when performing hierarchical data clustering. Both multilayered spiking neural networks mentioned above require a priori setup for hidden layers number that is usually hard to do. In this paper an evolutionary approach is proposed to be used for fuzzy spiking neural network architecture setup that allows researcher to tune number of spiking neurons layers during the network learning.The paper also introduces a generalization of receptive neuron activation function. Originally Gaussian-like functions were suggested for receptive neurons [1] but their setup proposed looks rather unnatural because it almost does not consider input data specifics. Later it was shown activation functions of receptive neurons in a pool can be treated as membership functions of a certain linguistic variable, thus allowing researcher to setup receptive neurons according to a priori knowledge of input data structure. Mitaim and Kosko revealed, though, that membership function in some cases may take negative values in order to present non-membership levels [11,12], and Gaussian-likes functions do not meet that requirement. In order to overcome the difficulty, an adaptive wavelet activation-membership function is introduced in this paper as a generalization of receptive neuron activation function.In summary, a computational intelligence hybrid system [13] driven from the liquid computations paradigm [14] and combining the merits of conventional spiking neural networks, fuzzy clustering, wavelet membership functions, and evolving systems [15,16] is suggested in this paper, such that it builds up its layers in the course of data processing for achieving the required clustering quality in the presence of overlapping irregular form classes.Evolving spiking wavelet-neuro-fuzzy self-learning system for data clustering has a heterogeneous multilayered feed-forward architecture that includes population coding layer and several layers of spiking neurons. Population coding layer (the first hidden layer) transforms input signals into pulse-position form. Layers of spiking neurons perform hierarchical clustering. The overall architecture of the system is depicted on Fig. 1and is a modification of architectures introduced in [1,10].Similar to biological neurons, spiking neurons process information presented in pulse-position form [5]. One of the major purposes of population coding layer is to perform transformation of input patterns from pulse-amplitude form to pulse-position one. Another purpose of the layer is to improve cluster separation capability of spiking neurons [1].Population coding layer consists of pools of receptive neurons. Each dimensional component of input pattern is processed by all receptive neurons of a certain pool.Generally speaking, population coding layer acts as follows. It takes(n×1)-dimensional input patterns x(k) (here n is the dimensionality of input space,k=1,N¯is a pattern number, N is number of patterns in incoming set) and produces (hn×1)-dimensional vector of incoming spikes t[0](x(k)) (h is the number of receptive neurons in a pool). In general case firing time of each spike emitted by a receptive neuron lies in a certain interval[0,tmax[0]]referred to as coding interval and is described by the following expression:(1)tli[0](x(k))=tmax[0](1−ψ(|xi(k)−cli[0]|,σi))where•is the floor function,ψ(•,•)is an activation function of receptive neuron,cli[0]is the center of the lth receptive neuron in the pool of the ith input,σiis the width of receptive neuron activation function in the pool of the ith input.‘In this paper, the adaptive wavelet activation-membership functions [17,18] are used for receptive neuron activation functions:(2)ψ(|xik−cli[0]|,σi)=(1−αliτli2)e(−τli2/2)whereτli2=(xi(k)−cli[0])2σi−2,αliis a tuning parameter (0≤αli≤1).Thus, the input for the first hidden layer is a sampled (n×1)-dimensional vector of patternsxto be clustered, and its output is a temporal (hn×1)-dimensional vector of spikest[0](x).Using notion of automatic control theory, one can readily see that a spiketli[0](xi(k))(herelis the number of receptive neuronRNliin pool for the ith input) is similar to the Dirac delta functionδ•[19]. Thus, its Laplace transform is(3)Ltli[0](xi(k))=Lδ(t−tli[0](xi(k)))=e−tli[0](xi(k))swhere s is the Laplace operator.It is notable that population coding layer is identical to a fuzzification layer of neuro-fuzzy systems like Takagi–Sugeno–Kang networks, ANFIS, etc. [20]. From this point, pool of receptive neurons can be interpreted as a certain linguistic variable and receptive neuron (more precisely, fuzzy receptive neuron) within the pool – as a linguistic term whose membership function corresponds to activation function of the neuron [19]. Such approach allows receptive neuron parameters to be adjusted on the basis of a priori knowledge of data structure in order to get better clustering results, and use of adaptive activation-membership functions ensures better flexibility of population coding as it allows membership function to take negative values that reflect small membership or non-membership levels [11,12]. In case non-membership is not considered for a certain data processing task, the adaptive wavelet can take form of conventional Gaussian (that is controlled by tuning parameterαliin (2)).Number of spiking neurons layersMcorresponds to number of levels in the data hierarchy defined by an expert and builds up as the system architecture evolves (constructive approach) until the required clustering quality is achieved in fuzzy clustering layer where the final membership levelsμj(k)of patternsx(k)to the jth cluster are determined. Each layer of spiking neurons corresponds to a certain level of data hierarchy in final partition. The second hidden layer (i.e. the first layer of spiking neurons) represents the most specific level of hierarchy, and the last hidden layer represents the most general level of hierarchy.Within a layer of trained neural network, each spiking neuron responds to patters of a certain class. The first fired neuron defines the class that input pattern belongs to (and firing time defines the temporal distance of input pattern to the corresponding cluster center [1]). Thus, a layer of spiking neurons performs crisp data partitioning on a certain level of hierarchy by indicating patterns membership via firing times. Number of spiking neurons in the preceding layermr−1is always greater than one in the following layermr:(4)m1>m2>⋯>mr−1>mr>⋯>mM≥2that is number of neurons decreases in each next layer in the course of the system architecture evolvement.A spiking neuron of the following layer detects synchrony in firing of the preceding layer neurons, performing grouping of nearby classes. The purpose of a learning algorithm for such network is to adjust synaptic weights the way to strengthen connections between neurons that fire together. Application of such approach allows of irregular form clusters detecting [1]. In this work it is considered researcher controls results of data clustering during the network learning and adds new spiking neurons layer each time if after a few epochs of learning, results are not satisfactory (since spiking neural networks learn fast, a few epochs are sufficient to train a layer as a rule). In general case, the network evolving can be controlled by a certain clustering quality measure to keep optimal architecture of the network [21].In this paper, a spiking neuron is designed in terms of control systems theory (Fig. 2), namely, neuron synapse is modeled as a second-order critically damped response unit, and neuron soma is modeled on the basis of bang-bang control systems concept [19] (comprehensive convention notion on spiking neuron architecture can be found in [7]).Synapses between neurons of two adjacent layers are multiple structures. A multiple synapse consists of a set ofqsubsynapses connected in parallel, and each subsynapse is formed by a group of time delay, second-order critically damped response unit, and gain connected in series. A subsynapse transforms incoming signal from pulse-position form to continuous-time one and have the following transfer function:(5)Ujgp,[r](s)=τPSPwjgp,[r]e1−dps(τPSPs+1)2,wherep∈1,2,…,qis number of the subsynapse within multiple synapseMSjg[r]of spiking neuronSNj[r],r∈1,2,…,Mis number of spiking neurons layer,jis number of spiking neuron in the following layer,gis number of spiking neuron in the preceding layer (two indexes,landi, are used for a receptive neurons),τPSPis membrane potential decay time constant whose value can be obtained empirically (PSPmeans ‘postsynaptic potential’), its value is the same for all neurons in the network, w is a synaptic weight, d is a time delay.On its output, subsynapse produces continuous-time signal, weighted delayed postsynaptic potentialujgp,[r](t)(or in terms of Laplace transform,ujgp,[r](s)=e−tg[r−1](x(k))sUjgp,[r](s).Multiple synapseMSjg[r]transfer function is [10](6)Ujg[r](s)=∑p=1qUjgp,[r](s)=∑p=1qτPSPwjgp,[r]e1−dps(τPSPs+1)2Multiple synapse generates total postsynaptic potentialujg[r](t)that arrives to spiking neuron soma whose state variable, membrane potentialuj[r](t), accumulates all incoming total postsynaptic potentials as follows:(7)Uj[r](s)=∑g=1mr−1Ujg[r](s)=∑g=1mr−1∑p=1qτPSPwjgp,[r]e1−dps(τPSPs+1)2Spiking neuron soma performs transformation that is opposite to one of the synapse. It takes continuous-time internal signaluj[r](t)and produces pulse-position signalδ(t−tj[r](x(k))). In doing so soma responds each time its membrane potentialuj[r](t)reaches a certain threshold valueθs.n.(s.n. means ‘spiking neuron’). In other words, spiking neuron soma acts as a threshold detection system and consequently it can be designed on the basis of bang-bang control systems concept [19].Threshold detection behavior of a neuron soma can be modeled by an element relay with dead zoneθs.n.that is defined by the nonlinear function(8)Φrelay(uj[r](t),θs.n.)=sign(uj[r](t)−θs.n.)+12wheresign(•)is the signum function.Soma firing can be described by a derivative unit that is connected with the element relay in series and produces a spike each time the relay switches. In order to avoid a negative spike that appears as a response to the relay resetting, a usual diode is added next to the derivative unit. The diode is defined by the following function:(9)Φdiode(δ(t−trelay[r]))=δ(t−trelay[r])H(δ(t−trelay[r]))wheretrelay[r]is a spike produced by the derivative unit upon the relay switching,H(•)is the Heaviside step function.Now we can define the Laplace transform of an outgoing spiketj[r](x(k)), namely,(10)Lδ(t−tj[r](x(k)))=e−tj[r](x(k))s=LΦdiode(sLΦrelay(uj[r](t),θs.n.))In the analog–digital architecture of spiking neuron, the refractoriness is modeled by a feedback circuit. As shown on Fig. 2, it is a group of time delay, second-order critically damped response unit, and gain that are connected in series. The time delay defines duration of a spike generation perioddspike(usually,dspike→0for all neurons in the network). The second-order critically damped response unit defines a spike after-potential that sets neuron refractoriness. Generally, spike after-potential can be represented by a second-order damped response unit, but for the sake of simplicity, we use critically damped response unit as it can be defined by one parameter only, namely,τSAP(SAPmeans here ‘spike after-potential’). This parameter controls duration of the refractory period. Finally, the gain unit sets amplitude of the spike after-potentialwSAP. Obviously,wSAPshould be much greater than any synaptic weight. Parameters of spike-after potential are the same for all neurons in the network.Thus, transfer function of the feedback circuit is(11)GF.B.(s)=wSAPe−dspikes(τSAPs+1)2whereF.B.means ‘feedback circuit’, and transfer function of the soma is(12)Gsoma(s)=GF.F.1+GF.F.GF.B.whereGF.F.is defined by (9) (F.F.means ‘feed-forward circuit’).The output layer, namely output fuzzy clustering layer, takes vector of spikest[M](x(k))arriving from the last layer of spiking neurons, and performs fuzzy partitioning of the input patternsx(k):(13)μj[M](k)=(tj[M](x(k)))(2/1−ζ)∑ι=1m(tι[1M](x(k)))(2/1−ζ)whereμj[M](k)is a membership level of the input patternx(k)to the jth cluster in the Mth layer of spiking,ζis a fuzzifier that determines boundary between clusters and controls the amount of fuzziness in the final partition. It is easily seen that the output layer evaluates fuzzy membership similarly to well-known fuzzy C-means algorithm [7,9]The purpose of an unsupervised learning algorithm for network with one layer of spiking neurons is to adjust centers of spiking neurons so as to make each of them to correspond to centroid of a certain data cluster [1]. Such learning algorithm was introduced on the basis of two learning rules, namely, ‘Winner-Takes-All’ rule and temporal Hebbian rule [22]. The first one defines which neuron should be updated, and the second one defines how it should be updated. The algorithm adjusts neuron centers through synaptic weights updating on each learning epoch.A learning epoch consists of two phases. Competition, the first phase defines a neuron-winner. Being laterally linked with inhibitory connections in originally suggested architecture of spiking neural network [22], spiking neurons compete to respond to input pattern. The one wins (and fires) whose center is the closest to the pattern. After competition has been completed, weights adjusting takes place. The learning algorithm adjusts synaptic weights of the neuron-winner to move it closer to the input pattern. It strengthens weights of those subsynapses which contributed to the neuron-winner's firing (i.e. the subsynapses produced delayed spikes right before the neuron firing) and weakens ones which did not contribute (i.e. the delayed spikes appeared right after the neurons firing or long before it). Generally, the learning algorithm for a conventional spiking network can be expressed as(14)wjlip,[1](K+1)=wjlip,[1](K)+ηw(K)L(Δtjlip,[1]),j=j˜,wjlip,[1](K),j≠j˜,whereKis the current epoch number,ηw(•)>0is the learning rate (while it is constant in [22], it can depend on epoch number in the general case, w means ‘weights’),L(•)is the learning function [14],j˜is the number of neuron-winner on the current epoch,Δtjlip,[1]is the time delay between delayed spiketli[0](xi(k))+dpproduced by the pth subsynapse of the lith synapse and spiking neuron firing timetj[1](x(k)):(15)Δtjlip,[1]=tli[0](xi(k))+dp−tj[1](x(k))As a rule, the learning function has the following form [23]:(16)L(Δtjlip,[1])=(1+β)exp−(Δtjlip,[1]−α)22(κ−1)−β(17)κ=1−ν22lnβ1+βwhereα<0,β>0,νare the shape parameters of the learning functionL(•)that can be obtained empirically [22]. Effect of the shape parameters values on clustering results can be found in [3].In multilayered spiking neural network originally proposed [1], the learning algorithm (14) adjusts weights between neurons of two adjacent layers while there is a separate learning algorithm that adjusts weights of lateral connections in order to handle irregular, complex form of clusters. The concept of lateral connections learning algorithm is to adjust the weights the way to achieve synchrony in firing of neighbor neurons (in temporal distance sense) [1]. The same concept can be implemented in more natural way, just by generalizing the learning algorithm (14) to get it updating not only spiking neuron-winner, but also its neighbors [10]. This approach is known as ‘Winner-Takes-More’ rule. It implies that there is a cooperation phase before weights adjustment. Neuron-winner determines a local region of topological neighborhood on each learning epoch. Within this region, the neuron-winner fires along with its neighbors, and the closer a neighbor is to the winner, the more its weights are adjusted. The topological region is represented by the neighborhood functionφ(|Δtjj˜[r]|)that depends on difference(|Δtjj˜[r]|)between the neuron-winner firing timetj˜[r](x(k))and the neighbor neuron firing timetj[r](x(k))(distance between the neurons is in temporal sense) and a parameter that defines effective width of the region. As a rule,φ(•)is a kernel function that is symmetric about its maximum at the point whereΔtjj˜[r]=0. It reaches unity at that point and decreases asΔtjj˜[r]tends to infinity.The functions that are the most frequently used as neighborhood function are Gaussian, Epanechnikov, Mexican Hat, and many others. Instead of those conventional neighborhood functions, we propose to use adaptive wavelet (2) that can take different forms, from Gaussian to Mexican Hat, according to value of adjustable parameterα.Thus, the learning algorithm based on ‘Winner-Takes-More’ rule can be expressed in the following form for evolving spiking wavelet-neuro-fuzzy self-learning system:(18)wjgp,[r](K+1)=wjgp,[r](K)+ηw(K)φ(|Δtjj˜[r]|)L(Δtjgp,[r])whereΔtjj˜[r]is a temporal distance defined as follows:(19)Δtjj˜[r]=tj˜[r](x(k))−tj[r](x(k))Analysis of competitive unsupervised learning convergence showed that width parameter of the neighborhood function should decrease during synaptic weights adjustment [24]. For the adaptive wavelet neighborhood function(20)φ(|Δtjj˜[r]|,K)=(1−αjj˜τjj˜2)e(−jj˜2/2)where(21)τjj˜2=(Δtjj˜[r])2ρ2(K)width parameterρcan be adjusted as follows:(22)ρ(K)=γρ(K−1),0<γ<1Learning algorithm (18) requires modification of multilayered self-learning spiking neural network architecture originally suggested [1] as described in [10]. Adjustable lateral connections in each hidden layer of spiking neurons should be replaced with signal transmitting connections that will fire neighbor neurons along with neuron-winner on each epoch of learning stage according to the algorithm (18).Since the proposed evolving spiking neural network solves general task of hierarchical clustering, it can be used in any area where it is required to process data in unsupervised way (one of the major areas here is image processing and segmentation).In order to check the introduced data processing system efficacy, it was used to perform clustering of two overlapping clusters of complex form (Fig. 3).The system output during learning is shown on Fig. 4a–c. Initial network architecture contained population coding layer (two pools, each consisting of 5 receptive neurons with the adaptive wavelet activation-membership functions) and a layer of 8 spiking neurons. The second layer of 5 spiking neurons was added after visual inspection of output results on the second epoch. On the subsequent learning, the second layer detected 4 clusters (two spiking neurons were not activated due to the spiking network adaptability [22]). Next layer of 3 spiking neurons was added after the fifth learning epoch (one neuron was not activated) and the learning was stopped on the seventh epoch. The same dataset was processed for comparison by hyperellipsoidal method that is known as a powerful tool to detect complex form clusters [25].The proposed evolving spiking wavelet-neuro-fuzzy self-learning system was used to process a realistic image and compared with robust fuzzy clustering method [26] (Fig. 5). Training set was generated by 25% of randomly selected pixels (Fig. 5b). The initial evolving system included layer of 8 spiking neurons, the second layer of 4 spiking neurons was added after the first learning epoch. The system output after the second epoch is presented on the Fig. 5c. 8 clusters were considered for robust fuzzy clustering method as well. Its output after 42-nd learning epoch is shown on Fig. 5d.It is worth to note that the expert decisions on adding spiking neurons layers (as well as setting the initial number of spiking neurons in the first layer) play some significant part in achieving good efficacy of finial data clustering and in case of high-dimensional data set, when it is hard for expert to make decision on next layer adding, the efficacy may be affected. This leads to necessity of introducing more robust criteria for spiking neural network evolution.

@&#CONCLUSIONS@&#
Evolving spiking wavelet-neuro-fuzzy self-learning system proposed in this paper presents several innovations in spiking neural network theory development. Spiking neural network is presented as a basis for evolving self-learning system. Activation functions of receptive neurons and the temporal Hebbian learning algorithm are improved and generalized based on the adaptive wavelet activation-membership functions. The proposed system ensures high performance and high quality clustering of data sets containing clusters of arbitrary form. Further studies might continue to introduce analytical criteria (and not expert knowledge only) for the number of spiking neurons layer determination based on clustering quality measure.