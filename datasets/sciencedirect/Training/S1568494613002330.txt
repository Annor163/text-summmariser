@&#MAIN-TITLE@&#
Performance of radial basis and LM-feed forward artificial neural networks for predicting daily watershed runoff

@&#HIGHLIGHTS@&#
ANN models provided more accurate predictions compared with the MLR model.Using upstream records could significantly increase the accuracy of ANN and MLR models.LMNN model is superior to RBF for base and high flow.RBF model is superior to LMNN model for simulating flood events.The applied models under-predicted the flow rates.

@&#KEYPHRASES@&#
Artificial neural network,Multiple linear regression,Runoff prediction,K-fold cross validation,Mann-Whitney test,Levene's test,

@&#ABSTRACT@&#
This study investigated the effects of upstream stations’ flow records on the performance of artificial neural network (ANN) models for predicting daily watershed runoff. As a comparison, a multiple linear regression (MLR) analysis was also examined using various statistical indices. Five streamflow measuring stations on the Cahaba River, Alabama, were selected as case studies. Two different ANN models, multi layer feed forward neural network using Levenberg–Marquardt learning algorithm (LMFF) and radial basis function (RBF), were introduced in this paper. These models were then used to forecast one day ahead streamflows. The correlation analysis was applied for determining the architecture of each ANN model in terms of input variables. Several statistical criteria (RMSE, MAE and coefficient of correlation) were used to check the model accuracy in comparison with the observed data by means of K-fold cross validation method. Additionally, residual analysis was applied for the model results. The comparison results revealed that using upstream records could significantly increase the accuracy of ANN and MLR models in predicting daily stream flows (by around 30%). The comparison of the prediction accuracy of both ANN models (LMFF and RBF) and linear regression method indicated that the ANN approaches were more accurate than the MLR in predicting streamflow dynamics. The LMFF model was able to improve the average of root mean square error (RMSEave) and average of mean absolute percentage error (MAPEave) values of the multiple linear regression forecasts by about 18% and 21%, respectively. In spite of the fact that the RBF model acted better for predicting the highest range of flow rate (flood events, RMSEave/RBF=26.8m3/s vs. RMSEave/LMFF=40.2m3/s), in general, the results suggested that the LMFF method was somehow superior to the RBF method in predicting watershed runoff (RMSE/LMFF=18.8m3/s vs. RMSE/RBF=19.2m3/s). Eventually, statistical differences between measured and predicted medians were evaluated using Mann-Whitney test, and differences in variances were evaluated using the Levene's test.

@&#INTRODUCTION@&#
Time-series forecasting is an important research and application area in hydrological events. Much effort has been devoted over the past several decades to develop and improve time-series forecasting models. Traditionally, the multiple linear regression technique has been one of the most widely used models in simulating hydrological time series. However, when the nonlinear phenomenon is significant, the multiple linear will fail to develop an appropriate predictive model. ANNs provide an analytical substitute for traditional techniques, which are often limited by strict assumptions of linearity, variable independence, etc. [1,2]. Lately, ANNs have been appropriately used to forecast time-series data [3,4]. There are plenty of examples for the application of ANNs and artificial intelligence techniques in the hydrology which mainly concentrate on predicting and estimating stream flows [5–10].Piotrowski and Napiorkowski verified the applicability of a number of developed evolutionary computation optimization methods to multi-layer perceptron neural networks training for daily rainfall–runoff forecasting [11]. Talei et al. investigated the effect of inputs used on event-based runoff forecasting by adaptive network-based fuzzy inference systems [12]. Maheswaran and Khosa attempted to forecast monthly stream flow by combining wavelet decomposition with Volterra models [13]. Guo et al. tried to forecast monthly streamflow based on an improved support vector machine model [14]. In another study, a multi-model approach for long-term runoff modeling was applied using a recurrent fuzzy system, with parameters estimated by a genetic algorithm using observed rainfall as input [15]. Nourani et al. linked the wavelet analysis to the ANN concept for modeling Ligvanchai watershed rainfall–runoff process [16]. In another research, the genetic-programming approach was applied to flow prediction for the Kirkton catchment in Scotland (U.K.) [17]. Rezaeian-Zadeh et al. predicted monthly discharge volume by different ANN algorithms including resilient back-propagation, scaled conjugate gradient, variable learning rate, and Levenberg Marquardt in semi-arid regions [18].It is observed that the prediction of the stream flow of a river in one location from the upstream flow records is intermittent in the literature, though such prediction is pretty beneficial in practice for finding the flow rates in a location where a river works such as a reservoir or a weir is to be constructed. Kerh and Lee introduced a back propagation neural network model for predicting flood discharge in an unmeasured station using the records of an upstream station [19]. Turan and Yurdusev applied ANNs and a neuro-fuzzy model to estimate the missing data of the daily flows of Birs river in Switzerland [20]. A particle swarm optimization model was adopted to train perceptrons by Chau [21]. The approach was applied to predict water levels in Shing-Mun River on the basis of the upstream gauging stations or stage/time history at the specific station [21].This paper investigated the capability of ANNs and influences of upstream stations on predicting one day ahead watershed flows. Streamflow data of 5 stations of Cahaba watershed (located in Alabama, the USA) were implemented for model application. The simulation models were based on two most common ANNs: (1) multi-layer feed forward neural network using Levenberg–Marquardt learning algorithm and, (2) radial basis function (RBF) neural network, as well as, (3) multiple linear regression model. The Levenberg–Marquardt algorithm was used to train the ANN models in this study because of being fast, accurate and reliable [22]. In the end, the modeling processes and performance of the ANNs and MLR models were described and compared with the corresponding data.In this study, two of the most common neural network structures were chosen for modeling: the multi-layer feed forward neural network using Levenberg–Marquardt learning algorithm (LMFF) and the radial basis function (RBF) neural network. Moreover, multiple linear regression model (MLR) was also used to predict the next day Cahaba watershed runoff using records of four upstream stations.Many problems in engineering sciences involve exploring the relationships between two or more variables. Regression analysis is a statistical technique that is very useful for these types of problems. Many applications of regression analysis involve situations in which there are more than one regressor variable. A regression model that contains more than one regressor variable is called a multiple regression model. The following relation describes target (Y) as a function of other independent parameters Xiin an MLR approach [23].(1)Y=P0+P1X1+P1X2+…+PnXnwhere Pi(i=0,…,n) are the parameters generally estimated by least squares method and Xi(i=1,…,n) are the explanatory variables (predictors). Multiple linear regression analysis was developed and tested with the same data sets used for the ANN models and the developed regression equations were referred to as trained models. Then, the predictive aptitude of the model was also validated with the same data sets used to test the ANN models, therefore, making the results comparable [24].ANNs are inspired by biological brain and nervous system. ANNs contain a set of processing elements, so-called neurons, which operate in a parallel procedure. The connections between the neurons make the task of ANN. Furthermore, ANNs are a proved and efficient method for modeling complex input–output relationships [25], which learn the relationship directly from the data being modeled [3].The RBF simulates the unknown function using a network of Gaussian basis functions in the hidden layer and linear activation functions in the output layer [26,27]. Fig. 1shows a schematic diagram of a general RBF network. The input layer is composed of n input nodes and the hidden layer consists of j locally tuned units and each unit has a radial basis function acting like a hidden node. The hidden node output zj(x) calculates the closeness of the input and projects the distance to an activation function. The activation function of jth hidden node used in this study was the Gaussian function given by:(2)zj(x)=exp−||x−μj||22σj2where x is known as input vector; μjis the center of the radial basis function for the hidden node j; ||x−μj|| denotes the Euclidean distance between the center of the radial basis function and input and σjis a parameter for controlling smoothness properties of the RBF. The third layer of the network is the output layer with L nodes that are fully interconnected to each hidden node. The output of the network is the sum of the linear weighted zj(x).(3)yl=∑j=0Jwljzj(x);z0(x)=1where ylis the lth component of the output layer;wljis the synaptic weight between the jth node of hidden layer and the lth node of output layer.Investigating the recent scientific works clarifies that the feed forward approach is the most popular neural network architecture which is used nowadays [22]. In feed forward neural networks, the learning procedure tries to find a set of weights which gives a mapping that fits well with the training set. The following function as an arbitrary non-linear function could be assumed:(4)F(x,w)=ywhere x is the input vector presented to the network, w is the weight of the network and y is the corresponding output vector which is approximated or predicted by the network. The weight vector w is commonly ordered first by layer, then by neurons and finally by the weights of each neuron plus its bias. Neural networks can be viewed as highly non-linear functions. From this perspective, the training problem can be considered a general function optimization problem with the adjustable parameters of weights and biases of the network and the Levenberg–Marquardt can be straightly applied in this case. The objective of training is to reduce the global error E defined as:(5)E=1N∑n=1N(y−t)2where N is the total number of training patterns and eiis equal to the error for training pattern n and t is the target [6]. The Levenberg–Marquardt (LM) algorithm is the most widely used optimization algorithm. The LM algorithm is an iterative technique which locates the minimum of a function that is expressed as the sum of squares of nonlinear functions. It has become a standard technique for nonlinear least-squares problems and can be thought of as a combination of the steepest descent and the Gauss–Newton method [28]. It outperforms simple gradient descent and other conjugate gradient methods in a wide variety of problems. It can be seen that simple gradient descent and Gauss–Newton iteration are complementary in the advantages they provide. The update rule is a blend of the above mentioned algorithms which is given below:(6)xk+1=xk−[JTJ+ηI]−1∇fwhere f is assumed a quadratic function and H is the Hessian matrix evaluated at xk. The Jacobian matrix J can be computed through a standard back-propagation technique which is much less complex than computing the Hessian matrix H=JTJ. The gradient can be computed as ∇f=JTe and e implies the vector of network errors. When the scalar η is zero, the method becomes equivalent to the Gauss–Newton's method whereas, for large η, the LM algorithm tends toward steepest descent algorithm. Gauss–Newton's method is faster and more accurate near an error minimum; so, the aim is to shift toward Gauss–Newton's method as quickly as possible. The performance of the update is compared with that of a linear model. If the error goes down following an update, it implies that the quadratic assumption on f(x) is working and η is reduced to relax the influence of the gradient descent. If the objective function increases, the test point is not accepted but lambda is also increased. Fig. 2shows a typical view of an LMFF network with one hidden layer.In the proposed LMFF model, hidden nodes with appropriate nonlinear sigmoid transfer functions (tansig) were used to process the information received by the input nodes.In this study, Cahaba River located in the Cahaba Basin was selected as the study site. The Cahaba is one of the most biologically rich rivers in the nation. Beneath its waters, the Cahaba is home to 131 different species of fish, more species per mile than any river of its size in the country. It also provides habitat for fresh-water mussels and aquatic insects found nowhere else in the world [29]. The Cahaba River is the longest free-flowing river in Alabama and is among the most scenic and biologically diverse rivers in the United States. With headwaters near Birmingham, AL, the Cahaba meanders to the southwest; then, at Heiberger, it turns southeast and joins the Alabama River at Cahaba, Alabama (in Dallas County). On its southwesterly journey, the river passes through parts of St. Clair, Jefferson, Shelby, Bibb, Tuscaloosa, Chilton, Perry and Dallas counties. Contained entirely within central Alabama, the Cahaba River is 191 miles (307km) long and drains an area of 1,870 square miles (4800km2) (Alabama Department of Environmental Management Water Quality Branch, Water Division, 2006). Fig. 3illustrates the location of Cahaba River running through the Cahaba watershed in the AL state.In this study, the daily flows of Cahaba River recorded in Marion Junction flow station were predicted considering the previous values of the four upstream stations of near Suttle, Centreville, Helena and Trussville. Table 1shows the general information of these stations which are tabulated from the most upstream station (Trussville) to the most downstream (Marion Junction) one. According to Table 1, the most downstream station had the lowest altitude and the widest drainage area. Moreover, the location of the 5 stations on the Cahaba River is depicted in Fig. 4a.Considering the drainage area of each station (Table 1), it is obvious that the flow characteristics of the upstream stations in terms of average and maximum of discharge were milder than the downstream stations. This issue can be confirmed according to Table 2; for a better perspective of the watershed, the stream flow characteristic of each station is shown in the form of individual value plot in Fig. 4b during (2000-1-1 to 2010-1-1). According to the plot of Fig. 4b, it can be judged that the stream flow behavior of the Suttle station was considerably similar to the Marion Junction whereas the farthest station (Trussville) had the least similarity to the Marion junction station among 4 upstream stations. In this study, a wide range of the historical data of 3654 daily flows of the Marion Junction station (station of interest) and the 4 upstream stations (Trussville, Helena, Centreville and Suttle) from the period of 01/01/2000 to 01/01/2010 were gathered from the United States Geological Survey (USGS) website.The experiment was conducted by splitting the samples into two sets: training and testing samples. To minimize the bias associates with the sampling of the training and testing data, the K-fold cross validation (CV) technique was employed. The implementation of the 5-fold CV was carried out by splitting the data into five sets. Four sets (80%) were used for training and the remaining set (20%) was used for testing. The training process was repeated for five times, at each time one of the sets was used as testing set. The structures of data set using K-fold CV are shown in Fig. 5.The statistical parameters of stream flow data of the station of interest (Marion Junction) and upstream stations are shown in Table 2 (Mean, Maximum, Minimum and standard deviation). All these statistics indicate the volatility of the discharge phenomenon of each station. Fig. 6illustrates the behavior of flow rate for Marion Junction.Before training ANNs and MLR, input data must be preprocessed. It was intended to develop models for predicting watershed runoff using observations which occurred previously in the station of interest in combination with other stations’ stream flow. Exploratory analysis of the runoff data was undertaken using correlation analysis which dealt with auto-correlation for all the five stations and cross-correlation between the station of interest and four upstream stations. This technique involved the use of auto-regressive moving average models to select input variables and lag times and a stepwise correlation analysis for determining appropriate combinations of inputs from these variables [30].The correlation analysis of the time series was employed for sighting the effects of the preceding flows. The auto-correlation (AC) and partial auto-correlation (PAC) statistics and the corresponding 95% confidence bands from Lag0 to Lag20 were estimated for the stream flow data for all the stations. Fig. 7illustrates the variations of AC and PAC for the Marion Junction station. It can be seen from Fig. 7 that the PAC function had significant correlation up to Lag 3 and, afterwards, fell within the confidence limits. The rapid decaying pattern of the PAC function confirmed the dominance of the auto-regressive process relative to the moving average process. The analysis suggested that the three antecedent flow values for all stations were adequate in the input vector to the ANNs [6].In order to make the training process of the ANN more effective, in addition to the historical data of the station of interest, the available data of other measuring stations were considered. As mentioned earlier, according to the regression analysis, time series modeling with a 3-day lead time was appropriate for all the stations. But, the question remained as: what is the best lead time between Marion Junction and other stations data? For that, the cross-correlation between rivers discharge was used to identify the time lag (offset) in which similarity was the highest. The cross correlations between Marion Junction and other upstream stations are shown in Fig. 8. Three maximum peaks of each test were selected for modeling in this study. Table 3shows the three highest amounts of cross-correlation between Marion Junction and other stations at time t(0).According to Table 3, a good correlation was noted for watershed runoff between the Marion Junction station and either one of the upstream stations when the upstream data were lagged by appropriate time intervals. Employing a longer lag time between each pair of stations could potentially include a part of the upstream data that exhibited no relationship with the downstream flows. It would also prolong the training time and reduce model performance [26]. Analyzing Table 3 indicates that the best cross-correlation between the flow rate in a specific day in Marion Junction stations and other stations began with preceding 4 days in Trussville to 1 day ahead at the Suttle station. A qualitative examination of the cross correlation curves revealed that the correlation was the highest for the lag of 0-day in Suttle, 1-day in Centreville, 2-day in Helena and 3-day in Trussville stations, implying that the discharges at a certain time level could be influenced heavily by the current one, two and three previous streamflows of the each station. These effective lag times were consistent with the distances (see Fig. 4) and slopes (see gauge datums in Table 1) between the stations. On the other hand, it could be expected that using the mentioned lead time on Table 3 would give the best results for the training daily data using ANN models. Before applying the ANN methods, the input data were normalized in the range 0–1. These normalized data were used to train and the test ANN and MLR models.The model structure in terms of input and output parameters for LMFF, RBF and MLR models is discussed as follows. In the text, M, S, C, H and T stand for Marion Junction, Suttle, Centreville, Helena and Trussville measuring stations, respectively. Also, subscript t determines the current day. Six different structures were considered for predicting one day ahead streamflow of the Cahaba River near Marion Junction station. Table 4shows different combinations of input data for each network. In all the cases, the output layer has only one neuron, which implies the next day streamflow of the Marion Junction station (QM(t+1)).The output parameter is Q(t+1) which refers to the next day discharge. In all stations, 3 previous streamflows were used as inputs to the models according to the PAC analysis. The time lag between Marion junction station and other stations was chosen according to the cross-correlation test (Table 3). In Table 4, QM(t, t−1, t−2) indicates streamflows of the Marion Junction station at time t, t−1 and t−2. However, it is obvious that, in the predicting process, there was no data for the next days for each station. Thereafter, as for the Suttle and Centreville stations, the input form of Q(t, t−1, t−2) was used instead of Q(t+2, t+1, t) and Q(t+1, t, t−1), respectively.In the current study two modifications were applied in improving the generalization performance of the neural networks, which are (a) modification of the generally used objective function; and (b) the weight initialization method.Most supervised neural networks are trained by minimizing the mean squared error (MSE) of the training set. The use of SSE/MSE in data modeling is commonly known as the least mean squares (LMS) method. However, supervised learning algorithms that use the LMS method make implicit assumptions such as normality and independence about the error. In addition, in the presence of outliers, the resulting neural network model can differ significantly from the underlying system that generates the data. Moreover, an uneven distribution of error size across the entire flow range was found with this criterion and the generalization capacity for the test patterns when flow is very small or very large was found to be less accurate than that of other test patterns [31]. One reasonable remedy is to keep the influence of most of the normal data unchanged while simultaneously suppressing the impact of noise and gross errors. Referring to the work of statisticians, Liano introduced the mean log squared error (MLSE) function [32]:(7)MLSE=1n∑i=1nlog1+12(ei)2whereei=xi−x⌢iis the reconstruction error. MLSE acts as a substitute for the MSE to make the model robust against potential gross errors in the original data set.The weight initialization influences the speed of convergence, the probability of convergence and the generalization. Under the point of speed of convergence, a particular initialization value can be closer or farther than another different value to the same final local minimum. So, the number of iterations of the training algorithm and the convergence time will vary depending on the weight initialization [4]. In this research, statistically controlled activation weight initialization (SCAWI) was applied for model initialization. This method used the meaning of paralyzed neuron percentage. This concept may be defined by testing how many times a neuron is in a saturated state and the magnitude of at least one output error is high.(8)Wijinput=1.31+Ninput.v2.rijIn the equations v is the mean squared value of the inputs and rijis a random number uniformly distributed in the range [−1, +1].The performance of ANN models can be evaluated using different statistical tests that describe the errors associated with the model. After calibrating each of the model structures using the training–testing data set, the performance can be then evaluated in terms of these statistical measures of goodness of fit. In order to provide an indication of goodness of fit between the observed and modeled values, the coefficient of root mean squared of error (RMSE), correlation coefficient (r) and the mean absolute error (MAE) can be used [22]. The ideal values of RMSE and MAE are zero. This way, a value near to zero indicates a better performing model. Absolute values for the r range from 0 to 1, with higher values indicating better model agreement. These statistical indicators can be expressed mathematically as follows:(9)r=∑i=1n(Oio−O¯o)(Oip−O¯p)∑i=1n(Oio−O¯o)∑i=1n(Oip−O¯p)(10)RMSE=∑i=1n(Oio−Oip)2n(11)MAE=1n∑i=1nOip−OioIn the above relations,Oiois the observed values (target),Oipis the predicted values (network output),O¯is the average of values and n is the total number of events. K-Fold cross-validation is one of the most commonly used methods for evaluating the results. As mentioned earlier, the data are split into K parts, with K−1 parts as the training set and 1 part as the testing set. The process is repeated K times. The advantage of this strategy is that all the past data have the chance to be trained and evaluated, giving the average error of the model which determines the accuracy of the method:(12)rAve=1K∑E=1KrE,RMSEAve=1K∑E=1KRMSEE,MAEAve=1K∑E=1KMAEEwhere K is the number of folds and k is the index of each fold.Before dealing with the real world application, the following well known test functions were used to compare the performance of the LMFF and RBF algorithms.In the peaks function approximation problem, 20×20=400 points (Fig. 9a) are applied as training set, in order to predict the values of 100×100=10,000 points (Fig. 9b) in the same range. The surface is generated by MATLAB function peaks and all training/testing points are uniformly distributed [33]. Evaluation of the results in terms of mean square error is shown in Table 5. It can be seen that RBF neural network acted better than the LMFF for both training and testing sets.The Hénon map is a discrete-time dynamical system. It is one of the most studied examples of dynamical systems that exhibit time series chaotic behavior. The Hénon Map is defined by the following equations:(13)xt+1=1−axn2+yn;yn+1=bxnAs the initial configuration, the parameters are set as a=1.4 and b=0.3 (Fig. 10). In this study the whole time series has 400 values with the 12 missing data. The missing data are divided into four blocks as follows: 98–100, 198–200, 298–300, and 398–400.The predictive error is described by MSE criteria:(14)MSE=∑t=98100(et−eˆt)23+∑t=198200(et−eˆt)23+∑t=298300(et−eˆt)23+∑t=398400(et−eˆt)23where e is the real value of the signal,eˆis the predicted value and t is the time step. The criterion MSE describes the prediction error for all 12 missing values. The results show that LMFF (2,6,1) performs slightly better than RBF (2,5,1) considering MSE equals 0.23 versus 0.27. Although RBF outperformed LMFF in simulating surface approximation, but it was not as good as LMFF in prediction variables of time series problem.To consider models fitness and generalization ability, the averages of error measurements (rave, RMSEave, MAEave) were used as the performance criteria by means of 5-fold cross validation. Models were built based on 4 of them, and checked by the remaining one. This process was repeated five times with different combinations of training and checking data sets. The most common way for identifying the architecture of an ANN associated with determining the number of neurons in each layer is the trial-and-error approach [34]. In this study, the technique used to determine the architecture of the ANNs was to start with a small network (one hidden layer and 3 hidden nodes) to increase the number of hidden nodes and select the network with the best performance. During the training process, the RMSE error on the testing set was monitored. When the testing RMSE was amplified, the training was stopped and the minimum of the testing error was taken as an indicator for the performance. In this study, several network structures were applied according to the conditions. It is worth noting that all of the networks had a single output neuron which was the watershed runoff. Accordingly, both LMFF and RBF networks with 3, 4, 6, 8, 10, 12, 16 and 20 hidden neurons were considered here. The best ANN architecture was selected (related to the minimum RMSEave) and tabulated for each model (Tables 6 and 7).In Tables 6–8, the RMSEave, MAEave and rave statistics of ANNs (LMFF/RBF) and MLR models are given. Both LMFF-6 and RBF-6 models had the smallest MAEave of 8.80 and 8.86m3/s, RMSEave of 18.80 and 19.23m3/s and the highest rave of 0.987 and 0.985 for the test period. As can be obviously seen from Tables 6 and 7, the LMFF model provided slightly better performance than the RBF from the RMSEave, MAEave and rave viewpoints. However, the results of MLR-6 model were inferior to those of the LMFF-6 and RBF-6 models (MAEave=11.19m3/s, RMSEave=22.92m3/s and rave=0.978).The results of the historic graphical outcomes of the applied models on the test data (fold#5) versus the observed flow rate are presented in Figs. 11 and 12. Scatter plots of LMFF-NP4 and RBF-NP4 in training and testing periods of the prediction part (fold#5)Fig. 12 nicely demonstrates that ANN models’ performance was generally accurate and good and almost every data point was quite near the line of agreement. Both ANN models as well as MLR model generally tended to under-predict the simulated base flow (discharge<200m3/s, 92.4% of the total data) and high flow (200<discharge<500m3/s, 6.3% of the total data) where most of the simulated data were placed under the agreement line. As for the flood currents (discharge>500m3/s, 1.3% of the total data), the applied models had a central tendency for simulating floods with the exception of the high flood event (discharge>900m3/s), where MLR totally under-predicted the simulated data while ANNs performed much better in those area (dotted-circles in Fig. 13).It can be concluded from Tables 6–8 that the best ANN model for the Cahaba watershed runoff prediction was LMFF-6. Similarly, the best radial basis function model was RBF-6. It can be also concluded that, for all modeling parts, the LMFF showed slightly better performance than the RBF.The best MLR and ANN models used the advantage of the previous observations of the Marion Junction station QM(t−1, t−2, t−3) as well as other stations. According to Tables 6 and 7, using the upstream records for predicting stream flow would improve the model outcomes, even using data of the farthest station (Trussville). The sorted flow rates versus corresponding and relative(Oio−Oip)/(Oio)residuals for testing period of the 5th fold are also provided in Fig. 13. It can be seen that the highest magnitude of relative residuals corresponded to the lowest amount of discharges (base flow). Although ANNs and MLR models had a tendency to under-predict stream flows, the over-predicted values had the highest relative errors. To have a better perspective, the results are tabulated in Table 9.As can be seen from graphs in Fig. 13, the predicted base flow values by LMFF are closer to the zero lines than those of the RBF and MLR. Table 9 indicates that RMSEave criterion for the base flow is 12.3m3/s which is lower than those of the RBF and MLR models (14.12 and 16.41m3/2) and the maximum amount of relative error (Max RR) of −127.3% has a better result than the RBF model (143.2%). It is clear from Fig. 13 that the residuals of the LMFF model show less violation than the RBF and MLR. This matter was confirmed by the results of Table 9 where LMFF had the lowest amount of RMSEave (12.3m3/s) and maximum relative error (41.2%).In general, LMFF model was found to be superior to the RBF and MLR models in prediction of one day ahead base and high flows. Anyway, contrary to the former results, the RBF model outperformed the LMFF and MLR models in predicting flood events (Fig. 13). Table 9 demonstrates the superiority of RBF model with the lowest RMSE of 26.79m3/s and maximum relative error of −6.1%.The non-parametric tests were employed to assess the models performance for testing set in predicting streamflow. The tests included Mann-Whitney test and Levene's test. The (Wilcoxon-) Mann-Whitney test is the non-parametric equivalent of a pooled 2-Sample t-test. The test assumes you have two independent samples from two populations, and that the samples have the same shapes and spreads, though they don’t have to be symmetric. The Mann-Whitney procedure is a statistical test of the difference between the two medians (η1 and η2) under the null hypothesis that they are equal.The test result is given in terms of p-values. The null hypothesis is that if the p-value is above the critical value (typically 0.05) at 95% confidence level, two populations have same median which means that they come from the same distribution. On the other hand, the Levene's test is used to check the variances of two populations, and whether they are equal in the case of continuous and not normally distributed data. When the p-value in Levene's test is less than the critical value, then the null hypothesis is rejected and it is said that there is a variability in variance between the two populations. Results are tabulated in Table 10.Results show that there is no significant difference for the medians and variances at the 95% confidence interval between the predicted values of the LMFF, RBF and MLR models and observed ones. However, the p-values of the Mann-Whitney for MLR model is low, and choosing 80% confidence interval indicates that there is statically difference for the median.

@&#CONCLUSIONS@&#
In this study, LMFF and RBF neural network models were developed to predict one day ahead stream flow of the Marion Junction station in Cahaba watershed (Alabama-USA). For achieving this goal, an investigation was carried out to prove the potential of using upstream discharge records for producing higher accuracy of prediction. Four upstream streamflow stations (Trussville, Helena, Centerville and Suttle) in the Cahaba watershed were considered. In order to obtain true and effective evaluation of the performance of ANN models, the same model structures were also trained and tested by multiple linear regression models using K-fold cross validation technique. In this study, exploratory analysis of the runoff data was undertaken using correlation analysis (auto-correlation and cross-correlation analysis). The cumulative results presented in this paper suggested that the ANN models performed better than the MLR model in prediction of one day ahead streamflows. The results showed that the performance of stream flow forecasts strongly depended on the upstream records. The results of the relative residuals analysis confirmed the superiority of the LMFF model in base and high flow prediction and superiority of the RBF model in simulating flood events. In general, analyses demonstrated that the applied models under-predicted the flow rates, which was more evident for the lower range of discharge. The usual notions underlying significance tests for evaluating the difference in medians (Mann-Whitney test) and variances (Levene's test) between observed and predicted values were also extended. However, both tests showed no significant differences at the level of 95% confidence in the predicted daily medians or variances of the streamflow.