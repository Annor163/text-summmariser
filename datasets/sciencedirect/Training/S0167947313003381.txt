@&#MAIN-TITLE@&#
Model based clustering of customer choice data

@&#HIGHLIGHTS@&#
A two-level finite mixture model for clustering customers and products is proposed.Clusters of products nested in segments of customers are determined.Customer/product features influence the allocation to segments/clusters.An appropriate EM algorithm is presented for the special case of purchase counts.The application shows high purchase rate segments linked with clusters of products.

@&#KEYPHRASES@&#
Model-based clustering,Conditional logit,Multinomial logit,Co-clustering,Bi-clustering,

@&#ABSTRACT@&#
In several empirical applications analyzing customer-by-product choice data, it may be relevant to partition individuals having similar purchase behavior in homogeneous segments. Moreover, should individual- and/or product-specific covariates be available, their potential effects on the probability to choose certain products may be also investigated. A model for joint clustering of statistical units (customers) and variables (products) is proposed in a mixture modeling framework, and an appropriate EM-type algorithm for ML parameter estimation is presented. The model can be easily linked with similar proposals appeared in various contexts, such as co-clustering of gene expression data, clustering of words and documents in web-mining data analysis.

@&#INTRODUCTION@&#
We propose a model-based approach to cluster individuals and products in disjoint individual- and product-specific groups, where the corresponding partitions are dependent. We will refer to individual-specific groups as segments, while the product-specific groups will be referred to as clusters. The motivation arises from empirical situations where customer data are analyzed to investigate on factors affecting the purchase behavior towards several products. The idea is to define individual-specific segments which are homogeneous in terms of customer product choices; the prior (conditional) probability for an individual to belong to a given segment is assumed to be a function of individual-specific covariates, and we are interested in investigating how such characteristics affect the segment memberships. We can also imagine that, within an individual-specific segment, a partition of the products may be identified depending on their characteristics. For example, customers with a given purchase profile may prefer a particular subset of products because of their features and such preferences may vary within segments of customers. In this view, we may be interested in studying whether individuals in a specific segment (representing a prototypical purchase behavior) choose specific subsets of products for their features. In this perspective, we aim at jointly partitioning customers and products to investigate about the determinants of the customer choices. This purpose might be linked with methods for joint partitioning of genes and tissues (or experimental conditions) in microarray data analysis (see e.g.  Martella et al. (2008)), of words and documents in web data analysis (see e.g.  Li and Zha (2006)), or, in general, when latent block-based clustering is pursued (see e.g.  Govaert and Nadif (2003)). Further interesting links can be established with multi-layer mixtures, see e.g.  Li (2005), and with hierarchical mixture of experts models, see e.g.  Titsias and Likas (2002). Such methodological connections will be further discussed to better focus and motivate our proposal.The plan of the paper is as follows. In Section  2, the model is introduced in a general framework, and, in Section  3, a ML approach to parameter estimation is described. An EM-type algorithm is detailed in Section  4 in the context of observed count data. In Section  5, the analysis of a benchmark data set is proposed. In the last section, concluding remarks and the future research agenda are discussed.LetYi,i=1,…,n, be ap-dimensional random vector and letyi,i=1,…,nrepresent the corresponding realization in a sample of sizen; letY=(Y1,…,Yn)Tdenote the (n,p) matrix of the observed valuesyij, for individuali=1,…,nand variablej=1,…,p. Just to give an example, and without loss of generality, we may suppose to considerncustomers andpproducts, whereyijrepresents the number of items of thej-th product thei-th customer has bought in a given time interval.In addition, we assume that a set of outcome-specific (price, weight, type of package, etc.) and of individual-specific (age, gender, educational level, income, etc.) covariates have been also recorded. Letxiandzjdenote the vectors containing the characteristics of thei-th individual, and of thej-th product,j=1,…,p, respectively. In the following, for the sake of clarity, groups of individuals and products will be termed segments and clusters, respectively.We adopt a mixture model framework and start by assuming that the population consists ofGsegments in proportionsπ1,…,πG,∑g=1Gπg=1,πg≥0,∀g=1,…,G. An unobservableG-dimensional binary indicator vectorui=(ui1,…,uiG)is associated with each individual and has a unique non null element, indicating whether thei-th individual belongs to theg-th segment or not,i=1,…,n,g=1,…,G, see e.g.  Titterington et al. (1985). In such a mixture sampling scheme, the sample is obtained by first drawing, independently for each unit, the corresponding segment label,uig,g=1,…,Gfrom the population with probability density function (pdf)h(ui∣π); then, values of the outcome variables are drawn from the population with pdf given by(1)fg(yi∣θg)=f(yi∣uig=1),wherefg=f(yi∣θg)is theg-th segment-specific density with indexing parameter vector given byθg. As usual, the individual segment indicatorsui=(ui1,…,uiG)are assumed to be independent multinomial random variables with probabilities given byπ=(π1,…,πG). Thus, each observationyi,i=1,…,n, is sampled from the finite mixture density(2)f(yi∣π,θ1,…,θG)=∑g=1Gπgfg(yi∣θg).Let(yi,ui),i=1,…,nbe an observed sample drawn under such a sampling scheme; the so-called complete data density function is given by(3)f(yi,ui∣θ,π)=f(yi∣θ,ui)h(ui∣π)=∏g=1G[πgfg(yi∣θg)]uig,and the resulting complete-data log-likelihood may be expressed as follows(4)ℓc(θ,π)=∑i=1n∑g=1Guiglog[πgfg(yi∣θg)]=∑i=1n∑g=1Guig{log(πg)+log[fg(yi∣θg)]}.The estimation of the segment-specific parametersθg,g=1,…,Gand the priors,πg,g=1,…,G, is usually based on an EM-type algorithm. Such estimates help us to identify the segment-specific densities and, as a byproduct, to assign each individual to a segment, through a maximum a posteriori (MAP) rule. That is, thei-th individual is assigned to theg-th segment if the following condition on the posterior probabilities holds:Pr(θ=θg∣yi)=maxlPr(θ=θl∣yi)l=1,…,G.Let us assume that, within theg-th individual-specific segment (g=1,…,G), we may identify a partition of the products inKgclusters. Fig. 1may clarify what kind of partition of the observed data we are discussing about.We may assume that individuals exhibit different propensities towards products; specifically, within the sameg-th segment, individuals share similar propensities to purchase products belonging to a given cluster. That is, they exhibit similar behaviors when a given cluster of products is entailed: in this respect, different purchase propensities allow to group products inKgdifferent clusters. To define a product-specific partition within an individual-specific segment, we introduce an unobservableKg-dimensional product-specific cluster indicatorvjg=(vjg1,…,vjgKg)indicating the product-specific cluster thej-th product belongs to, when only individuals within theg-th segment are considered. Letπgk=Pr(vjgk=1∣uig=1),j=1,…,p,k=1,…,Kg,g=1,…,G, denote the prior probability that thej-th product belongs to thek-th product-specific cluster within theg-th individual-specific segment; this term may be interpreted as a sort of average probability that an individual in theg-th segment chooses a product in subsetk.Let us denote byπg,k=Pr(uigvjgk=1)=Pr(uig=1)Pr(vjgk=1∣uig=1)=πgπgk,g=1,…,G,k=1,…,Kg,i=1,…,n,j=1,…,pthe prior (joint) probability for thei-th individual in theg-th segment and thej-th product in thek-th cluster. In the present context, we may want to use some observed covariates to model the prior segment/cluster memberships; such covariates may include features of the choice alternatives (for example price, details, etc.) as well as of the individuals (income, age, gender, etc.). Starting from the segment priors, we do not define directly the termsπg,g=1,…,G, but, rather, their conditional counterpartsτig:(5)τig=τig(β)=Pr(uig=1∣xi)∝exp(xiTβg),i=1,…,n,g=1,…,G, whereβgis a segment-specific vector of regression coefficients, andβG=0holds to ensure model identifiability. Similarly, if product-specific covariates are available, we may define the conditional counterpart ofπgkas(6)τjgk=τjgk(γ)=Pr(vjgk=1∣uig=1,zj)∝exp(zjTγgk),j=1,…,p,k=1,…,Kg, wherezjis the observed vector of the product-specific covariates, whileγgkis a vector of cluster-specific regression coefficients, withγgKg=0to ensure parameter identifiability. Given this parameterization, the joint probability for thei-th individual in theg-th segment and thej-th product in thek-th cluster can be written as(7)τig,jk=Pr(uigvjgk=1∣xi,zj)=τigτjgk,i=1,…,n,j=1,…,p,g=1,…,G,k=1,…,Kg. Under such modeling assumptions, the conditional probabilities may be written as follows:log(τig)∝xiTβg,log(τjgk)∝zjTγgkandlog(τig,jk)∝xiTβg+zjTγgk.Expressions above define multinomial logit models describing the influence of the individual- and product-specific covariates on the probability that thei-th individual and thej-th product belong to a given (segment, cluster) class(g,k)from the whole set of∑g=1GKgclasses. Since each observation is sampled from a finite mixture density, the marginal density ofyican be written as follows:(8)f(yi∣θ)=∑g=1Gπgf(yi∣θg,)=∑g=1Gπg[∏j=1pf(yij∣θg)]=∑g=1Gπg{∏j=1p[∑k=1Kgπgkf(yij∣θgk)]}.In the expression above,θgk,g=1,…,G,k=1,…,Kg, is the class-specific parameter indexing the class-specific distributionfgk=f(yij∣uig=1,vjgk=1,θgk),representing the propensity that an individual in theg-th segment “chooses” a product in thek-th cluster, andθ={θgk,g=1,…,G,k=1,…,Kg}shortly denotes the whole model parameter set. Should individual- and/or product-specific covariates be available, the conditional density follows from Eq. (8) by substituting the marginal priors with the conditional ones:(9)f(yi∣xi,zj)=∑g=1Gτigf(yi∣θg)=∑g=1Gτig[∏j=1pf(yij∣θg)]=∑g=1Gτig{∏j=1p[∑k=1Kgτjgkf(yij∣θgk)]}.Thus, if thepproducts are assumed to be independent, conditional on a given segment, and the covariates are available for both individuals and products, the log-likelihood has the following form:(10)ℓ(β,γ,θ)=∑i=1nlog[∑g=1Gτig(β)∏j=1p∑k=1Kgτjgk(γ)f(yij∣uig=1,vjgk=1)]=∑i=1nlog[∑g=1Gτig(β)∏j=1p∑k=1Kgτjgk(γ)f(yij∣θgk)],whereβandγdenote the segment- and cluster-specific regression parameter vectors, respectively. While information onβandγcome from segment and cluster memberships, respectively, information on indexing parametersθgkcome from the classes identified by the couples(g,k),g=1,…,G,k=1,…,Kg. In this sense, the individual- and product-specific partitions are linked through the model parameters,θgk, which can be interpreted as the product-specific cluster deviations from the segment-specific gross parameters,θg. These may help to identify clusters of products that correspond to different purchase behaviors when compared to the average behavior of the individuals within theg-th segment.As remarked, the proposed model may be related to several proposals in the literature, mainly in multilevel latent class analysis, text- and web-mining, block clustering methods. Main differences with the co-clustering method proposed by  Govaert and Nadif (2003) and with the multilevel latent class model of  Vermunt (2007) refer to the hierarchical structure we assume, as well as to the corresponding parameterization.  Govaert and Nadif (2003) consider the block clustering problem and build a block partition which is defined as the Cartesian product of the row-specific and the column-specific blocks. By slightly rearranging their original notation, the corresponding log-likelihood can be written as follows:f(yi∣π,θ)=∑g=1Gπg∑k=1Kgπkfkg(yi∣θgk).As it can be easily observed, while the parameterization is similar to the one discussed in this paper, the assumption of independent partitions makes the difference crucial. By assuming conditional independence within segments, it follows:fkg(yi∣θgk)=∏j=1pfkg(yij∣θgk).In the proposal by  Govaert and Nadif (2003), the association between row- and column-specific partitions is accounted for by the row/column-specific parameter vectorθgk,g=1,…,G,k=1,…,Kg; however, the approach they propose does not take into account, at least not explicitly, that observed variables may be measured on different scales (for example fast-moving consumption and durable goods), even whenfkg≡f,∀g,k. Some similarities can be found with the multilevel latent class model proposed by  Vermunt (2007); also in this case, we have a hierarchy of partitions with clusters of occasions nested within individual-specific segments. The resulting structure is based on the same number of clusters for the occasions within the segments of individuals, while parameters may be cluster-specific but not segment-specific. The hierarchical structure we propose here is similar to the one displayed in Eq. (2) of  Vermunt (2007) where, however, a three-way (units, variables, occasions) structure is considered. By slightly rearranging the notation, and assuming that only one occasion is available, the model proposed by  Vermunt (2007) reduces to a standard finite mixture model:f(yi∣π,θ)=∑g=1Gπgfg(yi∣θ),since the parameter vector is not allowed to vary across individual-specific segments. To be more consistent with our proposal, let us assume that the occasions indicate different products: we do not have a multilevel structure, but simply a multivariate response. In this case, the model by  Vermunt (2007) corresponds to the following marginal density:f(yi∣π,θ)=∑g=1Gπg∏j=1p∑k=1Kπgkfk(yij∣θjk),where the number of the product-specific clusters does not vary across the individual-specific segments, and the cluster-specific parametersθjkvary only across the products and the product-specific clusters, see also Vermunt (2008).Further interesting connections with recent multilayer finite mixture models can be established. For example,  Li (2005) considers a two-way finite mixture of Gaussian distributions to allow for non-Gaussianity of the segment-specific densities; after some rearrangements, this model can be easily extended to non Gaussian multivariate responses by writing:f(yi∣π,θ)=∑g=1Gπg∑k=1Kgπgk∏j=1pfkg(yij∣θjkg).In this case, the number of product-specific clusters and the parameter vectors are allowed to vary with the individual-specific segments. The main difference with our proposal is that we do use mixed parametric densities to represent product-specific densities, and products are assumed to be independent conditionally on the individual-specific segment. Roughly speaking, the summation (onk) and the product (onj) are in the reverse order when compared with the proposal by  Li (2005). Finally, a further connection can be established with the work by  Martella et al. (2008), where the product-specific partitions are replaced by a dimensional reduction based on factor analyzers with a binary loading matrix.As usual, segment and cluster membership indicator labelsuigandvjgk,i=1,…,n,j=1,…,p,g=1,…,G,k=1,…,Kgare unobservable; thus, they can be considered as missing data and this naturally leads to the Expectation–Maximization (EM) algorithm for parameter estimation. The complete data vector for thei-th individual is given by (yij,ui,vjg),i=1,…,n,j=1,…,p. Assuming a multinomial distribution for the unobservable membership indicators,uiandvjg, the log-likelihood for the complete data can be written as(11)ℓc(⋅)∝∑i=1n∑g=1Guiglog(τig)+∑i=1n∑g=1G∑k=1Kg∑j=1puigvjgk{log(τjgk)+log[f(yij∣θgk)]}.In ther-th step of the EM algorithm, the log-likelihood for the observed data is defined by taking the expectation of the log-likelihood for the complete data over the unobservable component label vectorsuiandvjgiven the observed datayand the current maximum likelihood parameter estimatesΦ(r−1)={θ(r−1),β(r−1),γ(r−1)}.Letwig(r)=Pr(uig=1∣yi,Φ(r−1))andωijgk(r)=Pr(vjgk=1∣uig=1,yi,Φ(r−1))denote the posterior probability that thei-th individual belongs to theg-th segment and thej-th product belongs to thek-th cluster nested within theg-th individual-specific segment, conditional on the observed data and the current parameter estimates.Note thatξig,jk(r)=wig(r)ωijgk(r)=Pr(uigvjgk=1∣yi,Φ(r−1))holds. Thus, the computation of the expected value of the complete data log-likelihood (in the E- step of the EM algorithm) involves the updating of the posterior probabilities by means of an Upward/Downward-type algorithm. Even though not standard, we use this term, which has been introduced by  Vermunt (2007) to highlight the similarities with the Forward–Backward algorithms for ML estimation in hidden Markov models. It exploits the local independence assumption of the products given the memberships of the individuals within segmentg(i.e. conditionally on theg-th segment, thej-th product is independent of the other products). At ther-th step, replacinguigandvjgkby their conditional expectations, we may define the conditional expectation of the complete data log-likelihood given the observed data and the current parameter estimates as follows:(12)Q(r)(⋅)=EΦ(r−1)[ℓ(⋅)∣y]∝∑i=1n∑g=1Gwig(r)log[exp(xiTβg)1+∑l=1G−1exp(xiTβl)]+∑i=1n∑g=1G∑k=1Kg∑j=1pwig(r)ωijgk(r){log[exp(zjTγgk)1+∑t=1Kg−1exp(zjTγgt)]+log[f(yij∣θg)]}.The ML estimates for the model parameters may be derived through the maximization ofQ(r)(⋅)with respect toΦ={θgk,βg,γgk},g=1,…,G,k=1,…,Kg. However, due to the hierarchical structure of the observed data, the standard EM algorithm needs to be modified. In the following section, we will describe in details the E and M steps of the algorithm, by exploiting the similarities with the EM-type algorithms used in the field of multilevel multi-occasion models according to  Vermunt (2007).In the case of count data (when the observed values are the number of times thej-th product has been bought by thei-th customer), responsesyijare assumed to be (conditionally) independent Poisson random variables in thek-th product-specific cluster within theg-th individual-specific segment:yij∣uig=1,vjkg=1∼Poi(θjgk).At ther-th step, we solve the M-step equations, and obtain the following cluster-specific parameter estimates:θˆjgk(r)=∑i=1nyijξig,jk(r)∑i=1nξig,jk(r)j=1,…,p,g=1,…,G,k=1,…,Kg.However, in this case, we would not be able to distinguish the effects (in terms of model parameter estimates) of the individual/product-specific segments/clusters from the effects due to the observed product-specific scales. In fact, by looking at the estimates in the previous expression, we may notice thatθˆjgk(r)is calculated as an overall weighted mean with the posterior probabilities of each couple (segment, cluster) as weights. In the view of separating the effects (in terms of purchase behaviors) due to the similarities among individuals in the same segment and among products, from the scale effects (just due to the observed purchase counts), we define a more parsimonious model. Specifically, we assume that all cluster-specific parameters are equal across products, up to a scale product-specific parameter. That is, we assume the following identity holds:(13)θjgk=θjθgk,j=1,…,p,g=1,…,G,k=1,…,Kg. Here, similarities with Rasch-type models can be highlighted, where each item (here product) is associated with a fixed parameterθj, representing the item complexity (here, product overall purchase rate), and the individual-specific behavior is modeled through (parametric) random effects (here the distribution is discrete and a multilayer structure is adopted). Obviously, this parameterization leads to a high computational and model complexity, sincepfixed effect parameters should be estimated; however, the unconstrained parameterization is based ond=p×∑g=1GKgparameters, while the constrained in Eq. (13) is based ond=p+∑g=1GKgparameters and reduces the computational burden.Within each segment, the differential behavior of each individual takes into account potential departures from a homogeneous (across products) purchase behavior. This is achieved by considering the overall scale parameterθj,j=1,…,p, which helps to describe the different market share of each product, and the heterogeneity parametersθgk,g=1,…,G,kg=1,…,Kgaccounting for similarities across products within the same individual segment, once the counts are re-scaled. That is, once scale effects have been removed, the termsθgkcorrespond to products with similar departures from the overall product-specific average: the product-specific clusters define a joint partition of products and individuals, where the products are purchased with similar (relative) intensities by the individuals in theg-th segment,g=1,…,G. The overdispersed, segment-specific, distribution∏j=1pf(yij∣uig=1,θg)is itself a mixture of cluster-specific densities indicating set of products associated with subgroups of individuals.An appropriate EM algorithm is presented for the general case; only the substep for the estimation of the indexing parameter vectors is specified for the special case of purchase counts where conditional Poisson rvs are considered. In the case of different specifications, it may be modified straightforwardly. The algorithm requires a special implementation of the Expectation (E) step where an Upward/Downward-type algorithm, see e.g.  Vermunt (2007), is used at ther-th step to calculate at firstwig(r),i=1,…,n,g=1,…,Gand, after,ωijgk(r),j=1,…,p,k=1,…,Kgwithin each segmentg=1,…,G.Given the current model parameter estimates, calculateωijgk(r)=τjgk(r−1)f(yij∣θgk(r−1))∑l=1Kgτjgl(r−1)f(yij∣θgl(r−1))=hijgkhijgandwig(r)=τig(r−1)fg(yi∣θg(r−1))∑s=1Gτis(r−1)fs(yi∣θs(r−1))=τig∏j=1phijg∑s=1Gτig∏j=1phijs.Secondly, the joint posterior ofuigandvjgkin the expected complete data log-likelihood is computed by using a relation resembling the well-known Forward–Backward recursion:ξig,jk(r)=wig(r)ωijgk(r)=Pr(uigvjgk=1∣yi,Φ(r−1)),whereΦ(r−1)={θ(r−1),β(r−1),γ(r−1)}.Given the current values of the posterior probabilities for theg-th segment and thek-th cluster, the ML estimates for model parameters are found by solving the following score equations:∂Q(⋅)∂βg=∑i=1nwig(r)∂τig(xi)∂βg=0,∂Q(⋅)∂γgk=∑i=1nwig(r)∑j=1pωijgk(r)∂τjgk(zj)∂γgk=∑i=1n∑j=1pξig,jk(r)∂τjgk(zj)∂γgk=0,∂Q(⋅)∂θgk=∑i=1nwig(r)∑j=1pωijgk(r)∂log[f(yij∣θgk)]∂θgk=∑i=1n∑j=1pξig,jk(r)∂log[f(yij∣θgk)]∂θgk=0,g=1,…,G,k=1,…,Kg. Using the parsimonious parameterization introduced in Section  3.1, at ther-th step of the algorithm, the estimates for the indexing parameters can be calculated as follows:θˆgk(r)=∑i=1n∑j=1pξig,jk(r)yij∑i=1n∑j=1pξig,jk(r)θˆj(r−1)g=1,…,G,k=1,…,Kgandθˆj(r)=∑i=1n∑g=1G∑kg=1Kgξig,jk(r)yij∑i=1n∑g=1G∑kg=1Kgξig,jk(r)θˆgk(r),j=1,…,p.We consider an optical scanner panel data set on purchases for brands of saltine crackers in the Rome (Georgia) market, collected by Information Resources Incorporated. The data set contains information on 3292 purchases of 4 brand of saltine crackers (Nabisco, Sunshine, Keebler and a collection of private labels, in the following PrivLab) made byn=136households over about two years; for additional insight on the data set, see  Franses and Paap (2001). The total number of purchases has been summarized to get the number of purchases of each brand per household. We also have the average actual price (in US$) of the purchased brand and the average shelf price of other brands as product-specific covariates. Additionally, the information on whether there was a display and/or newspaper feature of the four brands at the time of purchase is available. Therefore, according to this setting, in the following we use 3 product-specific covariates, namely price, newspaper featured only, display and newspaper featured. It is worth noting that, since no household-specific covariates are available, we have covariate-free prior estimates for the household-specific segments. Table 1reports the definition of the product-specific covariates, while Table 2shows some data summaries.Table 3reports some descriptive statistics for the household-specific purchase counts, together with the market share for each brand. By a simple descriptive analysis, Nabisco is clearly the market leader (with a 54% share), while private labels represent a good second (31% share). Nabisco is also associated with a medium–high price and is more often display and/or newspaper featured.We have fitted the proposed model with varying number of household-specific segments,G=1,…,6and, for each choice ofG, for varying numbers of product-specific clusters,Kg=1,2,3; since we have a data set with onlyp=4products, we assume that the maximum number of product-specific clusters within an individual-specific segment cannot be greater thanKg=3. The optimal number of components can be selected by using penalized likelihood criteria, such as AIC or BIC; for the latter, the size used for calculation isn=136corresponding to the number of households in the sample. Table 4displays the values of the penalized likelihood indexes for several choices of the number of segments and clusters within segments, together with the maximized log-likelihood valuesℓand the number of estimated parametersd. For the sake of brevity, we do not report the values corresponding to the solutions forKg=3.As it can be easily observed by looking at Table 4, both AIC and BIC suggest the solution withG=5household-specific segments andKg=(2,2,2,2,1)product-specific clusters. The occurrence of local maxima and the problem of empty components imply that the maximized log-likelihood values, obtained by usingB=30starting values, have a non monotone behavior with increasing number of segments for a fixed sequence of the number of product-specific clusters. In our opinion, this point reflects the highly parameterized setting we are working with and its sensitivity to the occurrence of empty classes, rather than of local maxima, since we have not obtained better results (in terms of monotonicity) when the number of the starting configurations has been increased toB=50.Table 5reports the posterior probability estimates for the product-specific clusters,Pgk,k=1,2, within each household-specific segment,Pg,g=1,…,5, from the selected solution.As it can be observed, except for the last individual-specific segment where only one cluster of products is present, one of the two clusters is mainly linked with the brand Nabisco, while the other is associated with Sunshine and Keebler. PrivLab behaves somewhat in the middle between such well-defined sets of products, probably because it actually includes a collection of different brands. The cluster membership is stronger for clusters in segment 2, at least as far as Nabisco and the first product-specific cluster are concerned. In segment 4 a clear partition of the products is less evident, and the clusters seem to describe a common (to all products) heterogeneity with respect to the homogeneous reference distribution. In this respect, segment 4 is similar to segment 3 where, however, a more definite partition of the products can be evinced.Since posterior probabilities (termsξig,jk(r)=wig(r)ωijgk(r)in Eq. (12)) are household- and product-specific, we may look at the posterior classification (based on a maximum a posteriori procedure) of both households and products within each of theG=5segments. Table 6reports the derived partition for the first cluster within each segment, since the memberships for the second cluster (when present) can be simply obtained as complement to 1 of those in the table (a product is assigned to only one of the two product clusters for each household in the segment). Obviously, for the household-specific segment 5, we do not have any partition of the products. Probably, this is one of the main differences of the proposed method in comparison to standard finite mixture models, in that it produces a joint posterior classification of the households and of the products within each segment. Households may have different behaviors and, therefore, the interpretation of the within-segment clusters is mainly the result of the purchase choice of each household and, in particular, of the observed association among the purchased brands.Table 6 gives a further insight on product-specific clusters; if we exclude the last segment, where only one cluster of products is present, segments 2 and 3 may be easily interpreted, since only one/two of the observed posterior classifications are clearly prominent. Specifically, in segment 2 the sequence “1 0 0 0” (that is Nabisco) represents an estimated 67% of the total segment size, while in segment 3 the sequences “1 0 0 0” (that is Nabisco) and “0 1 0 0” (that is PrivLab) represent 45% and 43% of the total segment size, respectively. The distribution of the posterior allocations is more heterogeneous in segment 1, where, however, the sequences corresponding to Nabisco and PrivLab are the modes, and in segment 4, including only a small number of households (nˆ4=8) that exhibit quite constant propensities towards all the brands but Nabisco. Cluster-specific parameter estimates can be used to better characterize the purchase behaviors of the households. Table 7reports the parameter estimates, together with the overall product-specific estimates,θˆj. To ease the interpretation, we must remind that the cluster-specific estimates,θˆgk, represent the local departures from the overall means given by the product-specific effects,θˆj. Therefore, a cluster specific parameter estimate equal to 1 implies that the purchase behavior in that cluster is not different from the average one; while a parameter estimate greater (respectively smaller) than 1 indicates a higher (respectively lower) purchase rate for products in that cluster.By looking at the estimates in Table 7, we may say that households in cluster 1 in all segments, but the segment 5, exhibit a higher purchase rate than the average. Obviously, this empirical evidence implies that in cluster 2 of each segment, households have a lower purchase rate than the average. As far as the product features, the information on whether the brand was on display and/or newspaper featured have always a positive impact on the probability to belong to the first (high purchase rate) cluster in each segment, while price has always a negative effect.In this paper, we propose a two-level finite mixture model for clustering rows (units) and columns (variables) of a data matrix. The proposal has been sketched in the field of customer behavior for illustrative purposes, but it can be easily extended to other contexts, such as gene expression and text mining analyses, where a partition of objects and features is of interest. The model structure and the adopted parameterization have been compared with several proposals appeared in the recent literature on joint clustering of rows and columns of a two-way data matrix; the model has been proposed in a maximum likelihood framework and an appropriate EM-type algorithm has been outlined. Further extensions of this model might be introduced by looking at different representations for the cluster-specific model parameters.

@&#CONCLUSIONS@&#
