@&#MAIN-TITLE@&#
A novel background subtraction method based on color invariants

@&#HIGHLIGHTS@&#
We discuss the problem of integrated segmentation in color similar situation.The color invariants derived from a physical model are adopted to describe the colors.Realize the background–foreground segmentation in color similar situations.The proposed method is robust to abrupt illumination change.

@&#KEYPHRASES@&#
Background subtraction,Color invariants,Color similarity,Illumination change,Indoor environments,

@&#ABSTRACT@&#
This paper discusses the problem of segmenting foreground objects precisely in surveillance video images when foreground moving objects and the still backgrounds have the similar color parts. Motivated by the studies in color constancy, the notion of color invariants is introduced to realize integrated segmentation in color similar situations. Color invariants, which are derived from a physical model, are used as descriptors of image. Then a simple background subtraction method using the color invariants is performed to examine the effectiveness of color invariants in color similar situations. The experimental results demonstrated that the color invariants based method performed well in various situations of color similarity and also was robust to environmental illumination change. Moreover, the color invariants based method achieved higher accuracy and efficiency of background subtraction compared with other existing algorithms in practical real-time surveillance video images of indoor environments.

@&#INTRODUCTION@&#
Background subtraction is a fundamental issue of computer vision, which aims to segment foreground moving objects from images by parameters or descriptors so that the pixels residing in the foreground or background can be effectively differentiated. In some specific applications, background subtraction is demanded to exactly segment the foreground moving object from the background without apertures or discontinuities in segmentation. However in many common situations (e.g., the similar color of foreground moving objects and background, the abrupt change of illumination intensity), it is unable to realize the integrated object segmentation with background subtraction algorithm [1,2].In order to overcome the problems of apertures and discontinuities in segmentation, researchers [3–6] tried to improve the robustness of background subtraction algorithm for the illumination changes. Nevertheless, few researches worked on the apertures and discontinuities from the view of color model used by images [7]. Intuitively, the color discriminative ability of background algorithm is basically related to the way of representing colors in the image. RGB, the default color model generated by the surveillance camera, is an additive color model, which is formed by adding three basic colors with different proportions to reproduce a band of color arrays to represent colors in image. Since the formation of RGB color model is simple, it is widely used in the electronic devices [8]. Hence, most background subtraction algorithms used in the intelligence surveillance systems are directly processing images based on RGB color model, such as the GMM [9], Codebook [10], and etc. The GMM algorithm is one of the most prevalent background subtraction algorithms. Nonetheless, experiments proved that the GMM algorithm did not perform well when there were shadows; i.e., shadows were usually viewed as foreground objects by the GMM algorithm. This indicates the fact that the GMM algorithm is sensitive to the illumination intensity change. Funt et al. [12] confirmed in his research that the normalization of RGB values could eliminate the effect of illumination intensity. However, once the RGB value is normalized, images’ color values in the dark area become unstable [13]. The codebook algorithm uses the color distance to describe colors in order to make the best use of illumination intensity [10]. Although, theoretically, this method makes the algorithm independent of abrupt illumination change and discriminative to color similarity, it works poorly in practice [14].To get the full use of color information, the HSI color model is proposed, which is more intuitive and perceptual with human eyes compared with the RGB color model. In HSI color model, the intensity and chroma information are defined separately, this is better to cope with the color similarity as well as the illumination change. Unfortunately, the HSI color model is an unstable color model; i.e. when the color saturation is low, the H value changes randomly. As a result, HSI color model is unsuitable to the background subtraction algorithm.Luke et al. [11] proposed the YCrCbCg and HSv color models to improve the stability of the illumination change. The algorithm based on the YCrCbCg and HSv color models works well for various kinds of indoor environments, but it still unable to process the image under the color similarity situation.The color models used in the above mentioned algorithms describe image colors only with color spectral information, but without consideration of color spatial structure in image. The color of an object is not only a function of surface reflectance, but also a function of both the illumination spectrum and the sensing device [15]. Thus, it is necessary to consider the formation of color image as the combination of the image surface reflectance, ambient illumination and the photographing device. Therefore, it is assumed that a color model which integrates the color spectral information and spatial configuration will be more precise to describe colors in image.Geusebroek et al. [16,18] analyzed that the formation of colors in image was an integrated process of spectrum energy distribution in the spatial dimension at certain spectral scale-space and spatial scale-space. Then the Kubelka–Munk theory was adopted to model the formation of the color in term of physical basis, which integrated the color spectral information and spatial content to describe color [17]. The color invariants were derived from the Kubelka–Munk theoretic model, which were invariable to illumination changes and had excellent color discriminative capability in theory.In this paper, the color invariants are applied as descriptor for background subtraction method. The indoor environments are selected as the tentative environments. A background subtraction method based on a novel descriptor consist of color invariants is proposed. Experiments and results demonstrate that the proposed method realizes precise background–foreground segmentation in different color similarity situations and it has the robustness to illumination change. The method also achieves a high true-positive segmentation accuracy and runtime efficiency.The rest of this paper is organized as follows: Section 2 introduces the color invariant descriptors. In Section 3, the framework of background subtraction method based on color invariants is described. Section 4 presents the experiments and the results that examine the foreground segmenting capability and efficiency of the proposed method in different color similarity situations and its robustness to illumination.Color is an effective cue to discriminate objects in images. However, the general color models only use the color spectral information and its color representation for color spectrum is not as sophisticated as the color reflected light spectrum for human eyes. Therefore, in order to describe colors in a more discriminatively way, it is necessary not only to use the color spectral energy distribution coding color information, but also the spatial configuration of color [18]. The Kubelka–Munk theory as defined in Eq. (1) is established based on a physical model in term of spectral and spatial dimension of color [16]. Parameters with properties independent of illumination intensity and viewpoint are defined as the color invariants.(1)E(λ,x→)=e(λ,x→)(1-ρf(x→))2R∞(λ,x→)+e(λ,x→)ρf(x→)wherex→denotes the position in the image plane,λdenotes the wavelength,e(λ,x→)denotes the illumination spectrum,ρf(x→)denotes the Fresnel reflectance atx→, andR∞(λ,x→)denotes the material reflectivity. The set of color invariants derives from the Kubelka–Munk theory are shown in Table 1. In Table 1, C.I. is short for color invariant and the corresponding column lists the names of the defined color invariants. The second column is the definitions of each color invariant. The third and the forth columns give out the conditions and physical models from which the color invariants derived.The color invariants list in Table 1 are defined in an ideal physical model, i.e. the color invariants are defined in the given dimensional spectrum at an infinitesimal small spatial neighborhood. However, in practice, the spatial-spectral energy distribution is measurable only at a certain spatial extend and a certain bandwidth. According to Florack and Munk [17], the Gaussian function and its derivatives can be used as general probes for the measurement of spatial-spectral differential quotients. Therefore, in order to compute the color invariants in practice, Gaussian color model is adopted as the image color describing model. Gaussian color model is a human perceptual oriented color model which conveys color in term of spectral and spatial structure [18]. Hence, the spectral and spatial parameters in the definitions of color invariants can be calculated in the Gaussian color model.The spectral parameters of color in Gaussian color model: LetE(λ)be the energy distribution of the incident light as defined in Eq. (1), λ denotes wavelength. The observed spectral energy distributionE^(λ)in Gaussian color model may be approximated by a Taylor expansion atλ0with scaleσλ;(2)E^σλ(λ)=E^λ0,σλ+λE^λλ0,σλ+12λ2E^λλλ0,σλ+⋯whereE^λ0,σλ,E^λλ0,σλandE^λλλ0,σλare the incident light spectral energy distribution parameter with Gaussian aperture weighted atλ0with scaleσλ.Since the subspace spanned by the human visual system is of dimension 3, the third or higher order Taylor expansion of incident light spectral energy distribution is unobservable by human eyes [18]. Hence, the second order truncated Taylor expansion is sufficient to approximate the spectral energy distribution of color for human visual system. For color image,E^λ0,σλ,E^λλ0,σλandE^λλλ0,σλconsist of the spectral parameters of Gaussian color model atλ0with scaleσλ.The spatial parameters of color in Gaussian color model: The energy distribution of incident light at x direction can be expressed as Taylor expansion:(3)E^(λ,x)=E^+xλTE^xE^λx+12xλTE^xxE^xλE^λxE^λλxλ+⋯where(4)E^xiλj(λ,x)=E(λ,x)∗Gxiλj(λ,x;σλ,σx)Here,Gxiλj(λ,x;σλ,σx)is the Gaussian spatial-spectral probe.As previously mentioned, the human visual system can only be perceptual below the third order of Taylor expansion of light energy distribution. CoefficientsE^x,E^λx,E^λλxconsist of the spatial parameters of x direction of Gaussian color model. The spatial parameters of y direction;E^y,E^λy,E^λλycan be derived in the same way. Hence,E^,E^λ,E^λλ,E^x,E^λx,E^λλx,widehatEy,E^λyE^λλyconstitute the complete spectral-spatial parameters of Gaussian color model.In the second section, the definition of color invariants and their mathematical calculating method are presented. In this section, the procedures to obtain the color invariants from the RGB images and the background subtraction method with these color invariants are introduced.In Section 2, the spectral and spatial parameter of Gaussian color model ofλ0incident light at scaleσλ, σx, σywere derived. Colorimeter analysis is used to find out whenλ0=520mmandσλ=55mm, the spectral structure of Gaussian color model is most accord with the human visual system of color perception [18]. Eq. (5) shows the linear transformation from RGB image value to spectral parameters of Gaussian color model atλ0=520mmandσλ=55mm[19] as(5)E^E^λE^λλ0.060.630.270.30.04-0.350.34-0.60.17RGBSuggested by the scale-space theory [19,20], N-jet filtering can be utilized to generate the spatial parametersE^x,E^λx,E^λλx,E^yE^λyE^λλyat scale σx, σy.Based on the definitions of color invariants and indoor environment specific conditions, H,Wx,Wyare selected as the color invariants for the proposed method. Their definitions are given as below:(6)H=E^λE^λλ(7)Wx=E^xE^(8)Wy=E^yE^In order to examine to effectiveness of color invariants, a primitive background modeling method is conducted. N images of background sequence are used to the initial background model. For each image, color invariants H,Wx,Wyare calculated. Consequently, each image gets three feature planes, named H plane, Wxplane, and Wyplane respectively. Then, each pixel of the feature plane is modeled with single Gaussian distribution. The mean values of feature planes μH,μWx,μWyrepresent the average background color invariants, while standard deviations σH,σWx,σWyrepresent the pixel jitter. The mean values and the standard deviations are defined as follows:(9)μH(i,j)=1N∑k=1NHk(i,j)(10)σH(i,j)=1N∑k=1N(Hk(i,j)-Hμ(i,j))2(11)μWx(i,j)=1N∑k=1NWxk(i,j)(12)σWx(i,j)=1N∑k=1N(Wxk(i,j)-Hμ(i,j))2(13)μWy(i,j)=1N∑k=1NWyk(i,j)(14)σWy(i,j)=1N∑k=1N(Wyk(i,j)-Hμ(i,j))2Here, i and j denote the position of the current pixel in image, N denotes the number of reference background images, and μH,μWx,μWystand for the mean values of the pixel at (i, j) in H plane, Wxplane, and Wyplane respectively. While σH,σWx,σWystand for the standard deviations of the pixel at (i, j) in H, Wx, and Wyplanes respectively. The procedure of background modeling is illustrated in Fig. 1. As a result, the background model contains three feature planes; each pixel of the feature planes is a 2-tuple vector including μ and σ, which are statistical parameters calculated from background reference images.For each input video frame, the color invariants H, Wx, Wy,are calculated to obtain H plane, Wxplane, and Wyplane respectively. Then, the current frame’s feature plane compares with its corresponding feature plane in the background model. Judgment is implemented in pixel level based on the pre-set rules as defined in Eqs. (15), (16) to decide whether the pixel in the current input image belongs to foreground or not. H plane is used below as an instance to illustrate the judging conditions:(15)HΔ(i,j)=|H(i,j)-μH(i,j)|-t∗σH(i,j)(16)CH(i,j)=HΔ(i,j),ifHΔ(i,j)>00,ifHΔ(i,j)<0Here, t denotes the portion parameter, HΔ(i,j) denotes the difference between the current feature plane and its corresponding feature plane at (i, j) position in background model and CH(i,j) denotes the judged value for current H plane at (i, j). The Eqs. (15) and (16) are the pre-set rules which set the rules that if the current feature value were different from the mean value at the corresponding position in the background model but the difference was less than the t times of standard deviations at the corresponding position in the background model, then the current pixel would be judged as the background pixel; otherwise, the current pixel would be the foreground pixel. For Wxplane and Wyplane, the similar pre-set rules are used to make judgments. The pre-set rules for Wxplane and Wyplane are given in Eqs. (17)–(20). Fig. 2illustrates the changes between the current image and the background model of the three feature planes;(17)WxΔ(i,j)=|Wx(i,j)-μWx(i,j)|-t∗σWx(i,j)(18)CWx(i,j)=WxΔ(i,j),ifWxΔ(i,j)>00,ifWxΔ(i,j)<0(19)WyΔ(i,j)=|Wy(i,j)-μWy(i,j)|-t∗σWy(i,j)(20)CWy(i,j)=WyΔ(i,j),ifWyΔ(i,j)>00,7ifWyΔ(i,j)<0After obtaining the difference images, a fusion method is followed to fuse the three images into one total difference image. The operation is defined in Eqs. (21) and (22), Fig. 3shows the difference fusion image of the three difference images in Fig. 2.(21)Δ(i,j)=CH(i,j)∪CWx(i,j)∪CWy(i,j)(22)Δ(i,j)=Δ(i,j),ifΔ(i,j)<11,ifΔ(i,j)>1The difference fusion image achieved in the above steps is not sufficient due to the noise and incomplete region of foreground regions. Hence, some post-processing operations are required to obtain a pure background–foreground binary image. The post-processing operations are implemented in following steps:•Step a: Threshold the difference fusion image.•Step b: It is assumed that the foreground moving object is larger than any other foreground object in the thresholded image. Therefore, the largest foreground region is recognized as the foreground moving object. To this end, the areas of the foreground regions in the thresholded image F(x,y) are calculated. Then, the foreground regions whose areas are less than the largest one’s are deleted in the thresholded image F(x,y).Step c: Dilate and erode the thresholded image to make the foreground moving object complete; i.e. the region with valued zero surrounded by the pixels of valued one are then filled with ones.Step d: Use the media filter to make the silhouette of foreground moving object smooth.After implementing the above steps, the pure background–foreground segmented image is shown in Fig. 4.

@&#CONCLUSIONS@&#
