@&#MAIN-TITLE@&#
Robust validity index for a modified subtractive clustering algorithm

@&#HIGHLIGHTS@&#
This study proposes a novel robust validity index that evaluates the fitness of a partition generated by SC algorithm in terms of three properties: compactness, separation and partition index. Several experiments confirm that the preferences of the proposed index outperform all others.

@&#KEYPHRASES@&#
Partition index,Robust,Subtractive clustering (SC) algorithm,Validity index,

@&#ABSTRACT@&#
A novel robust validity index is proposed for subtractive clustering (SC) algorithm. Although the SC algorithm is a simple and fast data clustering method with robust properties against outliers and noise; it has two limitations. First, the cluster number generated by the SC algorithm is influenced by a given threshold. Second, the cluster centers obtained by SC are based on data that have the highest potential values but may not be the actual cluster centers. The validity index is a function as a measure of the fitness of a partition for a given data set. To solve the first problem, this study proposes a novel robust validity index that evaluates the fitness of a partition generated by SC algorithm in terms of three properties: compactness, separation and partition index. To solve the second problem, a modified algorithm based on distance relations between data and cluster centers is designed to ascertain the actual centers generated by the SC algorithm. Experiments confirm that the preferences of the proposed index outperform all others.

@&#INTRODUCTION@&#
Clustering algorithms are widely used to group objects based on attributes of data that describe the objects and their relation to one another. As such, a clustering algorithm aims to partition the data into groups; data with similar attributes are partitioned into one cluster and differentiated from data in other groups [1,2]. Clustering algorithms have been successfully used in data mining, pattern recognition, function approximation, machine learning, and system modeling.The major clustering algorithms proposed in the literature can be classified as partitioning-based algorithms, hierarchical algorithms, density-based clustering algorithms, grid-based algorithms, model-based algorithms, and conceptual clustering algorithms [3].The subtractive clustering (SC) algorithm proposed by Chiu [4] is a density-based clustering algorithm based on the density of data points (i.e., “potential values”) in the feature space. The core concept of the SC algorithm is to find the regions in the feature space with the highest density of data points. The point with the highest potential value is selected as the center of a cluster. The potential of data points within a prescribed radius are then removed, and the algorithm finds the point with the next highest potential value. This procedure is repeated until a predefined criterion is met.The SC algorithm is widely used in various real applications. Chen and Gaob [5] adopted SC algorithm for an adaptive network based fuzzy inference system (ANFI) to estimate the train station parking (TSP) error in urban rail transit. In Ref. [6], Bilgin et al. proposed an unsupervised hyperspectral image segmentation method that used a novel subtractive-clustering-based similarity segmentation approach and a novel method of cluster validation by one-class support vector (SV) machine. In Ref. [7], Chen adopted particle swarm optimization (PSO) techniques to obtain appropriate parameter settings for subtractive clustering (SC) and integrated the adaptive-network-based fuzzy inference system (ANFIS) model to construct a model for predicting business failures. In Ref. [8], a fuzzy classifier based on simulated annealing (SA) and subtractive clustering was used to optimize a fuzzy inference system for classification tasks. In Ref. [9], a subtractive based fuzzy inference system is introduced to estimate the potato crop parameters like biomass, leaf area index, plant height and soil moisture. In Ref. [10], Amina et al. proposed a wavelet neural network for prediction of electricity consumption of the power system of the Greek Island of Crete and the SC algorithm had been applied to the definition of fuzzy rules.Unlike the K-means, which require iterations of several epochs, the SC algorithm requires only one pass of the training data. However, the SC algorithm only roughly estimates the cluster centers, since the cluster centers obtained are located at some data points. Moreover, since no cluster validity is used, the clusters produced may not accurately represent the clusters [11]. To overcome the disadvantage of SC algorithm, the solution proposed in this study is to use a robust validity index for a modified SC algorithm.This paper is organized as follows: Section 2 surveys validity indexes proposed in the literature and proposes a new robust validity index for SC algorithm. A modified SC algorithm for evaluating cluster centers is introduced in Section 3. Section 4 presents the experiment results, and Section 5 discusses the results.The cluster validity index is widely used to evaluate partition fitness in clustering algorithms. To measure the qualities of the partitions provided by the output, a validity index assigns a value to the output of the clustering algorithm. The validity index for finding an optimal c, denoted c*, that adequately describes the data structure is the most intensively studied topic in cluster validity. Several popular validity indexes are reviewed in Refs. [1,12,13].An important quality of a clustering algorithm is the association between data points and cluster centers. The membership function is used to measure the strength of the association. If the data set contains c clusters, then each data point has c memberships which represent the close degree between the data point to the cluster center. For each data point, if one of the membership values of a particular data point belonged to a cluster is larger than the others, then this point is identified as being an element of that cluster [14]. Therefore, information about membership degrees can be represented by a single number indicating the fitness of the data point as classified by the clustering algorithms.The literature suggests that the two key factors in validity indexes of data clustering algorithms are compactness and separation. Compactness refers to the cohesion i.e., the concentration of data in each cluster. For a data clustering algorithm, high compactness indicates a good partition. Notably, compactness increases with cluster number and is highest when each datum forms a cluster. In contrast, separation is a measure of coupling between clusters. Low coupling indicates that the clusters have a weak relationship but that the partition is good. The validity index for measuring the goodness of partitions can be designed to consider both of these factors:(1)Validityindex(c)=CompactnessSeparation,or(2)Validityindex(c)=Compactness−Separation,where 2≤c≤cmax is the number of clusters. A reasonable approach is to evaluate the maximal value of Eq. (1) or Eq. (2) in searching for the optimal cluster number c*of a given data set. The most common validity indexes are reviewed below.The partition coefficient (PC) index proposed by Bezdek [15] in 1974 was the first index for the fuzzy c-means algorithm (FCM) algorithm. It indicates the amount of overlap between clusters obtained by FCM. Let μikbe the membership value of data xibelonging to cluster k. The PC index is defined as:(3)PC(c)=1n∑k=1c∑i=1n(μik)2,and(4)∑k=1cμik=1.In a clustering algorithm for partitioning a data set, the cluster number is optimal when PC(c) is maximal. The core concept of the PC index is that the cluster number is optimal when the cluster partition is least ambiguous. However, its disadvantage is that it only considers the fuzzy membership degree μikfor each cluster; i.e., it does not consider structure.Another index proposed by Bezdek is the partition entropy (PE) index [15,16], which is defined as(5)PE(c)=−1n∑k=1c∑i=1nμikloga(μik),where a is the base of the logarithm and the range of the PE(c) is [0,loga(c)]. To find the optimal c*, the author solvesargmin2≤c≤cminPE(c) to produce the best clustering performance for a data set. Like the PC index, the disadvantage of the PE index is that it obtains the minimum value for each hard partition. Both indices only evaluate fuzziness and do not consider the data structure of the clusters.In 1996, Dave defined a modified partition coefficient (MPC) [17] for the c-shell clustering algorithm. The MPC was defined as(6)MPC(c)=1−cc−1(1−PC(c))The MPC, which has a value of [0,1], is a normalized version of the PC index. When PC(c)=1/c, MPC=0 and PC(c)=1, MPC=1. Therefore, it has the same disadvantage as the PC index.The Xie and Beni (XB) index [18], which combines compactness and separation properties, is defined as(7)XB(c)=∑k=1c∑i=1nμik2||xi−vk||2n×mini≠j(||vi−vj||),where viand vjrepresent the centers of cluster i and j, respectively. In Eq. (7), the numerator indicates the compactness of the fuzzy partition while the denominator indicates the strength of the separation between clusters [19]. A small value in the numerator of the XB index represents a high compactness while a high value in the denominator denotes good cluster separation. Hence, the lowest XB(c), 2≤c≤cmax indicates the optimal cluster number for a data set X.The Fukuyama and Sugeno (FS) index [20] is another index which combines the properties of compactness and separation measurements. The FS index is defined as follows:(8)FS(c)=∑i=1n∑k=1c(μik)m||xi−vk||2−−∑k=1c∑i=1n(μik)2⋅||vk−v¯||,where 1<m<∞, andv¯=1/c∑i=1cvirepresents the mean of the cluster centroids. In Eq. (8), the first term represents the geometrical compactness of the clusters, and the second term indicates the separation between the clusters. When m is 2, the first term of the FS index equals the numerator of the XB index. The second term of the FS index is used for separation measurements. Cluster number is optimized by solving argmin2≤c≤n−1FS(c).The I-index proposed by Maulik and Bandyopadhyay [21] in 2002 is defined as(9)I(c)=1c×E1Ec×Dc2.Here,Ec=∑i=1n∑j=1cμij||xi−vj||,Dc=maxi,j=1c||vi−vj||.To find the optimal c*, the authors solvedargmax2≤c≤cmaxI(c) to optimize clustering performance for the data set. The I-index combines three factors: (1) 1/c, (2) E1/Ec, and (3) Dc. The first factor limits the cluster number since the I-index decreases as c increases. This factor avoids the situation in which cluster number is optimized when each datum forms a cluster. In the second factor, the E1 is a constant, and the Ecdecreases with an increase in c. The third factor Dc, which denotes maximum separation between two clusters over all possible pairs of clusters, increases as c increases. Thus, the optimal cluster number depends on these three competing factors.In Ref. [13], Žalik proposed a clustering validity index integrated compactness and overlap measures for FCM algorithm. Assume each data object xjbelongs to the ith cluster with a degree μji, The compactness of each cluster is obtained by summing the compactness rates of all data objects Cji(c,U) as follows:(10)C(c,U)=∑i=1cCi(c,U),Ci(c,U)=1n∑j=1nCji(c,U),(11)Cji(c,U)=μji,if(μji−μjk)≥Tc,k=1,2,…,c,k≠i0,otherwise,where Tcis 0.6.The overlap in the fuzzy partition is defined as the sum of the overlap values for each pair of clusters:(12)O(c,U)=∑a=1c−1∑b=a+1cOab(c,U),where(13)Oab(c,U)=1n∑j=1nOjab(c,U),(14)Ojab(c,U)=1−(μja−μjb),if(μja−μjb)≥To,a≠b0,otherwise.The index CO, which integrates compactness and overlap, is defined as(15)CO=C(c,U)−O(c,U).A ratio type validity index COralso uses the same measures of compactness and overlap:(16)COr=C(c,U)O(c,U).According to Eqs. (15) and (16), the compact and well-separated clusters correspond to the high values of validity indexes CO, and COr, i.e., the high value of compactness C and the small value of overlap O. Therefore,argmax2≤c≤cmaxCO orargmax2≤c≤cmaxCOrmust be solved to optimize the cluster number. Since FCM is sensitive to initialization values, the author repeated the FCM algorithm ten times for each number of cluster c to evaluate average values of CO, and COr.The PE, PC, and MPC indexes only consider membership μikwhen measuring the fitness of a partition within a data set X. The drawbacks of these indexes are [19] the following: (1) their dependency on the number of clusters and (2) their lack of direct connection to the geometry of the data.A common objective of the XB, FS, I, CO, and COris accurately estimating cluster number c such that each of the c clusters is compact and separate from the other clusters. In actual applications, however, data domains are often affected by noises and outliers. The resultant partitions obtained by clustering algorithms are consistently affected by those variables. This study proposes a novel robust index to indicate the adequacy of partitions in a data set contains noise and outliers.This subsection introduces the robust validity index proposed in Ref. [22] for a data set with noises and outliers. The features of this index include compactness, separation, and α-cut properties to indicate the fitness of the cluster number obtained by the modified SC clustering algorithm.Where X={x1, x2, …, xn}⊂ℜdis a data set in a d-dimensional feature space, letW={wik|0≤wik≤1}1≤i≤n, 1≤k≤c be a matrix where wikis the membership degree of xibelonging to cluster k, and let v={v1,v2,…,vc} be a set of the cluster centers wherevk∈ℜd. The wikis defined by(17)wik=exp−||xi−vk||ρ2,where ||xi−vk|| represents the Euclidean distance between xiand vkand where ρ is the width of the Gaussian function. In the proposed method,ρ=1/α, where α is defined by SC algorithm in next section. Unlike μik, which satisfies∑k=1cμik=1, the wikcan faithfully reflect an object such as real data or noise. Noises that are far from cluster centers have low wikvalues but high μikvalues for some k. Here, this property is used to reduce the effects of noise.An α-cut of a fuzzy set F is a crisp set Fαcontaining all elements in X that have membership values in F greater than or equal to α; that is,(18)Fα={xi∈X|wik≥α}.The assumptions are that α=0.51 and that Γ represents the number of data satisfyingwik≥α; that is,(19)Γ=|F0.51|,where |·| is the number of elements in a set. The wik,0≤wik≤1, represents the relation between data point and center point. When the data point xiis closer to the cluster center point vk, wikis closer to 1; when the data point xiis farther from the cluster center point vk, wikis closer to 0. Intuitively, when wik≥0.51, it has exceeded the median, 0.5, thus, the data point xiis regarded as being correctly clustered. In theories,wik=exp−||xi−vk||ρ2≥0.51,exp||xi−vk||ρ2≤10.51,||xi−vk||≤0.673ρ2,In addition, sinceρ=1/α, and α=4/ra,||xi−vk||≤0.673ρ2=0.673α≈0.17ra,where rais the pre-defined radius, and defined in SC algorithm. If rais a proper value, 0.17rameans that xiis very close to the cluster center. According to the above analysis, wik≥0.51 means that the data point xiis very close to the cluster center vk, so this paper regards it as being correctly partitioned.Let parameter τ be defined as the partition index, for identifying the ratio of data points regarded as clustered correctly in the data set. Thus, τ is defined as(20)τ=Γ|X|,To indicate the fitness of the number of clusters in a data set, the proposed validity index, CSα combines compactness, separation, and τ properties as follows:(21)CSα(c)=1τ×Dmin×com.Here,(22)Dmin=mini≠j(||vi−vj||),(23)com=1n∑k=1c∑i=1nμik2||xi−vk||2×∑k=1c∑i=1nμik||xi−vk||.Cluster number is optimized by solving argmin2≤c≤maxCSα(c). The CSα(c) is subject to three factors: 1/τ, 1/Dmin, and com. The first factor is a partition index of the inverse proportion of well-partitioned data to the whole data. A small 1/τ indicates that large amounts of data have been well partitioned. The second factor Dmin, which represents the minimal distance between clusters, is used to measure the separation between cluster centers. When each cluster is far away from other clusters, the Dmin is large, and 1/Dmin is small. Finally, the third factor com is used to measure the proper compactness of clusters. When each cluster is compact, the value of com is small. Thus, if the number of clusters is increased from 2 to cmax, the values of the first and third factors decrease, and the value of the second factor increases. Thus, these three competing factors are balanced to find the optimal cluster number. The validity indexes in the current literature only consider compactness and separation properties while the proposed validity index includes a partitioned index 1/τ to measure the ratio of well-partitioned data to all data.The fuzzy clustering algorithm based on fuzzy set theory is the most well known soft partition method. In the context of a fuzzy set, a μA(x) value close to 1 denotes a high degree of membership while a value close to 0 denotes a low degree of membership. Numerous successful applications of fuzzy clustering have been proposed, e.g., handwritten digit recognition [23], function approximation [24], and image processing [25].Although the FCM [26,27] has been the most popular fuzzy clustering approach for both theoretical and practical applications [28–33] in recent decades, it has two disadvantages: (1) the number of clusters must be known in advance but is unavailable in many applications; and (2) when the data training contains outliers, the resultant centers are always influenced by outliers.The mountain method (MM) proposed by Yager and Filev in 1994 [34] is an effective and simple method for identifying the cluster center of the sampling data [22].The MM for data clustering is divided into three steps. First, the mountain function constructs a grid on the data space where the intersections of the grid lines are candidate cluster centers. Assuming that the Ijdenotes the discussion range of the jth coordinate,(24)Ij=[mink(xkj),maxk(xkj)].After constructing a d-dimensional hypercube I1×I2×…Id, the MM discretizes each of the intervals Ijinto rjequidistant points. The resulting d-dimensional grid in the hypercube has nodesNi1,i2,…,idwhere ik∈[1, 2, …, rk], k=1, 2,…,d.The second step is constructing a mountain function for data energy measurement. The value of the mountain function at grid node Niis calculated by(25)M(Ni)=∑k=1nexp(−α⋅d(xk,Ni)),where α is an application-specific positive constant and d(xk,Ni) is the distance between the data point xkand the grid node Ni.The third step is iterative selection of the next cluster centers and sequential deconstruction of the mountain function value. In each iterative process, the grid node with the largest energy is selected as the cluster center, and the potentials of the remaining grid nodes must be eliminated to identify the next one. LetNk*, k=1, 2,…,cmax, denote the grid node with the maximal value of the mountain function at the kth iteration. The maximal value of the mountain functionMk*is denoted as(26)Mk*=maxiMk(Ni).The new modified value of the mountain function is defined as follows:(27)Mk+1(Ni)=Mk(Ni)−Mk*×e−βd(Ni−Nk*),where β is a prescribed positive constant. By continually producing new cluster centers, the MM reduces the value of the mountain function until the current maximumMk*is too low in comparison with the original maximumM1*.Although the MM is a simple data clustering technique and has robust properties against outliers, its limitation is that grid nodes and computational loads increase exponentially as the dimensions increase since the mountain function calculates its energies on the grid nodes.In 1994, Chiu [4] develop a function for calculating the clustering center of the sampling data. The proposed method is called the subtractive clustering algorithm (SC). In the SC algorithm, the data points are regarded to the energy sources. The peak potential generated at the location of the data point decreases rapidly at any point away from the data point [14]. The potential of each data point xiis defined by following equation:(28)P(xi)=∑k=1nexp(−α⋅d(xi,xk)),where α=4/raand where rais a positive constant that is effectively the radius defining a neighborhood. In Eq. (28), the P(xi) is formulated by the sum of the potentials contributed by each data xk. A reasonable assumption is that the peaks of the P(xi) correspond to cluster prototypes and that the valleys correspond to the decision boundaries between the clusters [14]. After having the potential of each data point, the SC algorithm selects the data point with the highest potential as the first cluster center. Wherex1*is the first cluster center and has a potential value ofp1*, the potential of each data point can be revised by(29)P(xi)=P(xi)−p1*e−βd(xi,x1*),where β=4/rb, and rbis a positive constant that is effectively the radius defining a neighborhood with measurable reductions in potential. In [4], a good choice for rbis 1.5ra. The revised potential is maximized to find the next cluster center,x2*, and the effects of the second cluster center are again removed. The process is repeated until a given threshold δ for the potential is obtained so that(30)pj*p1*<δ.In Eq. (30), the choice of δ is an important factor affecting the clustering results; a δ that is too large results in too few data points accepted as cluster centers, and a δ that too small results in excessive cluster centers [4].Although the SC algorithm need not partition the data space into grid nodes, it must calculate the potential value of each datum. Notably, two problems must be overcome: (1) the SC algorithm assumes that the clustering center isxi*obtained by the ith iteration. However, thexi*may be a rough cluster center that is not representative of the actual cluster center; (2) no index indicates the optimal number of clusters for a data set. The proposed solution to the first problem is the use of a modified algorithm based on distance measurements to identify the actual cluster centers. The second problem is solved by using the validity index described in Section 2 to indicate the goodness of the partitions in various cluster numbers.This subsection proposes a method based on distance relations between data points and cluster centers obtained to reduce the computing time of the SC algorithm and finds actual cluster centers.Letxi*, i=1, 2,…,cmax be the cluster centers obtained by the SC algorithm. This paper defines a distance relationship between the centersxi*and data xjby following equation:(31)Dij=exp(−α⋅d(xj−xi*)),i=1,2,…,cmax,j=1,2,…,n,where α is a constant defined in Eq. (28). The value of Dijrepresents the distance relationship between xjandxi*. That is, for a cluster centerxi*, a high value of Dijfor any data xjindicates that the data xjapproximates the cluster i. SetNεidenotes the number of data aroundxi*in which Dij≥ɛ, where ɛ≤0 is a constant. In this study, ɛ is set to 0.51. The new center of cluster i is defined by the following equation:(32)vi=∑k=1NεixkiNεi,wherexkiis a datum in which Dik≥ɛ. In Eq. (32), viis the mean of data close to cluster i. In each iterative epoch of proposed algorithm, Eq. (32) is used to evaluate the real cluster center when a new cluster centerxi*is found by SC algorithm; the potentials can be revised by(33)P(xki)=P(xki)−pi*e−βd(xki,vi),where β is defined in Eq. (29).The differences between the proposed and the original SC algorithms are (1) the cluster centers obtained by the proposed algorithm provide an actual cluster center that is more accurate than the cluster centers obtained by the SC algorithm; and (2), in each iterative epoch of the proposed method, thed(xj−xi*)in Eq. (31) is the same as that in Eq. (29), and the revised potential of Eq. (33) uses relative dataxki, which approximate the cluster center i, whereas the revised potential of the SC algorithm in Eq. (29) uses all sampling data. That is, the proposed method requires less computation of revised potential compared to the SC algorithm.The above procedure can be summarized by the following algorithm:Step 1: Set the values of cmax.Step 2: Calculate the potential P(xi) for each data xi, 1≤i≤n.Step 3: Set the initial number of c=1.Step 4: Evolve the maximal potential valuepc*from the P(xi). According topc*, find the data point xc.Step 5: Calculate the actual cluster center viaccording Eqs. (31) and (32).Step 6: Update the potential values according to Eq. (33).Step 7: Evaluate the validity index by Eqs. (17)–(23).Step 8: Where c=c+1, if c <=cmax, then go to Step 4.Step 9: Search the minimal value of CSα. The related value c of the minimal value of CSα is the optimal number of clusters.

@&#CONCLUSIONS@&#
