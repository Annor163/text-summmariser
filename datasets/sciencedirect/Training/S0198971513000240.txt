@&#MAIN-TITLE@&#
‘Truncate, replicate, sample’: A method for creating integer weights for spatial microsimulation

@&#HIGHLIGHTS@&#
Integerisation of weights obtained from iterative proportional fitting.Evaluation of five methods for integerisation.New method presented, based on truncation, replication and sampling (TRS).The new method outperforms previously published integerisation strategies.Easily reproducible results, using publicly available R code and data.

@&#KEYPHRASES@&#
Microsimulation,Integerisation,Iterative proportional fitting,

@&#ABSTRACT@&#
Iterative proportional fitting (IPF) is a widely used method for spatial microsimulation. The technique results in non-integer weights for individual rows of data. This is problematic for certain applications and has led many researchers to favour combinatorial optimisation approaches such as simulated annealing. An alternative to this is ‘integerisation’ of IPF weights: the translation of the continuous weight variable into a discrete number of unique or ‘cloned’ individuals. We describe four existing methods of integerisation and present a new one. Our method – ‘truncate, replicate, sample’ (TRS) – recognises that IPF weights consist of both ‘replication weights’ and ‘conventional weights’, the effects of which need to be separated. The procedure consists of three steps: (1) separate replication and conventional weights by truncation; (2) replication of individuals with positive integer weights; and (3) probabilistic sampling. The results, which are reproducible using supplementary code and data published alongside this paper, show that TRS is fast, and more accurate than alternative approaches to integerisation.

@&#INTRODUCTION@&#
Spatial microsimulation has been widely and increasingly used as a term to describe a set of techniques used to estimate the characteristics of individuals within geographic zones about which only aggregate statistics are available (Ballas, O’Donoghue, Clarke, Hynes, & Morrissey, 2013; Tanton & Edwards, 2012). The model inputs operate on a different level from those of the outputs. To ensure that the individual-level output matches the aggregate inputs, spatial microsimulation mostly relies on one of two methods. Combinatorial optimisation algorithms are used to select a unique combination of individuals from a survey dataset. This approach was first demonstrated and applied by Williamson, Birkin, and Rees (1998) and there have been several applications and refinements since then. Alternatively, deterministic reweighting iteratively alters an array of weights, N, for which columns and rows correspond to zones and individuals, to optimise the fit between observed and simulated results at the aggregate level. This approach has been implemented using iterative proportional fitting (IPF) to combine national survey data with small area statistics tables (e.g. Ballas et al., 2005a; Beckman, Baggerly, & McKay, 1996). A recent review, published in this journal, highlights the advances made in methods for simulating spatial microdata (Hermes & Poulsen, 2012) since these works were published. Harland, Heppenstall, Smith, and Birkin (2012) also discuss the state of spatial microsimulation research and present a comparative critique of the performance of deterministic reweighting and combinatorial optimisation methods. Both approaches require micro-level and spatially aggregated input data and a predefined exit point: the fit between simulated and observed results improves, at a diminishing rate, with each iteration.1In IPF, model fit improves from one iteration to the next. Due to the selection of random individuals in simulated annealing, the fit can get worse from one iteration to the next (Hynes, Morrissey, ODonoghue, & Clarke, 2009; Williamson et al., 1998). It is impossible to predict the final model fit in both cases. Therefore exit points may be somewhat arbitrary. For IPF, 20 iterations has been used as an exit point (Anderson, 2007; Lee, 2009). For simulated annealing, 5000 iterations have be used (Goffe, Ferrier, & Rogers, 1994; Hynes et al., 2009).1The benefits of IPF include speed of computation, simplicity and the guarantee of convergence (Deming, 1940; Fienberg, 1970; Mosteller, 1968; Pritchard & Miller, 2012; Wong, 1992). A major potential disadvantage, however, is that non-integer weights are produced: fractions of individuals are present in a given area whereas after combinatorial optimisation, they are either present or absent. Although this is not a problem for many static spatial microsimulation applications (e.g. estimating income at the small area level, at one point in time; for example see Anderson (2013)), several applications require integer rather than fractional weights. For example, integer weights are required if a population is to be simulated dynamically into the future (e.g. Ballas et al., 2005a; Clarke, 1986; Holm, Lindgren, Malmberg, & Mäkilä, 1996; Hooimeijer, 1996) or linked to agent-based models (e.g. Birkin & Clarke, 2011; Gilbert, 2008; Gilbert & Troitzsch, 2005; Pritchard & Miller, 2012; Wu, Birkin, & Rees, 2008).Integerisation solves this problem by converting the weights – a 2D array of positive real numbers(N∈R⩾0)– into an array of integer values(N′∈N)that represent whether the associated individuals are present (and how many times they are replicated) or absent. The integerisation function must perform f(N)=N′ whilst minimising the difference between constraint variables and the aggregated results of the simulated individuals. Integerisation has been performed on the results of the SimBritain model, based on simple rounding of the weights and two deterministic algorithms that are evaluated subsequently in this paper (see Ballas et al., 2005a). It was found that integerisation “resulted in an increase of the difference between the ‘simulated’ and actual cells of the target variables” (Ballas et al., 2005a, p. 26), but there was no further analysis of the amount of error introduced, or which integerisation algorithm performed best.To the best of our knowledge, no published research has quantitatively compared the effectiveness of different integerisation strategies. We present a new method – truncate, replicate sample (TRS) – that combines probabilistic and deterministic sampling to generate representative integer results. The performance of TRS is evaluated alongside four alternative methods.An important feature of this paper is the provision of code and data that allow the results to be tested and replicated using the statistical software R (R Core Team, 2012).2The code, data and instructions to replicate the findings are provided in the Supplementary Information: https://dl.dropbox.com/u/15008199/ints-public.zip. A larger open-source code project, designed to test IPF and related algorithms under a range of conditions, can be found on github: https://github.com/Robinlovelace/IPF-performance-testing.2Reproducible research can be defined as that which allows others to conduct at least part of the analysis (Table 1). Best practice is well illustrated by Williamson (2007), an instruction manual on combinatorial optimisation algorithms described in previous work. Reproducibility is straightforward to achieve (Gentleman & Temple Lang, 2007), has a number of important benefits (Ince, Hatton, & Graham-Cumming, 2012), yet is often lacking in the field.The next section reviews the wider context of spatial microsimulation research and explains the importance of integerisation. The need for new methods is established in Section 3, which describes increasingly sophisticated methods for integerising the results of IPF. Comparison of these five integerisation methods show TRS to be more accurate than the alternatives, across a range of measures (Section 4). The implications of these findings are discussed in Section 5.Spatial microsimulation is a modelling method that involves sampling rows of survey data (one row per individual, household, or company) to generate lists of individuals (or weights) for geographic zones that expand the survey to the population of each geographic zone considered. The problem that it overcomes is that most publicly available census datasets are aggregated, whereas individual-level data are sometimes needed. The ecological fallacy (Openshaw, 1983), for example, can be tackled using individual-level data.Microsimulation cannot replace the ‘gold standard’ of real, small area microdata (Rees, Martin, & Williamson, 2002, p. 4), yet the method’s practical usefulness (see Tomintz, Clarke, & Rigby, 2008) and testability (Edwards & Clarke, 2009) are beyond doubt. With this caveat in mind, the challenge can be reduced to that of optimising the fit between the aggregated results of simulated spatial microdata and aggregated census variables such as age and sex (Williamson et al., 1998). These variables are often referred to as ‘constraint variables’ or ‘small area constraints’ (Hermes & Poulsen, 2012). The term ‘linking variables’ can also be used, as they link aggregate and survey data.The wide range of methods available for spatial microsimulation can be divided into static, dynamic, deterministic and probabilistic approaches (Table 2). Static approaches generate small area microdata for one point in time. These can be classified as either probabilistic methods which use a random number generator, and deterministic reweighting methods, which do not. The latter produce fractional weights. Dynamic approaches project small area microdata into the future. They typically involve modelling of life events such as births, deaths and migration on the basis of random sampling from known probabilities on such events (Ballas et al., 2005a; Vidyattama & Tanton, 2010); more advanced agent-based techniques, such as spatial interaction models and household-level phenomena, can be added to this basic framework (Wu et al., 2008; Wu, Birkin, & Rees, 2010). There are also ‘implicitly dynamic’ models, which employ a static approach to reweight an existing microdata set to match projected change in aggregate-level variables (e.g. Ballas, Clarke, & Wiemers, 2005b).Individual-level, anonymous samples from major surveys, such as the Sample of Anonymised Records (SARs) from the UK Census have only been available since around the turn of the century (Li, 2004). Beforehand, researchers had to rely on synthetic microdata. These can be created using probabilistic methods (Birkin & Clarke, 1988). The iterative proportional fitting (IPF) technique was first described in 1940 (Deming, 1940), and has become well established for spatial microsimulation (Birkin & Clarke, 1989; Axhausen, 2010).The first application of IPF in spatial microsimulation was presented Birkin and Clarke (1988) and Birkin and Clarke (1989) to generate synthetic individuals, and allocate them to small areas based on aggregated data. They produced spatial microdata (a list of individuals and households for each electoral ward in Leeds Metropolitan District). Their approach was to select rows of synthetic data using Monte Carlo sampling. Birkin and Clarke suggested that the microdata generation technique known as ‘population synthesis’ could be of great practical use (Birkin & Clarke, 2012).Since the work of Birkin and Clarke (1988) and Birkin and Clarke (1989) there have been considerable advances in data availability and computer hardware and software. In particular, with the emergence of anonymous survey data, the focus of spatial microsimulation shifted towards methods for reweighting and sampling from existing microdata, as opposed to the creation of entirely synthetic data (Lee, 2009).This has enabled experimentation with new techniques for small area microdata generation. A significant contribution to the literature was made by Williamson et al. (1998). The authors presented microsimulation as a problem of combinatorial optimisation: finding the combination of SARs which best fits the constraint variables. Various approaches to combinatorial optimisation were compared, including ‘hill climbing’, simulated annealing approaches and genetic algorithms (Williamson et al., 1998). These approaches involve the selection and replication of a discrete number of individuals from a nationally representative list such as the SARs. Thus, subsets of individuals are taken from the global microdataset (geocoded at coarse geographies) and allocated to small areas. There have been several refinements and applications of the original ideas suggested by Williamson et al. (1998), including research reported by Voas and Williamson (2000), Williamson, Mitchell, and McDonald (2002), and Ballas, Clarke, and Dewhurst (2006).The methods described in the previous section involve the use of random sampling procedures or ‘probabilistic reweighting’ (Hermes & Poulsen, 2012). In contrast, Ballas, Dorling, Thomas, and Rossiter (2005c) presented an alternative deterministic approach based on IPF. It is the results of this method, that does not use random number generators and thus produces the same output with each run,3Probabilistic results can also be replicated, by ‘setting the seed’ of a predefined set of pseudo-random numbers.3that the integerisation methods presented here take as their starting point. The underlying theory behind IPF has been described in a number of papers (Deming, 1940; Mosteller, 1968; Wong, 1992). Fienberg (1970) proves that IPF converges towards a single solution.IPF can be used to produce maximum likelihood estimates of spatially disaggregated conditional probabilities for the individual attributes of interest. The method is also known as ‘matrix raking’, RAS or ‘entropy maximising’ (see Axhausen, 2010; Birkin & Clarke, 1988; Huang & Williamson, 2001; Jiroušek & Přeučil, 1995; Johnston & Pattie, 1993; Kalantari, Lari, Ricca, & Simeone, 2008). The mathematical properties of IPF have been described in several papers (see for instance Birkin & Clarke, 1988; Bishop, Fienberg, & Holland, 1975; Fienberg, 1970). Illustrative examples of the procedure can be found in Saito (1992), Wong (1992) and Norman (1999). Wong (1992) investigated the reliability of IPF and evaluated the importance of different factors influencing its performance; Simpson and Tranmer (2005) evaluated methods for improving the performance of IPF-based microsimulation. Building on these methods, IPF has been employed by others to investigate a wide range of phenomena (e.g. Ballas et al., 2005a; Mitchell, Shaw, & Dorling, 2000; Tomintz et al., 2008; Williamson et al., 2002).Practical guidance on how to perform IPF for spatial microsimulation is also available. In an online working paper, Norman (1999) provides a user guide for a Microsoft Excel macro that performs IPF on large datasets. Simpson and Tranmer (2005) provided code snippets of their procedure in the statistical package SPSS. Ballas et al. (2005c) describe the process and how it can be applied to problems of small area estimation. In addition to these resources, a practical guide to running IPF in R has been created to accompany this paper.4This guide, “Spatial microsimulation in R: a beginner’s guide to iterative proportional fitting (IPF)”, is available from http://rpubs.com/RobinLovelace/5089.4The aim of IPF, as with all spatial microsimulation methods, is to match individual-level data from one source to aggregated data from another. IPF does this repeatedly, using one constraint variable at a time: each brings the column and row totals of the simulated dataset closer to those of the area in question (see Ballas et al., 2005c and Fig. 5 below).Unlike combinatorial optimisation algorithms, IPF results in non-integer weights. As mentioned above, this is problematic for certain applications. In their overview of methods for spatial microsimulation Williamson et al. (1998) favoured combinatorial optimisation approaches, precisely for this reason: “as non-integer weights lead, upon tabulation of results, to fractions of households or individuals” (p. 791). There are two options available for dealing with this problem with IPF:•Use combinatorial optimisation microsimulation methods instead (Williamson et al., 1998). However, this can be computationally intensive (Pritchard & Miller, 2012).Integerise the weights: Translate the non-integer weights obtained through IPF into discrete counts of individuals selected from the original survey dataset (Ballas et al., 2005a).We revisit the second option, which arguably provides the ‘best of both worlds’: the simplicity and computational speed of deterministic reweighting and the benefits of using whole cases.In summary, IPF is an established method for combining microdata with spatially aggregated constraints to simulate target variables whose characteristics are not recorded at the local level. Intergerisation translates the real number weights obtained by IPF into samples from the original microdata, a list of ‘cloned’ individuals for each simulated area. Integerisation may also be useful conceptually, as it allows researchers to deal with entire individuals. The next section reviews existing strategies for integerisation.

@&#CONCLUSIONS@&#
The results show that TRS integerisation outperforms the other methods of integerisation tested in this paper. At the aggregate level, accuracy improves in the following order: simple rounding, inclusion threshold, counter-weight, proportional probabilities and, most accurately, TRS. This order of preference remains unchanged, regardless of which (from a selection of 5) measure of goodness-of-fit is used. These results concur with a finding derived from theory – that “deterministic rounding of the counts is not a satisfactory integerization” (Pritchard & Miller, 2012, p. 689). Proportional probability and TRS methods clearly provide more accurate alternatives.An additional advantage of the probabilistic TRS and proportional probability methods is that correct population sizes are guaranteed.12Although the counter-weight method produced the correct population sizes in our tests, it cannot be guaranteed to do so in all cases, because of its reliance on simple rounding: if more weights are rounded up than down, the population will be too high. However, it can be expected to yield the correct population in cases where the populations of the areas under investigation are substantially larger than the number of individuals in the survey dataset.12In terms of speed of calculation, TRS also performs well. TRS takes marginally more time than simple rounding and proportional probability methods, but is three times quicker than the threshold and counter-weight approaches. In practice, it seems that integerisation processing time is small relative to running IPF over several iterations. Another major benefit of these non-deterministic methods is that probability distributions of results can be generated, if the algorithms are run multiple times using unrelated pseudo-random numbers. Probabilistic methods could therefore enable the uncertainty introduced through integerisation to be investigated quantitatively (Beckman et al., 1996; Little & Rubin, 1987) and subsequently illustrated using error bars.Overall the results indicate that TRS is superior to the deterministic methods on many levels and introduces less error than the proportional probabilities approach. We cannot claim that TRS is ‘the best’ integerisation strategy available though: there may be other solutions to the problem and different sets of test weights may generate different results.13Despite these caveats, the order of accuracy identified in this paper is expected to hold in most cases. Supplementary Information (Section 4.4), shows the same order of accuracy (except the threshold method and counter-weight methods, which swap places) resulting from the integerisation of a different weight matrix.13The issue will still present a challenge for future researchers considering the use of IPF to generate sample populations composed of whole individuals: whether to use deterministic or probabilistic methods is still an open question (some may favour deterministic methods that avoid psuedo-random numbers, to ensure reproducibility regardless of the software used), and the question of whether combinatorial optimisation algorithms perform better has not been addressed.Our results provide insight into the advantages and disadvantages of five integerisation methods and guidance to researchers wishing to use IPF to generate integer weights: use TRS unless determinism is needed or until superior alternatives (e.g. real small area microdata) become available. Based on the code and example datasets provided in the Supplementary Information, we encourage others to use, build-on and improve TRS integerisation.A broader issue raised by the this research, that requires further investigation before answers emerge, is ‘how do the integerised results of IPF compare with combinatorial optimisation approaches to spatial microsimulation?’ Studies have compared non-integer results of IPF with alternative approaches (Harland et al., 2012; Rahman, Harding, & Tanton, 2010; Ryan, Maoh, & Kanaroglou, 2009; Smith, Clarke, & Harland, 2009). However, these have so far failed to compare like with like: the integer results of combinatorial approaches are more useful (applicable to more types of analysis) than the non-integer results of IPF. TRS thus offers a way of ‘levelling the playing field’ whilst minimising the error introduced to the results of deterministic re-weighting through integerisation.In conclusion, the integerisation methods presented in this paper make integer results accessible to those with a working knowledge of IPF. TRS outperforms previously published methods of integerisation. As such, the technique offers an attractive alternative to combinatorial optimisation approaches for applications that require whole individuals to be simulated based on aggregate data.