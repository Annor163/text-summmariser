@&#MAIN-TITLE@&#
The application of ubiquitous multimodal synchronous data capture in CAD

@&#HIGHLIGHTS@&#
A generic ubiquitous data capture framework is demonstrated via two case studies.Variety of inputs, interactions, biophysical data and design solutions are captured.Tight temporal synchronisation with commodity data logging tools is achieved.Demonstrates engineering knowledge capture linking CAD and PLM via generated metadata.The framework’s use in future CAD and PLM systems is extrapolated.

@&#KEYPHRASES@&#
Ubiquitous data capture framework,Multimodal data logging,User activity monitoring,Design review,Capturing design process,Synchronised data capture,

@&#ABSTRACT@&#
Design is an interactive and iterative process where the designer’s skills and knowledge are fused with emotive rationales aided by design tools. A design solution is thus influenced by the designer’s creativity, experience and emotional perception. Consequently, there is a need within computer aided design (CAD) research for ubiquitous tools to capture the affective states of engineers during design activities to further understand the product design process.This paper proposes a generic framework for ubiquitous multimodal synchronous data capture, based around the capture of CAD system activities, to monitor and log a variety of inputs, interactions, biophysical data and design solutions with a view to providing meta and chronological performance data for post design task analysis. The framework has been employed in two use cases namely, a CAD station activity and a collaborative design review. The results of these trials validated the architecture and use of the ubiquitous data capture approach demonstrating the practical application of time-phased data capture, analysis and the subsequent output of metadata in CAD environments providing a new perspective on, and a new way of investigating CAD-based design activities.This research also extrapolates the framework’s usefulness into future CAD and PLM systems by arguing why and how they need to adopt such ubiquitous platforms. It also subjectively points to potential opportunities and issues that might arise when implementing the ubiquitous multimodal metadata architecture in a real-life environment.

@&#INTRODUCTION@&#
Product design can be a complex and iterative process where the knowledge, experience, and skills of a designer/engineer are articulated through the design tools. The reasoning given by the designer behind every action performed in a design activity is highly complex and comprises inferred information, explicit and tacit knowledge, emotive communication and transient exposures to the subject matter [1]. Despite the associated product constraints, a designer’s knowledge and rationale play a significant role in the design. These are fuzzy by nature, difficult to capture and therefore difficult to document. Although self-reporting at each design phase can be applied, it is heavily dependent on the memory of the designer, open to interpretation and also creates interruption during the design process itself.Present-day computer aided design (CAD) solutions can produce a history of the designed solution within the internal bounds of their software package, e.g. revision history of a CAD part, tracking of changes made to a solution, etc. However they miss out the valuable information about external endeavours particularly those that happen during the design activity such as dialogues, sketches, notes, informative directives, knowledge and the emotional states of the designer, as well as other physical activities which influence the design process.The literature demonstrates that there is a whole host of data required to be monitored and logged to enable design tasks in computer aided environments to be revisited, interrogated, analysed and understood [2,3]. The future of design task analysis and knowledge/information capture lies in studies that involve the logging and analysis of formal computer aided engineering data, e.g. specifications, geometries, properties, calculations, simulations, in conjunction with the multimodal information about the activities performed during the design process. In general, multimodal interaction capture involves capturing audio-visuals, document access history, inter-personal interactions and psycho-physiological data in a ubiquitous, time-phased manner. As a consequence of this, there is a requirement for standard protocols which researchers can use to implement such a monitoring methodology as a tool to take forward the next generation of design research.This paper proposes a generic framework for ubiquitous multimodal synchronous data capture, based around the capture of CAD system activities, which was employed to monitor and log a variety of inputs, interactions, psycho-physiological data and design solutions with a view to providing meta and chronological performance data for post task analysis. The framework predominantly addresses a technical solution for the ubiquitous and holistic capture of design activity. Through this work it has been demonstrated that it is possible to build up an end-to-end pipeline which ubiquitously captures the design activity and represents the captured meta-information against CAD records in a time-phased manner.Jin and Ishino [4] proposed a design activity knowledge acquisition (DAKA) framework to extract the design activity knowledge by capturing the designers’ design moves and used a function-based design operation-mining algorithm to extract meaningful design operation sequences. DAKA established the product model road map that represents the trajectory designers walked through during the design process by capturing the sequence of events occurring while interacting with CAD software.Besides the CAD software and CAD models, a designer would also interact with other media such as text, audio, video and sketches. Shipman and McCall [5] proposed an integrated approach to capturing design rationale and associated design communications by means of the ‘hypermedia’. Two systems, PHIDIAS and Hyper Object Substrate (HOS), were used to capture and integrate a variety of hypermedia related to the design process and to structure the unstructured hypermedia information. A vector-based node-link structure was used to represent the connectivity between hypermedia, serving as the basis for design rationale. Although the structuring of hypermedia is studied, no temporal records were utilised in this work to relate them with the chronological performance of the design process.Design activities are dynamic and any information resources accessed during a design session can be captured and potentially reused in future design tasks. However, structuring design knowledge, e.g. inferred, tacit, explicit and transitive, in a machine accessible format for storage and retrieval for computer-based knowledge support is not trivial. Campbell et al. [6,7] explored a methodology for profiling computer-based designed activities relative to the order and timing of resources accessed during a design session, where the data is captured along with the creation of formal design representations, e.g. design drawings, reports, etc., based on Bayesian inference. They then tried to relate it to the designer’s focus or goal which was interpreted by the actions observed at the computer interface, e.g. keyboard input and interaction with the CAD software, so that a relationship between documents based on the context of their usage can be identified.The Knowledge Enhanced Notes (KEN) system introduced by Conway et al. captures collaborative activities in a design meeting and generates an enhanced documented record of the meeting with a view of re-using at later stages in the product lifecycle. In addition to the discussions and decisions made during the meeting, KEN records the resources accessed and the temporal information about them [2]. The benefits of the multimodal information have been generally realised specifically as informal data about the design process in addition to formal design records, which enables revisiting and understanding the design process in product’s extended lifecycles [8,9].Revisiting and interrogating the design process with regard to the multimodal metadata requires a pristine link between the formal data and informal data. Generally, this has been overlooked and the informal data searchable or not-searchable on its own will not have the associativity with the formal data. Therefore the captured metadata should be associated with the part name or a product lifecycle management (PLM) identification property in order to provide the bidirectional accessibility. Preferably, the best time to capture this link is during the actual activity, when the formal design records are being created or accessed and the informal data is being captured.Analogous to CAD, Rea et al. [10] used BAMZOOKi, an intuitive 3D design environment, to study design activities. This work specifically enabled the automatic capture of the design process via a customised BAMZOOKi interface and generated log files based on extensible mark-up language (XML) and Process Specification Language (PSL) during the design activity. The log files were subsequently processed to output various representations of design knowledge. Integrated computer-aided manufacturing DEFinitions IDEF0 [11] and Design Rationale Editor DRed [12] diagrams were automatically generated to formally represent the designer’s activity and design rationale. English-syntax instructions, annotated video clips and a design storyboard, were generated to represent the design knowledge and processes. Similarly design activities in a virtual reality based cable organisation system were also studied by capturing user activities identical to the CAD design process [13,14]. This further demonstrates and validates that design activities can be potentially transformed into understandable and CAD-neutral format knowledge representations.Comparable to analysing a CAD activity, capturing user activities is common in Human Computer Interaction (HCI) studies. Usability studies for investigating software application user interface (UI) extensively used user activity monitoring systems [15–17]. A survey by Hilbert and Redmiles [18] classified and described a wide range of techniques used in HCI research for extracting usability information from user interfaces. They categorised which usability indicators were analysed, e.g. user behaviour, performance, cognition, attitude, stress level, and which multimodal data collection techniques such as UI events, audio, video, psychophysical recording and subjective measures such as interviews and surveys, were used to study the usability. This work also highlighted that user activities can be classified and analysed into different levels according to their level of granularity and abstraction; the abstracted activities can be inferred by interpreting a stream of events related to the low-level interactions [18].Fig. 1illustrates examples of CAD user activities during a typical CAD task, approximately representing the variation from low-level interactions to inferred abstract tasks. A balance between the granularity of the data capture and the level of inference needed to incorporate the abstraction should be maintained while capturing and analysing user activities [19]. Accordingly, adequate low-level, granular activity data should be captured to recognise reasonably abstract tasks [16]. Too much data, when combined with extraneous information, create difficulties in drawing inferences about the task and can demand high computational and storage resources.Standard operations in a PC, such as file operations, text manipulation, accessing the web, voice calls, etc., are often monitored to analyse the user activity [17]. Capturing events automatically generated by application, instead of low-level keyboard and mouse interactions, is a commonly used technique in user activity monitoring systems. These events are usually pre-determined and the applications customised to generate these events automatically while the user interacts with the system [19,20]. Events are chosen and organised based around the domain of study, for example a study about document interactions considers file system events, internet browser events, and application window events [6].Software tools are normally required to capture and exchange event information across the UI application and monitoring software. Microsoft component object models (COM) and dynamically linked libraries (DLL) are often used to build the infrastructure for receiving event notifications in Microsoft applications [19,20,17]. Intercepting messages invoked by the operating system for keyboard and mouse events is another commonly used technique for monitoring user activity [19,21].Events raised by the application or the user interface represent instantaneous activities occurring for a discrete period of time. In contrast, user activities can be observed in a continuous manner, e.g. video and audio recording, screen capture, and continuous mouse movements [15,22]. Psycho-physiological measures are increasingly being employed to establish a better understanding of the internal state of the user. Heart rate/electrocardiography (EKG), facial muscle activities, eye tracking, pupillometry, electroencephalography (EEG), electromyography (EMG), and galvanic skin response (GSR) are some of the commonly used biometric measures to monitor user activity [15,23–27].User activity monitoring systems have largely focused on logging events occurring between the user and the UI. However, the recording user activity in a task-based situation creates more challenges. A design task involves human factors beyond examining the interaction solely between a designer and software. Personal aspects such as knowledge, experience, creativity and emotional perception do influence the design solution. Consequently, psycho-physiological data provides an alternative for analysing affective design activities. Complementary to UI event logging, which provides an explicit form of information about the user interaction within the UI, the human-centric data represents an implicit form of user interaction. A need for a common set of tools to integrate and build up a user activity monitoring system has been noticeably identified in the past [28]; however, this work is the first attempt at specifying, designing and implementing such a generic framework for CAD.The case for using informal metadata relating to the design process in addition to formal design records has been described and consequently the requirement of multimodal capture is established throughout the Sections  1.1 and 1.2. The unobtrusiveness in the design process necessitates a ubiquitous and dynamic system that operates in such a manner that the capture tools are embedded into the working environment; therefore it does not create an interruption of or add extra workload to the on-going process. The use of ubiquitous, multiple modalities necessitate tight temporal synchronisation tolerances between captured data streams in order that a meaningful understanding of the CAD activity relationships can be established. For this reason systematic, implementable arrangements are presented that accommodates a wide range of data capture tools and blends them seamlessly into the environment and the process. It particularly addresses a technical solution for consistently extendable, ubiquitous metadata capture systems.The rest of paper is organised as follows. Section  2 presents a generic ubiquitous framework detailing the implemented key modules and elements. The way components of the framework are implemented to solve issues as they arise in a ubiquitous capture environment is described. Section  3 provides two use cases, i.e. a CAD station and a collaborative design review system which are described to demonstrate how the generic ubiquitous framework was employed to monitor and log a variety of multimodal information during design activities. As an outcome of applying the framework, a variety of inputs, interactions and designer biophysical data are represented with a view to providing meta and chronological performance data during a CAD task. It has been also demonstrated that a design task can be captured and analysed in minute detail with the aid of various captured data representations.The appropriateness of the framework is extrapolated into its use in future CAD and PLM solutions in the Section  5. Based on the previously described use cases, two possible configurations of metadata collection systems are proposed. Various issues that could potentially arise when such a metadata collection system is built in a real-life design environment are also outlined. Section  6 summarises the key contributions to the field of ubiquitous synchronous data capture while pointing towards target users of the framework in engineering and other fields.The ubiquitous capture framework is presented in abstract terms, whereas C++ and C# are used to develop the actual data capture system. The abstractness ensures that it is generic in form and is maintained throughout the actual implementation by the use of object oriented programming concepts such as classes and inheritance. Integrating multiple capture devices, handling data flow between components and synchronisation, are the main issues addressed by the framework.Multimodal data capture tools include both data logging software and hardware-based capture devices. Hardware-based capture devices are usually used in combination with an application programming interface (API) or Software Development Kit (SDK) where the API/SDK provides an interface to hardware control and access to the captured data. For example, functions for initialising the device, releasing the device, starting data capture, stopping data capture and configuring parameters are typically available control functions in an API. Although the actual tasks performed by control functions are often similar, naming conventions and ways to access these functions vary significantly across different device manufactures [29,30]. A possible reason is attributed to different connection interfaces, e.g. USB, Serial, Ethernet, etc., and protocols, e.g. USB, TCP, UDP, RS232, etc. Although the term ‘devices’ generally represents hardware-based data capture tools, this term also refers to software-based data logging tools, e.g. CAD software, unless otherwise stated in this work.A unified access interface to support the ubiquitous nature of the framework has been defined such that all the multiple capture devices’ control functions can be accessed homogeneously. Generally, a wrapper interface is implemented which hides the device-specific function naming conventions and merges them if there are multiple function calls required to accomplish one specific function, i.e. loading API DLLs, initiating connections with the hardware and checking for device availability that can be merged into the ‘initialisation’. Principally the unified access interface abstracts device-specific function calls for data logging into a universal interface. Primary functions in this are as follows:•Initialise — initiating communication with the device and configuration; thereafter it is ready to be used.Release — freeing the device and the API.Start — starting the actual data capture.Stop — stopping the capture.Reset — resetting the device in case errors have occurred.Conversely, some features are unique for particular types of device only. For example, selecting the input channel, e.g. HDMI, DVI, and Composite, is an exclusive feature for video frame grabber devices only. Therefore, the framework allows access to these functions directly, bypassing the unified access interface. It is usually preferable to access device-specific features through a bespoke device-specific interface while accessing the generic functions through the unified interface. This enables multiple devices to be controlled via a centralised control panel without losing the diminutive control of the device. Fig. 2illustrates an example unified access interface and centralised control panel.The conventional method of building a data logging arrangement is to integrate every device into one single large program. This creates potential complications in a ubiquitous data capture system, especially when the data capture configuration changes, i.e. when a device is not available or a new device is added to the system. In this case a programmer would usually modify, duplicate or rewrite the code from the previous setup for the new configuration. The concept of a unified interface implemented in the framework allows devices to be used in a modular manner in a variety of configurations without rewriting or duplicating the code. An initial implementation of the unified interface for a device is always needed after which it can be reused to handle alternative multiple data logging arrangements without any alterations.The unified interface also allows devices to run independent of each other. It enables devices to attach and detach at runtime, permitting the ubiquitous data capture system to change dynamically. This unique feature embedded within the framework is particularly useful since it means that the operation of the system will not be disturbed if, say, a device fails or needs to be removed, when a new device is accessible to join the system, and where modifications in runtime are required. This is achieved by allocating a status parameter to every device which can be checked by the centralised control panel to determine the availability or status of a device. The key status variables are:1.Dormant — device is not available.Not-initialised — device is present but not ready for use.Ready — device is available for use.Running — device is capturing data.Dead — device is failed, unknown state.Each of the capture tools within the ubiquitous framework produces a variety of data types depending on the captured content, e.g. discrete key press, mouse movement coordinates, video, location-based multichannel physiological measurement such as EEG, etc. The characteristics of these multimodal data vary extensively based on the format of the data, its resolution and frequency. Table 1explains the mixture of types of data produced by devices used in CAD environment experiment in this study. Care should be taken to handle those dissimilar data types, for example, the way the 19 channel EEG data is treated, which corresponds to different locations on the head, is different than, say, 2-dimensional pixel based screen capture. Data buffers involved in retrieving the data and the processing algorithms used also vary.In addition to the dissimilarity in data formats, the method by which they are captured is also important. Several data channels are embedded within a stream, i.e. continuously at a constant frequency, e.g. EEG, eye tracking, video, while the others are captured as discrete events occurring in the environment, e.g. CAD activity logging, key presses. Remarkably, some of the data can be captured in both streams and in an event-based manner, e.g. mouse activity can be monitored for discrete clicks while tracking the cursor movement continuously, and tracking the eyes continuously while triggering specific events when the user looks at a specific region of interest on the screen.The bandwidth and frequency of a data channel are important factors in deciding how it should be handled. Biophysical signals, e.g. EEG, GSR, are captured at high frequency (2048 Hz) from USB hardware. There is the potential for missing data packets from devices if they have not been read at the required frequency. It is crucial to allocate appropriate buffer sizes and also remove the data from the buffers at an adequate rate. HD video capture needs handling vast amounts of data (1280×720 pixels × 3 byte colour depth X 30 fps=79.10 Mbytes per second) and potentially creates a bandwidth bottleneck within the data capture system. It is common to use a video codec to compress the video in real time; on the other hand runtime video compression puts high demand on the CPU. Although CAD logging is normally not a computationally demanding task, the events triggered by the CAD software are sporadic since it is based upon user activity, e.g. the user spins the model and changes the view zoom occasionally. This produces varying amounts of data in short but intensive bursts, producing a non-uniform load on computational resources which might cause issues in the real-time scheduling of tasks involved in the system.Data handling modules are implemented in the ubiquitous capture framework to address issues related to multimodal data. Abstract and modular-based implementations of stream type and event type data are built into the framework. A programmer may use these abstract modules as a skeleton to implement personalised data handling modules for different data types in a customised data capture system. Fig. 3shows various configurations of how the data handling units in the ubiquitous data capture environment can be linked using the appropriate modules. Brief descriptions of each configuration are now given.1.First in first out (FIFO) buffers are extensively used for sharing data from one unit to the other, where FIFO buffers support exchanging data between multiple execution threads. Multi-threading provides better handling between time critical tasks and long running background tasks. Collecting data from EEG hardware is a time critical high priority task. Therefore a high priority, high frequency task can collect EEG samples from the hardware and store it in the FIFO, whereas a long running storage task can transfer the data in batches from the FIFO to a disk storage. An abstract stream data module has been implemented in the framework and say it can be extended to support EEG data formats. Correspondingly, the same module was extended to support associated stream-based data types in the following configurations.It is reasonable to have a ubiquitous data capture system which has a real-time viewing capability which can be used for inspecting the captured data online. The video from any environment camera, capturing user activity viewed online while the recorded video frames are compressed by the codec and stored in a video file. However, online viewing is only a supplementary feature and has less importance than other components in any data capture system. It is good enough to miss video frames for viewing and hence appropriate to run a low frequency, low priority thread with flexible timing requirements. A feature has been implemented to retrieve samples from the FIFO without removing the samples from the buffer so that every frame can be securely collected by the storage thread.In contrast to the stream module, an event-based module is implemented in the framework which deals in particular with sporadic data. Events triggered in CAD software are passed through this event module and can be linked to the other system units if necessary.Two functions, namely capturing the user screen video and generating a screen shot when a mouse click event is fired, are combined in this configuration. A fusion unit was employed to retrieve a frame from the user screen video stream whenever a mouse click event is received.Data handling modules implemented in the framework are designed to be lightweight and high performance. In contrast to other data logging systems [29] which upload the data to a centralised database, data have been handled by the framework in a distributed manner, therefore enabling high bandwidth and low latency data handling. Nevertheless, the captured data can be committed to a central repository using a low priority task if required.Data samples collected via miscellaneous devices are routed through various data paths thus possessing disparate latencies. These streams must be aligned to produce valid correlations and hence useable information can be generated. The validity of the inferred data produced by fusion units depends on the temporal alignment of the data fed into these data fusions. For example, the screen video, eye tracking and GSR must be temporally synchronised to derive a reasonable conclusion about user’s emotions in an instant [31]. It is also important to synchronise the events occurring in the CAD environment, e.g. events triggered by the CAD software, mouse clicks, with the data streams such as biophysical signals and video. An illustration of the potential latencies involved in data channels is shown in Fig. 4.Assorted devices running on isolated platforms with physically separated hardware and clocks essentially run asynchronously. It is normal to observe jitter while accessing the live data streams. Although devices might be internally capable of sampling signals at sufficiently precise intervals, general purpose PC operating systems and connectivity solutions, i.e. USB, TCP, create non-deterministic distortions in the synchronisation of the data streams [32,33]. Commodity devices do not often provide advanced clock/stream synchronisation methods. Nevertheless, it is not pragmatic to expect a common synchronisation technique in a ubiquitous data capture environment where diverse and multimodal hardware is used. To address this, the framework incorporates a latency/offset compensation parameter for every data channel. The offset parameter is used in online-fusion units to compensate for the offsets in fused channels or in post-capture analysis to re-synchronise the channels. In the previous example in Fig. 3, configuration 4 is a fusion unit creating screenshots for mouse clicks where the screenshots are skewed by the difference in offsets in the data streams (Fig. 4b and c).Stream and event type data handling modules described in Section  2.2 are equipped with the offset and jitter parameters where a suitable method to estimate them can be chosen by the programmer, i.e. fix to a constant offset, programmatically choose the offset, online jitter monitoring, etc. Monitoring parameters online provides the information about synchronisation-related issues, e.g. clock drift in recording hardware, communication bandwidth fluctuations, etc.The diversity of devices used (Table 1) in monitoring CAD environments often requires special considerations in choosing hardware and software platforms. Support is provided by various device vendors for different operating system platforms and maintaining issues caused by legacy software/hardware influences selecting the hardware and software platforms. Incompatibility between devices and platforms occasionally requires a mix of platforms. The framework supports interconnecting devices running independently in isolated platforms with the aid of a communication interlink, e.g. TCP, UART Messages, that relay the unified interface function calls and also the synchronisation related messages. Furthermore when the data streams or an event is exchanged between components of the framework, the latency in these interlinks also needs to be counted when synchronisation is considered.The experimental ubiquitous data capture system was setup to demonstrate the applicability of the generic ubiquitous framework to an engineering design task. This experimental system is also a case study for the generic ubiquitous framework and to test out how its components fulfil the requirements for capturing an engineering design activity. Previously described concepts and modules of the framework are implemented and tested by applying them to the experimental capture of design activity and thereafter justified by examining the captured data during the experiment. This section presents an outline of the experimental design task followed by descriptions of the framework implementation and how its modules were used to handle issues related to the design environment.A design task to optimise a bracket to support the given pin force was to be trialled on an industry standard CAD package, i.e. Siemens NX™. A total of 24 engineering students were recruited to establish a record on their cognitive loads during design activities. There was no time restriction for the user to complete the task. The design task was divided into two stages:(a)To modify the bracket such that it will not fail given an applied pin force. The student was required to calculate the allowable tensile and shear stress based on the bracket material and then, through the interface, configure the cross-section (breadth and thickness) of the bracket (Fig. 6).To calculate the tensile and shear stress including the weld type and sizing for the bracket.In order to understand the affective states and the cognitive processes of the designer during the iterations of the given task, psycho-physiological measurements were recorded and synchronised with other modalities such as keyboard and mouse interactions, video recording, etc. (Fig. 5).High temporal synchronisation was required in the recorded data, which included the EEG, EKG, eye movement, NX log file, mouse clicks and a video of user design activities. The EEG and ECG data are processed for emotions and pupil sizes were correlated to the cognitive load. By studying the psycho-physiological attributes associated with a CAD-based engineering design process, it is envisaged that interrelated engineering behaviours can be more deeply understood and, consequently, lead to more natural and intuitive CAD user interfaces.Standard Siemens NX™CAD software was customised using the UG Open API according to the unified interface structure as proposed in the framework. The customised module also generated markers for predefined actions, e.g. modifying the values of text boxes, pressing command buttons, pan and zoom-view manipulations, etc., carried out in the CAD software. This module was also responsible for regulating the other capture tools setup in the environment. For example metadata capture of other devices is started/stopped when the designer starts/stops working on the CAD, i.e. controlling the whole metadata environment.Fig. 7shows how the capture devices were setup and how three different workstation platforms were used in the ubiquitous data capture environment for the CAD task. A low latency system, PC1 (Windows 7, 32 bit), is used as the centralised control panel and for time stamping purposes. Simultaneously, it is also used with EEG and eye tracking devices, for which platform drivers and APIs only support 32 bit platforms. PC2 (Windows 7, 64 bit) has been purposely tuned for high bandwidth and storage capabilities with RAID-0 disk storage and USB3 features. The CAD workstation (Windows XP, 32 bit) was built addressing the legacy support issues. TCP interlinks were implemented to interconnect distributed devices with the centralised control panel and to create connections between devices wherever the data flow between different distributed device units was required.Mouse clicks and screen video stream are captured by different devices and associated APIs (Table 1). The offset between these channels creates an observable skew when these channels are fused together. Grabbed video frames and the mouse clicks are analysed to validate synchronisation and the concept of latency/offset compensation. Although the offset compensation is applied for every channel, these two channels are chosen for analysis because the mouse pointer on the screen and the button click event can be manually observed in the video frames. Mousexandycoordinates logged in a click event by the Windows hooks is correlated with the screen video frames. Fig. 8shows the timeline of video frames, a mouse click event and the mouse click coordinates. The skew can be visually observed where the click coordinates and the mouse pointer do not match. When the offset compensation is applied the mouse pointer is then aligned with the logged coordinates in a subsequent frame and hence validating the synchronisation.Monitoring the jitter and frame rate of a device in run-time provides more information about the runtime behaviour of the device and data stream generated by the device. This information is captured by the data handling modules implemented in the framework (Section  2.3) which provides a precise understanding about the dynamic characteristics of the synchronisation. For example, the Nexus, bio-physical monitoring device, samples signals at 2048 Hz according to the manufacturer’s specification. The number of samples delivered by this device via the associated API is observed at fixed intervals. Subsequently the instantaneous sample rate was calculated using the time provided by a high precision hardware clock in the central processing unit (CPU). The continuously measured sample rate and the observation of jitter are illustrated in Fig. 9. The presence of jitter and the deviation in the sample rate in this data stream have been revealed using the data handling modules implemented in the framework. Similar distortions were observed in other devices, e.g. eye tracker, frame grabber which are possibly caused by the non-deterministic characteristics of the data paths, e.g. USB, Ethernet and the inaccuracies in task scheduling. Subsequently, FIFO buffers implemented in the framework were used to treat the jitter and offset compensations were applied to individual data channels so that they can be temporally aligned.Events triggered by the CAD software while the engineer performs the design task, which are logged by the customised Siemens NX interface. These events were logged in a log file (Fig. 10) with a distinctive event identifier and the corresponding time; therefore they can be synchronised and correlated with other data channels. These event identifiers were used by event handling modules in the framework to handle them unambiguously.A timeline of events triggered in the design environment and bio-physical signals are shown in Fig. 11. 19 channels of EEG signals and EKG are captured using the bio-physical monitoring device while eye gaze and pupil radius are captured by the eye tracker. These independently running devices and CAD logging are integrated using the framework to function together as a ubiquitous capture system.The eye blinks captured in the data can be used to justify the temporal alignment of integrated data streams. Eye blinks can easily be identified in the EEG data stream by the eye blink artefacts, while the eye tracker loses tracking of the eyes when the user blinks. Fig. 11 shows a tight temporal alignment of these data streams where EEG and eye tracking data can be visually examined for the blinks. This consequently substantiates the implemented data handling modules and synchronisation techniques of the framework.The time-phased log files recorded during the design activity were parsed by automatic tools [34] to produce various design activity representations. A modified DRed representation, which incorporates the time duration of tasks, is illustrated in Fig. 12. Time-phased captured EEG signals were automatically processed for emotions using a fuzzy model [35], was synchronised with the CAD events with the intention of analysing the engineers emotions during the design task. Fig. 13illustrates that the processed emotion data is mapped onto a manually drawn IDEF0 diagram using the events recorded by the framework as synchronisation milestones.Various studies were performed in the current state-of-the-art research with the aim of understanding the affective status of the user [36–40]. Employing the results from these studies in a ubiquitous CAD environment with multimodal signals is far from trivial. There are many technical challenges to overcome before building up such a complicated system and therefore this study centred on the technical resolution of multimodal ubiquitous data capture. Therefore the finer details of the psychophysiological data processing fall outside the scope of the paper.Synchronisation-related evidences were presented which exhibit the time and chronological event synchronisation aspects of the framework. IDEF0 and DRed illustrations were provided to demonstrate how the metadata can be mapped onto the CAD task progress. These illustrations associated various stages and iterations of the design solution with the user state interpreted from EEG. In summary, the evaluation of the framework via this CAD task demonstrates the technical capability of the framework and its ability to capture metadata in an engineering design task. Further it has been shown that a standard CAD system can be extended to support sophisticated metadata capture, an application for which it was not originally designedThe capture of knowledge relating to a product throughout its lifecycle has been an important issue in engineering industry for many decades, especially in relation to formal design reviews. Traditional knowledge capture methods have typically relied on manual techniques that are time-consuming and disruptive to engineers, resulting in costly overheads [41,42]. Therefore, there has been widespread research into automating the process of capturing engineering knowledge and rationale, and previous work by the authors have successfully demonstrated this through user-logging in virtual design environments [9,13].The Virtual Aided Design Engineering Review (VADER) system addresses the shortcomings of current design review methods by the unobtrusive time-phased capture of multimodal data during individual and group activities performed in a design review. The VADER system developed by the authors allows the 3D visualisation and annotation of computer-aided design models, where all captured data can easily be viewed and searched post-review. Furthermore, review reports and formalised representations of the captured knowledge can be automatically generated and stored in traditional product lifecycle management systems for future reuse. The ubiquitous data capture framework serves as a basis to integrate, capture and synchronise multimodal inputs during the design review. Various multimodal inputs, i.e. text input, audio, video, 3D selection/highlight from centralised and remote/distributed locations are integrated into the metadata system. Fig. 14illustrates the organisation of the capture devices integrated within the VADER system. The virtual reality interface for a design review room is shown in Fig. 15, whereas distributed participants can connect to the system using other simplified and web browser-based interfaces.The captured metadata from multiple sources is stored along with the relevant part (or assembly) name. The text based annotations are stored in XML nodes where a link to CAD data also maintained; therefore the stored data can be accessed bi-directionally. The framework preserves the chronological gathering of captured annotations, events and data streams and populates a time line based on a Unix time source provided by the framework (Fig. 16). The current system captures audio, video, keyboard and mouse input, but the modular nature of the capture framework permits extra input modalities to be easily added. The ubiquitous nature of the framework allows input devices to be plugged in and removed from the data capture system in runtime. This forms a perpetual metadata collection environment which runs in an open-ended fashion, where multiple people with various data interactive tools in distributed locations join and leave the environment as required.This use of the framework in a design review use case is intended to exhibit further capabilities of the framework in addition to the previous evaluation on a CAD system. The applicability of the framework in this environment proves its capability to serve the requirements of a multiuser collaborative environment. The application also demonstrated that users can be seamlessly connected from multiple, distributed sites simultaneously.

@&#CONCLUSIONS@&#
