@&#MAIN-TITLE@&#
A generalized Gilbert algorithm and an improved MIES for one-class support vector machine

@&#HIGHLIGHTS@&#
The primal maximum margin problem of OCSVM is equivalent to a nearest point problem.A generalized Gilbert (GG) algorithm is proposed to solve the nearest point problem.An improved MIES is developed for the Gaussian kernel parameter selection.The GG algorithm is computationally more efficient than the SMO algorithm.

@&#KEYPHRASES@&#
One-class support vector machine,Generalized Gilbert algorithm,Reduced convex hull,Gaussian kernel,Parameter selection,

@&#ABSTRACT@&#
This paper is devoted to two issues involved in the one-class support vector machine (OCSVM), i.e., the optimization algorithm and the kernel parameter selection. For appropriate choices of parameters, the primal maximum margin problem of OCSVM is equivalent to a nearest point problem. A generalized Gilbert (GG) algorithm is proposed to solve the nearest point problem. Compared with the algebraic algorithms developed for OCSVM, such as the well-known sequential minimal optimization (SMO) algorithm, the GG algorithm is a novel geometric algorithm that has an intuitive and explicit optimization target at each iteration. Moreover, an improved MIES (IMIES) is developed for the Gaussian kernel parameter selection. IMIES is implemented by constraining the geometric locations of edge and interior sample mappings relative to OCSVM separating hyper-planes. The experimental results on 2-D artificial datasets and benchmark datasets show that IMIES is able to select suitable kernel parameters, and the GG algorithm is computationally more efficient while achieving comparable accuracies to the SMO algorithm.

@&#INTRODUCTION@&#
As a robust multi-class classification method, support vector machine (SVM) [1] has attracted a great deal of attention as soon as it has been put forward. Over recent years, some fruitful methods have also been built on the top of the standard SVM and bring dramatic performance improvements [2–5]. Generally speaking, the training stage of SVM requires the samples from two classes at least. But in some cases, only the samples from one class are available, and the samples from other classes are too difficult or too costly to acquire. This type of classification problem is known as one-class classification. The available class is called the target class or the negative class, while all other possible classes are collectively referred to as the non-target class or the positive class. In the real world, we often encounter one-class classification problems. For example, a classifier applied to monitor a machine should detect the abnormal and faulty behavior of the machine. In faulty situations, however, the machine might be destroyed completely, leading to the difficult acquisition of abnormal data. On the other hand, the routine data from the normal operation are easy to obtain. As a result, the classifier is generally built only using the normal data. It can be seen that one-class classification plays a significant role in real world applications.Inspired by the principle of SVM, Schölkopf et al. [6] proposed one-class support vector machine (OCSVM) to solve one-class classification problems. Unlike the standard SVM which is originally designed for binary classification problems, OCSVM tries to describe the class of interest by learning from the training samples from only this class, and detects the samples not belonging to it. In addition, Tax and Duin [7,8] also proposed a similar method called support vector data description (SVDD). Instead of the hyper-plane, SVDD is aimed at finding a minimum-volume hyper-sphere enclosing almost all the training samples. Although the goals of OCSVM and SVDD are seemingly different from each other, they are intrinsically equivalent when Gaussian kernels are used [8]. Both OCSVM and SVDD inherit the well-known advantages of SVM, e.g., the utilization of the kernel trick, the robustness to outliers (achieved by adjusting a regularization parameter) and the sparseness of the solution. Due to these advantages, the two one-class classification methods have been successfully used in various applications, such as process monitoring [9–11], computer image [12–14] and network security [15,16]. Because of the equivalence of OCSVM and SVDD, we only focus on OCSVM in this paper.Similar to SVM, OCSVM can also be formulated as a quadratic programming (QP) problem [6]. Some standard algebraic algorithms have been used to solve this optimization problem. However, these standard algorithms generally have cubic time complexity and at least quadratic space complexity [17,18]. OCSVM will consequently suffer the same computational problem as SVM when dealing with large-scale datasets. The complexity, however, may be reduced if a fast optimization algorithm is used, such as the well-known sequential minimal optimization (SMO) algorithm [6,17]. The SMO algorithm breaks the original QP problem into a series of smallest possible QP problems, with each involving only two variables. Each QP sub-problem can be solved analytically in an efficient way. Moreover, a caching scheme is used in this algorithm, with only linear space complexity. In such manners, the SMO algorithm can often speed the training process of OCSVM. During each iteration of the SMO algorithm, only two variables are involved to be optimized, so the SMO algorithm still converges slowly. Nevertheless, as each iteration is computationally simple, an overall speedup is often observed in practice [18]. The SMO algorithm has become one of the most popular algebraic algorithms for OCSVM.From a geometric point of view, for appropriate choices of parameters the C-SVM or ν-SVM [19] optimization problem of finding the maximum margin between two sample sets is equivalent to the problem of finding the closest points on the respective reduced convex hulls of the two sets [20,21]. Therefore, the maximum margin problem can be solved equivalently by solving a closest pair of points problem. Based on the equivalence of the two types of optimization problems, Mavroforakis and Theodoridis [22] proposed a generalized Schlesinger–Kozinec algorithm for SVM. This geometric algorithm presents great computational superiority over the popular SMO algorithm while receiving comparable classification accuracies. As OCSVM originates from SVM, their basic principles are similar. Apart from the algebraic algorithms, it is entirely possible to develop a special geometric algorithm for OCSVM. Using the geometric algorithm, OCSVM is expected to perform more efficiently. Inspired by this idea, a generalized Gilbert (GG) algorithm is proposed for OCSVM in this work. Compared with the SMO algorithm, the GG algorithm is computationally more efficient and has a more intuitive optimization procedure.As OCSVM also uses the kernel trick, its performance is inevitably affected by the kernel parameter. In this paper, only the Gaussian kernel is considered. Thus the Gaussian kernel parameter σ needs to be optimized before the OCSVM model training. Over recent years, some methods have been proposed to select the Gaussian kernel parameter of OCSVM or SVDD (Note that the two methods are equivalent when the Gaussian kernels are used). Tax and Duin [8] proposed a method to tune the Gaussian kernel parameter of SVDD via the leave-one-out (LOO) error, which is estimated by the fraction of support vectors. When the LOO error estimate reaches a user-specific fraction, the corresponding kernel parameter value is considered the optimal one. Since only the target class is taken into consideration for the estimation of the LOO error, this method does not handle the potential non-target class properly, generally leading to an underfitting model. Evangelista et al. [23] defined a statistics2/(κ¯+ɛ)derived from the kernel matrix, where s2 andκ¯are the variance and the mean of non-diagonal entries of the kernel matrix, respectively, and ɛ is a small positive constant. The kernel parameter achieved by this method tends to be small. Deng and Xu [24] proposed a skewness based method to generate artificial outliers, and took the fraction of support vectors and the fraction of outliers accepted by the OCSVM model as the estimates of the type I error and the type II error, respectively. The Gaussian kernel parameter is optimized through minimizing the weighted sum of the two errors. However, this method depends on the quality of the generated outliers heavily, and its performance will deteriorate once the outliers are generated inappropriately. Khazai et al. [25] used a maximum distance based measure to calculate the kernel parameter. This measure only concerns the maximum distance between training samples and does not take the local distribution into consideration, thus the resulting kernel parameter tends to be large. Wang et al. [26] introduced a method called Min#SV+MaxL to select the optimal Gaussian kernel parameter of SVDD. This method seeks the trade-off between the minimum fraction of support vectors and the maximum value of the dual objective function. To be specific, the selection of the kernel parameter depends on the maximum reductions in both the fraction of support vectors and the value of the dual objective function. However, the fraction of support vectors decreases sharply when the kernel parameter  is small. Min#SV+MaxL is consequently prone to select a small parameter value. Xiao et al. [27] proposed an approach called DTL to choose the Gaussian kernel parameter of OCSVM. DTL detects the tightness of decision boundaries through a set of systematic rules, and tunes the kernel parameter iteratively by the bisection method until the candidate interval of σ is sufficiently small. Apart from DTL, Xiao et al. [28] also proposed another parameter selection method based on the tightness of decision boundaries, i.e., MIES. From the view of geometry, if a decision boundary fits the distribution of training samples appropriately, i.e., neither tightly nor loosely, the samples on the edge regions (referred to as the edge samples) of the sample distribution shape should lie close to the decision boundary, while the ones on the interior regions (referred to as the interior samples) should lie far from the decision boundary [28]. MIES utilizes the geometric locations of the edge and the interior samples relative to decision boundaries to determine whether a kernel parameter is suitable. Both DTL and MIES are developed based on the tightness of decision boundaries. In Refs. [27] and [28], the authors compared the two methods with the aforementioned parameter selection methods (proposed in Refs. [8,23–26]) and showed their superior performance.In the procedure of MIES, one key step is the selection of edge and interior samples. However, the selection method for the edge samples may miss critical edge samples when handling the samples on the concave regions, leading to an inappropriate kernel parameter. In order to find a more favorable edge sample detector that suits the cases of concave regions, an Edge Sample Detector based on Circular Paraboloids (ESDCP) is proposed. In addition, we replace the original objective function used in the MIES with a new one that considers the possibly improper selection of the interior sample set. Using the new edge sample selection method and the new objective function, we develop an improved MIES (IMIES) for the Gaussian kernel parameter selection.The remaining part of the paper is organized as follows. Section 2 briefly reviews the basic theory of OCSVM. Section 3 proposes the geometric optimization algorithm. Section 4 presents the Gaussian kernel parameter selection method. Section 5 gives the experiments conducted on 2D artificial datasets and benchmark datasets. Section 6 concludes the paper.AssumeX={xi}i=1lare training samples whose labels are all negative, where l is the number of training samples; Φ( · ) is the mapping function that maps samples from the input spaceX(generally the Euclidean space) to the feature spaceF(possibly a very high-dimensional space). The basic idea of OCSVM is to find an optimal separating hyper-plane that separates training samples from the origin with the maximum margin. This separating hyper-plane is denoted byΠ:f(x)=〈w*,Φ(x)〉−ρ*=0, where w* and ρ* are the normal and the threshold, respectively. The optimization problem of OCSVM has the following formula [6](1)minw∈F,ρ∈R,ξ∈Rl12∥w∥2−ρ+1νl∑i=1lξis.t.〈w,Φ(xi)〉≥ρ−ξi,ξi≥0,i=1,2,…,l.where ν ∈ (0, 1] is a user-specified parameter and ξi(i=1,2,…,l)are the slack variables.The dual problem of the primal problem (1) is formulated as [6](2)maxα∈Rl−12∑i=1l∑j=1lαiαjk(xi,xj)s.t.∑i=1lαi=1,0≤αi≤1νl,i=1,2,…,l.where αi(i=1,2,…,l)are the Lagrange multipliers andk(xi,xj)=〈Φ(xi),Φ(xj)〉is the kernel function. Given the optimal solutionα*=(α1*,α2*,…,αl*)T, the normal w* and the threshold ρ* of the separating hyper-plane Π can be computed using [6](3)w*=∑i=1lαi*Φ(xi),and(4)ρ*=∑j=1lαj*k(xnbsv,xj)where xnbsv is any non-bound support vector (i.e., a sample whose Lagrange multiplierαi*satisfies0<αi*<1/(νl)). Then, a new sample x is classified based on the sign of the decision functionf(x)=〈w*,Φ(x)〉−ρ*.Common choices for the kernel function are Gaussian, polynomial and sigmoid kernels, etc. In this work, only the Gaussian kernel is considered. The Gaussian kernel is given in the following form(5)k(xi,xj)=exp(−∥xi−xj∥22σ2)where σ is the Gaussian width parameter.

@&#CONCLUSIONS@&#
In this paper, we propose a novel optimization algorithm and a novel kernel parameter selection method for OCSVM. For appropriate choices of parameters, the primal maximum margin problem of OCSVM is equivalent to a nearest point problem whose goal is to find the nearest point from the reduced convex hull of training samples to the origin. The GG algorithm is proposed to solve the nearest point problem. It is a straightforward geometric algorithm for OCSVM, with an intuitive and explicit optimization target at each iteration. Moreover, we develop an IMIES for the selection of the Gaussian kernel parameter. Compared with the original MIES, IMIES employs a new edge sample detector called ESDCP to identify edge samples and a new objective function to select the optimal kernel parameter. The experimental results on the 2-D artificial datasets and the benchmark datasets show that IMIES is capable of selecting suitable kernel parameters. We also implement the GG algorithm and the SMO algorithm on the benchmark datasets to investigate their respective performance. From the experimental results, it can be concluded that the GG algorithm is computationally more efficient while achieving comparable accuracies to the SMO algorithm. In our experiments, the parameter ν is always given beforehand; but in fact, it may also affect the performance of OCSVM. Joint optimization of σ and ν can be taken into account in future.