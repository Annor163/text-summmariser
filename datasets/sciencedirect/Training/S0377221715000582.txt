@&#MAIN-TITLE@&#
A MILP formulation for generalized geometric programming using piecewise-linear approximations

@&#HIGHLIGHTS@&#
Converting a GGP problem to a MILP one using piecewise-linear approximations.The approach is simple and ready to implement and solve using commercial MILP solvers.The proposed approach promotes the applications of mathematical programming.

@&#KEYPHRASES@&#
Generalized geometric programming,Piecewise-linear approximation,Mixed-integer linear programming,Gglobal optimization,

@&#ABSTRACT@&#
Generalized geometric programming (GGP) problems are converted to mixed-integer linear programming (MILP) problems using piecewise-linear approximations. Our approach is to approximate a multiple-term log-sum function of the form log (x1 + x2 + ⋅⋅⋅ + xn) in terms of a set of linear equalities or inequalities of log x1, log x2, …, and log xn, where x1, …, xnare strictly positive. The advantage of this approach is its simplicity and readiness to implement and solve using commercial MILP solvers. While MILP problems in general are no easier than GGP problems, this approach is justified by the phenomenal progress of computing power of both personal computers and commercial MILP solvers. The limitation of this approach is discussed along with numerical tests.

@&#INTRODUCTION@&#
This paper focuses on generalized geometric programming (GGP) problems, which, unlike geometric programming problems, cannot be transformed to convex problems. Therefore, this leads to a global optimization problem that is difficult to solve. Global optimization in general is NP-hard. Many methods have been proposed to tackle the global optimization of GGP (e.g., Maranas and Floudas, 1997; Shen, 2005; Wang and Liang, 2005).Lange and Zhou (2014) use a Majorize-Minimization or a Minorize-Maximization (MM) algorithm to separate parameters and reduce the unconstrained GGP problem to a sequence of univariate minimization problems. By exponential variable transformation and exponential function properties, Qu, Zhang, and Wang (2008) solve a sequence of linear programming problems to find the global optimum of the GGP. Lin and Tsai (2012) propose an optimization-based approach to reduce the range of decision variables which use fewer break points in the mixed-integer nonlinear programs used to approximate the GGP. Shen and Li (2013) propose a new branch-reduction-bound algorithm for solving GGP problems. Xu (2014) converts the GGP problems to a series of standard geometric programs by using condense and transformation techniques, and then solves the geometric programming problems to obtain the global optimum of GGP.Among the global optimization algorithms for GGP, Maranas and Floudas (1997) proposed a method based on convex relaxation and a branch-and-bound algorithm. Shen (2005) exploits the exponential variable transformation to approximate the nonconvex parts of GGP by a sequence of linear programming problems. Some recent papers impose restrictions on the decision variables of GGP. For example, Tsai, Lin, and Hu (2007) investigated GGP problems with non-positive continuous variables; and Li and Lu (2009) considered GGP with mixed free-sign discrete variables. In these situations, special treatments are required to handle the additional binary variables incurred for describing the decision variables. Since most methods obtain an approximate optimal solution of a linear or convex relaxation of the original problem, such an optimal solution may not be feasible to the original problem. Shen, Ma, and Chen (2008) developed an algorithm that guaranteed to obtain a robust optimal solution, which is feasible to the original problem.Another popular approach for solving GGP is the convexification method (e.g., Björk, Lindberg, and Westerlund, 2003; Lu, 2012; Lundell and Westerlund, 2009; Pörn, Harjunkoski, and Westerlund, 1999). The basic idea is to transform non-convex signomial terms for convexification based on certain power functions or an exponential function. Li and Tsai (2005) developed another convexification strategy by using inverse transformation functions. The main advantage of the convexification approach is the computational efficiency. However, selecting the right transformation function for convexification may not be trivial. As a result, to enhance computational efficiency rules are imposed to guide the selection process (e.g., Lu, 2012). Our proposed approach is simpler: we use only one variable transformation method to convert the GGP problems. Our approach is not to convexify the GGP. Instead we seek to transform the GGP problems to linear models, which we believe can enjoy even more computational efficiency.In this paper, we utilize the logarithmic variable transformation. The key is to approximate a multiple-term “log-sum” function of the form log (x1 + x2 + ⋅⋅⋅ + xn) in terms of a set of linear equalities or inequalities of log x1, log x2, …, and log xn, where x1, …, xnare strictly positive (e.g., monomials). With this, a GGP problem can be converted to a linear program by directly taking the logarithm of the objective and each of the constraints (some may require rearrangement of terms first). To facilitate this step, we start by approximating a one-dimensional, two-term log-sum function. This process can then be iterated to approximate log-sum functions with three terms, four terms, …, etc.In this paper we use a traditional piecewise linearization approach to approximate the one-dimensional, nonlinear two-term log-sum function. Classic research results on piecewise linearization in literature include approximation formulations (e.g., Geoffrion, 1977; Thakur, 1978) and approximation error bounds (e.g., Güder and Morris, 1994; Thakur, 1978). The piecewise linearization approach has especially been applied in separable convex programs (e.g., Güder and Morris, 1994; Thakur, 1978; 1980; 1984; 1986). Basically, it approximates a one-dimensional, nonlinear function by a piecewise-linear (PWL) function that is the first-degree spline of the nonlinear function. Clearly, the more the segments used, the more accurate results become. Recently the piecewise linearization approach has also been applied to crude oil operations and pooling problems in process optimization (e.g., Gounaris, Misener, and Floudas, 2009; Li, Misener, and Floudas, 2012; Misener and Floudas, 2010; 2012; Misener, Thompson, and Floudas, 2011), which belong to the category of mixed-integer nonlinear programming problems (MINLP). In these applications, PWL underestimations were proposed to approximate bilinear terms. In general, the precision of this approach depends linearly and logarithmically on the number of binary variables incurred from the PWL approximations.However, a PWL approximation with more linear segments incurs more variables and constraints, which increases the computational complexity as well. In fact, given a function over a fixed interval, to reduce the approximation error by an order of magnitude, the required number of segments also increases by an order of magnitude. In this paper we employ PWL approximations to nonseparable and nonconvex GGP. As a result, our linearization requires binary variables. Therefore, in our approach a GGP problem is converted to a mixed-integer linear programming (MILP) problem, with many more variables and constraints than the original GGP problem. When computing power and computer memory are limited, such an approach may be deemed ineffective or even infeasible. Fortunately, the phenomenal progress of computing power for personal computers and improvements in commercial MILP solvers has enabled us to adopt this approach.It is true that MILP is NP-hard and may be no easier than GGP. But we think that our approach has merits. One main advantage is the existence of reliable commercial MILP solvers that are now widely used. These solvers are powerful enough and are being employed to solve large-scale MILPs in various real applications worldwide on a daily basis (e.g., airline scheduling, rail/fleet scheduling, revenue management, and financial portfolio management). Therefore, this paper contributes by providing an alternative method for solving GGP problems that is relatively easier to implement and solve by users who may not have advanced knowledge of global optimization.This paper is structured as follows. Section 2 provides a brief overview of the GGP formulation. We discuss the linearization of the two-term log-sum functions in Section 3. The linearization method is extended to general, multiple-term log-sum functions in Section 4. In Section 5, we propose a general framework to use our proposed approach to achieve ε-global optima. Section 6 discusses the complexity and limitations of the proposed approach. Numerical results are presented in Section 7. This paper concludes in Section 8.LetR++nbe the set of real n-vectors whose components are strictly positive. A functionh:R++n→R,defined as(1)h(x1,x2,…,xn)=d∏j=1nxjaj,where d > 0 and ai∈ R, is called a monomial. A sum of monomials, of the following form,(2)f(x1,x2,…,xn)=∑k=1Kdk∏j=1nxjajk,where dk> 0 and ajk∈ R, is called a posynomial (with K terms).Let x = (x1, x2, …, xn) and X ≡ log (x) = (log x1, log x2, …, log xn). It is clear that the logarithm of a monomial of x is a linear combination of X. Consider the following optimization problem of GGP.(3a)(P)minf0+(x)−f0−(x)(3b)s.t.fi+(x)≤fi−(x),i=1,…,m,(3c)0<xL≤x≤xU,wherefi+,fi−,i = 0, …, m are posynomials and xUand xLare the upper bound and lower bound of x, respectively. (P) can be converted to the following problem that yields the same optimal solution.(4)(P1)minω(5a)s.t.f0+(x)≤ω+f0−(x)(5b)fi+(x)≤fi−(x),i=1,…,m(5c)0<xL≤x≤xU.Note the right-hand side (RHS) of (5a) is also a posynomial. Let Ω ≡ log ω. Here we implicitly assume that ω > 0. In the general case, if ω cannot be guaranteed to be positive, we assume ω is bounded from below by − M < 0 such that M > 0 is known. We can then replace ω by ω + M > 0. Since M is merely a constant, adding it to the objective function will not affect the optimal solution of the problem. Next we consider the following equivalent problem (P2).(6)(P2)minΩ(7a)s.t.log(f0+(x))≤log(ω+f0−(x))(7b)log(fi+(x))≤log(fi−(x)),i=1,…,m(7c)log(xL)≤X≤log(xU).In (P2), eachlogfj+(x)orlogfj−(x)is called a “log-sum” function sincefj+andfj−are posynomials and are sums of monomials. Consider a special case that each log-sum function has only two terms of the form log (z1 + z2), where z1 and z2 are nonzero monomials of x. As already mentioned, both log z1 and log z2 are linear combinations of X. If each two-term log-sum function log (z1 + z2) can be converted to a linear function of log z1 and log z2, then (P2) can be converted to a linear program in the variables X.More precisely, the next step is to convert any (general) log-sum function to a set of linear constraints (including equalities and inequalities) of X and Ω. It will be shown that these linear constraints contain additional variables including binary ones. Thus, (P2) is converted to a MILP of X, Ω, and other additional variables. Its global minimum can be solved using commercial MILP solvers. In the next section, we will start with two-term log-sum functions of the form log (z1 + z2).Consider log (z1 + z2), where z1 and z2 are nonzero monomials of x. The objective is to approximate log (z1 + z2) to arbitrary precision in terms of a set of linear constraints of log z1 and log z2. We have log (z1 + z2) = log z1 + log (1 + s), where s = z2/z1 ∈ R++ and is also a monomial of x. Next, we will represent log (1 + s) in terms of a function of log s, say F(log s). Then we use a PWL function to approximate F( · ). Without loss of generality, we will assume all logarithms used in the remainder of the paper are natural logarithms.Proposition 1Let s ∈ R++ and S ∈ R such that S = log s (so s = eS). Consider a function F: R → R+ such that F(S) ≡ log (1 + eS). The following statements are true.(i)F(log s) = log (1 + s).F( · ) is continuous, differentiable, and convex.S = F(S) − F( − S)(1)Since S = log s, F(S) = F(log s) = log (1 + s).It is obvious that F( · ) is continuous and differentiable. So we have(8)dFdS=dFds·dsdS=11+s·1dS/ds=s1+s>0,and(9)d2FdS2=d(dF/dS)ds·dsdS=s(1+s)2>0.So F( · ) is convex, and (ii) is proved.To show (iii), let z1, z2 ∈ R+ be arbitrary, Z1 = log z1, and Z2 = log z2. It is clear that log (z1 + z2) = Z1 + F(Z2 − Z1) = Z2 + F(Z1 − Z2). This implies that Z1 − Z2 = F(Z1 − Z2) − F(Z2 − Z1). Since Z1 and Z2 are arbitrary, we have(10)S=F(S)−F(−S),∀S∈R,and (iii) is proved.□An illustration of F(S) is given in Fig. 1. When S ≤ 0, F( · ) is asymptotic to the S-axis; when S > 0, F( · ) is asymptotic to the straight line with unit slope that passes the origin. Also F(0) = log 2.We consider an overestimating PWL function for F( · ) by successive secant segments. More precisely, such an overestimating PWL function is the first degree spline of F. For a given secant, e.g., the one (solid line) given in Fig. 1 that intends to approximate F over the interval [S1, S2], the approximation error of this segment is defined as the maximal overshoot within the interval, which occurs at the point T, called an arc tangent point of the interval, whose tangent line is parallel to the secant line. The tangent line is displayed in Fig. 1 as a dashed line, which is a line segment underestimating F( · ) over [S1, S2]. Therefore,(11)m=slopeofthesegment=F(S1)−F(S2)S1−S2=F′(T)=eT1+eT,where the last equality is obtained from (8). That is,(12)T=logm1−m.Define the error function e(S1, S2) as the maximal overshoot of PWL approximation of F over [S1, S2]. For example, referring to the interval in Fig. 1,(13a)e(S1,S2)=F(S1)+m(T−S1)−F(T)(13b)=F(S2)+m(T−S2)−F(T)Proposition 2Consider a secant of F(S) on [S1, S2], whose slope is m. Then:(i)The slope of the secant of F(S) on [ − S2, −S1], the symmetric interval of [S1, S2] with respect to the vertical axis, is 1 − m.If the arc tangent point of [S1, S2] is T, the arc tangent point of [ − S2, −S1] is − T.e(S1, S2) = e( − S2, −S1). That is, any two intervals symmetric to the vertical axis have the same approximation error.(1)Given that the slope of the secant over [S1, S2] is m, and defined in (11), consider its symmetric counterpart interval [ − S2, −S1]. The slope of its secant is(14)F(−S2)−F(−S1)−S2+S1=F(S2)−S2−F(S1)+S1−S2+S1=1−m,where the property in (10) is used.For [S1, S2], the arc tangent point T is log m − log (1 − m), from (12). Similarly, for [ − S2, −S1], its arc tangent point can be obtained by replacing m in (12) by 1 − m using property (i) of this proposition, which gives log (1 − m) − log m = −T.Using (13a) and statements (i) and (ii) of this proposition, we have(15)e(−S2,−S1)=F(−S2)+(1−m)(−T+S2)−F(−T).Using (10) with some algebra, (15) can be reduced to (13b). So e( − S2, −S1) = e(S1, S2).□Given a desired error ε0 > 0, the next task is to obtain a PWL function that approximates F(S) and at each point the approximation error is no greater than ε0. We approximate F(S) over [ − Smax , Smax ] that approximates the real space R, where Smax  is a big number (in the logarithmic domain), say Smax  = 50. This is possible because F(S) ≈ 0 (or F(S) ≈ S) as S ≪ 0 (or S ≫ 0). Note that the following algorithm searches break points sequentially for the PWL function. The starting point of the search is S = log s = 0. While the search from S = 0 should go both rightward and leftward, from Proposition 2 only one direction is sufficient. Also the last break point is always Smax  or − Smax , depending on the search direction. The search algorithms for both directions are similar and are both described below.Data: ε0 > 0 is given; search direction: leftward (or rightward).Step 0:j ← 0; Sj← 0.Use root-finding techniques to determine Sj + 1 < Sj(or Sj + 1 > Sj) such that e(Sj + 1, Sj) = ε0 (or e(Sj, Sj + 1) = ε0), where the error tolerance for the root-finding should be much smaller than ε0 (e.g., we used 10−3ε0).j ← j + 1.If F(Sj) ≤ ε0 (or F(Sj) − Sj≤ ε0), go to Step 4; otherwise, go to Step 1.j ← j + 1, Sj= −Smax  (or Sj= Smax ). Stop.The algorithm above basically performs successive root-finding and can be easily implemented using bisection methods. A PWL function is represented in terms of a set of break points and a set of slopes of segments. Every two consecutive break points are associated with a linear segment and its corresponding slope. Suppose from the algorithm that J segments on the left-hand side of the origin (toward − ∞) are identified with the break points0>S˜1−>S˜2−⋯>S˜J−=−Smax. Using Proposition 2, another J symmetric segments on the right-hand side of the origin with break points0<S˜1+<S˜2+⋯<S˜J+=Smaxcan be identified, such thatS˜j−=−S˜j+,j = 1, …, J. We shall refer to the interval[S˜j−,S˜j−1−]as the jth “left” segment, and[S˜j−1+,S˜j+]as the jth “right” segment. Furthermore, let the slope of the secant on the jth left segment bem˜j−and that of the jth right segment bem˜j+such thatm˜j−=1−m˜j+,j = 1, …, J.When the above algorithm exits from Step 4, it implies that in the last (Jth) left segment, F(S) ≤ ε0; and F(S) − S ≤ ε0 in the last (Jth) right segment. In these two segments, the PWL approximations are very close to its asymptotes. Table 1summarizes the number of PWL segments required for various levels of pre-determined approximation errors ε0. Table 1 also reports the last break point that the algorithm locates before going to Step 4, which isS˜J−1. For example, if ε0 = 10−3, the last left segment of S is [ − 50, − 7.28] and the last right segment is [7.28, 50]. To achieve good results, we recommend to use an ε0 ≤ 0.01.Denote the overestimation PWL function of F(S) obtained in Section 3.1 asF¯(S),associated with ε0. Since in our construction, the error of each segment of the PWL function is a constant ε0, it is apparent thatF¯(S)−ϵ0is an underestimating PWL function of F(S) (see Fig. 1). Denote such an underestimating PWL function asF(S):(16)F̲(S)=F¯(S)−ϵ0.Note that bothF¯(S)andF(S) share the same break points and segment slopes. To be more precise, each of the last (Jth) left segment and the last right segment ofF(S) may have a portion falling below the linear asymptotes of F(S). To improve the quality of the lower bound, one can either add an additional segment on each side, or simply impose two additional constraints,F(S) ≥ 0 andF(S) ≥ S.The next task is to represent the PWL functions mathematically. To evaluate a PWL function, one needs to elect a reference break point. Due to the symmetric nature of the break points, we use the mid-point at S = 0 as the reference point, which is(S,F¯(S))=(0,log2)for an overestimating PWL function of F( · ), and is (S,F(S)) = (0, log 2 − ε0) for an underestimating PWL function. We denote this reference point as (0, FA(0)), where FA(0) is eitherF¯(0)orF(0) depending on the type of the estimating function.We define the length of the jth left segment asΔS˜j−,and that of the jth right segment asΔS˜j+,(17)ΔS˜j−=S˜j−1−−S˜j−=S˜j+−S˜j−1+=ΔS˜j+,j=1,…,J.Now we are ready to revisit log (z1 + z2), where z1 and z2 are nonnegative real numbers.(18)log(z1+z2)=logz1+log(1+z2/z1)Let Q be a new variable denoting the value of the linear approximation of log (z1 + z2) in (18). To use F( · ), we follow the same notation defined in Section 3 that s = z2/z1 and S = log s = Z2 − Z1. We have(19a)Q≈logz1+log(1+z2/z1)(19b)=Z1+F(S).Next we show the MILP formulation for F(S), which can be viewed as a standard building block of our proposed approach.(20a)S=∑j=1Jpj+−∑j=1Jpj−,(20b)F(S)=(FA(0)+∑j=1Jm˜j+pj+−∑j=1Jm˜j−pj−),where the terms in the parentheses in (20b) represent the PWL approximation. In (20b) eachpj−≥0andpj+≥0represent the value drawn from the jth left and right segment, respectively. Additional zero-one variablesuj+anduj−are assigned to indicate whether the jth right segment and the jth left segment, respectively, will be used for the approximation.(20c)ΔS˜j+·uj+1+≤pj+≤ΔS˜j+·uj+,j=1,…,J−1(20d)ΔS˜j−·uj+1−≤pj−≤ΔS˜j−·uj−,j=1,…,J−1(20e)0≤pJ+≤ΔS˜J+·uJ+and0≤pJ−≤ΔS˜J−·uJ−(20f)u1++u1−=1(20g)uj+,uj−∈{0,1},j=1,…,J.Note that (20c)–(20e) imply that:(i)(21)0≤pj+≤ΔS˜j+·uj+,j=1,…,Jand0≤pj−≤ΔS˜j−·uj−,j=1,…,J;(ii)(22)u1+≥u2+⋯≥uJ+andu1−≥u2−⋯≥uJ−;(iii)(23)pj+=ΔS˜j+,ifpj+1+>0andpj−=ΔS˜j−ifpj+1−>0.These imply that the values forpj+andpj−are drawn from the segments sequentially from a lower j to higher j; and a lower indexed segment must be depleted before its following one is drawn. Furthermore, with (20f) and (22), we can conclude that one set of{uj+,pj+}and{uj−,pj−}will be completely zeros.Remarks 11.When there are several two-term log-sum functions to be approximated, such as log (z1, i+ z2, i), i = 1, …, I. Each log-sum function should have its own variables for the PWL approximation, e.g., extending fromQ,(pj+,pj−,uj+,uj−)toQi,(pij+,pij−,uij+,uij−),i = 1, …, I.The PWL approximations for all log-sum functions are the same, if the error allowance ε0 for each approximation is the same. Namely, the same set of{ΔS˜j+,m˜j+}obtained can be used in all log-sum approximations. Therefore, there is the opportunity to store the PWL functions, in terms of{ΔS˜j+,m˜j+},of different error levels ε0 for repeated uses. The number of PWL segments required for various levels of required approximation errors ε0 can be found in Table 1.Consider a two-term posynomialg1(x1,x2,x3)=68x1x2+3651.2x1x2x3−1. We next show how we approximate g1 by a linear model. Let Xi= log xi, i = 1, 2, 3, and G1 = log g1. We have:(24)G1=log(68x1x2+3651.2x1x2x3−1)=log(68x1x2)+log(1+3651.2x1x2x3−168x1x2)=4.22+X1+X2+F(log(3651.2x1x2x3−168x1x2))=4.22+X1+X2+F((8.20+X1+X2−X3)−(4.22+X1+X2))=4.22+X1+X2+F(3.98−X3)Next we shall apply the building block of the MILP formulation for F(S) in Section 3.3 to approximate the last term in (24). Suppose ε0 = 0.01, from Table 1 we have J = 6. From the building block in Section 3.3, 24 variables are introduced:pj+,pj−,uj+,anduj−,j=1,…,6,to describe F(S1) with S1 = 3.98 − X3 using (20a)–(20g).The procedure described in Section 3 for approximating two-term log-sum functions can be used to approximate a general, multiple-term log-sum function. Consider a three-term log-sum function log (z1 + z2 + z3), where z1, z2, and z3 are nonzero monomials. We can group z1 + z2 + z3 to two terms, such as (z1 + z2) + z3. The MILP formulation is as follows.Constraints for the first log-sum include:(25)log(z1+z2)≈logz1+(FA(0)+∑j=1Jm˜j+p1j+−∑j=1Jm˜j−p1j−),(26)logz2−logz1=∑j=1Jp1j+−∑j=1Jp1j−.By the definition of FA(0), ifFA(0)=F¯(0)(or FA(0) =F(0)) in (25), the RHS of (25) overestimates (or underestimates) log (z1 + z2), which is on the LHS of (25). Constraints for the second log-sum include:(27)log((z1+z2)+z3)≈log(z1+z2)+(FA(0)+∑j=1Jm˜j+p2j+−∑j=1Jm˜j−p2j−),(28)logz3−log(z1+z2)=∑j=1Jp2j+−∑j=1Jp2j−.Again, if the FA(0) in (27) is set to beF¯(0)(so is the one in (25)), then the RHS of (27) overestimates log ((z1 + z2) + z3). Likewise, if both FA(0) areF(0), then the RHS of (27) underestimates the LHS. There are additional constraints for PWL approximation, extended from (20c)–(20g) and associated with(pij+,pij−),for i = 1, 2, j = 1, …, J.Now we state the general form for an n-term log-sum function, log (z1 + z2 + ⋅⋅⋅ + zn). First we express (z1 + z2 + z3 + ⋅⋅⋅ + zn) as ((((z1 + z2) + z3) + z4) + ⋅⋅⋅ + zn) and use an additional index k, k = 1, …, n − 1, to indicate the kth two-term log-sum, from inside out. That is, k = 1 refers to the innermost pair of parentheses for (z1 + z2) and k = 2 for ((z1 + z2) + z3). Let Zi≡ log (zi) and Γi≡ log (z1 + z2 + ⋅⋅⋅ + zi). So Γ1 = Z1. For a given n-term log-sum function, log (z1 + z2 + ⋅⋅⋅ + zn) = Γn, the following constraints are needed:(29)Γk+1=Γk+(FA(0)+∑j=1Jm˜j+pkj+−∑j=1Jm˜j−pkj−),(30)Zk+1=Γk+∑j=1Jpkj+−∑j=1Jpkj−.Other additional constraints for PWL approximation are extended from (20c)–(20g) and associated with(pkj+,pkj−),for j = 1, …, J, k = 1, …, n − 1. This formulation is linear in Zk, Γkand other variables for PWL approximations.As discussed earlier, if the PWL approximation for log (z1 + z2 + ⋅⋅⋅ + zn) is intended to be overestimating (or underestimating), all the FA(0) involved in (29) should be set toF¯(0)(orF(0)) for all k = 1, …, n − 1.One concern for representing a multiple-term log-sum function recursively, in terms of two-term log-sum functions, is the accumulation of approximation errors. Let ε0 be the error allowance used in approximating two-term log-sum functions. From (29) with k = 1 and Γ1 = Z1,(31)Γ2=Z1+(FA(0)+∑j=1Jm˜j+p1j+−∑j=1Jm˜j−p1j−),it can be seen that an error ε0 for approximating Γ2 is incurred from the operation on the RHS of (31). For k = 2,(32)Γ3=Γ2+(FA(0)+∑j=1Jm˜j+p2j+−∑j=1Jm˜j−p2j−),the error for approximating Γ3 is now 2ε0, with one ε0 from the two-term log-sum approximation and another ε0 carried from that approximating Γ2. More generally, the error for approximating an n-term log-sum function grows linearly in n. Exact error bounds are hard to write down since Γ3, for example, involves approximating both the function value itself and also the point at which the PWL approximation function is evaluated. To conclude, to approximate an n-term log-sum function requires approximating (n − 1) two-term log-sum functions simultaneously with all approximation errors accumulated.Example 2Continued from Example 1 in Section 3.3, consider a three-term posynomial g2(x1, x2,x3,x4)=g1(x1,x2,x3)+40,000x4−1. Let X4 = log x4. We next show how we approximate G2, the logarithm of g2, by a linear model. We have:(33)G2=log(g1+40,000x4−1)=logg1+log(1+40,000x4−1g1)=G1+F(log(40,000x4−1g1))=G1+F(10.60−X4−G1)In addition to the constraints given in Example 1, we apply the building block of the MILP formulation for F(S) in Section 3.3 again to approximate the last term in (33). With S2 = 10.60 − X4 − G1, and again assuming ε0 = 0.01 as in Example 1, another 24 variables will be introduced to describe F(S2) using (20a)–(20g).Thus far, we have discussed how to approximate the global optimal solution using PWL approximations. In this section, we further discuss how to bound an obtained approximated global minimum. Consider (P2) and we will rewrite its major constraints (7a) and (7b) as follows,(34)Fi+(x)≤Fi−(x),i=0,…,m,where we useFi+(x)=log(fi+(x)),i=0,…,m;F0−(x)=log(ω+f0−(x))andFi−(x)=log(fi−(x)),i=1,…,m. Assume the true optimal objective value of (P2) is Ω*. Next we state how to find the upper boundΩ¯and lower boundΩof Ω*. Assume the corresponding optimal objective value, upper bound, and lower bound for (P1) are ω*,ω¯,andω.To obtain the upper bound of Ω, we replaceFi+(x)≤Fi−(x)in (34) byF¯i+(x)≤F̲i−(x),whereF¯i+(x)≥Fi+(x)andF̲i−(x)≤Fi−(x),for all i and x. We have(35)Fi+(x)−Fi−(x)≤F¯i+(x)−F̲i−(x)≤0.It can be seen that the new constraintF¯i+(x)≤F̲i−(x)is more restrictive than the original oneFi+(x)≤Fi−(x). Therefore, any feasible solutions obtained in the new formulation remain to be feasible to the original problem. Thus, solving the new formulation yields an upper bound of Ω*.Likewise, to obtain the lower bound of Ω* we replaceFi+(x)≤Fi−(x)byF̲i+(x)≤F¯i−(x),whereF̲i+(x)≤Fi+(x)andF¯i−(x)≥Fi−(x),for all i and x. We have(36)F̲i+(x)−F¯i−(x)≤Fi+(x)−Fi−(x);but(37)F̲i+(x)−F¯i−(x)≤0¬⇒Fi+(x)−Fi−(x)≤0.The new constraint set can be viewed as a relaxation of the original constraint set and represents an increase of the feasible region. Therefore, solving the new formulation would yield a lower bound of Ω*. However, from (37) it can be seen that solving the lower bound may not yield a feasible solution to the original problem.How to obtain the upper bounding or the lower bounding function of an approximate function Fi(x) has been described in Section 4. In this section, we present two strategies to solve (P1) with bounds.Strategy 1Using the approach described in Section 5.2, we first obtain the lower bound of (P2) (Ωand the correspondingωfor (P1)). Use the optimal solution obtained from finding the lower bound as an initial solution for local search in the corresponding (P1) domain to obtain a feasible, near-optimal solution, whose objective valueω¯serves an upper bound of ω*.If finding a feasible solution of (P1) is difficult in Strategy 1, one can instead find the upper bound of Ω* first using the approach described in Section 5.1, which will guarantee to yield a feasible solution to (P2). Let this upper bound beΩ¯(andω¯corresponding to (P1)). Certainly, one can use the optimal solution obtained from finding the upper bound as an initial solution for local search in the corresponding (P1) domain to further improve the near-optimal solution. After that, one can repeat the process to find the lower bound of Ω* as described in Section 5.2.The quality ofω¯can be measured by the distance between the upper and lower bounds. That is,(38)|ω¯−ω*|≤ω¯−ω̲.Given an acceptable error ε > 0 to bound the approximated global minimum, if(ω¯−ω̲)/ω̲≤ϵ,ω¯is an ε-global minimum. Otherwise, one should look for more precise PWL approximations for F( · ) by reducing the value of the error ε0 used in the search algorithm given in Section 3. This will result in a PWL function with more segments. The process can then repeat until an acceptable ε-global minimum is obtained.Theoretically, a GGP problem can be converted to a MILP problem with any arbitrary precision desired if the MILP solver has an infinite computing power. The limitation of the proposed approach depends on the computing capability of the available MILP solver. Given a GGP problem in the form of (P1), one can count the equivalent number of two-term log-sum functions involved in the objective functions and constraints (for example, counting (n − 1) for an n-term log-sum function). From the analysis in Section 3, it can be seen that approximating each two-term log-sum function requires 2J continuous variables and 2J binary variables with about 2J linear constraints, where J depends on ε0 (see Table 1). As an example, consider a GGP problem with K = 20 equivalent two-term log-sum functions. If ε0 = 10−4, J = 56 from Table 1, then one can expect that the MILP formulation has about 4KJ = 4480 variables and 2KJ = 2240 constraints, a case that generally can be handled by modern commercial MILP solvers using a personal computer within minutes. Examples are demonstrated in the next section of numerical tests. Note that the analysis assumes that the numbers of original variables and constraints are much smaller than those incurred from the approximations.In this section, the proposed approach is tested on five test problems, the first four are taken from Dembo (1976) and Rijckaert and Martens (1978), and the last one from Lundell and Westerlund (2009) with some modification. Some of the test problems are associated with real applications. Each test problem has an explicit optimization formulation of the following form used in these two original papers:(39a)ming0(x)(39b)s.t.gi(x)≤1,i=1,…,m(39c)0<xL≤x≤xU,where xLand xUare lower bound and upper bound for x, respectively, and each gi, i = 0, 1, …, m is a linear combination of monomials and, therefore, may contain negative coefficients.For each problem, the problem statement, the optimal solution, the obtained approximate optimal solution, and the CPU time used for solving the test problem are documented. Our obtained optimal solutions, the upper bound (ω¯) and the lower bound (ω) of the optimal objective values are compared with the results obtained from literature. In each case, we report the optimality gap, defined as(ω¯−ω̲)/ω̲,to show how close our obtained approximate solution is to the exact optimal solution. Because our upper bound solutions are also feasible, they are viewed as the “best points” obtained by our proposed method when compared with the benchmarks.In solving all approximating MILP problems, an equality is considered to hold if the difference between the constraint value and its right-hand side is within 10−6. The same tolerance is applied to inequalities. Assume that the obtained optimal solution isxϵ0*using ε0 as the piecewise approximation error described previously. In the tables summarizing the test results, “Con-error” (a shorthand for constraint error) recordsmaxi|gi(xϵ0*)−1|,where giis an active constraint forxϵ0*. The constraint that yields the biggest constraint error is highlighted in bold face in each table. Again, Con-error can be compared with ε0 (normally ε0 ≫ 10−6) to observe the computational errors incurred by the proposed approach.Our approach for finding an ε-global minimum follows Strategy 2 presented in Section 5.3 without using any local search. The purpose is to show the crude outcomes (directly obtained from a PWL approximate MILP) of the proposed method so that the reader can better judge its potential. There are at least two implications. First, in some cases our best point (from finding the upper bound) may still be improved with a local search. Second, some test problem (Problem 1) can be converted to a convex problem and, therefore, the result in literature is most likely the exact global minimum. Our result, directly obtained from a PWL approximate MILP, can never be the exact global minimum because of the approximation. However, with a local search, it may reach the exact global minimum. As stated previously, the upper bound is obtained by a near-optimal, feasible solution. Therefore, in the test problems, the Con-errors of the upper bound solutions are all zero.We also test the problems using BARON software. BARON, the acronym of Branch-And-Reduce Optimization Navigator, is a commercial software for solving nonconvex optimization problems to global optimality (Tawarmalani and Sahinidis, 2005). We use BARON as a benchmark to test the applicability of our proposed approach. When solving a (minimizing) optimization problem, BARON reports an optimal solution (upper bound) and a lower bound. It declares global optimality when the corresponding optimality gap is less than a certain threshold. In the following tests, we report these data and the optimality gap.In the following tests, we report CPU times for both our MILP approach and BARON. For our approach, we use a general-purpose MILP solver, ILOG’s CPLEX 12.5, on a personal laptop, which has Dual Intel Core i5-3360M processors with a CPU at 2.80 gigahertz. The formulation is implemented through an ILOG OPL-CPLEX Development System, whose programming language takes a MILP problem in a form similar to the formulation described in this paper. In addition, we use C++ to call the CPLEX functions. The entire program is coded in C++. BARON is run on the same computer and is called as a function by MATLAB.Problem 1(Problem 3 in Rijckaert and Martens (1978), Beck and Ecker (1975))g0(x)=592x10.65+582x10.39+1200x10.52+370x10.22x2−0.22+250x10.40x3−0.40+210x10.62x3−0.62+250x10.40x4−0.40+200x10.85x4−0.85g1(x)=500x1−1+50x2x1−1+50x3x1−1+50x4x1−1This problem is a typical geometric programming problem and can be converted to a convex problem. It is, however, adopted here because of its eight-term log-sum objective function so that we can test the approximation error of multiple-term log-sum functions. This problem has 10 equivalent two-term log-sum functions to approximate. The test result is summarized in Table 2.From Table 2, it can be seen that both the Con-error and the optimality gap are at the same order of ε0 in this problem, although it involves an eight-term log-sum and a four-term log-sum in the objective and constraint, respectively. BARON solves this problem to optimality within a short time.Heat exchange design (Problem 5 in Dembo (1976), Avriel and Williams (1971))g0(x)=x1+x2+x3g1(x)=833.33252x1−1x4x6−1+100x6−1−83,333.333x1−1x6−1g2(x)=1250x2−1x5x7−1+x4x7−1−1250x2−1x4x7−1g3(x)=1,250,000x3−1x8−1+x5x8−1−2500x3−1x5x8−1g4(x)=0.0025x4+0.0025x6g5(x)=0.0025x5+0.0025x7−0.0025x4g6(x)=0.01x8−0.01x5100≤x1≤10,0001000≤x2≤10,0001000≤x3≤10,00010≤x4≤100010≤x5≤100010≤x6≤100010≤x7≤100010≤x8≤1000This problem has 8 variables and 6 constraints and is taken from Dembo (1976). It contains 12 equivalent two-term log-sum functions. Using ε0 = 10−3, the optimality gap is 2.4 percent. When ε0 is decreased to 10−4 and 10−5, it can be seen from Table 3, the optimality gap reduces to 0.2 percent and 0.02 percent, respectively. The Con-errors are observed to be at the same order of ε0.Using BARON, though a good feasible solution is quickly reached, which turns out to be optimal, it fails to improve its lower bound to ensure that the best solution obtained is indeed the global minimum within a preset time limit of 5000 seconds.A 3-stage membrane separation process (Problem 6 in Dembo, 1976)g0(x)=x11+x12+x13g1(x)=1.262626x8x11−1−1.231059x1x8x11−1g2(x)=1.262626x9x12−1−1.231059x2x9x12−1g3(x)=1.262626x10x13−1−1.231059x3x10x13−1g4(x)=0.03475x2x5−1+0.975x2−0.00975x22x5−1g5(x)=0.03475x3x6−1+0.975x3−0.00975x32x6−1g6(x)=x1x5−1x7−1x8+x4x5−1−x4x5−1x7−1x8g7(x)=0.002x2x9+0.002x5x8+x6+x5−0.002x1x8−0.002x6x9g8(x)=x2−1x3x9−1x10+x2−1x6+500x9−1−x9−1x10−500x2−1x6x9−1g9(x)=0.9x2−1+0.002x10−0.002x2−1x3x10g10(x)=x2x3−1g11(x)=x1x2−1g12(x)=0.002x7−0.002x8g13(x)=0.03475x1x4−1+0.975x1−0.00975x12x4−10.1≤x1≤10.1≤x2≤10.9≤x3≤10.0001≤x4≤0.10.1≤x5≤0.90.1≤x6≤0.90.1≤x7≤10000.1≤x8≤1000500≤x9≤10000.1≤x10≤5001≤x11≤1500.0001≤x12≤1500.0001≤x13≤150This problem is another classical application of GGP, which was formulated in terms of a rational function of posynomial terms. This model represented a 3-stage membrane separation process, which has 13 constraints and 13 variables. This problem is taken from Dembo (1976). The result is summarized in Table 4. Using ε0 = 10−3, the optimality gap is 7.2 percent. When ε0 is decreased to 10−4 and 10−5, the optimality gap reduces to 3.5 percent and 0.7 percent, respectively. This shows that our obtained solution (with ε0 = 10−5) is very close to optimal. In this case, although our obtained optimal objective value (upper bound) is slightly higher than that reported in Dembo (1976), their solution has a Con-error of 1.9 × 10−5 from g3, while ours is guaranteed to be feasible with a zero Con-error (Table 4).In this test problem, BARON finds a very good feasible solution. However, it fails to significantly improve the lower bound within the preset time of 5000 seconds. At the end, its optimality gap is only 12 percent, not sufficient to justify that the obtained solution is a global minimum.A 5-stage membrane separation process (Problem 7 in Dembo, 1976)g0(x)=1.262626(x12+x13+x14+x15+x16)−1.23106(x1x12+x2x13+x3x14+x4x15+x5x16)g1(x)=0.03475x1x6−1+0.975x1−0.00975x12x6−1g2(x)=0.03475x2x7−1+0.975x2−0.00975x22x7−1g3(x)=0.03475x3x8−1+0.975x3−0.00975x32x8−1g4(x)=0.03475x4x9−1+0.975x4−0.00975x42x9−1g5(x)=0.03475x5x10−1+0.975x5−0.00975x52x10−1g6(x)=x6x7−1+x1x7−1x11−1x12−x6x7−1x11−1x12g7(x)=x7x8−1+0.002x7x8−1x12+0.002x2x8−1x13−0.002x13−0.002x1x8−1x12g8(x)=x8+0.002x8x13+0.002x3x14+x9−0.002x2x13−0.002x9x14g9(x)=x3−1x9+x3−1x4x14−1x15+500x3−1x10x14−1−500x3−1x9x14−1−x3−1x8x14−1x15g10(x)=x4−1x5x15−1x16+x4−1x10+500x15−1−x15−1x16−500x4−1x10x15−1g11(x)=0.9x4−1+0.002x16−0.002x4−1x5x16g12(x)=0.002x11−0.002x12g13(x)=x11−1x12g14(x)=x4x5−1g15(x)=x3x4−1g16(x)=x2x3−1g17(x)=x1x2−1g18(x)=x9x10−1g19(x)=x8x9−10.1≤x1≤0.90.1≤x2≤0.90.1≤x3≤0.90.1≤x4≤0.90.9≤x5≤10.0001≤x6≤0.10.1≤x7≤0.90.1≤x8≤0.90.1≤x9≤0.90.1≤x10≤0.91≤x11≤10000.000001≤x12≤5001≤x13≤500500≤x14≤1000500≤x15≤10000.00001≤x16≤500This problem has 16 variables and 19 constraints, taken from Dembo (1976). This problem represents a 5-stage membrane separation process that is represented by a nonlinear mathematical model. A large number of chemical flow processes with many recycle streams is described by this model. With a large number of variables and nonlinear constraints, one of the difficulties of this model is to simply compute a feasible solution, as reported in Dembo (1976). Although using our approach, there is no difficulty of obtaining a feasible solution, this problem causes a challenge to our proposed approach in terms of the “curse of dimensionality,” faced by most MILP problems. As in other cases, we start with ε0 = 10−3, the optimality gap is 9 percent, which is too large. As we decrease ε0 to 10−4, the optimality gap is reduced to 2.2 percent; with an extremely long computational time.On the other hand, BARON performs very well. It finds the global minimum within 2600 seconds (Table 5).g0(x)=2x10.9x2−1.5x3−3+4.7x6−1.8x7−0.5x8+5x4−0.3x52.6g1(x)=7.2x1−3.8x22.2x34.3+0.5x4−0.7x5−1.6+0.2x64.3x7−1.9x88.5−0.3(x10.5x50.5+x20.5x60.5+x30.5x70.5+x40.5x80.5)g2(x)=10x12.3x21.7x34.5−x4−2.1x50.4g3(x)=0.6x4−2.1x50.4−x64.5x7−2.7x8−0.6g4(x)=6.2x64.5x7−2.7x8−0.6−x12.3x21.7x34.5g5(x)=3.1x11.6x20.4x3−3.8−0.3x45.4x51.3g6(x)=3.7x45.4x51.3−0.1x6−1.1x77.3x8−5.6g7(x)=0.3x6−1.1x77.3x8−5.6−0.3x11.6x20.4x3−3.8This test problem is modified from Example 6.2 in Lundell and Westerlund (2009), which is a geometric programming problem. We slightly modified the constraints by adding terms with negative coefficients to make it a GGP problem. It contains 8 variables and 7 constraints. The result is summarized in Table 6. Using ε0 = 10−3, we obtain an optimality gap of 0.32 percent. As we decrease ε0 to 10−4 and 10−5, the optimality gap is improved to 0.032 percent and 0.0032 percent, respectively. For this test problem, BARON again is able to find a very good feasible solution, but fails to justify its global optimality within 5000 seconds.

@&#CONCLUSIONS@&#
A simple MILP formulation has been proposed for GGP problems. The formulation is easy to implement and solve by commercial MILP solvers. An important feature of this approach is that the errors of the PWL approximations involved are controlled. Furthermore, only one PWL function is needed for all GGP problems. The approach has been applied to several test problems and shown to be able to locate a global optimum within a desirable error tolerance.In this approach, the only adjustable parameter to affect computational complexity, either to shorten the computing time or increase the solution quality, is the approximation error ε0. From our experience, an ε0 between 10−4 and 10−5 is sufficient for our testing, considering both computational feasibility and approximation precision. In the test problems presented in Section 7, the number of the equivalent two-term log-sum functions for each test problem ranges from 10 (Problem 1) to 41 (Problem 4); accordingly, the number of linear variables in the MILP ranges roughly from 2700 (Problem 1) to 14,000 (Problem 4) with ε0 = 10−4. This shows the wide variety of problems that can be tackled using the proposed approach.We believe that the proposed approach using PWL approximation to convert GGP to MILP has merits in practice and such an approach may be carried over to other global optimization problems. Without having advanced knowledge of global optimization, a user with a basic understanding of MILP may be able to use the proposed approach to solve some global optimization problems within desirable errors. In this sense, the proposed approach also contributes in promoting the applications of mathematical programming.