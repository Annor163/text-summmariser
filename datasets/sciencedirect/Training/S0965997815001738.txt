@&#MAIN-TITLE@&#
Efficient methodologies for reliability-based design optimization of composite panels

@&#HIGHLIGHTS@&#
This paper evaluates several methodologies for design optimization of composite panels under uncertainty.The accuracy and efficiency of several reliability analysis methods and surrogate models are studied and contrasted in an RBDO framework.An application example of a stiffened composite panel of an aircraft fuselage is presented to demonstrate the computational performance and the accuracy of the methods.

@&#KEYPHRASES@&#
Reliability analysis,Design optimization,Surrogate models,

@&#ABSTRACT@&#
The main factors governing the design of composite laminates are the geometrical dimensions, the stacking sequence –including ply thickness and orientation angles–, the mechanical properties of the materials, the applied loads and the performance requirements. Most of these factors are commonly affected by uncertainty and this should be taken into account when designing these structures. Thus, uncertainty quantification should be used to evaluate the performance requirements and a reliability-based procedure is advisable when the design is optimized. However, these methods present several drawbacks, like the lack of trustworthy information about the uncertainty present in the variables of the model or the high computational cost required to apply the algorithms to medium to large models. This paper evaluates several methodologies for design optimization of composite panels under uncertainty. The uncertainty quantification is performed using stochastic expansion and limit state approximation methods. Monte Carlo sampling is also used to verify reliability results. The optimization process is carried out using gradient-based and genetic algorithms, with either continuous or discrete design variables. Surrogate methods, including polynomial, kriging, multivariate adaptive regression splines, and artificial neural networks, as well as parallel computing, have been leveraged to keep analysis times under acceptable levels. An application example of a stiffened composite panel of an aircraft fuselage is presented to demonstrate the computational performance and the accuracy of the methods. Results show major improvement in analysis time without compromising on precision.

@&#INTRODUCTION@&#
Design optimization and reliability analysis are useful techniques that improve structural performance when applied to the design of composite panels. A recent review of selected methodologies and a summary of current developments in this field can be found in [1].The first works about design optimization of composites in presence of uncertainty were published in the 1990’s [2–5]. Since then, several authors have proposed new optimization algorithms or modifications to existing ones which can be successfully applied to optimize composite laminates when uncertainty is considered [6–9]. As discrete design variables are usually required for optimization of the stacking sequence, most of the contributions involve metaheuristic methods, like genetic algorithms [10] or particle swarm optimization [11,12], which are particularly suited for discrete problems.However, these methodologies are very time-consuming and, except in simple cases, they often reach an unacceptable computational cost, because multiple evaluations of implicit functions are required to obtain the structural response. This is the one of the reasons why reliability-based design optimization (RBDO) is not a mainstream design technique in the field of composite structures and most of the existent design procedures are based on semi-deterministic approaches [13,14]. Surrogate methods [15–17] allow the transformation of a complex implicit model into an analytic approximation that decreases in several orders of magnitude the computational cost without a significant loss of accuracy. Parallel computing [18,19] can be used to further decrease the computational cost when evaluating the surrogate approximation.Other researchers have already applied surrogate models to the design optimization of composite structures. Rais-Rohani and Singh [20] considered global and local polynomial surrogates, combined with the first order reliability method. Artificial neural networks have also been applied in this field, see for instance [21].The objective of this work is the application of different reliability analysis and optimization algorithms, combined with surrogate methods and parallel computing, in order to establish some efficient methodologies that make possible the application of RBDO procedures to complex composite structures with a large number of degrees of freedom and several design and random variables. Thus, several up-to-date reliability methods and surrogate models have been applied to a real design case of an aircraft structure, consisting of a curved stiffened panel of a fuselage composed by a carbon/epoxy composite skin stiffened by aluminum ribs and stringers. Two structural limit state functions –buckling and Tsai–Wu failure stress criterion– have been considered with a serial failure mechanism. The efficiency and accuracy of the results obtained with different methods is studied and compared.The RBDO problem is carried out by the performance measure approach and reliability analysis is performed using the first order reliability method, polynomial chaos expansion and stochastic collocation. Monte Carlo sampling is also used to verify reliability results. The optimization process is carried out using sequential quadratic programming when considering continuous design variables and genetic algorithms when dealing with discrete design variables. The surrogate models are built using polynomial, kriging, multivariate adaptive regression splines, and artificial neural networks. In the next sections the formulation is presented and the methodologies are applied to a demonstration example.In engineering design, structural safety is usually verified by means of semi-deterministic procedures with safety coefficients that estimate the effects of uncertainty in the structural system, increasing the value of loads and decreasing the level of the material strength. In relation to composites, there are several national and international standards for the design of certain classes of composite structures that make use of safety coefficients [13,14].Reliability analysis methods assess the structural safety taking into account the random nature of all phenomena affecting to a structural system [22]. In these methods, the design region is divided by a limit state function g(a) as(1)Failuredomain:F={a|g(a)<0},(2)Safetydomain:S={a|g(a)⩾0}.The boundary between failure and safety domains is the failure surface or limit state surface, which generally is a hypersurface ofn−1dimensions in the n-dimensional space of random variablesa. According to this, the probability of failure pfis formulated as(3)pf=P[g(a)<0]=∫…∫g(a)<0fA(a)da,where fA(a) is the joint probability density function of the random variables. Except in some particular cases, the integral expression (3) cannot be resolved analytically, because of the nonlinearity of fA(a) and also due to the fact that the number of random variables usually employed is large, and therefore the dimension of the problem. Besides, the joint probability density function is usually not available. In consequence, alternative methods have to be used to solve (3). In this work, different methods of reliability analysis are applied to evaluate the uncertainty constraints within an optimization procedure. Some of these methods and their implementation have been studied by the authors in the past [23,24], demonstrating a good performance and giving accurate results. The methods are briefly described next.The FORM or first order reliability method [25] approximates the limit state function by the Taylor series expansion of first order at the most probable point of failure (MPP)af′,which is the minimum distance point on the limit state surface from the origin of the coordinate system in the standardized domain of the random variablesa′:(4)g′(a′)≃g(af′)+∇g(af′)T(a′−af′).The reliability index is invariant with respect to the formulation of the limit state function and is obtained as(5)β=−af′T∇g(af′)∇g(af′)T∇g(af′).In order to use non-Gaussian random variables, a transformation needs to be applied. In this work, we have employed the Nataf transformation [26]. This transformation only needs the marginal cumulative distribution functionFAi(Ai)and the correlation matrix of the variables CA. Thus, transformed variables can be expressed in the normal space as(6)ai′=Φ−1[FAi(ai)]where Φ is the cumulative distribution function of a standard normal random variable. The joint probability density function ϕncan be characterized in the space of transformed variables as(7)ϕn(a′,CA)=1(2π)n|CA′|exp[−12a′TCA′−1a′].The stochastic expansion methods are based on the work by [27]. These methods employ the concepts of projection, orthogonality and weak convergence by means of multidimensional polynomial approximations. The final solution is expressed as a functional mapping, instead of a set of statistics. In this paper, polynomial chaos expansion and stochastic collocation have been considered [28,29].Polynomial chaos expansion (PCE) is based on multidimensional orthogonal polynomials to approximate the functional form between the stochastic response output and each one of the random inputs. The Wiener–Askey scheme [30,31] is used in this work. A simple definition of the PCE for a Gaussian random response R as a convergent series is as follows(8)R(a)=θ0Γ0+∑i1=1∞θi1Γ1(ξi1(a))+∑i1=1∞∑i2=1i1θi1i2Γ2(ξi1(a),ξi2(a))+∑i1=1∞∑i2=1i1∑i3=1i2θi1i2i3Γ3(ξi1(a),ξi2(a),ξi3(a))+⋯where{ξi(a)}i=1∞is a set of Gaussian random variables,Γp(ξi1,⋯,ξip)is the generic element of a set of multidimensional Hermite polynomials, andθi1,⋯,θipare deterministic constants. Eq. (8) can be written more simply as(9)R(a)=∑i=0PbiΨi[ξ(a)],where biand Ψi[ξ(a)] are one-to-one correspondences between the coefficientsθi1,⋯,θipand the functions Γp(ξi1, ..., ξip), respectively. The general expression to obtain the Hermite polynomials [32] is formulated as(10)Γp(ξi1,⋯,ξip)=(−1)n∂ne−12ξTξ∂ξi1,⋯,∂ξipe12ξTξ.Stochastic collocation (SC)[33] is a stochastic expansion approach that is closely related to PCE. It is defined as a sum of multidimensional Langrange interpolation polynomials, considering one polynomial per collocation point. The coefficients of the expansion are the response values at each collocation point, and can be formulated as(11)R(a)=∑j=1NcprjLj(ξ(a)),where Ncp is the number of collocation points, rjis a coefficient to reproduce the response values andLjare Lagrange polynomials, which are defined as(12)Lj=∏k=1k≠jmξ−ξkξj−ξkwhere m is the order of the polynomial.The application of the Monte Carlo method [34] to obtain the solution of (3) requires the introduction in the integrand of a functionυ(a)such that(13)υ(a)={1,ifg(a)⩽00,ifg(a)>0.This way, the domain of integration in Eq. (3) includes the whole real domain and the probability of failure can be formulated as(14)pf=P[g(a)⩽0]=∫…∫υ(a)fA(a)da.If a Monte Carlo sampling is applied to the previous equation so that m samples are generated, of which mflead to g(a) ≤ 0, then the probability of failure pfcan be estimated as(15)pf≃1m∑j=1mυ(aj)=mfm.The accuracy of the previous estimation increases with the number of samples m, although it is related with the probability of failure too, since a very low value of pfrequires a larger number of samples to obtain some results in the failure domain.Reliability-based design optimization combines a conventional deterministic design optimization algorithm with a reliability analysis method to evaluate the design constraints that depend on the random variables of the problem. This is an active research topic and many authors have contributed so far. Some interesting references can be found in Youn and Choi [35], Aoues and Chateauneuf [36], Choi et al. [29] and Yao et al. [37], and some current applications in the field of composite materials can be found in [38,39].In a RBDO problem both design variables and parameters can contain deterministic and/or random quantities. According to that,xis designated as the vector of design variables andprepresents the design parameters. On the other hand,acontains the random variables anddthe deterministic quantities. These vectors can be expressed as(16)x={xaxd},p={papd},a={xapa},d={xdpd};where the subscripts a and d indicate random or deterministic, respectively. Furthermore,μacontains the mean value of the random variables when they need to be considered in a deterministic procedure.The formulation of a RBDO problem can be set out in different ways, depending on the point where the procedure to evaluate the reliability constraints is connected to the optimization process. Thus, bi-level or double loop approaches consider the reliability constraints within the optimization loop. On the other hand, mono-level approaches replace the probabilistic constraints with approximate deterministic values, converting the double loop in a single loop. Finally, decoupled approaches solve the RBDO problem as a sequence of deterministic optimization procedures.The general formulation of the bi-level approach can be expressed as(17)minF(μa,xd,pd)subject to:(18)pf,i(a,d)⩽pf,i,maxi=1,…,m(19)hj(μa,xd,pd)⩽0j=m+1,…,Mwhere F is the objective function, pf, iare the probability of failure of the limit state functions, which have to be below the maximum probability of failure pf, i, maxconsidered, and hjare the deterministic design constraints. The number of probabilistic constraints is m,M−mis the number of deterministic constraints and M is the total number of constraints.When using FORM as reliability method, there are two types of bi-level approaches: reliability index approach (RIA) [40] and performance measure approach (PMA) [41]. In the former, the outer loop uses reliability indexes to define the non-deterministic constraints. The inner loop obtains those reliability indexes from the limit state functions. A RBDO problem with RIA approach is formulated as(20)minF(μa,xd,pd)subject to:(21)βi(a,d)⩾βi,mini=1,…,m(22)hj(μa,xd,pd)⩽0j=m+1,…,Mwhere βiare the reliability indexes, which have to be above the minimum level of reliability βi, min. Alternatively, a double loop RBDO problem with PMA approach is formulated as(23)minF(μa,xd,pd)subject to:(24)Gi(a,d)⩾0i=1,…,m(25)hj(μa,xd,pd)⩽0j=m+1,…,Mwhere Giare the probabilistic design constraints. The RIA is less stable, in numerical terms, than PMA [41–45]. In consequence, when using FORM, a double loop framework with PMA has been selected as RBDO methodology.Reliability methods have been described before and have been chosen in order to contrast widespread and up-to-date methods. When selecting optimization algorithms, it must be had into account the type of design variable considered in each case. Thus, if all variables are defined in a continuous domain, a gradient-based algorithm is the most recommendable choice given its relative reduced computational cost and high convergence guarantees. In this study, the sequential quadratic programming (SQP) [46,47] is employed due to its capabilities and general acceptance in the industry. On the other hand, when discrete variables are also considered, a derivative-free method must be employed. There are a wide range of recent metaheuristic algorithms [48–50] to carry out this task, however, as this work is presented as a guide for industrial application cases, genetic algorithms (GA) [51,52] are preferred due to its extensive application in the industry, tested performance and availability in commercial software packages. Nevertheless, any other metaheuristic algorithm can be employed, which in some cases could improve the performance of the optimization phase.Surrogate methods [53–56] are a valuable tool to deal with the extensive computational cost of RBDO procedures [57–61], especially those on which reliability analysis is performed by a MPP search or a sampling method; or the optimization loop is led by an expensive algorithm like GA. All of these procedures usually require many evaluations of the structural response and very often this response is obtained using a finite element analysis or some other relatively complex numerical simulation. Surrogate methods replace a complex model with an approximation that is much faster to evaluate making them very suitable for computationally intense methodologies.There is a wide range of surrogate models [17], classified into three main groups: data fit surrogate models, multifidelity surrogate models and reduced order models. In the data fit surrogate case, the surrogate model is a non-physics-based approximation created from a set of data generated with the original model. These methods can be characterized by the number of data points used in the fit: local approximations (data from a single point), multipoint approximations (a small number of data points) and global approximations, which use a set of data points distributed over the domain of interest. Due to the nature of the problem in study, a global approximation will be used.Polynomial surrogates are defined using multivariate polynomials, according to the general expression(26)f^P(x)=c0+∑k=1p(∑i1=1n⋯∑ik⩾ik−1nci1⋯ik∏j=1kxij),wheref^pis the response of the surrogate model, p is the order of the polynomial, n is the number of variables,xijare the components of the n-dimensional vector of variables and the terms c0 andci1⋯ikare the polynomial coefficients. The number of coefficients ncneeded for a polynomial approximation of order p is(27)ncP=∏i=1pn+ii,and this is the minimum number of samples required to form a fully determined linear system and solve for the polynomial coefficients.Kriging emulatorf^K[62,63] combines a regression model or trend functionκ(x)Tρwith a stationary gaussian process error model ε(x) that is used to correct the trend function(28)f^K(x)=κ(x)Tρ+ɛ(x).Common regression models involve zero, first and second-order polynomials. The stationary gaussian process has zero mean, constant variance and a stationary autocorrelation function r(x, x′). There are many options for r(x, x′), but the most used class of autocorrelation functions is the anisotropic generalized exponential model(29)r(x,x′)=exp(−∑k=1Dθk|xk−xk′|γ),where D is the number of input dimensions, θkare the correlation parameters and γ is a parameter which must satisfy 0 < γ < 2.The multivariate adaptive regression splines (MARS) surrogate model approaches the objective function by means of a continuous surface of splines. MARS consists of a non-parametric regression technique first described by Friedman in 1991 [64]. The approximation is of the form:(30)f^MARS(x)=∑m=1MbamBm(x),where amare the coefficients of the power basis functions Bmand Mbis the number of functions. The model consists of a weighted sum of Mbbasis functions, Bm(x). These basis functions take the form of hinge functions, described in detail in [64]. The design space is split into sub-regions, in each of which regression methods are applied to fit a local approximation. Then, these approximations are joined together to produce a global, smooth surrogated model.The artificial neural network (ANN) model consists of training a network of neurons by adjusting their connecting weights. One of the first approaches was made by McCulloch and Pitts [65]. The ANN surface fitting method used in this work employs a stochastic layered perceptron (SLP) neural network based on the approach of Zimmerman [66], which is designed to have a lower training cost than traditional ANNs. The approximation is of the form:(31)f^ANN(x)≈tanh(tanh((xA0+θ0)A1+θ1))where the termsA0, θ0,A1 and θ1 are the matrices and vectors that correspond to the neuron weights and offset values in the ANN model.In order to obtain the coefficients of the surrogate models, a data set must be generated from the real model using a design of computer experiments procedure (DACE). Latin Hypercube Sampling (LHS) [67] will be used for this task. LHS is a stratified sampling technique where each dimension of the design space is divided into as many segments as the number of samples Npand one sample is randomly obtained within each one of these hypercubes. The nature of the probability distribution specified for each random variable will determine the relative lengths of the aforementioned segments. The surrogate model is fitted to the Npsamples by a least-squares procedure which gives the value of the surrogate model coefficientscas(32)mincp∑i=1Np[fi(x)−f^i(x,c)]2.It must be pointed out that the number of samples Np, as well as how they are generated, is one of the main issues with the accuracy of the surrogate models because it determines the parameter estimation process. The number of samples is strongly conditioned by the number of variables, both random or design variables, that is considered in the process, as well as the particular characteristics of the domain of study, the sampling method or the surrogate model employed. The quality of the approximation of the limit state functions is crucial for obtaining accurate results in the RBDO, and it mainly depends on the number of samples for its generation which is defined by the number of random design variables. In the same way, the accuracy of the design domain depends on the number of samples employed to approximate the objective function, and this number is a consequence of the number of design variables. Besides, even when the number of samples is enough, the parameter estimation and training process becomes more complicated in problems with a large number of variables, fact that affects to the accuracy of the surrogate model. More details about this issue can be found in Forrester et al. [17]In order to test the accuracy of the obtained approximations, the correlation coefficient r2 will be used,(33)r2=(Np∑i=0Npfif^i−∑i=0Npfi∑i=0Npf^i[Np∑i=0Npfi2−(∑i=0Npfi)2][Np∑i=0Npf^i2−(∑i=0Npf^i)2])2,as well as the root mean squared error (RMSE) metric, which is defined as(34)RMSE=∑i=0Np(fi−f^i)2Np.The values of those coefficients are employed to accept or reject a hypersurface as a surrogate of the model under study with a cross-validation strategy.This model validation technique consist of spliting the sample of data into k roughly equal subsets called folds. One of the subsets is removed and the model is fitted to the remainingk−1subsets, aggregately. The model is then compared to the data points in the fold set aside, allowing to test the model predictive capabilities. If we call ϒ to a mapping so that ϒ: {1, ..., n} → {1, ..., k}, this mapping allocates the n training points to one of the k subsets. We also definef^−Υi(x)as the value of the surrogate model obtained by removing the fold ϒi. The cross-validation measure, used to estimate the error of the prediction, is(35)ɛ(c)=12∑i=1n[fi(x)−f^−Υi(xi,c)]2Fig. 1describes the organization of a RBDO procedure using a surrogate model to obtain the responses required by the optimization and reliability analysis algorithms. The iterator is in charge of obtaining the optimum point, starting from the initial design, using either SQP or GA algorithms. To evaluate the probabilistic constraints, a reliability analysis algorithm is required. In this work FORM, PCE, SC and Monte Carlo are used. All the previous algorithms require values of the structural responses at some point. With a non-surrogate evaluator, a simulation would have to be run every time the iterator requires a response. Instead, when a surrogate method is used, the simulations are run only a limited number of times when the surrogate model is built. After that, the structural responses required by the iterator are returned by the surrogate model, being this much faster than the simulator.Besides, both the sampling process required to obtain the data for building the surrogate model and the RBDO process are carried out using parallel computing [68–73]. This allows the convergence of the optimization and reliability algorithms in a fraction of the time that their serial counterpart require, as well as drastically reduces the wall-clock time of the sampling process.The structure selected to test the performance of the previous methodologies is a stiffened panel with a composite skin reinforced with four aluminum frames and stiffeners (Fig. 2). The panel is 2056 mm long and 554 mm wide and has a curvature with a radius of 2012 mm. Two loads are applied on the structure: a compression load and a shear load. The compression load consists of an edge load applied along the main dimension of the panel, while the shear load is applied along the other two edges. Both loads are considered as random variables with the properties defined in Table 1.The composite material is carbon/epoxy with an ultimate longitudinal strength in tension Xtand compression Xcof 2070 MPa and 1160 MPa, respectively. The ultimate transverse strength in tension Ytis 29 MPa and 157.9 MPa in compression Yc. The interplane shear strength S is 91 MPa. The longitudinal Young’s modulus E1 has a value of 129.9 GPa, while the transverse Young’s modulus E2 is 9.2 GPa and the Poisson’s ratio is 0.36. The density of the composite is 15.6 kN/m3. All these quantities are deterministic design parameters except Xt, Xcand E1, which are considered random variables with the properties shown in Table 1. The stacking sequence of the composite skin is [± 45/90/0]sand the thickness of each layer (ti) is a random design variable with the uncertainty indicated in Table 1. It must be noticed that the mean value μ of the thickness of each layer can change during the optimization process, and consequently the coefficient of variation δ, while the standard deviation σ is considered constant during the process. The value indicated in Table 1 for the mean of the thickness is employed as the initial point in the optimization. All these random variables are considered independent.The stiffening system consist of four frames and four stiffeners with a pitch of 656 mm and 150 mm, respectively. These frames and stiffeners are made of aluminum with an elastic modulus of 72 GPa and density of 27.7 kN/m3. The cross-section of the frame is a C profile with a height of 50 mm, flange width of 10 mm and a thickness of 0.5 mm. The cross-section of the stiffener is an omega profile with a height of 10 mm, thickness of 0.5 mm and 10 mm width of flanges and head. The aluminum properties are considered as deterministic design parameters.The objective function in the optimization phase is the total weight of the structure and two limit state functions have been considered as design constraints in order to verify the structural reliability. The first limit state evaluates the buckling factor λ of the first buckling mode (Fig. 3), which must be greater than a minimum valueλmin=100. This condition can be expressed as(36)G1(x)=Φ−1(1−p[g1(x)⩽0])−βmin⩾0,where(37)g1(x)=λλmin−1.The minimum reliability index β considered in all cases to verify the probabilistic constraint isβmin=3,which is associated with a probability of failure of(38)pf=1−Φ(βmin)=1.35·10−3.The second limit state is based on the Tsai–Wu first ply failure criterion. This criterion takes into consideration the interactions between different stress components and is expressed as(39)G2(x)=Φ−1(1−p[g2(x)⩽0])−βmin⩾0,where(40)g2(x)=1−[σ12XtXc+σ22YtYc+τ122S2−σ1σ2XtXcYtYc+(1Xt−1Xc)σ1+(1Yt−1Yc)σ2]where Xcand Xtare the compressive and tensile strengths parallel to the fiber directions. Ycand Ytare the compressive and tensile strengths orthogonal to the fiber directions and S is the shear strength. σ1, σ2 and τ12 are the stresses in material directions 1 and 2 and the shear stress, respectively. All the material strengths are design parameters. When deterministic optimizations are carried out in order to verify probabilistic results, g1(x) ≥ 0 – Eq. (37)– and g2(x) ≥ 0 –Eq. (40)– are considered as deterministic constraints.The structure is discretized with a finite element mesh using four node shell elements for the skin and beam elements for the frames and the stiffeners, resulting in 14,592 degrees of freedom. All the computations were carried out in a high performance computing cluster with a theoretical peak performance of 5.1 TFLOP’s, a physical memory of 1792Gb and 512 computing cores. The simulation code was Abaqus/Standard 6.11 [74] and the complete RBDO procedure was implemented using Dakota framework [75]. The coupling of these codes has been validated in the past by the authors, see for instance [76].The panel has been studied under two different combinations of random and design variables. Besides, in order to study the performance of different methods in the models, several analysis cases are defined in each model. The results are presented next.A simple model with only one random design variable (RDV) and one random variable (RV) is considered to demonstrate the computational capabilities of the methodology (model 1). Besides, several surrogate models and algorithms, continuous and discrete variables, local and global optimization, as well as several reliability methods are employed. All the analysis cases are configured such that the thickness of the composite skin is minimized, subject to the limitations imposed by the buckling and Tsai–Wu design constraints. Thus, the thickness of the stacking sequence of the composite skin of the panel is considered as a random design variable, with an initial value of 5 mm, a lower bound of 3 mm, an upper bound of 7 mm and a standard deviation σ of 0.25, which corresponds to a coefficient of variation δ of 5% in the initial point. It is worth mentioning that, although the mean of the random variable changes its value every time the optimization algorithm moves away from the previous point, the standard deviation remains invariant. Thus, the δ is changing in each iteration. Other configurations are possible, like fixing the δ and varying the standard deviation.The surrogate models employed are cubic polynomials, kriging with a reduced quadratic trend function, ANN with 15 nodes for the deterministic surrogate and 30 for the probabilistic and MARS with 15 bases. All the surrogate models are obtained from 40 or 20 samples, depending on if they are probabilistic or deterministic, whose generation mean time is 18.13 s and 17.89 s. Given that parallelism is employed, the computational time of these sampling is equivalent to the maximum time used in all samples. This data is generated by latin hypercube sampling.RBDO is carried out with two limit state functions considering a serial failure mechanism. The optimization process is situated in the outer loop and is carried out by sequential quadratic programming. SQP convergence tolerance is set to1·10−8, while gradients are obtained by forward finite differences with a step size of 0.01. The constraints satisfaction tolerance, which defines how tightly constraint functions are to be satisfied at convergence, is set to1·10−3. This allows that the constraints defined in Eqs. (37) and (40) can reach negative values closed to 0 without being considered as violated constraints, as can be seen in Table 2.Four reliability methods are used in the RBDO, being later compared with the deterministic solution. These methods are FORM, PCE, SC and MCS. FORM is carried out with SQP as the optimization algorithm used to perform MPP searches. This SQP has the same parameters as the one used in the outer loop. PCE uses Askey approach with Hermite polynomials. The coefficients biof the expansion are obtained by a LHS with 40 samples. The order of the expansion is quadratic. SC also uses the Askey approach. To form the multidimensional interpolants Lj, a sparse grid from 40 samples is used. MCS is carried out with 105, 106 and 107 samples. Hence, for a 95% confidence interval, relative errors [77] are under 10.67%, 3.37% and 1.07%, respectively.Table 2 shows the results and performance of RBDO using different surrogate models and reliability methods. Time, number of evaluations and iterations are shown to contrast the computational requirements between different methods and models, and the improvement of using surrogate models or emulators instead of evaluating recursively the original simulation. It must be pointed out that the evaluations shown in Table 2 are simulations in the case of non-surrogate RBDO, and emulations in case of using surrogate models. In the surrogate case, the simulations are carried out in the sampling process. The time shown in Table 2 is the total wall-clock time of each RBDO process, but when surrogate models are employed the time required for the sampling process is not included in order to facilitate comparisons, as indicated in the table.The computational times of the training process are irrelevant for polynomials, ANN and MARS models, reaching values lowers than 0.5 s is all cases. In the case of kriging models, the convergence of the training process is much more slower, reaching values that can be up to 95% of the process time shown in Table 2. The time required for the RBDO process is also very reduced if surrogate models are used, and whereas the algorithm needs to evaluate recursively the simulation, the time of this process depends on the cost of the simulation. Stochastic expansion methods converge in several orders of magnitude less time than FORM in the non surrogate case. Notwithstanding, this does not affect the computational time in the surrogate case. MCS is much more time-consuming due to the high number of evaluations required, even in the surrogate case. It must be noticed that the number of iterations and evaluations are quite similar for all surrogate methods and the non-surrogate case.In conclusion, and focusing now in the total wall-clock time, the non-surrogate case obviously requires more time than the cases where a surrogate model is used. However, in the latter case the loss of accuracy is irrelevant. It is also expected that the weight of the computational cost lies in the construction of the surrogate model. This is mainly due to the required computer experiments running over the time-consuming ‘truth’ model, while the computational cost of the analytic evaluations is negligible. Thus, with a complex model considering more variables, the computational cost will be similar, as it will be shown below, although the number of samples needed for the surrogate model construction should increase with the number of variables. This fact has no influence on the computational cost, since all evaluations are run in parallel. Figs. 4and 5illustrate the effect of parallelism allowed by surrogate models. While the number of evaluations is higher (Fig. 4), wall-clock time is some orders of magnitude lower (Fig. 5).With the aim of estimating the accuracy of the three surrogate response surfaces (objective function and constraints), the correlation coefficient r2 and the RMSE with cross-validation considering 10 folds are employed. These values are used to accept or reject a surface. Polynomial models can be perfectly tested by correlation coefficients, which in the cases shown in this work are1−r2=2.00·10−11,1−r2=2.20·10−8and1−r2=9.10·10−7,respectively; showing a good correlation for the three surfaces. Besides, the RMSE found with cross-validation are4.68·10−6,3.43·10−4and2.97·10−4,respectively. In the case of ANN models, the RMSE found for the three surfaces are, respectively,3.13·10−1,1.63·10−1and5.41·10−3. Finally, the RMSE founded in MARS surfaces are3.60·10−2,2.18·10−2and7.43·10−3.Regarding the numerical results, it must be pointed out that all the employed surrogate models allow to obtain accurate results with reduced relative errors, as shown in Table 2. MCS is used as reference to validate the results of other methods. Although this is unaffordable in non-surrogate approaches, in the surrogate cases gives an accurate idea of the solution. When the number of samples increases, the results are closer to the results obtained from stochastic expansion methods, which means that these methods obtain a very accurate result as well. However, FORM method converges to a very close result but with a relative error greater than the other methods. The active constraint is the Tsai–Wu criterion, while the buckling constraint is far from being active.Surrogate-based RBDO is also very useful when the algorithms used in the RBDO analysis have a very high computational cost, like global optimization methods. In this study, the use of non-gradient based global optimization methods in RBDO, like genetic algorithms, is also considered when dealing with discrete design variables. The results of applying those methods to model 1 with a MARS model with 15 bases are shown in Table 3.The genetic algorithm employed in these cases is used with a two point crossover with a probability of 0.8. The mutation type is offset uniform and the probability of mutation is 0.8. The population size is 50 individuals and the elitism considered is 5. The maximum number of generations allowed is 1 · 104 and the reliability analysis is carried out by SC.The discrete design variable has the same initial point, lower bound and upper bound than the continuous case. However, in the discrete case, it can only take values from the lower to the upper bound with an increment of 0.1. The random properties are the same as in the continuous case.Table 3 shows a comparison between continuous and discrete results, as well as between surrogate and non-surrogate approaches. Besides, deterministic results are also shown. Deterministic analysis can be computed without surrogate models, given that the average computational cost of an evaluation of the model is lower than 20 s, and the number of variables is very reduced. However, RBDO with discrete variables, even with only one RDV and one RV, became very time-consuming because of the use of expensive algorithms in both the inner and outer loop.In order to test the performance of the methods in larger models, the composite panel is now studied increasing the number of random and design variables. This model (model 2) tries to optimize the thicknesses of each ply in the stacking sequence. Besides, loads, material properties, and even the thicknesses of each ply are considered as random variables with the properties shown in Table 1. This means that there are four random design variables (ti), and five random variables. The four design variables have an initial point of 0.6985 mm, lower bound of 0.1397 mm and upper bound of 1.397 mm. These values are multiples of 0.1397 mm, which is the minimum thickness of a ply.The surrogate model employed is MARS due to its good performance in the previous model. This time, 50 bases are used in order to represent accurately the three surfaces, which now depend on more variables. All the surrogate models are obtained from 1000 or 250 samples, depending on if they are probabilistic or deterministic, whose generation mean time is 20.11 s and 21.60 s, respectively. Reliability and optimization methods are implemented with the same parameters defined in Section 5.1, although 100 samples are used for PCE and SC.Results for model 2 are shown in Table 4. Given the difference in computational time between FORM and PCE or SC (Table 2), only PCE and SC are calculated without a surrogate model. It is easy to appreciate the increase of the required time in these methods due to the increment in the number of variables of the model. However, computational time of the surrogate models remains under 10 s (except for MCS), provided that the set of data is obtained in parallel and the cost of the evaluation of the surrogate surfaces is irrelevant.Non-surrogate RBDO carried out by stochastic expansion methods converge to a different design of the obtained by deterministic analysis. The latter increases the value of t45 and t90, leavingt−45and t0 with lower values, due to the configuration of the loads. In the former case, the influence of uncertainty in materials and loads leads the optimization to a different design, in whicht−45and t90 have similar influence and t45 increases its value. Surrogate-based optimization reaches similar results, although part of the thickness of ply 45 is moved to 0 ply. The accuracy of reliability methods is verified by the results of a MCS with 10 million samples. Besides, it must be mentioned that in this model, non-surrogate RBDO with stochastic expansion methods are still competitive in terms of computational cost.The active constraint in deterministic optimization is again the Tsai–Wu criterion, but when all the random variables are had into account, buckling constraint becomes active and Tsai–Wu is still very close to be active as well.The RMSE found for the three MARS response surfaces are6.94·10−4,6.32·10−2and2.32·10−2,respectively. This values have been obtained with cross-validation considering 10 folds.The convergence of the objective function contrasting both stochastic expansion methods is plot in Fig. 6, while the convergence of the design variables in RBDO with PCE and SC is shown in Figs. 7and 8. Due to the combination of the compression and shear loads, the45−∘ply thickness reaches high values, whereas the trend of other thicknesses is to maintain the same value except the0−∘ply, which reaches the lower bound. In terms of accuracy, it can be seen in Tables 2, 3, 4 that the obtained results in non-surrogate and surrogate models are very close, so the computational savings justify the use of the methodology.However, the special characteristics of composite materials require the use of discrete variables and global optimization methods. With this aim, the random design variables are now considered as discrete in the outer loop. These variables are the number of plies of each orientation, with a thickness of 0.1397 mm. The settings of the genetic algorithm are the same as the employed in model 1, although the population size and the elitism are increased to 500 and 50, respectively, due to the increment in the number of variables.Table 5shows the results of the RBDO with discrete variables and a comparison with its continuous and deterministic counterparts. Non-surrogate computational time increases proportionally to the increment in continuous optimization, while surrogate approaches remain in the same order of magnitude. As happens with continuous variables, the variations on the design produced by the inclusion of uncertainty can be summarized with the variation in the variablet−45.The evolution of the stacking sequence for model 2 as a consequence of varying the optimization approach is shown in Fig. 9. These designs are taken from the results shown in Table 5.Regarding to the accuracy of the metamodels, in terms of number of random variables, it must be pointed out that increments in the number of involved variables makes more difficult the calibration of the model to obtain acceptable approximations. Thus, when increasing this number, the number of samples generated for the surrogate model construction should rise in accordance to it. Otherwise, the surrogate surface will probably show lack of accuracy in parts or the whole domain. Consequently, this is one of the main issue when using metamodels. For this reason, the number of samples employed for the RBDO applied to model 1 and model 2 has been increased from 40 to 1000, given that the number of design variables has been risen from 1 to 4 and the number of random variables from 1 to 9. This allows to keep the RMSE for the three response surfaces in the same order of magnitude, which in the MARS case have been modifed from3.60·10−2to6.94·10−4,2.18·10−2to6.32·10−2and7.43·10−3to2.32·10−2,respectively. These values can be considered as acceptable and consequently the increase in the number of samples is adequate. In the same line, the number of bases employed in the MARS has been risen from 15 to 50, while in the case of SC and PCE methods the variation was from 40 to 100.

@&#CONCLUSIONS@&#
Some methodologies for reliability-based design optimization using surrogate methods and parallel computing applied to the design of composite panels have been presented in this work. Some considerations about the influence of the methods in the computational cost and the accuracy of the results have been done, as well as about the structural behavior in different reliability and design situations. The proposed methodologies allow the use of computationally expensive algorithms in reliability-based design optimization applied to complex models. Composites are high performance materials that may use uncertainty quantification procedures, as well as local and global optimization tools, to achieve a reliable design with minimum weight.A stiffened composite panel of an aircraft fuselage has been considered in this study under two different configurations of random and design variables, which includes the thickness of each layer of the stacking sequence as random design variable and the random behavior of some material properties as well as load definitions. The characterization of the model properties can be as complex as the designer requires, leading to problems with hight number of both random and design variables, fact that encourages the use of the methodologies employed in this work. For instance, it is common in the design of composite panels to modify the plies orientation of the stacking sequence, as well as consider these play angles as random variables due to possible misalignment of the plies, which leads to more random design variables. Besides, this kind of variables involves the increase of local minima in the domain of design, which supports the use of global optimization algorithms. In the same way, the number of random variables because of material characterizations are commonly very high due to the uncertainties associated with these materials, radically increasing the computational cost.The use of surrogate methods instead of simulations of the real model improves drastically the efficiency of optimization procedures with uncertainty, maintaining the computational cost under acceptable levels. The computational cost of a reliability-based design optimization using surrogates can be split into the simulations of the real model, needed to build the surrogate model, and the subsequent analytic evaluations of that model. As the simulations in the building phase are completely independent from each other, they can be run in parallel and, if enough computational power is available in terms of computing cores, then the cost of building the surrogate is the same as the cost of one simulation. On the other hand, the cost of the analytic evaluations of the surrogate model required by the reliability and optimization algorithms is usually much lower than the simulations. Therefore, the computational requirements of the optimization and reliability analysis methods are irrelevant, because most of the evaluations are analytic.A consequence of this affirmation is that sampling methods, that historically were limited to simple problems, can also be applied to complex structures. With respect to the optimization phase, metaheuristic algorithms, such as genetic algorithms, are very convenient to optimize laminate composite materials, since they avoid local minima and converge to the global minimum better than gradient-based local methods. In addition to that, composite structures commonly demand the use of discrete design variables, so metaheuristic algorithms are usually preferred over gradient-based ones. Their main drawback is the larger number of model evaluations required to obtain the optimum. Surrogate methods are able to provide an approximate model with enough accuracy, so that the evaluations can be computed in a fraction of the computational cost of the original model. In the same line, bi-level approaches for reliability-based design optimization turn out to be as competitive as other approaches in terms of computational cost.As a final conclusion, the decrease of the accuracy when using surrogate methods is usually related with a bad selection of the approximation method, a reduced data set, or a distant initial optimization point in the case of the random design variables. All of these situations can be avoided with a minimum knowledge of the problem.