@&#MAIN-TITLE@&#
Unified multi-lateral filter for real-time depth map enhancement

@&#HIGHLIGHTS@&#
New multi-lateral filter to efficiently increase the spatial resolution of low-resolution and noisy depth maps in real-time.ToF camera coupled with a 2-D camera of higher resolution to which the low-resolution depth map will upsampled.We account for the inaccuracy of depth edges position due to the low-resolution ToF depth maps.Unwanted artefacts such as texture copying and edge blurring are almost entirely eliminated.The proposed filter is convolution-based and achives a real-time performance by data quantization and downsampling.The proposed filter has been effectively and efficiently implemented for dynamic scenes in real-time applications.The proposed filter can be easily adapted for alternative depth sensing systems than ToF cameras.

@&#KEYPHRASES@&#
Depth enhancement,Data fusion,Sensor fusion,Multi-modal sensors,Adaptive filters,Active sensing,Time-of-Flight,

@&#ABSTRACT@&#
This paper proposes a unified multi-lateral filter to efficiently increase the spatial resolution of low-resolution and noisy depth maps in real-time. Time-of-Flight (ToF) cameras have become a very promising alternative to stereo-based range sensing systems as they provide depth measurements at a high frame rate. However, there are actually two main drawbacks that restrict their use in a wide range of applications; namely, their fairly low spatial resolution as well as the amount of noise within the depth estimation. In order to address these drawbacks, we propose a new approach based on sensor fusion. That is, we couple a ToF camera of low-resolution with a 2-D camera of higher resolution to which the low-resolution depth map will be efficiently upsampled. In this paper, we first review the existing depth map enhancement approaches based on sensor fusion and discuss their limitations. We then propose a unified multi-lateral filter that accounts for the inaccuracy of depth edges position due to the low-resolution ToF depth maps. By doing so, unwanted artefacts such as texture copying and edge blurring are almost entirely eliminated. Moreover, the proposed filter is configurable to behave as most of the alternative depth enhancement approaches. Using a convolution-based formulation and data quantization and downsampling, the described filter has been effectively and efficiently implemented for dynamic scenes in real-time applications. The experimental results show a sensitive qualitative as well as quantitative improvement on raw depth maps, outperforming state-of-the-art multi-lateral filters.

@&#INTRODUCTION@&#
Time-of-Flight cameras have become an alternative 3-D sensing system to stereo-based range sensing systems such as stereo vision systems, laser scanners or structured light. They present several advantages such as simultaneously providing intensity and depth information for every pixel at a high frame rate. Moreover, the recent advances in industrializing and producing economic, compact, robust to illumination changes and low-weight ToF cameras is starting to have an impact on commercial applications [1–3]. This is the case of the Xbox One's Kinect version 2, which uses a ToF based depth camera. It has shown a significant improvement over the structured light based Kinect version that was shipped with the Xbox 360 [3]. However, as a compromise for their higher robustness to ambient conditions; i.e., larger working temperature range and higher reliability under sun light; and in contrast to depth sensing systems intended for gaming applications or research purposes, the resolution of ToF cameras that are used in automotive or industrial automation is still far below the resolution of common 2-D cameras. Indeed, a very few ToF cameras slightly exceed the Quarter-Quarter Video Graphics Array (QQVGA) resolution, i.e., (160×120) pixels. Note that consumer depth cameras such as Microsoft Kinect or Asus Xtion Pro Live are not compliant to the standards in automotive or industrial automation. Hence, their rather low spatial resolution and the high noise level within their depth measurements are currently restricting their use.A suitable solution to handle the limited resolution of ToF cameras is sensor fusion [4–11,47]. That is, depth data is complemented with data from a coupled sensor, usually 2-D cameras [12]. Previous works in ToF and 2-D data fusion have proven to deliver dense depth maps at near real-time frame rates, outperforming, in some cases, alternative 3-D sensing systems [4,6]. In this paper, we first introduce existing low-resolution depth map enhancement methods and discuss their limitations. Then, we propose a multi-lateral filter that efficiently tackles inaccurate depth measurements from low-resolution depth maps, producing an enhanced depth map of the same resolution as the considered 2-D guidance image. To this end, we rely on a confidence measure, to be incorporated in the filter formulation, that describes the reliability of each depth measurement. In addition, depth pixels with low confidence will be further treated to overcome unwanted artefacts resulting from data fusion. We also discuss the practical issues such as the configuration of the filter's parameters in order to make it behave as most of the existing multi-lateral filters in the literature. That is, we propose a unified formulation for fusion based depth enhancement filters. Finally, we propose a mathematical formulation that enables the filter to be used in real-time applications. We remark that the proposed filter was already introduced in [13] where we provided advice on how to speed up its performance as well as some preliminary results. Herein, we provide a detailed description of its components as well as a comprehensive qualitative and quantitative evaluation and comparison against alternative depth enhancement filters. In addition, we discuss its possible configurations to be adapted for different sensing system technologies and/or applications.The remainder of the paper is organized as follows: Section 2 covers the background and a literature review on depth map enhancement by means of low-level data fusion. In Section 3 we present our multi-lateral filter as well as the confidence measure that is used to combine depth and 2-D information. Section 4 proposes a mathematical formulation that enables for a real-time implementation. In Section 5, we compare and quantify the proposed filter with alternative depth enhancement methods using synthetic scenes from the Middlebury dataset as well as our own recorded sequences. Finally, concluding remarks are provided in Section 6.The idea of coupling a ToF camera and a 2-D video camera in a hybrid multi-camera rig is a convenient strategy to overcome the limitations of ToF cameras. Indeed, sensor fusion exploits the advantages of each of the cameras in the camera rig avoiding their individual drawbacks. In this paper we target a low-level data fusion in contrast to higher fusion levels that concern post-processed data [14]. To this end, we need first to ensure a good data matching between the cameras that constitute the camera rig. Existing 3-D warping techniques, i.e., forward warping techniques [15,16,11], assign to each depth pixel its corresponding RGB pixel. However, low-level data fusion resulting in a high-resolution depth map requires the opposite mapping, that is, to assign to each high-resolution 2-D pixel its corresponding depth pixel. Note that this so-called backward warping is far from a trivial task due to the need of determining the distance-dependent disparity between each 2-D and depth pixel correspondences. Herein, we use the real-time backward warping algorithm proposed in [17], an iterative approach that addresses backward warping mapping with an accuracy of half the high-resolution 2-D pixel.Among the early results on low-resolution depth map enhancement, we emphasize those that use Markov Random Fields (MRFs) to fuse different sources of data. Diebel et al. [5] proposed a direct application of MRFs to range sensing with very promising results. Their approach was later extended by Gloud et al. [7] in the field of robotics, enabling a robot to detect and recognize objects from enhanced depth data. However, the authors did not tackle depth discontinuities along depth edges in a dedicated. This leads to strange artefacts within depth edges. Besides, the evaluation of depth enhancement methods based upon an MRF is in general computationally intensive and thus not suitable if real-time is required. Park et al. [18] proposed a dedicated edge weighting scheme to be used in combination with a non-local means regularization in a least-square optimization approach. User interaction and depth edge mark-up might be required to obtain optimal results as well as image segmentation to define the colour similarity for their edge confidence weighting. Yang et al. [9] proposed to improve the quality of a given low-resolution depth map through an iterative refinement module based upon a cost volume. But again, the authors neither tackle depth discontinuities and real-time remained an open question. An alternative solution for depth enhancement is to consider bilateral filtering [19,20] while fusing ToF and 2-D data. Although bilateral filtering is commonly used in image processing and/or computer vision applications for 2-D noise removal and edge-preserving, it has become the basis of recent depth enhancement approaches, outperforming, in some cases, those approaches based upon an MRF or iterative techniques. A more noticeable aspect is the possibility of performing in real-time thanks to its latest implementation strategies [21–23]. Indeed, the real-time aspect was one of the strongest reasons that motivated us to opt for bilateral filtering when fusing the data. In the following, we introduce the bilateral filter. Later, we review the literature on depth enhancement approaches based on bilateral filtering.The bilateral filter was first introduced by Tomasi et al. as an alternative to iterative approaches for image noise removal [20] such as anisotropic diffusion, weighted least squares, and robust estimation [19]. This non-iterative filter formulation is a simple weighted average of the local neighbourhood samples, i.e.,(1)JBFp=∑q∈NpwBFpq⋅Iq∑q∈NpwBFpq,where N(p) is the neighbourhood at the pixel indexed by the position vector p=(i,j)T, with i and j indicating the row, respectively column corresponding to the pixel position. The weights are computed based on spatial and radiometric distances between the centre of the considered sample and the neighbouring samples. That is, the kernel is decomposed into a spatial weighting term fS(⋅) that applies to the pixel position p, and a range weighting term fI(⋅) that applies to the pixel value I(p) as follows(2)wBFpq=fSpq⋅fIIp,Iq.The weighting terms fS(⋅) and fI(⋅) are generally chosen to be Gaussian functions with standard deviations σS and σI, respectively. The parameter σS defines the spatial dimension of the kernel, i.e., the size of the pixel neighbourhood N(p), whereas σI defines the resolution in I for edge preserving. That is, pixels p and q influence each other as long as their values differ within the range ±σI. When both standard deviations are well chosen, the resulting filtered image JBF is a smoothed version of I, with preserved edges and significantly reduced noise level. Thus, by replacing I by a low-resolution depth map D, the resulting filtered depth map JBF will present much less noise even though the low-resolution problem will not be tackled.In order to tackle the low-resolution problem, most of the recent depth enhancement techniques by fusion [4,24,6,13] turn to the Joint Bilateral Upsampling (JBU) filter, introduced by Kopf et al. in [8]. The JBU filter is a modification of the bilateral filter in which the data to be filtered and the data through which the weighting term is computed are not coincident. That is, I in (1) is replaced by the low-resolution depth map D, i.e.,(3)JJBUp=∑q∈NpwBFpq⋅Dq∑q∈NpwBFpq,whereas the weighting term wBF in (2) considers the corresponding high-resolution 2-D image I. Thereby and in the following depth enhancement techniques by fusion, the low-resolution depth map D has to be upsampled to have the same spatial resolution as I. To this end, nearest-neighbour interpolation might be used [25,48] since alternative data interpolation approaches such as bilinear or bicubic interpolation might introduce non-valid depth measurements among the upsampled edges of the depth map [17]. As in (1), the resulting depth map JJBU is an enhanced version of D that has been upsampled to the 2-D guidance image resolution. Nevertheless, the violation of the heuristic assumption in which depth and colour are strongly correlated [2] may lead to erroneous copying of 2-D texture into actually smooth geometries within the depth map. That is, depth measurements within nearby pixels will be mixed together if their corresponding 2-D intensity values are close enough, yielding to a texture copying effect within the enhanced depth map. This effect is illustrated in Fig. 5f, where we show an enhanced depth map resulting from the JBU filter in which texture from the guidance image (see Fig. 5a) has been copied into smooth areas of the enhanced depth map. In addition, depth edges that have no corresponding edges in the 2-D image, i.e., in situations where objects on either side of a depth discontinuity have a similar colour, will be blurred. The same occurs when depth edges and their corresponding 2-D edges are not perfectly aligned. An effect which often occurs when filtering real data as the low-resolution of the acquired depth map restricts the accuracy of the data mapping process, as illustrated in Fig. 1c. We note that processing time restrictions and memory constraints usually force the use of a grayscale guidance image instead of the originally recorded colour image, making this latest effect more noticeable. Hereafter, we present different strategies from the literature that extend and/or adapt the JBU filter to overcome common artefacts in low-level data fusion.Kim et al. [26] proposed a modification of the weighting term of the JBU filter by adding an additional range weighting term fD(⋅), i.e.,(4)wNJBUpq=fSpq⋅fIIp,Iq⋅fDDp,Dq.By doing so, the authors slightly reduce the JBU's texture copying artefact as well as the global amount of noise. However, this straightforward combination of the weighting terms does not cope with unmatched edges between the given depth map and its corresponding guidance image. The authors developed a post-processing framework for boundary refinement using a colour segment set. The refined depth edges result from the colour segment set after removing outside pixels from the object boundary, or extending inside pixels to the boundary using linear interpolation. As discussed in the original JBU filter version (see Section 2.2), the low-resolution depth map to be enhanced must be upsampled in order to have the same spatial resolution as its corresponding guidance image.Chan et al. proposed in [4] an improved version of the JBU filter that intends to preserve the benefits of using the JBU filter while preventing artefacts in those areas where JBU is likely to cause erroneous texture copying. This filter is referred to as Noise-Aware Filter for Depth Upsampling (NAFDU). In contrast to directly combining 2-D and depth information within the filter kernel as Kim et al. did in [26], the NAFDU filter splits each data source contribution within the weighting term as follows(5)wNAFDUpq=fSpq⋅αΔNp⋅fIIp,Iq+1−αΔNp⋅fDDp,Dq,where α(⋅) is the blending function that decides how much each of the data sources I and D contributes to the kernel. A high weight α makes the filter behaves like the original JBU filter whereas a low weight α makes it behaves like the standard bilateral filter, i.e., both spatial and range weighting terms apply to the same data source D, without considering the 2-D image I information. Intuitively, NAFDU tries to preserve the benefits of JBU except in the areas that are geometrically smoothed but heavily contaminated by noise within the distance measurements. The blending function is defined asαΔNp=1/1+e−εΔNp−τ, with ΔN(p) the difference between the maximum and minimum measured depth values in the pixel neighbourhood N(p). We note that the authors apply a low-pass filter on D before computing ΔN(p). By doing so, they minimize the influence of noise within depth measurements. Parameters ε and τ control at what min-max difference the blending interval shall be centred. The downside of this approach is related to the complexity of determining those values, which in addition, have to be manually tuned. Besides, the NAFDU expression corresponds to a weighted average of two non normalized kernels, making the contribution of each of the kernels inconsistent and inaccurate.More recent work on low-resolution depth enhancement can be summarized as an adaptation of the above presented filters to some specific cases or scenarios. For example, we proposed in [27] a new colour space to which one can encode the guidance image to preserve the perceptual properties of the hue-chroma-luminance (HCL) colour representation and thus, leading to a solution that is more accurate than when using JBU with grayscale guidance images. In [28], Choi et al. improved the performance of the NAFDU filter by reducing its complexity. To do so, the authors proposed a new blending function based on a depth threshold value. Although we herein focus on depth enhancement methods by fusing depth and 2-D data acquired at the same time stamp, there are alternative spatio-temporal based approaches that consider multiple video frames within their fusion process [29,30]. That way, the authors minimize temporal depth flickering artefacts on stationary objects. Min et al. [31,32] proposed a weighted mode filtering based on a joint histogram. The final solution results from seeking a global mode on the histogram constructed from the similarity measure between a reference pixel and its neighbourhood. A measuring colour distance approach is proposed to prevent edge blurring and, similarly to [29,30], temporal consistency is considered to generate flicker-free depth video. Another possible solution would be to fuse ToF and stereo data acquired by on-demand passive stereo systems [33–35]. The use of stereo data allows to get rid of texture copying and to improve the accuracy of depth measurements. However, the aforementioned depth enhancement approaches are highly sensitive to the registration of depth and 2-D data. That is, edge blurring will appear if depth and 2-D data are not pixel-wise matching.Adjusting the right distance measurement along object boundaries without mismatching with texture is quite challenging. Indeed, depth pixels from low-resolution depth maps may cover foreground and background objects at the same time and thus, the depth estimation in these cases cannot be accurate. In addition, the depth estimation principle used by ToF cameras requires a certain integration time to estimate depth, which might increase this uncertainty in dynamic scenes where objects and/or people constantly change their location. To fix the right distance measurement within object boundaries, we proposed in [6] an improvement of the JBU filter, to which we referred as Pixel Weighted Average Strategy (PWAS) filter. In contrast to alternative depth enhancement methods proposed in the literature, the PWAS filter adds an additional weighting term Q(⋅) to the expression in (2). We referred to this term as credibility map as it assigns a reliability weight to each depth map value as a function of the scene's geometry. By doing so, depth measurements that are considered to be unreliable are replaced by reliable values in their neighbourhood while being adjusted to the 2-D guidance image. The weighting term for the PWAS filter to be used in (3) is defined as(6)wPWAS−Ipq=fSpq⋅fIIp,Iq⋅Qq.Similarly to the filters presented in Section 2, the weighting functions fS(⋅) and fI(⋅) are taken to be Gaussian functions with standard deviations σS and σI, respectively.As discussed before, the estimated depth along object boundaries may be inaccurate. In addition and, as depicted in Fig. 1c, depth edges are not always perfectly aligned with respect to the 2-D guidance edge from which results the edge blurring artefact discussed in Section 2. Hence, the introduction of this new factor Q(⋅) within the weighting term allows to explicitly account for the unreliability of depth measurements along depth edges. In [6], the credibility map was defined as Q=fQ(|∇D|), with fQ(⋅) being a Gaussian function with standard deviation σQ. ∇ stands for the gradient operation on D. From our experiments, the Sobel operator was a valid solution to compute image derivatives and thus, the gradient of D. A separable kernel of (3×3) was used to calculate the derivative. Note that no manual tuning or parameter setting is required for the computation of Q(⋅). In a nutshell, low-weighted credibility map pixels define those pixels in the depth map that contain unreliable depth measurements, whereas high-weighted credibility map pixels define reliable depth pixels. From the credibility map, low-weighted pixels are not considered during the filtering process. Note that in general, pixels along depth edges are low-weighted in the credibility map, and thus, depth edges are adjusted according to the 2-D guidance image. We show, in Fig. 2, the credibility map computed on the raw depth map shown in Fig. 1b. From the figure, we can observe how low weight values are assigned to depth pixels representing object boundaries. By doing so, their depth values will be replaced by valid depth measurements in their neighbourhood and the depth edge will be adjusted according to the 2-D guidance image.Although the PWAS filter correctly addresses depth measurements along depth edges with a significant reduction of the texture copying effect, the fact that the range weighting term fI(⋅) only considers the 2-D information may cause texture copying in regions that actually are geometrically smooth and where depth values are used to be reliable. We therefore proposed in [13] the Unified Multi-Lateral (UML) filter in order to increase the accuracy of depth measurements within smooth regions. To this end, we defined two separate normalized kernels with each one considering a different data source, 2-D and depth information, as illustrated in the flow diagram of Fig. 3. The decision on which kernel the filter has to consider is directly given by the reliability weight β(⋅) assigned to the pixel to be filtered. The UML filter takes the form of(7)JUMLp=1−βp⋅JPWAS−Ip+βp⋅JPWAS−Dp.The PWAS filter from which results JPWAS−I(⋅) uses the weighting term in (6). That is, both 2-D and depth information are considered within the weighting term. Instead, the PWAS filter from which results JPWAS−D(⋅) uses the following weighting term(8)wPWAS−Dpq=fSpq⋅fDDp,Dq⋅Qq,in which both spatial and range weighting terms are applied on the depth information D. Note that the credibility map clearly reflects those pixels within the depth map presenting accurate depth measurements. Hence, we can use the credibility map to identify those regions in which if we consider 2-D information while filtering, we might introduce texture copying. Therefore, we herein set the reliability weight equal to the credibility map, i.e., β=Q.We chose the weighting functions fS(⋅), fI(⋅), fD(⋅), and fQ(⋅) to be Gaussian functions with standard deviations σS, σI, σD, and σQ, respectively. One of the main reasons to choose Gaussian functions is because of their constant time computation [36]. We notice that these standard deviations are data-dependent and thus, they cannot be fixed to a unique value. Nevertheless, we next define how to automatically set the standard deviations for each weighting function. The parameter σS must be at least as large as the depth edge width which is, in fact, the width of low-weighted object boundaries within the credibility map. In general, this value coincides with the upsampling scale factor between the low-resolution depth map D and the high-resolution 2-D guidance image I (see Section 2.2). The parameters σI and σD define the resolution in I, and respectively D, for depth edge preserving. That is, pixels p and q influence each other as long as their values differ within the range ±σI and ±σD, respectively. We recommend to set them equal to the mean of the 2-D image gradient∇I¯and depth map gradient∇D¯, respectively. Finally, the value of σQ is directly related to the noise level within the depth data.Depending on the application and/or on the sensing system technology to be used, one can configure the UML filter in order to adapt its behaviour to specific cases or scenarios. To do so, the reliability weight β(⋅) has to be considered as a data source flag. That is, by setting β(⋅)=1, the guidance information will be the depth map D whereas otherwise β(⋅)=0, it will be the 2-D image I. Another parameter to be taken into account is the standard deviation σQ for the credibility map. If it tends to infinity σQ→∞, the credibility map becomes constant and equal to one for all pixel values, that is, we neglect its contribution. Hence, the multiple configurations of the UML filter allow it to behave like a:•Bilateral filter, by setting the data source flag β(⋅)=1 and σQ→∞ to neglect the credibility map contribution.JBU filter, by setting the data source flag β(⋅)=0 and σQ→∞.PWAS filter, by setting the data source flag β(⋅)=0. The remaining part in (7) coincides with the PWAS filter.In contrast to the NAFDU filter, the proposed multi-lateral filter is a weighted average of two normalized kernels. Thus, each kernel in (7) provides a consistent contribution. Making our filter behaves like the NAFDU filter implies a complex β(⋅) function that contains the pixel-dependent normalization factor of the two kernel contributions and thus, it does not represent a limit case. The same occurs with the JBU extension presented in Section 2.3.Although bilateral filtering is known to be time consuming, its latest implementation strategies based on data quantization and downsampling enable for a high-performance. Indeed, there are a few studies [21–23] that prove that the use of such implementation strategies outperforms state-of-the-art methods for accuracy, speed and memory consumption. Based on these techniques, we next propose a real-time implementation for the UML filter presented in Section 3.2.Similarly to [23], we quantize the range of the 2-D intensity values and depth measurements, i.e., Ik=sI⋅k, and Dl=sD⋅l, with k=0,…,K and l=0,…,L. sI and sD are the 2-D and depth quantization factors; thus (sI×K) and (sD×L) are equal or larger than the maximum 2-D intensity values and depth measurements, respectively. Then, inserting in (6) and in (8) the quantized levels Ikand Dlfor I(p), respectively D(p), one obtains a different weighting term for each level, i.e.,(9)wPWAS−IpIk=fSpq⋅fIIk,Iq⋅Qq,and(10)wPWAS−DpDl=fSpq⋅fDDl,Dq⋅Qq.We define four mappings, i.e.,EIk⋅andFIk⋅, for a quantized intensity value at the pixel position p such that(11)EIk:q↦wPWAS−IpIk⋅Dq,(12)FIk:q↦wPWAS−IpIkandGDl⋅andHDl⋅for a quantized depth measurement at the pixel position p, such that(13)GDl:q↦wPWAS−DpDl⋅Dq,(14)HDl:q↦wPWAS−DpDl.We then may rewrite JPWAS−I(⋅) and JPWAS−D(⋅) as follows(15)JPWAS−IpIk=∑q∈NpfSpq⋅EIkq∑q∈NpfSpq⋅FIkq,and(16)JPWAS−DpDl=∑q∈NpfSpq⋅GDlq∑q∈NpfSpq⋅HDlq.We note that fS(p,q) is a function of the difference (p−q). We may hence write (15) and (16) as:(17)JPWAS−IpIk=fS*EIkpfS*FIkp,and(18)JPWAS−DpDl=fS*GDlpfS*HDlp,where * denotes the convolution between functions.The filtered value JPWAS−I(p,I(p)) results from a linear interpolation of the filtered depth images JPWAS−I(p,⋅) obtained for the different levels at position p and intensity value I(p) between Ikand Ik+1, i.e.,(19)JPWAS−Ip,Ip=interpolateJPWAS−Ip⋅,Ip=1sIIk+1−IpJPWAS−IpIk+1+Ip−IkJPWAS−IpIk.The same applies to JPWAS−D(p,D(p)); thus from a linear interpolation between Dland Dl+1:(20)JPWAS−Dp,Dp=interpolateJPWAS−Dp⋅,Dp=1sDDl+1−DpJPWAS−DpDl+1+Dp−DlJPWAS−DpDl.Finally, the enhanced depth map JUML results from (7), considering JPWAS−I in (19), and JPWAS−D in (20).As discussed by Paris et al. in [21] and also shown in Section 5.4, the enhanced depth map does not present significant errors when filtering using a downsampled version of the input data. This in turn reduces both running time and memory consumption. We therefore propose, in addition to quantizing the data range, to downsample the input data to be filtered, i.e., I↓=downsample(I,λ), D↓=downsample(D,λ), and Q↓=downsample(Q,λ), with ↓ standing for downsampled version, and λ being the downsampling scale factor.Two low-resolution filtered depth maps JPWAS−I↓ and JPWAS−D↓ will result from Eq. (19), respectively (20), by replacing I and D by their respective downsampled versions I↓ and D↓. In order to recover JPWAS−I(p,I(p)) and JPWAS−D(p,D(p)), we spatially interpolate the low-resolution depth maps using a bi-linear (i.e., four point) interpolation, i.e.,(21)JPWAS−Ip,Ip=interpolateJPWAS−I↓⋅,Ip,p/λ,and(22)JPWAS−Dp,Dp=interpolateJPWAS−D↓⋅,Dp,p/λ.We note that to perform this bi-linear, the low resolution depth maps JPWAS−I↓ and JPWAS−D↓ would have to be computed for each value I(p) and D(p) of their respective higher resolution versions. At this point, we combine both the linear range interpolation and the bi-linear spatial interpolation to a tri-linear (i.e., eight point) interpolation as follows(23)JPWAS−Ip,Ip=interpolateJPWAS−I↓⋅⋅,p/λ,Ip,and(24)JPWAS−Dp,Dp=interpolateJPWAS−D↓⋅⋅,p/λ,Dp.Thereby, JPWAS−I↓(⋅,⋅) and JPWAS−D↓(⋅,⋅) is the set of low resolution filtered depth maps calculated at each level Ikand Dl, respectively. The final output of the UML filter results, according to (7), from superposing the two filter outputs in (23) and (24), and using the credibility map Q that defines a pixel-dependent weight for each of the two contributions.In practise, the spatial resolution of the original/raw depth map D acquired by a low-resolution ToF camera is lower than the resolution of its corresponding guidance image I. In such a case, it might not be necessary to upsample D to the same resolution of I, as discussed in Section 2.2, and then downsample it to obtain D↓. However, we have to ensure a good data matching between D↓ and I and thus, either D↓, I, or both data sources might be warped. Hence, a further optimization of the proposed real-time implementation can be obtained when considering that both the high-resolution depth map D, and thus the credibility map Q result from upsampling a low-resolution depth map, respectively its weighted gradient (approximated by the nearest pixel in the low-resolution versions of the maps). Consequently, the weighting of the superposition can be done on low-resolution level before the interpolation, and the tri-linear interpolation in (24) of JPWAS−D can be approximated by a bi-linear spatial interpolation of a single low resolution filtered image JPWAS−D↓=JPWAS−D↓(⋅,D↓(⋅)). This interpolation for JUML takes the following form(25)JUMLp=interpolateQ↓⋅JPWAS−I↓⋅⋅,p/λ,Ip+interpolate1−Q↓⋅JPWAS−D↓⋅,D↓p,p/λ.The main benefit of this implementation is, apart from some run-time optimization, the fact that no high resolution image except the 2-D image I has to be kept in memory.In order to avoid filtering artefacts due to the data quantification and sampling introduced above, the standard deviations σI, σD, and σS shall be chosen greater than sI, sD, and λ, respectively. Otherwise, the approximation may be poor, i.e., numerically unstable. According to the above mappings (see Eqs. (12) and (14)), the noise due to quantization only affects the range mapping functions, i.e.,FIkandHDl, and both the intensity values of the 2-D image I(q) as well as the depth measurements of the depth map D(q) are preserved.Background pixels correspond to those pixels in the imager that have not been able to estimate a distance measurement. These pixels are identified during the generation of the raw depth map and set to a pre-defined value. In the case of ToF cameras, these pixels describe the background of the scene, i.e., a situation in which the light power density of the active illumination is too low. Background pixels must be identified and treated separately during the filtering process in order to not misuse their pre-defined value as a valid measurement. Indeed, if we do not pay special attention to background pixels, non-valid distance measurements might appear within the enhanced depth map. To do so, we propose to compute a relative background weight wbgas follows(26)wbgp=∑q∈Nbgp⋅fSpq⋅Qq⋅Bq∑q∈NpfSpq⋅Qq,with Nbg(p) the neighbourhood of p identified as background pixels. B is a binary mask of same resolution as D in which only those pixels identified as background pixels in D are set to 1. Non-background pixels are set to 0. The final value for the selected pixel p within the enhanced depth map, i.e., JUML(p), will correspond to a pre-defined background value when wbg(p)≥0.5. Otherwise, the filtered value resulting from (7) will be used. We note that in the above presented expressions, only valid depth pixels (non-background pixels) are considered within the neighbourhood of a pixel p, i.e., N(p).

@&#CONCLUSIONS@&#
