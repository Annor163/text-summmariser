@&#MAIN-TITLE@&#
Cloud computing-based map-matching for transportation data center

@&#HIGHLIGHTS@&#
Propose a leapfrog method to improve the efficiency of map-matching algorithm.Use MapReduce to adapt the serial map-matching algorithm for cloud computing environment.Propose a privacy-aware map-matching model over hybrid clouds.

@&#KEYPHRASES@&#
Map-matching,MapReduce,Hadoop,Vehicle tracking data,GPS privacy protection,

@&#ABSTRACT@&#
Transportation data center has recently become a common practice of modern integrated transportation management in major cities of China. Being the convergence center of large-scale multi-source vehicle tracking data, it caused great challenge on GPS map-matching efficiency and privacy protection. In this paper, we propose a secure parallel map-matching system based on Cloud Computing technology to meet the demand of transportation data center. The main contributions are as follows: (1) we propose a leapfrog method to improve the efficiency of traditional serial map-matching algorithm on the increasingly common high sampling rate GPS data; (2) we adapt the serial leapfrog map-matching algorithm for cloud computing environment by reforming it in the MapReduce paradigm; (3) we propose a privacy-aware map-matching model over hybrid clouds to realize the sensitive GPS data protection. We implemented the proposed map-matching system in the hadoop platform and tested its performance with a large-scale vehicle tracking dataset, which exceeds 100 billion records. The experimental results show that our approach is highly efficient and effective on massive vehicle tracking data processing.The new economy of E-commerce heavily relies on just-in-time performance and integrated transportation logistics systems. To satisfy the challenges and demands that are placed on transportation systems and trucking services, the emerging and evolving technology known as Intelligent Transportation System (ITS) has become the key solution to sustain the new economy’s growth and competitiveness.Vehicle location awareness technology, particularly the wide used Global Positioning System (GPS), is notably essential to ITS. Many important ITS technologies, such as vehicle monitoring, route suggestion, and traffic flow prediction, etc., rely on the analysis of vehicle tracking data that are collected using GPS-instrumented probe vehicles.Unfortunately, because of the measurement error caused by the limited GPS accuracy, and the sampling error caused by the sampling rate (Brakatsoulas et al. 2005), the reported locations in GPS points are not precise for the post hoc analysis. Thus, Map Matching (MM), which can integrate positioning data with the road network to identify the correct path on which a vehicle is traveling, is an indispensable step of GPS applications.Large amounts of research works on MM algorithm have been conducted over the past decades (Quddus et al. 2007). In earlier studies, the proposed MM algorithms were usually designed for the sole use of one application (such as the on-board navigation system). In these one-to-one scenarios, vehicle tracking data are collected from one or few GPS devices; thus, the data scale is relatively small and can be processed promptly even with hand held devices. Therefore, the researchers generally focused most of their efforts on improving the matching accuracy. The matching speed, which is another important measure of the algorithm performance, has not been pushed to the extreme.However, the once sufficient matching speed has been facing severe challenge in recent years. Consider the scenario of map-matching process in transportation data center (Zhou 2012). As an important infrastructure of the emerging Smart City development strategy, a transportation data center is build by the municipal government to collect and pre-process huge volumes of multi-source traffic data all together, and share them with various modern integrated transportation management systems. In this one-many scenario, the MM program that runs in the data center must process a huge amount of vehicle tracking data that are required by various upper-layer ITS applications in notably tight time schedule; thus, the matching speed becomes a vital measure for the MM algorithm.For example, Beijing Transportation Data Center collects vehicle tracking data from public transit vehicles and commercial fleet of more than 60,000 taxis, 30,000 buses and many other probe vehicles. The tracking data upload rate reaches 50,000 GPS records per second, and the accumulated historical raw GPS data set that waits for MM processing has already exceeded 100 billion records. Mean while, the reported processing speeds of the current MM algorithms are at most 10,000 GPS records per second (when the average GPS sampling period is 15s), according to the result of ACM SIGSPATIAL GIS Cup 2012 (Ali et al. 2012), a well-known competition on MM algorithms. The processing efficiency is apparently beyond the demand of the transportation data center.The centralized GPS data processing and sharing mode of transportation data center also brings up another serious issue – vehicle tracking data privacy protection. Despite all the merits (such as cost-effective computing and storage power, interoperability between systems, etc.), some organizations are still reluctant to handle their sensitive vehicle tracking data to transportation data center for map-matching process, due to the concern that data exposure may leak their operational information or customers’ privacy, and subsequently made them suffer from financial loss or legal judgment. For example, the users’ moving trajectories recorded by navigation/LBS service providers can easily be used by malicious attackers to infer user’s detailed activities, or to track and predict the user’s daily movements (Krumm 2009). Accordingly, measures must be taken to protect the sensitive GPS data during map-matching process in transportation data center.To address the unprecedented challenges of large-scale vehicle tracking data processing in urban transportation data centers, this paper proposes a secure parallel map-matching system that aims to achieve high efficiency and user privacy protection. The main ideas of this work include the following:1.Topological MM algorithm improvement with the leapfrog methodThe traditional MM algorithm tends to exploit all available GPS points in the tracking dataset to increase the matching accuracy. That excessive effort causes extra needless computation in certain circumstances.We observed that in the case of a high GPS sampling rate or a long road segment, a large part of the intermediate GPS points between two consecutive road intersections do not contribute to the vehicle travel path determination. Based on this observation, we implanted a new leapfrog method into the popular topological MM algorithm to find these superfluous points and save considerable computing time by fully skipping them in the candidate path determination.Parallelization of the MM programMost existing MM algorithms are designed to run serially. Their capabilities are constrained by the computation capacity of a single computer, which is far below the MM requirement in a transportation data center. Hence, it is natural to split the huge GPS dataset to a number of compute nodes to gain a multiplied computing power.Instead of distributing serial MM programs to standalone computers, we turn to cloud computing techniques to gain more power from connected computers (Li et al. 2011). In other words, we use the MapReduce paradigm to parallelize our improved topological MM algorithm and implement it in the hadoop platform. The well-designed parallelization solved former problems in serial MM, such as large search space, slow data sorting, and long map-loading time. The inherent merits of the hadoop platform, such as load balance and fault tolerance, also make our system more adaptive to big data processing in the transportation data center environment.Privacy-Aware MM model on hybrid cloudCurrent researches on GPS data privacy protection have mainly focused on introducing uncertainty or error into location data, e.g., location anonymity and obfuscation (Seidl 2014). These approaches fall short because they all degrade the quality of positioning data, thus caused trouble in subsequent use of ITS applications.In this paper we propose a novel solution aimed at preserving the location privacy without sacrificing the accuracy of MM system. We first build a hybrid cloud in transportation data center. For the sensitive GPS data map-matching tasks, most stages of MM are accomplished in public cloud, while the final stage of merging intermediate results into discernible travel paths is carried out within the organization’s private cloud. A control program is implemented to automatically arrange data isolation and placement among private cloud and public cloud according to the data privacy characteristic.The remainder of this paper is organized as follows. In Section 2, we present some preliminary information of our work. The proposed leapfrog method is discussed in Section 3, the parallel map-matching algorithm is presented in Section 4, and the privacy-aware map-matching model is described in Section 5. Section 6 reports the empirical experiment results and describes the evaluation in terms of efficiency, accuracy and scalability. Finally in Section 7, we conclude the paper.Vehicle tracking data are a stream of spatio-temporal points that represent the trajectory of moving vehicle. For a probe car, a trajectory P consists of n observed positioning points(pi)i=t0tnduring the time interval[t0,tn]. Each pointpt(referred to as a GPS record) is a 4-tuple (CarID, X, Y, t), where X is the longitude, Y is the latitude, and t is a represents timestamp.The values of X and Y in the GPS record is actually imprecise because of the measurement error caused by the limited GPS accuracy. The typical measurement error is in the range of 2–10m. In certain situations (shadowed and reflected signals), the position recorded in P can differ from the actual location with an error of up to hundreds of meters (Brakatsoulas et al. 2005).Because ITS applications usually must know the vehicle’s travel path, the original tracking data P must be aligned with the road network on a given digital map using a pre-processing procedure known as map-matching. TakingP=(pi)i=t0tnas the input, the map matching procedure should choose from the road network a corresponding sequence of consecutively connected road linksL=(li)i=0mthat represent the vehicle’s travel path.In addition to the measurement error, the correctness of the matching result is also deeply affected by the second error of the tracking data, which is known as the measurement error and is directly related to the frequency with which the position samples are taken (sampling rate) from the GPS device.Considering a typical sampling rate of 30s and a travel speed of 50km/h, the moving vehicle may cover a distance of 417m between two consecutive GPS sample points, with several possible routes for the vehicle to travel from the first point to the second one. Wenk et al. (2006) used the following figure (see Fig. 1) to show these two errors and their effect on the travel path determination. The green road links within the active region are identified as candidate paths according to their proximity to the sampling points and the road topological information such as road connectivity/continuity. The sophisticated part of the map-matching process is to determine the actual route that the vehicle travelled with certainty.In past decades, researchers developed many map-matching algorithms. The approaches can be categorized into three groups: geometric map matching, topological map matching and advanced map matching.Geometric MM uses only geometric information to estimate the travel path and suffered with poor accuracy in a complex road network. Topological MM (tMM) uses all available information (such as the road connectivity and contiguity, vehicle speed and heading information, etc.) to find the most probable travel path among the candidate paths set (Liu et al. 2012). The accuracy can be significantly improved using a well-designed weighing scheme on the probabilities of each candidate path. Advanced MM algorithms use more refined approaches such as the probability theory (Nie et al. 2013) and fuzzy logic to address the inaccuracies caused by low sampling rate and complex road network, but their running speed is not comparable to that of tMM.Because the tMM algorithm achieves a balance between matching speed and accuracy and is easy to implement, it has become the most popular approach of the map-matching process. A typical tMM process can be described as follows (Stojanovic and Stojanovic 2011):1.For each GPS samplept, determine a set of candidate locations{xt0,xt1,…}, which are typically the perpendicular projections on road links{yt0,yt1,…}within the active region.Calculate weights for each candidate linkyt.Output the candidate path with the max weight.The weighing scheme in tMM is used to find the most probable travel path in a set of candidate paths, which uses many weighting factors such as the Euclidean distance, road connectivity, driving directions, and turning information (Liu et al. 2012). Because of the weight factors’ computation complexity, the weighing step becomes one of the most time-consuming steps in the map-matching process.The tMM algorithm is sensitive to the GPS sampling rate. In the early stage of the GPS application, the sampling period is usually set to at least 1min, which is sufficiently long for a vehicle to travel across several road intersections. The sparseness of sampling points makes each GPS record precious information for the path determination. Therefore, most tMM algorithms were designed to thoroughly investigate every GPS point. In other words, in traditional tMM, each GPS point invokes a series of weighing factors’ calculation and consumes considerable computing resources.However, with the development of global positioning technologies and the decrease of communication cost, the GPS sampling rate of probe vehicles was increased notably quickly in recent years. In the Beijing Transportation Data Center, we observed that most GPS sampling rates are currently higher than 1 point per 30s. There is also a large number of high-sampling-rate data (1 point per 1–3s), and their portion actually increases at the quickest pace.The high sampling rate provides sufficient (and even excessive) information to determine the path. Consider the Fig. 2.In the scenario of a high sampling rate, the GPS points cover each road link along the travel path. Because the sampling error (distance between two consecutive points) is diminished to less than 14m, it will no longer produce multiple candidate paths (because two consecutive points can cover no more than one road intersection). Therefore, we can assert that if a vehicle passes by two consecutive intersections, it is moving on the road link between these two intersections.Based on this observation, some researchers perceived that the increase in sampling rate provided a chance for a breakthrough on the map-matching efficiency. Li et al. (2013) proposed an efficient passby★algorithm to match only the points near intersections (such as the red points in Fig. 2) and avoid computation at the intermediate points (black points in Fig. 2) because these points are superfluous in the travel path determination, and they achieved high efficiency.However, when we attempt to avoid the intermediate points, a big question arises: how do we determine which points are the intermediate points?Li et al. (2013) proposed to build indices on all GPS points and road intersections. Then, a spatial query can be performed to find all points near intersections (within a radius of 25m), and the remaining points in the dataset are identified as intermediate points.This approach only saves part of the computation because it still requires spatial index building and spatial query operations at the intermediate points. Although these operations are relatively lightweight, they can accumulate to become a heavy burden for huge data volumes.In sector III we propose an improved tMM algorithm, which uses a new leapfrog method to identify the intermediate points with a travel speed estimation. More computation resources can be saved using this approach.The capability of the traditional serial map-matching algorithm is constrained by the computation capacity of a single computer. Hence, it is natural to split the huge GPS dataset to a number of different computers to gain the multiplied computing power.We can simply use a serial map-matching program on some stand-alone computers to accelerate the speed. However, the high performance architectures based on the distributed parallel processing principles can achieve better efficiency, high scalability and easy maintenance over connected computers.With the increasing challenge of computational- and data-intensive problems caused by huge volumes of geospatial data (such as GPS data), the high-performance computing application in GIS has recently become a prominent research area (Li et al. 2011, Ma et al. 2009, Stojanovic and Stojanovic 2011). Many researchers discussed techniques of applying high-performance-computing techniques on GIS computation (Li et al. 2011, Ma et al. 2009). For map matching, Natalija Stojanovic̆ (Stojanovic and Stojanovic 2011) proposed MPI (Message Passing Interface) to implement the distributed application for map-matching computation using a network of workstations (NoW); Li et al. (2011) tested a map-matching algorithm in hadoop and concluded that MapReduce is the best method to satisfy data-intensive computing requirements. These studies validated the effectiveness of the parallel map-matching method. However, the methodologies/design principles of the parallelized map-matching algorithm were not thoroughly discussed.There are many types of high-performance computing paradigms, such as cluster computing and grid/cloud computing. The MapReduce paradigm is notably appealing among these paradigms because of the simplicity of programming, autonomous load balancing and the easy scaling nature (Ma et al. 2009). In Section 4, we adapt our improved tMM algorithm using the MapReduce paradigm to fully utilize the distributed parallel computing power.Privacy concerns are a serious impediment to wider usage of GPS data. Current research on location privacy protection has mainly focus on supporting anonymity and obfuscation (Seidl 2014), wherein approximate location and identities are used instead of the exact values. These approaches prevent accurate identification of the locations of the users, or hides the user among k other users (e.g., k-anonymity), and thus improves privacy. However, many ITS applications need high accuracy trajectory data to support their clockwork precision services, therefore these approaches are not suitable for transportation data center due to the inaccuracy, imprecision, and vagueness they introduced.Inspired by Zhang et al. (2011), we propose hybrid cloud to handle the sensitive GPS data’s map-matching tasks. This hybrid cloud is a combination of government owned public cloud and organizations owned private clouds. Private cloud is trustworthy since the organization owns the total control. The hybrid cloud provides the possibility to distribute computation among the public and private clouds to achieve both absolutely limiting the sensitive data in private cloud and fully outsourcing the non-sensitive data to public cloud. In this way we preserved the location privacy without sacrificing the accuracy and efficiency of MM system. Section 5 discussed the implementation details.In this section, we propose a leapfrog method to improve the efficiency of the tMM algorithm when addressing high-sampling-rate GPS data.The baseline algorithm that we choose is the incremental weighting-based topological map-matching algorithm. The algorithm takes an entire trajectory data as the input, and processes its GPS points sequentially using timestamps. The current and previous GPS points were used to determine the weight value of each candidate path. The impossible paths can be identified based on weight value and are removed from the set of candidate paths. Refer to Liu et al. (2012) for a more detailed description of the weighing scheme.The leapfrog method is called when the set of candidate paths contains only one path. Because the travel path and the next road intersection are determined in this case, the leapfrog method finds and jumps to the point near the next intersection and skip the intermediate points to save computational time.To define the intermediate points, consider the scenario that a vehicle travels along a road linkl=(vfrom,vto), wherevfromandvtorepresent the start and the end vertices of l. The whole GPS trajectory of vehicle can be denoted asP=(pi)i=0n, wherepiis the GPS point. Supposepuis the first point in P that takes the road link l as the only candidate path, andpvis the first point in P that falls in the 25m radius aroundvto. Then the Intermediate Points setPintercan be defined asPinter=(pi)i=uv-1.The flowchart of the improved algorithm is described as Fig. 3.We use the Fig. 4to help understanding the operation of the leapfrog method.Assume that the map-matching process starts from pointp1near intersection A. Because of the measurement error, road linksl1,l2, andl3become the probable future paths and are added into the set of candidate paths.Then, the points in the trajectory are sequentially processed. The set of candidate paths remains unchanged until pointp4is processed. Becausel2andl3are not withinp4’s active region, they are pruned and leavel1as the only path in the set of candidate paths.It is now certain that the vehicle was traveling along pathl1. Therefore, the map-matching process can skip subsequent points ofp4until it reaches pointp5(which is the first point within a radius of 25m of the next road intersection B) where it once again faces path determination.Afterp4is processed, the leapfrog method is called to calculate the number N of intermediate points betweenp4andp5. After N is calculated, the matching process skips N subsequent points ofp4and jumps directly top5to continue processing.The leapfrog method uses the travel speed to estimate the number N of intermediate points. Assume that the vehicle travels at an average speed of V on the road link betweenp4andp5, then:(1)N=DisV·rwhere Dis is the distance betweenp4andp5, and r is the sampling rate (point/s).However, the actual speed V is unknown when the leapfrog method is called. Thus, we use an estimated speedVeto calculate the number of intermediate points. The speed estimation methods are discussed in the next section.The estimated speedVeis rarely equal to the actual speed V. WhenVeis smaller than V, the calculated numberNeis also smaller than the actual number N; thus, the landing point of the jump is in front ofp5. In this case, we call the leapfrog method to jump again.Assume that the sampling rate is 1s/record, and then the calculated number of intermediate pointsNeis:(2)N=DisVe·r=DisVeAfter the first jump, and the number of remaining points to be jumped over is:(3)N1=N-Ne=Ve-VVeVDis=Ve-VVeNAssume that we perform i jumps, then the remaining points are:(4)Ni=Ve-VVeiNThe jump stops whenNj=1. The total jump time j is:(5)j=logVeVe-VNThe total numberTJumpand proportionPJumpof skipped points can be derived as follows:(6)TJump=N-j=N-logVeVe-VN(7)PJump=TJumpN=1-logVeVe-VNNIn another case whenVeis larger than V, the landing pointpiis behindp5. Ifpiremains within a radius of 25m of intersection B, then the jump is successful. We can continue the matching procedure frompi. Otherwise, the jump fails and we decreaseVeby a third and start jumping again from pointp4.The accuracy of the travel speed estimation is the key of the leapfrog method. Eq. (7) shows that a smaller difference betweenVeand V corresponds to more points that the process can skip.Computational complexity is the other major requirement. Some widely used technologies, such as the Kalman filter (Guo et al. 2014), are not suitable because of their high computation costs, which erodes the time that we save from pruning the intermediate points.We propose a simple yet effective method to achieve high-accuracy speed estimation.The main idea is use the average speedVpcalculated using the intermediate points between the jumping start point and the current landing point to adjust the estimation ofVe:(8)Ve=k·Disn/rk∈(1,1.8]where Dis and n are the distance and difference of sequence number between the start point and the jump point, respectively; r is the sampling rate; and k is an adjusting coefficient.To ensure that jump lands before the next intersection, we set the initial k value to 1.3, which makesVeslightly larger than the actual traveling speed.Then, the k value is adaptively adjusted in the following jumping operation according to the land position. The k value is slightly decreased or increased when it jumps over or before the next intersection, respectively. It is described as Algorithm 1:Algorithm 1SpeedEstimation()1:i⇐1;2:k⇐1.3;3:V0⇐VLimit/*the initial speed set to the road speed limit*/4: while Firstcycle or JumpOver(nextIntersection,currentPoint,prePoint) do5: /*jumpover() judge whether the landing point is behind next intersection.*/6:if lengthof(nextIntersection,currentPoint)<50mthen7:Break;/*currenPoint is the current matching point in queue of trajectory P*/8:end if9:Ve⇐k*V0;10:i⇐min(lengthof(nextIntersection,currentPoint)/Ve);11:currentPoint⇐getPoint(P,i);12: /*Read the i-th point after current point from trajectory data P*/13:V0⇐lengthof(prePoint,currentPoint)/i;14:prePoint⇐currentPoint;15:if JumpOver(nextIntersection,currentPoint,prePoint) then16:k⇐1.5;17:currentPoint⇐prePoint;18: continue;19:else20:k⇐k-0.1;21:end if22: end whileWe compared the estimated speed with the actual travel speed of the trajectory data. The result is shown in Fig. 5.We measured the accuracy of the proposed speed estimation method using the MRE (mean relative error):(9)δMRE=1N∑i=1N|Vei-Vi|ViδMREis 0.3 on the given trajectory data shown in Fig. 5.We also applied this speed estimation method into practice and found that the average jump time between two consecutive road intersections is 1.6 times using the data statistics. This value is quite acceptable because its computational complexity is notably low.Actually, there is a more effective way to estimate the travel speed: we can turn to the real-time traffic information system to obtain an accurateVevalue of the designated link (see Fig. 6) .The real-time traffic information system is an important off-the-shelf ITS service. The system performs well-designed algorithms on large-scale traffic sensors’ data (induction coil, microwave radar or floating-car data) to work out reliable urban road traffic information and deliver it to travelers in real-time. Many countries have provided this service in their major cities, such as VICS in Japan (Vics 2013) and ATIS in China (Du et al. 2009).TheδMREof the travel speed prediction of ATIS in Beijing (BTIC 2011) is approximately 0.1, which is lower than the simple method that we proposed. The corresponding jump times are approximately 1.2 times according to the statistics.In the Beijing Transportation Data Center, we stored all historical traffic data from 2008in a database. When we estimate the travel speed in the leapfrog method, we must only query the database for a pre-calculated speed value, which is much more convenient and efficient than the real-time speed estimation methods. Thus, for those who can access the historical traffic data, we recommend this approach for speed estimation.The leapfrog method only works on the intermediate points. For a low sampling rate, a high travel speed and a short road link, the number of intermediate points is too few for the leapfrog method to work properly.We investigated the percentage of intermediate points on the large-scale vehicle tracking dataset of Beijing Transportation Data Center. The dataset is divided into two parts: urban data and suburban data within or outside of the Fifth Ring of Beijing. In urban area of Beijing, the road link is short (240m in average), and the travel speed is slow (30km/h in average). In suburban area, by contrast, the average road link length increases to 800m, and the average travel speed increases to 45km/h. The statistic result is shown in Table 1.The table clearly shows that percentage of intermediate points rapidly decreases with the increase of sampling period.The thresholds of the sampling period are 10s and 15s in urban and suburban areas, respectively. When the sampling period is larger than the threshold, the leapfrog method does not increase but undermines the efficiency of the map-matching process. Therefore, we set the sampling period of 10s and 15s in urban and suburban areas as the thresholds to determine whether we use the leapfrog method or not.However, note that this one-size-fits-all criterion actually cannot fit all situations. For example, when traffic is notably slow or congested (speed below 5km/h), the percentage of intermediate points can reach 50% at the low sampling rate of 1 point per 30s. More refined criteria remain for further discussion in future work.In this section, we use the MapReduce paradigm to reform the proposed serial tMM algorithm, make it suitable for hybrid cloud computing environment, and implement it on hadoop platform to satisfy the map-matching demand of large-scale vehicle tracking data processing in transportation data center.First, we investigated the time cost of each stage of the aforementioned tMM algorithm. We tested it with 16 million GPS records. The entire processing time is approximately 4100s. The costs of each computing stage are shown in Fig. 7.The chart indicates that “adjacent road data reading” is the most time-consuming stage in the serial tMM algorithm.To calculate the weight of the candidate paths, the stage of “adjacent road data reading” reads the necessary road topological information in a radius of 100 meter from the current GPS position. A vehicle trip may cause 100 times of reading on average. In a single computer environment, this is a serious I/O problem.We can use MapReduce to address the problem. The basic idea is to divide the road map into many grids and assign them to different compute nodes. Each node processes and only processes the GPS data in the designated grids. Thus, the node can retain most of the grid information in memory to dramatically reduce the swapping times between the memory and the disk.“GPS records sorting” is another time-consuming stage. The purpose of this stage is to order raw vehicle tracking dataset based on the vehicle trip id and timestamp, so that subsequent procedures can read and process the tracking data based on the unit of vehicle trajectory. The sorting process is a tough task for desktop computers because of the huge scale of historical GPS data. According to our experiment, 200 million records cost more than 1h to process using an external merge sort method.In contrast, MapReduce is notably suitable for the data-intensive processing in nature. As for the big-data sorting problem, MapReduce paradigm can use all compute nodes to fulfill the task once for all, and can simultaneously distribute the sorted data to each node. On this account, we will use MapReduce to reform the sort procedure.MapReduce was originally proposed by Google to index and annotate data on the Internet (Dean and Ghemawat 2008). It is a parallel programming model that facilitates the processing of large distributed datasets. The core of this paradigm is two functions: map and reduce. The map function takes a key-value pair as the input, performs the user specified function, and outputs a list of intermediate key-value pairs that may be different from the input.The runtime system groups all values associated with the same key and automatically forms the input to the reduce function. The Reduce function takes a key-value pair as the input, performs user-defined function, and outputs a list of values. Note that the input values are the list of all values associated with the same key.Apache hadoop is an open-source implementation of the MapReduce paradigm that supports data-intensive distributed applications. It supports the running of applications on large clusters of commodity hardware. We choose hadoop as the infrastructure of our parallel tMM implementation.In our implementation, we established a two-stage MapReduce model. The first MapReduce procedure (referred to as MR I. See Fig. 8) is applied to preprocess the raw historical GPS data. The MR I projects all GPS points on a road map and find road links that surrounds them. Then, the point-to-link distance value is calculated and used as the weight of the candidate path.We took the unordered raw GPS dataset (stored in txt files) as the input of MR I. Each row in the dataset was named as “gpsInfo”, which contains information such as the vehicle ID, latitude and longitude of point, and GPS timestamp. We also divided the road map into small grids (200m×200m) and assigned a unique number (“gridID”) to them. The grid can reduce the complexity of the weight calculation of the candidate paths by constraint the data within its scope. In other words, after reading the GPS data, the Map function extracts the longitude and latitude from gpsInfo and maps the GPS point into a grid. The number of grid (gridID) is designated as the key in a 〈key,value〉 pair, whereas the gpsInfo is the “value”. Thus, the Map function will output a 〈gridID, gpsInfo〉 pair. Hadoop can automatically group all gpsInfo associated with identical gridIDs and form the input of the Reduce function. Note the 〈gridID, gpsInfo〉 pairs with identical keys belong to one Reduce function. The Map function is expressed as follows:Map〈gridID,gpsInfo〉⇒list〈gridID,gpsInfo〉The Reduce function takes the 〈gridID, list〈gpsInfo〉〉 pairs that are merged by the Shuffle procedure as the input. It will load the road link information of current grid into memory and maintain it until a new grid comes. Based on the topological information of the road links, the Reduce function calculates the point-to-link distances between a GPS point and its adjacent road links within the current grid. Only the link with a distance less than the pre-set threshold can be appended to gpsInfo. Lastly, MR I removes redundant information from gpsInfo to reduce the scale of the intermediate data. The Reduce function is expressed as follows:Reduce〈gridID,list〈gpsInfo〉〉⇒list〈(gpsInfo+list〈linkID,dis〉)〉A vehicle trip may across many grids (approximately 50 grids on average according to statistic data). The serial tMM algorithm swaps the grid data from the disk to the memory whenever the gridID changes. Thus, the efficiency of serial tMM algorithm suffers heavily from the I/O problem. As a comparison, the MR I requries far fewer swapping because the gridID of each node is relatively stable. Thus, MR I reduces many I/O operations using the MapReduce model.The second MapReduce procedure (referred to as MR II. See Fig. 9) takes the results of MR I as the input and maps them into trips according to the vehicle ID. In other words, MR II groups the GPS data of the same vehicle during a period of time into a “trip” identified by “tripID”. MR II obtains the final map-matching result trip by trip.In MR II, the Map function reads the gpsInfo and extracts the key of the vehicle number and GPS timestamp. The output of MR II is 〈(ID_Time), gpsInfo〉 pairs. After the shuffle procedure, the pairs are sorted by time and ID. We also use hadoop to automatically group all gpsInfo that are associated with the same ID to the trips. The Map function is illustrated as follows:Map〈(ID_Time),gpsInfo〉⇒list〈tripID,gpsInfo〉The Reduce function takes the 〈tripID,list〈gpsInfo〉〉 pairs as the input and calculates the entire weight of the trip using the weight values provided by MR I. Eventually, the function determines the most probable travel path according to the summarized weight values and outputs the result. Reduce function is expressed as follows:Reduce〈tripID,list〈gpsInfo〉〉⇒list〈tripID,value〉MR II can automatically sort the result of MR I by vehicle ID and GPS timestamp. For the serial tMM algorithm that runs on a single computer, the sorting process is a tough task when addressing huge amount of historical GPS data. In contrast, MapReduce is a calculation model based on the divide-and-conquers strategy, which is exactly the similar to the well-known quick-sort algorithm. It ensured that MR II can easily deal with big data sorting which is beyond the capability of single computer power.The general flow chart of our parallelized tMM algorithm is shown in Fig. 10.Vehicle tracking data in transportation data center can be divided into sensitive and non-sensitive parts. For non-sensitive data, we use the previously described parallel tMM algorithm to process them in public cloud of transportation data center. For those sensitive GPS data, we distribute the map-matching stages between public cloud and private cloud, in this way the organization can still keep their sensitive data protected when they gain cost-effective computing power from public cloud. The methodology can be described as follow:First we build the hybrid cloud environment. The public cloud and private cloud are connected by high-speed network. Private cloud is relatively small in scale since most computing tasks are outsourced to public cloud (see Fig. 11).Note that the original hadoop Client is modified in this framework to automatically allocate sensitive and non-sensitive part of GPS record. The sensitive data here is the personal or operational information, such as user ID, name, payment, etc. The modified hadoop framework will identify these sensitive data and store them in private cloud’s data nodes. Only the left trajectory information of GPS dataset, e.g., location information and timestamp, will be sent to public cloud in random order. Detailed implementation can be found in Zhang et al. (2011).When the sanitized GPS data is sent to public cloud, we can start the proposed parallel tMM processing that is described in previous section. But to conceal the final matched paths from public, we divide the original MR II into two stages, as shown in Fig. 12.The new MR II will stop at the step of ’summarize the weight of each candidate paths within a map grid’. The intermediate outcomes of public cloud processing will then be transferred to MR III, which will be executed within private cloud. In MR III the whole matched path will be finally generated, and the once separated sensitive part of GPS data will be restored to the match result.With our proposed method, the time-consuming stages of map-matching can be outsourced to the public cloud, while keeping the operations on the private data within the private cloud.In this section, we conduct experiments to evaluate the performance of our proposed algorithms.Our experiment environment consists of two parts: Hadoop cluster, and stand-alone computers used for comparison.For hadoop, we use a cluster of 20 computers. Each computer has a single 3.0GHz Intel Core i7 processor, 4GB DRAM memory, and 1TB 7.2K RPM hard disks. All computers are interconnected with a gigabit Ethernet switch. We use the Cloudera distribution of hadoop 1.0.4 runs on Ubuntu Linux 12.04LTS as MapReduce platform.We also use 10 computers with the same configuration for corresponding serial tMM algorithm tests. Each computer will run a serial tMM map-matching program during test.We use real traffic GPS dataset obtained from transportation data center of Beijing Municipal Committee of Transportation in our experiment. The dataset contains unsorted raw GPS data collected from over 90,000 probe vehicles in Beijing. The sampling period varies from 1s to 30s, with an average value of 10s. The whole data size is over 100 billion GPS records.Our evaluation is performed based on the road network of Beijing which has 106,579 road nodes and 141,380 road segments.First we tested the performance of our improved serial tMM algorithm on a stand-alone computer.The baseline tMM algorithm is referred to Liu et al. (2012). The efficiencies of tMM algorithms with or without the leapfrog method are shown in Fig. 13.As shown in Fig. 13, leapfrog-enhanced tMM algorithms are much faster than the traditional baseline algorithm when the data sampling rate is high. Particularly in suburban area, where road links are relatively longer, the improved algorithm can be five times faster than its counterpart.The efficiency of the improved algorithm rapidly declines when the data-sampling period increases because fewer intermediate points can be found in low-sampling-rate data. When the sampling period reaches the threshold (10s and 15s in urban and suburban areas, respectively), we disabled the leapfrog method, and the two algorithms become identical.We also tested the matching accuracy of the proposed algorithm. The result is shown in Fig. 14. Note that the improved tMM algorithm shows an identical high accuracy as the baseline algorithm because we only call leapfrog method when a candidate path is determined (i.e., the set of candidate paths contains only one path), so the two algorithms actually share an identical path determination mechanism.We implemented the proposed parallel leapfrog tMM algorithm in the hadoop platform.The ability of our 20-nodes parallel map-matching system was tested with datasets of different sizes. The result is shown in Table 2.Our system can process more than 120,000 records per second. As a rough comparison, it is much faster than a similar method proposed by Li et al. (2011), which is below 20,000 records per second when running in a 16-nodes hadoop system.We also compared the efficiency of our parallel tMM program and serial tMM program with the same datasets and hardware configuration. For the serial tMM program tests, we divided the datasets and manually assigned the slices to each standalone computer manually.The test result is shown in Fig. 15.The figure indicates that the hadoop cluster runs faster than the same number of standalone computers. Larger dataset shows a more obvious the trend. For 40 million records, the efficiency of the hadoop cluster was 28.8% higher than that of standalone computers. For 100 million records, the improvement is more than 54% (a 10 nodes hadoop cluster uses about 32min, whereas stand-alone computers used 50min). Because our parallel algorithm uses the MapReduce paradigm to address data- and I/O-intensive computing tasks, it gains more computing power from connected computers and shows the effect of1+1>2.We evaluated the scalability of our cloud-enabled HOM algorithm by varying the number of computational nodes. Likewise, we used a serial HOM algorithm on the same number of standalone computers for comparison.Fig. 16shows the performance of two implementations on the same dataset.Naturally, the capability of serial map-matching system showed almost linearly growth with the increase of node numbers. However, the parallel map-matching system shows an even more rapid and stable increase in processing speed. One reason is that our algorithm was well designed to take advantage of parallel computing power. In addition, the hadoop platform can dynamically move data and computing jobs among the nodes, which automatically ensures the load balance of each node. Thus, the hadoop system shows greater potential when the number of computing nodes increases.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
