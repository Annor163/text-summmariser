@&#MAIN-TITLE@&#
Origin and early evolution of corner polyhedra

@&#HIGHLIGHTS@&#
We describe the practical origins of the column generation method.We describe the discovery of periodicity in knapsack problems.We describe how knapsack periodicity led to a relaxation of the integer programming problem.We describe the origins of Corner Polyhedra, subadditivity and minimality.We show how Corner Polyhedra generate a great variety of cutting planes.

@&#KEYPHRASES@&#
Integer programming,Cutting,Linear programming,Corner polyhedra,

@&#ABSTRACT@&#
Corner Polyhedra are a natural intermediate step between linear programming and integer programming. This paper first describes how the concept of Corner Polyhedra arose unexpectedly from a practical operations research problem, and then describes how it evolved to shed light on fundamental aspects of integer programming and to provide a great variety of cutting planes for integer programming.This article is a personal account of my experiences with Corner Polyhedra and some closely related integer programming problems. It will not be a survey of the related literature, a survey which, because of my intermittent connection with the subject, I really could not write in an informed and balanced way.11This paper is based on my lecture “Forty Years of Corner Polyhedra” given on July 11, 2012 at EURO XXV. I want to thank the many researchers who have sent me their papers over the years, including the many years that I have spent away from integer programming, being engaged in other work. Trying to mention their work and place it in the proper context would be a very large and very worthwhile enterprise, but I do not attempt it in this paper.The article starts by describing work on the practical problems of paper mills. The necessity of dealing with the very large size of these problems motivated the invention of what is now called column generation. Then the results obtained using these methods turned out to have a completely unexpected periodicity.Explaining that periodicity led through various stages of understanding to the creation of the polyhedra that I named Corner Polyhedra. We will see how the Corner Polyhedra then took on a life of their own, giving insight into the structure of integer polyhedra and generating new families of cutting planes for general integer programming.Throughout I will do my best to describe the surroundings and motivation that drove this evolution and to make the various steps as clear as possible and illustrate them by examples. I will also occasionally point to directions which seem to me to have unrealized possibilities.In the 1950’s Operations Research was a new and exciting part of Applied Mathematics. It was appealing to me because it promised to extend the reach of Mathematics beyond the traditional fields of Science and Engineering and closer to ordinary life. And that did happen.But in addition, many times, Operations Research work motivated by practical needs, has also turned out to be mathematically beautiful.Many believe that Applied Mathematics, and especially Operations Research, is mainly a routine use of mathematics. Many believe that operations researchers find a problem, apply to it some well understood piece of mathematics, and the answer comes out. That ends it, the problem is solved.This certainly can happen, but often applied work is much more complicated than that. In applied work finding a way to formulate a problem mathematically can be difficult in itself. Then if you do succeed in finding a mathematical formulation, its sheer size and complexity may overwhelm standard approaches. You may have to split out tractable parts and leave the rest, or you may have to find a way to approximate, or you may have to invent.Sometimes you may succeed in all this only to find that your hard won solution is met with hostility by those who might be affected by it, or alternatively, you may be fortunate and find that those affected by your work are surprisingly eager to adopt it for reasons quite unconnected with what you have done.And, every now and then, you may turn up something unexpected, something you stumble across while pursuing something else. When I think of this sort of thing I always think of Christopher Columbus.Although Columbus lived in a world very different from ours, he had some problems that resemble those we have today. In modern terms we would say that Columbus had major difficulties getting his project funded, and then, after his plan to reach China and India by sailing west was finally accepted, he failed to get there.Columbus promised to find a new route to the Indies. But Columbus didn't find a new route to the Indies. Instead he discovered a whole new world. This too can happen.About 370 years after the Queen of Spain sent Columbus off toward the Indies, a modern ruler, IBM, sent off another smaller group of explorers. Our group of explorers was chartered to see if the mathematical methods of Operations Research could find and conquer rich new territories for computers.Among our explorers were Benoit Mandelbrot, Paul Gilmore, and T.C. Hu. Later we were joined by Philip Wolfe and Ellis Johnson.Columbus’ expedition embarked in three ships, the Nina, the Pinta, and the Santa Maria. Our smaller expedition also relied on three ships. Ours were named Linear, Dynamic, and Integer Programming. As it turned out, we needed all three for our voyage.Our little group within IBM's Research Division, was aware of the general stock cutting problem. This is the problem of starting with a stock of large pieces of some material and then cutting those large pieces into needed quantities of smaller sizes while creating as little waste as possible. We understood that a great variety of stock cutting problems could be formulated as linear or integer programming problems; so we wondered if there was something in this area that we could put into actual use.As a first step we tried a problem we had heard about that involved cutting up big steel girders for bridges; but although we could formulate the problem mathematically, the data from real bridges gave us integer programming problems that were way beyond the scale that anyone at that time could handle.Next Paul Gilmore and I took a look at a different stock cutting problem, the paper trim problem, the problem of cutting the very wide rolls of paper that paper mills produce into the smaller width rolls that people actually use. We had heard that there were special aspects of the paper trim problem that might allow us to use ordinary linear programming instead of integer programming; if true, that would make the problem more tractable.In the paper trim problem, as in the bridge problem, the actual numbers matter. So here is a description, based on our later experience, of a typical paper trim problem.22Gilmore and Gomory (1963) has a detailed description of an actual problem as an appendix.Paper comes streaming out of an enormous paper machine in a paper mill and is rolled up on metal spindles. The paper the machine makes has a fixed large width W, which depends on the paper machine. A typical width W is 200 inches or more. The mill's customers want rolls of paper in a variety of much smaller sizes, thirty different customer widths wj of 20 to 80 inches would be typical. So the mills are obliged to cut up the wide rolls they manufacture into the quantities bj of these smaller rolls that the customers want.To produce the right quantities of the smaller rolls, the mills have to cut up their wide rolls in many different ways. Each way to cut up one wide roll is called a cutting pattern. The ith cutting pattern is a list Ai that gives the number ai, j of rolls of a width wj that the pattern produces.It was well known that the paper trim problem could be formulated as an integer programming problem. In this standard formulation xi is the (integer) number of times the ith cutting pattern is used, n is the number of different cutting patterns Aithat are available (this is often a very large number), m is the number of different customer widths wjthat are demanded, and the goal is to choose integers xi(i = 1,…,n) that fill the m customer orders bjwhile minimizing the cost.Cost is taken to be the number of wide rolls required to fill all the orders. So here is a standard formulation:(1)MinimizeV=∑i=1i=nxisubjectto(1A)∑i=1i=nai,jxi≥bjforj=1,…,mIf we choose, we can add non-negative slack variables to (1A) to produce a formulation free of inequalities.Now (1) and (1A) together are a very straightforward integer linear programming problem. The only question is: is it too large to handle?What we had heard about the paper industry was that it was acceptable for the customer requirements, the bjnot to be exact requirements. It was acceptable to produce more than the amounts the customer asked for by a few percent, the customers were willing to take the extra rolls.What that meant to us was this: if the linear programming solution to (1) and (1A) gave us some non-integer xj, cutting patterns that were used a non-integer number of times, perhaps we could just round up those xjand the customer would accept the extra rolls that were generated.That sounded promising.However, even though it was now possible that we were dealing with an ordinary linear programming problem, our mathematical formulation still presented a difficulty: the enormous number of possible cutting patterns. With realistic paper industry problems, with for example 30 different customer widths to choose from, the number of cutting patterns, and therefore the number of columns in the linear programming problem, could easily run into many millions.Now solving a linear program with millions of columns was well beyond what even the largest computers of that time could do, while the paper mills at that time were yet to purchase even a small computer.So we had to think hard about how to get the problem down to something that could run on a small computer, and then hope that the cost savings our calculation might produce would cover its cost.We thought hard about what the simplex method actually does in (1) and (1A) and eventually found an approach.Suppose we start the simplex calculation of (1) and (1A) using an arbitrarily chosen set of cutting patterns Ai that we take as an initial basis. To form a first basic feasible solution we need only m cutting patterns if there are m different customer widths. In addition to this initial set we added to our starting problem a few more arbitrarily chosen cutting patterns which might turn out to be useful for improving from our initial basis.Note that In our linear programming formulation (1) and (1A) the column for each cutting pattern Ai has in addition to the m elements ai,jthat describe the rolls it produces, an entry −1 in the objective function row (1) that reflects the wide roll it is using up.Let M be the matrix that contains a column for each cutting pattern in the basis, all of them with −1’s in the row that represents the objective function. M also has an additional first column for the V that represents the objective function. This is a unit column, with 1 at the top row and then zeroes. M is a (m+1) x (m+1) matrix.The simplex method starts by finding the inverse M−1 of M. Then, using M−1and following the simplex method, we transform the entire starting matrix into a first basic feasible solution. The extra cutting patterns that we are carrying along are now columns associated with non-basic variables.If we look at the top row of the entire transformed matrix, this is the new cost row. The top entry in every transformed cutting pattern column is the result of taking the dot product of the top row of M−1 with the cutting pattern column (-1, Ai).). The top row of M−1 has a 1 in the column representing V, and then a string P of m non-negative numbers (p1, p2,…,pm); so the dot product with the column containing Aiis (1, P) · (−1, Ai) = (−1, P· Ai) = (−1, p1ai1, p2ai,2,…,pmai,m)The piare the shadow prices that reward the cutting pattern for the customer widths ai,j it produces, while the −1 in the V column penalizes it for using up the wide rolls that the mill produces.The simplex method tells us that if the cost row entry of a non-basic cutting pattern column is positive, that is if the value of what it produces outweighs its cost, then using that column to move to a new simplex basis will give an improved solution.So the problem of finding an improved solution has become this: can we find a new cutting pattern Ai such that its contribution outweighs its cost? Since the non-basic columns were arbitrarily chosen, and could be any of the millions of possible cutting patterns, we are asking this: is there, among all possible cutting patterns Ai, an Ai that will make a positive contribution given the current values (p1, p2,…,pm) of the shadow prices. Is there an Aiwith P· Ai>1?While any improving cutting pattern would be welcome, we would have an especially warm welcome for the cutting pattern that makes the biggest positive contribution. So let us shift gears slightly and look for that. So we are now looking for the cutting pattern Aithat maximizes P· Ai.Although the words we are using are different, we now have a classical knapsack problem. In the knapsack problem we have an assortment of objects with different values and different weights. The knapsack problem is to find the most valuable assortment of objects to put into the knapsack without exceeding a specified total weight.Our problem here is to find the most valuable cutting pattern given the current prices pjfor each customer width wjproduced. Our constraint here is not a specified total weight, it is a total width. To qualify as a cutting pattern, the collection of widths in a cutting pattern must add up to a total width not exceeding W, the width of the roll the paper machine produces.So, in looking for the cutting pattern Ai that maximizes P· Ai, we have, in different words, a knapsack problem.This is good news because there are many simple ways, including standard Dynamic Programming, to solve knapsack problems.All this enabled an algorithm that no longer required millions of columns.After obtaining the initial basis as described in Sections 2.4 and 2.5, we take as the value for each customer width wj the shadow price pj, and solve our constrained width knapsack problem using dynamic programming. Then we take the winning cutting pattern as a new column for entry into the next basic feasible solution of the simplex method.We can repeat this process over and over, generating new columns, going to new and improved feasible bases, until we have a basis with the property that the best new cutting pattern does not make a positive contribution. That means that there are no cutting patterns that will improve our current basis. We have the best possible basis, the best possible collection of cutting patterns.We had found a way to deal with the millions of cutting patterns. We now had all the elements for an algorithm.Carol Shanesy, our wonderful programmer, soon gave us a FORTRAN program that combined this method of generating new cutting patterns with the steps of the simplex method. We had an algorithm that actually ran.At this point we felt we had done what we could do from a distance; we had an approach to the integer requirement by rounding, and an approach to the problem of the millions of patterns through column generation. It was time to see some real paper mills and see what could actually be accomplished. Did our mathematical model fit what was really going on? Would the rounding yield acceptable results? Or were there other conditions that were real and important that we did not include in our formulation.Even if our formulation did match the actual situation, or could be changed to match it, would our methods save enough paper to make the use of our methods worthwhile? After all, the mill would have to buy a computer.Clearly the next step was to find a way into the land of real paper mills. Fortunately we had guides.Our guides were always IBM salesmen, people accustomed to selling IBM equipment to paper mills. Since at that time paper mills did not have computers, our guides usually sold the mills things like time clocks or accounting machines. The salesmen liked the idea of having something new to sell, so they were willing to work with us.So Paul Gilmore and I set out. We hoped, as a minimum, to learn more about the problem by visiting actual paper mills. We also hoped, if our approach still seemed reasonable when confronted by the realities of actual mills, to find a mill that would actually try out our ideas.Paper mills were then, and probably still are now, very impressive places. At a paper mill big hunks of wood are thrown in at one end of a paper making machine, which can easily be 400 feet long, and a wide stream of paper comes out at fifty miles an hour at the other end. As it emerges, this stream of paper is rolled up on a succession of big metal spindles and is ready for the cutting process.We soon learned that at the mills there were some very special people who cut up the wide rolls. They decided how to cut by an intuitive feel developed by experience, lots of experience. There were some people who did this job well, and there were others who did not; it was a skill not everyone could develop. Some mill managers were worried; many workers who did a good job of cutting were getting older, it was not clear they could easily be replaced when they retired.Partly because of this concern there were a few mills that were willing to try what was for them a fairly painless experiment. They gave us their customer orders for a month or so and asked us to come back with our calculated results. They could then see how our results compared with what they had actually done.We took their data back to our home base at IBM Research and started running calculations. After a few false starts our programs ran on their data, and what came out looked good to us. When we had enough results we sent them back to our IBM salesmen who then took them over to the mills.At the mills there was agreement that rounding was not a problem and that our solutions did reduce waste. Usually our solutions saved only a few percent, but sometimes more. But even a few percent of the output of a paper mill is a lot of paper. Though savings varied quite a bit from one mill to another there was often enough saved to make it economically attractive for the mill, provided the computation could be done at the mill and on a small computer.So this was starting to be serious.The hard problem was running time.We knew that to succeed we needed to cut down running times. The mills needed to get the results that we were getting on our big computer at IBM Research on a small computer that was affordable to the mills, and the results had to come out reasonably quickly.We worked to reduce running time and looked at lots of examples. We learned by trial and error. We learned to avoid cutting patterns that included both rolls for which there was high demand and rolls for which there was low demand. We learned when it was time to cut off the calculation and to stop, even though the result was still improving slowly etc.In our calculations we found that we were spending much more time solving the knapsack problems that found a new cutting pattern, than on the next linear programming step that used that new cutting pattern, so Paul and I worked hard at ways to improve the knapsack calculation itself.In the end what we did worked. We got the running time down to the point where paper mills started to buy computers for the first time. Paul Gilmore and I were even named “IBM Science Salesmen of the Month”, for May 1961 or possibly May 1962. Being IBM science salesman of the month is not quite the same as winning the Nobel Prize, but we felt good about it nevertheless.Eventually our methods and improvements on them became very widely used in paper mills.We also extended our methods to classes of two dimensional problems (glass is an example) and wrote an article describing fast methods for the knapsack problem papers (Gilmore & Gomory, 1961, 1963; 1965). The idea of column generation became quite widespread. And for our paper that described many of these things Paul Gilmore and I were awarded the Lanchester prize of the Operations Research Society in 1963.However while we were visiting paper mills and glass plants and improving and extending our methods, we were also collecting lots of data. One of the things we collected data on was the effect on waste of the width of the paper machine33An example of that relation for a real problem appears as an appendix in (Gilmore & Gomory, 1963).. We also looked at the knapsack problems separately from the rest of our algorithm to see how much individual cutting patterns improved with increasing W.Here we show in somewhat idealized form a typical knapsack graph (Fig. 1).The values (shadow prices) and widths of the individual rolls are fixed. The graph shows the total value of the small rolls that can be cut from one wide roll as a function of machine width W. We can see that the total value of the knapsack increases steadily with increasing width, as it should, but in addition, we see that, after an initial period of irregular increases in total value, the increasing total value becomes periodic.We had not expected this at all; but there it was in the data.What exactly did we have there? Once we started looking, an answer was not hard to find.We soon realized that the period that we saw in the periodic part of the graph was always exactly the same as the width of one or the other of the small rolls that we were cutting from the wide roll. In fact we found that the small roll that determined the periodicity was always the roll wjthat had the most value per unit of width, the roll that was the most rewarding for the space it used up.It is plausible that if the size W of the knapsack is big enough, the optimizing solution is likely to have some of these value densest rolls in it. If it does, it is likely that the optimizing solution for a knapsack problem of size W+wi, obtained by adding one more value dense roll to the optimizing solution for the knapsack of size W, has a good chance to be the optimal solution. If this actually occurred for all sufficiently large knapsacks, this behavior would lead to periodicity.Paul Gilmore and I gave a rigorous explanation along these lines in our paper on the Theory and Computation of Knapsack Functions (Gilmore & Gomory, 1966).However I had a feeling that there was something more there to be discovered.Now we are going to shift gears and focus on the knapsack problem. The knapsack problem, which is the one-dimensional integer programming problem, appeared in Section 2.6 because of its role in the paper trim problem. Now we focus on it alone and try to better understand its periodicity.We will also make a transition in notation as we leave the paper trim problem behind.When working on the paper trim problem the size of the roll that was to be cut up was determined by the width of the paper machine. It was natural to use W for that width and wjwidth of the rolls cut out of W. However when talking about the knapsack problem alone, without the paper mill setting it has been, oddly enough, more usual to talk about the length L of the knapsack and lower case letters li for the lengths of the individual pieces, and also values virather than prices pi for the values of the individual lengths. This is what we did, for example, in our paper on the theory and computation of knapsack functions. We will make that notation transition here.Using length terminology for our knapsack problem we assume we have available n kinds of pieces, the ith kind has length liand value vi. We want to find the integer quantities xiof each piece that fit in the available length L and have the largest total value V.We could write the condition of fitting in as an inequality, as we did with the equations in Section 2.2, but here we will write the knapsack problem in equation form. Our pieces will always include one piece of length 1 unit44The unit is whatever fineness the pieces are being measured to. It could be an inch, a tenth of an inch, or one millimeterwith value 0. Multiples of this piece substitute for an inequality and represents wasted length, and all our other lengths are integer multiples of its length.So we will move forward with knapsack problems in equation form:(2)maximizeV=∑i=1i=nvixiwith∑i=1i=nlixi=LAll xiinteger.We start our investigation into periodicity with the linear programming solution to (2), then gradually turn that into an integer solution to the knapsack problem. We will see what this approach can tell us about the periodicity of large knapsack problems and we will illustrate what is happening with a small numerical example.Starting with the linear programming solution to (2) means that we will fill the entire length L of the knapsack using only the densest piece. We will let x1 represent the (usually non-integer) quantity of that densest piece, so the basic solution is:(3A)x1=Ll1−∑i=2i=nlil1xi.In (3A) the basic variable x1, which tells us how many rolls of the densest piece are used, has value L/l1. The non-basic variables xi are zero. At this basis, the objective function V of (2) looks like this:(3B)maximizeV=v1Ll1+∑i=2i=nvi−xiwithallvi−=li(vili−v1l1).At our current basic feasible solution the objective function V has value v1L /l1. and the coefficients v−iin the objective function row are now all negative. That is as it should be since using any of the non-basic variables takes up space in the knapsack that could be used better by the densest piece x1.To find an integer solution to (3A) requires increasing some of the non-basic variables xifrom their present zero values to new positive integer values that make the basic variable x1 a non-negative integer. We want to find the change in the non-basic variables that produces that x1 with the least possible increase in cost. Those values of the non-basic variables, together with the x1 they produce, solve the integer knapsack problem.Our approach, which will turn out to be the beginning of Corner Polyhedra, will be this: we will focus on finding the least cost change in the non-basic variables that makes x1 an integer. We will not require x1 to be non-negative.Once we know those non-basic variable values, it will turn out to be straightforward to see for what values of L, the knapsack length, the x1 is non-negative..Since the condition of being an integer is that x1 ≡ 0, (Mod 1), we apply that condition to (3A). (3A) then becomes what we will call the Relaxed Knapsack Equation.(4)∑i=2i=nlil1xi≡Ll1Mod(1),or equivalently(4A)∑i=2i=nlixi≡LMod(l1)We also make a small change in our objective function (3B), omitting the constant term v1L /l1 . This gives us an objective function that measures directly the cost of departing from the linear programming solution. So we take as the objective function for the Relaxed Knapsack Equation(4B)minimizeV−=∑i=2i=n−vi−xi.V— and all the individual terms on the right are now non-negative.If we look closely at our new minimization problem, the objective function V− from (4B) with the Relaxed Knapsack constraint (4A), we will realize that we are really minimizing V−−over a group. What exactly does that mean?Consider the group of the integers (Mod l1). We can map the integer lengths liand L that appear in (4A) into the group elements l*iand L* they correspond to in that group. This mapping is addition preserving, so (4A) becomes(4C)∑i=2i=nli*xi=L*Conversely if we have group elements l*iand L* that satisfy (4C) then if we substitute for those group elements any lengths li and L that are mapped into them these lengths will satisfy (4A).To see concretely what all this means let us look at a small example in a way that allows us to visualize and solve the group minimization problem that we now have.Here is a very small example. We have a knapsack of length 27 and pieces of lengths li= 10, 14, 8, 6, 5 and 1. The values viof the pieces are 15, 20, 10.25, 5.5, 5.5 and 0. The piece of most value per unit length is the piece of length 10.In (4A) we replace lengths liby their corresponding group elements from the group of integers (Mod 10). Using * to indicate group elements, our pieces of lengths 1, 5, 6, 8, and 14 correspond to the group elements 1*, 5*, 6*, 8*, and 4*. The knapsack length 27 corresponds to the group element 7*. If the knapsack length L had been 17 or 37 or 47, rather than 27, L* would still be is 7*. However if L had been 28, L* would be 8*. All possible knapsack lengths end up being one of the 10 group elements.We will call L* the group goal element.Fig. 2shows the integers (Mod 10) as nodes of a graph which we will refer to as the group graph H. The arcs in the group graph show the effect of adding to the group element 0* the group elements (1*, 4*, 5*, 6*, 8*), which correspond to the knapsack pieces of length (1, 14, 5, 6, 8).The goal element L* is 7* and is shown in red.The connection of the group graph with finding solutions to (4A) is very direct. Any path p in H leading from 0* to the goal element L* produces a solution to (4A). The values of li and L that solve 4A are any values that map into the group elements included in the path p, and the xithat appear in (4A) are the number of times the ith group element appears in the path.If we assign to the ith arc, as its cost, the value −v−−i,,, the minimization problem (4B) becomes the problem of finding the least cost path from 0* to L* in the graph H based on the costs −v−−i.In the path shown in Fig. 3the first number on each arc is v—i, while the second number is the actual length of the piece.The minimal cost path from 0* to 7* is quickly found55Any shortest path method, including dynamic programming, will do. Methods for solving knapsack problems are discussed extensively in (Gilmore & Gomory, 1966).to be {4*, 5*, 8*}, the path shown in Fig. 3. The total cost of the path is 1+2+1.75 = 4.75 and its total length is 14+5+8 = 27.For knapsacks of any length L equivalent to 7, the total path cost 4.75 is the reduction in the value V caused by raising the non-basic variables attached to the pieces of length 14, 5, and 8 from 0 to 1. This change will produce an integer x1 for any knapsack with L equivalent to 7 (Mod 10) so it applies to knapsacks of length L = 7, 17, 27, 37, 47, 57… and so on indefinitely.We can follow this same procedure if the goal element is not 7* but instead any other group element. We will find that the cost of the least cost paths from 0* to goal elements g= (1*, 2*, 3*, 4*, 5*, 6*, 7*, 8*, 9*) are (1.5, 2.75, 3.75, 1, 2, 3.5, 4.75, 1.75, 3) respectively, and the total length of each of those least cost paths is TL(g) = (1, 22, 13, 14, 5, 6, 27, 8, 19) .In Fig. 4the vertical axis displays the increases in value as the knapsack length is increased by 10. The horizontal axis shows the group goal element. The blue line shows the increase in the L.P. solution value. The red line shows the increase in the value obtained with the best integer solution.This periodic pattern repeats indefinitely.Now the question is: when are the integers x1 that are being produced by this periodic pattern non-negative integers? When they are non-negative they solve the original knapsack problem.Fortunately it is now straightforward to find out what values of the knapsack length L make x1 non-negative.We will ask and answer that question for our example, then the method will be plain. We have goal element 7* and therefore our least cost path is, as shown in Section 5.4, is (4*, 5*, 8*). These are the non-basics that were 0 in the LP solution but are now 1 in the least cost path. To determine x1 we substitute that path value into the sum in (3A). Putting the lengths (14, 5, 8) in the sum and multiplying through by l1 leaves us with 10x1 = L-28.Now the situation is clear, for goal element 7* we will have x 1 values that are negative if L <28, for example 7 or 17 or 27. This does not provide a solution to the knapsack problem. If L is 28, x1 will be 0, a valid solution, there will be no rolls of 10 in the knapsack solution for this special value. For L = 37, there will be one densest roll, for L = 47 there will be two of them, and then the solution to the knapsack problem continues indefinably adding more and more of the densest roll to the fixed values of the non-basic variables that appear in the least cost path.More generally, given a goal element and its attendant least cost path we substitute its xi values and their lengths in the sum in (3A) which is simply the total path length TL(g*) of the least cost path.We then obtain l1x1=L−TL(g*). So the critical value is L = T(g*). For L ≥ T(g*) x1 will be non-negative and solves the knapsack problem for goal element g*.To find solutions for the whole knapsack problem we simply find the group element g* with largest TL(g*) value TLmax. For L≥ TLmax our least cost paths solve the Knapsack problem; we simply use the least cost paths with increasing numbers of densest rolls added.This provides periodicity with period l1.For problems with goal group elements (0*,1*, 2*, 3*, 4*, 5*, 6*, 7*, 8*, 9*), the least cost path lengths turn out to be (0, 1, 22, 13, 14, 5, 6, 27, 8, 19).Therefore for knapsack lengths of 27 or longer, all the group shortest paths can be used for the knapsack solution with the appropriate number of 10’s added. For all L we will have complete periodicity of order 10 in the knapsack.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
