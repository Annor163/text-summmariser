@&#MAIN-TITLE@&#
Seasonal and trend time series forecasting based on a quasi-linear autoregressive model

@&#HIGHLIGHTS@&#
We forecast the seasonal and trend time series using a quasi-linear autoregressive model.A combined genetic optimization and gradient-based optimization algorithm is applied for automatic selection of proper input variables and the model-dependent variable, and optimizing the model parameters simultaneously.The comparison results show the effectiveness of the proposed approach for the seasonal time series.

@&#KEYPHRASES@&#
Seasonal and trend time series,Forecasting,Varying coefficient model,Hybrid training approach,

@&#ABSTRACT@&#
Modeling and forecasting seasonal and trend time series is an important research topic in many areas of industrial and economic activity. In this study, we forecast the seasonal and trend time series using a quasi-linear autoregressive model. This quasi-linear autoregressive model belongs to a class of varying coefficient models in which its autoregressive coefficients are constructed by radial basis function networks. A combined genetic optimization and gradient-based optimization algorithm is applied for automatic selection of proper input variables and model-dependent variables, and optimizing the model parameters simultaneously. The model is tested by five monthly time series. We compare the results with those of other various methods, which show the effectiveness of the proposed approach for the seasonal time series.

@&#INTRODUCTION@&#
Many time series data contain the trend and seasonal patterns as its basic components. Trend and seasonality are very commonly encountered in economic and business time series. Accurate prediction of trend and seasonal time series is vital for decisions in retail, marketing, production and many other business sectors [1,2]. There are several approaches to deal with the trend and seasonal time series. Traditional approaches are to remove the seasonal variations using certain seasonal adjustment method, e.g., decomposes the series into trend, seasonal, cyclical and irregular components [3]. Empirical studies suggest that the seasonal adjustment method is with controversy [4,5]. Another type of classical approach is based on the well-known Box–Jenkins models, or called autoregressive integrated moving average (ARIMA) models. Differencing is first used to derive a stationary time series, and then the ARMA model is applied. The seasonal ARIMA technique is described in details in [6]. The limitation of this approach is that the underlying process studied is assumed to be linear and consequently the models may fail to capture nonlinear features commonly encountered in practice [7]. Nowadays, a popular topic in modern data analysis is the neural networks (NNs) which have become one of the most valuable tools for time series modeling and prediction [8–12]. Due to their universal approximation capability, the NNs are also important candidates for forecasting seasonal time series. For example, Zhang and Qi [3] investigated the issue of how to effectively model time series with seasonal and trend patterns using NNs. Zhang and Kline [13] presented a NN approach to forecasting quarterly time series. Qi and Zhang [14] investigated how to best model trend time series using NNs. Hamzacebi [15] proposed a new NN structure for seasonal time series forecasting. Taskaya-Temizel and Casey [16] gave a comparative study of autoregressive neural networks hybrids on seasonal time series forecasting.Recently, the varying coefficient models [7,17,18] have become a very important tool to explore the dynamic pattern in many scientific areas, such as economics, finance, ecology and so on. One such kind of the models is the radial basis function network-based state-dependent autoregressive (RBF-AR) model [19–21]. The RBF-AR model is derived by using a set of RBF networks to approximate the coefficients of a state-dependent AR model [22]. It is appealing to interpret the RBF-AR model as a locally linear AR model in which the evolution of the process is governed by a set of AR coefficients. From this perspective, the RBF-AR model also belongs to a class of quasi-linear AR models [23]. Although the RBF-AR models have already some successful applications [24–26], they are never applied in seasonal time series. In this paper, we model and forecast the seasonal and trend time series using the RBF-AR model. In [14], Qi and Zhang's research results indicate that differencing the trend time series data first is the best practical approach to building an effective NN forecasting model. So, differencing is first applied to derive a stationary time series in this paper. Many researchers [2,6] point out that it is important in practice that models should contain as few parameters as possible consistent with achieving an adequate fit. Therefore, to improve the prediction performance of the RBF-AR model, we propose a combined genetic optimization and gradient-based optimization algorithm for term and variable selection, including selection of proper input variables, the number of hidden nodes, the model-dependent variables, and optimizing the model parameters simultaneously.The rest of the paper is organized as follows. Section “The model and its identification method” briefly introduces the RBF-AR model and its identification method. Section “Data sets and results” presents the experimental investigation and comparison results. Finally, we conclude this paper in “Conclusions” section.Denoting the series under study by {xt:t=1, 2, …, M}, a general nonlinear autoregressive model may be written as(1)xt=f(xt−1,xt−2,...,xt−p)+etwhere f(·) is a nonlinear map, p is a positive integer often referred to as the order of the model, and etdenotes noise usually regarded as Gaussian white noise independent of the previous observations. Obviously, model (1) is too general to be useful in practice. Usually, some specific form is assumed. A flexible structure and without lack of generality is the RBF-AR model [21](2)xt=ϕ0(Xt−1*)+∑i=1pϕi(Xt−1*)xt−i+etϕ0(Xt−1*)=c0+∑k=1mckexp{−λk||Xt−1*−Zk||2}ϕi(Xt−1*)=ci,0+∑k=1mci,kexp{−λk||Xt−1*−Zk||2}Zk=(zk,1,…,zk,d)TwhereXt−1*=(xt−i1,…,xt−id)Tis the model-dependent variable (state vector) with i1, …, idas the threshold lags; p is the model order, m is the number of hidden layer nodes of the RBF network, and d is the dimension of state vectorXt−1*;ϕ0(Xt−1*)andϕi(Xt−1*)are the state-dependent functional coefficients which are all composed of Gaussian RBF networks; λk(k=1, 2, …, m) are the scaling parameters (widths), and Zk(k=1, 2, …, m) are the centers of the RBF networks; ck(k=0, 1, 2, …, m) and ci,k(i=1, 2, …, p; k=0, 1, 2, …, m) are the linear weights of the RBF networks; ||·||2 denotes the vector 2-norm.The RBF-AR model can be regarded as a quasi-AR model, in which its coefficients are constructed by a set of RBF networks. Obviously, the RBF-AR model not only has the merit of the RBF networks in function approximation, but also contains various similar linearity properties to those of the linear AR model. This makes the RBF-AR model is quite valuable in practice. It should be noted that if m=0 the coefficientsϕ0(Xt−1*)andϕi(Xt−1*)are constants in this case), the RBF-AR model (2) reduces to a linear AR model.The identification of the RBF-AR model includes the choice of input variables, estimation of all the parameters (the number of hidden nodes, center vectors, widths, and the linear output weights), and selection of the appropriate state vector. As mentioned in “Introduction” section, selecting proper model terms and variables to produce a parsimonious model is very beneficial for improving forecasting performance. Both random search algorithms [11,27,28] and gradient search algorithms [29] are used to train the NNs. In this subsection, we introduce a combined genetic optimization and gradient-based optimization algorithm for estimating the RBF-AR model. This hybrid algorithm is originally presented in [12]. In the encoding scheme, the selection of input variables and state vector, and the number of hidden nodes are binary-coded and the parameters (centers and widths) are real-coded. The encoding scheme is shown in Fig. 1. The first three parts are evolved by a genetic algorithm (GA) and the fourth parts (parameters) are optimized by a gradient based optimization method at each iteration of the GA. The first two parts deals with the selection of input variables and state vector (1 denotes the corresponding input or state is selected). The third part deals with the number of hidden nodes. The fourth part deals with the optimization of the network parameters. Since the weights ckand ci,kin Eq. (2) can be estimated by pseudo-inverse algorithm, explicit inclusion of these parameters into the chromosome is unnecessary. Therefore, only the centers and widths are encoded into the chromosome. With the encoding scheme at hand, it is now very easy to evolve the model using a normal GA. It is worthy to note that the parameter optimization algorithm we used for the fourth part is the structured nonlinear parameter optimization method (SNPOM) [30]. This gradient based algorithm aims at accelerating the computational convergence and gaining better modeling precision. The summarized procedure of the identification is given below.Step 1. Initialization. An initial population of N individuals using the encoding scheme described in Fig. 1 is produced in accordance with a uniform distribution ranging over problem space. Each chromosome represents a set of model structure and the parameters.Step 2. Calculation of fitness functions. For each chromosome in the population, the first, second and third parts are the selected input vector, state vector and the number of hidden nodes respectively, and the fourth part is the network parameters which are used as initial values for the SNPOM. The larger of the root mean square error (RMSE) for the training and validation data sets is regarded as the fitness function of each chromosome, which is obtained by the SNPOM. The networks parameters of each chromosome are optimized by SNPOM as follows.(1)In order to apply the SNPOM to parameter optimization of the RBF-AR model, we rewrite model (2) in the regression form(3)xt=Ψ(θN,Xt−1*)TθL+etwhereθN≜(λ1,…,λm,Z1T,…,ZmT)T, i.e., including all nonlinear parameters; θL≜(c0, c1, …, cm, c1,0, c1,1, …, c1,m, …, cp,0, cp,1, …, cp,m)T, i.e., including all linear parameters, and Ψ is the regression vector. The objective function is(4)V(θL,θN)=12||F(θL,θN)||2whereF(θL,θN)=xˆp+1−xp+1xˆp+2−xp+2⋮xˆM−xMwherexˆis the one-step-ahead prediction based on model (3), {xi|i=p+1, p+2, …, M} is the measured data set, and M is the number of data observation. The parameter optimization problem is then to compute:(5)(θˆL,θˆN)=arg minθL,θNV(θL,θN)The optimization calculation on the search forθNk+1at the kth iteration, followed by the immediate update of linear weightsθLk+1using the linear least-squares method (LSM) as follows:(6)θLk+1=∑t=p+1MΨ¯t,k+1Ψ¯t,k+1T−1∑t=p+1MΨ¯t,k+1xtΨ¯t,k+1=Ψ(θNk+1,Xt−1*)The updating formula for the nonlinear parametersθNk+1is(7)θNk+1=θNk+βkdkwhere dkis the search direction, and βkis a scalar step length parameter representing the distance to the minimum. In order to increase the robustness of the search process, which is based on the Levenberg–Marquardt method (LMM), the dkin (7) is obtained from a solution of the set of linear equations:(8)[J(θNk)TJ(θNk)+γkI]dk=−J(θNk)TF(θLk,θNk)whereJ(θNk)is the Jacobian matrix ofF(θNk,θLk)with respect toθNk, and the scalar γkcontrols both the magnitude and direction of dk.Step 3. Perform the standard GA operators (selection, crossover and mutation [31,32]).Step 4. Calculate the fitness of each new individual in the population.Step 5. Repeat the evolution process Steps 2–5 until a predetermined number of generations is reached.Of course, one may consider a faster convergence criteria by also checking (in logical or with the maximum number of iterations) if the fitness is changing over a predefined number of iterations. This would allow one to select the maximum number of iterations according to the hardware capability. For a more clear procedure of the algorithm, a flowchart is provided in Fig. 2.Five real time series (US retail sales) from US census Bureau are used to test the performance of the RBF-AR model described in section “The model and its identification method”. The five retail series are retail department stores, book stores, clothing stores, furniture stores, and hardware stores. All data series starts from January 1992 and ends in December 2001. These series are monthly and are not seasonally adjusted; therefore there are 120 data points in each of them. Monthly series are selected as they exhibit stronger seasonality. For each of the series, the data set is divided into three parts: the last 12 data have been reserved for testing, the preceding 12 data for validation, whilst the rest for estimation. This division for estimation, validation, and testing is kept same with Zhang and Qi [3] and Taskaya-Temizel and Casey [16] to make a fair comparison.The best model structure is determined by the one which produces the smallest RMSE for both the training and validation data. This is because if a model is “over-fitted”, it may produce a small RMSE for the training data set but tends to produce a large RMSE for the validation data set. On the other hand, if a model produces a large training RMSE and a small validation RMSE, it may indicate that the forecasts for the validation happen to fit the data well. Therefore, Kajitani et al. [33] recommend using the model which makes smallest the larger of the RMSE for the training and validation data sets.The five monthly time series are plotted in Fig. 3. It is very clear that all the series exhibit trend together with seasonal patterns. Because Qi and Zhang's research results [14] suggests that differencing often gives meritorious performance for trend time series, differencing is first applied to obtain a stationary time series. The differenced series are plotted in Fig. 4. We consider the 12 variables x(t−1), x(t−2), …, x(t−12) as input candidates to affect the present x(t), and the dimension of state vectorXt−1*is set to 1 for simplicity. We also only consider the number of hidden nodes m=0 or 1. The state vectorXt−1*is selected from input candidates. The parameters used to run the GA are given as follows: population size=60, crossover probability=0.8, mutation probability=0.05, maximum generation=500.Columns 2–4 in Table 1list the mean and standard deviation of RMSE results for 30 runs for the training, validation and testing samples, respectively. The values in the parentheses are the standard deviations. The average time of every run is about 6min when the experiments are carried out using MATLAB on a 2.7GHz desktop PC with Windows XP. Table 1 also presents other recent results from Zhang and Qi [3] and Taskaya-Temizel and Casey [16], and the results of support vector regression (SVR) for comparison. The standard deviations provided by the RBF-AR model in Table 1 are very small, which reflects that the estimation algorithm is capable of performing a robust and stable search. From Table 1, we can see that compared with the ARIMA model [3] and TDNN [16], the RBF-AR model performs better on three out of five data sets; compared with the ANN [3], the RBF-AR model achieves much better results on department stores and hardware stores data sets, while obtain similar results on clothing stores and book stores data sets; compared with the SVR, the RBF-AR model achieves better results on all data sets. Therefore, one can conclude that the RBF-AR model achieves very competitive results to other state-of-the-art techniques.Table 2reports the selected inputs, state vector, number of hidden nodes, and RMSEs of the RBF-AR model for one run. From the results in Table 2, we can see that not all of the input candidates (x(t−1), x(t−2), …, x(t−12)) are selected. This indicates that some of the terms in the model are redundant, and the learning algorithm can delete them automatically.Instead of comparing the values of RMSE results directly, we next test whether the RMSE results of these methods are statistically different or not using F-statistics. Table 3summarizes the results (at 0.1 significant level) of RBF-AR model versus other models for the testing data. In Table 3, ‘<’ denotes the RMSE of one method is significantly smaller than the other method; ‘≈’ denotes the RMSEs of the two methods are statistically indistinguishable; and ‘>’ denotes the RMSE of one method is significantly bigger than the other method. From Table 3, we note that for most of the cases the RBF-AR model is ‘<’ or ‘≈’ to other methods. This further demonstrates the effectiveness of the RBF-AR model for the seasonal time series.

@&#CONCLUSIONS@&#
