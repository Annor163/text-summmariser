@&#MAIN-TITLE@&#
Improved pruning of large data sets for the minimum enclosing ball problem

@&#HIGHLIGHTS@&#
We develop pruning strategies to accelerate minimum enclosing ball computations.We give improved pruning bounds that are valid in a number of existing algorithms.Using these bounds, we achieve twice the effect compared to earlier approaches.We demonstrate substantial speedups of several state-of-the-art algorithms.

@&#KEYPHRASES@&#
Minimum enclosing balls,Bounding spheres,Acceleration techniques,Pruning,Culling,

@&#ABSTRACT@&#
Minimum enclosing ball algorithms are studied extensively as a tool in approximation and classification of multidimensional data. We present pruning techniques that can accelerate several existing algorithms by continuously removing interior points from the input. By recognizing a key property shared by these algorithms, we derive tighter bounds than have previously been presented, resulting in twice the effect on performance. Furthermore, only minor modifications are required to incorporate the pruning procedure. The presented bounds are independent of the dimension, and empirical evidence shows that the pruning procedure remains effective in dimensions up to at least 200. In some cases, performance improvements of two orders of magnitude are observed for large data sets.

@&#INTRODUCTION@&#
Given a center pointc∈Rdand a radiusr∈R, letB(c,r)denote the ball{x∈Rd:‖x-c‖⩽r}, that is, the subset ofRdwithin Euclidean distance r from c. Then some given finite set of pointsP={p1,…,pn}⊂Rdis enclosed byB(c,r)if‖pj-c‖⩽rholds forj=1,…,n. The minimum enclosing ball (MEB) problem is to find the unique ball with minimum radius that encloses P. Henceforth, we denote the center point and radius of the MEB byc∗andr∗, respectively, and we writeB(c∗,r∗)compactly asB∗. The MEB problem, which is also known as the 1-center problem or the minimax location problem, has been studied for more than a century, and still receives much attention today due to its relevance in important application areas such as rendering, animation, collision detection, robotics, and machine learning.It is well-known thatB∗is determined by at mostd+1boundary points from P. Thus, the fundamental task of an exact MEB solver is to locate these support points[15,8,7]. In many types of input, however, a majority of the points are strictly insideB∗. Being able to identify and eliminate, or prune, such points early on during MEB computations is therefore likely to speed up the subsequent processing.In situations where the exact solution is not required, a(1+∊)-approximation ofB∗, i.e., an enclosing ballB(c,r)such thatr⩽(1+∊)r∗, can be computed efficiently by collecting a small core-set of representative input points [4,3]. This subset has the property that its MEB enlarged by a factor of at most(1+∊)encloses also P. A number of algorithms have been presented that compute a(1+∊)-approximate solution by finding a core-set of sizeO(1/∊), which is independent of both n and d[2,10,14,3,16,12]. Eliminating interior points has the potential to accelerate also these algorithms.Clearly, the support points belong to the convex hull of the input set, and it would be possible to initially eliminate all points not on the hull. However, the time complexity of computing the convex hull would exceed that of most MEB algorithms used in practice. Furthermore, when all input points are on the hull, nothing would be pruned. On the other hand, the algorithm by Megiddo [13] uses a sophisticated prune-and-search method that always manages to reduce the input points by a constant factor in each algorithm pass. Thus, termination is guaranteed in linear time in fixed dimension. A realization of Megiddo’s approach, however, would be intractable due to its exponential dependency on the dimension.A simpler and more practical pruning approach is described by Ahipaşaoğlu and Yıldırım [1]. By interleaving pruning passes with the main iterations of the two algorithms by Yıldırım [16], they obtain significant performance improvements in dimensionsd⩽100. Both of these algorithms run inO(dn/∊)time, and arrive at a(1+∊)-approximation ofB∗through a sequence of intermediate approximations with monotonically increasing radii that converge tor∗from below. In each iteration, a scan of the entire input set is performed to find the point farthest from the current center point. These repeated farthest-point queries, each takingO(dn)time, dominate the execution time. Thus, reducing the size of the input gives immediate speed benefits in subsequent iterations.Given such an intermediate approximationB(c,r), wherer⩽r∗, they derive the following upper boundΔ‾on the distance‖c-c∗‖:(1)Δ‾=R2-r2,whereR=maxpj∈P‖pj-c‖. Using this bound, they also derive the following conservative condition to determine if a point p is enclosed in the interior ofB∗:(2)‖p-c‖<r-Δ‾.Thus, any point satisfying this condition can safely be removed from the input.In each pruning pass, Eq. (1) is first applied to the most recent intermediate ball, and then the pruning condition in Eq. (2) is evaluated for each point in P. In order to avoid the overhead of invoking the pruning procedure in situations where little or nothing is pruned, the procedure is skipped every time the right-hand side of Eq. (2) evaluates to a value smaller than or equal to0.55r, where the factor0.55was determined empirically.Nielsen and Nock [14] propose a distance-filtering technique to speed up repeated searches for the farthest point. Using the Cauchy–Schwarz inequality, they are able to compute an upper bound on each squared distance‖pj-c‖2in constant time during the search. In this way, theO(d)cost of computing the exact squared distance can be avoided whenever the upper bound does not exceed the largest squared distance encountered so far. An advantage of this approach is that no additional parameters, such as r and R above, are required, which might make it more generally applicable. A disadvantage, however, is that the effectiveness of this method is sensitive to how the input points are distributed in relation to the origin. Alternative distance filtering strategies that do not suffer from this problem are proposed in a recent publication [9]. A drawback shared by all these filtering approaches, however, is that every input point must be at least touched in every iteration, which is in stark contrast to the pruning procedures discussed herein, which continuously eliminate points entirely from further processing.In this article, we improve the approach by Ahipaşaoğlu and Yıldırım so that more points are eliminated in each pass. We develop our methods from the concept of viability of the intermediate balls used to derive the pruning bounds. Specifically, our main contributions are (i) the key insight that viability is in fact satisfied in many existing algorithms for the MEB problem, which makes our pruning methods widely applicable, (ii) a tighter boundΔ‾, which in itself immediately leads to more effective point reductions, (iii) an improved alternative to the pruning condition in Eq. (2) and (iv) a thorough empirical evaluation where the presented techniques are applied to several state-of-the-art algorithms. In addition to enabling improved pruning, we believe that our theoretical results bring additional understanding of the MEB problem and may be useful in a wider perspective.This section presents the theoretical results of the paper. In the first part, we discuss the general assumptions underlying our approach. Then in the second part, we present bounds that enable improved pruning.We will require the following property on any approximate solution used to derive our bounds.Definition 1A ballB(c,r)is viable if it satisfies(3)r2+‖c-c∗‖2⩽r∗2.Geometrically, this means that at least one diameter ofB(c,r)is enclosed inB∗, as shown in Fig. 1. In order to assess under which circumstances this property is satisfied, first consider the following formulation of the MEB problem as a convex optimization problem overRd×R:(4)minc,γγ(5)s.t.‖pj-c‖2⩽γforj=1,…,n.Hereγdenotes the squared ball radius. The above minimization problem can be reformulated as the following dual problem [6], which is a concave maximization problem overRnfor n input points:(6)maxu∑j=1nuj‖pj‖2-∑j=1nujpj2(7)s.t.∑j=1nuj=1,(8)uj⩾0forj=1,…,n.A feasible solution u to this dual problem, whose feasible region is the unit(n-1)-simplex, translates directly into an approximationB(c,r)ofB∗, such thatr⩽r∗, by(9)c=∑j=1nujpj,(10)r2=∑j=1nuj‖pj‖2-∑j=1nujpj2.Consequently, the optimal ball, corresponding to the solutionu∗to the dual problem, has its center given by a convex combination of the input points and its squared radius given by the objective function onu∗. In particular,uj∗>0holds only whenpjis a support point, which means thatc∗lies in the convex hull of the support set. As the following theorem shows, Eqs. (9) and (10) translate any feasible solution u into a viable ball.Theorem 1Any ballB(c,r)corresponding to a feasible solution u to the dual problem is viable.Without loss of generality, assumec∗=0. Then the following holds:(11)r2+‖c-c∗‖2=r2+‖c‖2(12)=∑j=1nuj‖pj‖2-∑j=1nujpj2+‖c‖2(13)=∑j=1nuj‖pj‖2-‖c‖2+‖c‖2(14)=∑j=1nuj‖pj‖2(15)⩽r∗2.The second and third equalities follow from Eqs. (10) and (9), respectively, and the inequality follows from Eq. (7) and the fact that‖pj‖=‖pj-c∗‖⩽r∗holds forj=1,…,n.□Theorem 1 immediately implies that viability holds in any algorithm based on the dual formulation. Notable examples are Yıldırım’s two algorithms [16] and the simple algorithm by Bădoiu and Clarkson [2]. In the latter, only a center point is maintained, but a viable ball is obtained by computing a radius using the objective function on the multipliers defining the center point. The above algorithms can be regarded as adaptations of the classical Frank–Wolfe algorithm, which solves the dual problem approximately [5]. In fact, we have recently discovered that the algorithm by Larsson and Källberg [12] also belongs to the same category, which is perhaps surprising since this algorithm was derived using a geometrical approach. See A for a more detailed discussion of this result. In addition, other algorithms have been presented which generate viable balls. An example is the exact solver by Gärtner [8], which uses Welzl’s algorithm [15] as a subroutine to compute the exact MEB of subsets of P. Each such MEB clearly has a corresponding feasible solution to the dual problem and is therefore viable. All these algorithms perform similar repeated farthest-point queries, and may therefore benefit from pruning.We start by restating a well-known fact that is commonly invoked when reasoning about the MEB problem [1,4,2,3,12].Lemma 1Every hemisphere ofB∗contains at least one support point, i.e., a point in P at distancer∗fromc∗.This lemma, which follows from the fact thatc∗lies in the convex hull of the support points (as discussed in Section 2.1), equips us to derive the following boundΔ‾for viable balls.Theorem 2Suppose the ballB(c,r)is viable, and letR=maxpj∈P‖pj-c‖. The distance‖c-c∗‖is then bounded above by(16)Δ‾=R2-r22.LetΔ=‖c-c∗‖. Eq. (3) gives the following lower bound onr∗:(17)r∗2⩾r2+Δ2.In the casec≠c∗, Lemma 1 gives thatB∗has at least one support point on the hemisphere facing away from c, that is, the hemisphere on the positive side of the plane with normalc∗-cthat passes throughc∗. Let s be such a support point. Then the law of cosines gives(18)‖s-c‖2⩾r∗2+Δ2,as illustrated by the trianglecc∗sin Fig. 2, which must have an obtuse or right angle atc∗. Furthermore, s must lie within distance R from c, which limits the zone of the considered hemisphere ofB∗where s can be located, shown in Fig. 2 as highlighted arcs. Thus,(19)r∗2+Δ2⩽‖s-c‖2⩽R2,which gives the following upper bound onr∗:(20)r∗2⩽R2-Δ2.Note that this upper bound trivially holds also whenc=c∗. The lower bound onr∗cannot be larger than the upper bound:(21)r2+Δ2⩽R2-Δ2,which gives(22)Δ⩽R2-r22.□Clearly, this bound is tighter than that of Eq. (1). As shown by the following generalization of Theorem 2, however, it is also possible to deriveΔ‾for a viable ball using the maximum distance from an arbitrary point, as opposed to the center of the ball, to a point in P.Theorem 3Suppose the ballB(c,r)is viable, and letR′=maxpj∈P‖pj-c′‖andδ=‖c′-c‖for some pointc′∈Rd. The distance‖c-c∗‖is then bounded above by(23)Δ‾=δ+2(R′2-r2)-δ22.Note that in the special casec′=c, we getδ=0andR′=maxpj∈P‖pj-c‖, and Eq. (23) simplifies to Eq. (16). Below follows the proof of the general case.ProofLetΔ=‖c-c∗‖andΔ′=‖c′-c∗‖. As in the proof of Theorem 2, we use bounds onr∗to determine the valid range ofΔ. As before, the viability ofB(c,r)gives the lower bound(24)r∗2⩾r2+Δ2.In analogy with Eq. (20),R′andΔ′give the upper bound(25)r∗2⩽R′2-Δ′2.By the reverse triangle inequality, we have(26)Δ′⩾δ-Δ.This allows Eq. (25) to be rewritten into a bound that is expressed in terms ofΔinstead ofΔ′:(27)r∗2⩽R′2-(δ-Δ)2.The lower bound must be less than or equal to the upper bound:(28)r2+Δ2⩽R′2-(δ-Δ)2.Thus,(29)Δ⩽δ+2(R′2-r2)-δ22,that is,Δis bounded above by the larger root of the quadratic polynomial.To verify that the discriminant above is never negative, we first rearrange the terms in Eqs. (24) and (25) to get(30)r2⩽r∗2-Δ2,(31)R′2⩾r∗2+Δ′2.Furthermore, the triangle inequality gives(32)δ⩽Δ+Δ′.Thus, the discriminant satisfies(33)2(R′2-r2)-δ2⩾2(Δ2+Δ′2)-(Δ+Δ′)2(34)=2(Δ2+Δ′2)-(Δ2+Δ′2+2ΔΔ′)(35)=Δ2+Δ′2-2ΔΔ′(36)=(Δ-Δ′)2(37)⩾0.□Although the improvedΔ‾from Theorems 2 and 3 can be readily plugged into the original condition in Eq. (2), the following theorem shows how the viability property can also improve the pruning condition itself.Theorem 4Suppose the ballB(c,r)is viable and thatΔ‾is an upper bound on‖c-c∗‖. A point p then lies in the interior ofB∗if(38)‖p-c‖<r2+Δ‾2-Δ‾.LetΔ=‖c-c∗‖, and assume that Eq. (38) is satisfied. Then the following holds:(39)‖p-c∗‖⩽‖p-c‖+Δ(40)<r2+Δ2(41)⩽r∗.The first step simply states the triangle inequality. SinceΔ⩽Δ‾, andr2+x2-xdecreases monotonically for non-negative x, the assumption that Eq. (38) holds implies(42)‖p-c‖<r2+Δ2-Δ,which gives the second inequality. The third inequality follows from the viability ofB(c,r).□Note that the pruning condition can be regarded as defining an open ball centered at c whose radius is given by the right-hand side of the condition. The presented bounds ensure that this ball is fully enclosed in the interior ofB∗. Clearly, the pruning ball defined by Eq. (38) is larger than the one defined by Eq. (2).This section concretizes the theory of the previous section and discusses how it can be applied in practice. The pseudocode in Fig. 3shows how to realize the pruning procedure in a generalized(1+∊)-approximation MEB algorithm called ApxMEB1. The algorithm is described so as to capture the general structure of all the approximation algorithms discussed in Section 2. Also the algorithm by Gärtner follows a very similar structure, except it does not terminate the loop until the exact MEB is obtained. Since these algorithms differ in how they initialize and update the current ball, these steps are represented by abstract subroutines in the code (Lines 1 and 7, respectively). However, these subroutines are assumed to maintain viability of the ballB(c,r). Several of the algorithms also construct a core-set which is used in the update step, but this is left out of the pseudocode for clarity. The subroutine FarthestPoint, invoked on Lines 2 and 8, is assumed to return both the pointq=argmaxpj∈P‖pj-c‖and the distanceR=‖q-c‖. Finally note that sincer⩽r∗⩽Rholds throughout the algorithm, the termination criterion of the loop ensures that the radius R of the returned ball satisfiesR⩽(1+∊)r∗.In the pruning procedure, which is carried out on Lines 4–6, a pruning radiusρis first computed using Theorems 2 and 4, and then a pruning pass is invoked to eliminate all points in P that fall inside the pruning ball.In Fig. 4, an alternative way to apply pruning to the same abstract algorithm is shown. Instead of pruning in a separate pass, this version of the algorithm integrates the pruning procedure with the search for the farthest point by computing a pruning ball that shares its center point with the updated ball. Since this requires computingΔ‾before having updated R, this version of the algorithm uses Theorem 3 instead of Theorem 2 for this step, utilizing the preceding values of c and R (Lines 4, 6, and 7). In the subroutine FarthPtPrune invoked on Line 9, each squared distance‖pj-c‖2is first used to evaluate the pruning condition, then, if this did not causepjto be pruned, the same squared distance is used to determine ifpjis the farthest point so far. The advantage of this approach, and the motivation behind Theorem 3, is reduced overhead: whereas ApxMEB1 processes the remaining points twice in every iteration (Lines 6 and 8), ApxMEB2 needs to process them only once per iteration (Line 9).Note that while the pseudocode in Fig. 4 assumes that no pruning is possible before the loop, this might indeed be possible if the ball initialization of a particular algorithm provides the parameters necessary to give a pruning radiusρ>0. For example, in all the algorithms considered in Section 4, the ball is initialized asc=(q1+q2)/2andr=‖q2-q1‖/2, whereq1andq2are input points found by two consecutive farthest-point queries. If applying the method in Fig. 4 on these algorithms, pruning can be integrated into the farthest-point search before the loop by usingR=‖q2-q1‖andδ=‖q2-q1‖/2to computeρ. Consequently, this pruning method gets a head start over the method of Fig. 3.As an illustrative example, Fig. 5visualizes the pruning procedure at different times during the computation of a1.001-approximate bounding sphere using ApxMEB1, in this case embodied by the second algorithm by Yıldırım [16]. Each image shows the pruning ball and what remains of the input in iteration i (only those triangles with at least one remaining vertex are shown). These images reveal that a large part of the input set is eliminated at an early stage. This is also suggested by Fig. 6, where the values ofr,R, andρduring the same computation are plotted. Hereρgrows beyond90%of r early on and then continues to grow throughout the remaining computations, albeit not monotonically. Notice how the slow but steady increase in r makes the dependency ofρon R particularly visible in this figure: every change in R is mirrored by a (more pronounced) change inρ.Although the second pruning method above avoids the cost of an additional pruning pass in every iteration, it is possible to reduce the overhead of the first method as well if one is willing to accept an increase in memory consumption. Observe that the point-distance computations in the left-hand side of the pruning condition will always be repetitions of the point-distance computations already performed during the most recent search for the farthest point, because the same center point is used. The search can easily be modified to store each squared distance‖pj-c‖2into position j of a distance cache, which is then used in the subsequent pruning pass. This way each pruning pass takes onlyO(n)time as opposed toO(dn)time. The distance cache, of course, incurs anO(n)memory overhead, which the second pruning method avoids. This might be prohibitive for very large n, particularly as the relative overhead increases with smaller d.

@&#CONCLUSIONS@&#
Our experimental results demonstrated the usefulness of the pruning procedure, and our tighter bounds led to significantly improved effects compared to previous bounds [1]. The pruning method most successful in reducing computation times was KL1 when coupled with distance caching.As shown by the pseudocode in Figs. 3 and 4, the pruning procedure can be incorporated into several recent state-of-the-art algorithms with minimal effort. Furthermore, it is possible to modify several of these algorithms to handle input objects other than points, such as balls and ellipsoids. Given fully functioning such adaptations, all that is required to employ the presented pruning techniques is a fast method to determine when such an input object is fully contained in the pruning ball.In our experiments, pruning gave performance improvements of up to two orders of magnitude in Yıldırım’s algorithms. In the case of FastApxBall, however, more moderate speedups of around2×were obtained, and in some cases, the execution time was even increased by up to40%. However, this could be considered a mild overhead in relation to the potential performance gains.We used∊=10-3in all presented tests, which appears to be a common choice [10,16,1]. Note, however, that using values of∊<10-3to compute finer approximations would likely result in even larger speedups, since this would increase the iteration counts of the algorithms. Conversely, values of∊>10-3would of course reduce the number of iterations.It is clear that the success of the presented pruning techniques is limited by the dimension. For points uniformly distributed in a ball, we hit the wall atd=200. Moreover, extrapolating the figures from Tables 1–3 supports the conclusion that eventually, the techniques will fall short also on the other point distributions as d increases further.There are a number of interesting directions for further studies. One such direction would be to consider tailored solutions to tackle the more challenging cases, such as inputs uniformly distributed in a ball. We also believe that similar practical pruning techniques can be developed for computation of other enclosing shapes in high dimensions, such as(1+∊)-approximate ellipsoids [11].Another interesting direction would be to investigate solutions for on-line prediction of pruning effectiveness to avoid wasted pruning efforts. For example, it would be useful if the number of points to be pruned in the next iteration could be predicted with sufficient accuracy. Furthermore, if the number of remaining iterations of the algorithm itself could be estimated, the pruning procedure could be disabled when further point reductions would have limited opportunities to pay off.Finally, there are opportunities for parallelization of these algorithms. In particular, the search for the farthest point lends itself for both thread-level and data-level parallelization features of modern CPUs [12] and GPUs [9]. The proposed pruning techniques should be fully compatible with such solutions, and it would be interesting to evaluate the combined impact.