@&#MAIN-TITLE@&#
Visual grids for managing data completeness in clinical research datasets

@&#HIGHLIGHTS@&#
We develop two visualizations for managing missing data in clinical trial data.Binary completeness grid shows a visual depiction of a patient data record.Gradient completeness grid shows a visual depiction of an entire dataset.Three datasets were visualized and studied to improve clinical trial management.

@&#KEYPHRASES@&#
Missing data,Data visualization,Clinical trial data,Data completeness,

@&#ABSTRACT@&#
Missing data arise in clinical research datasets for reasons ranging from incomplete electronic health records to incorrect trial data collection. This has an adverse effect on analysis performed with the data, but it can also affect the management of a clinical trial itself. We propose two graphical visualization schemes to aid in managing the completeness of a clinical research dataset: the binary completeness grid (BCG) for single patient observation, and the gradient completeness grid (GCG) for an entire dataset. We use these tools to manage three clinical trials. Two are ongoing observational trials, while the other is a cohort study that is complete. The completeness grids revealed unexpected patterns in our data and enabled us to identify records that should have been purged and identify missing follow-up data from sets of observations thought to be complete. Binary and gradient completeness grids provide a rapid, convenient way to visualize missing data in clinical datasets.

@&#INTRODUCTION@&#
The initial product of a clinical trial is the dataset collected during its execution. Missing data in such a dataset complicates the statistical analysis process. In fact, missing data may preclude certain types of analysis because there is not enough data to analyze; in the extreme case, the entire dataset may not be useful. However, discovering that data are incomplete after the data collection period is often not sufficient, particularly in the case of a study in which follow ups at various time points are collected. Therefore, managing missing data during the execution of a clinical trial is a critical aspect of the clinical trial process.Data for medical research studies, such as retrospective chart reviews or prospective randomized clinical trials (RCT), are typically collected using paper or electronic case report forms (CRFs) [1]. An investigator, or a designee, examines the electronic health record (EHR) or paper chart for data required by the study, and then transfers these data to a CRF by hand or by retyping it into an electronic system. In the case of a study in which follow ups are required, the process usually involves the investigator contacting the subject by telephone or through a follow up clinical visit to collect further data on their condition and progress.Study data collection policies and practices and differences in electronic health records systems (EHRs) have the most effect on the quality of data collected for a clinical trial. First, if data are missing or not captured in the EHR, it will also be missing in the CRF and subsequently the study database. Moreover, data required for a clinical trial is often stored as unstructured free text in an EHR; data collection personnel are required to locate these data, which are often located in clinician’s notes, which may necessitate a clinician’s experience to successfully parse. Furthermore, in a facility still using paper charts, potential legibility problems may interfere with transcribing data to the CRF that, in turn, leads to missing data in the study database.Outside of the EHR, typographical errors and misunderstanding of data entry policy by those entering data may also lead to data quality issues. Several preventative measures can be used to address these problems including: documenting the data collection process (e.g. manual of operations), training data collection staff on proper data collection procedures, performing double-data entry, fostering frequent communication between investigators and data collectors regarding potential data issues, and using monitoring reports to track the state of the dataset [2].The data quality literature frequently categorizes missing data as “completeness”, a characteristic of the Contextual Information Quality dimension [3–6]. Completeness is specifically defined as “the extent to which data are not missing” [7]. The statistical analysis literature is replete with warnings about the effects of missing data on research results, as well as approaches to mitigate the problems it causes [8–11]. The two most significant issues with missing data are that they can introduce bias into the statistical results and they reduce the power of the study. Common techniques for handling missing data include complete case analysis, analysis of observations available, using data imputation to “fill-in” data with statistical methods, and intent to treat [12].Missing data can be more effectively recognized using data visualization techniques. Data visualization is a means of representing data in a graphical form allowing the recognition of details and patterns that might otherwise remain obscure. Ways to represent data while taking into account missing or incomplete datasets is an active area of research in data visualization and data quality. Many visualization methods for missing data depend on knowing the uncertainty associated with data [13]. Different types of visual attributes, like color hue and texture can be used in a dataset visualization to draw attention to uncertainty values [14,15]. However, in the case of many sparse datasets, missing data is the norm, and estimation and imputation should be avoided in favor of alerting the viewer as to which data fields are empty [16].There are also many statistical software packages for visualizing and interacting with missing data. Missing Are Now Equally Treated (MANET), XGobi, GGobi and Mondrian, were all designed to allow users to explore and visualize data and missing data [17–19]. The R programming language contains many packages and functions for data analysis and visualization, and the VIM package has been designed specifically to facilitate the analysis and interactive exploration of missing data [20].One drawback of previous missing data visualizations is their interdependence with complex data analysis. Many stakeholders in the clinical data collection process are interested in quickly and easily viewing the completeness of a dataset, but do not have the background to engage in complex analysis. To facilitate this we have developed two visualization methods, the binary completeness grid and the gradient completeness grid, tools for clinical trial data managers to monitor and evaluate missing data and effectively visualize large two-dimensional datasets like those collected for clinical trials.Fig. 1shows a typical workflow for using completeness grids. As data is collected for a study and entered into a database, the entire dataset or subsets can be exported in an easy-to-process format such as a comma-separated value (csv) file. Completeness grids can then be generated providing a visual representation of the completeness of the dataset. Using this information, data monitors can investigate unexpected patterns and resolve potential problems with the database or data collection methodology.Completeness grids account for the visualization needs of individual users in a clinical study. Investigators are generally involved in many studies, and want a high level representation of the state of data collected for a study. They should be able to use the visualizations to stay current on study progress, and show issues involving data completeness at organizational meetings. Trial statisticians and data managers need a more specific view of data from the visualization. They need to be able to use the visualization to recognize interesting patterns, but also be able to quickly investigate those patterns. These types of activities utilize visual perceptual processes that support cognitive tasks such as identification and discrimination [21]. Both types of users need to be able to discriminate differences between groups of data entries within a dataset and identify the reasons for the differences.Two completeness grid algorithms facilitate the necessary visualizations: the binary completeness grid (BCG) and the gradient completeness grid (GCG). The binary completeness grid is based on a simple missing data schematic often referred to as a shadow matrix [22,23]. This matrix has a one-to-one correspondence with the dataset, and a value of 1 in a position where data are present in that dataset and a value of 0 where data are not present. Visually, this is better represented with graphics, where color or shading differences indicate the presence of data. We call the visual representation for each data point a data symbol, which is a square arrangement of pixels representing a single entry from the dataset. These representations are adequate for small datasets but a grid of data symbols with thousands of rows or columns would be very large, and require constant scrolling to explore. Comparing two datasets this way would also be difficult due to the constant changes in visual context. Keeping information representations in as compact a space as possible is an important feature of effective visualizations; perception is more efficient and enables the eye to process relevant information with fewer movements [24]. We achieve this in the binary completeness grid by wrapping the data symbols for a single observation (i.e., an entire row in a traditional data matrix), much the way a word processor wraps text to conform to a certain margin width. The most visually compact form for this grid is a square, and so the ideal row length r of a binary completeness grid will always be roughly the square root of the number of variables v in the dataset rounded up. Thusr=v. Fig. 2shows a typical BCG for an observation in a dataset.We also developed the gradient completeness grid. The GCG is based on the shading matrix a precursor to the cluster heat map, a well-known data visualization technique for displaying hierarchical cluster data [25]. Shading matrices consist of square or rectangular grids of rows and columns with the individual cells of a grid colored to represent some quantity or value. One of the earliest known shading matrices was designed by Loua in 1873 to summarize social statistics for the city of Paris [26]. Like the binary completeness grid, the gradient completeness grid is wrapped to conform to a square shape based on the number of variables in the dataset. Each square data symbol represents a variable in the dataset and is shaded according to the percentage of completeness across all observations. While any color could be chosen for 100% completeness, dark colors provide the best gradients and a dark blue was used for the generated data sets. A completeness of 0% is represented by white just as white represents missing data in the BCGs. The color for 100% completeness or full completeness will have color F with color components ofFred,Fgreen,andFblue. The gradient color G(v) for a graphical element representing a variable v will be a percentage of the full color F corresponding to the completeness of the variable v soG(v)red=completeness(v)∗Fred,G(v)green=completeness(v)∗FgreenandG(v)blue=completeness(v)∗Fblue.Fig. 3illustrates how a dataset is used to generate binary and gradient completeness grids. On the left a dataset with 325 rows and 692 columns is rendered without any wrapping and with a data symbol that is one pixel in size. On the right each binary completeness grid contributes to the gradient completeness grid that depicts the entire dataset in a more convenient size.Our visualization tool was used to manage three pre-publication clinical research datasets. The first dataset (DS1) was from our Rapid Empiric Treatment with Oseltamivir Study (NCT01248715), a multi-center prospective, randomized, unblinded clinical study of hospitalized patients with acute lower respiratory tract infections admitted to one of nine hospitals in Jefferson County, Kentucky. The study, which was conducted from December 1 to May 1 for influenza seasons 2010–2011, 2011–2012, and 2012–2013 was designed to compare outcomes between hospitalized adult patients with influenza community-acquired pneumonia who were administered Oseltamivir within 8–12h of hospital admission versus patients who were administered standard of care empiric treatment. This study collected demographic, microbiologic workup, medication administration and outcome data on over 1200 subjects. Patients provided written consent for the inclusion of their data in the study.The second dataset (DS2) was from data collected for a retrospective, observational study of female HIV patients followed at the University of Louisville 550 Clinic, formerly WINGS, from January 2000 to December 2012. This clinic has been serving HIV positive patients since 1999 and currently has over 1600 patients enrolled in care with nearly 350 of these being female. This study collected demographic, disease progression, comorbidity, and outcomes data.The last dataset (DS3) was from the Bone and Joint International Organization osteomyelitis database, which has been used for numerous retrospective observational studies on bone and joint infections. The dataset contains almost 300 subjects with various diabetic foot, prosthetic joint, or septic joint infections and includes demographics, microbiological workup, thirty day, six month, and one-year outcomes. The number of observations, variables and total number of entries for each these datasets are shown in Table 1.For each dataset, a binary completeness grid was generated for every observation, as well as a gradient completeness grid that summarized the entire dataset. Since gradient completeness grids operate on any set of 2-dimensional data, subsets of a dataset can be visualized to ascertain the differences in completeness levels for different stratifications such as institution or personnel. The DS1 dataset was partitioned into 9 subsets for each of the participating centers in the study and GCGs were generated for each center. The DS2 dataset was divided into subsets according to the staff responsible for entering the record into the database, and GCGs were generated for each of the 15 subsets. The algorithm for generating the completeness grids was implemented in the Python programming language. A total of 1913 completeness grids were generated from the three datasets. Table 2summarizes these by dataset and grid type.A web application was also developed that enables a user to view a completeness grid for an input dataset in csv file format [27]. Several JavaScript graphical libraries are available for creating rich data visualizations. We chose the D3.js library to power our visualizations since it provided the best balance between performance and flexibility. It is also optimized to support querying large data collections [28]. The sample dataset provided with the web application is from a prospective observational study of 40 hospitalized patients with Community-Acquired Pneumonia as seen in Fig. 4. The application shows a binary completeness grid for individual observations on the left and a gradient completeness grid for the entire dataset on the right.The web application provides additional information to the viewer due to the interactive nature of JavaScript supported in modern web browsers. For the binary completeness grid, the user can hover over an individual visual element representing a data entry and a tooltip will appear. The tooltip contains the name of the field, and the content of the data in the field, if it is present. Hovering over a visual element in the GCG shows the name of the field and the percentage of missing data for that field across all observations. This interactivity provides a valuable way for users to orient themselves in relation to a dataset. Upon seeing a potential pattern the user is able to investigate the fields corresponding to missing data without changing their visual context, which a traditional matrix layout would not allow.

@&#CONCLUSIONS@&#
Assessing data completeness in clinical research datasets is an important aspect of data quality management. Binary and gradient completeness grids provide a convenient visualization of data completeness for both individual records and aggregated data for either a subset or entire dataset. The algorithms are simple to implement and flexible, allowing for complex visualizations. Furthermore, they can be used as stand-alone tools or integrated into a larger data quality framework as necessary.