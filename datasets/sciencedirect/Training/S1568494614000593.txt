@&#MAIN-TITLE@&#
A hybrid evolutionary dynamic neural network for stock market trend analysis and prediction using unscented Kalman filter

@&#HIGHLIGHTS@&#
A dynamic neural network is used to predict stock market prices and trends.A new hybrid DE and unscented Kalman filter is used to update the weights of the DNN.The parameters of the UKF when optimized by DE produce a robust and accurate forecast.Comparison with several neural network based forecasts shows the simplicity and accuracy of the simple DEUKF trained DNN.

@&#KEYPHRASES@&#
IIR filter NN,UKF,DE,Stock indices,Trend prediction,

@&#ABSTRACT@&#
Stock market prediction is of great interest to stock traders and investors due to high profit in trading the stocks. A successful stock buying/selling generally occurs near price trend turning point. Thus the prediction of stock market indices and its analysis are important to ascertain whether the next day's closing price would increase or decrease. This paper, therefore, presents a simple IIR filter based dynamic neural network (DNN) and an innovative optimized adaptive unscented Kalman filter for forecasting stock price indices of four different Indian stocks, namely the Bombay stock exchange (BSE), the IBM stock market, RIL stock market, and Oracle stock market. The weights of the dynamic neural information system are adjusted by four different learning strategies that include gradient calculation, unscented Kalman filter (UKF), differential evolution (DE), and a hybrid technique (DEUKF) by alternately executing the DE and UKF for a few generations. To improve the performance of both the UKF and DE algorithms, adaptation of certain parameters in both these algorithms has been presented in this paper. After predicting the stock price indices one day to one week ahead time horizon, the stock market trend has been analyzed using several important technical indicators like the moving average (MA), stochastic oscillators like K and D parameters, WMS%R (William indicator), etc. Extensive computer simulations are carried out with the four learning strategies for prediction of stock indices and the up or down trends of the indices. From the results it is observed that significant accuracy is achieved using the hybrid DEUKF algorithm in comparison to others that include only DE, UKF, and gradient descent technique in chronological order. Comparisons with some of the widely used neural networks (NNs) are also presented in the paper.

@&#INTRODUCTION@&#
Price index forecasting is one of the most important problems in financial markets that make the investors worry about their investments. In the past decades, the stock market prediction has played a vital role for the investment brokers and individual investors and researchers are on the constant lookout for a reliable method for predicting stock market trends. The nonlinear and nonstationary characteristics of the stock market make it difficult and challenging for forecasting stock indices in a reliable manner. Traditionally the basic methodology for financial time series has been a series of statistical methods like ARMA, ARIMA, GARCH, etc., which have been used over the years but suffer from the assumption of linear variation of the stock prices during a certain period of time. In general, the statistical models do not yield a process that can be easily automated, as it requires adaptation and changes at every stage requiring certain regularities and stationary nature in the target data. Accordingly, traditional statistical methods cannot be used to track the complexity and nonstationary nature of the stock markets.In their search for methods for addressing these shortcomings, many market analysts, traders, and researchers have investigated the various intelligent system techniques for analyzing the stock markets and making trading decisions. The various tools of computational intelligence include artificial NN (ANN) [1–4], fuzzy logic systems [5–8], support vector machines (SVM) [9,10]. The different types of ANNs used for stock market analysis include radial basis function NN (RBFNN) [11], recurrent NN (RNN) [12,13], multilayer perceptron (MLP) [14–16], generalized regression NN (GRNN) [17], random vector functional link net (FLANN) [18,19], local linear wavelet NN (LLWNN) [20,21], wavelet NN (WNN) [22], etc. These neural models, however, do not perform so successfully due to the dimensionality, volatility, and noise of the stock price data. Fuzzy logic theory is preferred by several investigators due to its superior capabilities to handle uncertainties in the data and a synergic combination of fuzzy logic systems and NN have evolved for stock market prediction. These models include fuzzy NN (FNN) [23], adaptive network fuzzy information system (ANFIS) [24], wavelet fuzzy NN (WLFNN) [25], etc. To improve the accuracy of prediction and automate stock market forecasting and trend analysis other inputs like the technical indicators and qualitative factors due to political effects, etc. are used along with the price data. Even with these indicators, the NN models suffer from computational complexity because they need as many as 800 neurons in the input layer and 600 neurons in the hidden layers for predicting reasonably well stock price indices over a certain time frame. Further search techniques like the genetic algorithm (GA), particle swarm optimization (PSO) [26], bacterial foraging optimization (BFO), DE [27], etc. are used to obtain optimal parameters of the network that includes the network weights, the number of neurons in the hidden layer, etc. to improve the accuracy of the prediction.DNN models provide an excellent means for forecasting and prediction of nonstationary time series. In particular, a neural architecture, known as locally recurrent NN (LRNN) [28], is preferred to the traditional MLP because the time varying nature of a stock time series can be better represented using LRNN. The use of LRNN has demonstrated superioriority in comparison to other NN approaches for temporal sequence processing and a number of successful applications exists in the areas of electrical load forecasting, non linear system prediction, and economic time series prediction.Therefore, in this paper, a simple feed forward DNN comprising one or more layers of dynamic neurons is presented for forecasting stock price indices and profits from one day ahead to 30 days in advance. The DNN includes one or more IIR (infinite impulse filter) filters in the forward path providing feedback connections between outputs and inputs. This allows signal flow in both forward and backward directions, giving the network a dynamic memory useful to mimic dynamic systems. Further for globally recurrent networks, the stability is hard to be proved, but locally recurrent networks allow easy checking of the stability by the examination of poles of their internal filters However, training these networks presents difficulties due to the feedback connections. The possible algorithms that can be used for training RNNs are: (1) the back propagation through time (BPTT) [29] where the recurrent network is unfolded into a multilayer feed forward network that increases by one at each time step; (2) the real time recurrent learning (RTRL) which is based on the supervised learning method where the feedback from the output of a unit is replaced by the output of the true system in subsequent computations. However, the RTRL algorithm suffers from low convergence speed and multiple local minima.Considering the chaotic nature of the time series prediction and noise problem, extended Kalman filter (EKF) [30] and unscented Kalman filter (UKF) [31,32] have been proposed for fast convergence and good tracking performance. A key property of the EKF is that it is the minimum-variance estimator for the state of a nonlinear dynamical system and when applied to IIR filter NN training, it produces faster convergence than gradient-based algorithms; also, it overcomes the vanishing-gradient problem. The EKF algorithm provides first-order approximations to optimal nonlinear estimation through the linearization of the nonlinear system, which might produce large errors in the true posterior mean and covariance of the transformed (Gaussian) random variable, leading to suboptimal performance, and sometimes, filter divergence. The UKF first proposed by Julier and Uhlmann [31] and further extended by Wan and van der Merwe [32], is an alternative to the EKF algorithm and provides third-order approximation of process and measurement errors for Gaussian distributions. Consequently, the UKF does not require the computation of Jacobeans, needed for linearizing the process and measurement equations and thus produces better estimates of the weights of the recurrent NN.UKF is based on the unscented transformation (UT) theory, and has a greater ability to cope with non-linearities in one-step ahead prediction. Instead of linearizing a nonlinear system, a statistical distribution of the state is propagated through the non-linear system that provides better estimates of the actual state and the posterior covariance matrices. Thus, instead of using the standard gradient or back propagation algorithm for adjusting the weights and some parameters of the DNN, an UKF is used in this paper to provide fast convergence and better accuracy with respect to chaotic variations in the inputs. However, its accuracy significantly reduces, if Signal-to-Noise Ratio is low and the noise co variances and some of the parameters used in unscented transformation are not chosen correctly. Thus to obtain an optimal forecasting performance, it is proposed in this paper to use DE technique for optimizing the objective function obtained from the UKF algorithm alternately during the beginning of the learning cycle. Once initialized using DE, the optimized UKF algorithm updates the weights of the DNN and exhibits superior convergence in tracking a chaotic time series like the stock market indices. Unlike traditional EAs (Evolutionary algorithms), DE employs the difference of the parameter vectors to explore the objective function landscape. The advantage of DE is that there is a possibility of finding the global minimum of a multimodal function irrespective of the values of its initial parameters. Additionally, DE takes very few control parameters (typically the population size Np, the scale factor F, and the crossover rate Cr), which makes it easy to use.It is well known that the technical indicators play a great role in predicting the dynamic behavior of the stock market indices, and hence they can be used to predict the next day's trend whether the stock indices are going up or down. Turning point of this trend can be used as a buy or sell decision for the traders and investors in the market. The technical indicators chosen for this purpose are the 25 and 65 days moving average, relative strength index, trading volume, stochastic oscillators, and William indicator, etc. With the help of a few simple rules, the up and down trend of the stock indices can be ascertained, which can be used for automatic buying or selling decision.This paper is organized as follows: in Section 2, the architecture of the DNN is proposed. The state space model of the UKF is described in Section 3 along with the learning strategy of the DNN and this section also deals with the DE technique and various steps in implementing the algorithm are described. In Section 4 the performance of the prediction algorithms are analyzed using various technical indicators, while the stock market trend analysis is presented in Section 5. Section 6 provides the original datasets and an overview of input selection for the various stock market data and the stock value prediction results and the errors for different data sets with and without the use of DE. Trend prediction using certain technical indicators and a pertinent rule base are also given in this section. Finally, conclusion is given in Section 7. Further some of the well known NNs like the FLANN, LLWNN, and a slightly modified RBFNN are used for comparison.The IIR-MLP NN architecture considered in this paper is similar to the multi-layered feed-forward one comprising locally recurrent dynamic neurons. The proposed model exhibits dynamic properties by introducing an infinite impulse response (IIR) filter into the neuron structure. Fig. 1shows the structure and the topology of the dynamic neuron and multilayered dynamic neural network (DNN). Consequently incorporating an IIR filter between input weights and the output, the neuron can reproduce its own past inputs using two signals: the lagged input pattern input and filter output. For the jth dynamic neuron, the weighted sum of the inputs (lagged stock indices and technical indicators) to it is computed at the kth instant as(1)sj,k=∑i=1pwjixiwhere p is the number of inputs (x1,x2,x3,…,xp), and the weight vector is given byWj=[wj1,wj2,…,wjp]T. The output of the filter is obtained from a linear difference equation of the form(2)zj,k+1=∑i=0nbjisj,k−i+∑i=1majizj,k−iwhere the feedback and feedforward filter weights are given by(3)A=[a1,a2,a3,…,am]T,B=[b1,b2,…,bn]TThe final output is obtained from the jth dynamic neuron with tanh activation function as(4)yj,k+1=tanh(θ1zj,k+1+σ1)whereθ1, andσ1are tunable parameters.In a multilayered formulation the neurons transmit signals from one layer to the next one, till it reaches the output neuron without any feedback loop. The IIR filter, however, provides the dynamic feedback thus giving recurrent nature to the network. Considering a simple DNN architecture comprising an input layer, a hidden layer and an output layer with one neuron, the final output of the IIR-MLP is given by(5)yk+1=tanh∑j=1qθjzj,k+1+σwhere q is the total number of dynamic neurons in the hidden layer, and θ1, θ2,…,θq, σ are tunable parameters.The parameters to be predicted can be arranged as a state vector given by(6)x=[W,A,B,θ,σ]Twhere the weight matrix W, and the vectorθare obtained as(7)W=w11w12…w1nw21w21…w2n⋮⋮⋮⋮wn1wn2…wnnθ=θ1,θ2,…,θqTAlthough a simple architecture has been used for this model, more numbers of dynamic neurons can be added to match the nonlinear nature of the stock prices. The next section describes four algorithms that include the RTRL algorithm based on gradient descent, the UKF, and the DE, and a hybrid of DE and UKF (DEUKF) for learning the parameters of the above model in a recursive manner.The most common gradient based algorithms used for on-line training of recurrent IIR filter based NNs are back-propagation (BP) algorithms and real-time recurrent learning (RTRL) [29]. In both these algorithms gradient descent based on first order derivatives is used to update the synaptic weights of the network. However, both these approaches, exhibit slow convergence rates because of the small learning rates required, and most often they become trapped to local minima. On the other hand, the UKF has been used [31,32] in the weight update recursions, where better conditioned training is accomplished in comparison to BP and the RTRL method. The following learning algorithms are used in this paper for training the DNN for the prediction of stock market indices.For the IIR filter based NN the formula for updating the current filter coefficients is more complex due to the dependencies of the recursive IIR formulation on the past filter coefficients. Normally the limits of AR part of the IIR filter are set within −1 and +1 for stable operation [36,37]. For simplicity an Adaline and IIR filter neural system is considered for updating the weights in the following way:In this approach the mean squared error is minimized to yield the updating formula for the moving average and filter coefficients as(8)ai,k=ai,k−1+μeksec2h(θ1z1,k+σ1)∂z1,k∂ai,k(9)bi,k=bi,k−1+μeksec2h(θ1z1,k+σ1)∂z1,k∂bi,k(10)wi,k=wi,k−1+μeksec2h(θ1z1,k+σ1)∂z1,k∂wi,kand the errorekis obtained asek=ykd−yk,whereykdis the desired stock price on the forecasting day andykis the corresponding output from the DNN.Further the gradients in the above equations are updated recursively as(11)∂z1,k∂ai,k=zk−i+∑j=1maj,i∂zk−j∂ai,k,i=1,2,…,m(12)∂z1,k∂bi,k=sk−i+∑j=1nbj,i∂zk−j∂bi,k,i=1,2,…,n(13)∂z1,k∂wi,k=xk−i+∑j=1pwj,i∂zk−j∂wi,k,i=1,2,…,pThis procedure does not yield in a closed form expression for the gradient but it does produce a recursive relation by which the gradients can be generated using Eq. (13). In the beginning of the learning cycle all the gradients are made zero or very small. It is well known that the use of this recursion is to make the problem nonlinear in terms of the coefficient parameters. Also, the current filter parameters now depend directly upon previous time-varying filter coefficients, often leading to mean square error surfaces that are not quadratic in nature and thus lead to local minima.Any EKF-based training algorithm is a second order, recursive procedure that is particularly effective in training both recurrent and feed forward NN architectures for a wide variety of problems. It typically requires fewer training cycles than does its RTRL counterpart and tends to yield superior input–output mapping. Recently, Julier and Uhlmann [12–14] have introduced a new filter founded on the intuition that it is easier to approximate a Gaussian distribution than it is to approximate arbitrary nonlinear functions. They named this filter as unscented Kalman filter (UKF). The UKF leads to more accurate results than EKF and in particular it generates much better estimates of the covariance of the state. Therefore, a more robust algorithm using UKF is presented in the next section.The UKF overcomes some of the limitations of the well known extended Kalman filter (EKF) as it does not use linearization avoiding the loss of higher order terms in the Taylor series expansion for the Jacobean, resulting in the improvement of the performance of the predictor. Unlike the EKF, the estimates obtained by UKF are not biased and the computational advantages accrue due to absence of matrix inversion at each iterative step. The UKF belongs to the family of sigma-point filters and an unscented transformation (UT) is used to evaluate the statistics of a random variable undergoing nonlinear transformation. Further the UT relies on the principle that a Gaussian distribution is relatively easy to approximate than a nonlinear function and provides higher order information about the distribution of the system state, even using a small number of sigma points. Instead of linearizing the Jacobean matrices, a deterministic sampling approach is used in the UKF to obtain mean and covariance estimates with a minimal set of 2×n+1, sigma points based on a square-root decomposition of the prior covariances, and n represents the order of the system input state. Propagating these sigma points through the nonlinearity, a weighted mean and covariance is found. Like the EKF, the UKF uses a recursive algorithm that uses the system model, measurements, and known statistics of the noise mixed with the signal. The UKF was originally designed to estimate the states of a dynamic system and for nonlinear control applications.For training the RNN, the following discrete-time equations are formulated as(14)xk+1=xk+w(k)yk=g(xk)+v(k)and,(15)xk=[W,A,B,θ,σ],g(xk)=tanh∑j=1qθjzj,k+1+σwhere w(k) andv(k)are process noise and measurement noise, andx(k) andy(k) stand for parameter vector and observation vector, respectively. Given a state vector at x at the step k−1, sigma points are computed and stored in the columns of n×(2n+1) sigma point matrixXk−1, where n=dimension of the state vector. The unscented transform (UT) is used to calculate a set of vectors known as sigma points. Thus for estimating the stock market prediction model with 10 state variables, n becomes equal to 10 and thusXk−1is a 10×21 matrix. The UKF algorithm is summarized in the following steps:Step 1: Unscented transformation and sigma points calculationGiven a state vector x at time step k−1, sigma points are computed and stored in the columns of L×(2L+1) sigma point matrix X. The sigma points are computed as(16)X0,k−1=xˆk−1,i=0Xi,k−1=xˆk−1+(n+λ)Pk−1i,i=1,2,...,nXi+n,k−1=xˆk−1−(n+λ)Pk−1i,i=n+1,...,2n(n+λ)Pk−1iis the ith column of the matrix square root [22,23] of(n+λ)Pk−1.The parameter λ is used to control the covariance matrix, and is given by(17)λ=α2(n+κ)−nThe scaling parameters λ and κ determine the spread of the sigma points aroundxˆ. Also the factor κ has the effect of reducing the higher order errors of the mean and covariance approximations. The constant α is usually set to −4≤α≤1, andκ=3−n, or zero.The weight vector of the IIR recurrent NN and the UKF state, process and noise covariance matrices are initialized as(18)xˆ(0)=E[x]P(0)=E[(x−xˆ(0))(x−xˆ(0))T=ε−1I,R(0)=r,Q(0)=qIwhere ɛ, r, and q are chosen appropriately and I is a unit matrix of appropriate dimension.After computing the sigma points the time update of state estimates are given by(19)xˆk−=∑i=02nWimXi,k|k−1where the weightsWi(m)are defined by(20)W0(m)=λn+λ,Wi(m)=λ2(n+λ),Wi+L(m)=12(n+λ),i=1,....,nThe a priori error covariance is given by(21)P¯k=∑i=02nWi(c)[Xi,k|k−1−xˆk−][Xi,k|k−1−xˆk−]T+Qk(22)W0(c)=λ(n+λ)+(1−α2+β),Wic=12(n+λ)+(1−α2+β),(23)Wi+L(c)=12(n+λ),i=1,....,nThe sigma points are propagated through the nonlinear stock market output prediction model to estimate the mean and covariance of y. The estimated output becomes equal to(24)Yi,k|k−1=gk(Xi,k|k−1)(25)yˆk−=∑i−02LWi(m)Yi,k|k−1Step 2: Measurement updateThe unscented transformation yields the covariance matrix of the vector Y as(26)Sk=∑i=02LWi(c)[Yi,k|k−1−yˆk−][Yi,k|k−1−yˆk−]T+RkFurther the cross covariance matrix between X and Y is obtained as(27)Uk=∑i=02LWi(c)[Xi,k|k−1−xˆk−][Yi,k|k−1−yˆk−]TThen the Kalman gainKk, the state estimatexˆk, and the error covariancePkare obtained as(28)Kk=UkSk−1The observation error is obtained as(29)ek=(yk−yˆk−)and the updated parameter vector is obtained as(30)xˆk=xˆk−+KkekTo improve the tracking performance of the filter, the error covariance matrixPkis obtained as(31)Pk=[P¯k]−1−KkSkKkTforSk>εS¯k(32)andPk=[(P¯k)−1−γ−2I]−1−KkSkKkT,otherwise.(33)S¯k=ekekT,k=0ξS¯k,k−1+ekekTξ+1,k>0andξis a forgetting factor and is chosen as 0.98.whereS¯=E[eekTk]is the real covariance matrix of the innovation (measurement error). The parameter ɛ>0(ɛ=0.9) is used to tune the threshold while the filter is implemented. Further another tuning parameter γ is introduced to minimize the estimation of errors by the filter due to sudden fluctuations of the data. It is seen that the robustness of the filter increases by reducing γ, but the mean square error also increases. Thus a suitable value of γ needs to be selected for making the filter robust and reduce the mean square error simultaneously. The design objective for the robustness is to guarantee the norm of the transfer function between the external disturbances (modeling errors and system noises) and the estimation error to be less than a prescribed attenuation level γ.(34)||I⋅(xˆk−xˆk−)||2||Sk||2+||P¯k||2<γNormally a value of γ=1.98 is used to ensure good tracking performance.Step 3: Adaptation procedureThe performance of the UKF algorithm depends on a proper choice of the parameters α, β, and the initial values of the covariances Q and R. The covariance matrices Q and R are the elements that determine the performance and stability of the unscented filter and hence in this paper these two parameters are adaptively tuned. This is required as the uncertainties in the system model and fluctuations in the measured values will affect the UKF gain matrix. For this purpose a time-averaged innovation covariance is computed as(35)Gk=1N1∑i=k−N+1kυkυkTwhere N1 is the estimation window size andυk=(yk−yˆk−), and the filter covariance is obtained as(36)Gˆk=∑i=02LWi(c)[Yi,k|k−1−yˆk−][Yi,k|k−1−yˆk−]T+RkA cost function to minimizeGˆkis formulated as(37)Vk=tr[(Gk−Gˆk)2]The cost function can be minimized by choosing the value of q of the Q matrix as(38)qk=qk−1−μ(Vk−Vk−1)(qk−1−qk−2))In a similar way the measurement covariance matrix is updated as(39)Rk=ρRk−1+Kkyk−∑i−02LWi(m)Yi,k|k−12ρ is a forgetting factor and is initially taken as 0.9.The forgetting factor can be further tuned as(40)ρk=ρmin+(ρmax−ρmin)exp(−|(rk/r0)|)where the estimation error covariance is obtained iteratively as,(41)rk=ρk−1r+k−1(yk−yˆk−)2and the value of r0 is obtained by summing the measurement error over a few samples (10 samples for example),ρmin,ρmaxare suitably chosen parameters having valuesρmin=0.85,ρmax=0.99.Further to improve the estimation performance of the adaptive UKF, a stochastic optimization technique like the DE is used to obtain the initial values of α, β, Q, andR, instead of trial and error approach.The optimization is aimed at minimizing the MSE or maximizing the fitness value. Normally a sample size of 10 is used in this paper to obtain the suitable parameters α, β, q, r for starting the UKF program. Further Q and R matrices are tuned recursively after the filter weights are initialized. This new formulation is known as DEUKF method. Another alternative learning procedure is the use of DE for the entire training data and the IIR NN weights. The next section describes the DE algorithm.The DE algorithm is a simple yet powerful population-based stochastic search technique that provides effective global optimization in the continuous search domain. Many successful applications of DE include diverse areas like power systems, pattern recognition, communications, etc. Although there exists several trial vectors generation paradigms in DE, a few may be appropriate in yielding the optimal solutions for a particular problem [38,39]. Also the optimization performance of DE is governed by three crucial control parameters like the population size, scaling factor, and crossover rate. Further the DE algorithm aims at evolving a population of Np D – dimensional parameter vectors, known as individuals, which encode the candidate solutions toward the global optimum. The pseudo code for DE implementation is given below:Input: population size Np, no. of variables to be optimized (Dimension D), initial scaling and mutation parametersF1,F2, and Cr, upper and lower limits of variables, total number of generations G. Strategy candidate pool: “DE/rand/2/bin”.(1)Generation:The parameters of the ith vector for the generation g are given by Eq. (42).(42)XX(i)g=xx(i,1)g,xx(i,2)g,…,xx(i,j)g,…,xx(i,D)gA random initialization is done for the jth parameter of the ith vector at the 1st generation as(43)xxi,j1=xxjmin+rand(i,j)×xxjmax−xxjminWhile stopping criterion is not satisfieddoMutationfori=1to Np(44)F1=0.5+0.45rand(0,1)F2=0.3+0.5rand(0,1)Generate the mutant vector from the target vector using Eq. (42)V(i)g=v(i,1)g,v(i,2)g,…,v(i,j)g,…,v(i,D)g(45)V(i)g=XX(μ)g+F1×XX(δ)g−XX(γ)g+F2×XX(ζ)g−XX(η)gThe indices,μ,δ,γ,ξ,η∈[1,Np]end forCrossoverfori=1to Npjrand=rndinit(1,D)forj=1to DTo increase the diversity of population crossover is used for each target vector, and the crossover rate is adapted as(46)Cr=rand3,ifrand4≤0.1Cr,otherwisewhere rand 3 and rand 4 are random numbers between [0,1].Generate a trial vectoru(i,j)gusing Eq. (47)U(i)g=u(i,1)g,u(i,2)g,…,u(i,j)g,…,u(i,D)g(47)u(i,j)g=v(i,j)gif(rand(i,j)≤Cr)orj=jrandx(i,j)gotherwiseend forend forSelection:fori=1to NpEvaluate the objective function of the trial vectorU(i)g=u(i,1)g,u(i,2)g,…,u(i,j)G,…,u(i,D)gThe objective function is obtained from the UKF algorithm asf(U(i)g)=(1/MM)∑k=1MMek2, MM=iteration number used for evaluation from Eq. (29).and the fitness of the ith vector(48)Fi=11+f(U(i)g)(49)XX(i)g+1=U(i)giff(U(i)g)≤f(XX(i)g)XX(i)giff(U(i)g)>f(XX(i)g)end forg=g+1end While

@&#CONCLUSIONS@&#
