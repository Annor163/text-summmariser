@&#MAIN-TITLE@&#
Model-based approach for high-dimensional non-Gaussian visual data clustering and feature weighting

@&#HIGHLIGHTS@&#
We introduce the Multidimensional asymmetric generalized Gaussian Mixture (AGGM).We propose two novel inference frameworks for unsupervised non-Gaussian feature selection.We have used these frameworks for action and facial expression recognition.

@&#KEYPHRASES@&#
Unsupervised learning,Feature selection,Clustering,Asymmetric generalized Gaussian,Action recognition,Facial expression recognition,

@&#ABSTRACT@&#
Clustering is the task of classifying patterns or observations into clusters or groups. Generally, clustering in high-dimensional feature spaces has a lot of complications such as: the unidentified or unknown data shape which is typically non-Gaussian and follows different distributions; the unknown number of clusters in the case of unsupervised learning; and the existence of noisy, redundant, or uninformative features which normally compromise modeling capabilities and speed. Therefore, high-dimensional data clustering has been a subject of extensive research in data mining, pattern recognition, image processing, computer vision, and other areas for several decades. However, most of existing researches tackle one or two problems at a time which is unrealistic because all problems are connected and should be tackled simultaneously. Thus, in this paper, we propose two novel inference frameworks for unsupervised non-Gaussian feature selection, in the context of finite asymmetric generalized Gaussian (AGG) mixture-based clustering. The choice of the AGG distribution is mainly due to its ability not only to approximate a large class of statistical distributions (e.g. impulsive, Laplacian, Gaussian and uniform distributions) but also to include the asymmetry. In addition, the two frameworks simultaneously perform model parameters estimation as well as model complexity (i.e., both model and feature selection) determination in the same step. This was done by incorporating a minimum message length (MML) penalty in the model learning step and by fading out the redundant densities in the mixture using the rival penalized EM (RPEM) algorithm, for first and second frameworks, respectively. Furthermore, for both algorithms, we tackle the problem of noisy and uninformative features by determining a set of relevant features for each data cluster. The efficiencies of the proposed algorithms are validated by applying them to real challenging problems namely action and facial expression recognition.

@&#INTRODUCTION@&#
Clustering, a common technique for statistical data analysis, is one of the most important tools to find a structure in a collection of unlabeled data. It is used to group items that seem to fall naturally together [1]. A cluster is therefore a collection of objects which are similar between them and are dissimilar to the objects belonging to other clusters. Lately, the problem of clustering data into homogeneous groups has been widely studied due to its significance in a variety of areas such as image processing, data mining, and computer vision. The vast majority of these approaches are distance-based algorithms which partition the data set into subsets according to some defined distance measure. However, these algorithms perform poorly on high-dimensional data because distances between points become more uniform [2]. Therefore, some subspace clustering approaches were introduced in order to address this concern. Though, these approaches make strong assumptions about the distributions of the data, and rely on heuristics to find clusters satisfying the assumptions [3]. Furthermore, in high-dimensional data sets, traditional clustering algorithms tend to break down both in terms of accuracy, as well as efficiency, so-called curse of dimensionality [4]. Here, we present an alternative algorithm based on finite mixture model designed especially for high-dimensional clustering that makes reasonable distributional assumptions and provides satisfying theoretical guarantees.Finite mixture models provide a natural representation of heterogeneity in a finite number of latent classes by modeling a statistical unknown distribution by a weighted sum of other distributions. Thus, lately, finite mixture models have been widely used to provide a formal framework for clustering. However, there are several challenges that should be handled when using finite mixture models such as the choice of the statistical distribution that represents each group or cluster and the learning algorithm used for mixture's parameters estimation, the selection of the model order (i.e. number of clusters), and the number of relevant features in high dimensional data. The main objective of this paper is to sum up all these challenging interrelated problems in one unified model. In the field of finite mixtures, the mixture of Gaussians is generally used for its simplicity, nonetheless, it has been observed that the Gaussian distribution is unsuitable for modeling data in complex real life applications [5] and especially in the case of computer vision problems [6]. Therefore, in order to overcome the rigidity of the Gaussian distribution, researchers were encouraged to exploit various distributions such as the generalized Gaussian distribution (GGD). The GGD is a continuous probability distribution capable of modeling data with different shapes [7–9]. The GGD includes the Laplacian and the Gaussian as special cases as well as the uniform distribution as limiting case [10] and has been employed in many challenging problems (see, for instance [11–14]). Even with the higher flexibility that GGD offers, it is still a symmetric distribution inappropriate to model non-symmetrical data. In this article, we suggest the use of the asymmetric generalized Gaussian distribution (AGGD) capable of modeling non-Gaussian asymmetrical data [15]. The AGGD has two variance parameters for left and right parts of the distribution, which allow it not only to approximate a large class of statistical distributions (e.g. impulsive, Laplacian, Gaussian and uniform distributions) but also to include the asymmetry.A standard method to learn finite mixture models is maximum likelihood which generally estimates the parameters through the expectation maximization (EM) framework. The EM algorithm enables us to update the mixture parameters with respect to a data set. In order to use the EM algorithm, an appropriate number of clusters should be predefined, otherwise, the EM algorithm will lead to a poor result. Thus, another important part of the mixture modeling problem concerns determining the number of consistent components which best describes the data. For this purpose, many approaches have been suggested for automatic selection of the number of clusters (see, for instance, [16–18]). Examples of selection criteria include Akaike information criterion (AIC) [19], minimum description length (MDL) [20] and Laplace empirical criterion (LEC) [21]. These criteria, however, may overestimate or underestimate the number of clusters. Furthermore, they are time-consuming and may lead to a sub-optimal solution because model selection and parameters estimations are determined in two separate steps. In our first algorithm, we propose a learning method that can integrate simultaneously parameter estimation and model selection by implementing MML criterion [22] into an EM algorithm [23].In [24], the authors used another clustering method, Rival Penalized Competitive Learning (RPCL), which could detect the real number of clusters with little prior knowledge. In RPCL, the assigned number of clusters must be bigger than the true number of clusters, then, the algorithm can automatically select the correct cluster number by gradually driving extra seed points far away from the input data set. However, its performance is sensitive to the selection of the de-learning rate, such that if it is not well selected, the RPCL may completely break down. In order to overcome this problem, the Rival Penalized EM (RPEM) algorithm was proposed for density mixture clustering [25]. The RPEM learns the model parameters by making the mixture components compete with each other at each time step; this can be done by not only updating the winning density component parameters to adapt to the input but also all rivals parameters are penalized with the strength proportional to the corresponding posterior density probabilities. Therefore, the RPEM is able to automatically select an appropriate number of densities by fading out the redundant densities from a density mixture which can save the computing time. To the best of our knowledge, this is the first attempt to employ the RPEM algorithm for AGGM model selection and parameter learning.Feature selection, the task of identifying relevant or discriminative features, represents an essential step in high-dimensional data clustering. This is actually an important step, since the main goal is not only the determination of clusters and their parameters but also to provide the most parsimonious model that can accurately describe the data [26,27]. Furthermore, feature selection can speed up learning and improve model accuracy and generalization. Therefore, the selection of relevant features in multidimensional data represents a major concern in several image processing, computer vision and pattern recognition applications such as object detection [28], handwriting separation [29], image retrieval, categorization and recognition [30]. However, the majority of research in mixture models assume that all features have the same weight and use a pre-processing step such as principal components analysis to transform the original features into a new dimension-reduced space. The main drawback of that approach is that the physical meaning of the original features is generally lost [31]. Moreover, the learning of the mixture parameters (i.e. both model selection and parameters estimation) is greatly affected by the quality of the features used as shown for instance in [32,33]) where the authors considered the Gaussian assumption by assuming diagonal covariance matrices for all clusters. The work of [34] proposed a new feature selection method using Markov Blanket capable of eliminating irrelevant and redundant features. They integrated feature selection with RPEM for Gaussian mixtures clustering. Therefore, data clustering, model learning and feature selection were performed in a single learning algorithm. In [35], a novel approach to combine clustering and feature selection by implementing a wrapper strategy for feature selection was presented. Thus, features were directly selected by optimizing the discriminative power of the used partitioning algorithm. The authors in [36] presented an algebraic approach to variable weighting by maximizing a score based on the spectral properties of the kernel matrix. This work presented a definition of relevancy based on spectral properties of the Laplacian of the features measurement matrix. Then, the feature selection process is based on a continuous ranking of the features defined by a least-squares optimization process. Zeng et al. [37] associated a weight to each feature or kernel and incorporated it into the built-in regularization of the Learning-Based Clustering (LLC) [38] algorithm to take into account the relevance of each feature or kernel for clustering. Their idea was that the resulting weighted regularization with an additional constraint on the weights is equivalent to a known sparse-promoting penalty. Hence, the weights of those irrelevant features or kernels can be shrunk toward zero. In this article, and following recent approaches (see, for instance [30,32]), we approach the feature selection problem in unsupervised learning by casting it as an estimation problem, thus avoiding any combinatorial search. For each feature, we associate a relevance weight which measures the degree of its dependence on class labels.The remainder of the paper is structured as follows: In Section 2 we review the feature selection model based on the asymmetric generalized Gaussian mixture. In Section 3, we propose our first learning algorithm by implementing MML criterion into an EM algorithm. In Section 4, we integrate the concept of feature saliency into the RPEM algorithm for the AGGM model. In Section 5, we assess the performance of our approach for action and facial expression recognition; while comparing them to other state-of-the-art methods. Our last section is devoted to the conclusion and some perspectives.Recently, finite mixture models have attracted a great deal of interest as a powerful framework for probabilistic inference and allow for reasoning with incomplete data. LetX={X→1,…,X→N}be a collection of N data points to be clustered, where eachX→i∈Rd;i∈{1,…,N}; is a vector of d dimensions. We aim to fit the data points inXby a mixture model with M clusters. In this article, we specify the conditional density functionp(X→|ξj)for each cluster j;j∈{1,…,M}; to follow an asymmetric generalized Gaussian distribution as follows:(1)p(X→i|ξj)=∏k=1dβjk[Γ(3/βjk)Γ(1/βjk)]1/2(σljk+σrjk)Γ(1/βjk){exp⁡[−A(βjk)(μjk−Xikσljk)βjk]ifXik<μjkexp⁡[−A(βjk)(Xik−μjkσrjk)βjk]ifXik≥μjkwhereA(βjk)=[Γ(3/βjk)Γ(1/βjk)]βjk/2;ξj={μ→j,σl→j,σr→j,β→j};μ→j=(μj1,…,μjd),σl→j=(σlj1,…,σljd), andσr→j=(σrj1,…,σrjd)are the mean, the left standard deviation, and the right standard deviation of the d-dimensional AGGD, respectively. The parameterβ→j=(βj1,…,βjd)controls the tails of the pdf and determines whether it is peaked or flat: the larger the value ofβ→j, the flatter the pdf, and the smallerβ→jis, the more peaked the pdf. The AGGD is chosen to be able to fit, in analytically simple and realistic way, symmetric or non-symmetric data by the combination of the left and right variances. Furthermore, the asymmetric Generalized Gaussian distribution (AGGD) can be reduced to the Generalized Gaussian distribution (GGD), the Laplace distribution (LD) as well as the Gaussian distribution (GD) as explained in Appendix A.Here, we assume that each vector follows a mixture of asymmetric generalized Gaussian distributions:(2)p(X→i|Θ)=∑j=1Mp(X→i|ξj)pjwherepjare the mixing proportions which must be positive and sum to one and the set of parameters of the mixture with M classes is defined byΘ=(μ→1,…,μ→M,β→1,…,β→M,σ→l1,…,σ→lM,σ→r1,…,σ→rM,p1,…,pM). Thus, the likelihood corresponding to this case is:(3)p(X|Θ)=∏i=1N∑j=1Mp(X→i|ξj)pjFor each variableX→i, letZibe an M dimensional vector known by the unobserved or missing vector that indicates to which componentX→ibelongs, such that:Zijwill be equal to 1 ifX→ibelongs to class j or 0, otherwise. The complete-data likelihood is then(4)p(X,Z|Θ)=∏i=1N∏j=1M(pjp(X→i|ξj))ZijNote that Eq. (2) assumes that the d features have equal importance and carry pertinent information which is not usually the case, since many of which can be irrelevant for the intended application [39–42]. We approach this problem by assuming that irrelevant features follow a background Gaussian distribution with parameterλ→={η→,δ→}for all classes, whereη→andδ→represent the mean and standard deviation of the Gaussian distribution, respectively. We adopt the feature relevancy approach suggested in [32], because it is suitable for unsupervised learning.The main idea is to consider thekthfeature as irrelevant if its distribution is independent of the class labels and can follow our common Gaussian densityp(Xk|λk). Then, the mixture density in Eq. (2) can be written as:(5)p(X→i|Θ,λ→,φ→)=∑j=1Mpj∏k=1dp(Xik|ξjk)φkp(Xik|λk)1−φkwhereφ→=(φ1,…,φd)is a set of binary parameters, such thatφk=1if thekthfeature is relevant andφk=0otherwise. Note that,φ→can be considered as missing variables. By assuming that allφksare mutually independent and independent of the hidden component label Z, then:(6)p(X→i,φ→)=p(X→i|φ→)p(φ→)=(∑j=1Mpj∏k=1dp(Xik|ξjk)φkp(Xik|λk)1−φk)(∏k=1dωkφk(1−ωk)(1−φk))=∑j=1Mpj∏k=1d(ωkp(Xik|ξjk))φk((1−ωk)p(Xik|λk))(1−φk)Marginalizing over φ we get [32]:(7)p(X→i|ΘM)=∑φp(X→i,φ→)=∑j=1Mpj∏k=1d∑φk=01(ωkp(Xik|ξjk))φk((1−ωk)p(Xik|λk))(1−φk)=∑j=1Mpj∏k=1d[ωkp(Xik|ξjk)+(1−ωk)p(Xik|λk)]whereΘM={Θ,ω→,λ→}is the complete set of parameters fully characterizing the mixture. We suppose that not all the features of an observation are important, through the weight relevancy of these features. That is, the weight is denoted asω=(ω1,…,ωd)with0≤ωk≤1, whereωkrepresents the probability that thekthfeature is relevant to all the clusters(ωk=p(φk=1)).In the following, we present our first unsupervised learning approach for simultaneous clustering and feature selection. In particular, we propose an approach to find the optimal number of model components using MML and to estimate the different parameters using EM.In this section, we develop the equations that learn the parameters of the AGGM while simultaneously consider the relevancy of features. To achieve this goal, we adopt common EM approach which generates a sequence of models with non-decreasing log-likelihood on the data. First, we suppose that the number of components M is known. The maximum likelihood method consists of getting the mixture parameters that maximize the log-likelihood function given by:(8)L(X,ΘM,Z,φ)=∑i,j,φp(Zi=j,φ|X→i){log⁡pj+∑k(φk(log⁡p(Xik|ξjk)+log⁡wk)+(1−φk)(log⁡p(Xik|λk)+log⁡(1−wk)))}=∑i,jp(Zi=j|X→i)log⁡pj+∑i,j∑k∑φk=01p(Zi=j,φk|X→i)(φk(log⁡p(Xik|ξjk)+log⁡wk)+(1−φk)(log⁡p(Xik|λk)+log⁡(1−wk)))Thus, following [32], the EM algorithm for parameter estimation can be given by:•Expectation step:(9)ζijk=ωkp(Xik|ξjk)+(1−ωk)p(Xik|λk)(10)h(j|X→i,ΘM)=pj∏k=1dζijk∑j=1Mpj∏k=1dζijkMaximization step:(11)pjnew=∑i=1Nh(j|X→i,ΘM)N(12)μjknew=μjkold−[(∂2L(X,ΘM,Z,φ)∂μjk2)−1(∂L(X,ΘM,Z,φ)∂μjk)](13)βjknew=βjkold−[(∂2L(X,ΘM,Z,φ)∂βjk2)−1(∂L(X,ΘM,Z,φ)∂βjk)](14)σljknew=σljkold−[(∂2L(X,ΘM,Z,φ)∂σljk2)−1(∂L(X,ΘM,Z,φ)∂σljk)](15)σrjknew=σrjkold−[(∂2L(X,ΘM,Z,φ)∂σrjk2)−1(∂L(X,ΘM,Z,φ)∂σrjk)](16)ηknew=∑i=1N[∑j=1Mqijkh(j|X→i,ΘM)]Xik∑i=1N∑j=1Mqijkh(j|X→i,ΘM)(17)δk2new=∑i=1N[∑j=1Mqijkh(j|X→i,ΘM)](Xik−ηk)2∑i=1N∑j=1Mqijkh(j|X→i,ΘM)(18)ωknew=∑i=1N∑j=1Mrijkh(j|X→i,ΘM)NUsually, the maximum likelihood estimate favors higher values for M which leads to overfitting. Thus, a model selection criterion is needed to estimate the number of components in a mixture model. The MML is a model selection technique based on information theory that is used to evaluate statistical models according to their ability to compress a message containing the data. Its basic idea is to find a model that minimizes the total length of a two-part message, where the first part encodes the model using only prior information about the model and no information about the data and the second part encodes only the data in a way that makes use of the model encoded in the first part. Therefore, in the case of MML, the optimal number of classes in the mixture is obtained by minimizing the following cost function [22,23]:(19)MessLens≈−log⁡p(ΘM)+c2(1+log⁡112)+12log⁡|I(ΘM)|−log⁡p(X|ΘM)wherep(θM),I(ΘM), andp(X|θM)denote the prior distribution, the Fisher information matrix, and the likelihood, respectively. The constantc=M+d+4dM+2drepresents the total number of parameters, and|.|denotes the determinant. Note that the information matrix of the model is very difficult to obtain analytically, therefore, we assume the independence of the different groups of parameters, which allows the factorization of bothp(ΘM)and|I(ΘM)|. Furthermore, we approximate the Fisher information|I(ΘM)|using the complete likelihood which assumes labeled observations. Additionally, since we have no knowledge about the parameters, we adopt the uninformative Jeffrey's prior for each group of parameters as prior distribution. From this, we obtain the following MML objective:(20)MessLens≈c2(1+log⁡112)+c2(log⁡N)+2M∑k=1dlog⁡ωk+2d∑j=1Mlog⁡pj+∑k=1dlog⁡(1−ωk)−log⁡p(X|θM)which we minimize under the constraints0<pj≤1,0<ωk≤1, and∑j=1Mpj=1in a manner similar to [32]. In order to use the MML approach the EM algorithm undergoes a minor modification in the calculation of the mixing proportionspjand the feature relevancyωk:(21)pjnew=max⁡(∑i=1Nh(j|X→i,ΘM)−2d,0)∑j=1Mmax⁡(∑i=1Nh(j|X→i,ΘM)−2d,0)(22)ωknew=max⁡(∑i=1N∑j=1Maijk−2M,0)Twhere(23)T=max⁡(∑i=1N∑j=1Maijk−2M,0)+max⁡(∑i=1N∑j=1Mbijk−1,0)(24)aijk=h(j|X→i,ΘM)ωkp(Xik|ξjk)ζijk(25)bijk=h(j|X→i,ΘM)(1−ωk)p(Xik|λk)ζijkThe following instructions summarize the main steps of the algorithm used for the AGGM parameters estimation and model selection1.InitializeΘM:•The feature relevancyω→=0.5→.The number of parametersM=Mmax.The AGGM parameters Θ are initialized using the Fuzzy C-means. Note that, we initialized both the left and right standard deviations with the standard deviation values obtained from the Fuzzy C-means and the shape parametersβ→=2.Perform the common Gaussian densityλ→parameters estimation to cover the whole data.Implement theEM+MMLapproachWhileM<Mmaxdo{(a)While not converged do {i.Perform E-step according to Eqs. (9) and (10).Perform M-step according to Eqs. (12) to (17), (21) and (22)Ifpj=0, Then thejthcomponent is eliminatedIfωk=0, Thenp(Xik|ξjk)is eliminatedIfωk=1, Thenp(Xik|λk)is eliminatedCalculate the associated message length using Eq. (20)Remove the component j with the smallestpjReturn the model parameters with the smallest message length.Recently, the RPEM algorithm [25] has been suggested to determine the model order automatically together with the estimation of the model parameters. This algorithm introduces unequal weights into the likelihood; thus the weighted likelihood in our case is written below:(26)Q(ΘM,X)=1N∑i=1NM(ΘM,X→i)with(27)M(ΘM,X→i)=∑j=1Mg(j|X→i,ΘM)ln⁡{pj∏k=1d[ωkp(Xik|ξjk)+(1−ωk)p(Xik|λk)]}−∑j=1Mg(j|X→i,ΘM)ln⁡h(j|X→i,ΘM)whereg(j|X→i,ΘM),j=[1,…,M], are designable weight functions, satisfying the two constraints below:(28)∑j=1Mg(j|X→i,ΘM)=1(29)∀jg(j|X→i,ΘM)=0ifh(j|X→i,ΘM)=0Thus, following [25], the weightg(j|X→i,ΘM)can be expressed by:(30)g(j|X→i,ΘM)=(1+ε)I(j|X→i,ΘM)−εh(j|X→i,ΘM)where ε is a small positive quantity which we took as 1. Also(31)I(j|X→i,ΘM)={1ifj=c0ifj≠candc=arg⁡max{1≤j≤M}h(j|X→i,ΘM). More details about RPEM can be found in [25]. In the following, we summarize the steps for the AGGM feature weighted RPEM (FW-RPEM) algorithm [43].1.InitializeΘM:•The feature relevancyω→=0.5→.The number of parametersM=Mmax.The AGGM parameters Θ are initialized using the Fuzzy C-means. Note that, we initialized both the left and right standard deviations with the standard deviation values obtained from the Fuzzy C-means and the shape parametersβ→=2.Perform the common Gaussian densityλ→parameters estimation:(32)ηk=1N∑i=1NXik(33)δk2=1N∑i=1N(Xik−ηk)2Repeated until convergence for eachX→i,i=1,…,N•Expectation steph(j|X→i,ΘM)=pj∏k=1dζijk∑j=1Mpj∏k=1dζijk(34)g(j|X→i,ΘM)={2−h(j|X→i,ΘM)ifj=c−h(j|X→i,ΘM)ifj≠cMaximization step(35)ϕjnew=ϕjold+γϕ∂M(ΘM,X→i)∂ϕj|ΘMold=ϕjold+γϕ(g(j|X→i,ΘM)−pjold)(36)μjknew=μjkold+γ∂M(ΘM,X→i)∂μjk|ΘMold=μjkold+γg(j|X→i,ΘM)ωkoldζijk∂p(Xik|ξjkold)∂μjk(37)βjknew=βjkold+γ∂M(ΘM,X→i)∂βjk|ΘMold=βjkold+γg(j|X→i,ΘM)ωkoldζijk∂p(Xik|ξjkold)∂βjk(38)σljknew=σljkold+γ∂M(ΘM,X→i)∂σljk|ΘMold=σljkold+γg(j|X→i,ΘM)ωkoldζijk∂p(Xik|ξjkold)∂σljk(39)σrjknew=σrjkold+γ∂M(ΘM,X→i)∂σrjk|ΘMold=σrjkold+γg(j|X→i,ΘM)ωkoldζijk∂p(Xik|ξjkold)∂σrjk(40)ωknew=ωkold+γω∂M(ΘM,X→i)∂ωk|ΘMold=ωkold+γω∑j=1Mg(j|X→i,ΘM)υijkζijkifωk>1thenωk=1ifωk<0thenωk=0where(41)pj=exp⁡(ϕj)∑j=1Mexp⁡(ϕj)for1≤j≤M(42)υijk=p(Xik|ξjk)−p(Xik|λk)and∂p(Xik|ξjkold)∂μjk,∂p(Xik|ξjkold)∂βjk,∂p(Xik|ξjkold)∂σljk, and∂p(Xik|ξjkold)∂σrjkare given in Appendix C.

@&#CONCLUSIONS@&#
