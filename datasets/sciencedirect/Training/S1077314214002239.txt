@&#MAIN-TITLE@&#
Multiple-concept feature generative models for multi-label image classification

@&#HIGHLIGHTS@&#
Novel generative model representing fusion of multiple concepts in feature generation.Learning algorithms based on likelihood and margin maximization without line search.Performance improvement in several multi-label image classification tasks.

@&#KEYPHRASES@&#
Probabilistic graphical models,Multi-label classification,Concept prediction,Image classification,

@&#ABSTRACT@&#
We consider the problem of multi-label classification where a feature vector may belong to one of more different classes or concepts at the same time. Many existing approaches are devoted for solving the difficult estimation task of uncovering the relationship between features and active concepts, solely from data without taking into account any sensible functional structure. In this paper, we propose a novel probabilistic generative model that aims to describe the core generative process of how multiple active concepts can contribute to feature generation. Within our model, each concept is associated with multiple representative base feature vectors, which shares the central idea of sparse feature modeling with the popular dictionary learning. However, by dealing with the weight coefficients as exclusive latent random variables encoding contribution levels, we effectively frame the coefficient learning task as probabilistic inference. We introduce two parameter learning algorithms for the proposed model: one based on standard maximum likelihood learning via the expectation–maximization algorithm, the other focusing on maximally separating the margin of the true concept configuration away from the class boundary. In the latter we suggest an efficient approximate optimization method where each iteration admits closed-form update with no line search. For several benchmark datasets mostly from the multi-label image classification, we demonstrate that our generative model with proposed estimators can often yield superior prediction performance to existing methods.

@&#INTRODUCTION@&#
The multi-label classification is recognized as a very important problem in machine learning and visual pattern recognition. This is because it naturally arises in virtually any application area where data feature can be generated from, or related to, multiple different sources. The examples are, among others, the multi-label image classification where image consists of multiple concepts or objects of different categories therein. In the blind source separation task in signal processing, we aim to find a set of basic components that comprise the observed signal. In the text classification task, the documents often come from multiple different topics, and one wants to find the most relevant topics that are related to the given document.Unlike the standard single-label classification, the multi-label classification deals with multiple class labels, and aims at finding all active classes that contribute to the observed data. A feature vector may belong to one of more classes at the same time, where the relevant classes can affect generation of the final feature vector in a rather complex way. The inherent difficulty lies in the exponential number of possible class configurations compared to the small number of training data available. Another notable aspect that makes the problem more challenging is that the class labels are often highly sparse, meaning that only a few of them are active for most samples.In tackling the multi-label classification problem, recent approaches attempt to capture the central relationship between the feature space and the class label space [1,2] with consideration of the label co-occurrences [3,4] or exclusiveness [5] constraints. For detailed summary of the related work, please refer to Section 4. However, many approaches aim at solving the difficult estimation task of uncovering the relationship between features and active concepts, solely from data without taking into account any sensible functional structure. For instance, the latest method in [5] deals with a non-parametric model to find a linear representation that can be shared for both features and class labels. Such a blind estimation without any sensible structure can be difficult and often prone to overfitting.In this paper, we propose a novel probabilistic generative model that aims to describe the core generative process of how multiple active concepts can contribute to feature generation. The main idea is to confine the model space by a sensible and plausible functional structure to compensate for relatively small number of sparse training data. To achieve this, we first consider to model each concept/class as a set of representative features which we call the base features. Once the active concepts are chosen, each active concept can select one of its base features, together with the significance level indicating its degree of contribution to the final feature vector. Then we form the observed feature as a significance-weighted sum of the selected base features, which can effectively mimic the complex process of multi-class fusion in the observed feature vector.In this model each concept is associated with multiple representative base feature vectors, while the final feature vector is generated as a significance-weight linear combination of the base features. Similar in nature, this shares the central idea of sparse feature modeling with the popular sparse coding and dictionary learning [6–11]. However, by dealing with the weight coefficients as exclusive latent random variables, the sparsity, in terms of the number of chosen base features, can be enforced at each concept level (much like the exclusive group sparseness [12]), which can be more appealing in certain contexts. Furthermore, the coefficient learning can be effectively framed as a task of probabilistic inference.In addition, we incorporate the label exclusive group constraints in parameter estimation, a sort of prior knowledge available from pre-inspection of data, whose usefulness was previously verified in several recent work in multi-label or multi-task learning [5,12]. From the observed data, it can be inferred that certain sets of concepts do not occur concurrently, e.g., book vs. truck. This can be learned from data using the recent Graph-Shift algorithm [13] which basically finds the modes of the graph defined with exclusive-labels edges.There are latent Dirichlet allocation (LDA) like topic models that are also generative models for documents. As topic corresponds to a concept, these models can be used for multi-label classification naturally, where class prediction is done by inference on latent (observed in data in the multi-label setup) topic variables. However, as they model topic-wise multinomial distributions that generate words comprising a document, hence, the features are restricted to be a kind of histogram or any other form based on frequencies only. In our model, the features are not limited to histograms, but can be arbitrary as we can learn them in a way like dictionary learning. Also the contribution level of each active concept/topic is unable to be controlled in a principled way like ours.We introduce two parameter learning algorithms for the proposed model: one based on standard maximum likelihood learning via the expectation–maximization algorithm, the other focusing on maximally separating the margin of the true concept configuration away from the class boundary. In the latter we suggest an efficient approximate optimization method where each iteration reduces to a convex quadratic problem that admits a closed-form update with no line search. In both learning algorithms, we resort to posterior distributions on the latent variables. Our model incorporates fully-connected pairwise random fields over the concept variables, and the probabilistic inference is performed approximately with the loopy belief propagation (LBP) algorithm [14,15].The paper is organized as follows: In the subsequent section, we formally describe the problem and introduce the notations used in the paper. The proposed model is introduced in Section 2 where the parameter learning algorithms are described in Section 3. After reviewing some related prior work in Section 4, we provide the empirical evaluations on some benchmark image classification datasets in Section 5.We denote byxa d-dim feature vector, which for image data is typically formed by combining one or more popular image features like SIFT [16,17] and color information. Each feature vectorxis associated with multiple class labels (there are K classes in total), indicated by a K-dim 0/1 vectorcwhose j-th conceptcj=1(0)implies the presence (absence) of the concept/class j inxforj=1,…,K.Throughout the paper we consider a fully supervised setup, where one is given the n dyadic training samplesD={(ci,xi)}i=1n. The ultimate goal of the multi-label classification is to estimate the prediction functionc=f(x)such that the predicted label vectorcfor a new test featurexis as close as possible to the true labels.We propose a probabilistic generative modelP(c,x)that represents the core generative process of how multiple active concepts impact on the generation of a feature vector. To make a sensible model, we will introduce a set of latent random variables that are useful for modeling the generative process. In the below the major concept-feature generation process is described while the detailed formal description follows in Section 2.1.Our main motivation is that each concept j (j=1,…,K) is characterized by a set of representative feature vectors which we call the base features. Instead of having a single representative feature vector, we consider multiple base features per concept since a concept usually exhibits several different observations. In image classification, for instance, each object/concept can bear multiple different sub-categories, also with diverse appearances depending on view points and/or pose variations.The base features of the concept j are denoted by1Although one can define different numbers of base features across concepts, we let all concepts have the same number of bases M, for simplicity.1Bj=[b1j,b2j,…,bMj]wherebij∈Rd. Our modeling intention is that when the j-th concept is active, one of its base features is selected and contributed to form the featurex. To indicate the index of the selected base feature, we hence introduce the hidden random variable,zj. The prior distributionP(zj)is modeled as the multinomial overzj∈{1,2,…,M}, where the parameters might be different across j, and can be learned from data. We often use the vector notationz=[z1,z2,…,zK]⊤to refer to the selection across all K concepts.The selected base features across the active concepts may have equal effects on the final feature vector. However, it is more reasonable to allow them to have different contributions to the feature generation. In image classification, for instance, certain objects may appear large in an image while others occupy only smaller portions. In such cases one can naturally observe larger impacts of the former features in the overall image descriptor than the latter. To account for it, we consider a[0,1]-scaled significance value to indicate the level of contribution that each concept makes.More formally, for each active concept j (cj=1), we decide its significance levelsj, a positive valued (bounded above by 1) latent random variable that encodes the amount of contribution the j-th concept makes in the final feature generation. To make the probabilistic inference computationally feasible, we confinesjto be discrete, taking one of the S predefined values,sj∈{a1,a2,…,aS}, typically we chooseai=i/S. For instance, forS=10different contribution levels, we getsj∈{0.1,0.2,…,1.0}. The conditional distributionP(sj|cj=1)can be modeled as the multinomial, whose parameters can be learned from data. We also need to deal with the case of inactive concepts, where a natural choice is to fix it as a delta function atsj=0, i.e.,P(sj=0|cj=0)=1, since inactive concepts give zero contribution to the feature generation. Similarly, we use the vector notations=[s1,s2,…,sK]⊤.Given the selected base feature vectors(bz11,bz22,…,bzKK)with the corresponding significance levels(s1,s2,…,sK), the feature vectorxis considered to be generated as a significance-weighted sum over the K concepts, namely,(1)x=∑j=1Ksjbzjj+v,wherev∼N(0,σ2I).Here we add Gaussian spherical noise in the emission process.Therefore, the feature vector is basically formed by a linear combination of the base features, which has a close connection to the popular dictionary learning. In our model, however, each concept is composed of a group of base features, among which only one base feature is exclusively selected. This essentially has an effect of enforcing exclusive group sparsity, similar in nature to the exclusive group lasso [12], quite effective in certain application contexts. Furthermore the coefficients for the selected base features are generated in a probabilistic fashion as random variables of significance levels that can be estimated from observation by inference in the graphical model.The top level of the generative model is the joint class prior modelP(c)which is represented by a fully-connected pairwise undirected graphical model, Markov random field (MRF). The overall generative process is summarized below, while the graphical model representation is shown in Fig. 1(Top).1.The concept variables{cj}j=1Kare generated fromP(c).For each conceptj,zj∈{1,…,M}is selected fromP(zj), determining the base feature selected for the concept,bzjj.For each conceptj,sj∈{a1,…,aS}is chosen fromP(sj|cj), which indicates the contribution of the concept j to the observed feature.Generate the final feature vectorxfrom (1).For ease of exposition, we combine two latent variablessjandzjinto a new variablehj=(sj,zj), and definefhjj=sjbzjjthat can be seen as a significance-weighted base feature. Thus the sole latent variables areh=[h1,…,hK]⊤. This merging process can be precisely specified as the following construction. We first single outhj=0to indicate the zero vector,f0j=0. For non-zero significance (i.e.,sj=alfor somel∈{1,…,S}) we definehj=(zj-1)·S+l(hencehj≠0), and associate it withfhjj=sjbzjj. In this way we havehj∈{0,1,2,…,H0}whereH0=MS. We also letH=H0+1, the cardinality ofhj.The induced base features for concept j, each associated with each value ofhj, can then be expressed as:(2)Fj≔[0,b1j⊗A,b2j⊗A,…,bMj⊗A],whereA=[a1,…,aS], and⊗is the Kronecker product. Note thatFjis(d×H)matrix. For example, ifsj∈A=12,1andBj=[b1j,b2j], then the induced base features for the concept j, corresponding tohj∈{0,1,2,3,4}, are:Fj=[f0j,f1j,f2j,f3j,f4j], wheref0j=0,f1j=12b1j,f2j=b1j,f3j=12b2j,f4j=b2j. Here,H0=4andH=5.Being equipped with the combined latent variableshand the significance-weighted base feature vectorsF≔[F1,…,FK](of dimension(d×KH)), the full joint model can be written as:P(c,h,x)=P(c)·P(h|c)·P(x|h). The overall graphical model is depicted in Fig. 1(Bottom). We describe each of the component models and the associated parameters in the below.We utilize the Markov random field (i.e., undirected graphical model) for modelingP(c). A fully connected pairwise MRF is considered, which can be written as:(3)P(c)∝∏j∈VΨjV(cj)·∏e=(j,l)∈EΨeE(cj,cl),where the underlying graphG=(V,E)is composed of the vertex setV={1,…,K}and the edge setE={(j,l)|j<l,j,l∈V}. Following the log-linear parameterization, we specifically define the node potential function as:(4)ΨjV(cj)=eθj·I(cj=1),with the node parameters{θj}j=1K. HereI(x)is the indicator returning 1(0) if x is true (false). The edge potential at edgee=(j,l)is defined to be:(5)ΨeE(cj,cl)=eθe11·I(cj=1,cl=1)+θe01·I(cj=0,cl=1)+θe10·I(cj=1,cl=0),with the edge parameters{θe=[θe11,θe01,θe10]⊤}e∈E. Concatenation of all the node and the edge parameters as a vector is denote byθwhile we letϕ(c)the indicators (statistics) stacked accordingly. In this way the class prior model can be written succinctly as:(6)P(c)∝expθ⊤ϕ(c).The base feature selection model is fully factorized over the concepts,P(h|c)=∏j=1KPj(hj|cj)where we model eachPj(hj|cj)as a multinomial distribution. Depending on whethercjis active or not, the base selection would conform to a different distribution. Forcj=1, it is natural to assign zero probability tohj=0(corresponding tof0j=0) while non-negative probabilities tohj=1,…,H0. We denote the multinomial parameters forPj(hj|cj=1)asηj=[0,η1j,…,ηH0j]⊤which lies in the probability simplex. Forcj=0, on the other hand, the entire mass has to go tohj=0(i.e.,Pj(hj=0|cj=0)=1), and its multinomial parameters are denoted asη∼j=[1,0,…,0]⊤. Unlikeηj’s that can be learned from data, no parameter learning is required forη∼j’s since they are always fixed.It is a simple Gaussian with additive mean of all features, essentially adding only the selected active base features:(7)P(x|h)=Nx;∑j=1Kfhjj,σ2I.This effectively accounts for all possible significancessj’s.With the proposed model, the ultimate task is to predict the concept labels of an unseen test featurex. We consider to optimize the joint configuration(c,h)for the modelP(c,h|x),(8)(c∗,h∗)=argmax(c,h)P(c,h|x)=argmax(c,h)P(c,h,x).The approximate probabilistic inference can be done by max-product version of the loopy belief propagation (LBP) performed on the joint MRF model proportional toP(c,h,x). The detailed description of constructing its node and edge potentials is provided in Appendix A.Now we have the full joint modelP(c,h,x)with the model parametersΘ={θ,η,F,σ2}whereη={ηj}j=1KandF=[F1,…,FK]. Note here thatFare not free independent variables; they are rather structured asFj=BjD0derived from the actual free parametersB≔[B1,…,BK]. From (2),D0is a(M×H)matrix shown in Fig. 2. Then we have the relationship:(9)F=BD,whereDis a(MK×HK)matrix, comprised of(K×K)blocks with each block having size(M×H).Dis filled with all zero blocks except the diagonal blocks all equal toD0. Our basic strategy in learning is as follows: we perform optimization with respect toF, then use the structure (9) to find the optimalB.Given the training dataD={(ci,xi)}i=1n, the standard maximum likelihood (ML) learning aims at maximizing the data log-likelihood, namelymaxΘ∑i=1nlogP(ci,xi). Since the objective can be decomposed aslogP(c,x)=logP(c)+logP(x|c)with each term involving separate parameters, we can perform two optimizations independently: (i) maximizing the class prior likelihood,maxθ∑ilogP(ci)and (ii) maximizing the class conditional,max{η,F,σ2}∑ilogP(xi|ci).The class prior likelihood maximization can be done by the standard MRF learning using gradient ascent where the objective value atθand its gradient can be expressed as (for each training sample i):(10)logP(ci)=θ⊤ϕ(ci)-log∑cexpθ⊤ϕ(c)(11)∂logP(ci)∂θ=ϕ(ci)-E[ϕ(c)]where the expectation in (11) is performed with respect toP(c)atθ. Note that due to the fully connected dependency structure overcj’s in our model, the computation of log-partition function in (10) and the inferenceP(cj,cl)required in (11) cannot be done exactly. Rather, we use the approximate inference methods, typically the loopy belief propagation (LBP) [14,15].While we follow the gradient ascent for optimizingθ, not all of its parameters are updated. We exploit the label exclusive group constraints, whose usefulness was previously verified in several recent work in multi-label learning [5,12]. The constraints are imposed in the learning stage. Specifically, by pre-inspecting data using the Graph-Shift algorithm [13], we obtain a set of groups, denoted byLEG={g1,…,gR}. The groupgiis composed of nodes (concepts) that should not co-occur in any samplec.Then for eachgi, ife=(j,l)∈gi, meaning that it is infeasible to observe bothcj=1andcl=1, then we imposeθe11=-∞. This has an effect of forcing any configurationcthat has bothcj=1andcl=1to be assignedP(c)=0, thus effectively discarding such configurations predicted in the decoding stage. So, in gradient ascent, we do not update the edge parametersθe11that belong to any group inLEG, and fix them as negative infinity from the outset.The class conditional likelihoodP(x|c)=∑hP(h|c)P(x|h)is associated with the parametersη,σ, andF(later, we will exploit the structure ofFin terms ofBas in (9)), while entailing marginalization over the latent variablesh. Following the conventional expectation maximization (EM) learning [18] for latent-variable models, we maximize the lower bound of the log-likelihood using the Jensen’s inequality. This amounts to alternating the following two steps until convergence:1.E-step: Inferqi(h)=P(h|ci,xi)fori=1,…,n, with respect to the current model parameters{η,F,σ}fixed.M-step: Maximize∑i=1nEqi(h)[logP(h,xi|ci)]with respect to{η,F,σ}usingqi(h)’s obtained from the E-step.We first deal with the inferenceqi(h)in the E-step, which can be basically done by forming an MRF overhfor each i. From the following derivation,(12)qi(h)∝P(h,xi|ci)=P(h|ci)P(xi|h)(13)=∏jPj(hj|cji)·Nxi;∑jfhjj,σ2I(14)∝exp∑jI(cji=1)·logηhjj+I(cji=0)·logη̃hjj-12σ2xi-∑jfhjj⊤xi-∑jfhjj,it is easy to see thatqi(h)can be factorized into pairwise (and unary) potential functions ofhjandhl. More specifically, the MRF corresponding toqi(h)can be defined as:(15)qi(h)∝exp∑jφj(hj)+∑e=(j,l)∈Eφe(hj,hl),with the complete edge set E, while the node (unary) and the edge (pairwise) log-potentials being:(16)φj(hj)=I(cji=1)·logηhjj+I(cji=0)·logη̃hjj+1σ2fhjj⊤xi-12σ2‖fhjj‖22(17)φe(hj,hl)=-1σ2fhjj⊤fhllFor this MRF, indeed all that is required in the M-step is the pairwise posterior distributionsqi(hj,hl)(and, of course, the unaryqi(hj)). This is because the quantitylogP(h,xi|ci)of which we take expectation is fully decomposed into unary and pairwise terms as shown in (12)–(14). To retrieve these pairwise posteriors, we perform approximate probabilistic inference (e.g., LBP) on each MRFqi(h).Having computedqi(hj,hl)andqi(hj)for alli=1,…,n, we solve the M-step optimization. Utilizing the decompositionlogP(h,xi|ci)=logP(h|ci)+logP(xi|h), where the former involved inηand the latterFandσ2, the optimization can be done independently for each term.In the first term, the expectation over q can be derived as:(18)Eqi(h)[logP(h|ci)]∼∑j=1KEqi(hj)[I(cji=1)·logηhjj]=∑j=1KI(cji=1)∑hjqi(hj)logηhjj,where∼indicates equivalence up to constant. The objective is the sum of (18) overi=1,…,n, which can be maximized independently with respect to eachηjforj=1,…,K. Setting up the gradient to be zero, and solving forηjwhile minding the simplex constraint yields the optimal estimate in a closed form,(19)ηrj=∑i=1nI(cji=1)·qi(hj=r)∑i=1nI(cji=1)·qi(hj≠0),r=1,…,H0.The expectation oflogP(xi|h)for each i can be derived as:(20)Eqi(h)[logP(xi|h)]∼-d2logσ2-12σ2Eqi(h)xi-∑jfhjj22,where the expectation in the second term becomes:(21)Eqi(h)xi-∑jfhjj22∼-2(xi)⊤∑j=1KEqi(hj)[fhjj]+∑j=1KEqi(hj)[‖fhjj‖22]+2∑j<lEqi(hj,hl)[(fhjj)⊤fhll]To have succinct matrix–vector notation, we introduce, for each i, the(HK×1)vectorqiand the(HK×HK)matrixQithat can be evaluated from the posterior distributionqi(h)as follows: Letqji=[qi(hj=0),…,qi(hj=H0)]forj=1,…,K, then we define:(22)qi≔[q1i,…,qKi]⊤.Also, we letQibe composed of(K×K)blocks where each block is of size(H×H). Denoting the(j,l)block ofQiby(Qi)j,lforj,l=1,…,K, the blocks are defined as: (i)(Qi)j,j≔diag(qji)wherediag(v)indicates the diagonal matrix formed from vectorv, (ii) forj<l, we define:(23)(Qi)j,l≔qi(hj=0,hl=0)⋯qi(hj=0,hl=H0)⋮⋱⋮qi(hj=H0,hl=0)⋯qi(hj=H0,hl=H0),and (iii)(Qi)l,j≔(Qi)j,l⊤forj<l.Then it is not difficult to see that the first term of (21) can be expressed as:(24)-2(xi)⊤∑j=1KEqi(hj)[fhjj]=-2(xi)⊤(Fqi),while the second term being:(25)∑j=1KEqi(hj)[‖fhjj‖22]+2∑j<lEqi(hj,hl)(fhjj)⊤fhll=Tr(FQiF⊤),whereTr(·)indicates the trace operator.Hence, minimization of (21) (i.e., maximization of (20)) summed over i, with respect toF, amounts to convex quadratic optimization, which admits the closed-form solution,(26)F=∑i=1nxi(qi)⊤∑i=1nQi†,whereA†is the Moore–Penrose pseudoinverse ofA. After (26), one may need to resetf0j=0forj=1,…,Kif necessary.Furthermore, one can take into account the dependency ofFon the actual base featuresB, i.e., (9). ReplacingFwithBDin (24) and (25) followed by solving the optimization with respect toBresults in the following M-step optimum:(27)B=∑i=1nxi(qi)⊤D⊤D∑i=1nQiD⊤†.Finally, the optimalσcan be obtained by maximizing (20) with the optimalFplugged in, which becomes:(28)σ2=1nd∑i=1nEqi(h)xi-∑jfhjj22The overall maximum likelihood learning algorithm is summarized in Algorithm 1.Algorithm 1Maximum Likelihood Learning.Input: Training data{(ci,xi)}i=1nand initialΘ={θ,η,F,σ2}.Output: Learned model parametersΘ.Learnθby gradient ascent MRFP(c)training with LEG constraints.Alternate the following EM steps until convergence:(E) For eachi=1,…,n, inferqi(h)=P(h|ci,xi)w.r.t. currentΘ.(M) Solvemax∑i=1nEqi(h)[logP(h,xi|ci)]via (19), (26)–(28).Return the finalΘ.In the ML learning the main focus is on overall fitting of the model to data, hence not so discriminative that can result in suboptimal classification performance. As recently studied in the machine learning literature, the estimator that aims to explicitly separate (and enlarge) the margin at the class boundaries can yield superior performance to the traditional ML learning [19–23]. While the large-margin learning has been studied more on standard single-label classification,2Models for multi-label classification have been considered in some recent work [24,25]. Unlike our approach, however, these methods either focus on models with no latent variables, or approximates learning problems to quadratic energy optimization for tractable inference.2we extend it to our multi-concept generative model for the multi-label classification problem.To formulate the maximum-margin learning, we consider the score functions(c,x), defined to be the log-likelihood under the joint class-feature distribution model, namely(29)s(c,x;Θ)≔logP(c,x;Θ).The score function can be seen as a negative energy function that assigns higher confidence value to the more likely class configurationcgivenx. We will often drop the dependency onΘin notation for simplicity.For a training pair(ci,xi), the margin between the true class vectorciand any other candidatec≠cican be defined as (or proportional to) their score difference, i.e.,s(ci,xi;Θ)--s(c,xi;Θ). Naturally, models that result in large positive margins for allc≠ciare preferred to others. However, as is also observed in [22], instead of imposing equal importance across allc≠ci, it is rather more reasonable to allow small margins for thosec’s quite similar toci, while strongly enforcing the candidates far more distant fromcito retain larger margins.This idea can be represented as the following optimization with the large margin constraints (with the non-negative slack variablesξi’s introduced):(30)minΘ∑i=1nξi+γ2‖Θ‖22s.t.s(ci,xi;Θ)-s(c,xi;Θ)⩾l(c,ci)-ξi,∀c≠ci,ξi⩾0,i=1,…,n,wherel(c,c′)is the distance (or discrepancy) betweencandc′, having small values ifcandc′are close to each other, and vice versa. A typical choice, also used throughout in our experiments, is the Hamming distance,l(c,c′)=∑j=1KI(cj≠cj′). Note that we also regularize the model smoothness in the objective (with the balancing constantγ⩾0), which indeed applies only to the undirected model parameters (i.e.,θinP(c)).In the constraints of (30), if we confine3It does not incur any loss of generality since one can always add a constant offset tol(·,·)without affecting the optimal solution.3l(c,c)=0, including the casec=cisimply reduces toξi⩾0. Hence, includingc=ciwhile removing the non-negativity constraints, does not alter the original problem. The rephrased constraints are as follows:(31)s(ci,xi;Θ)-s(c,xi;Θ)⩾l(c,ci)-ξi,∀c,i=1,…,n.Furthermore, by focusing on the most violating case, (31) can be equivalently written as:(32)s(ci,xi;Θ)-maxcs(c,xi;Θ)+l(c,ci)⩾-ξi,i=1,…,n.As (32) essentially forms the lower bound for eachξi, the original problem (30) can be equivalently expressed as:(33)minΘγ2‖Θ‖22+∑i=1nmaxcs(c,xi;Θ)+l(c,ci)-s(ci,xi;Θ).In optimizing (33), there are two computational issues: (i) evaluating the score functions(c,x)is demanding since it requires marginalization overhand (ii) maximization overcin the objective requires to consider the exponential number of configurations. In both, the main difficulty lies in dealing with the exponential numbers of cases even for evaluating the objective, not alone its gradient.To address these issues we propose a novel approximation strategy for faster optimization. The basic idea is to approximate the sum (or max) of the function values over a set of exponential cardinality by a single function value at its mode, while treating that mode point as constant. To retain accurate approximation, we confine the feasible model space to be a local neighborhood of the current modelΘsuch that the mode of the function does not vary much alongΘ. Once we find the optimal model near the current one, we repeat the procedure (evaluate the mode points and have functions approximated at the modes treating them constant) on and on until convergence. The technical details are described in the following.Since the score function on each training sample(ci,xi)in (33) is the soft-max function oflogP(ci,h,xi)overh, namelys(ci,xi)=log∑helogP(ci,h,xi), we first approximate it as the max function, that is,maxhlogP(ci,h,xi). Lettingh^i≔argmaxhP(ci,h,xi), we can further approximate the score as:(34)s(ci,xi)≈logP(ci,h^i,xi),withh^ifixed as constant during the optimization. Precisely, however, sinceh^iis dependent onΘ, this approximation is valid only for the model parameters in the vicinity of the current iterateΘ. Obtainingh^ican be done by running an approximate decoding algorithm (e.g., max-product version of the LBP) over the MRF modelP(h|ci,xi), exactly the same model used in (15)–(17) constructed at the current model.Next, to handle the max function in (33),maxcs(c,xi;Θ)+l(c,ci), again we first replace the score function (soft-max) therein by the max overh, namely(35)maxcmaxhlogP(c,h,xi)+l(c,ci)=maxc,hlogP(c,h,xi)+l(c,ci).We denote the optimal pair of the joint maximization (35) by(c∼i,h∼i). Finding the optimal pair can be done similarly by approximate decoding on the joint MRF model over(c,h), where its potential functions are built fromP(c,h,xi)as well as the Hamming distance term incorporated in the node potentials. The details can be found in Appendix A. The score is then approximated as:(36)maxcs(c,xi;Θ)+l(c,ci)≈logP(c∼i,h∼i,xi)+l(c∼i,ci),with(c∼i,h∼i)assumed fixed as constant. Treatingc∼ias constant (in the vicinity of the current model) renders the second terml(c∼i,ci)independent of the model parameters, hence we drop it in the optimization.Plugging the above approximates, we have the following optimization problem that approximates the original (33):(37)minΘγ2‖Θ‖22+∑i=1nlogP(c∼i,h∼i,xi)-logP(ci,h^i,xi)s.t.‖Θ-Θold‖2⩽∊,whereΘolddenotes the current model parameters for which all the mode points are evaluated, and∊is a small positive constant that defines the feasible space, the neighborhood around the current model. It is worth noting that our strategy of approximating the objective function in the vicinity of the current parameters is closely related to the trust-region methods [26]. Although we fixed the neighborhood size∊in our experiments, one can potentially extend it to be adjusted dynamically based on the quality of approximation, which is a well-known strategy in the trust region literature.Instead of solving the constrained optimization problem of (37), we migrate the vicinity constraint into the objective in a penalized form, namelyλ2‖Θ-Θold‖22, yielding an unconstrained problem. Hereλ(⩾0)is inversely related to∊, and can be properly chosen. Due to our log-linear parameterization ofP(c)as well as the linear Gaussian feature emission modelingP(x|h), the full joint log-likelihoods in (37) are quadratic functions ofF(orB), and linear inθ. Note, however, that they are rather more involved with respect toσandη. Hence we fixσandηas those from the ML estimates for simplicity, and do optimization overθandF(orB) only.We introduce some more notations below. The difference of features in the MRF modelP(c)is denoted as:(38)Δϕi≔ϕ(c∼i)-ϕ(ci).For a givenh, we defineq(h)as the(HK×1)vector similarly structured as (22) except that the posterior functionqi(·)replaced by the predicateI(·). Similarly,Q(h)is the(HK×HK)matrix of the form (23) withqi(·)replaced byI(·). Then we define the following differences:(39)Δqi≔q(h∼i)-q(h^i),ΔQi≔Q(h∼i)-Q(h^i).Using these new notations, it is easy to see that the optimization problem (i.e., the unconstrained version of (37) withσ,ηfixed) can be written as:(40)minθ,Fγ2‖θ‖22+λ2‖θ-θold‖22+λ2‖F-Fold‖22+∑i=1nθ⊤Δϕi-12σ2Tr(FΔQiF⊤)-2(xi)⊤FΔqi.It is also straightforward to rewrite (40) in terms ofBusing (9) in lieu ofF.The problem thus becomes convex quadratic optimization (forλlarge enough), and the closed-form optima can be obtained, without resorting to time-consuming line search, as:(41)θ=λγ+λθold-1γ+λ∑i=1nΔϕi(42)F=λFold-1σ2∑i=1nxi(Δqi)⊤λI-1σ2∑i=1nΔQi†(43)B=λBold-1σ2∑i=1nxi(Δqi)⊤D⊤λI-1σ2D∑i=1nΔQiD⊤†Hence, the optimization can be done very fast as one-shot learning once the mode pointsh^i,h∼i, andc∼iare decoded from probabilistic inference.Algorithm 2Max-Margin LearningInput: Training data{(ci,xi)}i=1nand initialΘfrom the MLE.Output: Learned model parametersΘ.Repeat until convergence:SetΘold=Θ.Decode for eachi=1,…,n,h^i=argmaxhP(ci,h,xi;Θold), and(c∼i,h∼i)=argmaxc,hlogP(c,h,xi;Θold)+l(c,ci).UpdateΘwithηandF(orB) using (41)–(43).Return the finalΘ.After we find the optimal model near the current one as above, we repeat the procedure (i.e., evaluate the mode points and approximate functions at the modes treating them constant) on and on, until convergence. The overall algorithm is summarized in Algorithm 2. The time complexity of the algorithm is identical to that of the EM learning, while in practice the MM learning is slower since the amount of updates in successive iterations is restricted to be no greater than the neighborhood radius∊.

@&#CONCLUSIONS@&#
