@&#MAIN-TITLE@&#
Improved variable step-size NLMS adaptive filtering algorithm for acoustic echo cancellation

@&#HIGHLIGHTS@&#
A new technique for acoustic echo cancellation.An improved variable step size normalized least mean square adaptive filtering algorithm.A comparative study using some variable step size algorithms and the proposed algorithm.Experiments in single talk and double talk scenarios using TIMIT database.Standardized performances measures: Mean Square Error and Normalized misalignment.

@&#KEYPHRASES@&#
Adaptive filtering,Acoustic echo canceller,Normalized least mean square (NLMS) algorithm,Variable step-size,Mean error sigmoid variable step-size NLMS,Misalignment evaluation,

@&#ABSTRACT@&#
Acoustic echo canceller (AEC) is used in communication and teleconferencing systems to reduce undesirable echoes resulting from the coupling between the loudspeaker and the microphone. In this paper, we propose an improved variable step-size normalized least mean square (VSS-NLMS) algorithm for acoustic echo cancellation applications based on adaptive filtering. The steady-state error of the NLMS algorithm with a fixed step-size (FSS-NLMS) is very large for a non-stationary input. Variable step-size (VSS) algorithms can be used to decrease this error. The proposed algorithm, named MESVSS-NLMS (mean error sigmoid VSS-NLMS), combines the generalized sigmoid variable step-size NLMS (GSVSS-NLMS) with the ratio of the estimation error to the mean history of the estimation error values. It is shown from single-talk and double-talk scenarios using speech signals from TIMIT database that the proposed algorithm achieves a better performance, more than 3 dB of attenuation in the misalignment evaluation compared to GSVSS-NLMS, non-parametric VSS-NLMS (NPVSS-NLMS) and standard NLMS algorithms for a non-stationary input in noisy environments.

@&#INTRODUCTION@&#
In recent years, acoustic echo cancellation has become an attractive challenge due to the fact that improving speech quality remains one of the most important goals of communication systems, such as teleconferencing, hands-free and mobile communications. It is often used to remove undesirable echoes resulting from the coupling between the loudspeaker and the microphone.Acoustic echo cancellation is an important application of adaptive filtering. The basic principle is to build a model of the echo path impulse response that needs to be identified with an adaptive filter, which provides at its output a replica of the echo, that is further subtracted from the reference signal [1]. The adaptive least mean square (LMS) algorithm is a well-known technique in adaptive filtering due to its simplicity and numerical robustness.However, it suffers from slow convergence, since the echo path is usually very long and the speech signals are non-stationary and highly correlated. Moreover, a large number of filter coefficients are needed for identifying this long path [2]. In the NLMS algorithm, the problem can be mitigated by normalizing the fixed step-size (FSS), since it depends on the power of the input signal, but the performance is still insufficient.In order to improve the convergence speed and reduce the static error, many variable step-size (VSS-NLMS) algorithms have been proposed in the literature [3–5]. They are divided into two classes [6]. The first class is obtained by taking into account some limit conditions, like the estimation error, which decreases the fastest according to the step-size. The second class is named “heuristic” algorithms, which are based on some intuitionistic rules, like increasing the step-size to increase the convergence rate, or decreasing the step-size to decrease the steady-state error. Numerous algorithms use the sigmoid function to improve the adaptive algorithm behavior [6–8].In this paper, a new variable step-size NLMS algorithm is proposed. It is based on the sigmoid function, where the step-size is controlled by the power of the estimation error. The sigmoid function will be generalized. So, the exponent of the exponential function (which is the subtraction of the power of the estimation error from the power of the estimated noise) will be replaced by the ratio of the estimated error to the mean history of estimation error values. This ratio is used to eliminate the influence of the non-stationary speech signal and noise disturbances, which can improve the performance of the algorithm, whereas a large step-size is needed for fast convergence and a small step-size for reduction of the steady-state error.This paper is organized as follows: a brief introduction presents the related works. In Section 2, we introduce the principle of acoustic echo cancellation systems. Section 3 describes briefly the adaptive filtering and the NLMS algorithm. Section 4 discusses some variable step-size algorithms. Section 5 gives a description of the proposed variable step-size MESVSS-NLMS algorithm. In Section 6, simulation results are given to investigate the performance of the proposed algorithm for various scenarios. A comparative study, using GSVSS-NLMS, NPVSS-NLMS and the standard NLMS algorithms is also presented. Finally, conclusions are given in Section 7.The basic principle of acoustic echo cancellation (AEC) is modeling the echo path between the loudspeaker and the microphone [9]. Fig. 1shows an AEC general configuration, where an adaptive filter is used to identify the unknown echo path by adaptively adjusting its coefficients. The estimated coefficients are then used to provide a replica of the echoes which can be subtracted from the target signal to achieve cancellation.We define the input signalx(n)to be the received far-end speech signal at each time index n.x(n)is then the output at the near-end loudspeaker, passing through the loudspeaker. This audio signal is then reverberated in a real environment and picked up by the system microphone (audio sink) resulting in the original intended signal plus attenuation. Time-delayed images of the original speech signal produce the echo signaly(n)[10]. The microphone signald(n)consists of the actual echo signal and the background noise signalb(n). Additionally, it includes the speech signals(n)of an active near-end speaker. The acoustic echo canceller (AEC) seeks to minimize the contribution of the echo signaly(n)to the power of the error signale(n)by subtracting an estimate of the echo signalyˆ(n)from the microphone signald(n)[11].Adaptive filter is a good supplement for achieving a good replica because the echo path is usually unknown and time-varying. So, the three main steps of the acoustic echo path can be summarized as follows:(1)Estimate the characteristics of the acoustic echo pathhof the loudspeaker-enclosure-microphone (LEM) system;Create a replica of the acoustic echo signalyˆ(n);Subtract the echo from microphone signal to obtain a clean signal.At each sample time n,e(n)is the error signal between the adaptive filter outputyˆ(n)and the desired signald(n). The error signal corresponds to the echo cancelled signal and it is obtained by subtracting this estimateyˆ(n)from the microphone signal.(1)e(n)=d(n)−yˆ(n)where(2)d(n)=y(n)+s(n)+b(n)is a combination of the acoustic echoy(n), the near-end speechs(n)and the background noiseb(n). The acoustic echo signal is modeled by the equation:(3)y(n)=XT(n)hwhere(4)X(n)=[x(n)x(n−1)…x(n−L+1)]T(5)h=[h0h1h2⋯hL−1]TX(n)is the length-L history of the received signal, or the far-end input signal, andhis the true echo path impulse response, assumed to be a finite impulse response (FIR) filter of length L. The superscript(.)Tdenotes the transpose of a vector. The residual echo signal which represents the estimation error is:(6)er(n)=y(n)−yˆ(n)Adaptive filtering techniques have been successfully used for many years. An adaptive filter is a self-designing and time-varying system that uses a recursive algorithm to continuously adjust its tap weights for operation in an unknown environment [12]. It attempts to model the relationship between two signals in an iterative manner [13]. Fig. 2shows a typical structure of the adaptive filter, which consists of two basic functional blocks: a digital filter to perform the desired filtering and an adaptive algorithm to adjust the tap weights of the filter.The digital filter shown in Fig. 2 can be designed using many different filter structures. To obtain simpler adaptive algorithms, finite impulse response (FIR) filters are preferable over infinite impulse response (IIR) filters [14].The desired signald(n)represents the microphone signal. In the single-talk scenario,d(n)contains the echo signaly(n)corrupted by ambient noiseb(n), whereas the resulting error signale(n)consists of the residual echo signaler(n)and the ambient noiseb(n). In the double-talk scenario,d(n)is defined by Eq. (2), wheree(n)is the sum ofs(n),er(n)andb(n). The output signal of the digital filter is defined by the following equation:(7)yˆ(n)=XT(n)W(n)whereW(n)is the vector of the adaptive filter weights.(8)W(n)=[w0(n)w1(n)…wL−1(n)]TThe adaptive filter is updated using an adaptive algorithm, where the adaptation terms are added to the weight vector. The adaptation terms differ according the adaptive filter algorithms [15].The LMS algorithm, first proposed by Widrow and Hoff in 1960 [16], is the most widely used adaptive filtering algorithm because of its simple structure, low computational complexity, good stability [17], and their performance can be defined by the cost function. The mean square error (MSE) has been the most extensively used criterion in the LMS algorithm and its variants [18,19]. Using this cost function implicitly assumes that the error is a random variable with a Gaussian distribution [20]. The rate of adaptation of the filter coefficients may be controlled by adjusting a parameter called the step-size. A larger step-size will increase the rate adaptation, and a smaller step-size will decrease it [21].The weight update equation for the adaptive filter is:(9)W(n+1)=W(n)+αX(n)e(n)where α is the step-size parameter, and the stability condition of the LMS algorithm is:(10)0<α<2λmaxwhereλmaxis the largest eigenvalue of the autocorrelation matrix of the input signalx(n).The NLMS algorithm may be viewed as the solution to a constrained optimization (minimization) problem [22]. This adaptive algorithm is widely used owing to its low computational complexity and ease of implementation, inherent numerical stability, very good tracking properties, and high robustness in a wide range of operating conditions [23,24]. This algorithm adapts the tap-weights sequence using a gradient descent algorithm that reduces the squared estimation error at each instant. The updated algorithm is given below:(11)W(n+1)=W(n)+μX(n)e(n)ε+XT(n)X(n)where μ is the adaptation step-size (0<μ<2), andε>0is a regularization constant used to improve the adaptation stability and to avoid division by zero.Conventional adaptive NLMS filters use a fixed step-size parameter, which can only be a compromise between the conflicting goals of fast convergence and small steady-state error. An appropriate step-size control is needed.The disadvantage of the classical NLMS, or the fixed step-size NLMS (FSS-NLMS), is that the step-size factor cannot meet the convergence rate and the steady-state error at the same time, especially for a non-stationary input such as the speech signal. In addition, the performances of FSS-NLMS deteriorate for colored inputs.A variable step-size algorithm is used to balance the trade-off between the rate of convergence and the steady-state error. The idea is to use a large step-size in the initial stages of convergence of the algorithm to enhance the convergence rate; on the other hand a smaller step-size is used in the stopping convergence stage of the algorithm, which corresponds to a small steady-state error.The NLMS algorithm has variable step-size parameters, with the weight update recursion given by:(12)W(n+1)=W(n)+μ(n)X(n)e(n)ε+XT(n)X(n)whereμ(n)is the variable step-size which has a variable positive scalar included to control the filter coefficients update.To balance the trade-off between different aspects of adaptive filtering such as convergence rate, mean-square error, and computational cost of the NLMS algorithm, an appropriate step-size control is needed [25]. In the literature, numerous variable step-size VSS-NLMS algorithms have been proposed. We describe in the following two examples: the non-parametric and the generalized sigmoid algorithms.The NPVSS-NLMS algorithm proposed in [26,27] uses a more reliable approach, which can be obtained by adjusting the step-size value in accordance with the criterion of attempting to reduce the squared error at each instant, imposing the conditionE{ϵ2(n)}=E{b2(n)}whereE{⋅}denotes mathematical expectation, andϵ(n)is an a posteriori estimation error defined by:(13)ϵ(n)=d(n)−XT(n)W(n+1)The a priori estimation errore(n)is defined by equation:(14)e(n)=d(n)−XT(n)W(n)The optimal variable step-size is given by the following equation used in NPVSS-NLMS algorithm:(15)μNPVSS(n)={β(n)ifσˆe(n)>σb0otherwisewhere(16)β(n)=1ε+XT(n)X(n)(1−σbϵ+σˆe(n))ϵ is a positive very small number to avoid division by zero.The variableσˆe2(n)=E{e2(n)}defines the power of the error signal,σb2=E{b2(n)}is the power of the system noise, known or estimated. An estimation used to calculateσˆe(n)is updated as follows:(17)σˆe(n)=λσˆe2(n−1)+(1−λ)e2(n)where λ is an exponential weighting factor lying in the interval]0,1]. Table 1summarizes the NPVSS-NLMS.The GSVSS-NLMS algorithm, proposed in [6], uses a sigmoid function based on the adjusted variable step of the adaptive filter algorithm. Its principle is that the step-size should be large in the initial stage in order to get great tracking speed. But the step should be small at the convergence stage to maintain steady-state error [8].The sigmoid variable step-size in GSVSS-NLMS algorithm is shown as follows:(18)μSVSS(n)=B(11+exp⁡(−A|e(n)|)−0.5)where A,0.001<A<0.1, and B,0<B<2, are respectively the shape and the range controlling parameters of the variable step-size function.The generalized sigmoid variable step-size in GSVSS-NLMS algorithm is done by:(19)μGSVSS(n)=B(11+exp⁡(−A(σe(n)−σb)m)−0.5)where m,0<m<5, is the parameter of the generalized sigmoid function. The GSVSS-NLMS is summarized in Table 2.In this section, we describe the proposed algorithm, named MESVSS-NLMS (mean error sigmoid VSS-NLMS), which is an improvement of the GSVSS-NLMS algorithm. This algorithm uses the sigmoid step-size and replaces the estimation error by the estimation error of the mean of the block error ratio, where the length of block error varies according to a rule decomposition. The variable step depends on the estimated error. The main idea of the proposed algorithm is to decompose the estimation error into a sub-block.Fig. 3shows an example of error decomposition for 3 levels of decomposition, where D is the maximum level of error decomposition initialized in the first, n and N are the iteration number and the total number of iterations respectively, which correspond to the index time n and the length of the input signalx(n). The proposed algorithm consists of two stages.Stage 1:In this stage, the length of the block of the estimation error varies, where the maximum length isLms, which is defined by the following equation:(20)Lms=N2DIn this block error, the modified errorem(n)equals the estimated errore(n)to the mean of the error vectorE(n)ratio, as is expressed by the equation:(21)em(n)=e(n)1n∑0nE(n)whereE(n)cumulates the estimation errore(n). The mean ofE(n)is calculated for a varying length between 0 andLms−1, inn=0the mean1n∑0nE(n)=e(0). The objective is to accelerate the convergence in this stage, where the estimation error is very important in the beginning stage.Stage 2:The length of the block errorLmin the second stage is fixed for each decomposition level l,l=1…D, whereLmis defined by:(22)Lm=N2(D−l)The modified errorem(n)for each block is calculated by the equation:(23)em(n)=e(n)1Lm∑n−LmnE(n)The purpose of this technique is to minimize the error in the steady-state stage.In this algorithm, the modified errorem(n)is replaced with(σe(n)−σb)value in the GSVSS-NLMS algorithm [6], and the variable step-size is adapted as:(24)μMESVSS(n)=B(11+exp⁡(−A(em(n))m)−0.5)A, B, and m are same as described in Eqs. (18) and (19).The proposed algorithm is summarized in the Table 3.Fig. 5 shows the curve of the variable step-size functionμ(n)for 2000 iterations, with the variablee(n), which varies between −1 and 1 as depicted in Fig. 4. The two functions, the generalized sigmoidμGSVSS(n)and the mean error sigmoidμMESVSS(n)are adjusted using the following parameters:A=0.002,B=0.7,m=1,D=4,λ=0.999.The proposed variable step-size function is varied according the two successive following stages. The modified errorem(n)in the first stage is calculated according to Eq. (21) while the modified errorem(n)in the second stage is evaluated using Eq. (23). In these two stages the proposed function has a good behavior compared to the sigmoid function in terms of obtaining a large step-size when thee(n)values are large. On one hand, the convergence rate is improved. On the other hand, this function has a small step-size in the stopping stage of the convergence when we obtain a lower steady-state error.

@&#CONCLUSIONS@&#
