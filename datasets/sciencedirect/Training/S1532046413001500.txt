@&#MAIN-TITLE@&#
Determining the difficulty of Word Sense Disambiguation

@&#HIGHLIGHTS@&#
We explore estimating WSD performance on a range of ambiguous biomedical terms.We evaluate the difficulty predictions against the output of two WSD systems.Supervised methods are the best predictors but limited by labeled training data.Unsupervised methods all perform well and can be applied more widely.Best performance was obtained using the relatedness measure proposed by Lesk.

@&#KEYPHRASES@&#
Natural Language Processing,NLP,Word Sense Disambiguation,WSD,Ambiguity,Biomedical documents,

@&#ABSTRACT@&#
Automatic processing of biomedical documents is made difficult by the fact that many of the terms they contain are ambiguous. Word Sense Disambiguation (WSD) systems attempt to resolve these ambiguities and identify the correct meaning. However, the published literature on WSD systems for biomedical documents report considerable differences in performance for different terms. The development of WSD systems is often expensive with respect to acquiring the necessary training data. It would therefore be useful to be able to predict in advance which terms WSD systems are likely to perform well or badly on.This paper explores various methods for estimating the performance of WSD systems on a wide range of ambiguous biomedical terms (including ambiguous words/phrases and abbreviations). The methods include both supervised and unsupervised approaches. The supervised approaches make use of information from labeled training data while the unsupervised ones rely on the UMLS Metathesaurus. The approaches are evaluated by comparing their predictions about how difficult disambiguation will be for ambiguous terms against the output of two WSD systems. We find the supervised methods are the best predictors of WSD difficulty, but are limited by their dependence on labeled training data. The unsupervised methods all perform well in some situations and can be applied more widely.

@&#INTRODUCTION@&#
Word Sense Disambiguation (WSD) is the task of automatically identifying the appropriate sense of an ambiguous word based on the context in which the word is used. For example, the term cold could refer to the temperature or the common cold, depending on how the word is used in the sentence. Automatically identifying the intended sense of ambiguous words improves the performance of biomedical and clinical applications such as medical coding and indexing; applications that are becoming essential tasks due to the growing amount of information available to researchers.A wide range of approaches have been applied to the problem of WSD in biomedical and clinical documents [1–7]. Accurate WSD can improve the performance of biomedical text processing applications, such as summarization [8], but inaccurate WSD has been shown to reduce an application’s overall performance [9]. The disambiguation of individual terms is important since some of those terms are more important than others when determining whether there is any overall improvement of the system [8]. The importance of WSD is likely to depend on the application and research question. For example, Weeber et al. [10] found that it was necessary to resolve the ambiguity in the abbreviation “MG” (which can mean “magnesium” or “milligram”) in order to replicate the connection between migraine and magnesium identified by Swanson [11].It is now possible to perform very accurate disambiguation for some types of ambiguity, such as abbreviations [12]. However, there is considerable difference in the performance of WSD systems for different ambiguities. For example, Humphrey et al. [3] report that the performance of their unsupervised WSD approach varies between 100% (for terms such as culture and determination) and 6% (for fluid). Consequently, it is important to determine the accuracy of a WSD system for the ambiguities of interest to get an idea of whether it will be useful for the overall application, and if so, which terms should be disambiguated.Historically, supervised machine learning approaches have been shown to disambiguate terms with a higher degree of accuracy than unsupervised methods. The disadvantage to supervised methods is that they require manually annotated training data for each term that needs to be disambiguated. However, manual annotation is an expensive, difficult and time-consuming process which is not practical to apply on a large scale [13]. To avoid this problem, techniques for automatically labeling terms with senses have been developed [12,14] but these can only be applied to limited types of ambiguous terms, such as abbreviations and terms which occur with different MeSH codes. Therefore, it would be useful to be able to predict the difficulty of a particular term in order to determine whether applying WSD would be of benefit to the overall system.This paper explores approaches to estimating the difficulty of performing WSD on ambiguities found in biomedical documents. By difficulty we mean the WSD performance that can be obtained for the ambiguity since, in practise, performance is the most important factor in determining whether applying WSD to a particular ambiguity is likely to be useful. Ambiguities for which low WSD performance is obtained are considered to be difficult to disambiguate while those for which the performance is high are considered to be easy to disambiguate.Some of the methods applied in this paper are supervised since they are based on information derived from a corpus containing examples of the ambiguous term labeled with the correct sense. Other methods do not require this resource and only require information about the number of possible senses for each ambiguous term which is normally obtained from a knowledge source, such as the UMLS Metathesaurus (see Section 2.1.1).Section 2 provides background information on relevant resources and techniques for computing similarity or relatedness in the biomedical domain. Section 3 describes a range of methods for estimating WSD difficulty, including ones that have been used previously and an unsupervised method based on the similarity/relatedness measures described in Section 2. Experiments to evaluate these are described in Section 4 and their results in Section 5. Finally, conclusions are presented in Section 6.This section presents the resources that are used in the experiments described later in the paper. In particular, they are used by the similarity and relatedness measures described in Sections 2.2.1 and 2.2.2.The Unified Medical Language System (UMLS) is a repository that stores a number of distinct biomedical and clinical resources. One such resource, used in this work, is the Metathesaurus [15].The Metathesaurus contains biomedical and clinical concepts from over 100 disparate terminology sources that have been semi-automatically integrated into a single resource containing a wide range of biomedical and clinical information. For example, it contains the Systematized Nomenclature of Medicine–Clinical Terms (SNOMED CT), which is a comprehensive clinical terminology created for the electronic exchange of clinical health information, the Foundational Model of Anatomy (FMA), which is an ontology of anatomical concepts created specifically for biomedical and clinical research, and MedlinePlus Health Topics, which is a terminology source containing health related concepts created specifically for consumers of health services.The concepts in these sources can overlap. For example, the concept Cold Temperature exists in both SNOMED CT and MeSH. The Metathesaurus assigns the synonymous concepts from the various sources Concept Unique Identifiers (CUIs). Thus both the Cold Temperature concepts in SNOMED CT and MeSH are assigned the same CUI (C0009264). This allows multiple sources in the Metathesaurus to be treated as a single resource.Some sources in the Metathesaurus contain additional information such as a concept’s synonyms, its definition,1Not all concepts in the UMLS have a definition.1and its related concepts. The Metathesaurus contains a number of relations. The two main hierarchical relations are: the parent/child (PAR/CHD) and broader/narrower (RB/RN) relations. A parent/child relation is a hierarchical relation between two concepts that has been explicitly defined in one of the sources. For example, the concept Cold Temperature has an is-a relation with the concept Freezing in MeSH. This relation is carried forward to the CUI level creating a parent/child relations between the CUIs C0009264 [Cold Temperature] and C0016701 [Freezing] in the Metathesaurus. A broader/narrower relation is a hierarchical relation that does not explicitly come from a source but is created by the UMLS editors. For this work, we use the parent/child relations.MEDLINE2http://www.ncbi.nlm.nih.gov/pubmed/.2is a bibliographic database that currently contains over 22 million citations to journal articles in the biomedical domain and is maintained by the National Library of Medicine (NLM). The 2009 MEDLINE Baseline Repository3http://mbr.nlm.nih.gov/.3encompasses approximately 5200 journals starting from 1948 and contains 17,764,826 citations; consisting of 2,490,567 unique unigrams (single words) and 39,225,736 unique bigrams (two-word sequences). The majority of the publications are scholarly journals but a small number of other sources such as newspapers and magazines are included.UMLSonMedline, created by NLM, consists of concepts from the 2009AB UMLS and the number of times they occurred in a snapshot of MEDLINE taken on 12/01/2009. The frequency counts were obtained by using the Essie Search Engine [16] which queried MEDLINE with normalized strings from the 2009AB MRCONSO table in the UMLS. The frequency of a CUI was obtained by aggregating the frequency counts of the terms associated with the CUI to provide a rough estimate of its frequency.The Medical Subject Headings (MeSH) Thesaurus ([17]) is the NLM’s controlled vocabulary thesaurus consisting of biomedical and health related terms/concepts created for the purpose of indexing articles from MEDLINE. Each MEDLINE citation is associated with a set of manually annotated MeSH terms that describe the content of the article. The MeSH terms are organized in a hierarchical structure in order to permit searching at various levels of specificity. The 2013 version contains 26,853 terms organized into 11 different hierarchies.4http://www.nlm.nih.gov/pubs/factsheets/mesh.html.4This section described measures of similarity and relatedness between biomedical concepts that have been previously explored in the literature.Existing semantic similarity measures can be categorized into two groups: path-based and information content (IC)-based. Path-based measures use information about the number of nodes between concepts in a hierarchy, whereas IC-based measures incorporate the probability of the concept occurring in a corpus of text.Path-based Similarity Measures Rada et al. [18] introduce the conceptual distance measure which is the length of the shortest path between two concepts (c1 and c2) in MeSH using RB/RN relations from the UMLS. Caviedes and Cimino [19] later adapted this measure using the PAR/CHD relations in the UMLS.Our first measure, path, is a modification of Caviedes and Cimino’s approach. Similarity is defined as the reciprocal of the length of the shortest path between the two concepts in the UMLS hierarchy. This is shown in Eq. (1), where path_length(c1,c2) is the number of nodes in the shortest path between c1 and c2.(1)simpath(c1,c2)=1path_length(c1,c2)Wu and Palmer [20] extend this measure by incorporating the depth of the Least Common Subsumer (LCS). The LCS of a pair of concepts is the lowest concept in the hierarchy which subsumes that pair. In this measure, the similarity is twice the depth of the two concepts LCS divided by the product of the depths of the individual concepts as defined in Eq. (2), where depth is the number of nodes between c and the root node in the hierarchy.(2)simwup(c1,c2)=2*depth(lcs(c1,c2))depth(c1)+depth(c2)IC-based Similarity Measures Information content (IC) is formally defined as the negative log of the probability of a concept [21]. The probability of a concept, c, is obtained by summing the number of times it or one of its descendants is seen in a corpus. The concepts descendants are obtained from some concept hierarchy, such as one of those contained in the UMLS Metathesaurus. Very general concepts have high probabilities since their descendants are mentioned frequently and this leads to them having low IC values. Conversely, specific concepts have low probabilities and high IC values. Resnik [22] modified IC for use as a similarity measure. He defined the similarity of two concepts to be the IC of their LCS, see Eq. (3).(3)simres(c1,c2)=IC(lcs(c1,c2))=-log(P(lcs(c1,c2)))Jiang and Conrath [23] and Lin [24] extended Resnik’s IC-based measure by incorporating the IC of the individual concepts. Lin defined the similarity between two concepts by taking the quotient between twice the IC of the concepts’ LCS and the sum of the IC of the two concepts as shown in Eq. (4). This is similar to the measure proposed by Wu and Palmer; differing in the use of IC rather than the depth of the concepts.(4)simlin(c1,c2)=2*IC(lcs(c1,c2))IC(c1)+IC(c2)Jiang and Conrath defined the distance between two concepts to be the sum of the IC of the two concepts minus twice the IC of the concepts’ LCS. This measure is often modified to return a similarity score by taking the reciprocal of the distance as shown in Eq. (5).(5)simjcn(c1,c2)=1IC(c1)+IC(c2)-2∗IC(lcs(c1,c2))Lesk [25] introduces a measure that determines the relatedness between two concepts by counting the number of shared terms in their definitions. An overlap is the longest sequence of one or more consecutive words that occur in both definitions. When implementing this measure in WordNet, Banerjee and Pedersen [26] found that the definitions were short, and did not contain enough overlaps to distinguish between multiple concepts. They extended this measure by including the definition of related concepts in WordNet.Patwardhan and Pedersen [27] extend the measure proposed by Lesk using second-order co-occurrence vectors. In this method, a vector is created for each word in the concepts definition containing words that co-occur with it in a corpus. These word vectors are average to create a single co-occurrence vector for the concept. The similarity between the concepts is calculated by taking the cosine between the concepts second-order vectors.There has been little previous work on estimating the difficulty of WSD. Kilgarriff and Rosenzweig [28] analysed the difficulty of disambiguating terms used in the first SemEval WSD evaluation exercise [29] and found the entropy of the sense distribution to work well. This is calculated as follows:(6)Entropy(S)=-∑i=1NPr(si)log2Pr(si)where S={s1,s2…sN} is the set of possible senses for some ambiguous term and Pr (si) the probability of sense Siobtained from a labeled corpus.In domain-independent WSD the Most Frequent Sense (MFS) is commonly used to indicate the difficulty of a particular term [30,31]. MFS is simply the sense that is found most frequently in a training corpus and is computed as follows:(7)MFS(S)=argmaxiP(si)MFS is often used as a simple baseline for supervised WSD systems [32]. Like entropy, MFS also requires labeled training data.Both of these approaches are based on the distribution of senses in text and the assumption behind them is that this information is a useful predictor of the difficulty of disambiguating that term. For example, consider an ambiguity where one of the senses is much more likely to appear than the others. The ambiguity will probably be easy to disambiguate, since always assigning the most probable sense will lead to reasonable WSD performance.Stevenson and Guo [33] applied entropy and MFS to analyse the difficulty of automatically generating labeled WSD training data. However they did not explore whether they could be used to determine the difficulty of WSD for particular terms.Stevenson and Guo [33] also made use of additional measures. One was the number of possible senses for the ambiguous term. The advantages of this measure is that it is very simple to compute and does not require any labeled training data. The intuition behind this approach is that ambiguities with a large number of possible senses will be difficult to disambiguate, simply because of the number of senses to choose from.Stevenson and Guo [33] also describe an approach that relies on computing the average pairwise similarity between the possible senses of ambiguous terms (see Section 2.2.1). Like counting the number of possible senses, this approach also has the advantage of not requiring any labeled training data.The assumption behind this approach is that if the possible meanings of an ambiguous term are similar then that term will be more difficult to disambiguate than one where the meanings are clearly distinct. This is motivated by previous work on manual annotation of word senses which have shown that humans often struggle to distinguish between closely related meanings [13,34].We extend this approach by considering the maximum similarity between senses in addition to the average. Two metrics were applied: mean similarity and maximum similarity. For the mean similarity, the degree of similarity between the concepts of each of the ambiguous word’s possible senses is computed and combined by taking the mean of the similarities. This is calculated as follows:(8)mean_similarity(S)=∑{si,sj}∊(S2)sim(si,sj)S2where S is the set of senses and sim(si,sj) is the similarity between two of these senses as determined by one of the measures described in Section 2.2.The maximum similarity measure is computed in a similar way. However, instead of taking the mean of the pairwise similarities the maximum is chosen:(9)max_similarity(S)=argmax{si,sj}∊S2sim(si,sj)

@&#CONCLUSIONS@&#
