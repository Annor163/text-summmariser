@&#MAIN-TITLE@&#
Surrogate duality for robust optimization

@&#HIGHLIGHTS@&#
We show a set containment characterization with data uncertainty.We investigate surrogate strong duality theorem for robust quasiconvex programming with its constraint qualification.We investigate surrogate min–max duality theorem for robust quasiconvex programming with its constraint qualification.We obtain a surrogate duality theorems for semi-definite optimization problems in the face of data uncertainty.

@&#KEYPHRASES@&#
Nonlinear programming,Quasiconvex programming,Robust optimization,

@&#ABSTRACT@&#
Robust optimization problems, which have uncertain data, are considered. We prove surrogate duality theorems for robust quasiconvex optimization problems and surrogate min–max duality theorems for robust convex optimization problems. We give necessary and sufficient constraint qualifications for surrogate duality and surrogate min–max duality, and show some examples at which such duality results are used effectively. Moreover, we obtain a surrogate duality theorem and a surrogate min–max duality theorem for semi-definite optimization problems in the face of data uncertainty.

@&#INTRODUCTION@&#
Mathematical programming problems with data uncertainty are becoming important in optimization due to the reality of uncertainty in many real-world optimization problems. Robust optimization, which has emerged as a powerful deterministic approach for studying mathematical programming with data uncertainty, associates an uncertain mathematical program with its robust counterpart. Many researchers [1,8–11,13] have investigated duality theory for linear or convex programming problems under uncertainty with the worst-case approach(the robust approach). They used mainly the duality theorem for linear programming, the Lagrange duality theorem, and Sion’s min–max theorem. This research gives elegant, powerful, and completely characterized results for robust convex optimization.On the other hand, recently, many authors [4–6,15–18] investigated surrogate duality for quasiconvex programming. Surrogate duality is used in not only quasiconvex programming but also integer programming and the knapsack problem [3–6,15–17]. Surrogate duality is also closely related to Lagrange duality. In [18], we investigated a necessary and sufficient constraint qualification for surrogate duality. Also, we investigated that the constraint qualification is weaker than the CCCQ, which is a necessary and sufficient constraint qualification for Lagrange duality.In the present paper, we investigate surrogate duality theorems for quasiconvex programming under data uncertainty via robust optimization. We also propose new constraint qualifications and compare these with previous ones. The remainder of the present paper is organized as follows. In Section 2, we introduce some preliminaries. In Section 3, we investigate surrogate strong duality for robust optimization. In Section 4, we investigate surrogate min–max duality for robust optimization, showing some examples. Finally, in Section 5, we obtain a surrogate duality theorem and a surrogate min–max duality theorem for semi-definite optimization problems in the face of data uncertainty.Let 〈v,x〉 denote the inner product of two vectors v and x in the n-dimensional Euclidean spaceRn. Given a setA⊂Rn, we denote the closure, the convex hull, and the conical hull generated by A, by clA, coA, and coneA, respectively. The indicator function δAis defined byδA(x)≔0x∈A,∞otherwise.Let f be a function fromRntoR¯, whereR¯=[-∞,∞]. Here, f is said to be proper if for allx∈Rn,f(x)>-∞and there existsx0∈Rnsuch thatf(x0)∈R. We denote the domain of f by domf, that is,domf={x∈Rn|f(x)<∞}. The epigraph of f, epif, is defined asepif={(x,r)∈Rn×R|f(x)⩽r}, and f is said to be convex if epif is convex. In addition, the Fenchel conjugate of f, f∗ :Rn→R¯, is defined as f∗(u)=supx∈domf{〈u,x〉−f(x)}. The subdifferential of f at x is defined as∂f(x)={x∗∈Rn|∀y∈Rn,f(y)⩾f(x)+〈x∗,y-x〉}. Also, the normal cone of A at x∈A is defined asNA(x)={x∗∈Rn|∀y∈A,〈x∗,y-x〉⩽0}. It is clear that NA(x)=∂δA(x). Recall that f is said to be quasiconvex if for allx1,x2∈Rnand λ∈(0,1), f((1−λ)x1+λx2)⩽max{f(x1), f(x2)}. Define level sets of f with respect to a binary relation♢onR¯asL(f,♢,β)={x∈Rn|f(x)♢β}for anyβ∈R. Then, f is quasiconvex if and only if for anyβ∈R,L(f,⩽,β)is a convex set, or equivalently, for anyβ∈R,L(f,<,β)is a convex set. Any convex function is quasiconvex, but the converse is not true.In [7], Jeyakumar investigated the following set containment characterization. This result is very important and useful in the research of necessary and sufficient constraint qualifications for the Lagrange duality theorem in convex programming.Theorem 1[7]Let I be an arbitrary set, and for each i∈I, let gibe a convex function fromRntoR. In addition, let{x∈Rn|∀i∈I,gi(x)⩽0}be nonempty,x∗∈Rn,α∈R. Then (i) and (ii) given below are equivalent:(i){x∈Rn|∀i∈I,gi(x)⩽0}⊂{x∈Rn|〈x∗,x〉⩽α},(x∗,α)∈clconeco⋃i∈Iepigi∗.In [8], the following result was investigated. This result is one of set containment characterization with data uncertainty and is similar to Theorem 1.Theorem 2[8]Let I={1,…,m} and let gibe continuous functions fromRn×RqtoRsuch that for eachvi∈Rq,gi(·,vi)is a convex function. LetVi,i=1,…,m, be subsets ofRq,V=∏i=1mVi, andF={x|∀vi∈Vi,gi(x,vi)⩽0,∀i=1,…,m}≠∅. ThenepiδF∗=clco⋃v∈V,λ∈R+mepi∑i=1mλigi(·,vi)∗.Throughout this paper, let I={1,…,m}, gicontinuous functions fromRn×RqtoRsuch that for eachvi∈Rq,gi(·,vi)a convex function,Vi,i=1,…,m, nonempty subsets ofRq,V=∏i=1mVi,F={x∈Rn|∀i∈{1,…,m},∀vi∈Vi,gi(x,vi)⩽0}≠∅, andF(v,λ)=x∈Rn∑i=1mλigi(x,vi)⩽0.In this section, we investigate surrogate duality for robust quasiconvex optimization problem. First, we show a set containment characterization with data uncertainty.Theorem 3The following condition hold:epiδF∗=clco⋃v∈V,λ∈R+mclconeepi∑i=1mλigi(·,vi)∗.For allv∈V=∏i=1mViandλ∈R+m, it is clear that F⊂F(v,λ) and it is easy to verify thatepiδF∗⊃epiδF(v,λ)∗=clconeepi∑i=1mλigi(·,vi)∗⊃epi∑i=1mλigi(·,vi)∗.ThenepiδF∗⊃⋃v∈V,λ∈R+mclconeepi∑i=1mλigi(·,vi)∗⊃⋃v∈V,λ∈R+mepi∑i=1mλigi(·,vi)∗.By Theorem 2epiδF∗=clco⋃v∈V,λ∈R+mclconeepi∑i=1mλigi(·,vi)∗=clco⋃v∈V,λ∈R+mepi∑i=1mλigi(·,vi)∗.This completes the proof.□Assume that F≠∅. From the proof of Theorem 3,clco⋃v∈V,λ∈R+mepi∑i=1mλigi(·,vi)∗=clco⋃v∈V,λ∈R+mclconeepi∑i=1mλigi(·,vi)∗and if⋃v∈V,λ∈R+mepi∑i=1mλigi(·,vi)∗is closed and convex, then⋃v∈V,λ∈R+mclconeepi∑i=1mλigi(·,vi)∗,is closed and convex. But the converse does not hold as shown in the following example.LetV=[1,2], for eachv∈V,g(x,v)=v2(x-v)2x⩾v,0-v⩽x⩽v,v2(x+v)2x⩽-v.We can calculate Fenchel conjugate of g(·,v) as follows:(g(·,v))∗(w)=w22v-vww⩾0,w22v+vww⩽0.Then⋃v∈V,λ⩾0clconeepi(λg(·,v))∗={(x,α)∈R2||x|⩽α},and hence the set is closed and convex. However,⋃v∈V,λ⩾0epi(λg(·,v))∗={(x,α)∈R2||x|<α}∪{(0,0)},and hence the set is not closed.In the following theorem, we show a necessary and sufficient constraint qualification of surrogate duality for robust quasiconvex optimization problem.Theorem 4The following conditions are equivalent:(i)⋃v∈V,λ∈R+mclconeepi∑i=1mλigi(·,vi)∗is closed and convex,for all upper semicontinuous (usc) quasiconvex function f fromRntoR¯with dom f∩F≠∅, there existv¯∈Vandλ¯∈R+msuch thatinf{f(x)|x∈F}=inff(x)∑i=1mλi¯gi(x,vi¯)⩽0.for all continuous linear function f fromRntoR, there existv¯∈Vandλ¯∈R+msuch thatinf{f(x)|x∈F}=inff(x)∑i=1mλi¯gi(x,vi¯)⩽0.First, we show that (i) implies (ii). Let f be a usc quasiconvex function and m=infx∈Ff(x). If m=−∞, then (ii) holds trivially. So, assume that m is finite. If L(f,<,m) is empty, then putting λ=0 and taking anyv∈V, the equality holds. If L(f,<,m) is not empty, then there exists(x∗,α)∈Rn×Rsuch that for all x∈F and y∈L(f,<,m)〈x∗,x〉⩽α<〈x∗,y〉,since L(f,<,m)∩F=∅ and L(f,<,m) is a nonempty open convex set. By condition (i) and Theorem 3(x∗,α)∈epiδF∗=⋃v∈V,λ∈R+mclconeepi∑i=1mλigi(·,vi)∗.Hence, there existv¯∈Vandλ¯∈R+msuch that(x∗,α)∈clconeepi∑i=1mλi¯gi(·,vi¯)∗.Also, by Theorem 1epiδF(v¯,λ¯)∗=episupα⩾0α∑i=1mλi¯gi(·,vi¯)∗=clco⋃α⩾0epiα∑i=1mλi¯gi(·,vi¯)∗=clconeepi∑i=1mλi¯gi(·,vi¯)∗,in detail, see [2,7,14]. Hence,(x∗,α)∈epiδF(v¯,λ¯)∗. By using the above separation inequality, we can prove that for allx∈Rnmi=1mλi¯gi(x,vi¯)⩽0⇔x∈F(v¯,λ¯)⇒〈x∗,x〉⩽α⇒x∉L(f,<,m)⇔f(x)⩾m,that is,inff(x)∑i=1mλi¯gi(x,vi¯)⩽0⩾m, which shows that (ii) holds.It is clear that (ii) implies (iii).Finally, we show that (iii) implies (i). Because of Theorem 3, we only show thatepiδF∗⊂⋃v∈V,λ∈R+mclconeepi∑i=1mλigi(·,vi)∗. Let(x∗,α)∈epiδF∗. Then,δF∗(x∗)∈RandδF∗(x∗)=-infx∈F〈-x∗,x〉. Since −x∗ is a continuous linear function, by condition (iii), there existv¯∈Vandλ¯∈R+msuch thatinfx∈F〈-x∗,x〉=inf〈-x∗,x〉∑i=1mλi¯gi(x,vi¯)⩽0.Hence, for allx∈Rn∑i=1mλi¯gi(x,vi¯)⩽0⇒〈-x∗,x〉⩾-δF∗(x∗)⇔〈x∗,x〉⩽δF∗(x∗).This impliesδF(v¯,λ¯)∗(x∗)⩽δF∗(x∗)⩽α, and hence by Theorem 1(x∗,α)∈epiδF(v¯,λ¯)∗=clconeepi∑i=1mλi¯gi(·,vi¯)∗.This completes the proof.□Now we give an example illustrating Theorem 4.Example 2Consider the following optimization problem (UP) with an uncertainty parameter v:(UP)minimizef(x)≔x3subjecttog(x,v)⩽0,v∈V≔[0,1],where g is a function as follows:g(x,v)=v(x-2+v)2x⩾2-v,0-1-v⩽x⩽2-v,(1-v)(x+1+v)2x⩽-1-v.Then, f is continuous quasiconvex and F=[−1,1]. Also, we can check thatepiδF∗={(x,α)∈R2||x|⩽α}={(x,α)∈R2|0⩽x⩽α}⋃{(x,α)∈R2|0⩽-x⩽α}=clconeepi(g(·,1))∗⋃clconeepi(g(·,0))∗⊂⋃v∈V,λ⩾0clconeepi(λg(·,v))∗.Hence by Theorem 3,⋃v∈V,λ⩾0clconeepi(λg(·,v))∗is closed and convex. Moreover, let(v¯,λ¯)=(0,1), theninf{f(x)|∀v∈V,g(x,v)⩽0}=-1=inf{f(x)|λ¯g(x,v¯)⩽0}.We give examples showing that without the closed cone constraint qualification (in Theorem 4), Theorem 4 may not hold.Example 3Letg(x,v)≔v|x|-v, for allx∈Randv∈V≔[0,1]. Then, g(·,v) is convex and for eachv∈Vand λ⩾0,(λg(·,v))∗(x∗)=λv,x∗∈-λv,λv,∞,otherwise.Then, for each λ⩾0⋃v∈Vclconeepi(λg(·,v))∗={(x,y)∈R2|y>0}∪{(0,0)}.Hence,⋃v∈V,λ⩾0clconeepi(λg(·,v))∗is not closed, that is, the closed cone constraint qualification in Theorem 4 does not hold.Let f be a function fromRtoRas follows:f(x)=0x⩾0,-1x<0.Then, f is a usc quasiconvex function andinf{f(x)|∀v∈V,g(x,v)⩽0}=0. However, for allv∈Vand λ⩾0, inf{f(x)∣λg(x,v)⩽0}=−1, that is, surrogate duality does not hold.Furthermore, if f(x)=x3inf{f(x)|∀v∈V,g(x,v)⩽0}=supλ⩾0,v∈Vinf{f(x)|λg(x,v)⩽0}.However, the maximum does not attained. Actually, we can check thatinf{f(x)|∀v∈V,g(x,v)⩽0}=0. Letv∈Vand λ⩾0. If v=0 or λ=0, then inf{f(x)∣λg(x,v)⩽0}=−∞. If v>0 and λ>0, theninf{f(x)|λg(x,v)⩽0}=-vv<0.Let g(x,v)=v2∣x1∣+max{x2,0}−2v for allx=(x1,x2)∈R2andv∈V=[0,1]. Then,F={x∈R2|∀v∈V,g(x,v)⩽0}={x∈R2||x1|⩽2,x2⩽0},(λ1g1(·,v1))∗(x∗)=2λ1v1,x1∗∈-λ1v12,λ1v12,x2∗∈[0,λ1],∞,otherwise,for eachv∈Vandλ∈R+, and⋃v∈V,λ∈R+epi(λg(·,v))∗is not convex (see Example 2.1 in [8]). Hence, we cannot apply Theorem 3.1 in [8] to this function g. Furthermore,⋃v∈V,λ∈R+clconeepi(λg(·,v))∗is not convex. Actually, we can check that (2g((0,2),0))∗=0 and (2g((2,2),1))∗=4, that is((0,2),0)∈epi(2g(·,0))∗⊂⋃v∈V,λ∈R+clconeepi(λg(·,v))∗,((2,2),4)∈epi(2g(·,1))∗⊂⋃v∈V,λ∈R+clconeepi(λg(·,v))∗.However12((0,2),0)+12((2,2),4)=((1,2),2)∉⋃v∈V,λ∈R+clconeepi(λg(·,v))∗.If((1,2),2)∈⋃v∈V,λ∈R+clconeepi(λg(·,v))∗, then there existv∈Vandλ∈R+such that ((1,2),2)∈clconeepi(λg(·,v))∗. Since clconeepi(λg(·,v))∗ is closed, ((1,2),2)∈coneepi(λg(·,v))∗ and hence there exists γ>0 such that γ((1,2),2)∈epi(λg(·,v))∗. This implies that-λv2⩽γ⩽λv2,0⩽2γ⩽λ,and2λv⩽2γ.Then, we can see thatγλ⩽12andγλ⩾1, which is a contradiction. Hence,⋃v∈V,λ∈R+clconeepi(λg(·,v))∗is not convex. So we cannot apply Theorem 4 to this function g.Let f be a function fromRtoRas follows:f(x1,x2)=-1x2>0,0x2⩾0.Then, we can check that surrogate duality does not hold by the similar way of Example 3.Next, we investigate uncertainty in the objective function. The following theorem indicates that the constraint qualification also characterizes completely surrogate duality for uncertainty in the objective function.Theorem 5The following conditions are equivalent:(i)⋃v∈V,λ∈R+mclconeepi∑i=1mλigi(·,vi)∗is closed and convex,for all continuous function f fromRn×RptoRsuch that f(·,u) is quasiconvex for eachu∈Rpand f(x,·) is quasiconcave for eachx∈Rn, and any compact convex subsetUofRp,infx∈Fmaxu∈Uf(x,u)=maxu∈U,v∈V,λ∈R+minff(x,u)∑i=1mλigi(x,vi)⩽0.We first prove that (i) implies (ii). Let f be a continuous functionRn×RqtoRsuch that f(·,u) is quasiconvex for eachu∈Rpand f(x,·) is quasiconcave for eachx∈Rn, and letUbe a compact convex subset ofRp. Then,maxu∈Uf(·,u)is continuous and quasiconvex. So, by Theorem 4, there existv¯∈Vandλ¯∈R+msuch thatinfx∈Fmaxu∈Uf(x,u)=infx∈F(v¯,λ¯)maxu∈Uf(x,u).We know thatF(v¯,λ¯)is convex since gi(·,vi) is convex for all i and vi. Hence, by Sion’s min–max theorem [12]infx∈F(v¯,λ¯)maxu∈Uf(x,u)=maxu∈Uinfx∈F(v¯,λ¯)f(x,u).Thus there existu¯∈Usuch thatinfx∈Fmaxu∈Uf(x,u)=infx∈F(v¯,λ¯)f(x,u¯).And also, for allλ0∈R+m,v0∈Vandu0∈Uinfx∈Fmaxu∈Uf(x,u)⩾infx∈F(v0,λ0)maxu∈Uf(x,u)⩾maxu∈Uinfx∈F(v0,λ0)f(x,u)⩾infx∈F(v0,λ0)f(x,u0).Hence (ii) holds.The converse implication is obtained by choosing for f a continuous linear form.□In this section, we consider a surrogate min–max duality theorem for robust convex optimization problem.For all(v,λ)∈V×R+m,(1)NF(v,λ)(x¯)=x∗∈Rn|(x∗,〈x∗,x¯〉)∈clconeepi∑i=1mλigi(·,vi)∗.Actually, by using Theorem 1, we can prove that for each(v,λ)∈V×R+mx∗∈NF(v,λ)(x¯)⇔δF(v,λ)∗(x∗)⩽〈x∗,x¯〉⇔(x∗,〈x∗,x¯〉)∈epiδF(v,λ)∗⇔(x∗,〈x∗,x¯〉)∈clconeepi∑i∈I(x¯)λigi(·,vi)∗,and then we havex∗∈Rn|(x∗,〈x∗,x¯〉)∈clconeepi∑i=1mλigi(·,vi)∗=NF(v,λ)(x¯). Also, since F⊂F(v,λ), we can check thatNF(v,λ)(x¯)⊂NF(x¯).Now, we define the following constraint qualification:NF(x¯)=⋃(v,λ)∈J(x¯)x∗∈Rn|(x∗,〈x∗,x¯〉)∈clconeepi∑i=1mλigi(·,vi)∗,whereJ(x¯)=(v,λ)∈V×R+m∀i∈I,λigi(x¯,vi)=0. We show that this constraint qualification is a necessary and sufficient constraint qualification for surrogate min–max duality for robust optimization.Theorem 6The following conditions are equivalent:(i)NF(x¯)=⋃(v,λ)∈J(x¯)x∗∈Rn|(x∗,〈x∗,x¯〉)∈clconeepi∑i=1mλigi(·,vi)∗,for any real-valued convex function f onRn,x¯is a minimizer of f over F if and only if there existv∈Vandλ∈R+msuch thatλigi(x¯,vi)=0andf(x¯)=minf(x)∑i=1mλigi(x,vi)⩽0.First, we show that (i) implies (ii). Let f be a convex function. Then,x¯is a minimizer of f over F if and only if0∈∂f(x¯)+NF(x¯). By condition (i), there exist(v,λ)∈J(x¯)such that0∈∂f(x¯)+x∗∈Rn|(x∗,〈x∗,x¯〉)∈clconeepi∑i=1mλigi(·,vi)∗.By the Eq. (1), we can prove thatx¯is a minimizer of f over F(v,λ), that is, (ii) holds.Conversely, letv∈NF(x¯); thenx¯is a global minimizer of −v in F. From (ii), there exist(v,λ)∈J(x¯)such that〈-v,x¯〉=min〈-v,x〉∑i=1mλigi(x,vi)⩽0.For allx∈Rn∑i=1mλigi(x,vi)⩽0⇒〈-v,x〉⩾〈-v,x¯〉⇔〈v,x-x¯〉⩽0,that isv∈NF(v,λ)(x¯). So, by the Eq. (1), (i) holds.□Let g(x,v)=v2∣x1∣+max{x2,0}−2v for allx=(x1,x2)∈R2andv∈V=[0,1]. Then, by Example 4,F={x∈R2|∀v∈V,g(x,v)⩽0}={x∈R2||x1|⩽2,x2⩽0},(λg(·,v))∗(x∗)=2λv,x1∗∈[-λv2,λv2],x2∗∈[0,λ],∞,otherwise,for allv∈Vandλ∈R+, and⋃v∈V,λ∈R+clconeepi(λg(·,v))∗is not convex. So we cannot apply Theorem 4 to this function g.However, we can apply Theorem 6 to this function g. Let x=(x1,x2)=(2,−1). Then x∈F andNF(x)=⋃(v,λ)∈J(x)x∗∈Rn|(x∗,〈x∗,x〉)∈clconeepi(λg(·,v))∗.We can show thatNF(x)=⋃(v,λ)∈J(x)NF(v,λ)(x). Let v=1 and λ=1. Then, (v,λ)∈J(x) andNF(x)=NF(v,λ)(x). This implies that the above equation hold. So, we can apply Theorem 6 to this function g at x=(2,−1). Unfortunately, we cannot apply Theorem 6 to g at for all y∈F. Actually, the above equation does not hold at (2,0).The following conditions are equivalent:(i)NF(x¯)=⋃(v,λ)∈J(x¯)x∗∈Rn|(x∗,〈x∗,x¯〉)∈clconeepi∑i∈I(x¯)λigi(·,vi)∗,for any continuous function f fromRn×RptoRsuch that f(·,u) is convex for eachu∈Rpand f(x,·) is quasiconcave for eachx∈Rn, and any compact convex subsetUofRp,x¯is a minimizer of f over F if and only if there existv∈Vandλ∈R+msuch that λigi(x,vi)=0 andf(x¯)=maxu∈Uminf(x,u)∑i=1mλigi(x,vi)⩽0.We first prove that (i) implies (ii). Let f be a continuous function such that f(·,u) is convex for eachu∈Rpand f(x,·) is quasiconcave for eachx∈Rn, and letUbe a compact convex subset ofRp. By assumption,maxu∈Uf(·,u)is continuous and convex. So, by Theorem 6,x¯is a minimizer ofmaxu∈Uf(·,u)over F if and only if there existv∈Vandλ∈R+msuch that λigi(x,vi)=0 andf(x¯)=minmaxu∈Uf(x,u)∑i=1mλigi(x,vi)⩽0. By Sion’s min–max theorem, we can show thatf(x¯)=maxu∈Uminf(x,u)∑i=1mλigi(x,vi)⩽0.The converse implication can be proved as in the proof of Theorem 4.□In this section, we obtain a surrogate duality theorem and a surrogate min–max duality theorem for semi-definite optimization problem in the face of data uncertainty.Let Snbe the space of n×n symmetric matrices. For A∈Sn, A⪰0 mean that A is positive semidefinite. Let T={A∈Sn∣A⪰0} and I={0,1,…,m}. We denote the trace of the matrix A by Tr[A].Following the proof of Theorem 4, we can prove the following surrogate duality theorem for semi-definite optimization problems in the face of data uncertainty.Theorem 8LetVi,i=0,1,…,m, be a closed and convex subset ofSn,V=∏i=0mVi, andF=x∈RmA0+∑i=1mxiAi⪰0,∀Ai∈Vi,∀i∈I≠∅. Then the following conditions are equivalent:(i)⋃Z∈T,A∈V{(-Tr[ZA1],…,-Tr[ZAm],Tr[ZA0]+δ)T|δ⩾0}is closed and convex,for all usc quasiconvex function f fromRntoR¯with domf∩F≠∅, there existZ¯∈TandA¯∈Vsuch thatinf{f(x)|x∈F}=inf{f(x)|Tr[Z¯A0¯]+∑i=1mxiTr[Z¯Ai¯)]⩾0}.Let K(Z,A)={(−Tr[ZA1],…,−Tr[ZAm], Tr[ZA0]+δ)T∣δ⩾0} for each Z∈T andA∈V. Then, we can prove that⋃Z∈T,A∈VclconeK(Z,A)=⋃Z∈T,A∈VK(Z,A).Indeed, it is clear that⋃Z∈T,A∈VclconeK(Z,A)⊃⋃Z∈T,A∈VK(Z,A). LetZ¯∈TandA¯∈V. If(-Tr[Z¯A¯1],…,-Tr[Z¯A¯m])=0, thenclconeK(Z¯,A¯)={0}×cone{Tr[Z¯A¯0]+δ|δ⩾0}={0}×[0,∞)=K(0,A¯)⊂⋃Z∈T,A∈VK(Z,A)sinceZ¯A¯0is positively semi-definite. Assume that(-Tr[Z¯A¯1],…,-Tr[Z¯A¯m])≠0. Then(a¯,α¯)∈clconeK(Z¯,A¯). Then, there exist{(ak,αk)}⊂Rm×Rsuch that(ak,αk)∈coneK(Z¯,A¯)and (ak,αk) converges to(a¯,α¯). For eachk∈N, there exist λk⩾0 and δk⩾ 0 such that(ak,αk)=λk(-Tr[Z¯A¯1],…,-Tr[Z¯A¯m],Tr[Z¯A¯0]+δk). Sinceλk(-Tr[Z¯A¯1],…,-Tr[Z¯A¯m])converges toa¯, λkconverges to someλ¯⩾0anda¯=λ¯(-Tr[Z¯A¯1],…,-Tr[Z¯A¯m]). Also, λkδkconverges some δ⩾0 andα¯=λ¯(Tr[Z¯A¯0])+δ. Since T is a cone,(a¯,α¯)=λ¯(-Tr[Z¯A¯1],…,-Tr[Z¯A¯m],Tr[Z¯A¯0])+(0,δ)=(-Tr[λ¯Z¯A¯1],…,-Tr[λ¯Z¯A¯m],Tr[λ¯Z¯A¯0]+δ)∈⋃Z∈T,A∈VK(Z,A).Hence, by using Theorem 4, this completes the proof.□Following the proof of Theorem 6, we can prove the following surrogate min–max duality theorem for semi-definite optimization problem in the face of data uncertainty. We omit its proof.Theorem 9LetVi,i=0,1,…,m, be a closed and convex subset ofSn,V=∏i=0mVi, andF=x∈RmA0+∑i=1mxiAi⪰0,∀Ai∈Vi,∀i∈I≠∅. LetJ(x¯)=(Z,A)∈T×VTr[ZA0]+∑i=1mxiTr[ZAi]=0. Then the following conditions are equivalent:(i)NF(x¯)=⋃(Z,A)∈J(x¯){(-Tr[ZA1],…,-Tr[ZAm])T},for any real-valued convex function f onRn,x¯is a minimizer of f over F if and only if there existZ¯∈TandA¯∈Vsuch thatTr[Z¯A0¯]+∑i=1mxiTr[Z¯Ai¯]=0andf(x¯)=minf(x)|Tr[Z¯A0¯]+∑i=1mxiTr[Z¯Ai¯)]⩾0.

@&#CONCLUSIONS@&#
