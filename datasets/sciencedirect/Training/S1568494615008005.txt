@&#MAIN-TITLE@&#
Which algorithm should I choose: An evolutionary algorithm portfolio approach

@&#HIGHLIGHTS@&#
A novel predictive measure that predicts which algorithm is best at any given point in time. Always select the predicted best algorithm to run for the next generation.The performance of our method is very competitive if viewed as another novel “individual” algorithm.A very interesting positive synergistic effect is found between algorithms in our method.A novel performance evaluation method that makes a lot of sense when one has an absolute maximum number of function evaluations allowed in your application.Application on a Heating Ventilation and Air Conditioning design problem demonstrates that the method can be used to solve real world problems with constraints and that it performs better than choosing an algorithm randomly.

@&#KEYPHRASES@&#
Multi-method search,Algorithm portfolio,Performance prediction,Synergy,Scalability,Real world application,

@&#ABSTRACT@&#
Many good evolutionary algorithms have been proposed in the past. However, frequently, the question arises that given a problem, one is at a loss of which algorithm to choose. In this paper, we propose a novel algorithm portfolio approach to address the above problem for single objective optimization. A portfolio of evolutionary algorithms is first formed. Covariance Matrix Adaptation Evolution Strategy (CMA-ES), History driven Evolutionary Algorithm (HdEA), Particle Swarm Optimization (PSO2011) and Self adaptive Differential Evolution (SaDE) are chosen as component algorithms. Each algorithm runs independently with no information exchange. At any point in time, the algorithm with the best predicted performance is run for one generation, after which the performance is predicted again. The best algorithm runs for the next generation, and the process goes on. In this way, algorithms switch automatically as a function of the computational budget. This novel algorithm is named Multiple Evolutionary Algorithm (MultiEA). The predictor we introduced has the nice property of being parameter-less, and algorithms switch automatically as a function of budget. The following contributions are made: (1) experimental results on 24 benchmark functions show that MultiEA outperforms (i) Multialgorithm Genetically Adaptive Method for Single Objective Optimization (AMALGAM-SO); (ii) Population-based Algorithm Portfolio (PAP); (iii) a multiple algorithm approach which chooses an algorithm randomly (RandEA); and (iv) a multiple algorithm approach which divides the computational budget evenly and execute all algorithms in parallel (ExhEA). This shows that it outperforms existing portfolio approaches and the predictor is functioning well. (2) Moreover, a neck to neck comparison of MultiEA with CMA-ES, HdEA, PSO2011, and SaDE is also made. Experimental results show that the performance of MultiEA is very competitive. In particular, MultiEA, being a portfolio algorithm, is sometimes even better than all its individual algorithms, and has more robust performance. (3) Furthermore, a positive synergic effect is discovered, namely, MultiEA can sometimes perform better than the sum of its individual EAs. This gives interesting insights into why an algorithm portfolio is a good approach. (4) It is found that MultiEA scales as well as the best algorithm in the portfolio. This suggests that MultiEA scales up nicely, which is a desirable algorithmic feature. (5) Finally, the performance of MultiEA is investigated on a real world problem. It is found that MultiEA can select the most suitable algorithm for the problem and is much better than choosing algorithms randomly.

@&#INTRODUCTION@&#
Rapid advances in Evolutionary Computation (EC) have been witnessed in the past two decades. There are now many powerful Evolutionary Algorithms (EAs) that are applied to scientific and engineering applications to find good quality solutions for challenging optimization problems. Famous examples include Genetic Algorithm (GA), Evolution Strategy (ES), Evolutionary Programming (EP), Particle Swarm Optimization (PSO), Differential Evolution (DE), Ant Colony Optimization (ACO), Artificial Immune System (AIS), Cultural Algorithm (CA), Estimation of Distribution Algorithm (EDA), Artificial Bee Colony algorithm (ABC), Biogeography Based Optimization (BBO), and others [1].In spite of the proliferation of algorithms that use the evolution metaphor, for general users dealing with an optimization scenario, there is little readily available guideline of which algorithm to choose. Frequently, one resorts to words of mouth or fame of the algorithm, or try it out one by one in an exhaustive manner. The problem is compounded by the fact that individual algorithms will need parameter tuning to obtain the best performance, which is computational expensive or even prohibitive [2]. The current state of affairs motivates this paper.Usually an algorithm has a standard recommended set of parameters defined by the researchers. This set of parameters is usually arrived at after many tests on benchmark functions and practical applications. Thus for each algorithm, we use the recommended set of parameters and do not attempt the challenging problem of parameter tuning and control [2]. Instead, we believe that the multiple algorithms can be complementary: an algorithm which does not work well on one problem will be replaced by an algorithm that works well for it. So the problem is how to select algorithms.Note also that the question of choice of algorithm should be a function of the computational budget. For example, one algorithm may converge fast to a shallow local optimum, another may converge slower but to a deeper local optimum given enough time, still another may converge the slowest but eventually reach the global optimum. Which algorithm should one choose? If fitness evaluations are expensive, then only a small computational budget is allowed and the first one should be chosen. If fitness evaluations are relatively inexpensive or we have a design problem such that we can tolerate longer runs, then the second algorithm should be chosen. Finally, if fitness evaluations are cheap and we aim at solving a scientific problem of which finding the global optimum is essential, then the third algorithm should be chosen.An algorithm portfolio approach is advocated to tackle the problem. Our conceptual framework is simple: (1) put promising EAs together in a portfolio; (2) an initialization is conducted in which each algorithm is run for some number of generations until there is a change in fitness; (3) use a predictive measure to predict the performance of each algorithm at the nearest common future point; (4) select the algorithm which has the best predicted performance to run for one generation; and (5) repeat step (3) and (4) until a given computational budget is reached.Note that as the algorithm which has the best predicted performance may be different at different stages of the search, our approach will switch from one algorithm to another automatically and seamlessly.In a nutshell, we propose to choose, at any point of the search, the algorithm which has the best predicted performance to run (for one generation). A typical scenario of running our algorithm is that after some trials in which each algorithm runs in parallel and interacts indirectly, an algorithm which has the best predicted performance by a considerable margin stands out, and only it is run for quite some time. If it is a very good algorithm that excels in small, medium and large budgets, then only it will run from then on. However, if it is an algorithm that converges fast to a local optimum, as discussed above, then it will run for awhile and gradually the predicted performance will not be as good compared with other algorithms. At which time a second algorithm, which has a better predicted performance, will take over. Like changes would occur as the search progresses.A novel online performance prediction metric is proposed to advise which algorithm should be chosen to generate the next new generation of solutions. The metric is parameter-less – it does not introduce any new control parameter to the system – thus avoiding the difficult parameter tuning and control problem [2]. We name our algorithm Multiple Evolutionary Algorithm (MultiEA). It is designed for single objective optimization.We choose four algorithms to compose the portfolio of MultiEA. They are (1) Covariance Matrix Adaptation Evolution Strategy (CMA-ES); (2) History driven Evolutionary Algorithm (HdEA); (3) Particle Swarm Optimization (PSO2011); and (4) Self adaptive Differential Evolution (SaDE).These algorithms are chosen because they represent current state of the art methods:CMA-ES [3] is one of the most powerful EAs available. It adapts its search strategy by evolving the covariance matrix of the current solutions. The idea is to increase the variance of the directions in which the search is successful and vice versa. A nice fundamental property which is unique to CMA-ES is its invariance to linear transformations of the search space.HdEA [4] is a novel EA that uses the entire search history to make decision. The history is stored in a binary space partitioning (BSP) tree. When a new solution is generated by the EA, it finds the region it is contained within efficiently by traversing the tree to its leaf node. The local gradient is approximated by the local history information in the tree and a parameter-less gradient descent is executed to find a mutated solution. The performance of HdEA is tested on thirty four benchmark functions with dimensions ranging from 2 to 40. It outperforms eight benchmark evolutionary algorithms, which includes a Real Coded Genetic Algorithm (RCGA), classic DE, two improved DEs, CMA-ES, two improved PSOs, and EDA.PSO introduces a new search paradigm which simulates the swarm behavior in birds and other animals. The velocity of each individual particle is modulated by both the historical local best position found by the particle and the historical global best position found by the whole swarm. Since its inception, two revised “standards” have been promulgated: PSO2007 and PSO2011. PSO2007 abandons the global best position concept. Instead, each individual chooses its own set of informants, and follows its local best as well as the global best amongst the informants. The idea is to distribute the search effort to avoid prematurely converging on the global best. It performs much better than the classic PSO. The PSO2011 [5] further improves PSO2007 by making the PSO more immune to linear transformations in the search space.DE is also a powerful EA. Recently improved DE variants have won many EC competitions [6]. A fundamental idea in DE is to use the sum of a vector and the scaled difference of another two vectors to form the mutant vector. The recently proposed SaDE [7] employs four popular DE mutation strategies. During any one generation, DE maintains a set of trial vectors in its population pool. For each trial vector, a dice is rolled to select a DE strategy. Initially, all strategies have the same probability of being selected. Records are kept of the outcome of employing the strategies. A strategy is regarded as successful if it produces an offspring that survives into the next generation; otherwise, it is regarded as a failure. The success probability for each strategy is computed for the previous LP generations, where LP is the user defined learning period. The probability of selecting a particular strategy is proportional to its success probability. This probability is lower bounded by a small constant to preclude a strategy of attaining zero success probability and henceforth eliminated from all future considerations. SaDE shows remarkable performance. It outperforms classic DE and several recent adaptive DEs.Though these are good EAs, it is also well acknowledged that they may not perform well for certain functions. For example, DE is known to be doing less well for non-separable functions [8] of which CMA-ES excels, and it is well known that CMA-ES performs poorly for the Rastrigin function if no artificial remedy such as enlarging the population is taken [9]. Finally, by the well known no free lunch (NFL) theorems, it is unlikely to find an algorithm which would do well for all problems [10,11]. So it would be of interest to see whether a portfolio approach can be successful in identifying the correct algorithm for a problem.Experiments are conducted on 24 benchmark functions [12]. The effectiveness of MultiEA is demonstrated by comparing with four multiple algorithm approaches. They are (i) Multialgorithm Genetically Adaptive Method for Single-Objective Optimization (AMALGAM-SO) [13]; (ii) Population-based Algorithm Portfolio (PAP) [14]; (iii) a multiple algorithm approach which randomly chooses an EA in each run (RandEA); and iv) a multiple algorithm approach with multiple algorithms running in parallel, in which the total budget is shared evenly amongst the algorithms (ExhEA). It is found that MultiEA outperforms the four algorithms.Experiments are also conducted to compare MultiEA with the four individual algorithms (i.e., CMA-ES, HdEA, PSO2011, SaDE) in the portfolio in a neck to neck manner. This is a challenging test for MultiEA for the following reason: Given a problem, there will be an algorithm amongst the four which performs the best for that problem [10], though we do not know which. For MultiEA, it inevitably has to “waste” some fitness evaluations before deciding on which is the best and then execute it. Thus our aim is to test whether MultiEA is a competitive algorithm if viewed as a standalone EA. Of course, if it is, we have good reason to adopt MultiEA as it can literally combine the strengths and avoids the pitfalls of its component EAs. It is found that MultiEA is very competitive in the neck to neck comparison.Through this experiment, an interesting positive synergic effect is discovered, namely, MultiEA can sometimes perform better than the sum of its individual EAs. This synergic effect embodies the principle of “one plus one is larger than two” or “the whole is larger than the sum of parts”.The scalability of MultiEA is studied. Since MultiEA is composed of individual algorithms, the best we should hope for is that it scales as well as the best algorithm in the portfolio. We found that this is indeed the case.Finally, MultiEA is applied to a real world problem. It is found that MultiEA is able to select the most suitable algorithm for the problem.The rest of the paper is organized as follows: Section 2 reviews past works on this problem. Section 3 introduces the MuliEA and the new performance prediction metric. Section 4 reports the experimental results. Section 5 gives the conclusions, limitations of the work, and directions for future research.The idea that combining multiple algorithms into a portfolio may result in superior performance than using a single algorithm and that it is possible to improve performance by learning dates back to 1963 [15].Huberman et al. [16] studied the benefits of using a portfolio of algorithms from an economical viewpoint by making analogies with a financial investment portfolio with a user defined overall risk. Here, stochastic algorithms that always find the correct solution are considered (i.e., Las Vegas algorithms). The risk of an algorithm is intuitively defined as its variance. Using the same computational time budget, an algorithm portfolio that allocates fractions of the budget to each algorithm is compared with a single algorithm that uses the whole budget. Assuming that the probability distribution of the time to obtain the correct solution is known for each algorithm, they report a systematic method of allocating the fractions that can obtain simultaneously lower expected time and lower risk. Other early multiple agent architectures include the A-Teams [17], which share a common population.Memetic algorithm (MA) [18] uses a hybrid approach. An EA is run in the background. Periodically, another algorithm is applied to improve selected individuals. This algorithm is usually a problem specific local improvement heuristic. The class of MAs can be considered as a two algorithm approach, with algorithms applied in sequence.An interesting related paradigm is algorithm selection [19]. It aims to extract some distinguishing features from the problem. Machine learning techniques are used to map these features to the most suitable algorithm (e.g. see [20]).Fukunaga [21] considered an algorithm portfolio with independent algorithms, i.e., there is no information exchange between algorithms. The utility of an algorithm is defined as the expected value of the best fitness found, subtracted by the weighted variance, with a user defined weighting factor. He considered optimizing the portfolio for tackling problems in a user specified problem class. The total computation time is divided into small, equally sized units. Several instances of the problem are run, and the utility of each algorithm as a function of units allocated is recorded in a performance database. The recorded information is used to build up a bootstrap probability distribution. Using the distribution, all possible combinations of the algorithm portfolio are then evaluated and the optimal algorithm portfolio is found for the problem class. The multiple algorithms are GAs with different control parameter values. The approach is applied to the traveling salesman problem. It is found that the portfolio approach performs better than a single GA optimized using parameter tuning.Gagliolo et al. [22] reported an interesting approach. Their portfolio consists of a set of GA with different parameters. The convergence curve of each GA is plotted, and the curve is extrapolated using linear regression with an adaptive sliding window to estimate the time needed to reach a user-defined fitness level. A unit of resource is distributed by an allocator such that GA with small estimated time and has been run for substantial time is given more resources. All GAs are then run in parallel using the resource allocated. Then the process iterates.Vrugt et al. [13] reported a self adaptive multi-method search known as AMALGAM-SO. The idea is to use a set of q EAs {A1,…,Aq} that have a common population with N individuals. Initially, the number of offspring {N1,…,Nq} that each algorithm produces is fixed by the user, where∑i=1qNi=N. After the N offspring are generated, the N parents and the N offspring are sorted by decreasing fitness. Individuals with higher fitness are inserted into the population pool of the next generation one by one. To maintain diversity, an individual will be inserted only if its Euclidean distance is larger than a user-specified threshold compared with all existing individuals in the pool. The process goes on until N new individuals are generated, which makes up the next generation. Each algorithm has its own stopping criterion. The multimethod search stops when one of the stopping criteria is fulfilled. Then the search is re-run with the population size doubled and the number of offspring is recalculated asNi=NΨi/∑i=1qψk, where Ψiis the number of generations for which algorithm i is responsible for fitness improvement. To prevent an algorithm to be driven extinct, the minimum Ni/N ratio is fixed by the user. To pass information from one run to the other, the best found solution is included as one of the individuals in the otherwise randomly generated initial population of the next run, provided that a numerical condition is fulfilled. On simple uni-modal problems, the method obtains similar performance as existing individual algorithms. For complex multi-modal problems, the method is superior.Peng et al. [14] proposed an algorithm called Population-based Algorithm Portfolio (PAP). q EAs are run in parallel, each with their own populations. There are two important parameters in the algorithm: migration interval I and migration size s. A migration across population occurs every I generations. The migration process is as follows: for each algorithm, the s best individuals are found amongst all the individuals in the remaining (q−1) populations. These s individuals are appended to the population. Then the worst s individuals in the population are discarded. It is experimentally found that PAP performs better than running each individual algorithm with the same computational budget. The authors conclude that the performance gain is due to the synergy of the algorithms through the migration process.For multiobjective problems, Vrugt and Robinson [23] proposed a self adaptive multi-method search named AMALGAM. A common population and a set of algorithms are used. In each generation, for each algorithm i, the number of offspring Nithat successfully goes into the next generation is recorded. The number of offspring that each algorithm can generate in the next generation is made proportional to Ni, that is, proportional to its reproductive success rate. To avoid an algorithm becoming extinct, an algorithm is guaranteed to generate a user-defined minimum number of offspring. In common with [23], Grobler et al. [24] also used a common population. They studied single objective problems. In their method, each individual in the population may select different algorithms to produce offspring. Different selection methods are compared in [24].Recently, Yuen and Zhang [25,26] and independently Tang et al. [27] propose an original research problem: given a known set of problems, how to find an automatic method to compose a portfolio. Clearly, the composition result is a function of the portfolio algorithm used. In [25,26], the algorithm proposed in this paper is used and in [27], the PAP is used.Within the context of single algorithm, parameter control techniques [2] have been extensively investigated. Under this branch of research, operator selection and its associated credit assignment have been investigated. In the context of multiple algorithms with interaction, operator selection has some analogies with algorithm selection. The researches in the above have mainly used probability matching for algorithm selection. Let pmin be the user defined minimum probability for an operator. A known problem of probability matching is that with q operators, the maximum probability of selecting an operator pmax is 1−(q−1)pmin. It degrades the performance because the best operator that delivers the highest expected reward should always be chosen, instead of being chosen with a probability [28]. Thierens [28] addressed this problem by using a winner take all strategy; the best algorithm is moved toward pmax, while others are uniformly moved toward pmin. The movement rate is governed by a user defined greediness factor. Calculations and experiments in [28] find that for a large range of values of this factor, it is better than using probability matching. Recently, Fialho et al. [29] extended this idea by formulating the problem as a dynamic multi-armed bandit problem [30,31]. At any one instance, the algorithm to execute is the one which has the highest average reward so far. Unfortunately, the model is quite complicated and as the reward distribution may change over time, an additional change detection method is needed. Finally, it is unclear whether the multi-armed bandit paradigm is an appropriate model for algorithm selection.Recently, an online racing algorithm, Max-Race Portfolio (MRP), is proposed to select algorithms in a portfolio [32]. Different from other existing approaches, the racing algorithm aims to pinpoint algorithms that are statistically unlikely to succeed online. When sufficient statistical evidence is obtained, that algorithm is removed from the portfolio. The statistical model is based on extreme value theory, which in itself has several user-defined parameters which require tuning. Another potential problem on all racing approach is that it permanently removes an algorithm, which may later be found to be very useful when there is a larger budget. Experimental results in [32] show that MRP outperforms AMALGAM-SO, PAP and MultiEA on problems generated by a Gaussian landscape generator (using a different suite of algorithms to compose the portfolio and different experimental settings.) The performance remains to be verified for problems which do not share a common way of generation. In particular, whether the parameters in the extreme value theory are problem dependent needs to be further investigated.1.Early algorithm portfolio approaches (e.g. [16]) are designed for Las Vegas algorithms. However, the usual applications of EA do not require or guarantee the finding of the global optimal solution (i.e., they are Monte Carlo algorithms).Some algorithm portfolio approaches (e.g. [21]) require a problem class to have been well defined and previous history is used to advise the algorithm selection. This is not applicable to the scenario in which a multiple algorithm EA is requested to handle a single problem instance with little or no applicable prior knowledge.All existing algorithms invariably introduce more control parameters into the algorithm (e.g. the risk in [16], the initial number of offspring for each algorithm, the minimum probability, diversity threshold, stopping criteria for each algorithm, and the condition for the best found solution in [13], the two migration parameters in [14], the resource to be allocated in each iteration in [22], the minimum number of offspring [23], the number of iterations elapsed before the next selection [24], the parameters in the extreme value theory [32]). A careful tuning of these control parameters is needed. Such tuning is computationally expensive [2]. Even when this has been done, there is no guarantee that it would work well in a new unknown problem [11].Many multiple algorithm researches have used probability matching for algorithm selection, whereas some recent research [28] argues that it is not the best approach. On the other hand, the related researches within parameter control introduce new methods for operator selection and credit assignment [2]. This involves introducing new control parameters, which itself requires a non-trivial parameter tuning.While recent researches [13,14,23,24] have demonstrated that a multiple algorithm approach is promising, a self adaptive approach is taken, i.e., incorporate the algorithm selection decision within the evolutionary process. It is known that a self adaptive approach does not mitigate completely the problem of premature convergence, as the whole population may be focused on only a part of the search space which then biases the future available search strategies. Moreover, offspring generated from different algorithms may mislead each other.Another disadvantage of the self adaptive approach is that it sheds no physical insight on why an algorithm is good for a problem, as it does not have an easily understood physical model for credit assignment. Also, the process sheds no new insight on which algorithm is good for which problem.In this research, we present a novel alternative approach which overcomes the above limitations. It does not require the global optimal solution to be known; it works on a single problem instance; it does not introduce any new control parameter; it uses a winner take all strategy rather than probability matching; as the algorithms run independently, it does not suffer from the limitations of a self adaptive approach, and offspring from different algorithms will not mislead each other. An easily understood physical model for credit assignment based on fitness prediction is introduced. It also gives direct insights on “which algorithm is good for which problem” as a function of computational budget.Existing multiple algorithm approaches [13,14,23,24] find a good synergy by self adaptive information exchange between algorithms through a common population. The present approach aims to select the best algorithm at each instance by predicted future performance, based on past history. It switches between algorithms dynamically as new search information is obtained which enables revised predictions to be made. A synergy is achieved indirectly. Our approach is complementary to existing works.The most closely related approach to the present paper is [22]. Both use predicted performance to allocate resources, and both advocate using algorithms which are independent and do not interact with each other. However, in our approach, we do not require the user defined required fitness level, which in general is impossible to know a priori. Moreover, we do not extrapolate to a far future; we merely predict at the nearest common future point. The rationale is that we reckon that prediction into the far future is very difficult and error prone (see Fig. 2). Furthermore, there is also a difference in design philosophy. Our algorithm does not allocate a fixed amount of resource and distribute to the component algorithms, which are run in parallel; we allocate all resources to the algorithm which is predicted to perform best at the common future point, execute that algorithm one generation and then iterates and predicts again. This allows prediction to be updated as soon as new data arrives. Finally, instead of an adaptive sliding window approach, we attempt to use all historical data in the convergence curve of the algorithm to make a better prediction.Recently, we have reported the preliminary result of this paper in [33]. Instead of the benchmarking suite [12], we have selected another set of algorithms to form our portfolio and applied MultiEA to another widely used benchmarking suite CEC 2005 [34]. The conclusions reached corroborate well with the result in this paper. In particular, it is found that MultiEA is highly competitive in a neck to neck comparison with the individual algorithms which form the portfolio. Also, MultiEA is superior over a baseline EA which randomly selects algorithms to execute. It is also found that MultiEA outperforms AMALGAM-SO and PAP.In this paper, we have applied MultiEA to the benchmarking suite [12] with a portfolio composed of a different set of algorithms. The experimental results agree with that in [33]. This gives an independent verification that the findings in this paper are transferrable to other problem-algorithm contexts.Section 4.5.1 compares MultiEA with other existing algorithm portfolios, namely, AMALGAM-SO and PAP, as well as two baseline algorithm portfolios which randomly selects algorithms. Section 4.5.2 is a neck to neck comparison of MultiEA with individual algorithms that make up the portfolio.The following new results are also reported in this paper: a positive synergic effect of MultiEA is reported in Section 4.5.3. The scalability of MultiEA is studied in Section 4.5.4. The performance of MultiEA when applied to a real world application is investigated in Section 4.5.5.This research focuses on single objective continuous parameter optimization with dimension D. Without loss of generality, let the optimization be a minimization, and assume one has a problem P for which we have no priori knowledge about which algorithm is the best. Assume that q EAs have been chosen and placed within a portfolio AP={A1, …, Aq}. The q algorithms are executed independently and there is no information exchange between them.Let αibe the number of generations that algorithm Aihas run. Let fi(j) be the best (i.e., smallest) fitness found by Aiat generation j. Whether the best so far solution participates in the evolution, it is reasonable to keep a copy of it. Hence fi(k)≤fi(j) iff k≥j. An indicator of the performance is the convergence curve Ci={(j, fi(j))}, which records the entire history of the convergence trend. A typical Ciis shown in Fig. 1.The relative performance of the algorithms can be compared by comparing the convergence curves. This is also the standard practice in many EC papers. A popular measure used in the EC community is comparing fi(j) while keeping the total number of evaluations a constant. It simply means running each algorithm by a fixed user defined number of evaluations and then chooses the winner. This exhaustive approach is very computationally expensive. It can be considered as a generalized form of parameter tuning, only that in this case, the parameter is replaced by algorithm. Parameter tuning is useful if P is a typical problem of the class of problems to be solved. However, in practical optimization scenarios, usually the class of problems is not well defined, and it is impossible to know beforehand what a typical problem is.It is more reasonable to predict fitness at some future point common to all algorithms and choose the winner. This requires extrapolating the convergence curve. Fig. 2shows extrapolation using fitting of (a) exponential; (b) polynomial and (c) Taylor series.It shows some interesting characteristics of extrapolation using standard functions. If one wishes to extrapolate a little, then these functions perform well. However, if the extrapolation is far into the future, then these functions give bad results. Moreover, these extrapolations do not use the knowledge that the curve is non-increasing.This paper proposes a novel prediction measure to tackle this problem. Let revisit the convergence curve C={(j, f(j))}. For clarity, we have dropped the subscript i. Define C(l)={(α-l, f(α-l)),…,(α, f(α))}as the sub-curve which includes all the data points from the (α-l)th generation to the αth generation, for which l is the history length. For each sub-curve C(l), a linear regression is done to find(1)arg(l1,l2)min∑(x,y)∈C(l)(y−(ax+b))2which gives a straight line with slope a and y-intercept b that minimizes the mean squared error. The predicted fitness using data points in C(l) for future generation t is(2)pf(t,l)=at+bSince l may take on values from 1 to α−1, we have α−1 predicted values pf(t, 1) to pf(t, α−1). We treat these predicted values as sample points of an unknown distribution, and fit a bootstrap probability distribution bpt(t) to it. In this paper, the distribution is computed by Matlab statistical toolbox function ksdensity. (Strictly speaking, ksdensity has some parameters that can be varied; in this paper, we have used the standard values provided in MATLAB for a “standard” interpolation.) We predict the fitness at t by sampling from bpd(t) to get the predicted fitness pf(t).The underlying idea is to treat each linear regression model C(l) with equal probability. The distribution model is designed to model the uncertainty in the prediction. In the special case that the convergence curve is indeed linear, then the bootstrap distribution will be a single spike. Since more recent points appear in more regression models, implicitly a heavier weighting is put on more recent points and vice versa.Now for each algorithm i, the predicted best fitness pfi(t) (t>αi) predicts the fitness of the algorithm if it were evaluated t generations.Assume that the population size m of the algorithms is equal. Then the nearest future tiis αi+1. This is the nearest future in which algorithm i has run one more generation. In this case, the unit of time is the number of generations. Let tmin=max(t1, …, tq). This is the nearest common future of all the algorithms. The algorithm which will generate the next generation isargimini(pfi(tmin)), where pfi(tmin) for each i is sampled from its bootstrap probability distribution bdpi(tmin).It has a clear physical meaning: The algorithm that will generate the next generation is one for which the predicted best fitness is the smallest if they were evaluated the same number of generations at the nearest common future point.In general, let mibe the population size of algorithm i. Then the nearest future is miti, of which ti=αi+1. This is the nearest future in which algorithm i has run one more generation. In this case the unit of time is the number of fitness evaluations. Let tmin≡(mt)max=max(m1t1, …, mqtq). This is the nearest common future of all the algorithms. The algorithm which will generate the next generation isargimini(pfi(tmin)), where pfi(tmin) for each i is sampled from its bootstrap probability distribution bdpi(tmin).Note that only efficient algorithms will be selected often; inefficient algorithms will be selected sparsely since its predicted fitness is large. Also, note that the above metric is parameter-less. It does not introduce any additional parameter into the algorithm portfolio. Moreover, the metric uses a winner take all strategy, thus circumventing the problem of probability matching.In the above, a linear regression is used for the estimation. A nonlinear kernel (e.g. an exponential function) may conceivably be used. However, finding the parameters of the nonlinear kernel in general requires a nonlinear optimization, which requires its own optimization methods. Such a method would also be much more computationally intensive. Thus we have decided to use the simpler linear kernel.In principle, one can abstract the bootstrap probability distribution by its statistics (e.g. mean or median), and make the prediction deterministic. We do not take such an approach because such summary statistics throws away much useful information. For example, αi−1 data points will be replaced by a single number (mean or median), which would throw away information contained in αi−2 points. Also, a probability distribution is ideal to represent the fact that the prediction is by nature stochastic and uncertain.In [33], we experiment with mean and median as alternative measures and confirm the above prediction that they are not good predictive measure. Amongst several heuristic measures that we have compared in [33], we find that the predictive measure reported in this paper has the best performance. Also, a predictive measure that uses the raw histogram instead of ksdensity to construct the probability distribution has only slightly worse performance. Note that the use of the raw histogram makes the algorithm truly parameter-less.One has to ensure that each algorithm is able to make a non-trivial prediction. Thus each algorithm Aiis run until there is a decrease in the best fitness, i.e., until the smallest αisuch that fi(αi−1)≠fi(αi). Note that the initial number of generations αiis determined automatically, without introducing any extra control parameter.We illustrate the above concepts by a hypothetical scenario. In this example, MultiEA consists of three candidate EAs: A1, A2, and A3, with population sizes m1=15, m2=20 and m3=30, respectively. Suppose MultiEA has already run 18 generations, of which α1=7 generations is by A1, α3=6 generations is by A2 and α3=5 generations is by A3. Thus A1, A2 and A3 have been evaluated m1α1=105, m2α2=120 and m3α3=150 solutions, respectively. Fig. 3shows the convergence curves of the EAs. At this point, we have to make a choice of which algorithm to run next. As tmin=(mt)max=max(105+15,120+20,150+30)=180, we need to predict the fitness values of A1, A2 and A3 after the 12th (180 evaluations), the 9th (180 evaluations) and the 6th generations (180 evaluations), respectively. In this case, the number of evaluations is the same for the three algorithms. If not, an algorithm is predicted at the number of generations which gives the number of evaluations smaller than but closest to 180.The selection of EA for the 19th generation of MultiEA starts from obtaining the distributions of the predicted fitness values of A1. Since A1 has been executed 7 generations, we are going to formulate 6 prediction models C1(1), …, C1(6) for which 6 predicted fitness values pf1(tmin, a) (tmin=180 evaluations, a=1, …, 6) are guessed (Fig. 4(a)–(f)). For each of the sub-figures, a (gray ○ or black ○) circle represents the best fitness value at each generation; a black circle represents the data points used for formulating the linear regression model at a particular history length l. The dashed line represents the linear regression prediction model. Symbol ‘’ at each sub-figure represents the predicted fitness pf1(tmin, l). Using these 6 predicted fitness, we construct the bootstrap probability distribution bdp1(tmin) (Fig. 5(a)).We then repeat the previous procedures to construct the bootstrap distributions bdp2(tmin) and bdp3(tmin) of the estimated best fitness value of A2 and A3 at the 9th and the 6th generations (Fig. 5(b) and (c)). Afterwards, for each algorithm, the predicted fitness is randomly sampled from the corresponding probability density distribution. For example, suppose the random sampled values for A1, A2 and A3 are pf1(tmin)=−1.5, pf2(tmin)=−5.1, and pf3(tmin)=−5.8, respectively. Since pf3(tmin) is the smallest, A3 is executed a generation and its convergence curve is lengthened to α3=6. In the next generation of MultiEA, as the convergence curve of A3 is lengthened, we have to re-formulate the prediction model of A3, and construct the corresponding revised bootstrap distribution.We summarize the proposed method by presenting it in algorithmic format after Fig. 5.Algorithm (MultiEA)Input: A portfolio of q EAs AP={A1,…,Aq}; Aihas population size mi; single objective minimization problem P with dimension D; maximum number of evaluations N1. for i=1 to qrun Aiuntil there is a change of fitness. Let αibe the number of generations that Aihas run2. compute the nearest common future point tmin≡(mt)max=max(m1t1, …mqtq), where ti=αi+13. for i=1 to q{construct convergence curveCi={(j,fi(j))j=1,…αi}construct sub-curvesCi(1),…,Ci(αi−1)for each sub-curve Ci(l){least square line fit to get line parameters (a,b)predict the fitness at the smallest common future point pfi(tmin, l)}use the αi−1 sample points pfi(tmin, l) (l=1, …, αi−1) to construct bootstrap probability distribution bpdi(tmin)Sample bpdi(tmin) to get pfi(tmin)}4. choose the algorithm with indexargmini(pfi(tmin)), which has the best predicted performance5. run it for one generation. αi←αi+16. Record the best-so-far solution sbest found by the portfolio7. Stop if total number of evaluations≥N. Otherwise goto Step 2OutputBest solution found sbest

@&#CONCLUSIONS@&#
Evolutionary algorithms (EAs) have proliferated as one of the best tools for solving the ubiquitous optimization problem in the real world. Many excellent EAs have been reported in the past. However, there arises an important open problem which bewilders researchers and engineers alike: Confronted with an optimization problem, which algorithm should I choose?This paper is an attempt to answer the above question. A novel algorithm, known as Multiple Evolutionary Algorithm (MultiEA), is proposed for single objective optimization. A portfolio is first formed by selecting several state of the art EAs. A novel predictive measure is reported to predict the performance of individual algorithms if they were extrapolated to the same number of evaluations in the nearest future. The algorithm with the best predicted performance is chosen to run for one generation. New search information is received and the history is updated. The predicted performance of the algorithms is updated and the algorithm with the best predicted performance is re-selected, which may or may not be the same algorithm in the last generation. Experimental results show that the measure is stable and is a reasonably effective predictor. The idea is simple and natural. It is parameter-less. It does not introduce any new control parameter to the algorithm, thus avoiding the challenging parameter tuning and control problem [2].Each component algorithm retains and uses their recommended set of parameters, which is the standard practice in the evolutionary community. No parameter tuning and control is required. Instead, our approach expects individual algorithms to play complementary roles. Different algorithms which excel in different problems will stand out when required. Moreover, as different algorithms may be the best for different computational budgets in the sense of absolute maximum number of fitness evaluations, the approach fully allows the selection of the best algorithm given a fixed computational budget, as well as automatic algorithm switching as the budget varies.Some recent multiple algorithm portfolio approaches use a common population and a self adaptive approach to apportion different algorithms at different stages. Compared with these approaches, ours use a distinctly different philosophy, namely, we concentrate on selecting the best algorithm given the current computational budget and predicted performance. We believe that these two approaches are complementary, and this paper provides a fresh alternative.A novel performance evaluation measure is also proposed. In many previous papers, the algorithms are compared after a fixed number of evaluations. This is not very fair as some algorithms may have quickly converged to the target fitness and they should be declared superior to slower algorithms. Our measure overcomes this limitation by dynamically changing the number of evaluations on each run based on the first to target concept and an absolute maximum computational budget limit concept. The measure is general and can be applied to other EA and non-EA comparison context.Four algorithms, namely, Covariance Matrix Adaptation Evolution Strategy (CMA-ES), History driven Evolutionary Algorithm (HdEA), Particle Swarm Optimization (PSO2011), and Self adaptive Differential Evolution (SaDE) are selected to compose our portfolio in MultiEA in our experiments. These algorithms are selected because they are state of the art algorithms, and have significantly different characteristics, strengths and pitfalls.Experimental results on 24 benchmark functions show that MultiEA outperforms (i) Multialgorithm Genetically Adaptive Method for Single-Objective Optimization (AMALGAM-SO); (ii) Population-based Algorithm Portfolio (PAP); (iii) a simple multiple algorithm approach which chooses an algorithm randomly (RandEA); and (iv) a simple multiple algorithm approach which divides the computational budget evenly and execute all algorithms in parallel (ExhEA). This shows that MultiEA outperforms existing portfolio approaches and the predictor is functioning well.A more stringent test is also performed by comparing MultiEA with the individual algorithms (i.e., CMA-ES, HdEA, PSO2011, SaDE) in a neck to neck manner. It is found that MultiEA is highly competitive. In the majority of cases, it ranks second, which is to be fully expected as it does have to spend some budget to identify the best algorithm within the portfolio. This means that the predictive measure that we propose is doing a competent job. Also, it suggests that MultiEA is competent as a novel standalone EA. In particular, MultiEA, being a portfolio algorithm, is sometimes even better than all its individual algorithms, and has more robust performance.Surprisingly, we find that for some cases, MultiEA ranks first, which leads to the discovery of an interesting positive synergic effect. This shows that the portfolio strategy of MultiEA can sometimes outperform even the best individual algorithm. Literally, “one plus one is larger than two,” or “the whole is larger than the sum of parts”. This gives interesting insights into why a portfolio approach is a good approach.A good algorithm should scale up nicely. A scalability test is also performed for MultiEA. Realistically, one should be content that MultiEA scales as good as the best performing component EA in the portfolio. We found that it is indeed the case.MultiEA is also applied to a real world application in Heating Ventilation and Air Conditioning (HVAC) engineering [35]. It is found that MultiEA can determine the most suitable algorithm in the portfolio for the problem, and the performance is much better than randomly choosing an algorithm. It also demonstrates that MultiEA is effective in problems involving constraints.Though the scope of this paper is restricted to portfolio of EAs, it is a general framework and good non-EA (e.g. Efficient Global Optimization algorithm (EGO) [37]) is also welcome.Theoretically, MultiEA, by virtue that it is just another EA, will still be under the no free lunch (NFL) theorems [11]. There are likely to be situations that MultiEA gets a bad result. However, the premise of the NFL is that all computable problems are equally likely to occur. It is hardly the case in the real world [38]. In fact, the promise of multi-method search is that by using many search metaphors inspired by nature, mathematics, sciences, arts, etc., one would build up better search strategies that are more able to deal with real world problems. It may perhaps be likened to synergy of individuals with different talents in a heterogeneous society.In this paper, we have used our personal expertise and judgment to select algorithms to compose the portfolio. In principle, if we have had selected a very bad algorithm, it would not affect the performance of MultiEA too much. This is because MultiEA will only select the best algorithm to run at any one time.Recently, a systematic and principled approach to solve this problem has been developed to solve this problem [25–27]. Given a set of (evolutionary or non-evolutionary) algorithms and a set of existing problems that have been encountered, the methods in [25–27] may be used to compose a suitable portfolio that performs well on average. Such research complements this research nicely.Another interesting future research is how to design algorithms with restarts in a portfolio. In the first restart onwards, the predictor may make use of information in the previous runs to arrive at better prediction. The pros and cons of making use of such information deserve further study.We have not considered the overheads due to solution generation costs. In many practical optimization, the fitness evaluation costs is much greater than the solution generation costs and the latter can be ignored. However, in the case that the solution generation costs are substantial, this factor needs to be taken into account. Different algorithms may have different solution generation costs due to its different algorithm complexity. For example, the solution generation cost of EGO [37] increases nonlinearly with the number of evaluations. Since the overall computational budget for the problem is fixed, a more sophisticated predictive measure will take into account of the differing solution costs when suggesting the most suitable algorithm at any particular point.In this work, we have concentrated on single objective optimization problem for which the fitness function is single-valued and has no uncertainty. While this is true for some real world applications, there are other applications that involve more complex considerations. As discussed in [39,40], (1) real world applications frequently involve multiple, potentially conflicting objectives; (2) their fitness functions may have time varying uncertainties arising out of different factors; (3) sometimes robust designs insensitive to parameter changes are required rather than optimized designs; (4) some fitness functions may have qualitative components or may require interactions with humans to obtain a meaningful values. Extensions and adaptation of MultiEA are required to tackle the issues above.Future research may also be done on developing better prediction measures, which predicts more accurately which algorithm should be applied at the nearest common future point.The initial motivation of this paper is a question from a colleague: there are so many evolutionary algorithms claimed to be powerful and useful, which algorithm should I choose? As this question comes from a user of evolutionary computation for optimization of practical problems, we hope that our paper has answered him in one way. We encourage more research in this interesting and important direction.