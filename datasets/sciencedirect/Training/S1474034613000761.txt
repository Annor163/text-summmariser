@&#MAIN-TITLE@&#
Vision-based action recognition of earthmoving equipment using spatio-temporal features and support vector machine classifiers

@&#HIGHLIGHTS@&#
We present a computer vision based method for equipment action recognition.Our vision-based method is based on a multiple binary SVM classifier and spatio-temporal features.A comprehensive real-world video dataset of excavator and truck actions is presented.We achieve accuracies of 86.33% and 98.33% for excavator and truck action classes.The presented method can be used for construction activity analysis using long sequences of videos.

@&#KEYPHRASES@&#
Computer vision,Action recognition,Construction productivity,Activity analysis,Time-studies,Operational efficiency,

@&#ABSTRACT@&#
Video recordings of earthmoving construction operations provide understandable data that can be used for benchmarking and analyzing their performance. These recordings further support project managers to take corrective actions on performance deviations and in turn improve operational efficiency. Despite these benefits, manual stopwatch studies of previously recorded videos can be labor-intensive, may suffer from biases of the observers, and are impractical after substantial period of observations. This paper presents a new computer vision based algorithm for recognizing single actions of earthmoving construction equipment. This is particularly a challenging task as equipment can be partially occluded in site video streams and usually come in wide variety of sizes and appearances. The scale and pose of the equipment actions can also significantly vary based on the camera configurations. In the proposed method, a video is initially represented as a collection of spatio-temporal visual features by extracting space–time interest points and describing each feature with a Histogram of Oriented Gradients (HOG). The algorithm automatically learns the distributions of the spatio-temporal features and action categories using a multi-class Support Vector Machine (SVM) classifier. This strategy handles noisy feature points arisen from typical dynamic backgrounds. Given a video sequence captured from a fixed camera, the multi-class SVM classifier recognizes and localizes equipment actions. For the purpose of evaluation, a new video dataset is introduced which contains 859 sequences from excavator and truck actions. This dataset contains large variations of equipment pose and scale, and has varied backgrounds and levels of occlusion. The experimental results with average accuracies of 86.33% and 98.33% show that our supervised method outperforms previous algorithms for excavator and truck action recognition. The results hold the promise for applicability of the proposed method for construction activity analysis.

@&#INTRODUCTION@&#
Equipment activity analysis, the continuous process of benchmarking, monitoring, and improving the proportion of time construction equipment spend on different construction activities, can play an important role in improving construction productivity. A combination of detailed assessment and continuous improvement can help minimize idle times, improves operational efficiency [1–5], saves time and money [6], and results in a reduction of fuel use and emissions for construction operations [7,8]. Through systematic implementation and reassessment, activity analysis can also extend equipment engine life and provide safer environments for equipment operators and workers.Despite the benefits of activity analysis in identifying areas for improvement, an accurate and detailed assessment of work in-progress requires an observer to record and analyze the entire equipment’s actions for every construction operation. Such manual tasks can be time-consuming, expensive, and prone to errors. In addition, due to the intra-class variability on how construction tasks are typically carried out, or in the duration of each work step, it is often necessary to record several cycles of operations to develop a comprehensive analysis of operational efficiency. Not only the traditional time-studies are labor intensive, but they also require a significant amount of time to be spent on manually analyzing data. The monotonous data analysis process can also affect the quality of the process as a result of the physical limitations or biases of the observer. Without a detailed activity analysis, it is unfeasible to investigate the relationship between the activity duty cycles vs. productivity, or fuel use and emissions [9]. There is a need for a low-cost, reliable, and automated method for activity analysis that can be widely applied across all construction projects. This method either vision-based or non-vision based, needs to remotely and continuously analyze equipment’s actions and provide detailed field data on their performance.Over the past few years, cheap and high-resolution video cameras, extensive data storage capacities, and the availability of Internet connection on construction sites have enabled capturing and streaming construction videos on a truly massive scale. Detailed and dependent video streams provide a transformative potential for gradually and inexpensively sensing actions of construction equipment, enabling construction companies to remotely analyze operational details and in turn assess productivity, emissions, and safety of their operations [10]. To date, the application of existing site video streams for automated performance assessment is still untapped and unexploited by researchers in most parts.Here, we address a key challenge: action recognition; i.e., determining various actions equipment performs over time. While in the past few years several studies have looked into these areas (Section 2), many challenging problems still remain unsolved. As a step forward, this paper focuses on the problem of recognizing single actions of earthmoving equipment from site video streams. Fig. 1shows examples of the actions of an excavator and a dump truck operation, wherein the excavator performs a cycle of digging, hauling (swinging with a full bucket), dumping, and swinging (with an empty bucket) and the truck performs a cycle of filling, moving, and dumping.Given videos taken by a fixed camera with small lateral movements (caused by wind or small ground vibrations), clutter, and moving equipment, the task is to automatically and reliably identify and categorize such actions. This paper presents an algorithm that aims to account for these scenarios. As such, the state-of-art research in this area is first overviewed. Next, a set of open research problems for the field are discussed, including action recognition under different camera viewpoints within dynamic construction sites. The specific focus of the proposed method and its details are then described. Also, a comprehensive dataset and a set of validation methods that can be used in the field for development and benchmarking of future algorithms are provided. The perceived benefits and limitations of the proposed method in the form of open research challenges are presented.In most state-of-the-art practices, the collection and analysis of the site performance data are not yet automated. The significant amount of information required to be manually collected may (1) adversely affect the quality of the analysis, resulting in subjective reports [11,12] and (2) minimize opportunities for continuous monitoring which is a necessary step for performance improvement [11–14]. Hence, many critical decisions may be made based on this inaccurate or incomplete information, ultimately leading to project delays and cost overruns.In recent years, a number of research groups have focused on developing techniques to automatically assess construction performance. The main goal of these methods is to support improvement of operational efficiency and minimize idle times. Several studies such as [1–4] emphasize on the importance of a real-time resource tracking for improving construction performance. To address this need, different tracking technologies such as barcodes and RFID tags [14–19], Ultra WideBand (UWB) [20–22,61–63,65,68], 3D range imaging cameras [21,23], global and local positioning systems(GPS) [21,23,64], and computer vision techniques [24,25,66,67,69–75] have been tested to provide tracking data for onsite construction resources. While dominantly used for tracking construction material, they have also been used in locating workers and recording the sequence of their movement necessary to complete a task; e.g., [2,6,24–28,59,60,72–76]. For the task of performance monitoring, there is a need for detailed data on activities of construction equipment and workers, which makes a low-cost vision-based method, an alternative appealing solution; particularly because a low-cost single camera (e.g., $40–100 Wi-Fi HD camera) can potentially be used for (1) recognizing activities of multiple equipment and workers for both performance monitoring and safety analysis purposes and (2) minimizing the need for sophisticated on-board telemetric sensory for each equipment (or other sensory mentioned above for each worker) which can come at a higher cost.Despite a large number of emerging works in the area of human action recognition for smart online queries or robotic purposes and their significance for performance assessment on construction sites, this area has not yet been thoroughly explored in the Architecture/Engineering/Construction (AEC) community. The work in [13] is one of the first in this area, which presented a vision-based tracking model for monitoring a tower crane bucket in concrete placement operations. Their proposed method is focused on action recognition of crane buckets and hence it cannot be directly applied to earthmoving operations. In a more recent work, Gong and Caldas [2] proposed an action recognition method based on an unsupervised learning algorithm and showed promising results. However, generalizing the applicability of unsupervised learning models for unstructured construction sites can be challenging. In this paper we show that a supervised learning method may provide better performance in the equipment action recognition task. Zou and Kim [6] also presented an image-processing approach that automatically quantifies the idle time of a hydraulic excavator. The approach uses color information for detecting motion of equipment in 2D and thus can be challenged by changes of scene brightness and camera viewpoint. Also for performance assessment purposes, detailed data beyond idle/non-idle times can be very beneficial. Others such as Rezazadeh Azar et al. [73,74] benefit from location data for recognizing detecting activities and show promising performance. Such methods may require to be learned for every single site and mainly focus on detecting activities based on the location of the equipment. As such it is still challenging to differentiate between actions within a cycle (e.g., digging vs. hauling actions for an excavator) from location features alone.In the computer vision community, there are a large number of researches in the area of person recognition and pose estimation [29–34,46,47,59,60]. The results of these algorithms seem to be both effective and accurate and in some cases [32] they can also track deformable configurations which can be very effective for action recognition purposes. A number of approaches adopted visual representations based on spatio-temporal points [35,36]. This can be combined with discriminative classifiers (e.g., SVMs) [37,38], semi-latent topic models [39], or unsupervised generative models [40,41]. Other methods have shown the use of temporal structures for recognizing actions using Bayesian networks and Markov models [42,43], and the incorporation of spatial structures [44]. To leverage the power of local features, [40] introduced a new unsupervised model to learn and recognize the spatial–temporal features. Savarese et al. [45] introduced correlations that describe co-occurrences of code words within spatio-temporal neighborhoods. While not directly applicable, certain elements of all of these works can be effectively used to create new methods suitable for equipment action recognition.Previous research on sensor-based or vision-based approaches has primarily focused on location tracking of workers and equipment. In practice, when faced with the requirement for continuous benchmarking and monitoring of construction operations, techniques that can support automated identification of construction actions as supplementary modules can be beneficial. Site video streams offer great potential for benchmarking and monitoring both location and action of construction resources. Current overall limitations of the state of the art computer vision approaches in action recognition for construction “activity analysis” are as follows:1.Lack of comprehensive datasets of action recognition of various construction equipment which capture various actions of the equipment from almost all possible viewpoints, under various illumination and background clutter conditions;Lack of automated techniques that can detect articulated actions of construction equipment and workers plus their body posture necessary for performance assessments. Majority of vision-based approaches focus on recognizing simple actions where people are asked to perform distinct and single action in a rather more controlled environment; e.g., walking, jogging, running and boxing (please see the definition of simple actions in [77]). In contrast to these actions wherein similar body posture is repeated multiple times in a single-action video, construction equipment actions are not repeated and each atomic-action video only contain one instance of the action;Assuming a prior knowledge of starting temporal points for each action within a sequence. Without a proper knowledge of these starting points, a time-series of actions cannot be formed for further construction activity analysis;None of the existing techniques look into simultaneous recognition of multiple actions, rather they look into simultaneous action recognition per single class of objects. For example, in pedestrian tracking, the focus is to detect a group action (i.e., multiple people conducting the same action such as walking) as opposed to multiple individual actions of pedestrians (i.e., one pedestrian walking, the other running, the other hand waving).None of the existing approaches take a holistic approach to benchmarking, monitoring, and visualization of performance information. Without a proper visualization, it will be difficult for practitioners to control the excessive impacts of performance deviations. In addition, understanding the severity levels of performance deviations will not be easy.There is a need for techniques that can support automation of the entire process of benchmarking, monitoring, and control of performance deviations by identifying the sequence of resource actions, and determining idle/non-idle periods. Timely and accurate performance information brings awareness on project specific issues and empowers practitioners to take corrective actions, avoid delays, and minimize excessive impacts due to low operational efficiency [48]. In this paper, we address two of these limitations with the following contributions: (1) We introduce a new dataset for benchmarking and evaluating the performance of action recognition algorithms for commonly used earthmoving equipment (dump trucks and excavators) and (2) We propose an algorithm for vision-based analysis of articulated actions of single earthmoving equipment. The proposed algorithm is presented in the following section.Our action recognition approach fits within an overall strategy depicted in Fig. 2. This strategy consists of equipment detection, tracking, and action recognition modules from each video stream. The idea here is that tracking module can isolate each equipment and as result action recognition module can be focused on single equipment action recognition.Given a collection of site video streams collected from fixed cameras, our research objective is to (1) automatically learn different classes of earthmoving equipment actions present in the video dataset and (2) apply the model to perform action recognition in new video sequences. In this sense, the fundamental research question that we attempt to answer is the following: “Given a video which is segmented to contain only one execution of an equipment action, how we can correctly and automatically classify the video into its action category?” Answering this question will help us formulate new models over the temporal domain that will be able to detect the transitions between actions of interest within each video as well as the duration of these actions. Our proposed approach which is inspired by the work of [35,40,49] is illustrated in Fig. 3.Our method can cope with some limited camera motion. Specifically, as shown later in the paper we have validated our approach on videos recorded with a camera mounted on a tripod which has been subject to natural wind forces. However, one cannot assume the camera is completely static since there may be undesired motions due to external forces or example caused by strong wind. Also, the videos are expected to contain typical dynamic construction foregrounds and backgrounds that can generate motion clutter. In the training stage of our proposed method, it is assumed that each video only contains one action of particular equipment. This assumption is relaxed at the full testing stage, where the proposed method can handle observations cluttered by the presence of other equipment performing various actions.To represent all possible motion patterns for earthmoving equipment, a comprehensive video dataset for various actions is created. These videos, each containing single equipment performing only one action are initially labeled. First for each video, the local space–time regions are extracted using the spatio-temporal interest point detector [35]. A Histogram of Oriented Gradients (HOG) descriptors [30] is then computed from each interest point. These local region descriptors are then clustered into a set of representative spatio-temporal patterns, each called a code word. The set of these code words, from now on, is called a codebook. The distribution of these code words is learned using a multi-class one-against-all Support Vector Machine (SVM) classifier. The learned model will then be used to recognize equipment action classes in new video sequences. In the following each step is discussed in detail.There are several choices in the selection of visual features to describe actions of equipment. In general, there are three popular types of visual features: static features based on edges and limb shapes [50], dynamic features based on optical flow measurements [31], and spatio-temporal features obtained from local video patches [35,36,51,52]. Spatio-temporal features are shown to be useful in the articulated human action categorization [40]. Hence, in our method, videos are represented as collections of spatio-temporal features by extracting space–time interest points. To do so, it is assumed that during video recording, lateral movements do exist but are minimal. Our interest points are defined around the local maxima of a response function. To obtain the response, similar to [35,40] we apply 2D Gaussian and separable linear 1D Gabor filters as follows:(1)R=(I⊗g⊗hev)2+(I⊗g⊗hod)2where I(x, y, t) is the intensity at location (x, y, t) of a video sequence, g(x, y, σ) is the 2D Gaussian kernel applied along the spatial dimensions,hev(t;τ,ω)andhod(t;τ,ω)are the quadrature pairs of the 1D Gabor filter which are applied temporally.(2)g(x,y,σ)=12πσ2exp-x2+y22σ2(3)heV(t;τ,ω)=-cos(2πtω)×exp-t2/τ2(4)hod(t;τ,ω)=-sin(2πtω)×exp-t2/τ2The two parameters σ and τ correspond to the spatial and temporal scales of the detectors respectively. Similar to [35,40], in all cases, ω=4/τ is used, and hence the response function R is limited to only two input parameters (i.e., σ and τ). In order to handle multiple scales of the equipment in the 2D video streams, the detector is applied across a set of spatial and temporal scales. To simplify the process, in the case of spatial scale changes, the detector is only applied using one scale and thus the codebook is used to encode all scale changes that are introduced and observed in the video dataset; i.e., our video dataset contains multiple spatial scales of each equipment for training purposes. It is noted in [35,40] that any 2D video region with an articulated action can induce a strong response to the function R. This is due to the spatially distinguishing characteristics of actions, and as a result those 2D regions that undergo pure translational motion or do not contain spatially distinguishing features will not induce strong responses. The space–time interest points are small video neighborhoods extracted around the local maxima of the response function. Each neighborhood is called a cuboid and contains the local 3D video volume that contributed to the response function (3rd dimension is time). The size of the cuboid is chosen to be six times the detection scales along each dimension (6σ×6σ×6τ). To obtain a descriptor for each cuboid, a Histogram of Gradients (HOG) [37] is then computed. The detailed process is as follows:At first, the normalized intensity gradients on x and y directions are calculated and the cuboid is smoothed at different scales. Here the normalized intensity gradients are representing the normalized changes of the average intensities, and the 2D Gaussian smoothing is conducted using the response function R. The gradient orientations are then locally histogrammed to form a descriptor vector. The size of the descriptor is equal to (the number of spatial bins in the cuboid)×(the number of temporal bins)×(the number of gradient direction bins). In our case, this descriptor size is (3×3)×2×10=180. In addition to the application of HOG descriptors, histograms of optical flow [53] was also considered. As validated in Section 4, the HOG descriptor results in superior performance. Fig. 4shows an example of interest points detected for an excavator’s ‘digging’ action class. Each small box represents a detected spatio-temporal interest point. Fig. 5shows an example of the HOG descriptor for one of the interest points from the excavator’s digging action class.In order to learn the distribution of spatio-temporal features in a given video, first a set of HOG descriptors corresponding to all detected interest points in the entire training video dataset is generated. Using the k-means clustering algorithm and the Euclidean distance as the clustering metric, the descriptors of the entire training dataset are clustered into a set of code words. The result of this process is a codebook that associates a unique cluster membership with each detected interest point. Hence, each video is represented as a distribution of spatio-temporal interest points belonging to different code words. Fig. 6illustrates the action codebook formation process. A total of 350 cluster centers are considered for the best action recognition performance. The effect of the codebook size (the number of code word, also the number of clusters) on the action classification accuracy is explored in Section 4.4.3 of this paper.To train the learning model of the action categories, a multi-class one-against-all Support Vector Machine (SVM) classifier is used. The SVM is a discriminative machine learning algorithm which is based on the structural risk minimization induction principle [54]. In this work, it was hypothesized that traditional classifiers such as Naïve Bayes [55] or unsupervised learning methods such as probabilistic Latent Semantic Analysis (pLSA) [56] may not obtain the best recognition performance. For equipment action classification, the number of samples per class can be limited and consequently these methods tend to result in over-fitting. In the following, the multiple SVM classifier are briefly introduced. For validation, the performance of the proposed algorithm for learning equipment action classes is compared to Naïve Bayes and pLSA in Section 4.3 and the hypothesis for application of a multiple one-against-all supervised SVM classifier is validated.To classify all K action categories, we adopt K one-against-all SVM classifiers [57], so that video instances associated category (k) are within the same class and the rest of the videos are in another. For example, one of the binary SVM classifiers decides whether a new excavator video belongs to the ‘Digging’ or ‘non-Digging’ action classes. Given N labeled training data {xi,yi}, i=1,…,N; yi∈{0,1}, xi∈Rd, wherein xiis the distribution of the spatio-temporal interest points for each video (i) with d dimensions (occurrence histograms of visual words), and yiis the binary action class label, the SVM classifier aims at finding an optimal hyper-plane wTx+b=0 between the positive and negative samples. We assume there is no prior knowledge about the distribution of the action class videos. We use a conventional learning approach to SVM which seeks to optimize the following:(5)minw,b12‖w‖2+C∑i=1Nξisubject to:yi(w·xi+b)⩾1-ξifor1,…,Nξi⩾0for1,…,NIn this formula, C represents a penalty constant which is determined by cross-validation. For each action classifier, the classification decision score is stored. Among all classifiers, the one which results in the highest classification score is chosen as the equipment action class and the outcome of each video’s classification is labeled accordingly.Before testing our algorithm, it was important to assemble a comprehensive action recognition dataset. The new dataset accounts for variability in form and shape of construction equipment, different camera viewpoints, different lighting conditions, and static and dynamic occlusions. As a first step, our dataset includes combination of excavators and dump trucks for five types of excavator actions (i.e., moving, digging, hauling [swing with full bucket], swinging [empty bucket], and dumping) and three types for dump truck actions (i.e., moving, filling, and dumping) for three types of excavators (manufacturers: Caterpillar, Komatsu, and Kobelco) and three types of dump trucks (manufacturers: Caterpillar, Trex, and Volvo). This dataset – in which each video only contains one equipment performing a single action – was generated using videos collected over the span of 6months. To ensure various types of backgrounds and level of occlusions, the videos were collected from five different construction projects (i.e., two building and three infrastructure projects). Due to various possible appearances of equipment, particularly, their actions from different views and scales in a video frame, as shown in Fig. 7, several cameras were set up in two 180° semi-circles (each camera roughly 45° apart from one another) at the training stage. The different distances of these two semi-circles from the equipment, enables the equipment actions to be videotaped at two different scales (full and half high definition video frame heights). Combined with the strategy used to encode spatial scale in the codebook, all possible scales are considered. Overall a total of 150–170 training videos were annotated and temporally segmented for each action of equipment (overall 895 videos for four and three action classes of excavators and dump trucks). Each video has different durations, and hence various possible temporal scales for each action are introduced into the training dataset. Table 1summarizes the technical characteristic of our dataset.To quantify and benchmark the performance of the action recognition algorithm, we plot the Precision–Recall curves and study the Confusion Matrix. These metrics are extensively used in the Computer Vision and Information Retrieval communities as set-based measures; i.e., they evaluate the quality of an unordered set of data entries. More details can be found in [78,79].

@&#CONCLUSIONS@&#
