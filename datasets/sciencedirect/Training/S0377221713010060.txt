@&#MAIN-TITLE@&#
Robust portfolio optimization with copulas

@&#HIGHLIGHTS@&#
We provide the copula formulation for Value at Risk.We extend Value at Risk to Conditional Value at Risk for copulas.Linear optimization problem for Worst Case Conditional Value at Risk with copulas.Numerical applications in portfolio optimization of stock markets.

@&#KEYPHRASES@&#
Convex programming,Robust optimization,Copulas,

@&#ABSTRACT@&#
Conditional Value at Risk (CVaR) is widely used in portfolio optimization as a measure of risk. CVaR is clearly dependent on the underlying probability distribution of the portfolio. We show how copulas can be introduced to any problem that involves distributions and how they can provide solutions for the modeling of the portfolio. We use this to provide the copula formulation of the CVaR of a portfolio. Given the critical dependence of CVaR on the underlying distribution, we use a robust framework to extend our approach to Worst Case CVaR (WCVaR). WCVaR is achieved through the use of rival copulas. These rival copulas have the advantage of exploiting a variety of dependence structures, symmetric and not. We compare our model against two other models, Gaussian CVaR and Worst Case Markowitz. Our empirical analysis shows that WCVaR can asses the risk more adequately than the two competitive models during periods of crisis.

@&#INTRODUCTION@&#
In this paper we look into the problem of portfolio optimization where the assets of the portfolio are described by random variables. In this situation the selection of the optimal portfolio depends on the underlying assumptions on the behavior of the assets and the choice on the measure of risk. Usually the objective is to find the optimal risk-return trade-off.One of the pioneers in portfolio optimization was Markowitz (1952) who proposed the mean-variance framework for risk return analysis. Although the most common measure for the estimation of the return of the portfolio remains the expected return many other ways of calculating the risk have been developed. A widely used measure of risk is Value at Risk (VaR). VaR is the measure of risk that is recommended as a standard by the Basel Committee. However, VaR has been criticized in recent years mainly for two reasons. Firstly, VaR does not satisfy sub-additivity and hence it is not a coherent measure of risk in the way that is defined by Artzner, Delbaen, Erber, and Heath (1998). Also, it is not a convex measure of risk and thus it may have many local extrema which cause technical issues when optimizing a portfolio. Secondly, it gives a percentile of loss distribution that does not provide an adequate picture of the possible losses in the tail of the distribution. Szego (2005) uses this argument to state that “VaR does not measure risk”. Then he suggests alternative measures of risk with one of them being Conditional Value at Risk (CVaR).CVaR is the expectation of the distribution above VaR. Thus, the value of CVaR is affected by the fatness of the tail of the distribution. Hence, CVaR provides a better description of the loss on the tail of the distribution. Rockafellar and Uryasev (2000, 2002) proposed a minimization formulation that usually results in a convex or linear problem. These are desirable aspects of CVaR and have paved the way of its use in risk management and portfolio optimization. A literature review on CVaR can be found in Zhu and Fukushima (2009) and the references therein.Following the formulation of Rockafellar and Uryasev (2000, 2002) in order to calculate CVaR one has to make some assumptions on the underlying distribution of the assets. This can also be in the form of an uncertainty domain like a hypercube or an ellipsoidal set in which all feasible uncertainty values lie (Zhu & Fukushima, 2009). An alternative is assuming some multivariate distribution (Zhu & Fukushima, 2009). In this paper we focus on the selection of multivariate distributions.Gaussian distribution is the most commonly used multivariate case. It is easy to calibrate and also there are very efficient algorithms to simulate Gaussian data. This also applies to some extent to the elliptical family of distributions with student-τdistribution being widely used in Credit Risk (Romano, 2002; Clemente & Romano, 2004; Chan & Kroese, 2010). One disadvantage of using the Gaussian distribution is its symmetry. This implies that the probability of losses is the same as the probability of gains. Studies suggest that at least in the context of financial markets, assets exhibit stronger comovements during a crisis as opposed to prosperity (Ang & Chen, 2002; Hu, 2002, 2006). The second disadvantage is that it uses linear correlation as a measure of dependence. As the name suggests, linear correlation is characterized by linear dependencies. Since the observation of asymmetric comovements mentioned above suggests non-linear dependencies, linear correlation may not be an adequate measure of dependence (Artzner et al., 1998; Szego, 2005).One way of addressing the limitations of the symmetry underlying elliptical distributions is to consider mixture distributions. A linear combination of a set of distributions is used to fit the given sample by optimizing the combination weights. Hasselblad (1966, 1969) was one of the first who looked into mixture distributions and how their parameters can be estimated. Zhu and Fukushima (2009) avoid the assumptions needed on the set of distributions and their parameters and also avoid the estimation of the weights. Subsets of historical returns are used to represent data arising from different distributions and a worst-case scenario approach is applied to avoid the calibration of the weights. Hu (2002, 2006) and Smillie (2007) use mixture copulas to fit their data samples for the bivariate case. The work of Hu (2002, 2006) and Zhu and Fukushima (2009) motivates us to introduce copulas within a worst case robust scenario framework.Copulas are multivariate distribution functions whose one-dimensional margins are uniformly distributed on the closed interval [0,1] (Cherubini, Luciano, & Vecchiato, 2004; Sklar, 1959; Nelsen, 2006). The uniform margins can be replaced by univariate cumulative distributions of random variables (Cherubini et al., 2004; Sklar, 1959; Nelsen, 2006). Hence, copulas consider the dependency between the marginal distributions of the random variables instead of focusing directly on the dependency between the random variables themselves. This makes them more flexible than standard distributions because it is possible to separate the selection of the multivariate dependency from the selection of the univariate distribution. As an extension to that, the calibration of the multivariate distribution can be separated into two steps (Cherubini et al., 2004). Also, the fact that copulas describe the dependency between the marginal distributions which are monotonic makes them invariant under monotonic transformations (Cherubini et al., 2004; Nelsen, 2006). Copulas are associated with many measures of dependence that measure the monotonic dependencies between two random variables. Furthermore, like copulas themselves these monotonic measures are invariant under monotonic transformations (Wolff, 1980; Schweizer & Wolff, 1981; Embrechts, Lindskog, & McNeil, 2001).In this paper, we mainly focus on Archimedian copulas. This is a family of copulas that exhibits some interesting characteristics that can be utilized in our distribution modeling as discussed in the next section.The paper is structured as follows. In Section 2 we introduce copulas and the associated measures of dependence together with some theoretical background. In Section 3 we derive CVaR for copulas. We extend CVaR to WCVaR through the use of mixture copulas. We conclude the section by stating the generalized optimization problem for WCVaR. In Section 4 we construct a model based on the theory of the previous sections. Then, we provide two numerical examples where we asses the performance of our model. Finally, we close with conclusions.Copulas arise from the theory of probabilistic metric functions and were first introduced by Sklar in 1959 (Sklar, 1959). Copulas are multivariate distribution functions whose one-dimensional margins are uniformly distributed on the closed intervalI≡[0,1]. A more rigorous definition for copulas is given below.Definition 2.1An n dimensional copula (n-copula) is a functionCfromIntoIwith the following properties1.C(u1,…,ui,…,un)=0if anyui=0fori=1,2,…,n(we also describe a function with this property as grounded).C(1,…,1,um<1,…,1)=umfor allum∈Iwherem=1,2,…,n.C(u)⩾0∀u∈In(we also describe a function with this property as n-increasing).We continue with the definition of distribution functions and joint multivariate distribution functions. This is used when we discuss the relation between distribution functions and copulas.Definition 2.2A distribution function is a function F fromRtoIwith the following properties:1.F is nondecreasing.F(-∞)=0andF(∞)=1.A joint multivariate distribution function is a function fromRntoIwith the following properties1.F is n-increasing.F is grounded.F(∞)=1.F(∞,…,∞,xm,∞,…,∞)=Fm(xm).wherem=1,2,…,n.We can use copulas to replace probability distributions in all of their applications thanks to Sklar’s theorem. Sklar’s theorem is probably the most important result that links copulas to probability distributions. This, together with the corollary that follows provide the relation between n-Copulas and multivariate distributions. Sklar introduced his theorem in 1959 (Sklar, 1959), where the proof for its bivariate case can also be found. The multivariate case is discussed by Schweizer and Sklar (1983) (proofs of the theorem and corollary can be found therein).Theorem 2.4Sklar’s TheoremLet F be an n-dimensional distribution function with marginsF1,…,Fn. Then there exists an n-copulaCsuch that, for allx∈Rn,(1)F(x1,…,xn)=C(F1(x1),…,Fn(xn)).Furthermore, ifF1,…,Fnare continuous, thenCis unique; otherwiseCis unique onRanF1×⋯×RanFn(Ran≡Range).Let F be an n-dimensional distribution function with marginsF1,…,Fn, and letCbe an n-copula. Then, for anyu∈In,(2)C(u1,…,un)=FF1-1(u1),…,Fn-1(un)whereF1-1,…,Fn-1are the quasi-inverses of the marginals.The marginsF1,…,Fnand the multivariate distribution function F are as defined by Definitions 2.2 and 2.3. The marginsuican be replaced byFi(xi), because they both belong to the domainIand are uniformly distributed, i.e. letu∼U(0,1), thenP(F(x)⩽u)=P(x⩽F-1(u))=F(F-1(u))=u).Using Theorem 2.4 and Corollary 2.5 we can also derive the relation between the probability density functions and the copulas. In the following definition f is the multivariate probability density function of the probability distribution F andf1,…,fnare the univariate probability density functions of the marginsF1,…,Fn.Definition 2.6The copulas density of a n-copulaCis the functionc:In→[0,∞)such that(3)c(u1,…,un)≡∂nC(u1…,un)∂u1…∂un=f(x1,…,xn)∏i=1nfi(xi)Eqs. (1)–(3) imply that copulas decompose the multivariate probability distribution from its margins. The marginsF1,…,Fncan be any distribution of our choice while the copula simply describes the monotonic relation between the margins. This is one of the biggest advantages of copulas because they divide the problem of finding the correct distribution into two parts; first is finding the distribution of the margins and second the dependency between them. This is much easier than finding directly the multivariate dependency between the random variables. Hence, the calibration of the copulas become an easier task. The calibration methods for copulas can be found in Cherubini et al. (2004). Also, an introduction to copulas can be found in Nelsen (2006) and Schweizer and Sklar (1983) discuss the relationship of copulas to probabilistic metric spaces and the underlying theory.We introduce the copulas to be used in our examples. Together with the copulas we consider their associated measures of dependence. The focus is on a special family of copulas called Archimedian. We also consider the Gaussian copula which is the copula version of the multivariate normal distribution.Archimedian copulas were firstly introduced by Ling (1965). They belong to the family of probabilistic metric spaces that have some of the properties of Archimedes triangle function and hence the name (Schweizer & Sklar, 1983). This is a family of copulas that arises differently from the rest. Instead of using Theorem 2.4 we construct them using directly a functionφ, known as a generator, which enables us to write the expression for the copula in a closed form.Definition 2.7Given a functionφ:I→[0,∞)such thatφ(1)=0andφ(0)=∞and having inverseφ-1completely monotone, an n-place Archimedian copula is a functionCφ:In→Isuch thatCφ(u)=φ-1(φ(u1)+…+φ(un)).Extended literature regarding the Archimedian copulas can be found in Nelsen (2006), Schweizer and Sklar (1983), Cherubini et al. (2004). In our analysis we focus on three Archimedian copulas, Clayton, Gumbel and Frank. Our motivation for using these particular copulas stems from Hu (2002, 2006). Hu (2002, 2006) focuses on the calibration of bivariate mixture copula (see also Hasselblad, 1966, 1969 for mixture distributions). There are two reasons why these particular copulas have been chosen. Each copula better describes a different type of dependency. Clayton and Gumbel are nonsymmetric copulas that describe more adequately negative and positive dependencies, i.e. stronger dependence below and above the 50th percentile respectively. The Frank copula is symmetric but it has different properties to the Gaussian copula. Hence, by using them in a mixture structure we cover a large spectrum of possible dependencies. Also, these three copulas are very easy to calibrate.The definitions of the three Archimedian copulas are the following:Definition 2.8Given a generator of the formφ(u)=u-α-1withα∈(0,∞)then, the Claytonn-copula is given byCCl(u)=maxu1-α+…+un-α-n+1-1/α,0.Given a generator of the formφ(u)=(-ln(u))αwithα∈(1,∞)then, the Gumbeln-copula is given byCGu(u)=exp-(-lnu1)α+…+(-lnun)α1/α.Given a generator of the formφ(u)=lnexp(-αu)-1exp(-α)-1withα∈(0,∞)then, the Frankn-copula is given byCFr(u)=-1αln1+(e-αu1-1)·…·(e-αun-1)(e-α-1)n-1.For the calibration of the free parameters,α, of the Archimedian copulas we will use Kendal’sτ. Kendal’sτis a bivariate measure of dependence and is defined by the following equation(4)τ(X1,X2)=4∫-∞∞∫-∞∞F(x1,x2)dF(x1,x2)-1=4∫01∫01C(u1,u2)dC(u1,u2)-1.As we can see from Eq. (4), Kendal’sτmeasures the dependency between the cumulative distributions of random variableX1andX2and does not depend on the random variables themselves. Thus,τis a measure of monotonic dependence and is invariant under monotonic transformations. This makes it a more robust measure of dependence when compared to linear correlation. For comparison purposes, we also define linear correlation as(5)ϱ(X1,X2)=1σ(X1)·σ(X2)∫-∞∞∫-∞∞[F(x1,x2)-F1(x1)F2(x2)]dx1dx2=1σ(X1)·σ(X2)∫01∫01[C(u1·u2)-uv]dF1-1(u1)dF2-1(u2),whereσ(Xi)denotes the variance of the random variableXi. It can be seen that in the copula version ofϱ, the dependency on the random variablesX1andX2remains in form of the volatilityσ(Xi). An extensive literature on monotonic measures of dependence can be found in Schweizer and Wolff (1981), Wolff (1980), Nelsen (2006), Smillie (2007) and the references therein.Let us denote the free parameter,α, of each of the Archimedian copulas byαClforCCl(Definition 2.8),αGuforCGu(Definition 2.9) andαFrforCFr(Definition 2.10). For these three cases we have closed form relations with Kendall’sτEq. (4) (Cherubini et al., 2004). ForCClwe have that(6)τ=1-αCl-1,forCGu(7)τ=αGuαGu+2,and forCFrwe have(8)τ=1+4[D1(αFr)]αFr,whereDk(α)=kαk∫0αxexp(x)-1dxfork=1,2.As we can see from Definitions 2.8–2.10 there is only one free parameter to calibrate regardless of the dimensions of the copula. On the other hand we have aτfor each pair of random variables. Our solution to this problem is to calculate theτfor all the pairs and select the largest. To do that we calculateτ(Xi,Xj)fori,j=1,2,…,nandi≠j. A higher positiveτimplies stronger negative dependence in the case of Clayton and stronger positive dependence in the case of Gumbel, etc. Hence, by selecting the highest positiveτwe deliberately choose the most extreme behavior for our copulas. This seems to be consistent with the use of copulas within a worst case robust framework. This also allows an easy calibration.Other ways used to estimateτare mentioned in the literature by Genest and Rivest (1993) and Smillie (2007). The former employ a more sophisticated approach where they use a semi-parametric methodology for the decomposition ofτ. The latter follows a similar approach to ours where instead of taking the maximumτ, takes the median from the estimatedτs. In both cases the focus is to provide the best possible fit for the data. Hence, a more optimisticτestimation will be provided affecting the robustness of the model. We believe that our method is more suitable for the worst case framework that we seek.Finally, we give the definition of the Gaussian copula (Embrechts et al., 2001; Smillie, 2007; Cherubini et al., 2004).Definition 2.11Given a n-place standard multivariate normal distribution functionΦnparameterized by a dispersion matrixP∈[-1,1]n×n, the Gaussian copula is the functionCGa:In→Isuch that(9)CGa(u)=Φ(Φ1-1(u1),…,Φn-1(un)).ForCGato be called a Gaussian copula all the margins{Φi}i=1nhave to be normally distributed but they can have different mean and variance.Having introduced the theorems that enable us to associate copulas with distributions we will derive the copula formulation of the Worst Case Conditional Value at Risk (WCVaR). Zhu and Fukushima (2009) have derived the WCVaR for distributions. We follow a similar approach to derive WCVaR for copulas. At every step involving the use of distributions we present the equivalent copula formulation. For the derivation of the copula formulation we use Eqs. (1)–(3).In order to define the WCVaR we first have to define Value at Risk (VaR) and Conditional Value at Risk (CVaR). We first discuss VaR.Proposition 3.1Letw∈W⊆Rmbe a decision vector,u∈Ina random vector,g̃(w,u)the cost function andF(x)=(F1(x1),…,Fn(xn))a set of marginal distributions whereu=F(x). Also, let us assume thatufollows a continuous distribution with copula density functionc(.). Then VaRβfor a confidence levelβis defined as(10)VaRβ(w)≜min{α∈R:C(u|g̃(w,u)⩽α)⩾β}.Given a decisionw∈Wand a random vectorx∈Rnwhich follows a continuous distribution with density functionf(.), the probability ofg(w,x)not exceeding a thresholdαis represented byΨ(w,α)≜∫g(w,x)⩽αf(x)dx=∫g(w,x)⩽αc(F(x))∏i=1nfi(xi)dx=∫g̃(w,u)⩽αc(u)du=C(u|g̃(w,u)⩽α),wherefi(xi)=∂Fi(xi)∂xiis the univariate probability distribution of the individual elements of the random vectorx(see Definition 2.6).g̃(w,u)=g(w,F-1(u))whereF-1(u)=F1-1(u1),…,Fn-1(un)maps the domain of the cost function fromRntoIn, as implied by the transformationui=Fi(xi). For the derivation of the copula version ofΨ(w,α)we use Eq. (3). Having definedΨ(w,α), we consider the VaR. Given a fixedw∈Wand a confidence levelβ, VaRβis defined asVaRβ(w)≜min{α∈R:Ψ(w,α)⩾β}=min{α∈R:C(u|g̃(w,u)⩽α)⩾β}.□We continue with the definition of the CVaR with respect to VaR.Proposition 3.2Givenw,u,F(x)andg̃(w,u)as inProposition 3.1we define CVaRβfor a confidence levelβas(11)CVaRβ(w)≜11-β∫g̃(w,u)⩾VaRβ(w)g̃(w,u)c(u)du.We start from the equation of CVaR that arises from the probability density functionf(.)and we derive the copula form.(12)CVaRβ(w)=11-β∫g(w,x)⩾VaRβ(w)g(w,x)f(x)dx=11-β∫g(w,x)⩾VaRβ(w)g(w,x)c(F(x))∏i=1nfi(xi)dx=11-β∫g̃(w,u)⩾VaRβ(w)g̃(w,u)c(u)du.□Following Rockafellar and Uryasev (2000) we formulate Eq. (12) as the following minimization problem(13)Gβ(w,α)=α+11-β∫x∈Rn[g(w,x)-α]+f(x)dx=α+11-β∫u∈In[g̃(w,u)-α]+c(u)du.Hence, we have(14)CVaRβ(x)=minα∈RGβ(w,α).By solving the minimization problem in Eq. (14), we directly obtain both the values of CVaR and VaR. From Proposition 3.1 we have that the value of VaR is the value ofα.In order for the above definitions to be computed, exact knowledge of the distributionf(x)or copula densityc(u)and the marginsF(x)is needed. As the aim in this paper is to represent distributions with copulas, we shall omit usingf(x)and usec(u)instead. The equivalence of the two has been discussed in Section 2. Knowledge of the copulaC(u)and its margins{ui=Fi(xi)}i=1nimplies knowledge off(x)andc(u). A copula representation of the distribution ofxcannot be expected to be exact. Thus, we assume that our copula representation belongs to a set of copulasc(.)∈C. The notion of robustness with respect toCinvolves the worst performing copula (or copulas, since the worst case may not be unique), i.e. the copula for which we obtain the greatest CVaR. Hence, we define WCVaR.Definition 3.3The WCVaR for fixedw∈Wwith respect toCis defined as(15)WCVaRβ(w)≜supc(.)∈CCVaRβ(w).It is known that CVaR is a coherent measure of risk (Szego, 2005; Zhu & Fukushima, 2009; Artzner et al., 1998). For a measure of riskρmapping a random vector X to be coherent it has to satisfy the following properties:(i)Subadditivitty: for all random vectors X andϒ,ρ(X+ϒ)⩽ρ(X)+ρ(ϒ).Positive homogeneity: for positive constantλ,ρ(λX)=λρ(X).Monotonicity: ifX⩽ϒfor each outcome, thenρ(X)⩽ρ(ϒ).Translation invariance: for constantm,ρ(X+m)=ρ(X)+m.Zhu and Fukushima (2009) prove that WCVaR preserves coherence. They also give the following lemma from Fan (1953) which allows us to formulate the problem into a tractable one.Lemma 3.4Suppose thatWandXare nonempty convex sets inRnandRm, respectively, and the functionz(w,x)is convex inwfor anyx, and concave inxfor anyw. Then we have(16)minw∈Wmaxx∈Xz(w,x)=maxx∈Xminw∈Wz(w,x)We also use Lemma 3.4 to extend the proof from Zhu and Fukushima (2009) to copulas and eventually formulate our problem as a minmax problem.In this example the distribution of the vector of returnsxis described by a mixture copula(17)C(F(x))=λTC→,whereλ∈Λ={λ:eTλ=1,λ⪰0,λ∈Rl}andC→=(C1(F(x)),…,Cl(F(x)))is the vector with copulas andF(x)=(F1(x),…,Fn(x))is the vector of the cumulative univariate distributions. We can apply Eq. (3) to Eq. (17) to obtain the density of the mixture copula. Then, we can use this density in Definitions 2.8–2.11 to obtain(18)Gβ(w,α,λ)=α+11-β∫u∈In[g̃(w,u)-α]+∑i=1lλici(u)du=∑i=1lλiGβi(w,α),where(19)Gβi(w,α)=α+11-β∫u∈In[g̃(w,u)-α]+ci(u)dufori=1,2,…,l.The optimization problem that we need to solve is stated by the following theorem and corollary from Zhu and Fukushima (2009):Theorem 3.5For eachw,WCVaRβ(w)with respect toCis given by(20)WCVaRβ(w)=minα∈Rmaxλ∈ΛGβ(w,α,λ),whereΛ={λ:eTλ=1,λ⪰0,λ∈Rl}.MinimizingWCVaRβ(w)overWcan be achieved by the following minimization(21)minw∈WWCVaRβ(w)=minw∈Wminα∈Rmaxλ∈ΛGβ(w,α,λ).More specifically, if(w∗,α∗,λ∗)attains the right hand side minimum, thenw∗attains the left hand side minimum.Zhu and Fukushima (2009) provide the proof for the case of mixture distributions. The theorems in Section 2, together with Proposition 3.1 and 3.2, show that Theorem 3.5 and Corollary 3.6 can be applied to copulas. For the sake of completeness we give the motivation behind the proof and we continue with the formulation of the optimization problem.In order to optimize the portfolio we need to solve(22)minw∈WWCVaRβ(w)≡minw∈Wmaxλ∈Λminα∈RGβ(w,α,λ)Since the mixture copula (17) is linear inλ, Zhu and Fukushima (2009) use Lemma 3.4 to show that (22) can be written as(23)minw∈Wminα∈Rmaxλ∈ΛGβ(w,α,λ).An epigraph formulation can be used to reduce problem (23) into a minimization problem as follows(24)min(w,α,θ)∈W×R×Rθ:∑i=1lλiGβi(w,α)⩽θ,∀λ∈Λandθmust satisfy(25)Gβi(w,α)⩽θ,fori=1,2,…,l.Problem (24) can thus be reduced to(26)min(w,α,θ)∈W×R×Rθ:Gβi(w,α)⩽θ,i=1,2,…,l.A straightforward approach for evaluating problem (26) is by Monte Carlo simulation. Rockafellar and Uryasev (2000) give an approximation ofGβ(w,α), where Monte Carlo simulation can be used. They writeGβ(w,α)as(27)G^β(w,α)=α+1S(1-β)∑k=1S[g̃(w,u[k])-α]+,whereu[k]is the kth sample vector (again here we give the copula version whereu[k]=F(x[k])). Thus, using Eq. (27) we can express problem (26) for evaluation using Monte Carlo simulations(28)min(w,α,θ)∈W×R×Rθ:α+1Si(1-β)∑k=1Si[g̃(w,u[k]i)-α]+⩽θ,i=1,2,…,l.,whereu[k]iis thekthsample arising from copulaCiof the mixture copula (Eq. (17)).Siis the size of the sample that arises fromCi.Following Zhu and Fukushima (2009) we write the minimization problem as(29)minθ(30)s.t.w∈W,v∈Rm,α∈R,θ∈R(31)α+1Si(1-β)(1i)Tvi⩽θ,i=1,…,l(32)vki⩾g̃w,u[k]i-α,k=1,…,Si,i=1,…,l(33)vki⩾0,k=1,…,Si,i=1,…,l,wherev=(v1;…;vl)∈Rmwithm=∑i=1lSiand1i=(1;…;1)∈RSi.In this section we demonstrate how the theory in Section 3 can be used for the optimization of a portfolio of financial assets. Financial assets can be described by distributions and their risk can be measured using CVaR.We consider a portfolio of n financial assetsA1,…,An. We assume that the returns of the assets are log-normally distributed and they are consistent with the Black and Scholes (1973) representation, given by(34)xi=dAi(t)Ai(t)=μidt+σidBi(t),whereμiandσiis the mean and the standard deviation of the random variablexianddBi(t)denotes a Wiener process. We have the return vectorx=(x1,…,xn)∈Rnandu=(u1,…,un)=(Φ1(x1),…,Φn(xn)). We also define the decision vectorw=(w1,…,wn)∈Rn, which denotes the amount of investment in each financial asset in the portfolio. We also define the loss function(35)g̃(w,u)=-wTΦ-1(u),whereΦ-1(u)=(Φ1-1(u1),…,Φn-1(un)). Hence, the loss function is the negative of the portfolio returnwTΦ-1(u).We have selected above the univariate distribution that describes the asset returns. We now consider the selection of the copula that describes the dependency between these returns. We first solve a simple optimization problem using the Gaussian copula. Consider the problem(36)minw∈WCVaR(w),whereWdefines the domain ofwas described by its constrains andu∼CGa(see Eq. (12)). This is equivalent to Eqs. (29) and (33) whenl=1.The advantage of problem (36) is that it employs the Gaussian copula. The latter is the most commonly used copula in practice for characterizing multivariate dependencies. It is also easy to use. Furthermore, the Gaussian copula is a desirable reference point for assessing portfolio performance. There are, however drawbacks to the Gaussian copula. The first is its symmetry. Studies show that assets have stronger negative comovements than positive (Hu, 2002, 2006; Ang & Chen, 2002). A second disadvantage is the linear correlation that can only capture linear dependencies between assets, which may not be realistic (Artzner et al., 1998; Szego, 2005). Both of these disadvantages may lead to bad performance in the presence of market shocks.We aim to compensate for some of these disadvantages by using a mixture copula. In problem (37), the mixture setCcontains the copulas from Section 2.1. The aim is to cover all types of dependencies and thereby use a robust measure of dependence (Kendall’sτ(4)). This robustness is further augmented by the worst-case approach. Thus, the second problem we solve is(37)minw∈WWCVaR(w),whereWis defined as above andu∼candc∈Cis a set of copulas (see Eq. (15))For further comparison we introduce a third and final model. The third model is an extension of Markowitz (1952). Markowitz (1952) model is given by(38)minθ∈R,w∈W{θ:wTΣw⩽θ},whereΣ, the covariance matrix, is symmetric positive semidefinite. Problem (38) constitutes a quadratic convex problem. The extended model takes into consideration the uncertainty that governsΣ. The solution is provided within a worst case framework whereΣis unknown, but is assumed to belong in a setS+, which comprises symmetric positive semidefinite matrices (Kontoghiorghes, Rustem, & Siokos, 2002; Rustem, Becker, & Marty, 2000). Worst Case Markowitz (WCM) has the form(39)minθ∈R,w∈WsupΣ∈S+{θ:wTΣw⩽θ}.Given a discrete setS+problem (39) can be reformulated into a convex optimization problem(40)minθ∈R,w∈W{θ:wTΣiw⩽θ,∀Σi∈S+}.WCM provides us with nice comparison model since it is an extended, robust (similarly to (37)) version of the well-known classic Markowitz model, and usesΣto describe the dependence structure, in a similar fashion to problem (36).Problems (36) and (37) are solved using Eqs. (29) and (33) and WCM using (40). Also, for all three problems we assumeWto be convex and, without loss of generality (and for simplicity), we define it by the following constraints:(41)eTw=1,whereeis the vector with all-unity elements. Furthermore, to assure portfolio diversification, the additional constrains(42)w̲⩽w⩽w‾can be imposed, for example in the case of a portfolio comprising only long positions. At this instance we may requirew̲,w‾∈[0.1,1]nwherew̲,w‾are lower and upper bounds respectively. This way we prevent the portfolio from having a single asset position. Of course, such constraints are not compulsory and vary according to the investors’ preference.Finally, since we optimize an asset portfolio we are interested in its performance. Hence, it is often desirable to impose an additional performance restriction in terms of the minimum expected returnπr(43)E(wTF-1(u))⩾πr.We use the following seven indices: Nikkei225, FTSE100, Nasdaq, DAX30, Sensex, Bovespa, Gold index. These represent six different stock exchange markets from different parts of the world and one commodity index. The markets corresponding to the indices are Japan, UK, USA, Germany, India and Brazil. These markets, with the inclusion of the commodity, are intended to lead to a diversified portfolio.The data used covers the period November 1998–July 2011, during which we look at the daily returns. This time line includes the dot-com bubble, South American crisis and Asian crisis. These three events took place between 1998 and 2002, and they had a large negative impact on the world markets. The data also include the 2008 Global Recession crisis. Both periods of crises can be observed in Fig. 1.In both numerical examples the data between 1998 and 2003 (1200 time steps) is used for the calibration of the three problems, (36), (37) and (40). This way we include data from a period of crisis. We expect to show that the risk and the dependencies between assets during a crisis period can be assessed more efficiently with the use of WCVaR as opposed to using only Gaussian copula CVaR. The Worst Case Portfolio (WCP), given by (37) should perform more robustly than the Gaussian Portfolio (GP), given by (36). Overall, and particularly during a period of crisis, we expect that the WCP will perform better in the presence of downside shocks. Also, it will be interesting to see how Worst Case Markowitz Portfolio (WCMP), given by (40), fares against the other two portfolios, since it combines some of their individual trades. Thus, the critical test is the performance of all three portfolios during the 2008 crisis.In our first example, we consider a static portfolio in which the weights of the portfolio are calculated only once. A daily rebalancing is performed using the same weights throughout the entire lifespan of the portfolio. For the computation of the weights we calibrate our copulas using the period between 1998 and 2003 and then solve problems (36) and (37). To create the setS+for WCM we calculate sixΣ. In more detail,Σ1is calculated from the period between 1998 and 2003.Σ2is the diagonal ofΣ1, with the off-diagonal elements being all equal to zero to assume independence.Σ3is calculated through the relationϱ=sin(0.5πτ)(Smillie, 2007) (which holds under normality assumption but in our case is different toΣ1).Σ4is calculated from a smaller sample concentrated around the peak of 2000 crisis for more extreme effects. Finally,Σ5andΣ6are calculated from different sample periods prior to 2003. The means, variances, and the covariance matrix (Σ1) of the seven assets as estimated from the period between 1998 and 2003 are given in Table 1. The former two are used for the univariate distributions of the assets as defined by Eq. (34). The univariate distributions together with the four copulas of Section 2 are used to run Monte Carlo simulations in order to provide the inputsu[k]ineeded to solve Eqs. (29) and (33). Problem (40) does not require any simulations because it is not a stochastic problem.Simulating data from Gaussian copula is straightforward with the use of the Cholesky decomposition of the correlation matrix. Simulating data from other copulas can be a difficult task, in general, which makes them less attractive. The simulation of data from the three Archimedian copulas uses the algorithms found in Melchiori (2006). Melchiori (2006) provides a summary of the results from Marshall and Olkin (1988), Devroye (1986), Nolan (2012).To solve problems (36), (37) and (40), we also have to define the constraintsW. We use as a starting point Eqs. (41)–(43). We specifyw⩾0. This implies that we only allow long positions in the portfolio. The upper boundw‾in Eq. (42) is implied by the budget constraint (41).The optimization problems are solved using the Yalmip Matlab package, together with the CPLEX solver. The PC used for the implementation of the numerical examples is an Intel Core 2 Duo, 2.8GHz with 4GB memory. For each of the four copulas, we run 10000 simulations for each of the seven assets. Simulating and solving all three problems takes less than a minute.All three problems are solved forπr=0–0.00025, whereπris the required minimum daily return (see (43)). The results are presented in Fig. 2, Tables 2 and 3. Table 2 illustrates the performance of the three portfolios (GP, WCP and WCMP) both for the ‘In Sample’ (November 1998–June 2003) and ‘Out of Sample’ (June 2003–July 2011). ‘In Sample’ performance shows that the lower boundπris always satisfied at least by one of the three portfolios in the form of AR. Also, the overall (in and out of sample) performance of the WCP portfolio always has higher volatility (Vol) and CVaR. This is to be expected since the WCP has to satisfy more constraints in the optimization problem, i.e., the same constraints that exist for the Gaussian copula in the GP have to be satisfied for all four copulas used in the WCP. Hence, the CVaR obtained for the WCP is the CVaR of the worst case copula, which is the equivalent the requirement imposed by inequality (25). These copulas also have fat tails, and hence higher Vol. On the other hand WCMP, as a variance minimization problem, has the smallest Vol and CVaR out of the three, at least forπr=0–0.00015. This advantage stops when higher returns are required.We expect WCP and WCMP to perform better than the GP, at least under the worst case scenarios. This should apply throughout the ‘Out of Sample’ testing period. Hence, we focus on the ‘Out of Sample’ period. Table 3 shows that the performance of WCP up to January 2008 (the beginning of the crisis) was similar or worse than GP with WCMP consistently having the highest. The better performance of WCMP may suggest that assuming some uncertainty in the estimation of the covariance or correlation matrix may improve your judgment of risk, at least during periods of prosperity. Whileπrincreases, the difference between the returns of the WCP and the other two portfolios increases. This can also be verified in Fig. 2 for the case ofπr=0. This is at most evident at the bottom Fig. 2 where the difference between the notional value of the WCP and GP is displayed. The difference up to January 2008 is very close to zero, after which it starts to increase with the WCP outperforming the GP.This behavior changes from 2008 onwards. In Table 2 we see that the performance of GP with respect to Average Return (AR) and Total Return (TR) is worse or similar to WCP for allπrtested. The reason is the robustness of the performance of the WCP during the 2008 crisis. It is surprising that the WCMP does not exhibit similar robustness. On the contrary its behavior is similar to the GP. Assessing the risk between the assets through the use of the covariance matrix, similarly to the correlation of problem (36), does not carry any information about the characteristics of the tail of the distribution of each asset. Such a model proves inadequate to in emulating risk during periods of extreme movements. This is evident from the Maximum Drawdown (MD2); this measures the greatest loss within a 6month period. For the given dataset this occurs at the time of the crisis. We can see that the WCP MD2 is better for allπr. Another observation from Table 2 is that the higher the requirement forπris, the more similar the performance of the three portfolios becomes with respect to AR and TR. This is the result of our demand for higher returns, which forces all three portfolios to select high return assets. Thus, the number of assets that can be selected becomes smaller and hence the portfolios become similar. This becomes more apparent in the case ofπr=0.00025, in which case in addition to the AR and TR, Vol and CVaR are much closer.With the above in mind, we can conclude that an optimistic portfolio, like the GP, is more suitable during times of prosperity. Also, the addition of uncertainty with respect to the values of the covariance matrix, like the WCMP, may improve our assessment of risk during prosperity even further. In contrast, a more robust portfolio like WCP proves to be beneficial during a crisis period.We consider the case when the optimal weights of the assets are recomputed. In this more dynamic portfolio, the weights of the assets are recalibrated on a monthly basis. At every step, we extend the in sample calibration window by a month, then we solve problems (36) and (37) to obtain the weights. For problem (40), to obtain the weights, we use the setS+, as described in the previous section, to which we addΣ7.Σ7is evaluated from the expanding sample. This way we make sure that new information are taken into account. Then, for all three portfolios, we keep the same weights for the rest of that month. During the month, daily rebalancing is performed using the constant weights. We do not adopt a moving window of calibration since we do not want to lose the information from the old crises, in our case the crises between 1998 and 2003.The expanding calibration window causes the estimated values ofμˆi, used in Eq. (34), to change every month. Thus, we have to make sure that the constraint (43) remains within the feasible set ofw. To do so we replace constraint (43) with the dynamic constraint(44)E(wTF-1(u))⩾max[wEqTμˆ,0],whereμˆ=(μˆ1,…,μˆn)is the vector of the asset return means as calculated using the calibration period andwEq=(1/n,…,1/n)i.e. all the weights are equal. The rest of the constraints remain as in Section 4.1.1.For comparison purposes, we also include a simple portfolio not based on optimization. The ‘Equally weighted’ portfolio (EWP) has equal positions in all assets i.e. we always usewEqas the weights of the portfolio.The ‘out of sample’ performance of all four portfolios (GP, WCP, WCMP and EWP) is shown in Fig. 3and Table 4. The observations are very similar to those of Section 4.1.1. The CVaR and Vol of the WCP are higher than the GP but the MD2, AR and TR of the WCP is significantly higher. WCMP exhibits similar behavior to the GP. Also, in the case of the EWP, the Vol and the CVaR lie between those of the other two portfolios. At the same time its performance is worse with respect to the AR, TR and MD2, i.e., the EWP sustains the largest losses during the 2008 Global Recession crisis.In this paper we demonstrated one way of using copulas in a portfolio optimization framework where the worst-case copula is considered. In particular, we focus on the derivation of CVaR and WCVaR for copulas. In the case of WCVaR we show how a mixture copula can be used in order to obtain a convex optimization problem.By introducing copulas in the CVaR framework we allow more flexibility in the selection of the distribution. The most commonly used distribution for modeling multivariate dependencies is the Gaussian copula. This is due to the simplicity of its construction and the availability of efficient methods of simulation. Its disadvantages are its symmetry and its ability to only describe linear dependencies via the use of linear correlation in its structure. However symmetric behavior and linear dependencies among assets are unrealistic (Hu, 2002, 2006; Ang & Chen, 2002; Artzner et al., 1998). We discuss alternative distribution functions in the form of copulas that can exhibit asymmetric behavior and utilize monotonic measures of dependence in their formulation. These are the three Archimedian copulas in Section 2.1 that are also easy to simulate from using the algorithms given by Melchiori (2006).The advantage of using nonsymmetric distribution functions was demonstrated in the numerical examples of Section 4.1. In Section 4.1 we provide a comparison between three competitive models, WCVaR, Gaussian copula CVaR and WCM. We show that for both static and dynamic strategies, for low minimum expected portfolio return, the WCP outperforms both the GP and the WCMP in every statistic except Vol and CVaR. In particular during the 2008 crisis the WCP performed more robustly than the other two portfolios, and that was true even for a high minimum expected return requirement. This shows that the assumption of symmetry and the use of the correlation or covariance matrix as a measure of dependence provide insufficient information for assessing the risk during periods of crisis.We compare the performance of dynamic portfolios with that of the EWP. The EWP performance shows that following a naive approach in which risk is neglected is not necessarily the correct way forward. Although the EWP performed relatively adequately soon after 2002, it suffered the biggest loss during the 2008 crisis. As a result, the EWP became the worst performing portfolio among all portfolios in the numerical examples.It seems reasonable to conclude that when optimizing a portfolio, the associated risk needs to be taken into account. All possible dependencies have to be considered in order to obtain robust results. One way of achieving this is through copulas and mixture copulas, that allow dependency systems with higher flexibility in their description than a single distribution. This flexibility allows non symmetric behavior and fat tails in the design of the system.

@&#CONCLUSIONS@&#
