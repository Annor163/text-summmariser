@&#MAIN-TITLE@&#
A study of voice activity detection techniques for NIST speaker recognition evaluations

@&#HIGHLIGHTS@&#
Discuss the characteristics of interview speech in NIST SREs.Use speech enhancement techniques as a pre-processing step of voice activity detection.Compare with the ASR transcripts provided by NIST, VAD in the ETSI-AMR, SM-VAD, and GMM-VAD.

@&#KEYPHRASES@&#
Speaker verification,Voice activity detection,NIST SRE,Statistical model based VAD,Spectral subtraction,

@&#ABSTRACT@&#
Since 2008, interview-style speech has become an important part of the NIST speaker recognition evaluations (SREs). Unlike telephone speech, interview speech has lower signal-to-noise ratio, which necessitates robust voice activity detectors (VADs). This paper highlights the characteristics of interview speech files in NIST SREs and discusses the difficulties in performing speech/non-speech segmentation in these files. To overcome these difficulties, this paper proposes using speech enhancement techniques as a pre-processing step for enhancing the reliability of energy-based and statistical-model-based VADs. A decision strategy is also proposed to overcome the undesirable effects caused by impulsive signals and sinusoidal background signals. The proposed VAD is compared with the ASR transcripts provided by NIST, VAD in the ETSI-AMR Option 2 coder, satistical-model (SM) based VAD, and Gaussian mixture model (GMM) based VAD. Experimental results based on the NIST 2010 SRE dataset suggest that the proposed VAD outperforms these conventional ones whenever interview-style speech is involved. This study also demonstrates that (1) noise reduction is vital for energy-based VAD under low SNR; (2) the ASR transcripts and ETSI-AMR speech coder do not produce accurate speech and non-speech segmentations; and (3) spectral subtraction makes better use of background spectra than the likelihood-ratio tests in the SM-based VAD. The segmentation files produced by the proposed VAD can be found in http://bioinfo.eie.polyu.edu.hk/ssvad.

@&#INTRODUCTION@&#
NIST speaker recognition evaluations (SREs) have been focusing on text-independent speaker verification over telephone channels since 1996. In recent years, NIST introduces interview-style speech into the evaluations. For example, the speech files in NIST 2008 SRE contain conversation segments of approximately 5min of telephone speech and 3min of interview speech, and the speech files in NIST 2010 SRE contain interview recordings with duration ranging from 3 to 15min. In each speech file, about half of the conversation contains speech, and the remaining part contains pauses or silence intervals. The inclusion of non-speech intervals in the speech files necessitates voice activity detection (VAD) because these intervals do not contain any speaker information. In particular, VAD can be used to identify speech segments prior to the feature extraction process.Speech/non-speech detection can be formulated as a statistical hypothesis problem aimed at determining to which class a given speech segment belongs. However, a high level of background noise can cause numerous detection errors, because the noise partly or completely masks the speech signal (Ramirez et al., 2007). A robust decision rule that works under noisy conditions is therefore essential. Most of the existing VAD algorithms are effective under clean acoustic environments, but they could fail badly under adverse acoustic conditions (Beritelli et al., 2002).Traditionally, VAD uses periodicity measure (Tucker, 1992), zero-crossing rate (Benyassine et al., 1997), pitch (Chengalvarayan, 1999), energy (Woo et al., 2000), spectrum analysis (Marzinzik and Kollmeier, 2002), higher order statistics in the LPC residual domain (Nemer et al., 2001), or combinations of different features (Tanyer and Ozer, 2000). More sophisticated VAD techniques have been proposed for real-time speech transmission on the Internet (Sangwan et al., 2002) and mobile communication services (Freeman et al., 1989). In particular, the adaptive multi-rate (AMR) codec option II (AMR2) (ETSI, 2001) uses a decision logic based on the energy of 16 frequency bands, background noise, channel SNR, frame SNR, and long-term SNR (Cornu et al., 2003). The VAD of this codec takes advantage of speech encoder parameters and is more robust against environmental noise than its earlier version (AMR1) and G.729 (Torre et al., 2006). Moreover, the VAD decision threshold can be adapted dynamically according to the acoustic environment, allowing on-line speech/non-speech detection under non-stationary acoustic environments.More recently, research has focused on statistical-model-based VAD where individual frequency bins of speech are assumed to follow a parametric density function (Sohn et al., 1999). In this approach, VAD decisions are based on likelihood ratio tests (LRTs) where the geometric mean of the log-likelihood ratios of individual frequency bins are estimated from observed speech signals. The statistical model can be Gaussian (Sohn et al., 1999) or generalized Gaussian (Góriz et al., 2010). However, it has been recently found that Laplacian and Gamma models are more appropriate for handling a wide variety of noise conditions (Chang et al., 2006). Using an online version of the Kolmogorov–Smirnov test, the type of models can be selected adaptively for different noise types and SNRs (Chang et al., 2006). To improve the robustness of VAD under adverse acoustic environment, contextual information derived from multiple observations has been incorporated into the LRT (MO-LRT) (Ramirez et al., 2007). Gaussian mixture models have been applied to model the static harmonic-structure information and the long-term temporal information of speech. VAD decisions are then based on the log-likelihood ratios computed from the clean and noise GMMs (Torre et al., 2006; Fukuda et al., 2010). In Sun et al. (2009), Wiener filtering is applied to remove noise before extracting acoustic features for training the speech and non-speech GMMs.Characteristics of speech and non-speech signals have also been modeled by hidden Markov models (HMMs). For example, in Varela et al. (2011), a decision-tree algorithm that combines the scores of HMM-based speech/non-speech models and speech pulse information was used for rejecting far-field speech in speech recognition systems. Both Fukuda et al. (2010) and Varela et al. (2011), Vlaj et al. (2012) use statistical models to characterize speech and non-speech signals, with some decision logics governing the switching between speech and non-speech states. The difference being that in the GMM-VAD of Fukuda et al. (2010), state duration is governed by the number of speech frames (as detected by the GMMs) in a fixed-length buffer, and that in the GMM-VAD of Vlaj et al. (2012) state duration is governed by a hangover and handbefore scheme which detects the consonants occurred at the beginning, middle and the end of words; whereas in the HMM-VAD of Varela et al. (2011), the state duration is controlled by the state-transition probabilities of the HMMs and speech pulse information. Note that both GMM- and HMM-based VADs require ground-truth speech/non-speech segments for training the statistical models. Unfortunately, these labeled segments are not available in NIST SREs.The VAD problem has also been formulated as an edge-detection problem. For example, in Li et al. (2002), two optimal 1D filters with responses invariant to various background noise levels are designed to detect the beginning edges and ending edges of the energy profile of speech signals. To detect the beginning edges, the filter has positive response to a beginning edge, negative response to an ending edge, and near-zero response to silence. The filter for detecting the ending edge has the opposite characteristics and has more time points. The filters are operated as a moving-average filter on the energy envelope and their outputs are compared with dynamic decision thresholds estimated from a 2-mode Gaussian mixture model.In recent NIST SREs, several sites provided the details of their VAD in the system descriptions. Typically, these systems use energy-based methods that estimate a file-dependent decision threshold according to the maximum energy level of the file (Kinnunen et al., 2009). Some sites used the periodicity of speech frames or the power of noise-removed speech frames to make speech/non-speech decisions (Hautamaki et al., 2007; Sun et al., 2008; Mak and Yu, 2010; Yu and Mak, 2011). An alternative approach is to use the ASR transcripts supplied by NIST to remove the non-speech segments (Dalmasso et al., 2009).In this paper, we propose a VAD that is specifically designed for NIST SREs. Special attention has been paid to address the low SNR, impulsive noise, and cross talks in the interview-style speech files. The main idea is to apply speech enhancement as a pre-processing step to boost the SNR of the speech segments, which facilitates the subsequence speech/non-speech decisions either by log-likelihood ratio tests or comparing with energy-based thresholds. While this strategy has been adopted in the past, e.g., Sun et al. (2009), Ramirez et al. (2004), and Marciniak et al. (2008), our proposed VAD has some important differences. For example, the VAD in Sun et al. (2009) requires the training of speech and non-speech GMMs, whereas ours does not require training. This requirement is a burden for situations like NIST SREs because labeled speech segments are not available. The Wiener filtering in Sun et al. (2009) and Ramirez et al. (2004) and the wavelet denoising in Marciniak et al. (2008) also need to strike a good balance between spectral distortion and the degree of noise removal because decisions of these VADs are based on the spectral features of the noise-reduced speech. Our VAD, on the other hand, does not use the spectral features for VAD decisions. Therefore, it can leverage the over-subtraction to boost the SNR for better discrimination between speech and non-speech. To the best of our knowledge, our study provides the first comprehensive comparison between different VADs for NIST SREs. Results based on NIST 2010 SRE suggest that the proposed VAD outperforms the VAD in AMR2, the transcriptions provided by NIST, and statistical model-based VAD.In Section 2, we highlight the characteristics of the interview speech files in NIST SRE and explain why conventional VAD techniques will encounter difficulty in detecting speech in these files. Then, in Section 3, we outline two state-of-the-art statistical VADs and explain how they can be applied to NIST SREs. Section 4 proposes using speech enhancement techniques as a pre-processing step for improving the statistic model based VAD and energy-based VAD. Experimental evaluations comparing different types of VADs under NIST 2010 SRE are then presented in Section 5.In early NIST SREs, researchers seldom pay attention to VAD. This is because the telephone speech files in early SREs have high signal-to-noise ratios (SNRs), making VAD a trivial task. The high SNR in telephone speech is resulted from the close proximity between speaker's mouth and the handset. In interview speech, however, different microphone types were used for recording. For example, 12 microphones were used in NIST 2008 SRE,11Some of these microphones are of the same models, but they were placed at different positions with respect to the speakers.and in NIST 2010 SRE, the interviewees used different types of far-field microphones, such as lavaliere microphones, camcorders, and hanging microphones (Martin and Greenberg, 2010). These microphones lead to files with the following characteristics:1Low SNR. Depending on the microphone types, some of the interview speech segments have low SNR, causing problems in conventional VAD. Fig. 1(a) shows the waveform of an interview speech file (ftvhv.sph) in NIST 2008 SRE, and Fig. 1(c) highlights a short segment of the same file. The NIST STNR Tool22This tool is part of the Speech File Manipulation Software(SPHERE) Package Version 2.7, available from http://www.nist.gov/itl/iad/mig/tools.cfm.indicates that the SNR of this file is 5dB. Although this level of SNR is not very low, it already causes numerous errors in an energy-based VAD, as indicated by “AE-VAD” in the lower panel of Fig. 1(c). Fig. 2shows the histograms of SNR of interview speech files in NIST 2008 and 2010 SRE. While the mean SNRs of these two databases are high (22dB and 21dB, respectively), about 2% of the files have SNR less than 5dB, i.e., about 2% of the files have situation similar to Fig. 1. The VAD errors in these files will have detrimental effect on speaker verification performance, which will be demonstrated in Section 5.Impulsive. Some of the files in NIST 2010 SRE contain a large number of spikes that seriously mask the amplitude of speech segments, as illustrated in Fig. 3.Low-energy speech superimposed on periodic background signals. Some files contain low-energy speech superimposed on periodic background noise, as exemplified in Fig. 4.Cross talk. Each interview speech file in NIST 2010 SRE contains two channels, one recording the speech of an interviewee and the other the speech of an interviewer. As far-field microphones were used for recording interviewee's speech, a low-energy crosstalk signal appears in the interviewee's channel when the interviewer is talking, causing the VAD mistakenly considers the crosstalk as belonging to the interviewee. This situation is exemplified in Fig. 5(a) in which the microphone of the interviewee's channel picks up the speech of the interviewer in Interval A.As shown in these figures, conventional energy-based VAD fails to detect the speech segments under such conditions.This section highlights the merit of the statistical model based VAD (Sohn et al., 1999) and GMM-based VAD (Fukuda et al., 2010) and explains how they can be applied to detect the speech segments of NIST SRE speech files. The section focuses on the decision logic and threshold determine methods that are specifically designed for the SREs.In SM-based VAD (Sohn et al., 1999), speech/non-speech segmentation is formulated as a hypothesis testing problem:(1)H0:speechabsent:Y(m)=B(m)H1:speechpresent:Y(m)=X(m)+B(m)where Y(m), X(m), and B(m) represent the DFT of noisy speech, clean speech, and background noise at frame m, respectively. The complex DFT coefficients in Y(m), X(m), and B(m) are assumed to be independent and normally distributed. For each frame m, a VAD score Γ(m) is computed based on the VAD score of the previous frame and the likelihood ratio Λ(m) at the current frame:(2)Γ(m)=P(H0)P(H1)a01+a11Γ(m−1)a00+a10Γ(m−1)Λ(m)≷H0H1ηwhere aij≜Pr(q(m)=Hj|q(m−1)=Hi) are state-transition probability and P(H0) and P(H1) are prior probability. Because DFT coefficients are assumed to be independent, we have(3)Λ(m)=∏k=0K−1p(Yk(m)|H1)p(Yk(m)|H0)1/Kwhere K is the number of frequency bins and p()'s are complex normal densities. The VAD score is then compared with a decision threshold η to make speech/non-speech decisions.To apply SM-based VAD to detect speech segments in NIST SRE files, the SM scores Γ(m) of the entire utterance are ranked in descending order as shown in Fig. 6. Then, a fixed percentage of scores in the lower and upper ends of the ranked list are selected and assumed to be the background frames and peak frames, respectively. The VAD's fixed decision threshold is a linear combination of the score mean of the lower end (Γ¯b) and the minimum score in the upper end:(4)η=νΓ¯b+(1−ν)min{Γ(p1),…,Γ(pL)},where 0≪ν<1 is a weighting factor and {Γ(p1), …, Γ(pL)} are top-L scores. Note that L cannot be too large; otherwise the rank list may include the peaks of some high-energy speech frames, which will lead to under-estimation of η. However, when L is too small, some medium-amplitude spikes will be missed. It was found that the influence of spikes can be largely eliminated by using the minimum amplitude in this ranked list, as evidenced by the VAD result in the Fig. 3.The above procedure raises the issue of determining an appropriate percentage for the lower and upper ends of the ranked score list. These percentages can be founded by inspecting several interview speech files in NIST 2005–2008 SREs. By examining some of these files, we found that it is fairly safe to assume that among all the frames in a speech file, 10% are background frames and 5% contain signal peaks.Mel-frequency cepstral coefficients (MFCCs) are known to be inadequate for discriminating speech and non-speech frames, primarily because of the similarity between the static MFCC vectors of speech and background noise. On the other hand, the harmonic structures of speech and background noise are more distinguishable and more noise robust (Gu and Rose, 2001). Based on this argument, Fukuda et al. (2010) extracted the harmonic-structure-based features from the middle range of the cepstral coefficients obtained from the discrete cosine transform (DCT) of the power spectral coefficients.The cepstral coefficients ci(m) with small and large indexes i are liftered out because they include long and short oscillations. On the other hand, the coefficients in the middle part of the cepstrum capture the harmonic structure information in the human voice. The liftered cepstrumcˆi(m)is converted back to the log power spectrum by inverse DCT, followed by the exponential transform to obtain the linear power spectrum. The power spectrum are finally converted to mel-cepstrumqˆn(m)by applying a mel-scale filter bank and DCT, where n is the bin number of the harmonic structure-based mel cepstral coefficients. This feature captures the envelope information of the local peaks in the frequency spectrum corresponding to the harmonic information in the speech signals. Fig. 7shows the procedure of extracting the harmonic-structure-based features.Dynamic (spectro-temporal) features capture the variation of the spectral envelopes along the time axis. They are typically obtained by estimating the derivative of 5–9 consecutive acoustic vectors. The first-order derivative of a sequence of cepstral vectors is called delta cepstrum, and the second-order derivative is called delta-delta cepstrum.In GMM-based VAD, the speech/non-speech decision at frame m is given by the log-likelihood ratio(5)L(m)=logp(y(m)|H1)−logp(y(m)|H0)≷H0H1η,where the acoustic vectorsy's are assumed to follow a mixture of Gaussian distribution:(6)p(y|Hi)=∑j=1JwijN(y;μij,Σij)wherewij,μijand Σijare the mixture weights, mean vectors and covariance matrices for either speech (i=1) or non-speech (i=0) model.The decision threshold η is determined by a strategy similar to that of SM-VAD (Eq. (4)) described in Section 3.1. Specifically, 20% and 5% of a speech file are assumed to contain background frames and signal peaks, respectively.Unlike the SM-based VAD, the GMM-based VAD requires the training of two GMMs – one representing speech and another one representing non-speech. This means that some speech files with speech and non-speech segmentations are required. In theory, the segmentations had better be the ground-truths, i.e., they need to be done by listening tests and human inspections of spectrograms. This is not a problem if clean speech files are available and the VAD is tested on the same files with noise added to them, e.g., the experiments in Fukuda et al. (2010). However, in NIST SREs, the requirement of ground-truth segmentations will cause difficulty because no clean speech files are available for the listening tests. Even if we can find some interview-style speech files with high enough SNR for the listening tests, they may be too clean and therefore cannot represent the realistic situations in other noisy speech files. Furthermore, hand labeling a large number of speech files is too laborious and time-consuming.Here, we propose an automatic method that can find speech and non-speech segments that are close enough to the ground-truths for training the GMMs. Fig. 8shows the procedure. Unlike the VAD in Fukuda et al. (2010), our GMM-based VAD contains an extra processing block – Frame Index Extraction – that finds the frame indexes of speech and non-speech segments with very high confidence of being correct. This seems to be a chicken-and-egg problem because if a reliable VAD exists, we do not need to build a new one in the first place. However, having some reliable speech and non-speech segments does not mean that we need a reliable VAD to detect both at the same time. The idea is that we can always make a simple energy-based VAD very reliable in detecting speech but extremely unreliable in detecting non-speech by adjusting the decision threshold such that it can achieve a very low false alarm (consider non-speech as speech) but having a very high missing rate (consider speech as non-speech). A similar argument applies to the reliable detection of non-speech. Because this simple VAD can only maintain either the false alarm or missing rate low but not both, it can only be used as a pre-processing step in more sophisticated VADs such as the one illustrated in Fig. 8.The idea is to leverage the large number of speech files in NIST SREs. Specifically, for each interview-style speech files in the training set (e.g., past NIST SREs), a simple energy-based VAD is used to determine the energy of all frames. Then, the frames are ranked in ascending order of energy as illustrated in Fig. 9. The top 5% of the ranked list are discarded because the high energy is most likely caused by spiky signals instead of speech. Because of the simplicity of the energy-based VAD, there will be many false alarms and misses in the detections. Therefore, only a small percentage in the upper- and lower-part of the ranked list are considered as speech and non-speech, respectively. More precisely, 99% of the frames in the middle of the ranked list will be discarded, and only the frames with a very high confidence of having a correct segmentation are retained for training the GMMs.Given the frame indexes of speech and non-speech segments, static harmonic features and long-term dynamic features are extracted and concatenated, forming two streams of feature vectors as shown in Fig. 8. These concatenated features vectors are then used to train the GMMs. In this work, 3569 interview-style utterances from NIST SRE 2005–2008 were used for training the GMMs. This amount to 280,010 training vectors per GMM. The number of mixtures J was set to 32, and all Gaussians have a full covariance matrix.One advantage of the GMM-based VAD is that it is less susceptible to spiky signals because these signals have low-level of harmonic contents and their temporal property is also different from that of speech signals. However, GMM-based VAD also has its own limitations. In particular, because the GMM-based VAD does not rely on SNR, it may falsely detect some weak cross-talks from other speakers as speech segments as long as the cross-talks contain speech-like characteristics. This drawback can be alleviated by using spectral subtraction as a pre-processor because the weak cross-talks will be considered as background signals so that they can be largely eliminated in the spectral subtraction process. Further discussions on the use of spectral subtraction as a pre-processor can be found in the next section.Noise removal is a vital step for pre-processing the interview speech files in NIST SREs because many of them have low SNR. This paper proposes to apply spectral subtraction (SS) with a large over-subtraction factor to discard the background noise as much as possible before passing the enhanced speech to an energy-based VAD. Advanced speech enhancement techniques (e.g. MMSE (Ephraim and Malah, 1984) and LSA-MMSE (Ephraim and Malah, 1985)) have not been used because audio quality of reconstructed speech is not the main concern.33Acoustic features (MFCCs) were extracted from the original signals instead of from the reconstructed signals.Instead, it is more important to increase the SNR in speech regions and to minimize the background noise in non-speech regions. Spectral subtraction meets this requirement without unnecessarily complicating the whole system.To obtain the enhanced speechxˆ(t)from the noisy speech y(t) at frame m, we implemented the spectral subtraction (Boll, 1979; Deller et al., 1993; Virag, 1999) of the form(7)Xˆk(m)=[|Yk(m)|−α(m)|Bˆk|]ejφk(m)if|Yk(m)|>(α(m)+β(m))|Bˆk|β(m)|Bˆk|ejφk(m)otherwise,where k is the frequency bin index, φk(m) is the phase of Yk(m),Bˆkis the average spectrum of some non-speech regions, α(m) is an over-subtraction factor for removing background noise, and 0<β(m)≪1 is a spectral floor factor ensuring that the recovered spectra never fall below a preset minimum. The value of α(m) and β(m) are computed as(8)α(m)=−12γ(m)+c(αmin≤α(m)≤αmax)β(m)=βminifγ(m)<1βmaxotherwisewhereγ(m)=(∑k|Yk(m)|)/(∑k|Bˆk|)is the a posteriori SNR, c is a constant (=4.5 in this work), αmin, αmax, βmin, and βmax constrain the allowable range of the over-subtraction factor and the noise floor. We set these values such that the speech spectra are over-subtracted when the SNR is low. In this work, we set αmax=4, αmin=0.5, βmax=0.05, and βmin=0.01. These values were determined empirically through experimentations on an i-vector systems (see Section 5.5) and by visual comparison between the original and reconstructed waveforms of several speech files. Because of the small βmax (≪1), musical noise occurs when some frequency components meet the condition in the upper branch of Eq. (7) while some others do not. While musical noise appears in both speech and non-speech regions, its energy in non-speech regions is not high enough to cause false detections, as evident in Fig. 1(d). Also, although this musical noise will degrade the perceptual quality of the denoised speech, it is not a concern here because the denoised speech is only used for VAD, not for speaker recognition, i.e., our goal is to detect voice activity rather than speech enhancement.Note that if the background noise is high, consonants with weak energy will be masked by the noise. Therefore, it is more appropriate to exclude these weak consonants by means of over subtraction. On the other hand, if the background noise is low,|Bˆk|in Eq. (7) is almost zero, meaning that consonants will also be included.Fig. 2 shows the histograms of the speech files in 2008 and 2010 SREs before and after spectral subtraction. Evidently, spectral subtraction can improve the SNR significantly.Fig. 10shows the structure of the proposed energy-based VAD, which we refer to as SS+AE-VAD. For each utterance, after noise removal, the energy of each 10-ms frame is computed at every 1ms. To avoid excessive fluctuation in the energy profile, a 40-tap moving average filter is applied to smooth the profile.The presence of spikes in some files affects the maximum SNR in these files, which needs to be taken care of when determining the VAD decision threshold. In particular, these spikes lead to overestimation of the decision threshold if it is based on the background amplitude and the maximum amplitude. Consequently, low-energy speech segments could be mistakenly detected as non-speech. To address this problem, we have developed a similar strategy as the one in Section 3.1, but considering signal amplitude rather than statistical scores. The decision threshold is a linear combination of the mean of background amplitude (a¯b) and the minimum of the signal peaks:(9)η=νa¯b+(1−ν)min{a(p1),…,a(pL)},where {a(p1), …, a(pL)} are the amplitudes (after the moving average filter) of L largest-amplitude frames. In this work, L was set to 1% of the total number of frames in the speech file. By comparing the amplitude of each frame in the file with the threshold, those frames with amplitude larger than the threshold are considered as speech frames.Fig. 1(b) and (d) shows the same speech file and segment as in Fig. 1(a) and (c) but after spectral subtraction. Evidently, with the background noise largely removed, speech and non-speech intervals can be correctly detected by an energy-based VAD. To highlight the advantage of spectral subtraction, Fig. 1(c) and (d) compares the segmentation results of SS+AE-VAD and that of the ETSI-AMR coder (Option 2). The figure suggests that this coder over-estimates the length of speech segments, whereas the SS+AE-VAD correctly detects the speech segments.

@&#CONCLUSIONS@&#
A voice activity detector specially designed for extracting speech segments from the interview-speech files in NIST SREs has been proposed and evaluated under the NIST 2010 SREs protocols. Several conclusions can be drawn from this work: (1) noise reduction is of primary importance for energy-based VAD under low SNR; (2) it is important to remove the sinusoidal background noise as this kind of background signal could lead to many false detection in energy-based VAD; (3) a reliable threshold strategy is required to address the spiky (impulsive) speech signals; and (4) our proposed spectral subtraction VAD outperforms the segmentations derived from the ASR transcripts provided by NIST, the VAD in the advanced speech coder (ETSI-AMR, Option2), the state-of-the-art statistical-model-based VAD, and Gaussian-mixture-model-based VAD in speaker verification.The proposed VAD is optimized for interview speech in NIST SREs. It is of interest to investigate its performance on other databases, including those in speech recognition such as CENSREC-1-C speech database (Kitaoka et al., 2007) and Aurora 2 database (Hirsch and Pearce, 2000). The present study assumes that background noise is stationary. It is of interest to apply methods – such as Ghosh et al. (2011) – that can deal with non-stationary noises in NIST SREs.