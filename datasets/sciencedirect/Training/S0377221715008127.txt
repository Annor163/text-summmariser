@&#MAIN-TITLE@&#
Sparse and robust normal and t- portfolios by penalized Lq-likelihood minimization

@&#HIGHLIGHTS@&#
We propose a methodology based on the minimization of a description length criterion.We propose a re-weighting algorithm to compute sparse and robust portfolios.Validation on simulated and real-world financial data support our method.

@&#KEYPHRASES@&#
Investment analysis,Penalized least squares,q-entropy,Sparsity,Index tracking,

@&#ABSTRACT@&#
Two important problems arising in traditional asset allocation methods are the sensitivity to estimation error of portfolio weights and the high dimensionality of the set of candidate assets. In this paper, we address both issues by proposing a new criterion for portfolio selection. The new criterion is a two-stage description of the available information, where the q-entropy, a generalized measure of information, is used to code the uncertainty of the data given the parametric model and the uncertainty related to the model choice. The information about the model is coded in terms of a prior distribution that promotes asset weights sparsity. Our approach carries out model selection and estimation in a single step, by selecting a few assets and estimating their portfolio weights simultaneously. The resulting portfolios are doubly robust, in the sense that they can tolerate deviations from both assumed data model and prior distribution for model parameters. Empirical results on simulated and real-world data support the validity of our approach.

@&#INTRODUCTION@&#
Asset allocation aims to determine an optimal portfolio from a large set of assets based on their performance and diversification. Typically, portfolio managers want to set up portfolios with optimal risk-return properties, along the lines of the mean-variance framework (i.e. multi-objective portfolio optimization), or which closely track an index (i.e. index tracking) by investing in a small number of assets, thereby limiting their transaction and monitoring costs. It is known, however, that the performance of optimal portfolios largely affected by the uncertainty about the model parameters and the data distribution, which is traditionally assumed to be Gaussian (Frost & Savarino, 1988; Markowitz, 1952; Merton, 1980; Michaud, 1989). Portfolio weights are very sensitive to errors for the model parameter estimates induced by model misspecification (Best & Grauer, 1991; Jagannathan & Ma, 2003); for example, even a small change in the covariance and mean estimates of the multivariate Gaussian model might cause substantial changes in the optimal portfolio asset allocations (De Miguel & Nogales, 2009). As a result, the out-of-sample performance of the portfolio is often unsatisfactory when compared to the in-sample performance. The effect of estimation bias is frequently enhanced by the large pool of candidate assets and by the strong correlation between asset returns. In addition, financial data are leptokurtic and contaminated by outliers (Cont, 2001). The misspecification of the underlying Gaussian distribution causes then imprecise estimates, if deviations from the assumed model are not appropriately addressed.To deal with these issues, two distinct classes of statistical methods have become increasingly popular in the financial literature. To achieve robust estimation of model parameters, various robust procedures to estimate the covariance matrix have been proposed (De Miguel & Nogales, 2009; Kolm, Tütüncü, & Fabozzi, 2014; Ledoit & Wolf, 2004; Vaz de Melo & Camara, 2005; Welsch & Zhou, 2007). Previous work (Ferrari & Paterlini, 2010), focusing only on minimizing an empirical version of the q-entropy in a mean-variance framework under normality, showed the advantages of using an estimator that can deal with the discrepancy between the data and the assumed model. To achieve sparse portfolios with a relatively small number of assets corresponding to active (i.e. non-zero) weights from a large pool of assets, several authors have advocated the use of penalized least squares methods. For instance, the least absolute shrinkage and selector operator (Lasso) (Tibshirani, 1996) has proved to be very useful in asset allocation since it not only increases the stability of portfolio weights, but also enforces sparse solutions by imposing a penalty on the asset weights (Carrasco & Noumon, 2012; De Miguel, Garlappi, Nogales, & Uppal, 2009; De Mol, Giannone, & Reichlin, 2008; Fan, Zhang, & Yu, 2012).Considering the stylized facts of financial data (Cont, 2001) and the high-dimensionality of the index tracking problem, our main target is to obtain well-tracking and sparse solutions: portfolios with few active weights that replicate as closely as possible a given index or benchmark. Then, we introduce a new class of algorithms for portfolio selection, which merges the strengths of both, robust and penalized estimation methods. So far, only the robust properties of a q-entropy estimator under normality in a mean-variance framework had been investigated (Ferrari & Paterlini, 2010). Here, instead our approach involves the minimization of a description length criterion that accounts for the uncertainty about the data as well as that related to the parametric model structure. Both sources of uncertainty are coded in terms of the q-entropy, a generalized information measure introduced by Havrda and Charvát (1967) and studied by Tsallis (1988) in statistical mechanics. The q-entropy coding ensures double robustness by protecting against two sources of model misspecification: (i) it down-weights observations that diverge from the data model assumed for the assets; (ii) it mitigates the effect of parameter estimates that are far from the assumed prior structure for the parameters. The overall behavior of our criterion-function depends on a tuning parameter q, which controls the trade-off between the statistical accuracy and the stability of estimates (Ferrari & La Vecchia, 2012). When q → 1 our procedure is equivalent to maximum a posteriori estimation of the parameters, yielding optimal statistical efficiency but scarce robustness; values of q < 1 yield instead robust estimates with negligible loss of efficiency. Importantly, our approach tackles for the first time the model selection and estimation in a single step by identifying optimal tracking portfolios with few active positions by means of a penalty function on the asset weights, with size depending on a regularization parameter λ. Larger penalty values imply portfolios with a relatively small number of assets. The solution to the new criterion can be efficiently solved by iterative algorithms that we develop in the paper.The remainder of the paper is structured as follows: in Section 2, we describe the general methodology based on the two-stage description length minimization framework. In Section 3, we propose an efficient re-weighting algorithm to compute optimal portfolios. In the same section, we also focus on two important cases, when the portfolio returns are assumed to follow a normal or a t- distribution, while the prior structure on the parameters is coded by a Laplace distribution. In Section 4, we compare the performance of our method by Monte Carlo simulation with other benchmark approaches. In Section 5, we illustrate our index tracking method by considering real-world data. Section 6 concludes the paper.LetX=(X1,…,Xp)′be a vector of p assets returns measured over a fixed period (e.g. one day) which are assumed to follow some unknown multivariate distribution. A financial portfolio return, can then be defined by the linear combinationX′β, whereβ=(β1,…,βp)′∈Rpis a vector of constants playing the role of asset allocation weights. Our main aim is to choose the asset weightsβto track as closely as possible a target market index return, Y. DefineZ=(Y−X′β)/σand assume that it has probability distribution G(z) and probability density function (pdf) g(z), withEG(Z)=0. Instead of the random target market index return, we allow specifying a fixed target, say y*. This is interpreted as a degenerate random variable with single mass point on y*. To emphasize possible model misspecifications we distinguish between the true (unknown) density g(z), and the model density f(z). The latter is an user-specified model chosen to represent the data which does not necessarily coincide with the true density g. For example, f may be the standard normal distribution or the t- distribution with ν degrees of freedom or any other suitable parametric model for financial data. Although σ can be regarded as fixed, we typically estimate both σ andβ.Given observation pairs{(yi,xi),i=1,…,n}at time i, of the target index Y and p-dimensional vectorXof assets over n time periods, we compute the portfolio weights,β^q,λ,by maximizing the following two-stage description length, or generalized description length (GDL) criterion:(1)D^q,λ(β,σ)=∑i=1nLq{f(yi−xi′βσ)}+∑j=1pLq{h(βj;λ)},for fixed tuning constants λ ≥ 0 and q ≤ 1. In (1), Lq(·) is the generalized q-logarithm(2)Lq(a)={(a1−q−1)/(1−q),q≠1,log(a),q=1,and h(β; λ) is a univariate symmetric density function with zero mean with scale parameter λ.Criterion (1) is regarded as a two-stage description of the total information. The first sum in (1) is a goodness-of-fit measure: it describes the information provided by the data given a nominal model f indexed byβand σ. When q → 1, Lq(·) → log (·), so the first term in (1) coincides with the usual log-likelihood function. The second sum,∑j=1pLq{h(βj;λ)},is a penalty term describing the information about the model structure itself and involves user-specified densities h(βj; λ),j=1,…,p. Such a penalty term drives the model selection step and leads to sparse optimal solutions by forcing to zero the components in the observed portfoliosxi′β,i=1,…,nthat are not much helpful in explaining the target values yi,i=1,…,n. To gain more insight on criterion (1), it is helpful to consider the case where q → 1, which implies Lq(·) → log (·) and(3)D^q,λ(β,σ)→log{∏i=1nf(σ−1(yi−xi′β))∏j=1ph(βj;λ)}.This shows that when q is near 1, minimization of (1) is equivalent to maximum a posteriori (MAP) estimation ofβ, where h(βj; λ) plays the role of prior density function for βj.Different probability density functions could be chosen as penalty functions h(βj; λ). For example, Fig. 1(left) shows Lq(h(β; λ)) for a single coefficient βjwhen h(· ; ·) is a Normal, a Laplace or a Double Pareto density withq=1/2andλ=1. When comparing the Laplace with the Double Pareto distribution, we notice that the former yields weaker penalties when βjis close to zero and much stronger when βjmoves further away from zero. In this paper, we focus on considering penalties based on the Laplace as penalty function, but the model can be easily extended to include other penalty functions without requiring any algorithmic modification. Fig. 1 (right) shows the effect of increasing the value of λ in the Laplace penalty of the β coefficients. The parameter λ controls for the size of h(βj; λ): increasing the λ value leads then to identify optimal solutions with a smaller number of active β coefficients. Note that for many choices of h, our penalty function has the property of being non-convex. This aspect is important to deal with the shortcoming of the convex Lasso penalty, which is known to produce biased estimates for large (absolute) coefficients (Fan & Li, 2001; Zou, 2006). As a solution, various authors proposed using penalties that are singular at the origin (just like the ℓ1-penalty) in order to promote sparsity, but non-convex, in order to countervail bias (see Gasso, Rakotomamonjy, and Canu (2009) for a discussion of benefits of using non-convex penalties).Differentiating the criterion function (1) with respect to parameters (β, σ)′ gives(p+1)estimating equations, which can be written as follows using vector notation(4)0=∇D^q,λ(β,σ)=∑i=1nf(zi)1−q∇f(zi)f(zi)+∑j=1ph(βj;λ)1−q∇h(βj;λ)h(βj;λ),wherezi=(yi−xi′β)/σand “∇” denotes the gradient operator, so that∇D^q,λ(β,σ)is a(p+1)-vector of derivatives with respect to the elements of the parameter vector (β, σ)′. Next, we introduce some notations by re-writing the estimating equations as(5)0=∑i=1nwq,i(β,σ)u(yi,xi,β,σ)+hλ′(β).whereu(xi,β,σ)=∇logf(σ−1(yi−xi′β))is the likelihood score vector with(p+1)elements and(6)wq,i(β,σ)=f(σ−1(yi−xi′β))1−qis a scalar data-dependent weight for the ith score. The termhλ′(β)in (5) denotes the(p+1)-vector of first derivatives with elements{hλ′(β)}j=∇Lq(h(βj;λ))=vq,j(βj,λ)s(βj,λ),j=1,…,p,and{hλ′(β)}p+1=0,where(7)s(βj,λ)=∇logh(βj;λ),vq,j(βj,λ)=h(βj;λ)1−qare the(p+1)-dimensional prior score vector and scalar importance weight for βj, respectively.It is important to note that the above estimating equations imply a double weighting scheme which at once controls for the importance of observations xi,i=1,…,n,and candidate coefficients βj,j=1,…,p. For unusual observations incompatible with the model f, the relative importance of the scoreu(xi,β, σ) is automatically reduced since the corresponding weights wq, i(β, σ) are proportional to a power-transformation of the assumed density model f. Particularly when q < 1, wq, iis typically small when the tracking distance|yi−xi′β|/σis large. Therefore, linear combinationsxi′βoccurring far away from the target yireceive small weights. An analogous behavior is implied for the penalty term by the weights vq, j(βj, λ), which are small if |βj| are large. For instance, ifβj∼N(0,λ−1),and h is the normal density function, then vq, jis small when βjis far away from 0, so that the penalization is allowed to be larger when the jth component is close to zero.A noteworthy special case of our approach is the popular Lasso method (Tibshirani, 1996) which is obtained when: q → 1, f(z) is the normal density function, and the penalty term uses the Laplace density functionh(β;λ)=λexp{−λ|β|}/2. In the Lasso case, the data weights wq, i,i=1,…,n,and parameters weights vq, j,j=1,…,p,are all equal to 1 and therefore do not affect the optimization process. The constant weighting scheme implied by the Lasso leads to unstable behavior and inaccurate selections for large coefficients (Fan & Li, 2001). To improve the accuracy of the estimates, Zou (2006) proposed to vary the penalty term introducing a weighting scheme, similar to our weights vq, j,j=1,…,p. Differently from our approach, however, Zhou’s weights are based on OLS-estimates and the varying tuning parameter is determined by either a prior distribution or a particular expectation on the market.In this section, we provide a general algorithm for portfolio selection and then derive two important special cases of the algorithm when the working model f for the data is represented by either the normal or t- distributions. Computing the optimal portfolios,β^q,λ,by direct minimization of (1) is challenging because such a criterion is typically non-convex in the parameters. This issue can be addressed by dividing the optimization of (1) into a sequence of simpler (convex) optimization steps. Particularly, when the weights wq, iand vq, jin the estimating Eq. (5) are fixed constants, finding the solution to (5) reduces to a convex penalized likelihood problem as long as the densities f and h are log-convex. This suggests an iteratively re-weighted strategy to find the estimates, where we iterate parameter estimation by solving (5) with given weights and then update the weights based on the latest parameter estimates. Since re-weighting is applied to both data and penalty scores, we call this algorithm a doubly re-weighted (2RE) algorithm for portfolio selection.Consider observations{(yi,xi),i=1,…,n},on the target index return Y and p-dimensional vector of assets returnsXover n time periods. For given tuning constants q ≤ 1, λ ≥ 0, the algorithm consists of the following steps:0.At Steps=0,compute initial parameter valuesβ^(s)andσ^(s).Sets=s+1,and update the data weightsw^q,i(s)=f((yi−xi′β^(s−1))/σ^(s−1))1−q,i=1,…,n,and the penalty weightsv^q,j(s)=h(β^j(s−1);λ)1−q,j=1,…,p.Find the parameter valuesβ˜andσ˜by minimizing(8)∑i=1nw^q,ilogf((yi−xi′β)/σ)+∑j=1pv^q,jlogh(βj;λ).Letz=(y−x′β)/σandz˜=(y−x′β˜)/σ˜, whereβ˜andσ˜are obtained in Step 2. Then the re-centered parametersβ^(s)andσ^(s)are found by solving the equationfq(z)=fq(z^(s))inβand σ, where fqis the power-transformation of f defined byfq(z)=f(z)q∫f(z)qdz.Note that solving the above equation is equivalent to a simple parameter transformation (e.g. when f is the N(0, 1) density, we haveβ^(s)=β˜andσ^(s)=σ˜/q). To ensure existence of the solution, we require that the power-transformed density fqis in the same family as f.Repeat Steps 1 and 2 until a stopping criterion is satisfied.Next we give some remarks on the 2RE algorithm. First, if the initial estimates are obtained by solving (5) with equal weightswq,i(β,σ)=1,i=1,…,n,andvq,j(βj,λ)=1,j=1,…,p,this corresponds then to the Lasso approach. However, since the Lasso solution is non-robust and typically sensitive to the presence of outliers, robust approaches for computing stable starting points could also be considered, for example by trimming out the 10% most extreme observations for each asset before computing the Lasso estimates. Second, the aim of the re-scaling described in Step 3, is to mitigate the potential bias implied by the first term of the estimating Eq. (5). Specifically, first we solve the re-centering equation(9)E[wq,i(β,σ)u(Y,X,β,σ)]=0where expectation is with respect to thef((Y−X′β0)/σ0),uis the likelihood score defined in (5),wq,i(β,σ)=f((Y−X′β)/σ,andβ0 and σ0 are regarded as the true parameter values. A calculation analogous to that in Ferrari and La Vecchia (2012) shows that (9) is satisfied if and only iff((Y−X′β0)/σ0)=fq((Y−X′β)/σ),where fqis the power-transformation in Step 3 of the above algorithm. In many common cases this equation has a simple closed-formed solution involving a simple parameter transformation. For example, if f is the normal pdf, then (9) is satisfied byβ0=βandσ0=σ/q. For the normal model, clearly re-centering of the portfolio coefficients is not required, but we still need to adjust the variance estimate from Step 2. Ferrari and La Vecchia (2012) use this re-centering method in the context of estimating estimation equation analogous to (5) but not involving a penalty term. Their estimator is Fisher consistent (i.e. the estimator is asymptotically unbiased whenf=g), which motivates the same re-centering strategy in this paper. Finally, we note that another potential source of bias implied by the estimating Eq. (5) is given by the penalty function ∑jLq(h(βj; λ)) in criterion (1); the heuristic derivation in the Appendix A illustrates how this bias can be controlled by choosing sufficiently large values of λ.If the nominal model f forZ=(Y−X′β)/σ(standardized difference between the target index Y and the portfolioX′β) is the univariate normal model andh(βj;λ)=λexp{−λ|βj|}/2,j=1,…,p,Step 2 of the algorithm in Section 3.1 computes the portfolio estimates by solving(10)β^(s)=argminβ{∑i=1nw^q,i(s−1)12(yi−xi′βσ^(s−1))2+λ∑j=1pv^q,j(s−1)|βj|}.The weightsw^q,iandv^q,jare computed using estimates obtained in Steps−1as follows:(11)w^q,i(s−1)=[12πσ^2(s−1)exp{−(yi−xi′β^(s−1))22σ^2(s−1)}]1−q,v^q,j(s−1)=[λ2exp{−λ|β^j(s−1)|}]1−q.The portfolio variance is also updated using estimates from Steps−1as(12)σ2^(s)=∑i=1nw^q,i(s−1)(yi−xi′β^(s−1))2q∑i=1nw^q,i(s−1).When the portfolio variance is not estimated and instead set equal to a fixed target value, say σ*2, we then haveσ^2(s)=σ*2,for all s ≥ 0.The updating rule in (10) is a weighted L1-penalized quadratic problem (Lasso) and can be solved efficiently using existing algorithms. Tibshirani (1996) and Turlach, Venables, and Wright (2005) proposed to use quadratic optimization to solve the L1-penalized optimization problem, while Friedman, Hastie, Hfling, and Tibshirani (2007) developed a coordinate-wise approach that works efficiently with convex optimization problems. To compute (10), we use the gradient projection algorithm developed by Figueiredo, Nowak, and Wright (2007), as Gasso et al. (2009) have shown that such algorithm is more efficient to solve problems such as ours compared to quadratic programming and coordinate-wise optimization.Assume that the variableZ=(Y−X′β)/σfollows a t- distribution with meanxi′βand variance σ with probability density function(13)fν(yi;xi,β,σ)=Γ(ν+12)νπΓ(ν2)(1+(yi−xi′β)2νσ2)−ν+12,where ν is the number of degrees of freedom and Γ is the Gamma function. The value of ν > 1 are fixed and will not be estimated. Under the t- model, for each Step s ≥ 1 we compute{β^(s),σ^(s)}by solving(14)argminβ,σ{−(ν+12)∑i=1nw^q,i(s−1)log{1+(yi−xi′β)2νσ2}+λ∑j=1pv^q,j(s−1)|βj|},where σ > 0 and ν > 1 and the estimation weightsw^q,iupdated from estimates obtained in Steps−1as follows(15)w^q,i(s−1)=[fν(yi−xi′β^(s−1))/(σ^(s−1))]1−q,i=1,…,n,where fνis the probability density function in (13) and the penalty weightsv^q,jhave the same expression as in (11).In this case, the updating rule (14) (Step 2 of the general algorithm in Section 3.1) represents a non-convex problem. Thus, solving (14) can lead to unreliable estimates, especially when the number of assets p is large. Under the t- model, a stable approach for (weighted) likelihood optimization is given by the Expectation-Maximization (EM) algorithm. The EM algorithm exploits the well-known fact that a t- distribution can be equivalently represented as a scale mixture of normals; particularly, one observation from the t- model can be written aszi∼N(μ,σ2Ui−1)where Uifollows a Gamma distribution Ui∼ Γ(ν/2, ν/2) (McLachlan & Krishnan, 2007). These considerations motivate the following EM steps, which specify Step 2 of the algorithm in Section 3.1 for the t- distribution.For any s > 0, given robust weightsw^q,i(s−1)andv^q,j(s−1)obtained in Steps−1,first set the initial mixture weightsu^i=1/n,i=1,…,n. Then, obtain the updated estimatesβ^(s)andσ^(s)by iterating the following EM steps•M-Step: this step computes weighted parameter estimatesβand σ using the mixing constantsu^i:(16)β*=argminβ{∑i=1nw^q,i(s−1)u^i(s−1)12(yi−xi′βσ^(s−1))2+λ∑j=1pv^q,j(s−1)|βj|},(17)σ*2=∑i=1nw^q,i(s−1)u^i(s−1)(yi−xi′β^)2∑i=1nw^q,i(s−1)u^i(s−1)×ν(ν+1)q−1.wherew^q,i(s−1)is proportional to the t- probability density function as in (15) evaluated inβ^(s−1),σ^(s−1),the parameter estimates obtained in Steps−1.E-Step: update the mixing constants as follows:(18)u^i=(νq+1)σ*2νqσ*2+w^q,i(s−1)(yi−xi′β*)2,i=1,…,n,whereβ^,σ^are computed in the M-Step andνq=(ν+1)q−1.We refer to Appendix B for the derivation of the M- and E-Steps.The parameter λ controls for the size of the penalty. Large λ values imply more parsimonious portfolios with a larger number of assets having zero weight. In the literature of penalized regression, similar tuning constants are chosen by information theoretical criteria, such as the AIC and BIC selection criteria yielding good empirical performances (Zhang, Li, & Tsai, 2010). We will follow the same approach here. Many authors, including Ronchetti (1997), Shi and Tsai (1998) and Machado (1993), have highlighted the non-robust nature of the traditional information theoretical criteria and stressed instead the importance of robust model selection procedures. Following Ronchetti (1997) and Machado (1993), we choose optimal values of λ for a given q by minimizing the following robust Bayesian Information Criterion (BIC):(19)BICq=−2∑i=1nLq{f(yi−xi′β^q,λσ^q,λ)}+log(n)k,where k ≤ p is the number of active portfolio positions (i.e. the number of β different from zero) and the traditional BIC is obtained when q → 1. Note that when the penalty log (n)k is replaced by 2k in (19) we obtain the robust AIC approach by Ronchetti (1997).We evaluate the performance of the proposed approaches for normal and t- portfolios (denoted by GDLNand GDLt, respectively) and compare our method to other popular penalization schemes. The data are simulated from the following models:1.Model 1: Np(μ, Σ), whereμj=1,if j ≤ k, andμj=0,if j > k, and the covariance matrix is such thatΣjj=1,j=1,…,p,and off-diagonal elementsΣjk=ρ,0 ≤ ρ < 1, j ≠ kModel 2: multivariate t- distribution tp(μ, Σ, ν), with ν degrees of freedom. The mean-variance structure is as in Model 1We comparethe resulting estimates with analogous estimates obtained by the following methods: Lasso (Tibshirani (1996)), the Zhang-penalty method by Gasso et al. (2009) withη=0.2,and the logarithm penalization by Weston, Elisseeff, and Schölkopf (2003) withϕ=1.5. For each method we select the solutions with the lowest BIC, as explained in Section 3.4. All the methods except Lasso involve the solution of non-convex problems, which we solve by the DC-programming approach proposed by Gasso et al. (2009). The optimization relies on iterative primal-dual approach for the non-convex penalty functions considered, where in each iteration a convex primal problem is solved by the gradient projection algorithm of Figueiredo et al. (2007). We use the MATLAB toolbox developed in Gasso et al. (2009)33Available at http://asi.insa-rouen.fr/enseignants/~arakoto/dclasso.zip.to implement the dual-convex optimization algorithm and compute the estimates for our GDL approach and its benchmarks, namely the LASSO, Log and Zhang penalties. The optimization algorithms have been implemented in MATLAB 8. To initialize all the algorithms we used ordinary least squares estimates, except for the EM algorithm of the GDLtapproach. In our numerical experiments, we noticed that the convergence of the EM algorithm can be sensitive to the initial estimates of the β coefficients, as also discussed in the statistical literature, so we use the robust estimates obtained by GDLNapproach as initial values. Both GDLtand GDLNare initialized using equal weightswq,i=1/n,i=1,…,n,andvq,j=1/p,j=1,…,p. The iterative algorithm for normal portfolios and the EM algorithm converge after a small number of iterations. For Model 1, the average number of iterations was 6.42 (se=0.02) and 4.04 (se=0.02) for GDLNand GDLt, respectively.Figs. 2 and 3 show boxplots for the number of active positions, F-measure and MSE (in logarithmic scale), obtained under mildly and strongly correlated data (ρ=0.2,0.8andq=0.9). In terms of sparsity, the GDL algorithms perform considerably better than all other approaches in the first two models (panels (a) and (d)), regardless of the correlation level. The number of active regressors is close to the optimal valuek=10. The GDL always outperforms the Lasso and Zhang penalties, which give over-fitting solutions with too many coefficients different from zero. The Log penalty shows a good performance whenρ=0.8. This is not surprising as previous studies have already shown the considerable model selection accuracy of the Log penalty in the presence of highly correlated data (Gasso et al., 2009). Panels (b) and (e) show that the GDL algorithms not only select the correct number of active regressors, but also tend to select the right assets more frequently compared to the other methods. Particularly, the boxplots of the F-measure are centered on larger values than those of other approaches and very close to F-value equal to 1 for low level of correlation (i.e.ρ=0.2), which corresponds to perfect matching. In terms of MSE, the Lasso has the worst overall performance, due to selecting too many unnecessary active weights (panels (c) and (f)). The good model selection performance of the GDL approaches comes with a relatively small price in terms of MSE compared to the Zhang and Log penalties. The GDLtslightly outperforms the GDLN, due to the advantage in terms of good initialization and the larger penalization for the observations in the tails.Comparing Figs. 2 and 3 shows that the GDL approaches work well regardless the amount of correlation, while the other approaches tend to keep too many variables, with the Log penalty being the only exception forρ=0.8. Clearly, this leads to smaller values of MSE, while the model selection performance, as quantified by the F-measures, is still inferior to the GDL approaches.Fig. 4shows the boxplots for the GDLtforq=0.5,0.7,0.9whenρ=0.2and 0.6. For Model 1, smaller values of q < 1 lead to a more parsimonious selection with fewer active components, while better F-measure values are reached forρ=0.2andq=0.7. In terms of MSE, in Model 1 we expect the GDLtapproach withq=0.9to perform best, as there is no need to robustify the estimates with respect to potential extreme values. For Model 2, using the GDLtaccounts automatically for the possible presence of fat tails, leading to satisfactory results for all q levels. However, when the correlation level increases, the degree of robustness provided byq=0.9does not give sufficiently sparse models, despite still resulting in satisfactory estimates when looking at the log(MSE).Summing up, the optimal choice of q depends on the distribution of the data. A larger value of q should be preferred when data do not exhibit fat tails, since there is no need to introduce robustness to improve the quality of the estimates. On the other hand, smaller values of q help to reduce the number of active beta in presence of high correlation and fat tails, despite paying a small price in terms of accuracy.One of the most important challenges in asset allocation is the so-called index-tracking problem. The aim of classic index tracking is to replicate the performance of a financial index by using a relatively small subset of its constituents. Indeed, investing in all index components would imply paying a fee proportional to the number of assets every time a transaction is needed (i.e when acquiring the assets and re-balancing their weights). A sparse tracking portfolio allows us to limit these transaction and monitoring costs. Typically, the problem can be set-up as a regression problem subject to a budget constraint (i.e. the sum of the coefficients of theβvector has to sum to 1) and a constraint on the 0-norm of theβvector, by imposing a maximum number of active positions. The latter constraint and the typical large dimensionality of the problem makes the optimization NP-hard and different solutions have been proposed in the literature (see Beasley, Meade, and Chang (2003) for a review). Recently, Giamouridis and Paterlini (2010) and Fastrich, Paterlini, and Winker (2014) have shown that using a Lasso or a non-convex approach can be beneficial in hedge fund replication and index tracking, by allowing to select automatically sparse solutions with good out-of-sample properties.In this section, we apply our new methodology to obtain optimal sparse tracking portfolios for a set of financial indexes with a different number of constituents. We consider daily observations from March 23, 2002 to March 27, 2008 of three financial indexes, Fama & French 100, S&P 200 and S&P 500, with number of constituents p equal to 100, 200 and 500, respectively. In Fig. 5, we show the log-returns for all indexes along with the normal and t- distributions, fitted by the maximum likelihood estimation method. The data are slightly asymmetric and leptokurtic and a satisfactory fit is given by the t- distribution with approximately 4 degrees of freedom. The F&F 100 is a capitalization-weighted index where 8.16% of the assets have weight larger than 5%, 11.23% have weights values between 1% and 5%, and the remaining 80.61% have weights smaller than 1%. The S&P indexes are market-value weighted indexes, characterized by much smaller positions in the constituents: no weight is larger than 5% and the majority of weights are smaller than 1% (89% for S&P 200, 95.80% for S&P 500).Our aim is to build sparse portfolios that can mimic the target index over time. Given the return times series of the index yiwithi=1,…,n,we use a rolling window scheme. The rationale of our study is to take the viewpoint of an investor who decides on his optimal allocation by exploiting the information in the in-sample window, and then holds the portfolio for one week, before revising it. For each dataset, we compare six tracking techniques: the GDLNand the GDLtapproaches withq=0.9andq=0.5,the Lasso, and the equally weighted strategy (1/N). We test the out-of-sample performance of these methods as follows: we adjust the asset weights periodically based on a rolling window of 250 in-sample observations (i.e. one trading year), moving each time window ahead by 5 observations (i.e. one week) and then discarding the oldest five data points for a total ofM=103windows and nos= 513 daily out-of-sample returns. For each window (m=1,…,M), we setymto be a vector of 250 in-sample index returns andy˜mequal to the first out-of-sample index return not included in window m. Then, we select the optimal portfolioβ^maccording to the BIC criterion, as described in Section 3.4. We assume to hold the portfolio fixed for five days and compute its out-of-sample returns in the next week asy^m=X˜m′β^m,withm=1,…,M,whereX˜mis the 5 × p matrix of the first 5 out-of-sample asset returns not included in window m andβ^mis the vector of asset weights estimated in window m.We assess the portfolio selection by looking at different performance measures. The out-of-sample excess return time seriesERm=y˜m−y^m,m=1,…,Mis determined by the difference between the index and the tracking portfolio on the entire out-of-sample period, while the tracking error volatility TEV is the standard deviation of the out-of-sample excess return. The smaller the TEV, the better the tracking performance of the optimal portfolio. If the portfolio is nearly tracking the index, we also expect the mean excess return, ER =1nos∑m=1MERm,to be as close as possible to zero as well as the information ratio, IR = ER/TEV, which is a typical measure of the risk/return performance. Moreover, to assess the tracking ability, we compute the portfolios’ out-of-sample correlation (Cor) between the time series of the out-of-sample log-returns of the index and the tracking portfolio. If the portfolio perfectly tracks the index, we expect Cor to be equal to 1. Finally, to evaluate sparsity and stability of portfolios, which have a direct impact on transaction costs, we compute the average number of active positions,k¯=1M∑m=1MI(|β^m|≠0),and the average turnover, TO =∑m=2M|β^m−β^m−1|/(M−1).Table 1reports the out-of-sample statistics of the six portfolios for the three indexes. Since the target is to track an index as closely as possible instead of outperforming it, an ideal tracking portfolio should have the smallest out-of-sample tracking volatility TEV (Column 2) and mean excess returns close to zero (Column 3). We observe that the GDL strategies constantly obtain lower TEVs compared to Lasso and ERs closest to zero overall. These results are consistent with our simulation analysis, showing that the GDL approaches usually identify solutions with smaller MSE. At the same time, the GDL methods obtain sparser portfolios than the Lasso, as shown by the average number of active positionsk¯in Column 5. The smaller levels of TEVs of the GDL approaches are obtained by using approximately 37% of the available positions for F&F 100, less than 15% for S&P 200 and less than 10% for S&P 500. Lasso identifies optimal tracking portfolios with always more than 65 constituents for each index. However, investing in a larger number of constituents gives a turnover rate lower than the GDL, as shown in Column 6. The GDL portfolios have values of correlation with respect to the index very close to 1. As expected, in terms of risk/return performance and correlation, the 1/N portfolio is a tough benchmark to beat, especially for S&P 200 and S&P 500, which are market weighted indexes with about 90% of weights smaller than 1%, while F&F 100 is a value-weighted index. However, investing in the equally weighted portfolio would imply having a position in each constituent, which could be undesirable due to monitoring and transaction costs. As expected, the GDL approaches reach IR values closer to zero than 1/N. This aspect is illustrated in Fig. 6, where we show the out-of-sample returns, rebased to 100, of the 1/N, Lasso and the GDL approaches for F&F 100 and S&P 200. A portfolio close to the index indicates a good out-of-sample tracking performance. The GDL approach provides satisfactory replications of the index by using a relatively small subset of its components. Instead, 1/N and Lasso show returns patterns which, despite outperforming the indexes in the selected period, are often far away from the indexes.We further test the tracking performance of the GDL approach on the period during and after the financial crisis. In particular, we extend the dataset for S&P 200 and S&P 500 from March 28, 2008 to March 1, 2011. The new data are characterized by the presence of high volatility, high correlation and extreme losses. As expected from our simulation analysis, in the crisis period, when the correlation between the index components ρ dramatically increases, the GDL approach selects less sparse portfolios and exhibits larger TEV and lower correlation with respect to the index, compared to the pre-crisis period. Nevertheless, it is able to outperform the Lasso in terms of sparsity and tracking performance, having in all but two cases smaller TEV and always average ER closer to zero. As GDL performance suffers from increased level of correlation between assets and presence of extreme losses during highly volatile markets, we believe that explicitly considering even fatter tail distributions than the t-Student could improve the tracking performance during crises. This is high on our research agenda.

@&#CONCLUSIONS@&#
