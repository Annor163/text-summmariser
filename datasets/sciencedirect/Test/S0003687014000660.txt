@&#MAIN-TITLE@&#
Validating the Strategies Analysis Diagram: Assessing the reliability and validity of a formative method

@&#HIGHLIGHTS@&#
Evidence regarding the performance of Ergonomics methods is often not forthcoming.We evaluated the reliability and validity of the Strategies Analysis Diagram method.Individual results were average. Pooled group results were significantly better.A structured multi-analyst approach is proposed for the Strategies Analysis Diagram.

@&#KEYPHRASES@&#
Strategies Analysis Diagram,Cognitive Work Analysis,Validation,

@&#ABSTRACT@&#
The Strategies Analysis Diagram (SAD) is a recently developed method to model the range of possible strategies available for activities in complex sociotechnical systems. Previous applications of the new method have shown that it can effectively identify a comprehensive range of strategies available to humans performing activity within a particular system. A recurring criticism of Ergonomics methods is however, that substantive evidence regarding their performance is lacking. For a method to be widely used by other practitioners such evaluations are necessary. This article presents an evaluation of criterion-referenced validity and test-retest reliability of the SAD method when used by novice analysts. The findings show that individual analyst performance was average. However, pooling the individual analyst outputs into a group model increased the reliability and validity of the method. It is concluded that the SAD method's reliability and validity can be assured through the use of a structured process in which analysts first construct an individual model, followed by either another analyst pooling the individual results or a group process pooling individual models into an agreed group model.

@&#INTRODUCTION@&#
Cognitive Work Analysis (CWA; Rasmussen et al., 1994; Vicente, 1999) is an Ergonomics method that is used to describe and evaluate complex sociotechnical systems. CWA has been applied in various complex sociotechnical systems such as defence (Burns et al., 2004), aviation (Ahlstrom, 2005), road transport (Cornelissen et al., 2013), and process control (Vicente, 1999).CWA is one of few formative methods within the discipline of Ergonomics. Formative methods aim to describe potential ways in which a system can operate, rather than describing how a system should operate (normative methods) or actually operates (descriptive methods) (Vicente, 1999). Formative methods are an important category of methods, as they are used to assist the design of adaptive and flexible systems and tools that can cope with non-routine situations. Non-routine situations, driven by emergence, are a feature of complex sociotechnical systems. Such systems exist in a highly changeable and demanding environment, in which adaptive capacity is essential (Vicente, 1999; Woods, 1988). Contemporary safety related concepts such as resilience (cf. Hollnagel, 2006) and performance variability (cf. Hollnagel, 2002, 2004) further underline the importance of methods to take adaptability of complex sociotechnical systems into account.Despite the popularity of CWA, reliability and validity analyses of formative methods remain largely absent from the Ergonomics literature. One of the few efforts to establish validity of CWA was conducted by Burns et al. (2004). They conducted a qualitative post hoc comparison of two independently conducted Work Domain Analyses (WDA), the first phase of CWA, on similar defence systems. More formal validation studies, including validation of latter phases of the CWA framework, are currently lacking.A recurring criticism aimed at the Ergonomics discipline is that some of the methods are only used by their developers and that methods are often chosen by practitioners based on familiarity and ease of use rather than based on reliability and validity evidence (Stanton et al., 2013). Many practitioners are unaware of whether the methods are reliable and valid (Stanton and Young, 1998). Validation studies can benefit Ergonomics methods by providing clear evaluation and empirical evidence of their performance and value (Stanton and Young, 1999, 2003). Further, a key requisite of ergonomics methods is that they are usable by non-experts and that they achieve acceptable levels of performance when used by other analysts (Stanton and Young, 1999, 2003). Without this, uptake of the method by ergonomics practitioners, designers and engineers may be limited.The aim of the study reported in this article was to provide a more formal reliability and validity analysis of CWA. This study was conducted using non-expert analysts to provide an evaluation of CWA's level of performance when used by other analysts and ensure uptake of the method by ergonomics practitioners.The CWA framework comprises five phases (Vicente, 1999). Each phase models a different set of constraints. First, WDA models the system constraints by describing what the system is trying to achieve and how and with what it achieves its purpose. Second, Control Task Analysis models situational constraints and decision-making requirements. Third, Strategies Analysis models ways in which activities within the system can be carried out. Fourth, social organisation and cooperation analysis models communication and coordination demands imposed by organisational constraints. Fifth, worker competencies analysis describes skills, rules and knowledge required for the activities possible within the system.To date, applications have focussed on the development and application of the first two phases: WDA and Control Task Analysis. The third phase, Strategies Analysis, is useful for providing insight into the different response options that enable a systems adaptive capacity. This phase has traditionally not been as well developed and neither applied as often as the earlier phases. Recently, the Strategies Analysis has regained attention (Cornelissen et al., 2012, 2013; Hassall and Sanderson, 2012; Hilliard et al., 2008). In particular, Cornelissen et al. (2013) developed and applied a structured method, the Strategies Analysis Diagram (SAD) to model strategies available in complex sociotechnical systems. Initial evaluations of this method have gathered evidence of the methods effectiveness to identify strategies possible (Cornelissen et al., 2012, 2013.); however, the method would benefit from more formal evaluations. This paper is a response to this requirement.The SAD method models how activities can potentially be executed within a system's constraints. It also models criteria for when or why work will be executed in a certain way. SAD builds upon the first phase of the CWA framework by adding verbs and criteria to the constraints identified. This allows further specification of courses of action possible within the system's constraints as well as criteria influencing the employment of courses of action.The SAD, see Fig. 1, is a networked hierarchical diagram using means ends links to represent ‘how’ and ‘why’ relationships between the different levels of the diagram. Links upwards explain why a certain object or function is there, whereas links downwards explain how a system works to achieve its purpose or execute its functions. The levels transferred from the WDA are illustrated in light grey and the SAD specific levels are dark grey.The lower half of the SAD models how activities can potentially be executed. The diagram describes, bottom up, verbs that describe potential interactions with or manipulations of objects in the system (e.g. follow), physical objects present in the system (e.g. lane markings), object related processes afforded by the physical objects (e.g. display information) and purpose related functions describing activities that need to be carried out for the system to achieve its purpose (e.g. determine path). The top half of the diagram describes why certain ways of work, or courses of action, are seen within the system. The top of the diagram describes top-down the functional purpose, this is the reason why the system exists (e.g. support negotiation of right hand turn by road users), values and priority measures evaluating the effectiveness of a system and driving behaviour (e.g. safety) and criteria, describing when certain courses of action at the lower end of the diagram are valid or likely to be chosen (e.g. high traffic volume). Together, nodes from the different levels provide analysts with syntax for strategy definition. For example bottom up a strategy could be defined as ‘assess’ ‘road users’ ‘show behaviour’ when ‘avoiding conflict with other road users’. Assess whether the ‘road user is unfriendly’, for ‘safety’ purposes and to ‘support negotiation of right hand turns by road users’.Based on initial evaluations of the SAD method (Cornelissen et al., 2012; Cornelissen et al., 2013) the evidence suggests that, when used by its developers, it can effectively identify a comprehensive range of strategies available to humans performing activity within a particular system. Following this, the next critical step is to more formally evaluate the methods usefulness and ensure that the SAD can be applied by other practitioners. The methods' reliability and validity when used by novices in the SAD method should be established.The aim of the study reported in this article was to evaluate the performance of the SAD method when used by analysts not already skilled in the CWA framework. Specifically, the study aimed to evaluate SAD in terms of its reliability and validity, when used by novice analysts to identify the range of strategies available to different road user groups at intersections. This allows for a more ample validation and reliability evaluation of formative methods.Evaluating the methods criterion-referenced validity entails ensuring that the method allows analysts to come up with a SAD that contains accurate, and not too much irrelevant, content. Evaluating its test-retest reliability comprises ensuring that the method produces a similar model when used by the same analyst for the same system more than once.Studies assessing the validity of Ergonomics methods have been reported in the literature (Baber and Stanton, 1996; Stanton et al., 2009; Stanton and Young, 2003). Many of those have focussed on human reliability and error prediction methods (Baysari et al., 2011; Kirwan et al., 1997; Stanton and Young, 2003). In those studies, the validity of methods was assessed by comparing a method's results (e.g. errors predicted) against actual observations (e.g. errors observed). Since the CWA framework is formative and models behaviour possible and include behaviour beyond that currently prescribed or actually seen within a system, a comparison of the methods results with actual observations would not provide sufficient conclusive evidence about the validity of the method.Alternatively, CWA results could be compared to results using a similar but validated method. However, CWA is unique in that it is a constraints-based approach and is one of few formative methods and substantial validation studies of such methods remain absent from the literature to date. Therefore, no method is available that could be used for SAD's validation.Expert models are used when other validated standards are not available (Gordon et al., 2005). Expert models in the context of CWA are models developed by expert analysts who are highly experienced in applying the methods from the CWA framework. It is worth noting that expert CWA models are not normative models representing the expert's knowledge of a system, but rather well developed formative CWA analyses conducted by expert analysts. The assumption is that the expert model is most accurate and highly valuable in circumstances in which uncertainty exists and behaviour has yet to occur, is noisy or complex (Bolger and Wright, 1992). In absence of other objective standards, a model developed by expert analysts with no time constraints, is likely to be the best standard against which to compare other analysts' CWA results.Once the standard against which the novice analyst's results will be assessed is established, measures to assess the quality of the novice results have to be determined. Quantitative methods to compare a expert results versus novices results (or predicted versus actual outcomes) are often based on the use of signal detection theory to calculate the sensitivity of the method under analysis (Baber and Stanton, 1994; Stanton et al., 2009; Stanton and Young, 2003). The signal detection theory sorts the method's outputs into hits, misses, false alarms and correct rejections. Hits represent items identified by novice analysts that were also identified by expert analysts. Misses represent items that were identified by expert analysts but not by novice analysts. False alarms refer to those items identified by novice analysts and not the expert analysts. Correct rejections are those items neither identified by the expert or novice analysts. The signal detection theory metrics are commonly used to assess the reliability and validity of ergonomics methods such as human error prediction (Stanton et al., 2009); however, not all of the metrics may usefully apply to formative methods.Since formative methods describe things that could be possible within a particular system, it is complicated to use the correct rejections category because they are unknown and possibly infinite. That is, theoretically the total number of items that could be included is anything in the world as we know it, as formative methods are not restrained by what is currently happening or should be happening. Therefore using an artificial number for the number of items representing the world would be infinite and it is hard to know what items were actively rejected from this large pool of items by expert and novice analysts. While others (Stanton and Baber, 2002; Stanton and Stevenage, 1998) have been able to argue for a theoretical maximum based on a set number of tasks and error categories provided by a taxonomy, such theoretical maximum would be artificially inflated when used for formative methods. Therefore, it is argued that measures using correct rejections are not suitable for assessing the validity of the SAD method.Measures involving hits, misses or false alarms can be used for the evaluation of CWA. Such measures include hit rate (hits divided by hits and misses), which allows comparison of items identified by novice analysts versus items identified by expert analysts. The false alarm rate cannot be used here as that includes using correct rejections. To still account for the number of false alarms a novice analyst identifies, a measure often used in clinical studies or recall studies can be used: positive predictive value (Descatha et al., 2009; Lindegård Andersson and Ekman, 2008). Predictive value (hits divided by hits and false alarms) reflects the amount of items identified by novice analysts that were also identified by expert analysts compared to the total number of items identified by novice analysts.Reliability of Ergonomics methods is often assessed using a test-retest paradigm (Baysari et al., 2011). Measures include percentage agreement (Baber and Stanton, 1996; Baysari et al., 2011) and Pearson's correlation (Harris et al., 2005; Stanton and Young, 2003). As discussed above, the formative nature of CWA provides some challenges to traditional Ergonomics reliability and validity measures. To avoid unwarranted overcomplicating of analysis, percentage agreement was applied here to evaluate the test-retest reliability. The reliability measure compares novice analyst's items identified at two different times.CWA is a resource intensive method. The formative nature of the method requires participants to go beyond what they currently know which is a challenging activity. Further, the analysis concerns a complex system of which analysts may understand a part of the system in great detail while missing the lack of knowledge on other parts. A multi-analyst approach, using more than one analyst to conduct the analysis, is therefore practicable for a method such as CWA to decrease resources required and compensate for shortfalls of individual analysts (Stanton et al., 2009).Other validation studies have pooled individual results (Harris et al., 2005) or proposed multiple analyst approaches (Stanton et al., 2009). These approaches are suggested to increase the validity and comprehensiveness of the methods' results.Unfortunately, one of the main weaknesses of a multi-analyst approach is the increase in false alarm rate (e.g. items identified that are not accurate) (Stanton et al., 2009). To ensure that the benefits of a multi-analyst approach outweigh the cost of introducing false alarms, it is worth exploring strategies to reduce the false alarms and ensure the quality of the output. For example, it is assumed that if a method is accurate, relevant content should be identified more consistently than irrelevant content. A practical solution for a multi-analyst approach to CWA would be to only include items if more than one novice analyst generated that item or multiple analysts agreed on the relevance of that item in a group discussion. Therefore, the present validity analysis includes both an unadjusted multi-analyst model (collating all individual items) and an adjusted pooled multi-analyst model (collating all individual items into one model, but eliminating all items that have only been identified by one novice analysts) to evaluate the value of a multi-analyst approach to SAD.Fig. 2summarises the approach taken to evaluate the SAD method. Validity was measured by quantifying hit rate and predictive value of participant's results. Reliability was be measured by using a test-retest paradigm. Results were analysed for individual analysts as well as for a pooled multi-analyst approach.There were a number of hypotheses. It is expected that if the SAD method is valid, novice analysts' models will be similar to the expert analyst model. It is expected, however, that novice analyst's models will fail to produce complete coverage of the expert analyst's model within the constraints of the study. Reasons for this include the use of novice analysts, time constraints of a reliability and validity study and the semi-structured formative approach of SAD. It is expected that by using a multi-analyst approach, and especially an adjusted multi-analyst model, results will improve and resemble the expert analyst's model better. If SAD can be used reliably to conduct a SAD and elicit strategies, novice analysts' models over time are expected to be similar.

@&#CONCLUSIONS@&#
